# 

## 分布式题目集锦

### 1. 负载均衡算法

- **随机访问策略。** 系统随机访问，缺点：可能造成服务器负载压力不均衡，俗话讲就是撑的撑死，饿的饿死。
- **轮询策略。** 请求均匀分配，如果服务器有性能差异，则无法实现性能好的服务器能够多承担一部分。
- **权重轮询策略。** 权值需要静态配置，无法自动调节，不适合对长连接和命中率有要求的场景。
- **Hash取模策略。** 不稳定，如果列表中某台服务器宕机，则会导致路由算法产生变化，由此导致命中率的急剧下降。


简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 0 ~ 2^32-1（即哈希值是一个32位无符号整形），整个哈希环如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/af8cec425b2943e896f1f913071f3e32.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bCP55Sf5Yeh5LiA,size_20,color_FFFFFF,t_70,g_se,x_16)
多个服务器都通过这种方式进行计算，最后都会各自映射到圆环上的某个点，这样每台机器就能确定其在哈希环上的位置，如下图所示。


![   ](https://img-blog.csdnimg.cn/f0738b54150445bd962072a39f8be144.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bCP55Sf5Yeh5LiA,size_20,color_FFFFFF,t_70,g_se,x_16)


那么用户访问，如何分配访问的服务器呢？我们根据用户的 IP 使用上面相同的函数 Hash 计算出哈希值，并确定此**数据在环上的位置**，从此位置沿环顺时针行走，遇到的第一台服务器就是其应该定位到的服务器。

![在这里插入图片描述](https://img-blog.csdnimg.cn/768d96b4323f48a1ba02d54b56470f21.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bCP55Sf5Yeh5LiA,size_20,color_FFFFFF,t_70,g_se,x_16)



### 2. 熔断和降级
###  2.1 熔断
一般是某个服务故障或者是异常引起的，当某个异常条件被触发，直接熔断整个服务，而不是一直等到此服务超时，为了防止防止整个系统的故障。

而采用了一些保护措施。过载保护。比如A服务的X功能依赖B服务的某个接口，当B服务接口响应很慢时，A服务X功能的响应也会被拖慢，进一步导致了A服务的线程都卡在了X功能上，A服务的其它功能也会卡主或拖慢。此时就需要熔断机制，即A服务不在请求B这个接口，而可以直接进行降级处理。


###  2.2 降级
服务器当压力剧增的时候，根据当前业务情况及流量，对一些服务和页面进行有策略的降级。以此缓解服务器资源的的压力，以保证核心业务的正常运行，同时也保持了客户和大部分客户的得到正确的响应。

自动降级：超时、失败次数、故障、限流

（1）配置好超时时间(异步机制探测回复情况)；

（2）不稳的api调用次数达到一定数量进行降级(异步机制探测回复情况)；

（3）调用的远程服务出现故障(dns、http服务错误状态码、网络故障、Rpc服务异常)，直接进行降级。

人工降级：秒杀、双十一大促降级非重要的服务。


### 3. 幂等

> 幂等性的核心思想，其实就是保证这个接口的执行结果只影响一次，后续即便再次调用，也不能对数据产生影响，之所以要考虑到幂等性问题，是因为在网络通信中，存在两种行为可能会导致接口被重复执行。

用户的重复提交或者用户的`恶意攻击`，导致这个请求会被多次重复执行。

在分布式架构中，为了避免网络通信导致的数据丢失，在服务之间进行通信的时候都会设计超时重试的机制，而这种机制有可能导致服务端接口被重复调用。所以在程序设计中，对于数据变更类操作的接口，需要保证接口的幂等性。

**使用 redis 里面提供的 setNX 指令，比如对于MQ消费的场景，为了避免MQ重复消费导致数据多次被修改的问题，可以在接受到MQ的消息时，把这个消息通过setNx写入到redis里面，一旦这个消息被消费过，就不会再次消费。**

1. 建去重表，将业务中由唯一标识的字段保存到去重表，如果表中存在，则表示已经处理过了。
2. 版本控制，增加版本号，当版本号符合时候，才更新数据。
3. 状态控制，例如订单有状态已支付，未支付，支付中，支付失败，当处于未支付的时候才允许修改成支付中。

### 4. 分布式事务
在分布式系统中，一次业务处理可能需要多个应用来实现，比如用户发送一次下单请求，就涉及到**订单系统创建订单，库存系统减库存**，而对于一次下单，订单创建与减库存应该是要同时成功或者同时失效，但在分布式系统中，如果不做处理，就很有可能`订单创建成功，但是减库存失败`，那么解决这类问题，就需要用到分布式事务，常用的解决方案如下：

1. 本地消息表：创建订单时，将减库存消息加入在本地事务中，一起提交到数据库存入本地消息表，然后调用库存系统，如果调用成功则修改本地。
2. 消息状态为成功，如果调用库存系统失败，则由后台定时任务从本地消息表中取出未成功的消息，重试`调用库存系统`。
3. 消息队列：目前 RocketMQ 中支持事务消息，它的工作原理是：
    1. 产订单系统先发送一条 half 消息到 Broker， half 消息对消费者而言是不可见的。
    2. 再创建订单，根据创建订单成功与否，向 Broker 发送 commit 或 rollback。
    3. 并且生产者订单系统还可以提供 Broker 回调接口，当 Broker 发现一段时间 half 消息没有收到任何操作命令，则会主动调此接口来查询订单是否创建成功。
    4. 如果消费失败，则根据`重试策略`进行重试，最后还失败则进入`死信队列`，等待进一步处理。

### 5. 分布式锁
在单体架构中，多个线程都是属于同一个进程的，所以在线程并发执行时，遇到资源竞争时，可以利用ReentrantLock、 synchronized等技术来作为锁来共享资源的使用。

而在分布式架构中，多个线程是可能处于不同进程中的，而这些线程并发执行遇到资源竞争时，利用 ReentrantLock synchronized 等技术是没办法，来控制多个进程中的线程的，所以需要分布式锁，意思就是，需要一个分布式锁生成器，分布式系统中的应用程序都可以来使用这个生成器所提供的锁，**从而达到多个进程中的线程使用同一把锁。**

### 6. 分布式主键id
在开发中，我们通常会需要一个`唯一ID`来标识数据，如果是单体架构，我们可以通过数据库的主键，或直接在内存中维护一个自增数字来作为ID都是可以的，但对于-个分布式系统，就会有可能会出现ID冲突,此时有以下解决方案:
1. uuid，这种方案复杂度最低，但是会影响存储空间和性能。
2. 利用单机数据库的自增主键，作为分布式ID的生成器，复杂度适中，ID长度较之uuid更短，**但是受到单机数据库性能的限制，并发量大的时候，此方案也不是最优方案。**
3. 利用redis、zookeeper的特性来生成id，比如`redis的自增命令、zookeeper的顺序节点`， 这种方案和单机数据库(mysq|)相比，性能有所提高，可以适当选用。
4. 4.雪花算法，一切问题如果能直接用算法解决，那就是最合适的，利用雪花算法也可以生成分布式ID，底层原理就是通过`某台机器在某一毫秒内对某一个数字自增`，这种方案也能保证分布式架构中的系统 id 唯一，但是只能保证`趋势递增`。业界存在`tinyid、 leaf` 等开源中间件实现了雪花算法。


### 7. 池化技术
对象池技术基本原理的核心有两点：`缓存和共享`，即对于那些被频繁使用的对象，在使用完后，不立即将它们释放，而是将它们缓存起来，以供后续的应用程序重复使用，**从而减少创建对象和释放对象的次数，进而改善应用程序的性能。**

> 事实上，由于对象池技术将对象限制在一定的数量，也有效地减少了应用程序内存上的开销。


### 其他
#### 2、分布式系统中的接口调用如何保证顺序性？
可以接入 MQ，如果是系统 A 使用多线程处理的话，可以使用内存队列，来保证顺序性，如果你要 100%的顺序性，当然可以使用分布式锁来搞，会影响系统的并发性。

#### 3、说说 ZooKeeper 一般都有哪些使用场景？
- 分布式协调：这个其实就是 ZooKeeper 很经典的一个用法，简单来说，就好比，你系统 A 发送个请求到 MQ，然后 B 消费了之后处理。那 A 系统如何指导 B 系统的处理结果？用 ZooKeeper 就可以实现分布式系统之间的协调工作。A 系统发送请求之后可以在 ZooKeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 ZooKeeper 那个节点的值，A 立马就可以收到通知，完美解决。
- 分布所锁：对某一个数据联系发出两个修改操作，两台机器同时收到请求，但是只能一台机器先执行另外一个机器再执行，那么此时就可以使用ZooKeeper 分布式锁，一个机器接收到了请求之后先获取 ZooKeeper 上的一把分布式锁，就是可以去创建一个 znode，接着执行操作，然后另外一个机器也尝试去创建那个 znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等等一个机器执行完了自己再执行。
- 配置信息管理：ZooKeeper 可以用作很多系统的配置信息的管理，比如Kafka，storm 等等很多分布式系统都会选用 zk 来做一些元数据，配置信息的管理，包括 Dubbo 注册中心不也支持 ZooKeeper 么。
- HA 高可用性：这个应该是很常见的，比如 hdfs，yarn 等很多大数据系统，都选择基于 ZooKeeper 来开发 HA 高可用机制，就是一个重要进程一般会主备两个，主进程挂了立马通过 ZooKeeper 感知到切换到备份进程。



#### 4、说说你们的分布式 session 方案是啥？怎么做的？
- Tomcat + Redis
其实还挺方便的，就是使用 session 的代码跟以前一样，还是基于 tomcat 原生的 session 支持即可，然后就是用一个叫做 tomcat RedisSessionManager 的东西，让我们部署的 tomcat 都将 session 数据存储到 Redis 即可.
- Spring Session + Redis
分布式会话的这个东西重耦合在 tomcat，如果我要将 web 容器迁移成 jetty，不能重新把 jetty 都配置一遍.所以现在比较好用的还是基于 java 的一站式解决方案，使用 spring session是一个很好的选择，给 spring session 配置基于 Redis 来存储 session 数据，然后配置一个 spring session 的过滤器，这样的话session 相关操作都会交给 spring session 来管了。接着在代码中，就是用原生的 session 操作，就是直接基于 spring session 从 Redis 中获取数据了

#### 5、分布式事务了解吗？
- XA 方案/两阶段提交方案
    - 第一个阶段（先询问）
    - 第二个阶段（再执行）
- TCC 方案
    - TCC 的全称是：Try、Confirm、Cancel

这个其实是用到了补偿的概念，分为了三个阶段
- Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留
- Confirm 阶段：这个阶段说的是在各个服务中执行实际的操作
- Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经成功的业务逻辑的回滚操作

- 本地消息表
- 可靠消息最终一致性方案
- 最大努力通知方案


#### 6、那常见的分布式锁有哪些解决方案？
- Reids 的分布式锁，很多大公司会基于 Reidis 做扩展开发
- 基于 Zookeeper
- 基于数据库，比如 MySQL
#### 7、ZK 和 Redis 的区别，各自有什么优缺点？
先说 Redis：
- Redis 只保证最终一致性，副本间的数据复制是异步进行（Set 是写，Get是读，Reids 集群一般是读写分离架构，存在主从同步延迟情况），主从切换之后可能有部分数据没有复制过去可能会丢失锁情况，故强一致性要求的业务不推荐使用 Reids，推荐使用 ZooKeeper。
- Redis 集群各方法的响应时间均为最低。随着并发量和业务数量的提升其响应时间会有明显上升（公有集群影响因素偏大），但是极限 QPS 可以达到最大且基本无异常。


再说 ZK：
- 使用 ZooKeeper 集群，锁原理是使用 ZooKeeper 的临时节点，临时节点的生命周期在 Client 与集群的 Session 结束时结束。因此如果某个 Client节点存在网络问题，与 ZooKeeper 集群断开连接，Session 超时同样会导致锁被错误的释放（导致被其他线程错误地持有），因此 ZooKeeper 也无法保证完全一致。

- ZK 具有较好的稳定性；响应时间抖动很小，没有出现异常。但是随着并发量和业务数量的提升其响应时间和 QPS 会明显下降。


#### 8、MySQL 如何做分布式锁？
方法一：

利用 MySQL 的锁表，创建一张表，设置一个 UNIQUE KEY 这个 KEY 就是要锁的 KEY，所以同一个 KEY 在 MySQL 表里只能插入一次了，这样对锁的竞争就交给了数据库，处理同一个 KEY 数据库保证了只有一个节点能插入成功，其他节点都会插入失败。

DB 分布式锁的实现：通过主键 id 的唯一性进行加锁，说白了就是加锁的形式是向一张表中插入一条数据，该条数据的 ID 就是一把分布式锁，例如当一次请求插入了一条 ID 为 1 的数据，其他想要进行插入数据的并发请求必须等第一次请求执行完成后删除这条 ID 为 1 的数据才能继续插入，实现了分布式锁的功能。

方法二：

使用流水号+时间戳做幂等操作，可以看作是一个不会释放的锁。




#### 9、你了解业界哪些大公司的分布式锁框架
- Google:Chubby

Chubby 是一套分布式协调系统，内部使用 Paxos 协调 Master 与 Replicas。
Chubby lock service 被应用在 GFS, BigTable 等项目中，其首要设计目标是高可靠性，而不是高性能。
Chubby 被作为粗粒度锁使用，例如被用于选主。持有锁的时间跨度一般为小时或天，而不是秒级。
Chubby 对外提供类似于文件系统的 API，在 Chubby 创建文件路径即加锁操作。
Chubby 使用 Delay 和 SequenceNumber 来优化锁机制。Delay 保证客户端异常释放锁时，Chubby 仍认为该客户端一直持有锁。Sequence number 指锁的持有者向 Chubby 服务端请求一个序号（包括几个属性），然后之后在需要使用锁的时候将该序号一并发给 Chubby 服务器，服务端检查序号的合法性，包括 number 是否有效等。

- 京东 SharkLock

SharkLock 是基于 Redis 实现的分布式锁。锁的排他性由 SETNX 原语实现，使用 timeout 与续租机制实现锁的强制释放。

- 蚂蚁金服 SOFAJRaft-RheaKV 分布式锁

RheaKV 是基于 SOFAJRaft 和 RocksDB 实现的嵌入式、分布式、高可用、强一致的 KV 存储类库。
RheaKV 对外提供 lock 接口，为了优化数据的读写，按不同的存储类型，提供不同的锁特性。RheaKV 提供 wathcdog 调度器来控制锁的自动续租机制，避免锁在任务完成前提前释放，和锁永不释放造成死锁。

- Netflix: Curator

Curator 是 ZooKeeper 的客户端封装，其分布式锁的实现完全由 ZooKeeper 完成。在 ZooKeeper 创建 EPHEMERAL_SEQUENTIAL 节点视为加锁，节点的EPHEMERAL 特性保证了锁持有者与 ZooKeeper 断开时强制释放锁；节点的SEQUENTIAL 特性避免了加锁较多时的惊群效应。
#### 10、请讲一下你对 CAP 理论的理解
在理论计算机科学中，CAP 定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：
- Consistency（一致性） 指数据在多个副本之间能够保持一致的特性（严格的一致性）

- Availability（可用性） 指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应（不保证获取的数据为最新数据）
- Partition tolerance（分区容错性） 分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障

Spring Cloud 在 CAP 法则上主要满足的是 A 和 P 法则，Dubbo 和 Zookeeper 在CAP 法则主要满足的是 C 和 P 法则。

CAP 仅适用于原子读写的 NOSQL 场景中，并不适合数据库系统。现在的分布式系统具有更多特性比如扩展性、可用性等等，在进行系统设计和开发时，我们不应该仅仅局限在 CAP 问题上。

现实生活中，大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在 CAP 理论诞生 12 年之后，CAP 之父也在 2012 年重写了之前的论文。

当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2选 1。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。

#### 11、请讲一下你对 BASE 理论的理解
BASE 理论由 eBay 架构师 Dan Pritchett 提出，在 2008 年上被分表为论文，并且 eBay 给出了他们在实践中总结的基于 BASE 理论的一套新的分布式事务解决方案。

BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和Eventually Consistent（最终一致性） 三个短语的缩写。BASE 理论是对 CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。

BASE 理论的核心思想是即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。

针对数据库领域，BASE 思想的主要实现是对业务数据进行拆分，让不同的数据分布在不同的机器上，以提升系统的可用性，当前主要有以下两种做法：
- 按功能划分数据库
- 分片（如开源的 MyCat、Amoeba 等）。
#### 12、分布式与集群的区别是什么？
分布式：一个业务分拆多个子业务，部署在不同的服务器上

集群：同一个业务，部署在多个服务器上。比如之前做电商网站搭的 redis 集群以及 solr 集群都是属于将 Redis 服务器提供的缓存服务以及 solr 服务器提供的搜索服务部署在多个服务器上以提高系统性能、并发量解决海量存储问题。
#### 13、请讲一下 BASE 理论的三要素
基本可用

基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。
但是，这绝不等价于系统不可用。

比如：
- 响应时间上的损失：正常情况下，一个在线搜索引擎需要在 0.5 秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2 秒
- 系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面

软状态

软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

最终一致性

强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

#### 14、请说一下对两阶段提交协议的理解
分布式系统的一个难点是如何保证架构下多个节点在进行事务性操作的时候保持一致性。为实现这个目的，二阶段提交算法的成立基于以下假设：

- 该分布式系统中，存在一个节点作为协调者(Coordinator)，其他节点作为参与者(Cohorts)。且节点之间可以进行网络通信。
- 所有节点都采用预写式日志，且日志被写入后即被保持在可靠的存储设备上，即使节点损坏不会导致日志数据的消失。
- 所有节点不会永久性损坏，即使损坏后仍然可以恢复。

第一阶段（投票阶段）

- 协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。
- 参与者节点执行询问发起为止的所有事务操作，并将 Undo 信息和 Redo 信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作）
- 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。

第二阶段（提交执行阶段）

当协调者节点从所有参与者节点获得的相应消息都为”同意”：
- 协调者节点向所有参与者节点发出”正式提交(commit)”的请求。
- 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。
- 参与者节点向协调者节点发送”完成”消息。
- 协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。如果任一参与者节点在第一阶段返回的响应消息为”中止”：
- 协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。
- 参与者节点利用之前写入的 Undo 信息执行回滚，并释放在整个事务期间内占用的资源。
- 参与者节点向协调者节点发送”回滚完成”消息。
- 协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。

#### 15、请讲一下对 TCC 协议的理解
Try Confirm Cancel
- Try：尝试待执行的业务 ，这个过程并未执行业务，只是完成所有业务的一致性检查，并预留好执行所需的全部资源。
- Confirm：执行业务，这个过程真正开始执行业务，由于 Try 阶段已经完成了一致性检查，因此本过程直接执行，而不做任何检查。并且在执行的过程中，会使用到 Try 阶段预留的业务资源。
- Cancel：取消执行的业务，若业务执行失败，则进入 Cancel 阶段，它会释放所有占用的业务资源，并回滚 Confirm 阶段执行的操作



























































