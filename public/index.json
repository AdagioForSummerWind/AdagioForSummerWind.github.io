[{"categories":["Coding"],"content":"https://github.com/geekhall/gof/blob/main/README.md?plain=1 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:0:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"1. 软件设计的七大原则 也叫solid原则 迪米特与合成/复用是后加的 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:1:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"Principle 1 : 开闭原则（Open Close Principle, OCP） 内容：对扩展开放，对修改关闭 实现方式： 抽象约束，封装变化 具体实例： Windows主题、网站主题，抽象主题的共同特点为抽象类， 将每个具体主题作为其子类，用户可根据需要选择或者增加新的主题而不需要修改源码。 所以它满足开闭原则 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:2:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"Principle 2 : 里氏替换原则(Liskov Substitution Principle, LSP) 内容： 继承必须确保超类所拥有的性质在子类中仍然成立，主要阐述了有关继承的一些原则，也就是什么时候应该使用继承，什么时候不应该使用继承， 只有当子类可以替换掉父类、软件单位的功能不受影响时，父类才能真正被复用，而子类也能够在父类的基础上增加新的行为 正是由于里氏代换原则，才使得开放-封闭成为了可能。 任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。 在使用继承时，遵循里氏替换原则，在子类中尽量不要重写父类已经实现了的方法。 实现方式： 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法 子类中可以增加自己特有的方法 当子类的方法重载父类的方法时，方法的前置条件（即方法的输入参数）要比父类的方法更宽松 当子类的方法实现父类的方法时（重写/重载或实现抽象方法），方法的后置条件（即方法的的输出/返回值）要比父类的方法更严格或相等 具体实例： 几维鸟不是鸟 正方形不是长方形 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:3:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"Principle 3 : 依赖倒置原则 (Dependence Inversion Principle, DIP) 内容：高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象 其核心思想是：要面向接口编程，不要面向实现编程 依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。 作用： 依赖倒置原则可以降低类间的耦合性。 依赖倒置原则可以提高系统的稳定性。 依赖倒置原则可以减少并行开发引起的风险。 依赖倒置原则可以提高代码的可读性和可维护性。 实现方法：面向接口编程 每个类尽量提供接口或抽象类，或者两者都具备。 变量的声明类型尽量是接口或者是抽象类。 任何类都不应该从具体类派生。 使用继承时尽量遵循里氏替换原则。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:4:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"Principle 4 : 单一职责原则 (Single Responsibility Principle, SRP) 主要内容：一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分 对象不应该承担太多职责，如果一个对象承担了太多的职责，至少存在以下两个缺点： 一个职责的变化可能会削弱或者抑制这个类实现其他职责的能力； 当客户端需要该对象的某一个职责时，不得不将其他不需要的职责全都包含进来，从而造成冗余代码或代码的浪费。 优点： 单一职责原则的核心就是控制类的粒度大小、将对象解耦、提高其内聚性。如果遵循单一职责原则将有以下优点。 降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。 提高类的可读性。复杂性降低，自然其可读性会提高。 提高系统的可维护性。可读性提高，那自然更容易维护了。 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。 接口一定要做到单一职责，类的设计尽量做到只有一个原因引起变化 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:5:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"Principle 5 : 接口隔离原则（Interface Segregation Principle, ISP） 内容：要求程序员尽量将臃肿庞大的接口拆分成更小的和更具体的接口，让接口中只包含客户感兴趣的方法。 定义：客户端不应该被迫依赖于它不使用的方法 定义：一个类对另一个类的依赖应该建立在最小的接口上 含义：要为各个类建立它们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 优点： 将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。 接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。 如果接口的粒度大小定义合理，能够保证系统的稳定性；但是，如果定义过小，则会造成接口数量过多，使设计复杂化；如果定义太大，灵活性降低，无法提供定制服务，给整体项目带来无法预料的风险。 使用多个专门的接口还能够体现对象的层次，因为可以通过接口的继承，实现对总接口的定义。 能减少项目工程中的代码冗余。过大的大接口里面通常放置许多不用的方法，当实现这个接口的时候，被迫设计冗余的代码。 实例： 学生成绩管理程序一般包含插入成绩、删除成绩、修改成绩、计算总分、计算均分、打印成绩信息、査询成绩信息等功能， 如果将这些功能全部放到一个接口中显然不太合理，正确的做法是将它们分别放在输入模块、统计模块和打印模块等 3 个模块中， ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:6:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"Principle 6 : 迪米特法则（Least Knowledge Principle， LKP） 内容：又叫做最少知识原则。只与你的直接朋友交谈，不跟“陌生人”说话。 含义：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 例子： 分析：明星由于全身心投入艺术，所以许多日常事务由经纪人负责处理，如与粉丝的见面会， 与媒体公司的业务洽淡等。这里的经纪人是明星的朋友，而粉丝和媒体公司是陌生人，所以适合使用迪米特法则 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:7:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"Principle 7 : 合成复用原则（Composite Reuse Principle, CRP) 内容： 又叫组合/聚合复用原则，它要求软件在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。 如果要使用继承关系，则必须严格遵循里氏替换原则。合成复用原则同里氏替换原则相辅相成的，两者都是开闭原则的具体实现规范。 重要性： 通常类的复用分为继承复用和合成复用两种，继承复用虽然有简单和易实现的优点，但它也存在以下缺点。 继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。 采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点。 它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。 新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。 实现方法： 合成复用原则是通过将已有的对象纳入新对象中，作为新对象的成员对象来实现的，新对象可以调用已有对象的功能，从而达到复用。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:8:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"一句话总结软件设计的七大原则 设计原则 一句话归纳 目的 开闭原则 对扩展开放，对修改关闭 降低维护带来的新风险 依赖倒置原则 高层不应该依赖低层，要面向接口编程 更利于代码结构的升级扩展 单一接口原则 一个类只干一件事，实现类要单一 便于理解，提高代码的可读性 接口隔离原则 一个接口只干一件事，接口要精简单一 功能解耦，高聚合、低耦合 迪米特法则 不该知道的不要知道，一个类应该保持对其它对象最少的了解，降低耦合度 只和朋友交流，不和陌生人说话，减少代码臃肿 里氏替换原则 不要破坏继承体系，子类重写方法功能发生改变，不应该影响父类方法的含义 防止继承泛滥 合成复用原则 尽量使用组合或者聚合关系实现代码复用，少使用继承 降低代码耦合 记忆口诀：访问加限制，函数要节俭，依赖不允许，动态加接口，父类要抽象，扩展不更改。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:9:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"设计模式概述 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:10:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"创建型模式 单例模式（Singleton）：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型模式（Prototype）：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法模式（FactoryMethod）：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂模式（AbstractFactory)：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者模式 (Builder)：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:11:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"创建型模式 1. 单例模式（Singleton） 定义：指一个类只有一个实例，且该类能自行创建这个实例的一种模式 单例模式的优点： 单例模式可以保证内存里只有一个实例，减少了内存的开销。 可以避免对资源的多重占用。 单例模式设置全局访问点，可以优化和共享资源的访问。 单例模式的缺点： 单例模式一般没有接口，扩展困难。如果要扩展，则除了修改原来的代码，没有第二种途径，违背开闭原则。 在并发测试中，单例模式不利于代码调试。在调试过程中，如果单例中的代码没有执行完，也不能模拟生成一个新的对象。 单例模式的功能代码通常写在一个类中，如果功能设计不合理，则很容易违背单一职责原则。 应用场景 需要频繁创建的一些类，使用单例可以降低系统的内存压力，减少 GC。 某类只要求生成一个对象的时候，如一个班中的班长、每个人的身份证号等。 某些类创建实例时占用资源较多，或实例化耗时较长，且经常使用。 某类需要频繁实例化，而创建的对象又频繁被销毁的时候，如多线程的线程池、网络连接池等。 频繁访问数据库或文件的对象。 对于一些控制硬件级别的操作，或者从系统上来讲应当是单一控制逻辑的操作，如果有多个实例，则系统会完全乱套。 当对象需要被共享的场合。由于单例模式只允许创建一个对象，共享该对象可以节省内存，并加快对象访问速度。如 Web 中的配置对象、数据库的连接池等。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:11:1","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"创建型模式 2. 原型模式（Prototype） 定义：用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象 原型模式的优点： Java 自带的原型模式基于内存二进制流的复制，在性能上比直接 new 一个对象更加优良。 可以使用深克隆方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化了创建对象的过程，以便在需要的时候使用（例如恢复到历史某一状态），可辅助实现撤销操作。 原型模式的缺点： 需要为每一个类都配置一个 clone 方法 clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则。 当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当。 实现： 原型模式包含以下主要角色。 抽象原型类：规定了具体原型对象必须实现的接口。 具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。 访问类：使用具体原型类中的 clone() 方法来复制新的对象。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:11:2","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"创建型模式 3. 工厂模式 (Factory) 简单工厂模式（Simple Factory） 定义：定义一个创建产品对象的工厂接口，将产品对象的实际创建工作推迟到具体子工厂类当中。 这满足创建型模式中所要求的“创建与使用相分离”的特点。 按实际业务场景划分，工厂模式有 3 种不同的实现方式，分别是简单工厂模式、工厂方法模式和抽象工厂模式。 简单工厂模式的主要角色如下： 简单工厂（SimpleFactory）：是简单工厂模式的核心，负责实现创建所有实例的内部逻辑。工厂类的创建产品类的方法可以被外界直接调用，创建所需的产品对象。 抽象产品（Product）：是简单工厂创建的所有对象的父类，负责描述所有实例共有的公共接口。 具体产品（ConcreteProduct）：是简单工厂模式的创建目标。 工厂方法模式（FactoryMethod） 简单工厂模式当增加新的产品时需要修改工厂类的创建产品方法，违背了开闭原则， 而工厂方法模式是对简单工厂模式的进一步抽象，可以在不修改原来代码的情况下引进新的产品，满足了开闭原则。 优点： 用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程。 灵活性增强，对于新产品的创建，只需多写一个相应的工厂类。 典型的解耦框架。高层模块只需要知道产品的抽象类，无须关心其他实现类，满足迪米特法则、依赖倒置原则和里氏替换原则。 缺点： 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 抽象产品只能生产一种产品，此弊端可使用抽象工厂模式解决。 工厂方法模式的主要角色如下。 抽象工厂（Abstract Factory）：提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法 newProduct() 来创建产品。 具体工厂（ConcreteFactory）：主要是实现抽象工厂中的抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应。 抽象工厂模式（AbstractFactory） 定义：是一种为访问类提供一个创建一组相关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。 抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。 使用抽象工厂模式一般要满足以下条件。 系统中有多个产品族，每个具体工厂创建同一族但属于不同等级结构的产品。 系统一次只可能消费其中某一族产品，即同族的产品一起使用。 抽象工厂模式除了具有工厂方法模式的优点外，其他主要优点如下。 可以在类的内部对产品族中相关联的多等级产品共同管理，而不必专门引入多个新的类来进行管理。 当需要产品族时，抽象工厂可以保证客户端始终只使用同一个产品的产品组。 抽象工厂增强了程序的可扩展性，当增加一个新的产品族时，不需要修改原代码，满足开闭原则。 其缺点是：当产品族中需要增加一个新的产品时，所有的工厂类都需要进行修改。增加了系统的抽象性和理解难度。 抽象工厂模式的主要角色： 抽象工厂（Abstract Factory）：提供了创建产品的接口，它包含多个创建产品的方法 newProduct()，可以创建多个不同等级的产品。 具体工厂（Concrete Factory）：主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:11:3","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"创建型模式 4 ： 建造者模式（Builder） 定义：指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示。 该模式的主要优点如下： 封装性好，构建和表示分离。 扩展性好，各个具体的建造者相互独立，有利于系统的解耦。 客户端不必知道产品内部组成的细节，建造者可以对创建过程逐步细化，而不对其它模块产生任何影响，便于控制细节风险。 其缺点如下： 产品的组成部分必须相同，这限制了其使用范围。 如果产品的内部变化复杂，如果产品内部发生变化，则建造者也要同步修改，后期维护成本较大。 建造者（Builder）模式的主要角色如下。 产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。 抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。 具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。 指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:11:4","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"结构型模式 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现的，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态地给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:12:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"结构型模式 1 ： 代理模式（Proxy） 定义：由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。 代理模式的主要优点有： 代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用； 代理对象可以扩展目标对象的功能； 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度，增加了程序的可扩展性 其主要缺点是： 代理模式会造成系统设计中类的数量增加 在客户端和目标对象之间增加一个代理对象，会造成请求处理速度变慢； 增加了系统的复杂度； 那么如何解决以上提到的缺点呢？答案是可以使用动态代理方式 代理模式的主要角色如下。 抽象主题（Subject）类：通过接口或抽象类声明真实主题和代理对象实现的业务方法。 真实主题（Real Subject）类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 代理（Proxy）类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能。 根据代理的创建时期，代理模式分为静态代理和动态代理。 静态：由程序员创建代理类或特定工具自动生成源代码再对其编译，在程序运行前代理类的 .class 文件就已经存在了。 动态：在程序运行时，运用反射机制动态创建而成 静态代理模式： 动态代理模式： 在前面介绍的代理模式中，代理类中包含了对真实主题的引用，这种方式存在两个缺点。 真实主题与代理主题一一对应，增加真实主题也要增加代理。 设计代理以前真实主题必须事先存在，不太灵活。采用动态代理模式可以解决以上问题，如 SpringAOP 动态代理的代理类是动态生成的，不是事先直接写好的。 动态代理分为两大类：基于接口的动态代理，和基于类的动态代理。 需要了解两个类：Proxy(代理类)， InvocationHandler（调用处理程序） 应用场景： 切换数据源 JDK Spring的AOP底层 MyBatis源码中 生活中的实例： 结婚找中介帮忙筹备婚礼 找房产中介帮忙租房 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:12:1","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"结构型模式 2 ： 适配器模式 （Adaptor) 定义：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。适配器模式分为类结构型模式和对象结构型模式两种，前者类之间的耦合度比后者高，且要求程序员了解现有组件库中的相关组件的内部结构，所以应用相对较少些。 该模式的主要优点如下。 客户端通过适配器可以透明地调用目标接口。 复用了现存的类，程序员不需要修改原有代码而重用现有的适配者类。 将目标类和适配者类解耦，解决了目标类和适配者类接口不一致的问题。 在很多业务场景中符合开闭原则。 其缺点是： 适配器编写过程需要结合业务场景全面考虑，可能会增加系统的复杂性。 增加代码阅读难度，降低代码可读性，过多使用适配器会使系统代码变得凌乱。 适配器模式（Adapter）包含以下主要角色。 目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。 适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。 适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。 按照实现方式可以分为类适配器（使用继承实现）和对象适配器（使用组合实现，常用） 适配器模式（Adapter）可扩展为双向适配器模式，双向适配器类既可以把适配者接口转换成目标接口，也可以把目标接口转换成适配者接口， 实际例子： 输入输出流转接：InputStreamReader(InputStream) Java的GUI中 SpringMVC中的DispatchServlet核心分发 SpringBoot中 第三方登录 生活中的实例： 电脑网线转接头，适配器 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:12:2","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"结构型模式 3 ：桥接模式 （Bridge) 定义：将抽象与实现分离，使它们可以独立变化。 它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 通过上面的讲解，我们能很好的感觉到桥接模式遵循了里氏替换原则和依赖倒置原则，最终实现了开闭原则，对修改关闭，对扩展开放。这里将桥接模式的优缺点总结如下。 桥接（Bridge）模式的优点是： 抽象与实现分离，扩展能力强 符合开闭原则 符合合成复用原则 其实现细节对客户透明 缺点是：由于聚合关系建立在抽象层，要求开发者针对抽象化进行设计与编程，能正确地识别出系统中两个独立变化的维度，这增加了系统的理解与设计难度。 桥接（Bridge）模式包含以下主要角色。 抽象化（Abstraction）角色：定义抽象类，并包含一个对实现化对象的引用。 扩展抽象化（Refined Abstraction）角色：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。 实现化（Implementor）角色：定义实现化角色的接口，供扩展抽象化角色调用。 具体实现化（Concrete Implementor）角色：给出实现化角色接口的具体实现。 应用案例： AWT中的Peer架构。 JDBC驱动程序也是桥接模式的应用。 生活中实例： 电脑和品牌 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:12:3","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"结构型模式 4 ： 装饰器模式(Decorator) 定义：指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式。 装饰器模式的主要优点有： 装饰器是继承的有力补充，比继承灵活，在不改变原有对象的情况下，动态的给一个对象扩展功能，即插即用 通过使用不用装饰类及这些装饰类的排列组合，可以实现不同效果 装饰器模式完全遵守开闭原则 其主要缺点是：装饰器模式会增加许多子类，过度使用会增加程序得复杂性。 装饰器模式主要包含以下角色。 抽象构件（Component）角色：定义一个抽象接口以规范准备接收附加责任的对象。 具体构件（ConcreteComponent）角色：实现抽象构件，通过装饰角色为其添加一些职责。 抽象装饰（Decorator）角色：继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。 具体装饰（ConcreteDecorator）角色：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。 主要应用场景： Java中的I/O标准库设计。如下面这些类都是抽象装饰类： InputStream的子类FilterInputStream， OutputStream 的子类 FilterOutputStream， Reader 的子类 BufferedReader 以及 FilterReader， Writer 的子类 BufferedWriter、FilterWriter 以及 PrintWriter 等， java BufferedReader in = new BufferedReader(new FileReader(\"filename.txt\")); String s = in.readLine(); ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:12:4","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"结构型模式 5 ： 外观模式（Facade） 定义：定义了一个高层接口。它包含了对各个子系统的引用，客户端可以通过它访问各个子系统的功能。 外观（Facade）模式包含以下主要角色。 外观（Facade）角色：为多个子系统对外提供一个共同的接口。 子系统（Sub System）角色：实现系统的部分功能，客户可以通过外观角色访问它。 客户（Client）角色：通过一个外观角色访问各个子系统的功能。 应用场景： 对分层结构系统构建时，使用外观模式定义子系统中每层的入口点可以简化子系统之间的依赖关系。 当一个复杂系统的子系统很多时，外观模式可以为系统设计一个简单的接口供外界访问。 当客户端与多个子系统之间存在很大的联系时，引入外观模式可将它们分离，从而提高子系统的独立性和可移植性。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:12:5","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"结构型模式 6 ： 享元模式（Flyweight） 定义：运用共享技术来有效地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。 享元模式的主要优点是：相同对象只要保存一份，这降低了系统中对象的数量，从而降低了系统中细粒度对象给内存带来的压力。 其主要缺点是： 为了使对象可以共享，需要将一些不能共享的状态外部化，这将增加程序的复杂性。 读取享元模式的外部状态会使得运行时间稍微变长。 享元模式的主要角色有如下。 抽象享元角色（Flyweight）：是所有的具体享元类的基类，为具体享元规范需要实现的公共接口，非享元的外部状态以参数的形式通过方法传入。 具体享元（Concrete Flyweight）角色：实现抽象享元角色中所规定的接口。 非享元（Unsharable Flyweight)角色：是不可以共享的外部状态，它以参数的形式注入具体享元的相关方法中。 享元工厂（Flyweight Factory）角色：负责创建和管理享元角色。当客户对象请求一个享元对象时，享元工厂检査系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。 UnsharedConcreteFlyweight 是非享元角色，里面包含了非共享的外部状态信息 info； Flyweight 是抽象享元角色，里面包含了享元方法 operation(UnsharedConcreteFlyweight state)，非享元的外部状态以参数的形式通过该方法传入； ConcreteFlyweight 是具体享元角色，包含了关键字 key，它实现了抽象享元接口； FlyweightFactory 是享元工厂角色，它是关键字 key 来管理具体享元； 客户角色通过享元工厂获取具体享元，并访问具体享元的相关方法。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:12:6","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"结构型模式 7 ： 组合模式（Composite) 定义： 有时又叫作整体-部分（Part-Whole）模式，它是一种将对象组合成树状的层次结构的模式，用来表示“整体-部分”的关系，使用户对单个对象和组合对象具有一致的访问性 组合模式的主要优点有： 组合模式使得客户端代码可以一致地处理单个对象和组合对象，无须关心自己处理的是单个对象，还是组合对象，这简化了客户端代码； 更容易在组合体内加入新的对象，客户端不会因为加入了新的对象而更改源代码，满足“开闭原则”； 其主要缺点是： 设计较复杂，客户端需要花更多时间理清类之间的层次关系； 不容易限制容器中的构件； 不容易用继承的方法来增加构件的新功能； 组合模式包含以下主要角色。 抽象构件（Component）角色：它的主要作用是为树叶构件和树枝构件声明公共接口，并实现它们的默认行为。在透明式的组合模式中抽象构件还声明访问和管理子类的接口；在安全式的组合模式中不声明访问和管理子类的接口，管理工作由树枝构件完成。（总的抽象类或接口，定义一些通用的方法，比如新增、删除） 树叶构件（Leaf）角色：是组合中的叶节点对象，它没有子节点，用于继承或实现抽象构件。 树枝构件（Composite）角色 / 中间构件：是组合中的分支节点对象，它有子节点，用于继承和实现抽象构件。它的主要作用是存储和管理子部件，通常包含 Add()、Remove()、GetChild() 等方法。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:12:7","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 模板方法（Template Method）模式：定义一个操作中的算法骨架，将算法的一些步骤延迟到子类中，使得子类在可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 1 ： 模版方法模式（Template method) 1）抽象类/抽象模板（Abstract Class） 抽象模板类，负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。这些方法的定义如下。 ① 模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。 ② 基本方法：是整个算法中的一个步骤，包含以下几种类型。 抽象方法：在抽象类中声明，由具体子类实现。 具体方法：在抽象类中已经实现，在具体子类中可以继承或重写它。 钩子方法：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。 2）具体子类/具体实现（Concrete Class） 具体实现类，实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的一个组成步骤。 应用场景： 算法的整体步骤很固定，但其中个别部分易变时，这时候可以使用模板方法模式，将容易变的部分抽象出来，供子类实现。 当多个子类存在公共的行为时，可以将其提取出来并集中到一个公共父类中以避免代码重复。首先，要识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。 当需要控制子类的扩展时，模板方法只在特定点调用钩子操作，这样就只允许在这些点进行扩展。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:1","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 2 ： 策略模式（Strategy） 定义：该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。 策略模式的主要优点如下。 多重条件语句不易维护，而使用策略模式可以避免使用多重条件语句，如 if…else 语句、switch…case 语句。 策略模式提供了一系列的可供重用的算法族，恰当使用继承可以把算法族的公共代码转移到父类里面，从而避免重复的代码。 策略模式可以提供相同行为的不同实现，客户可以根据不同时间或空间要求选择不同的。 策略模式提供了对开闭原则的完美支持，可以在不修改原代码的情况下，灵活增加新算法。 策略模式把算法的使用放到环境类中，而算法的实现移到具体策略类中，实现了二者的分离。 其主要缺点如下。 客户端必须理解所有策略算法的区别，以便适时选择恰当的算法类。 策略模式造成很多的策略类，增加维护难度。 策略模式的主要角色如下。 抽象策略（Strategy）类：定义了一个公共接口，各种不同的算法以不同的方式实现这个接口，环境角色使用这个接口调用不同的算法，一般使用接口或抽象类实现。 具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现。 环境（Context）类：持有一个策略类的引用，最终给客户端调用。 策略模式在很多地方用到，如 Java SE 中的容器布局管理就是一个典型的实例，Java SE 中的每个容器都存在多种布局供用户选择。在程序设计中，通常在以下几种情况中使用策略模式较多。 一个系统需要动态地在几种算法中选择一种时，可将每个算法封装到策略类中。 一个类定义了多种行为，并且这些行为在这个类的操作中以多个条件语句的形式出现，可将每个条件分支移入它们各自的策略类中以代替这些条件语句。 系统中各算法彼此完全独立，且要求对客户隐藏具体算法的实现细节时。 系统要求使用算法的客户不应该知道其操作的数据时，可使用策略模式来隐藏与算法相关的数据结构。 多个类只区别在表现行为不同，可以使用策略模式，在运行时动态选择具体要执行的行为。 在一个使用策略模式的系统中，当存在的策略很多时，客户端管理所有策略算法将变得很复杂，如果在环境类中使用策略工厂模式来管理这些策略类将大大减少客户端的工作复杂度 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:2","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 3 ： 命令模式（Command） 定义：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行储存、传递、调用、增加与管理。 命令模式的主要优点如下。 通过引入中间件（抽象接口）降低系统的耦合度。 扩展性良好，增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，且满足“开闭原则”。 可以实现宏命令。命令模式可以与组合模式结合，将多个命令装配成一个组合命令，即宏命令。 方便实现 Undo 和 Redo 操作。命令模式可以与后面介绍的备忘录模式结合，实现命令的撤销与恢复。 可以在现有命令的基础上，增加额外功能。比如日志记录，结合装饰器模式会更加灵活。 其缺点是： 可能产生大量具体的命令类。因为每一个具体操作都需要设计一个具体命令类，这会增加系统的复杂性。 命令模式的结果其实就是接收方的执行结果，但是为了以命令的形式进行架构、解耦请求与实现，引入了额外类型结构（引入了请求方与抽象命令接口），增加了理解上的困难。不过这也是设计模式的通病，抽象必然会额外增加类的数量，代码抽离肯定比代码聚合更加难理解。 命令模式包含以下主要角色。 抽象命令类（Command）角色：声明执行命令的接口，拥有执行命令的抽象方法 execute()。 具体命令类（Concrete Command）角色：是抽象命令类的具体实现类，它拥有接收者对象，并通过调用接收者的功能来完成命令要执行的操作。 实现者/接收者（Receiver）角色：执行命令功能的相关操作，是具体命令对象业务的真正实现者。 调用者/请求者（Invoker）角色：是请求的发送者，它通常拥有很多的命令对象，并通过访问命令对象来执行相关请求，它不直接访问接收者。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:3","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 4 ： 责任链模式（ChainOfResponsibility） 定义：为了避免请求发送者与多个请求处理者耦合在一起，于是将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。 责任链模式是一种对象行为型模式，其主要优点如下。 降低了对象之间的耦合度。该模式使得一个对象无须知道到底是哪一个对象处理其请求以及链的结构，发送者和接收者也无须拥有对方的明确信息。 增强了系统的可扩展性。可以根据需要增加新的请求处理类，满足开闭原则。 增强了给对象指派职责的灵活性。当工作流程发生变化，可以动态地改变链内的成员或者调动它们的次序，也可动态地新增或者删除责任。 责任链简化了对象之间的连接。每个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。 责任分担。每个类只需要处理自己该处理的工作，不该处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。 其主要缺点如下。 不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。 对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。 职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。 职责链模式主要包含以下角色。 抽象处理者（Handler）角色：定义一个处理请求的接口，包含抽象处理方法和一个后继连接。 具体处理者（Concrete Handler）角色：实现抽象处理者的处理方法，判断能否处理本次请求，如果可以处理请求则处理，否则将该请求转给它的后继者。 客户类（Client）角色：创建处理链，并向链头的具体处理者对象提交请求，它不关心处理细节和请求的传递过程。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:4","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 5 ： 状态模式（State） 定义：对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。 优点： 结构清晰，状态模式将与特定状态相关的行为局部化到一个状态中，并且将不同状态的行为分割开来，满足“单一职责原则”。 将状态转换显示化，减少对象间的相互依赖。将不同的状态引入独立的对象中会使得状态转换变得更加明确，且减少对象间的相互依赖。 状态类职责明确，有利于程序的扩展。通过定义新的子类很容易地增加新的状态和转换。 缺点： 状态模式的使用必然会增加系统的类与对象的个数。 状态模式的结构与实现都较为复杂，如果使用不当会导致程序结构和代码的混乱。 状态模式对开闭原则的支持并不太好，对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源码，否则无法切换到新增状态，而且修改某个状态类的行为也需要修改对应类的源码。 状态模式包含以下主要角色。 环境类（Context）角色：也称为上下文，它定义了客户端需要的接口，内部维护一个当前状态，并负责具体状态的切换。 抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为，可以有一个或多个行为。 具体状态（Concrete State）角色：实现抽象状态所对应的行为，并且在需要的情况下进行状态切换。 应用： 状态机、线程状态转换 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:5","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 6 ： 观察者模式（Observer） 定义：指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式。 优点： 降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。符合依赖倒置原则。 目标与观察者之间建立了一套触发机制。 缺点： 目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用。 当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率。 观察者模式的主要角色如下。 抽象主题（Subject）角色：也叫抽象目标类，它提供了一个用于保存观察者对象的聚集类和增加、删除观察者对象的方法，以及通知所有观察者的抽象方法。 具体主题（Concrete Subject）角色：也叫具体目标类，它实现抽象目标中的通知方法，当具体主题的内部状态发生改变时，通知所有注册过的观察者对象。 抽象观察者（Observer）角色：它是一个抽象类或接口，它包含了一个更新自己的抽象方法，当接到具体主题的更改通知时被调用。 具体观察者（Concrete Observer）角色：实现抽象观察者中定义的抽象方法，以便在得到目标的更改通知时更新自身的状态。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:6","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 7 ： 中介者模式（Mediator） 定义：定义一个中介对象来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。中介者模式又叫调停模式，它是迪米特法则的典型应用。 优点： 类之间各司其职，符合迪米特法则。 降低了对象之间的耦合性，使得对象易于独立地被复用。 将对象间的一对多关联转变为一对一的关联，提高系统的灵活性，使得系统易于维护和扩展。 其主要缺点是：中介者模式将原本多个对象直接的相互依赖变成了中介者和多个同事类的依赖关系。当同事类越多时，中介者就会越臃肿，变得复杂且难以维护。 中介者模式包含以下主要角色。 抽象中介者（Mediator）角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。 具体中介者（Concrete Mediator）角色：实现中介者接口，定义一个 List 来管理同事对象，协调各个同事角色之间的交互关系，因此它依赖于同事角色。 抽象同事类（Colleague）角色：定义同事类的接口，保存中介者对象，提供同事对象交互的抽象方法，实现所有相互影响的同事类的公共功能。 具体同事类（Concrete Colleague）角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:7","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 8 ： 迭代器模式（Iterator) 定义：提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示， 优点: 访问一个聚合对象的内容而无须暴露它的内部表示。 遍历任务交由迭代器完成，这简化了聚合类。 它支持以不同方式遍历一个聚合，甚至可以自定义迭代器的子类以支持新的遍历。 增加新的聚合类和迭代器类都很方便，无须修改原有代码。 封装性良好，为遍历不同的聚合结构提供一个统一的接口。 缺点： 增加了类的个数，这在一定程度上增加了系统的复杂性。 迭代器模式主要包含以下角色。 抽象聚合（Aggregate）角色：定义存储、添加、删除聚合对象以及创建迭代器对象的接口。 具体聚合（ConcreteAggregate）角色：实现抽象聚合类，返回一个具体迭代器的实例。 抽象迭代器（Iterator）角色：定义访问和遍历聚合元素的接口，通常包含 hasNext()、first()、next() 等方法。 具体迭代器（Concretelterator）角色：实现抽象迭代器接口中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:8","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 9 ： 访问者模式：（Visitor） 定义：将作用于某种数据结构中的各元素的操作分离出来封装成独立的类，使其在不改变数据结构的前提下可以添加作用于这些元素的新的操作，为数据结构中的每个元素提供多种访问方式。它将对数据的操作与数据结构进行分离。 优点： 扩展性好。能够在不修改对象结构中的元素的情况下，为对象结构中的元素添加新的功能。 复用性好。可以通过访问者来定义整个对象结构通用的功能，从而提高系统的复用程度。 灵活性好。访问者模式将数据结构与作用于结构上的操作解耦，使得操作集合可相对自由地演化而不影响系统的数据结构。 符合单一职责原则。访问者模式把相关的行为封装在一起，构成一个访问者，使每一个访问者的功能都比较单一。 缺点： 增加新的元素类很困难。在访问者模式中，每增加一个新的元素类，都要在每一个具体访问者类中增加相应的具体操作，这违背了“开闭原则”。 破坏封装。访问者模式中具体元素对访问者公布细节，这破坏了对象的封装性。 违反了依赖倒置原则。访问者模式依赖了具体类，而没有依赖抽象类。 主要角色： 抽象访问者（Visitor）角色：定义一个访问具体元素的接口，为每个具体元素类对应一个访问操作 visit() ，该操作中的参数类型标识了被访问的具体元素。 具体访问者（ConcreteVisitor）角色：实现抽象访问者角色中声明的各个访问操作，确定访问者访问一个元素时该做什么。 抽象元素（Element）角色：声明一个包含接受操作 accept() 的接口，被接受的访问者对象作为 accept() 方法的参数。 具体元素（ConcreteElement）角色：实现抽象元素角色提供的 accept() 操作，其方法体通常都是 visitor.visit(this) ，另外具体元素中可能还包含本身业务逻辑的相关操作。 对象结构（Object Structure）角色：是一个包含元素角色的容器，提供让访问者对象遍历容器中的所有元素的方法，通常由 List、Set、Map 等聚合类实现。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:9","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 10 ： 备忘录模式（Memento） 定义：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后当需要时能将该对象恢复到原先保存的状态。该模式又叫快照模式。 优点： 提供了一种可以恢复状态的机制。当用户需要时能够比较方便地将数据恢复到某个历史的状态。 实现了内部状态的封装。除了创建它的发起人之外，其他对象都不能够访问这些状态信息。 简化了发起人类。发起人不需要管理和保存其内部状态的各个备份，所有状态信息都保存在备忘录中，并由管理者进行管理，这符合单一职责原则。 缺点： 资源消耗大。如果要保存的内部状态信息过多或者特别频繁，将会占用比较大的内存资源。 备忘录模式的主要角色如下。 发起人（Originator）角色：记录当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。 备忘录（Memento）角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。 管理者（Caretaker）角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:10","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"行为型模式 11 ： 解释器模式（Interpreter） 定义：给分析对象定义一个语言，并定义该语言的文法表示，再设计一个解析器来解释语言中的句子。也就是说，用编译语言的方式来分析应用中的实例。这种模式实现了文法表达式处理的接口，该接口解释一个特定的上下文。 优点： 扩展性好。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变或扩展文法。 容易实现。在语法树中的每个表达式节点类都是相似的，所以实现其文法较为容易。 缺点： 执行效率较低。解释器模式中通常使用大量的循环和递归调用，当要解释的句子较复杂时，其运行速度很慢，且代码的调试过程也比较麻烦。 会引起类膨胀。解释器模式中的每条规则至少需要定义一个类，当包含的文法规则很多时，类的个数将急剧增加，导致系统难以管理与维护。 可应用的场景比较少。在软件开发中，需要定义语言文法的应用实例非常少，所以这种模式很少被使用到。 解释器模式包含以下主要角色。 抽象表达式（Abstract Expression）角色：定义解释器的接口，约定解释器的解释操作，主要包含解释方法 interpret()。 终结符表达式（Terminal Expression）角色：是抽象表达式的子类，用来实现文法中与终结符相关的操作，文法中的每一个终结符都有一个具体终结表达式与之相对应。 非终结符表达式（Nonterminal Expression）角色：也是抽象表达式的子类，用来实现文法中与非终结符相关的操作，文法中的每条规则都对应于一个非终结符表达式。 环境（Context）角色：通常包含各个解释器需要的数据或是公共的功能，一般用来传递被所有解释器共享的数据，后面的解释器可以从这里获取这些值。 客户端（Client）：主要任务是将需要分析的句子或表达式转换成使用解释器对象描述的抽象语法树，然后调用解释器的解释方法，当然也可以通过环境角色间接访问解释器的解释方法。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:13:11","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["Coding"],"content":"solid原则 在程序设计领域， SOLID（单一功能、开闭原则、里氏替换、接口隔离以及依赖反转）是由罗伯特·C·马丁在21世纪早期引入，指代了面向对象编程和面向对象设计的五个基本原则。 solid原则包括以下五个： 1、单一职责原则（SRP）：表明一个类有且只有一个职责。一个类就像容器一样，它能添加任意数量的属性、方法等。 2、开放封闭原则（OCP）：一个类应该对扩展开放，对修改关闭。这意味一旦创建了一个类并且应用程序的其他部分开始使用它，就不应该修改它。 3、里氏替换原则（LSP）：派生的子类应该是可替换基类的，也就是说任何基类可以出现的地方，子类一定可以出现。值得注意的是，当通过继承实现多态行为时，如果派生类没有遵守LSP，可能会让系统引发异常。 4、接口隔离原则（ISP）：表明类不应该被迫依赖他们不使用的方法，也就是说一个接口应该拥有尽可能少的行为，它是精简的，也是单一的。 5、依赖倒置原则（DIP）：表明高层模块不应该依赖低层模块，相反，他们应该依赖抽象类或者接口。这意味着不应该在高层模块中使用具体的低层模块。 ","date":"2022-07-31 14:17:48","objectID":"/patterns_principles/:14:0","tags":["design pattern"],"title":"Patterns_principles","uri":"/patterns_principles/"},{"categories":["interview"],"content":"Go ","date":"2022-06-28 15:30:06","objectID":"/interview/:1:0","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"slice 切片和数组的区别？ 数据类型不同。这是从数据结构上决定的，因为数组的数据结构只有元素类型和长度，而切片的数据结构是一个结构体，结构体有一个指向数组的指针，长度和容量，所以切片是一个引用类型而不是像数组一样的不可变的值类型。 额外一点说，在 go 中虽然函数传递中是值传递的，但是由于切片的数据结构中的结构体里面包含了指针，所以函数传递的时候会将指针的值拷贝走，从结果来说会操作到同一个切片。 为什么切片的应用比数组多？ 在日常业务上，更需要存储一个可变的序列。业务中有一个确定的长度序列的场景是比较少的，或者说要确定一个序列的长度是会需要做额外的操作损耗性能的。 切片的访问的复杂度？ 切片的底层实现是一块连续的内存地址，所以能进行 O(1) 的随机访问元素。 具体的源码都是在编译时编译好（类似于 内存地址=元素类型*下标，不在此处细究。 切片的扩容机制？ 当扩容的时候需要扩容的容量是现在容量的 2 倍以上时，会直接扩容需要扩容的容量。否则的话，如果现在的长度（1.16 改成了容量）是 1024 以下时，会直接翻倍扩容。但是如果超过 1024 时，会反复扩容 25 % 直到达到或超过需要扩容的容量 为什么切片底层不用链表？ 设计上，切片的设计上允许随机访问，而随机访问对于链表是不友好的 容量上，切片的扩容机制无论从小切片是翻倍扩容，还是到大切片不断扩容 25 % 直至扩容到指定值的扩容机制，在容量上已经能覆盖大多数的场景内存上，使用链表会产生内存碎片，并且对于 Go 的垃圾回收算法（三色标记法）在递归根对象寻找可达对象的过程中不友好 ","date":"2022-06-28 15:30:06","objectID":"/interview/:1:1","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"map map 的扩容机制？ 分为了翻倍扩容、等量扩容。还有确定了要扩容后，使用的是渐进式扩容去避免性能抖动。 扩容的触发条件有两个，状态装载因子超过 6.5（现在的桶装在的元素超过 80 %），也就是元素太多了，或者溢出桶太多（逻辑是判断现在桶的总数是否超过 map 结构体中的设置值，如果超过说明太多溢出桶），也就是太多元素被删除，元素非常稀疏。 前者会使用翻倍扩容，会申请现有翻倍的空间。当有读操作时仍会访问旧桶，当有插入或者删除操作的时候会将旧桶分流到两个新桶。后者会使用等倍扩容，重新申请一块和现有大小相同的新桶，也是使用渐进式扩容在插入或者删除操作中进行。 map 的存储是有序的吗？ ​ 存储结构上由于在扩容的时候会将 key 随机分流到另外两个桶，这导致了 key 的相对位置会发生改变，所以 key 是无序的。 遍历上来说，每次遍历是取一个随机数，随机从一个桶开始遍历。所以每次遍历出来的结果都是不一样的。额外一点是，当未发生扩容前，key 的相对位置是确定的。 map 是线程安全的吗？ 不是的，当赋值和删除时是置写操作位。当置了之后有别的协程用任何操作进行来时否都会报错。如果想要 map 线程安全，解决方案一般是用 sync.map 或者 互斥锁+map map 有缩容机制吗？ go 的 map 没有缩容的机制。map 内部的存储结构是基于拉链法的，里面的元素如果被大批量的删除后，会触发等量扩容。等量扩容时会申请原有大小一样的内存块，渐进式的扩容过去，让原有的 map 中因为很多元素被删除后导致元素排序稀疏的情况经过 rehash 后会排序会变得紧密，减少溢出桶的使用。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:1:2","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"sync sync.map 为什么是线程安全的 因为原有的 map 会在赋值和删除的置写标志，置了后别的协程来做任何操作都会报错。所以引出了 sync.map 解决多协程问题。 sync.map 使用了读写分离来去保证线程安全的，sync.map 的数据结构分为读 map、写 map、还有互斥锁以及一个记录穿透次数的值。具体实现是每个协程来读取时都会先读取读部分的 kv，没有则去读写部分的 kv（操作写部分时都会上锁）。当穿透到写部分的次数大于写部分的长度时就会将写部分同步到读部分并且把写部分清空。所以多协程下一般都会先打到无锁的读部分，这能保证读取性能 sync.mutex 作用 sync.mutex 是一把互斥锁，具体作用是锁住限定区域的代码逻辑，这段区域只能被一个协程占用。具体实现原理是，当一个协程去获取锁时（获取锁是 CAS 看锁的状态位），当获取不到会自选一定次数后加入到队列去休眠，当锁被释放时，正在自旋的协程 + 休眠中的任意一个协程会才能会被唤醒的会去抢锁，这是正常模式。还有一种模式是饥饿模式，由于被唤醒的协程有很大几率是抢不到正在自旋的协程的，所以当有协程超过 1 毫秒获取不到锁时将会进入饥饿模式。进入后会根据上面提到的队列中按照先进先出的模式依次获取锁，新来的协程直接进入队列中等待。当队列中已经清空或者队列中头个协程等待时间小于 1 毫秒后退化为正常模式。 sync.RWmutex 作用 sync.RWmutex 是读写锁，用于解决 reader / wirter 问题（保证一个写协程和其他所有协程互斥的同步问题）。也就是可以读锁可以被多个协程拥有，但是只能一个协程拥有写锁。读写操作互斥、写写操作互斥。当有协程获取写锁时，会阻塞所有新来获取读锁的所有协程，并且会等所有正在拥有读锁的协程释放后才能获取到 sync.singleflight 作用 sync.signleflight 能将对同一个资源访问 的多个请求合并为一个请求。常见的应用场景比如防缓冲击穿。具体的实现是使用了 map 对同一资源访问的请求进行去重，使用互斥锁让当个协程进入临界区后进行资源访问，其他线程阻塞等待资源访问完成后，共同拿到访问资源的结果并返回。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:1:3","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"context 有什么作用 传递上下文。使用 context 传递业务参数是很不推荐的，但是比较常用的用来传递整个链路的 trace id 来作为链路追踪 协程间同步信息。使用 context 后能在多个协程组成的协程树传递取消信号（通过 Cancel，超时计数器调用 Cancel 等） goroutine 进程、线程和协程的区别 进程是应用程序的实体，分配操作系统资源的最小单位，拥有自己的内存空间以及堆栈。 线程是内核操作系统调度CPU的基本单位，在进程的内存空间及堆栈上进行执行运行。 协程指的是用户态线程，由用户的应用程序进行调度。 协程的好处 切换上下文代价小，协程切换时不用进行系统调用，走内核态的指令。 没有多线程的竞争资源锁问题，多线程竞争资源还会锁内存走系统调用，而协程如果竞争资源直接在当前线程上的进行判断就可以了 内存占用小，只占用 2K，线程占用 2M ","date":"2022-06-28 15:30:06","objectID":"/interview/:1:4","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"GMP 模型中的 GMP 指的是什么 G 协程 M 线程 P 调度器 其中 M 可能会自旋，也有可能会休眠，GC 会把一些休眠的线程销毁。 GMP 模型中有什么组件 除开 GMP ，还有 P 的 G 本地队列， G 的全局队列，P 列表（MAXPROCS 决定），M 列表（最大限定 1W，runtime/debug 可以设置） GMP 模型中为什么需要 P 假设没有 P 会发生什么事情，（没有 P 本地队列，没有 P 全局队列） 出现资源竞争，由于没有 P 的本地以及全局队列的多级缓存，所以 G 都会放在一起，多个 M 去获取时会出现资源竞争 协程切换资源消耗大，由于没有 P ，也没有 g0 去负责切换协程堆栈，相当于协程堆栈以及一些运行现场全部都在内核态去维护 系统调用阻塞时切换成本高，M 会经常被阻塞和解阻塞切换内核态和用户态（类似于进程间切换），消耗大 引入了 P 相当于解决了上面的大部分问题，甚至引入了新的特性去榨干 CPU 的性能 引入本地队列和全局队列做多级缓存，获 取 G 时都会从本地队列获取，没有竞争，就算去全局队列获取也比较少的几率出现大量 P 去获取，降低了资源竞争的概率 切换协程堆栈效率提高，使用了 g0 的协程负责去管理协程切换的堆栈以及保护现场的工作，进入内核态去切换的时候少了很多切换指令以及寄存器的使用，甚至引入了 g0 去负责做垃圾回收部分工作的职能 系统调用的曲线救国方案，当 G 和 M 发生了系统调用时，P 会解绑 M ，带着本地队列的 P 去找空闲的 M 或者新创建的 M 去继续剩下的工作 还引入了「工作窃取」的功能，让基于和 M 绑定的 P 更加灵活的让每个 M 都能够最大限度的运行 task，榨干 CPU P 调度器的设计策略 work stealing 机制：当 P 本地队列无运行 G 时，会去其他线程绑定的 P 窃取 G ，若其他 P 本地队列也没有时会去 G 全局队列进行窃取 hand off 机制：当 G 因为系统调用阻塞时，P 会和 M 解绑，将 G 和 M 绑定，P 会和空闲的线程进行绑定 主动让出机制：当 G 占用了 CPU 超过 10MS 会主动让出（sysmon 轮询） ","date":"2022-06-28 15:30:06","objectID":"/interview/:1:5","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"mysql基础 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:0","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1. 数据库的三范式是什么？ 第一范式：强调的是列的原子性，即数据库表的每一列都是不可分割的原子数据项。 第二范式：要求实体的属性完全依赖于主关键字。所谓完全 依赖是指不能存在仅依赖主关键字一部分的属性。 第三范式：任何非主属性不依赖于其它非主属性。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:1","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"2. MySQL 支持哪些存储引擎? MySQL 支持多种存储引擎,比如 InnoDB,MyISAM,Memory,Archive 等等.在大多数的情况下,直接选择使用 InnoDB 引擎都是最合适的,InnoDB 也是 MySQL 的默认存储引擎。 MyISAM 和 InnoDB 的区别有哪些： InnoDB 支持事务，MyISAM 不支持 InnoDB 支持外键，而 MyISAM 不支持 InnoDB 是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高；MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针，主键索引和辅助索引是独立的。 Innodb 不支持全文索引，而 MyISAM 支持全文索引，查询效率上 MyISAM 要高； InnoDB 不保存表的具体行数，MyISAM 用一个变量保存了整个表的行数。 MyISAM 采用表级锁(table-level locking)；InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:2","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"3. 超键、候选键、主键、外键分别是什么？ 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:3","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"4. SQL 约束有哪几种？ NOT NULL: 用于控制字段的内容一定不能为空（NULL）。 UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。 PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CHECK: 用于控制字段的值范围。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:4","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"5. MySQL 中的 varchar 和 char 有什么区别？ char 是一个定长字段,假如申请了char(10)的空间,那么无论实际存储多少内容.该字段都占用 10 个字符,而 varchar 是变长的,也就是说申请的只是最大长度,占用的空间为实际字符长度+1,最后一个字符存储使用了多长的空间. 在检索效率上来讲,char \u003e varchar,因此在使用中,如果确定某个字段的值的长度,可以使用 char,否则应该尽量使用 varchar.例如存储用户 MD5 加密后的密码,则应该使用 char。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:5","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"6. MySQL中 in 和 exists 区别 MySQL中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。 如果查询的两个表大小相当，那么用in和exists差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。 not in 和not exists：如果查询语句使用了not in，那么内外表都进行全表扫描，没有用到索引；而not extsts的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:6","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"7. drop、delete与truncate的区别 三者都表示删除，但是三者有一些差别： ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:7","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"8. 什么是存储过程？有哪些优缺点？ 存储过程是一些预编译的 SQL 语句。 1、更加直白的理解：存储过程可以说是一个记录集，它是由一些 T-SQL 语句组成的代码块，这些 T-SQL 语句代码像一个方法一样实现一些功能（对单表或多表的增删改查），然后再给这个代码块取一个名字，在用到这个功能的时候调用他就行了。 2、存储过程是一个预编译的代码块，执行效率比较高,一个存储过程替代大量 T_SQL 语句 ，可以降低网络通信量，提高通信速率,可以一定程度上确保数据安全 但是,在互联网项目中,其实是不太推荐存储过程的,比较出名的就是阿里的《Java 开发手册》中禁止使用存储过程,我个人的理解是,在互联网项目中,迭代太快,项目的生命周期也比较短,人员流动相比于传统的项目也更加频繁,在这样的情况下,存储过程的管理确实是没有那么方便,同时,复用性也没有写在服务层那么好。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:8","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"9. MySQL 执行查询的过程 客户端通过 TCP 连接发送连接请求到 MySQL 连接器，连接器会对该请求进行权限验证及连接资源分配 查缓存。（当判断缓存是否命中时，MySQL 不会进行解析查询语句，而是直接使用 SQL 语句和客户端发送过来的其他原始信息。所以，任何字符上的不同，例如空格、注解等都会导致缓存的不命中。） 语法分析（SQL 语法是否写错了）。 如何把语句给到预处理器，检查数据表和数据列是否存在，解析别名看是否存在歧义。 优化。是否使用索引，生成执行计划。 交给执行器，将数据保存到结果集中，同时会逐步将数据缓存到查询缓存中，最终将结果集返回给客户端。 更新语句执行会复杂一点。需要检查表是否有排它锁，写 binlog，刷盘，是否执行 commit。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:9","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"索引 1. 索引是什么？ 索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。 索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。而且索引是一个文件，它是要占据物理空间的。 MySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。比如我们在查字典的时候，前面都有检索的拼音和偏旁、笔画等，然后找到对应字典页码，这样然后就打开字典的页数就可以知道我们要搜索的某一个key的全部值的信息了。 2. 索引有哪些优缺点？ 索引的优点 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 索引的缺点 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率； 空间方面：索引需要占物理空间。 3. MySQL有哪几种索引类型？ 1、从存储结构上来划分：BTree索引（B-Tree或B+Tree索引），Hash索引，full-index全文索引，R-Tree索引。这里所描述的是索引存储时保存的形式， 2、从应用层次来分：普通索引，唯一索引，复合索引。 普通索引：即一个索引只包含单个列，一个表可以有多个单列索引 唯一索引：索引列的值必须唯一，但允许有空值 复合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并 聚簇索引(聚集索引)：并不是一种单独的索引类型，而是一种数据存储方式。具体细节取决于不同的实现，InnoDB的聚簇索引其实就是在同一个结构中保存了B-Tree索引(技术上来说是B+Tree)和数据行。 非聚簇索引： 不是聚簇索引，就是非聚簇索引 3、根据中数据的物理顺序与键值的逻辑（索引）顺序关系： 聚集索引，非聚集索引。 4. 说一说索引的底层实现？ Hash索引 基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），并且Hash索引将所有的哈希码存储在索引中，同时在索引表中保存指向每个数据行的指针。 图片来源：https://www.javazhiyin.com/40232.html B-Tree索引（MySQL使用B+Tree） B-Tree能加快数据的访问速度，因为存储引擎不再需要进行全表扫描来获取数据，数据分布在各个节点之中。 B+Tree索引 是B-Tree的改进版本，同时也是数据库索引索引所采用的存储结构。数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址。相比B-Tree来说，进行范围查找时只需要查找两个节点，进行遍历即可。而B-Tree需要获取所有节点，相比之下B+Tree效率更高。 B+tree性质： n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 B+ 树中，数据对象的插入和删除仅在叶节点上进行。 B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。 5. 为什么索引结构默认使用B+Tree，而不是B-Tree，Hash，二叉树，红黑树？ B-tree： 从两个方面来回答 B+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B(B-)树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。 由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。 Hash： 虽然可以快速定位，但是没有顺序，IO复杂度高； 基于Hash表实现，只有Memory存储引擎显式支持哈希索引 ； 适合等值查询，如=、in()、\u003c=\u003e，不支持范围查询 ； 因为不是按照索引值顺序存储的，就不能像B+Tree索引一样利用索引完成排序 ； Hash索引在查询等值时非常快 ； 因为Hash索引始终索引的所有列的全部内容，所以不支持部分索引列的匹配查找 ； 如果有大量重复键值得情况下，哈希索引的效率会很低，因为存在哈希碰撞问题 。 二叉树： 树的高度不均匀，不能自平衡，查找效率跟数据有关（树的高度），并且IO代价高。 红黑树： 树的高度随着数据量增加而增加，IO代价高。 6. 讲一讲聚簇索引与非聚簇索引？ 在 InnoDB 里，索引B+ Tree的叶子节点存储了整行数据的是主键索引，也被称之为聚簇索引，即将数据存储与索引放到了一块，找到索引也就找到了数据。 而索引B+ Tree的叶子节点存储了主键的值的是非主键索引，也被称之为非聚簇索引、二级索引。 聚簇索引与非聚簇索引的区别： 非聚集索引与聚集索引的区别在于非聚集索引的叶子节点不存储表中的数据，而是存储该列对应的主键（行号） 对于InnoDB来说，想要查找数据我们还需要根据主键再去聚集索引中进行查找，这个再根据聚集索引查找数据的过程，我们称为回表。第一次索引一般是顺序IO，回表的操作属于随机IO。需要回表的次数越多，即随机IO次数越多，我们就越倾向于使用全表扫描 。 通常情况下， 主键索引（聚簇索引）查询只会查一次，而非主键索引（非聚簇索引）需要回表查询多次。当然，如果是覆盖索引的话，查一次即可 注意：MyISAM无论主键索引还是二级索引都是非聚簇索引，而InnoDB的主键索引是聚簇索引，二级索引是非聚簇索引。我们自己建的索引基本都是非聚簇索引。 7. 非聚簇索引一定会回表查询吗？ 不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。一个索引包含（覆盖）所有需要查询字段的值，被称之为\"覆盖索引\"。 举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select score from student where score \u003e 90的查询时，在索引的叶子节点上，已经包含了score 信息，不会再次进行回表查询。 8. 联合索引是什么？为什么需要注意联合索引中的顺序？ MySQL可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。 具体原因为: MySQL使用索引时需要索引有序，假设现在建立了\"name，age，school\"的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。 当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。 9. 讲一讲MySQL的最左前缀原则? 最左前缀原则就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。 mysql会一直向右匹配直到遇到范围查询(\u003e、\u003c、between、like)就停止匹配，比如a = 1 and b = 2 and c \u003e 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。 10. 讲一讲前缀索引？ 因为可能我们索引的字段非常长，这既占内存空间，也不利于维护。所以我们就想，如果只把很长字段的前面的公共部分作为一个索引，就会产生超级加倍的效果。但是，我们需要注意，order by不支持前缀索引 。 流程是： 先计算完整列的选择性 :select count(distinct col_1)/count(1) from table_1 再计算不同前缀长度的选择性 :select count(distinct left(col_1,4))/count(1) from table_1 找到最优长度之后，创建前缀索引 :create index idx_front on table_1 (col_1(4)) 11. 了解索引下推吗？ MySQL 5.6引入了索引下推优化。默认开启，使用SET optimizer_switch = ‘index_condition_pushdown=off’;可以将其关闭。 有了索引下推优化，可以在减少回表次数 在InnoDB中只针对二级索引有效 官方文档中给的例子和解释如下： 在 people_table中有一个二级索引(zipcode，lastna","date":"2022-06-28 15:30:06","objectID":"/interview/:2:10","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"事务 1. 什么是数据库事务？ 事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。 假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 2. 介绍一下事务具有的四个特征 事务就是一组原子性的操作，这些操作要么全部发生，要么全部不发生。事务把数据库从一种一致性状态转换成另一种一致性状态。 原子性。事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做 一致性。事 务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。因此当数据库只包含成功事务提交的结果时，就说数据库处于一致性状态。如果数据库系统 运行中发生故障，有些事务尚未完成就被迫中断，这些未完成事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是 不一致的状态。 隔离性。一个事务的执行不能其它事务干扰。即一个事务内部的//操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰。 持续性。也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的。接下来的其它操作或故障不应该对其执行结果有任何影响。 3. 说一下MySQL 的四种隔离级别 Read Uncommitted（读取未提交内容） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 Read Committed（读取提交内容） 这是大多数数据库系统的默认隔离级别（但不是 MySQL 默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓 的 不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的 commit，所以同一 select 可能返回不同结果。 Repeatable Read（可重读） 这是 MySQL 的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。 Serializable（可串行化） 通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 MySQL 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别 事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容):，但是你要知道的是InnoDB 存储引擎默认使用 **REPEATABLE-READ（可重读）**并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到**SERIALIZABLE(可串行化)**隔离级别。 4. 什么是脏读？幻读？不可重复读？ 1、脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据 2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果 不一致。 3、幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 不可重复读侧重于修改，幻读侧重于新增或删除（多了或少量行），脏读是一个事务回滚影响另外一个事务。 5. 事务的实现原理 事务是基于重做日志文件(redo log)和回滚日志(undo log)实现的。 每提交一个事务必须先将该事务的所有日志写入到重做日志文件进行持久化，数据库就可以通过重做日志来保证事务的原子性和持久性。 每当有修改事务时，还会产生 undo log，如果需要回滚，则根据 undo log 的反向语句进行逻辑操作，比如 insert 一条记录就 delete 一条记录。undo log 主要实现数据库的一致性。 6. MySQL事务日志介绍下？ innodb 事务日志包括 redo log 和 undo log。 undo log 指事务开始之前，在操作任何数据之前，首先将需操作的数据备份到一个地方。redo log 指事务中操作的任何数据，将最新的数据备份到一个地方。 事务日志的目的：实例或者介质失败，事务日志文件就能派上用场。 redo log redo log 不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入 redo 中。具体的落盘策略可以进行配置 。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启 MySQL 服务的时候，根据 redo log 进行重做，从而达到事务的未入磁盘数据进行持久化这一特性。RedoLog 是为了实现事务的持久性而出现的产物。 undo log undo log 用来回滚行记录到某个版本。事务未提交之前，Undo 保存了未提交之前的版本数据，Undo 中的数据可作为数据旧版本快照供其他并发事务进行快照读。是为了实现事务的原子性而出现的产物,在 MySQL innodb 存储引擎中用来实现多版本并发控制。 7. 什么是MySQL的 binlog？ MySQL的 binlog 是记录所有数据库表结构变更（例如 CREATE、ALTER TABLE）以及表数据修改（INSERT、UPDATE、DELETE）的二进制日志。binlog 不会记录 SELECT 和 SHOW 这类操作，因为这类操作对数据本身并没有修改，但你可以通过查询通用日志来查看 MySQL 执行过的所有语句。 MySQL binlog 以事件形式记录，还包含语句所执行的消耗的时间，MySQL 的二进制日志是事务安全型的。binlog 的主要目的是复制和恢复。 binlog 有三种格式，各有优缺点： statement： 基于 SQL 语句的模式，某些语句和函数如 UUID, LOAD DATA INFILE 等在复制过程可能导致数据不一致甚至出错。 row： 基于行的模式，记录的是行的变化，很安全。但是 binlog 会比其他两种模式大很多，在一些大表中清除大量数据时在 binlog 中会生成很多条语句，可能导致从库延迟变大。 mixed： 混合模式，根据语句来选用是 statement 还是 row 模式。 8. 在事务中可以混合使用存储引擎吗？ 尽量不要在同一个事务中使用多种存储引擎，MySQL服务器层不管理事务，事务是由下层的存储引擎实现的。 如果在事务中混合使用了事务型和非事务型的表（例如InnoDB和MyISAM表）,在正常提交的情况下不会有什么问题。 但如果该事务需要回滚，非事务型的表上的变更就无法撤销，这会导致数据库处于不一致的状态，这种情况很难修复，事务的最终结果将无法确定。所以，为每张表选择合适的存储引擎非常重要。 9. MySQL中是如何实现事务隔离的? 读未提交和串行化基本上是不需要考虑的隔离级别，前者不加锁限制，后者相当于单线程执行，效率太差。 MySQL 在可重复读级别解决了幻读问题，是通过行锁和间隙锁的组合 Next-Key 锁实现的。 详细原理看这篇文章：https://haicoder.net/note/MySQL-interview/MySQL-interview-MySQL-trans-level.html 10. 什么是 MVCC？ MVCC， 即多版本并发控制。MVCC 的实现，是通过保存数据在某个时间点的快照来实现的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。 11. MVCC 的实现原理 对于 InnoDB ，聚簇索引记录中包含 3 个隐藏的列： ROW ID：隐藏的自增 ID，如果表没有主键，InnoDB 会自动按 ROW ID 产生一个聚集索引树。 事务 ID：记录最后一次修改该记录的事务 ID。 回滚指针：指向这条记录的上一个版本。 我们拿上面的例子，对应解释下 MVCC 的实现原理，如下图： 如图，首先 insert 语句向表 t1 中插入了一条数据，a 字段为 1，b 字段为 1， ROW ID 也为 1 ，事务 ID 假设为 1，回滚指针假设为 null。当执行 update t1 set b=666 where a=1 时，大致步骤如下： 数据库会先对满足 a=1 的行加排他锁； 然后将原记录复制到 undo 表空间中； 修改 b 字段的值为 666，修改事务 ID 为 2； 并通过隐藏的回滚指针指向 undo log 中的历史记录； 事务提交，释放前面对满足 a=1 的行所加的排他锁。 在前面实验的第 6 步中，session2 查询的结果是 ses","date":"2022-06-28 15:30:06","objectID":"/interview/:2:11","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"锁 1. 为什么要加锁? 当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据库的一致性。 保证多用户环境下保证数据库完整性和一致性。 2. 按照锁的粒度分数据库锁有哪些？ 在关系型数据库中，可以按照锁的粒度把数据库锁分为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。 行级锁 行级锁是MySQL中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。 开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。BDB支持页级锁 开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 MyISAM和InnoDB存储引擎使用的锁： MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁，默认为行级锁 3. 从锁的类别上分MySQL都有哪些锁呢？ 从锁的类别上来讲，有共享锁和排他锁。 共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。 排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。 用上面的例子来说就是用户的行为有两种，一种是来看房，多个用户一起看房是可以接受的。 一种是真正的入住一晚，在这期间，无论是想入住的还是想看房的都不可以。 锁的粒度取决于具体的存储引擎，InnoDB实现了行级锁，页级锁，表级锁。 他们的加锁开销从大到小，并发能力也是从大到小。 4. 数据库的乐观锁和悲观锁是什么？怎么实现的？ 数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。 两种锁的使用场景 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 5. InnoDB引擎的行锁是怎么实现的？ InnoDB是基于索引来完成行锁 例: select * from tab_with_index where id = 1 for update; for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁，并发将无从谈起 6. 什么是死锁？怎么解决？ 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。 常见的解决死锁的方法 1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 如果业务处理不好可以用分布式事务锁或者使用乐观锁 7. 隔离级别与锁的关系 在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁； 在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。 8. 优化锁方面的意见？ 使用较低的隔离级别 设计索引，尽量使用索引去访问数据，加锁更加精确，从而减少锁冲突 选择合理的事务大小，给记录显示加锁时，最好一次性请求足够级别的锁。列如，修改数据的话，最好申请排他锁，而不是先申请共享锁，修改时在申请排他锁，这样会导致死锁 不同的程序访问一组表的时候，应尽量约定一个相同的顺序访问各表，对于一个表而言，尽可能的固定顺序的获取表中的行。这样大大的减少死锁的机会。 尽量使用相等条件访问数据，这样可以避免间隙锁对并发插入的影响 不要申请超过实际需要的锁级别 数据查询的时候不是必要，不要使用加锁。MySQL的MVCC可以实现事务中的查询不用加锁，优化事务性能：MVCC只在committed read（读提交）和 repeatable read （可重复读）两种隔离级别 对于特定的事务，可以使用表锁来提高处理速度活着减少死锁的可能。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:12","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"分库分表 1. 为什么要分库分表？ 分表 比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。 分表就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。 分库 分库就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 这就是所谓的分库分表。 2. 用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？ 这个其实就是看看你了解哪些分库分表的中间件，各个中间件的优缺点是啥？然后你用过哪些分库分表的中间件。 比较常见的包括： cobar TDDL atlas sharding-jdbc mycat cobar 阿里 b2b 团队开发和开源的，属于 proxy 层方案。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。 TDDL 淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。 atlas 360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。 sharding-jdbc 当当开源的，属于 client 层方案。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 mycat 基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。 3. 如何对数据库如何进行垂直拆分或水平拆分的？ 水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 两种分库分表的方式： 一种是按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。 或者是按照某个字段hash一下均匀分散，这个较为常用。 range 来分，好处在于说，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。 hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:13","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"读写分离、主从同步（复制） 1. 什么是MySQL主从同步？ 主从同步使得数据可以从一个数据库服务器复制到其他服务器上，在复制数据时，一个服务器充当主服务器（master），其余的服务器充当从服务器（slave）。 因为复制是异步进行的，所以从服务器不需要一直连接着主服务器，从服务器甚至可以通过拨号断断续续地连接主服务器。通过配置文件，可以指定复制所有的数据库，某个数据库，甚至是某个数据库上的某个表。 2. MySQL主从同步的目的？为什么要做主从同步？ 通过增加从服务器来提高数据库的性能，在主服务器上执行写入和更新，在从服务器上向外提供读功能，可以动态地调整从服务器的数量，从而调整整个数据库的性能。 提高数据安全-因为数据已复制到从服务器，从服务器可以终止复制进程，所以，可以在从服务器上备份而不破坏主服务器相应数据 在主服务器上生成实时数据，而在从服务器上分析这些数据，从而提高主服务器的性能 数据备份。一般我们都会做数据备份，可能是写定时任务，一些特殊行业可能还需要手动备份，有些行业要求备份和原数据不能在同一个地方，所以主从就能很好的解决这个问题，不仅备份及时，而且还可以多地备份，保证数据的安全 3. 如何实现MySQL的读写分离？ 其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 4. MySQL主从复制流程和原理？ 基本原理流程，是3个线程以及之间的关联 主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中； 从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，放进自己的relay log中； 从：sql执行线程——执行relay log中的语句； 复制过程如下： Binary log：主数据库的二进制日志 Relay log：从服务器的中继日志 第一步：master在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中。 第二步：salve开启一个I/O Thread，该线程在master打开一个普通连接，主要工作是binlog dump process。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。I/O线程最终的目的是将这些事件写入到中继日志中。 第三步：SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。 5. MySQL主从同步延时问题如何解决？ MySQL 实际上在有两个同步机制，一个是半同步复制，用来 解决主库数据丢失问题；一个是并行复制，用来 解决主从同步延时问题。 半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。 并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:14","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"MySQL优化 1. 如何定位及优化SQL语句的性能问题？ 对于低性能的SQL语句的定位，最重要也是最有效的方法就是使用执行计划，MySQL提供了explain命令来查看语句的执行计划。 我们知道，不管是哪种数据库，或者是哪种数据库引擎，在对一条SQL语句进行执行的过程中都会做很多相关的优化，对于查询语句，最重要的优化方式就是使用索引。 而执行计划，就是显示数据库引擎对于SQL语句的执行的详细情况，其中包含了是否使用索引，使用什么索引，使用的索引的相关信息等。 2. 大表数据查询，怎么优化 优化shema、sql语句+索引； 第二加缓存，memcached, redis； 主从复制，读写分离； 垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统； 水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表； 3. 超大分页怎么处理? 数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于select * from table where age \u003e 20 limit 1000000,10 这种查询其实也是有可以优化的余地的. 这条语句需要 load1000000 数据然后基本上全部丢弃,只取 10 条当然比较慢. 当时我们可以修改为select * from table where id in (select id from table where age \u003e 20 limit 1000000,10).这样虽然也 load 了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快。 解决超大分页,其实主要是靠缓存,可预测性的提前查到内容,缓存至redis等k-V数据库中,直接返回即可. 在阿里巴巴《Java开发手册》中,对超大分页的解决办法是类似于上面提到的第一种. 【推荐】利用延迟关联或者子查询优化超多分页场景。 说明：MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行SQL改写。 正例：先快速定位需要获取的id段，然后再关联： SELECT a.* FROM 表1 a, (select id from 表1 where 条件 LIMIT 100000,20 ) b where a.id=b.id 4. 统计过慢查询吗？对慢查询都怎么优化过？ 在业务系统中，除了使用主键进行的查询，其他的我都会在测试库上测试其耗时，慢查询的统计主要由运维在做，会定期将业务中的慢查询反馈给我们。 慢查询的优化首先要搞明白慢的原因是什么？ 是查询条件没有命中索引？是load了不需要的数据列？还是数据量太大？ 所以优化也是针对这三个方向来的， 首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。 分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。 5. 如何优化查询过程中的数据访问 访问数据太多导致查询性能下降 确定应用程序是否在检索大量超过需要的数据，可能是太多行或列 确认MySQL服务器是否在分析大量不必要的数据行 查询不需要的数据。解决办法：使用limit解决 多表关联返回全部列。解决办法：指定列名 总是返回全部列。解决办法：避免使用SELECT * 重复查询相同的数据。解决办法：可以缓存数据，下次直接读取缓存 是否在扫描额外的记录。解决办法： 使用explain进行分析，如果发现查询需要扫描大量的数据，但只返回少数的行，可以通过如下技巧去优化： 使用索引覆盖扫描，把所有的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果。 改变数据库和表的结构，修改数据表范式 重写SQL语句，让优化器可以以更优的方式执行查询。 6. 如何优化关联查询 确定ON或者USING子句中是否有索引。 确保GROUP BY和ORDER BY只有一个表中的列，这样MySQL才有可能使用索引。 7. 数据库结构优化 一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。 需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表 对于需要经常联合查询的表，可以建立中间表以提高查询效率。 通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。 表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 注意： 冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。 8. MySQL数据库cpu飙升到500%的话他怎么处理？ 当 cpu 飙升到 500%时，先用操作系统命令 top 命令观察是不是 MySQLd 占用导致的，如果不是，找出占用高的进程，并进行相关处理。 如果是 MySQLd 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。 一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。 也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等。 9. 大表怎么优化？ 类似的问题：某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？ 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 缓存： 使用MySQL的缓存，另外对重量级、更新少的数据可以考虑； 通过分库分表的方式进行优化，主要有垂直分表和水平分表。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:2:15","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:0","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"应用场景 缓存 共享Session 消息队列系统 分布式锁 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:1","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"单线程的Redis为什么快 纯内存操作 单线程操作，避免了频繁的上下文切换 合理高效的数据结构 采用了非阻塞I/O多路复用机制（有一个文件描述符同时监听多个文件描述符是否有数据到来） ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:2","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis 的数据结构及使用场景 String字符串:字符串类型是 Redis 最基础的数据结构，首先键都是字符串类型，而且 其他几种数据结构都是在字符串类型基础上构建的，我们常使用的 set key value 命令就是字符串。常用在缓存、计数、共享Session、限速等。 Hash哈希:在Redis中，哈希类型是指键值本身又是一个键值对结构，哈希可以用来存放用户信息，比如实现购物车。 List列表（双向链表）:列表（list）类型是用来存储多个有序的字符串。可以做简单的消息队列的功能。 Set集合：集合（set）类型也是用来保存多个的字符串元素，但和列表类型不一 样的是，集合中不允许有重复元素，并且集合中的元素是无序的，不能通过索引下标获取元素。利用 Set 的交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。 Sorted Set有序集合（跳表实现）：Sorted Set 多了一个权重参数 Score，集合中的元素能够按 Score 进行排列。可以做排行榜应用，取 TOP N 操作。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:3","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis 的数据过期策略 Redis 中数据过期策略采用定期删除+惰性删除策略 定期删除策略：Redis 启用一个定时器定时监视所有的 key，判断key是否过期，过期的话就删除。这种策略可以保证过期的 key 最终都会被删除，但是也存在严重的缺点：每次都遍历内存中所有的数据，非常消耗 CPU 资源，并且当 key 已过期，但是定时器还处于未唤起状态，这段时间内 key 仍然可以用。 惰性删除策略：在获取 key 时，先判断 key 是否过期，如果过期则删除。这种方式存在一个缺点：如果这个 key 一直未被使用，那么它一直在内存中，其实它已经过期了，会浪费大量的空间。 这两种策略天然的互补，结合起来之后，定时删除策略就发生了一些改变，不在是每次扫描全部的 key 了，而是随机抽取一部分 key 进行检查，这样就降低了对 CPU 资源的损耗，惰性删除策略互补了为检查到的key，基本上满足了所有要求。但是有时候就是那么的巧，既没有被定时器抽取到，又没有被使用，这些数据又如何从内存中消失？没关系，还有内存淘汰机制，当内存不够用时，内存淘汰机制就会上场。淘汰策略分为： 当内存不足以容纳新写入数据时，新写入操作会报错。（Redis 默认策略） 当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 Key。（LRU推荐使用） 当内存不足以容纳新写入数据时，在键空间中，随机移除某个 Key。 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 Key。这种情况一般是把 Redis 既当缓存，又做持久化存储的时候才用。 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 Key。 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 Key 优先移除。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:4","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的set和setnx Redis中setnx不支持设置过期时间，做分布式锁时要想避免某一客户端中断导致死锁，需设置lock过期时间，在高并发时 setnx与 expire 不能实现原子操作，如果要用，得在程序代码上显示的加锁。使用SET代替SETNX ，相当于SETNX+EXPIRE实现了原子性，不必担心SETNX成功，EXPIRE失败的问题。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:5","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的LRU具体实现： 传统的LRU是使用栈的形式，每次都将最新使用的移入栈顶，但是用栈的形式会导致执行select *的时候大量非热点数据占领头部数据，所以需要改进。Redis每次按key获取一个值的时候，都会更新value中的lru字段为当前秒级别的时间戳。Redis初始的实现算法很简单，随机从dict中取出五个key,淘汰一个lru字段值最小的。在3.0的时候，又改进了一版算法，首先第一次随机选取的key都会放入一个pool中(pool的大小为16),pool中的key是按lru大小顺序排列的。接下来每次随机选取的keylru值必须小于pool中最小的lru才会继续放入，直到将pool放满。放满之后，每次如果有新的key需要放入，需要将pool中lru最大的一个key取出。淘汰的时候，直接从pool中选取一个lru最小的值然后将其淘汰。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:6","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis如何发现热点key 凭借经验，进行预估：例如提前知道了某个活动的开启，那么就将此Key作为热点Key。 服务端收集：在操作redis之前，加入一行代码进行数据统计。 抓包进行评估：Redis使用TCP协议与客户端进行通信，通信协议采用的是RESP，所以自己写程序监听端口也能进行拦截包进行解析。 在proxy层，对每一个 redis 请求进行收集上报。 Redis自带命令查询：Redis4.0.4版本提供了redis-cli –hotkeys就能找出热点Key。（如果要用Redis自带命令查询时，要注意需要先把内存逐出策略设置为allkeys-lfu或者volatile-lfu，否则会返回错误。进入Redis中使用config set maxmemory-policy allkeys-lfu即可。） ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:7","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的热点key解决方案 服务端缓存：即将热点数据缓存至服务端的内存中.(利用Redis自带的消息通知机制来保证Redis和服务端热点Key的数据一致性，对于热点Key客户端建立一个监听，当热点Key有更新操作的时候，服务端也随之更新。) 备份热点Key：即将热点Key+随机数，随机分配至Redis其他节点中。这样访问热点key的时候就不会全部命中到一台机器上了。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:8","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"如何解决 Redis 缓存雪崩问题 使用 Redis 高可用架构：使用 Redis 集群来保证 Redis 服务不会挂掉 缓存时间不一致，给缓存的失效时间，加上一个随机值，避免集体失效 限流降级策略：有一定的备案，比如个性推荐服务不可用了，换成热点数据推荐服务 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:9","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"如何解决 Redis 缓存穿透问题 在接口做校验 存null值（缓存击穿加锁,或设置不过期） 布隆过滤器拦截： 将所有可能的查询key 先映射到布隆过滤器中，查询时先判断key是否存在布隆过滤器中，存在才继续向下执行，如果不存在，则直接返回。布隆过滤器将值进行多次哈希bit存储，布隆过滤器说某个元素在，可能会被误判。布隆过滤器说某个元素不在，那么一定不在。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:10","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的持久化机制 Redis为了保证效率，数据缓存在了内存中，但是会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件中，以保证数据的持久化。Redis的持久化策略有两种： RDB：快照形式是直接把内存中的数据保存到一个dump的文件中，定时保存，保存策略。当Redis需要做持久化时，Redis会fork一个子进程，子进程将数据写到磁盘上一个临时RDB文件中。当子进程完成写临时文件后，将原来的RDB替换掉。 AOF：把所有的对Redis的服务器进行修改的命令都存到一个文件里，命令的集合。 使用AOF做持久化，每一个写命令都通过write函数追加到appendonly.aof中。aof的默认策略是每秒钟fsync一次，在这种配置下，就算发生故障停机，也最多丢失一秒钟的数据。 缺点是对于相同的数据集来说，AOF的文件体积通常要大于RDB文件的体积。根据所使用的fsync策略，AOF的速度可能会慢于RDB。 Redis默认是快照RDB的持久化方式。对于主从同步来说，主从刚刚连接的时候，进行全量同步（RDB）；全同步结束后，进行增量同步(AOF)。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:11","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的事务 Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。 Redis事务没有隔离级别的概念，批量操作在发送 EXEC 命令前被放入队列缓存，并不会被实际执行，也就不存在事务内的查询要看到事务里的更新，事务外查询不能看到。 Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:12","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis事务相关命令 watch key1 key2 … : 监视一或多个key,如果在事务执行之前，被监视的key被其他命令改动，则事务被打断（类似乐观锁） multi : 标记一个事务块的开始（queued） exec : 执行所有事务块的命令（一旦执行exec后，之前加的监控锁都会被取消掉） discard : 取消事务，放弃事务块中的所有命令 unwatch : 取消watch对所有key的监控 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:13","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis和 memcached 的区别 存储方式上：memcache会把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。redis有部分数据存在硬盘上，这样能保证数据的持久性。 数据支持类型上：memcache对数据类型的支持简单，只支持简单的key-value，，而redis支持五种数据类型。 用底层模型不同：它们之间底层实现方式以及与客户端之间通信的应用协议不一样。redis直接自己构建了VM机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 value的大小：redis可以达到1GB，而memcache只有1MB。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:14","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的几种集群模式 主从复制 哨兵模式 cluster模式 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:15","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的哨兵模式 哨兵是一个分布式系统,在主从复制的基础上你可以在一个架构中运行多个哨兵进程,这些进程使用流言协议来接收关于Master是否下线的信息,并使用投票协议来决定是否执行自动故障迁移,以及选择哪个Slave作为新的Master。 每个哨兵会向其它哨兵、master、slave定时发送消息,以确认对方是否活着,如果发现对方在指定时间(可配置)内未回应,则暂时认为对方已挂(所谓的”主观认为宕机”)。 若“哨兵群“中的多数sentinel,都报告某一master没响应,系统才认为该master\"彻底死亡\"(即:客观上的真正down机),通过一定的vote算法,从剩下的slave节点中,选一台提升为master,然后自动修改相关配置。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:16","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的rehash Redis的rehash 操作并不是一次性、集中式完成的，而是分多次、渐进式地完成的，redis会维护维持一个索引计数器变量rehashidx来表示rehash的进度。 这种渐进式的 rehash 避免了集中式rehash带来的庞大计算量和内存操作，但是需要注意的是redis在进行rehash的时候，正常的访问请求可能需要做多要访问两次hashtable（ht[0]， ht[1]），例如键值被rehash到新ht1，则需要先访问ht0，如果ht0中找不到，则去ht1中找。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:17","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的hash表被扩展的条件 哈希表中保存的key数量超过了哈希表的大小. Redis服务器目前没有在执行BGSAVE命令（rdb）或BGREWRITEAOF命令，并且哈希表的负载因子大于等于1. Redis服务器目前在执行BGSAVE命令（rdb）或BGREWRITEAOF命令，并且哈希表的负载因子大于等于5.(负载因子=哈希表已保存节点数量 / 哈希表大小，当哈希表的负载因子小于0.1时，对哈希表执行收缩操作。) ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:18","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis并发竞争key的解决方案 分布式锁+时间戳 利用消息队列 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:19","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis与Mysql双写一致性方案 先更新数据库，再删缓存。数据库的读操作的速度远快于写操作的，所以脏数据很难出现。可以对异步延时删除策略，保证读请求完成以后，再进行删除操作。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:20","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Redis的管道pipeline 对于单线程阻塞式的Redis，Pipeline可以满足批量的操作，把多个命令连续的发送给Redis Server，然后一一解析响应结果。Pipelining可以提高批量处理性能，提升的原因主要是TCP连接中减少了“交互往返”的时间。pipeline 底层是通过把所有的操作封装成流，redis有定义自己的出入输出流。在 sync() 方法执行操作，每次请求放在队列里面，解析响应包。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:3:21","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"消息队列 ","date":"2022-06-28 15:30:06","objectID":"/interview/:4:0","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"为什么需要消息队列 解耦，异步处理，削峰/限流 ","date":"2022-06-28 15:30:06","objectID":"/interview/:4:1","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Kafka的文件存储机制 Kafka中消息是以topic进行分类的，生产者通过topic向Kafka broker发送消息，消费者通过topic读取数据。然而topic在物理层面又能以partition为分组，一个topic可以分成若干个partition。partition还可以细分为segment，一个partition物理上由多个segment组成，segment文件由两部分组成，分别为“.index”文件和“.log”文件，分别表示为segment索引文件和数据文件。这两个文件的命令规则为：partition全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:4:2","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Kafka 如何保证可靠性 如果我们要往 Kafka 对应的主题发送消息，我们需要通过 Producer 完成。前面我们讲过 Kafka 主题对应了多个分区，每个分区下面又对应了多个副本；为了让用户设置数据可靠性， Kafka 在 Producer 里面提供了消息确认机制。也就是说我们可以通过配置来决定消息发送到对应分区的几个副本才算消息发送成功。可以在定义 Producer 时通过 acks 参数指定。这个参数支持以下三种值： acks = 0：意味着如果生产者能够通过网络把消息发送出去，那么就认为消息已成功写入 Kafka 。在这种情况下还是有可能发生错误，比如发送的对象无能被序列化或者网卡发生故障，但如果是分区离线或整个集群长时间不可用，那就不会收到任何错误。在 acks=0 模式下的运行速度是非常快的（这就是为什么很多基准测试都是基于这个模式），你可以得到惊人的吞吐量和带宽利用率，不过如果选择了这种模式， 一定会丢失一些消息。 acks = 1：意味若 Leader 在收到消息并把它写入到分区数据文件（不一定同步到磁盘上）时会返回确认或错误响应。在这个模式下，如果发生正常的 Leader 选举，生产者会在选举时收到一个 LeaderNotAvailableException 异常，如果生产者能恰当地处理这个错误，它会重试发送悄息，最终消息会安全到达新的 Leader 那里。不过在这个模式下仍然有可能丢失数据，比如消息已经成功写入 Leader，但在消息被复制到 follower 副本之前 Leader发生崩溃。 acks = all（这个和 request.required.acks = -1 含义一样）：意味着 Leader 在返回确认或错误响应之前，会等待所有同步副本都收到悄息。如果和min.insync.replicas 参数结合起来，就可以决定在返回确认前至少有多少个副本能够收到悄息，生产者会一直重试直到消息被成功提交。不过这也是最慢的做法，因为生产者在继续发送其他消息之前需要等待所有副本都收到当前的消息。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:4:3","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Kafka消息是采用Pull模式，还是Push模式 Kafka最初考虑的问题是，customer应该从brokes拉取消息还是brokers将消息推送到consumer，也就是pull还push。在这方面，Kafka遵循了一种大部分消息系统共同的传统的设计：producer将消息推送到broker，consumer从broker拉取消息。push模式下，当broker推送的速率远大于consumer消费的速率时，consumer恐怕就要崩溃了。最终Kafka还是选取了传统的pull模式。Pull模式的另外一个好处是consumer可以自主决定是否批量的从broker拉取数据。Pull有个缺点是，如果broker没有可供消费的消息，将导致consumer不断在循环中轮询，直到新消息到t达。为了避免这点，Kafka有个参数可以让consumer阻塞知道新消息到达。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:4:4","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Kafka是如何实现高吞吐率的 顺序读写：kafka的消息是不断追加到文件中的，这个特性使kafka可以充分利用磁盘的顺序读写性能 零拷贝：跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区” 文件分段：kafka的队列topic被分为了多个区partition，每个partition又分为多个段segment，所以一个队列中的消息实际上是保存在N多个片段文件中 批量发送：Kafka允许进行批量发送消息，先将消息缓存在内存中，然后一次请求批量发送出去 数据压缩：Kafka还支持对消息集合进行压缩，Producer可以通过GZIP或Snappy格式对消息集合进行压缩 ","date":"2022-06-28 15:30:06","objectID":"/interview/:4:5","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Kafka判断一个节点还活着的两个条件 节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连接 如果节点是个 follower,他必须能及时的同步 leader 的写操作，延时不能太久 ","date":"2022-06-28 15:30:06","objectID":"/interview/:4:6","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Dubbo ","date":"2022-06-28 15:30:06","objectID":"/interview/:5:0","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Dubbo的容错机制 失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=“2” 来设置重试次数 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=“2” 来设置最大并行数。 广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息 ","date":"2022-06-28 15:30:06","objectID":"/interview/:5:1","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Dubbo注册中心挂了还可以继续通信么 可以，因为刚开始初始化的时候，消费者会将提供者的地址等信息拉取到本地缓存，所以注册中心挂了可以继续通信。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:5:2","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Dubbo提供的线程池 fixed：固定大小线程池，启动时建立线程，不关闭，一直持有。 cached：缓存线程池，空闲一分钟自动删除，需要时重建。 limited：可伸缩线程池，但池中的线程数只会增长不会收缩。(为避免收缩时突然来了大流量引起的性能问题)。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:5:3","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Dubbo框架设计结构 服务接口层：该层是与实际业务逻辑相关的，根据服务提供方和服务消费方的业务设计对应的接口和实现。 配置层：对外配置接口，以ServiceConfig和ReferenceConfig为中心，可以直接new配置类，也可以通过spring解析配置生成配置类。 服务代理层：服务接口透明代理，生成服务的客户端Stub和服务器端Skeleton，以ServiceProxy为中心，扩展接口为ProxyFactory。 服务注册层：封装服务地址的注册与发现，以服务URL为中心，扩展接口为RegistryFactory、Registry和RegistryService。可能没有服务注册中心，此时服务提供方直接暴露服务。 集群层：封装多个提供者的路由及负载均衡，并桥接注册中心，以Invoker为中心，扩展接口为Cluster、Directory、Router和LoadBalance。将多个服务提供方组合为一个服务提供方，实现对服务消费方来透明，只需要与一个服务提供方进行交互。 监控层：RPC调用次数和调用时间监控，以Statistics为中心，扩展接口为MonitorFactory、Monitor和MonitorService。 远程调用层：封将RPC调用，以Invocation和Result为中心，扩展接口为Protocol、Invoker和Exporter。Protocol是服务域，它是Invoker暴露和引用的主功能入口，它负责Invoker的生命周期管理。Invoker是实体域，它是Dubbo的核心模型，其它模型都向它靠扰，或转换成它，它代表一个可执行体，可向它发起invoke调用，它有可能是一个本地的实现，也可能是一个远程的实现，也可能一个集群实现。 信息交换层：封装请求响应模式，同步转异步，以Request和Response为中心，扩展接口为Exchanger、ExchangeChannel、ExchangeClient和ExchangeServer。 网络传输层：抽象mina和netty为统一接口，以Message为中心，扩展接口为Channel、Transporter、Client、Server和Codec。 数据序列化层：可复用的一些工具，扩展接口为Serialization、 ObjectInput、ObjectOutput和ThreadPool。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:5:4","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"linux ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:0","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"linux 基础 1. 什么是Linux Linux是一套免费使用和自由传播的类似Unix操作系统，一般的WEB项目都是部署都是放在Linux 操作系统上面。 Linux是一个基于POSIX和Unix的多用户、多任务、支持多线程和多CPU的操作系 统。它能运行主要的Unix工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了 Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 2. Windows和Linux的区别 Windows是微软开发的操作系统，民用操作系统，可用于娱乐、影音、上网。 Windows操作系统 具有强大的日志记录系统和强大的桌面应用。好处是它可以帮我们实现非常多绚丽多彩的效果，可 以非常方便去进行娱乐、影音、上网。 Linux的应用相对单纯很多，没有什么绚丽多彩的效果，因此Linux的性能是非常出色的，可以完 全针对机器的配置有针对性的优化， 简单来说Windows适合普通用户进行娱乐办公使用，Linux适合软件开发部署 3. Unix和Linux有什么区别？ Linux和Unix都是功能强大的操作系统，都是应用广泛的服务器操作系统，有很多相似之处，甚至 有一部分人错误地认为Unix和Linux操作系统是一样的，然而，事实并非如此，以下是两者的区 别。 开源性 Linux是一款开源操作系统，不需要付费，即可使用；Unix是一款对源码实行知识产权保护的 传统商业软件，使用需要付费授权使用。 跨平台性 Linux操作系统具有良好的跨平台性能，可运行在多种硬件平台上；Unix操作系统跨平台性能 较弱，大多需与硬件配套使用。 可视化界面 Linux除了进行命令行操作，还有窗体管理系统；Unix只是命令行下的系统。 硬件环境 Linux操作系统对硬件的要求较低，安装方法更易掌握；Unix对硬件要求比较苛刻，按照难度 较大。 用户群体 Linux的用户群体很广泛，个人和企业均可使用；Unix的用户群体比较窄，多是安全性要求高 的大型企业使用，如银行、电信部门等，或者Unix硬件厂商使用，如Sun等。 相比于Unix操作系统，Linux操作系统更受广大计算机爱好者的喜爱，主要原因是Linux操作 系统具有Unix操作系统的全部功能，并且能够在普通PC计算机上实现全部的Unix特性，开源 免费的特性，更容易普及使用！ 4. 什么是 Linux 内核？ Linux 系统的核心是内核。内核控制着计算机系统上的所有硬件和软件，在必要时分配硬件，并根 据需要执行软件。 系统内存管理 应用程序管理 硬件设备管理 文件系统管理 5. Linux的基本组件是什么？ 就像任何其他典型的操作系统一样，Linux拥有所有这些组件：内核，shell和GUI，系统实用程序 和应用程序。Linux比其他操作系统更具优势的是每个方面都附带其他功能，所有代码都可以免费 下载。 6. Linux 的体系结构 从大的方面讲，Linux 体系结构可以分为两块： 用户空间(User Space) ：用户空间又包括用户的应用程序(User Applications)、C 库(C Library) 。 内核空间(Kernel Space) ：内核空间又包括系统调用接口(System Call Interface)、内核(Kernel)、 平台架构相关的代码(Architecture-Dependent Kernel Code) 。 为什么 Linux 体系结构要分为用户空间和内核空间的原因？ 1、现代 CPU 实现了不同的工作模式，不同模式下 CPU 可以执行的指令和访问的寄存器不同。 2、Linux 从 CPU 的角度出发，为了保护内核的安全，把系统分成了两部分。 用户空间和内核空间是程序执行的两种不同的状态，我们可以通过两种方式完成用户空间到内核空 间的转移： 系统调用； 硬件中断。 7. BASH和DOS之间的基本区别是什么？ BASH和DOS控制台之间的主要区别在于3个方面： BASH命令区分大小写，而DOS命令则不区分; 在BASH下，/ character是目录分隔符，\\作为转义字符。在DOS下，/用作命令参数分隔 符，\\是目录分隔符 DOS遵循命名文件中的约定，即8个字符的文件名后跟一个点，扩展名为3个字符。BASH没 有遵循这样的惯例。 8. Linux 开机启动过程？ 1、主机加电自检，加载 BIOS 硬件信息。 2、读取 MBR 的引导文件(GRUB、LILO)。 3、引导 Linux 内核。 4、运行第一个进程 init (进程号永远为 1 )。 5、进入相应的运行级别。 6、运行终端，输入用户名和密码。 9. Linux系统缺省的运行级别？ 关机。 单机用户模式。 字符界面的多用户模式(不支持网络)。 字符界面的多用户模式。 未分配使用。 图形界面的多用户模式。 重启。 10. Linux 使用的进程间通信方式？ 1、管道(pipe)、流管道(s_pipe)、有名管道(FIFO)。 2、信号(signal) 。 3、消息队列。 4、共享内存。 5、信号量。 6、套接字(socket) 。 11. Linux 有哪些系统日志文件？ 比较重要的是 /var/log/messages 日志文件。 该日志文件是许多进程日志文件的汇总，从该文件可以看出任何入侵企图或成功的入侵。 另外，如果胖友的系统里有 ELK 日志集中收集，它也会被收集进去。 12. Linux系统安装多个桌面环境有帮助吗？ 通常，一个桌面环境，如KDE或Gnome，足以在没有问题的情况下运行。尽管系统允许从一个环 境切换到另一个环境，但这对用户来说都是优先考虑的问题。有些程序在一个环境中工作而在另一 个环境中无法工作，因此它也可以被视为选择使用哪个环境的一个因素。 13. 什么是交换空间？ 交换空间是Linux使用的一定空间，用于临时保存一些并发运行的程序。当RAM没有足够的内存来 容纳正在执行的所有程序时，就会发生这种情况。 14. 什么是root帐户 root帐户就像一个系统管理员帐户，允许你完全控制系统。你可以在此处创建和维护用户帐户，为 每个帐户分配不同的权限。每次安装Linux时都是默认帐户。 15. 什么是LILO？ LILO是Linux的引导加载程序。它主要用于将Linux操作系统加载到主内存中，以便它可以开始运 行。 16. 什么是BASH？ BASH是Bourne Again SHell的缩写。它由Steve Bourne编写，作为原始Bourne Shell（由/ bin / sh表示）的替代品。它结合了原始版本的Bourne Shell的所有功能，以及其他功能，使其更容易 使用。从那以后，它已被改编为运行Linux的大多数系统的默认shell。 17. 什么是CLI？ 命令行界面（英语：command-line interface，缩写]：CLI）是在图形用户界面得到普及之前使 用最为广泛的用户界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后，予以 执行。也有人称之为字符用户界面（CUI）。 通常认为，命令行界面（CLI）没有图形用户界面（GUI）那么方便用户操作。因为，命令行界面 的软件通常需要用户记忆操作的命令，但是，由于其本身的特点，命令行界面要较图形用户界面节 约计算机系统的资源。在熟记命令的前提下，使用命令行界面往往要较使用图形用户界面的操作速 度要快。所以，图形用户界面的操作系统中，都保留着可选的命令行界面。 18. 什么是GUI？ 图形用户界面（Graphical User Interface，简称 GUI，又称图形用户接口）是指采用图形方式显 示的计算机操作用户界面。 图形用户界面是一种人与计算机通信的界面显示格式，允许用户使用鼠标等输入设备操纵屏幕上的 图标或菜单选项，以选择命令、调用文件、启动程序或执行其它一些日常任务。与通过键盘输入文 本或字符命令来完成例行任务的字符界面相比，图形用户界面有许多优点。 20. GNU项目的重要性是什么？ 这种所谓的自由软件运动具有多种优势，例如可以自由地运行程序以及根据你的需要自由学习和修 改程序。它还允许你将软件副本重新分发给其他人，以及自由改进软件并将其发布给公众。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:1","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"磁盘、目录、文件 21. 简单 Linux 文件系统？ 在 Linux 操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出 设备、普通文件或是目录都被看作是一个文件。 也就是说在 Linux 系统中有一个重要的概念：一切都是文件。其实这是 Unix 哲学的一个体现，而 Linux 是重写 Unix 而来，所以这个概念也就传承了下来。在 Unix 系统中，把一切资源都看作是 文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就 可以用读写文件的方式实现对硬件的访问。 Linux 支持 5 种文件类型，如下图所示： 22. Linux 的目录结构是怎样的？ Linux 文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： 常见目录说明 23. 什么是 inode ？ 一般来说，面试不会问 inode 。但是 inode 是一个重要概念，是理解 Unix/Linux 文件系统和硬 盘储存的基础。 理解inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做\"扇区\"（Sector）。每个扇区储存512字节（相当于 0.5KB）。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇 区，即一次性读取一个\"块\"（block）。这种由多个扇区组成的\"块\"，是文件存取的最小单 位。“块\"的大小，最常见的是4KB，即连续八个 sector组成一个 block。 文件数据都储存在\"块\"中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的 创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名 为\"索引节点”。 每一个文件都有对应的inode，里面包含了与该文件有关的一些信息。 24. 什么是硬链接和软链接？ 硬链接：由于 Linux 下的文件是通过索引节点(inode)来识别文件，硬链接可以认为是一个指针， 指向文件索引节点的指针，系统并不为它重新分配 inode 。每添加一个一个硬链接，文件的链接 数就加 1 。 不足： 不可以在不同文件系统的文件间建立链接； 只有超级用户才可以为目录创建硬链接。 软链接：软链接克服了硬链接的不足，没有任何文件系统的限制，任何用户可以创建指向目录的符 号链接。因而现在更为广泛使用，它具有更大的灵活性，甚至可以跨越不同机器、不同网络对文件 进行链接。 不足：因为链接文件包含有原文件的路径信息，所以当原文件从一个目录下移到其他目录 中，再访问链接文件，系统就找不到了，而硬链接就没有这个缺陷，你想怎么移就怎么移； 还有它要系统分配额外的空间用于建立新的索引节点和保存原文件的路径。 25. RAID 是什么? RAID 全称为独立磁盘冗余阵列(Redundant Array of Independent Disks)，基本思想就是把多个 相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、 容量巨大 的硬盘。RAID 通常被用在服务器电脑上，使用完全相同的硬盘组成一个逻辑扇区，因此操作系统 只会把它当做一个硬盘。 RAID 分为不同的等级，各个不同的等级均在数据可靠性及读写性能上做了不同的权衡。在实际应 用中，可以依据自己的实际需求选择不同的 RAID 方案。 当然，因为很多公司都使用云服务，大家很难接触到 RAID 这个概念，更多的可能是普通云盘、 SSD 云盘酱紫的概念。 26. 一台 Linux 系统初始化环境后需要做一些什么安全工作？ 1、添加普通用户登陆，禁止 root 用户登陆，更改 SSH 端口号。 修改 SSH 端口不一定绝对哈。当然，如果要暴露在外网，建议改下。 2、服务器使用密钥登陆，禁止密码登陆。 3、开启防火墙，关闭 SElinux ，根据业务需求设置相应的防火墙规则。 4、装 fail2ban 这种防止 SSH 暴力破击的软件。 5、设置只允许公司办公网出口 IP 能登陆服务器(看公司实际需要) 也可以安装 VPN 等软件，只允许连接 VPN 到服务器上。 6、修改历史命令记录的条数为 10 条。 7、只允许有需要的服务器可以访问外网，其它全部禁止。 8、做好软件层面的防护。 设置 nginx_waf 模块防止 SQL 注入。 把 Web 服务使用 www 用户启动，更改网站目录的所有者和所属组为 www 。 27. 什么叫 CC 攻击？什么叫 DDOS 攻击？ CC 攻击，主要是用来攻击页面的，模拟多个用户不停的对你的页面进行访问，从而使你的系统资 源消耗殆尽。 DDOS 攻击，中文名叫分布式拒绝服务攻击，指借助服务器技术将多个计算机联合起来作为攻击 平台，来对一个或多个目标发动 DDOS 攻击。 攻击，即是通过大量合法的请求占用大量网络资源，以达到瘫痪网络的目的。 怎么预防 CC 攻击和 DDOS 攻击？ 防 CC、DDOS 攻击，这些只能是用硬件防火墙做流量清洗，将攻击流量引入黑洞。 流量清洗这一块，主要是买 ISP 服务商的防攻击的服务就可以，机房一般有空余流量，我们一般 是买服务，毕竟攻击不会是持续长时间。 28. 什么是网站数据库注入？ 由于程序员的水平及经验参差不齐，大部分程序员在编写代码的时候，没有对用户输入数据的合法 性进行判断。 应用程序存在安全隐患。用户可以提交一段数据库查询代码，根据程序返回的结果，获得某些他想 得知的数据，这就是所谓的 SQL 注入。 SQL注入，是从正常的 WWW 端口访问，而且表面看起来跟一般的 Web 页面访问没什么区别，如 果管理员没查看日志的习惯，可能被入侵很长时间都不会发觉。 29. Shell 脚本是什么？ 一个 Shell 脚本是一个文本文件，包含一个或多个命令。作为系统管理员，我们经常需要使用多个 命令来完成一项任务，我们可以添加这些所有命令在一个文本文件(Shell 脚本)来完成这些日常工 作任务。 30.如何选择 Linux 操作系统版本? 一般来讲，桌面用户首选 Ubuntu ；服务器首选 RHEL 或 CentOS ，两者中首选 CentOS 。 安全性要求较高，则选择 Debian 或者 FreeBSD 。 需要使用数据库高级服务和电子邮件网络应用的用户可以选择 SUSE 。 想要新技术新功能可以选择 Feddora ，Feddora 是 RHEL 和 CentOS 的一个测试版和预发布 版本。 【重点】根据现有状况，绝大多数互联网公司选择 CentOS 。现在比较常用的是 6 系列，现 在市场占有大概一半左右。另外的原因是 CentOS 更侧重服务器领域，并且无版权约束。 CentOS 7 系列，也慢慢使用的会比较多了。 31. 如何规划一台 Linux 主机，步骤是怎样？ 1、确定机器是做什么用的，比如是做 WEB 、DB、还是游戏服务器。 不同的用途，机器的配置会有所不同。 2、确定好之后，就要定系统需要怎么安装，默认安装哪些系统、分区怎么做。 3、需要优化系统的哪些参数，需要创建哪些用户等等的。 32. 请问当用户反馈网站访问慢，你会如何处理？ 有哪些方面的因素会导致网站网站访问慢？ 1、服务器出口带宽不够用 本身服务器购买的出口带宽比较小。一旦并发量大的话，就会造成分给每个用户的出口 带宽就小，访问速度自然就会慢。 跨运营商网络导致带宽缩减。例如，公司网站放在电信的网络上，那么客户这边对接是 长城宽带或联通，这也可能导致带宽的缩减。 2、服务器负载过大，导致响应不过来 可以从两个方面入手分析： 分析系统负载，使用 w 命令或者 uptime 命令查看系统负载。如果负载很高，则使用 top 命令查看 CPU ，MEM 等占用情况，要么是 CPU 繁忙，要么是内存不够。 如果这二者都正常，再去使用 sar 命令分析网卡流量，分析是不是遭到了攻击。一旦分 析出问题的原因，采取对应的措施解决，如决定要不要杀死一些进程，或者禁止一些访 问等。 3、数据库瓶颈 如果慢查询比较多。那么就要开发人员或 DBA 协助进行 SQL 语句的优化。 如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等。然后，也可以搭建 MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。 4、网站开发代码没有优化好 例如 SQL 语句没有优化，导致数据库读写相当耗时。 针对网站访问慢，怎么去排查？ 1、首先要确定是用户端还是服务端的问题。当接到用户反馈访问慢，那边自己立即访问网站看 看，如果自己这边访问快，基本断定是用户端问题，就需要耐心跟客户解释，协助客户解决问题。 不要上来就看服务端的问题。一定要从源头开始，逐步逐步往下。 2、如果访问也慢，那么可以利用浏览器的调试功能，看看加载那一项数据消耗时间过多，是图片 加载慢，还是某些数据加载慢。 3、针对服务器负载情况。查看服务器硬件(网络、CPU、内存)的消耗情况。如果是购买的云主 机，比如阿里云，可以登录阿里云平台提供各方面的监控，比如 CPU、内存、带宽的使用情况。 4、如果发现硬件资源消耗都不高，那么就需要通过查日志，比如看看 MySQL慢查询的日志，看 看是不是某条 SQL 语句查询慢，导致网站访问慢。 怎么去解决？ 1、如果是出口带宽问题，那么久申请加大出口带宽。 2、如果慢查询比较多，那么就要开发人员或 DBA 协助进行 SQL 语句的优化。 3、如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等等。然后也可以搭建MySQL 主 从，一台 MySQL 服务器负责写，其他几台从","date":"2022-06-28 15:30:06","objectID":"/interview/:6:2","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"34. 基本命令 cd （change directory：英文释义是改变目录）切换目录 cd ../ ;跳到上级目录 cd /opt ;不管现在到那直接跳到指定的opt文件夹中 cd ~ ;切换当前用户的家目录。root用户的家目录就是root目录。 pwd （print working directory：显示当前工作目录的绝对路径） pwd 显示当前的绝对路径 ls （ls：list的缩写，查看列表）查看当前目录下的所有文件夹（ls 只列出文件名或目录名） ls -a ;显示所有文件夹,隐藏文件也显示出来 ls -R ;连同子目录一起列出来 ll （ll：list的缩写，查看列表详情）查看当前目录下的所有详细信息和文件夹（ll 结果是详细,有时间, 是否可读写等信息） ll -a ;显示所有文件,隐藏文件也显示出来 ll -R ;连同子目录内容一起列出来 ll -h ;友好展示详情信息,可以看大小 ll -al ;即能显示隐藏文件又能显示详细列表。 touch （touch：创建文件）创建文件 touch test.txt ;创建test.txt文件 touch /opt/java/test.java ;在指定目录创建test.java文件 mkdir （mkdir：创建目录） 创建目录 mkdir 文件夹名称 ;在此目录创建文件夹 mkdir /opt/java/jdk ;在指定目录创建文件夹 cat （concatenate：显示或把多个文本文件连接起来）查看文件命令（可以快捷查看当前文件的内 容）（不能快速定位到最后一页） cat lj.log ;快捷查看文件命令 Ctrl + c ;暂停显示文件 Ctrl + d ;退出查看文件命令 more （more：更多的意思）分页查看文件命令（不能快速定位到最后一页） 回车：向下n行，需要定义，默认为1行。 空格键：向下滚动一屏或Ctrl+F B：返回上一层或Ctrl+B q：退出more less （lese：较少的意思）分页查看文件命令（可以快速定位到最后一页） less -m 显示类似于more命令的百分比。 less -N 显示每行的行号。(大写的N) 两参数一起使用如：less -mN 文件名，如此可分页并显示行号。 空格键：前下一页或page down。 回车：向下一行。 b：后退一页 或 page up。 q：退出。 d：前进半页。 u：后退半页 tail（尾巴） 查看文件命令（看最后多少行） tail -10 ;文件名 看最后10行 cp（copy单词缩写，复制功能） cp /opt/java/java.log /opt/logs/ ;把java.log 复制到/opt/logs/下 cp /opt/java/java.log /opt/logs/aaa.log ;把java.log 复制到/opt/logs/下并且改名为 aaa.log cp -r /opt/java /opt/logs ;把文件夹及内容复制到logs文件中 mv（move单词缩写，移动功能，该文件名称功能） mv /opt/java/java.log /opt/mysql/ ;移动文件到mysql目录下 mv java.log mysql.log ;把java.log改名为mysql.log rm（remove：移除的意思）删除文件，或文件夹 -f或--force 强制删除文件或目录。删除文件不包括文件夹的文件 -r或-R或--recursive 递归处理，将指定目录下的所有文件及子目录一并删除。 -rf 强制删除文件夹及内容 rm 文件名 ;安全删除命令 （yes删除 no取消） rm -rf 强制删除文件夹及内容 rm -rf * 删除当前目录下的所有内容。 rm -rf /* 删除Linux系统根目录下所有的内容。系统将完蛋。 find （find：找到的意思）查找指定文件或目录 * 表示0~多个任意字符。 find -name 文件名;按照指定名称查找在当前目录下查找文件 find / -name 文件名按照指定名称全局查找文件 find -name '*文件名' ;任意前缀加上文件名在当前目录下查找文件 find / -name '*文件名*' ;全局进行模糊查询带文件名的文件 vi （VIsual：视觉）文本编辑器 类似win的记事本 （操作类似于地下的vim命令，看底下vim 的操 作） vim （VI IMproved：改进版视觉）改进版文本编辑器 （不管是文件查看还是文件编辑 按 Shift + 上或 者下可以上下移动查看视角） 输入”vim 文件名” 打开文件，刚刚时是”一般模式”。 一般模式：可以浏览文件内容，可以进行文本快捷操作。如单行复制，多行复制，单行删除，多行删除，（退 出）等。 插入模式：可以编辑文件内容。 底行模式：可以进行强制退出操作,不保存 :q! 可以进行保存并退出操作 :wq 按下”i”或”a”或”o”键，从”一般模式”，进入”插入模式（编辑模式）”。 在编辑模式下按”Esc” 即可到一般模式 在一般模式下按”:”，冒号进入底行模式。 在一般模式下的快捷键 dd ;删除一整行 X ;向前删除 等同于windowns系统中的删除键 x ;向后删除和大写x相反方向 Ctrl + f ;向后看一页 Ctrl + b ;向前看一页 u ;撤销上一步操作 /word ;向下查找word关键字 输入:n查找下一个,N查找上一个（不管是哪个查找都是全局查找 只不过 n的方向相反） ?log ;向上查找log关键字 输入:n查找上一个,N查找下一个 :1,90s/redis/Redis/g ;把1-90行的redis替换为Redis。语法n1,n2s/原关键字/新关键字/g，n1 代表其实行,n2代表结尾行,g是必须要的 :0 ;光标移动到第一行 :$ ;光标移动到最后一行 :300 ;光标移动到300行,输入多少数字移动到多少行 :w ;保存 :w! ;强制保存 :q ;退出 :q! ;强制退出 5dd ;删除后面5行,打一个参数为自己填写 5x ;删除此光标后面5个字符 d1G ;删除此光标之前的所有 d0 ;从光标当前位置删除到此行的第一个位置 yy ;复制 p ;在光标的下面进行粘贴 P ;在光标的上门进行粘贴 管道命令（把多个命令组合起来使用） 管道命令的语法：命令1 | 命令2 | 命令3。 grep （grep ：正则表达式）正则表达式，用于字符串的搜索工作(模糊查询)。不懂可以先过 单独使用： grep String test.java ；在test.java文件中查找String的位置，返回整行 一般此命令不会单独使用下面列几个常用的命令（地下通过管道命令组合起来使用） ps aux|grep java ；查找带java关键字的进程 ll |grep java ；查找带java关键字的文件夹及文件 yum install -y lrzsz 命令（实现win到Linux文件互相简单上传文件） #（实际上就是在Linux系统中下载了一个插件）下了了此安装包后就可以实现win系统到linux之间拉文件拉 文件 #等待下载完了就可以输入： rz 从win系统中选择文件上传到Linux系统中 sz 文件名 选择Linux系统的文件复制到win系统中 tar （解压 压缩 命令） 常用的组合命令： -z 是否需要用gzip压缩。 -c 建立一个压缩文件的参数指令(create) –压缩 -x 解开一个压缩文件的参数指令(extract) –解压 -v 压缩的过程中显示文件(verbose) -f 使用档名，在f之后要立即接档中(file) 常用解压参数组合：zxvf 常用压缩参数组合：zcvf 解压命令： tar -zxvf redis-3.2.8.tar.gz ；解压到当前文件夹 tar -zxvf redis-3.2.8.tar.gz -C /opt/java/ ；解压到指定目录 压缩命令：（**注意** 语法有点反了，我反正每次都搞反） tar -zcvf redis-3.2.8.tar.gz redis-3.2.8/ ;语法 tar -zcvf 压缩后的名称 要压缩的 文件 tar -zcvf 压缩后的文件（可指定目录） 要压缩的文件（可指定目录） ps （process status：进程状态，类似于windows的任务管理器） 常用组合：ps -ef 标准的格式查看系统进程 ps -aux BSD格式查看系统进程 ps -aux|grep redis BSD格式查看进程名称带有redis的系统进程（常用技巧） //显示进程的一些属性,需要了解（ps aux） USER //用户名 PID //进程ID号,用来杀死进程的 %CPU //进程占用的CPU的百分比 %MEM //占用内存的的百分比 VSZ //该进程使用的虚拟內存量（KB） RSS //该进程占用的固定內存量（KB） STAT //进程的状态 START //该进程被触发启动时间 TIME //该进程实际使用CPU运行的时间 clear 清屏命令。（强迫症患者使用） kill 命令用来中止一个进程。（要配合ps命令使用，配合pid关闭进程） （ps类似于打开任务管理器，kill类似于关闭进程） kill -5 进程的PID ;推荐,和平关闭进程 kill -9 PID ;不推荐,强制杀死进程 ifconfig命令 用于查看和更改网络接口的地址和参数，包括IP地址、网络掩码、广播地址，使用权限是超级用户。","date":"2022-06-28 15:30:06","objectID":"/interview/:6:3","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"进程 进程 进程是系统进行资源分配和调度的一个独立单位。每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销(栈、寄器、虚拟内存、文件句柄等)比较大，但相对比较稳定安全。 线程 线程是进程的一个实体，是CPU调度和分派的基本单位，它是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。线程间信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。 协程 协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。 1.进程和线程的区别? 一个程序至少有一个进程，一个进程至少有一个线程 从系统调度上看：进程是资源管理的基本单位，线程是程序执行的基本单位。 从上下文切换上看：线程上下文切换比进程上下文切换要快得多。 从系统开销上看：创建或撤销进程时，系统都要为之分配或回收系统资源，如内存空间，I/O设备等，OS所付出的开销显著大于在创建或撤销线程时的开销，进程切换的开销也远大于线程切换的开销。 2.协程与线程的区别? 一个线程可以有多个协程，一个进程也可以有多个协程。线程和进程都是同步机制，而协程是异步机制。 协程是由程序自身控制，没有线程切换的开销，执行效率非常高 协程不需要多线程的锁机制：在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。 线程是抢占式，而协程是非抢占式的。需要用户释放使用权切换到其他协程，因此同一时间其实只有一个协程拥有运行权，相当于单线程的能力。 协程不被操作系统内核管理，而完全是由程序控制。线程是被分割的CPU资源，协程是组织好的代码流程，线程是协程的资源。但协程不会直接使用线程，协程直接利用的是执行器关联任意线程或线程池。 并发与并行的区别是什么？ 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。 并发的关键是你有处理多个任务的能力，不一定要同时。 并行的关键是你有同时处理多个任务的能力。 所以我认为它们最关键的点就是：是否是『同时』。 4.进程与线程的切换流程? 进程切换分两步: 切换页表以使用新的地址空间，一旦去切换上下文，处理器中所有已经缓存的内存地址一瞬间都作废了。 切换内核栈和硬件上下文。 对于linux来说，线程和进程的最大区别就在于地址空间，对于线程切换，第1步是不需要做的，第2步是进程和线程切换都要做的。 因为每个进程都有自己的虚拟地址空间，而线程是共享所在进程的虚拟地址空间的，因此同一个进程中的线程进行线程切换时不涉及虚拟地址空间的转换。 12.进程调度策略有哪几种? 先来先服务，短作业优先，最短剩余时间优先，时间片轮转，优先级调度。 先来先服务：非抢占式的调度算法，按照请求的顺序进行调度。有利于长作业，但不利于短作业，因为短作业必须一 直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。另外，对I/o密集型进程也不利，因为这种进程每次进行I/o操作之后又得重新排队。 短作业优先：非抢占式的调度算法，按估计运行时间最短的顺序进行调度。长作业有可能会饿死，处于-直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。 最短剩余时间优先：最短作业优先的抢占式版本，按剩余运行时间的顺序进行调度。 当一.个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。 时间片轮转：将所有就绪进程按FCFS的原则排成一一个队列，每次调度时，把CPU时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把CPU时间分配给队首的进程。时间片轮转算法的效率和时间片的大小有很大关系:因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进程切换上就会花过多时间。而如果时间片过长，那么实时性就不能得到保证。 优先级调度：为每个进程分配一个优先级，按优先级进行调度。为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。 进程有哪些状态? 进程一共有5种状态，分别是新建、就绪、运行、阻塞、终止。 什么是用户态和内核态? 用户态和系统态是操作系统的两种运行状态: 用户态：用户态运行的程序只能受限地访问内存，只能直接读取用户程序的数据，并且不允许访问外围设备，用户态下的CPU不允许独占，也就是说CPU能够被其他程序获取。 内核态：内核态运行的程序可以访问计算机的任何数据和资源，不受限制，包括外围设备，比如网卡、硬盘等。处于内核态的CPU可以从一个程序切换到另外一个程序，并且占用CPU不会发生抢占情况。 将操作系统的运行状态分为用户态和内核态，主要是为了对访问能力进行限制，防止随意进行一些比较危险的操作导致系统的崩溃，比如设置时钟、内存清理，这些都需要在内核态下完成。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:4","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"讲一讲IO多路复用? IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。IO多路复用适用如下场合: 当客户处理多个描述字时(一般是交互式输入和网络套接口)，必须使用I/O复用。. 当一个客户同时处理多个套接口时，而这种情况是可能的，但很少出现。 如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用。 如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用。 如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用。 与多进程和多线程技术相比，IO多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。 IO多路复用模型指的是：使用单个进程同时处理多个网络连接IO，他的原理就是select、poll、epoll不断轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。该模型的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。 下面举一个例子，模拟一个tcp服务器处理30个客户socket。 假设你是一个老师，让30个学生解答一道题目，然后检查学生做的是否正确，你有下面几个选择: **(select)**第一种选择：按顺序逐个检查，先检查A，然后是B，之后是C、D。。。这中间如果有一个学生卡住，全班都会被耽误。这种模式就好比，你用循环挨个处理socket，根本不具有并发能力。 (poll)第二种选择：你创建30个分身，每个分身检查一个学生的答案是否正确。 这种类似于为每一个用户创建一个进程或者线程处理连接。 (select)第三种选择，你站在讲台上等，谁解答完谁举手。这时C、D举手，表示他们解答问题完毕，你下去依次检查C、D的答案，然后继续回到讲台上等。此时E、A又举手，然后去处理E和A。。。 这种就是IO复用模型，Linux下的select、poll和epoll就是干这个的。将用户socket对应的fd注册进epoll，然后epoll帮你监听哪些socket上有消息到达，这样就避免了大量的无用操作。此时的socket应该采用非阻塞模式。 这样，整个过程只在调用select、poll、epoll这些调用的时候才会阻塞，收发客户消息是不会阻塞的，整个进程或者线程就被充分利用起来，这就是事件驱动，所谓的reactor模式。 select、poll 和epoll之间的区别? select：时间复杂度O(n) select仅仅知道有I/O事件发生，但并不知道是哪几个流，所以只能无差别轮询所有流，找出能读出数据或者写入数据的流，并对其进行操作。所以select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。 poll：时间复杂度O(n) poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的。 epoll： 时间复杂度O(1) epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll 会把哪个流发生了怎样的I/O事件通知我们。所以说epoll实际上是事件驱动(每个事件关联上fd)的。 select，poll，epoll都是IO多路复用的机制。l/O多路复用就是通过一种机制监视多个描述符，一旦某个描述符就绪(一般是读就绪或者写就绪)，就通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O,因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步l/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:5","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"软链接和硬链接有什么区别? 软链接可以理解成快捷方式，也清楚知道原文件在哪里。它和Windows下的快捷方式的作用是一样的，删除源文件，快捷方式也就无法使用了。 硬链接可以看成一个备份，但并没有增加空间，因为文件就一个，这种可以防止源文件删除后不能使用问题；同时也有局限性，不能对目录和跨文件系统使用。 语法格式(ln)区别： 　硬链接：ln 源文件 链接名 　软链接：ln -s 源文件 链接名 　注意：链接的源文件路径要写绝对路径，否则会报错。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:6","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"中断的处理过程? 保护现场：将当前执行程序的相关数据保存在寄存器中，然后入栈。 开中断：以便执行中断时能响应较高级别的中断请求。 中断处理 关中断：保证恢复现场时不被新中断打扰 恢复现场：从堆栈中按序取出程序数据，恢复中断前的执行状态。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:7","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.1 Linux里如何查看一个想知道的进程？ 查看进程运行状态的指令：ps命令。“ps -aux | grep PID”，用来查看某PID进程状态 ps使用示例 //显示当前所有进程 ps -A //与grep联用查找某进程 ps -aux | grep apache //查看进程运行状态、查看内存使用情况的指令均可使用top指令。 top ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:8","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.2 Linux里如何查看带有关键字的日志文件？ cat 路径/文件名 | grep 关键词 # 返回test.log中包含http的所有行 cat test.log | grep \"http\" grep -i 关键词 路径/文件名 （与方法一效果相同，不同写法而已） # 返回test.log中包含http的所有行(-i忽略大小写） grep -i \"http\" ./test.log ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:9","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.3 说说你对grep命令的了解？ grep 命令。强大的文本搜索命令，grep(Global Regular Expression Print) 全局正则表达式搜索。 grep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:10","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.4 Linux修改主机名的命令是什么？ 如果只需要临时更改主机名，可以使用hostname命令。 sudo hostname \u003cnew-hostname\u003e # 例如: sudo hostname myDebian #myDebian为修改名 如果想永久改变主机名，可以使用hostnamectl命令 sudo hostnamectl set-hostname myDebian #myDebian为修改名 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:11","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.5 Linux开机自动执行命令如何实现？ 方法 #1 - 使用 cron 任务 除了常用格式（分 / 时 / 日 / 月 / 周）外，cron 调度器还支持 @reboot 指令。这个指令后面的参数是脚本（启动时要执行的那个脚本）的绝对路径。 然而，这种方法需要注意两点： a) cron 守护进程必须处于运行状态（通常情况下都会运行），同时 b) 脚本或 crontab 文件必须包含需要的环境变量。 方法 #2 - 使用 /etc/rc.d/rc.local 这个方法对于 systemd-based 发行版 Linux 同样有效。不过，使用这个方法，需要授予 /etc/rc.d/rc.local 文件执行权限: # chmod +x /etc/rc.d/rc.local 然后在这个文件底部添加脚本。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:12","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.6 Linux查看内存的命令是什么？ 查看内存使用情况的指令：free命令。“free -m”，命令查看内存使用情况。 查看进程运行状态、查看内存使用情况的指令均可使用top指令。 free命令 Linux free命令用于显示内存状态。 free指令会显示内存的使用情况，包括实体内存，虚拟的交换文件内存，共享内存区段，以及系统核心使用的缓冲区等。 参数如下: -b 以Byte为单位显示内存使用情况。 -k 以KB为单位显示内存使用情况。 -m 以MB为单位显示内存使用情况。 -h 以合适的单位显示内存使用情况，最大为三位数，自动计算对应的单位值。 单位有: B = bytes K = kilos M = megas G = gigas T = teras -o 不显示缓冲区调节列。 -s\u003c间隔秒数\u003e 持续观察内存使用状况。 -t 显示内存总和列。 -V 显示版本信息。 实例:显示内存使用情况 # free //显示内存使用信息 total used free shared buffers cached Mem: 254772 184568 70204 0 5692 89892 -/+ buffers/cache: 88984 165788 Swap: 524280 65116 459164 top命令 top命令。显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等 前五行是当前系统情况整体的统计信息区。 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下: 00:12:54 — 当前系统时间 up ？days, 4:49 — 系统已经运行了？天4小时49分钟（在这期间系统没有重启过） 21users — 当前有1个用户登录系统 load average: 0.06, 0.02, 0.00 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 第二行，Tasks — 任务（进程），具体信息说明如下: 系统现在共有256个进程，其中处于运行中的有1个，177个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。 第三行，cpu状态信息，具体属性说明如下: 0.2%us — 用户空间占用CPU的百分比。 0.2% sy — 内核空间占用CPU的百分比。 0.0% ni — 改变过优先级的进程占用CPU的百分比 99.5% id — 空闲CPU百分比 0.0% wa — IO等待占用CPU的百分比 0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比 0.0% si — 软中断（Software Interrupts）占用CPU的百分比 第四行，内存状态，具体信息如下: 2017552 total — 物理内存总量 720188 used — 使用中的内存总量 197916 free — 空闲内存总量 1099448 cached — 缓存的总量 第五行，swap交换分区信息，具体信息说明如下: 998396 total — 交换区总量 989936 free — 空闲交换区总量 8460 used — 使用的交换区总量 1044136 cached — 缓冲的交换区总量 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:13","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.7 free命令有哪些选项？ Linux free命令用于显示内存状态。 free指令会显示内存的使用情况，包括实体内存，虚拟的交换文件内存，共享内存区段，以及系统核心使用的缓冲区等。 参数如下: -b 以Byte为单位显示内存使用情况。 -k 以KB为单位显示内存使用情况。 -m 以MB为单位显示内存使用情况。 -h 以合适的单位显示内存使用情况，最大为三位数，自动计算对应的单位值。单位有: B = bytes K = kilos M = megas G = gigas T = teras -o 不显示缓冲区调节列。 -s\u003c间隔秒数\u003e 持续观察内存使用状况。 -t 显示内存总和列。 -V 显示版本信息。 答案解析 实例：显示内存使用情况 # free //显示内存使用信息 total used free shared buffers cached Mem: 254772 184568 70204 0 5692 89892 -/+ buffers/cache: 88984 165788 Swap: 524280 65116 459164 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:14","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.8 Linux中压缩文件的命令是什么？ Linux中压缩文件与解压文件的命令有：tar命令、gz命令、bz2命令、compress命令、zip命令、unzip命令。 答案解析 tar 命令详解 Linux tar（英文全拼：tape archive ）命令用于备份文件。 tar 是用来建立，还原备份文件的工具程序，它可以加入，解开备份文件内的文件。 //命令格式: tar [-ABcdgGhiklmMoOpPrRsStuUvwWxzZ][-b \u003c区块数目\u003e][-C \u003c目的目录\u003e][-f \u003c备份文件\u003e][-F \u003cScript文件\u003e][-K \u003c文件\u003e][-L \u003c媒体容量\u003e][-N \u003c日期时间\u003e][-T \u003c范本文件\u003e][-V \u003c卷册名称\u003e][-X \u003c范本文件\u003e][-\u003c设备编号\u003e\u003c存储密度\u003e][--after-date=\u003c日期时间\u003e][--atime-preserve][--backuup=\u003c备份方式\u003e][--checkpoint][--concatenate][--confirmation][--delete][--exclude=\u003c范本样式\u003e][--force-local][--group=\u003c群组名称\u003e][--help][--ignore-failed-read][--new-volume-script=\u003cScript文件\u003e][--newer-mtime][--no-recursion][--null][--numeric-owner][--owner=\u003c用户名称\u003e][--posix][--erve][--preserve-order][--preserve-permissions][--record-size=\u003c区块数目\u003e][--recursive-unlink][--remove-files][--rsh-command=\u003c执行指令\u003e][--same-owner][--suffix=\u003c备份字尾字符串\u003e][--totals][--use-compress-program=\u003c执行指令\u003e][--version][--volno-file=\u003c编号文件\u003e][文件或目录...] //常用参数: //必要参数有如下: -A 新增压缩文件到已存在的压缩 -c 建立新的压缩文件 -d 记录文件的差别 -r 添加文件到已经压缩的文件 -u 添加改变了和现有的文件到已经存在的压缩文件 -x 从压缩的文件中提取文件 -t 显示压缩文件的内容 -z 支持gzip解压文件 -j 支持bzip2解压文件 -Z 支持compress解压文件 -v 显示操作过程 -l 文件系统边界设置 -k 保留原有文件不覆盖 -m 保留文件不被覆盖 -W 确认压缩文件的正确性 //实例 //1.压缩 tar -cf hhh.tar hhh //打包 hhh 文件为 hhh.tar tar -jcf hhh.tar.bz2 hhh //压缩打包 hhh 文件为 hhh.tar.bz2 tar -czf hhh.tar.gz hhh //压缩 hhh 文件为 hhh.tar.gz tar -tzvf test.tar.gz //列出压缩文件内容 //2.解压文件 tar -tzvf test.tar.gz gz命令详解 Linux gzip命令用于压缩文件。 gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多出\".gz\"的扩展名。 //命令格式: gzip [-acdfhlLnNqrtvV][-S \u0026lt;压缩字尾字符串\u0026gt;][-\u0026lt;压缩效率\u0026gt;][--best/fast][文件...] 或 gzip [-acdfhlLnNqrtvV][-S \u0026lt;压缩字尾字符串\u0026gt;][-\u0026lt;压缩效率\u0026gt;][--best/fast][目录] //常用参数: -a或--ascii 使用ASCII文字模式。 -c或--stdout或--to-stdout 把压缩后的文件输出到标准输出设备，不去更动原始文件。 -d或--decompress或----uncompress 解开压缩文件。 -f或--force 强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接。 -h或--help 在线帮助。 -l或--list 列出压缩文件的相关信息。 -L或--license 显示版本与版权信息。 -n或--no-name 压缩文件时，不保存原来的文件名称及时间戳记。 -N或--name 压缩文件时，保存原来的文件名称及时间戳记。 -q或--quiet 不显示警告信息。 -r或--recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -S\u003c压缩字尾字符串\u003e或----suffix\u003c压缩字尾字符串\u003e 更改压缩字尾字符串。 -t或--test 测试压缩文件是否正确无误。 -v或--verbose 显示指令执行过程。 -V或--version 显示版本信息。 -\u003c压缩效率\u003e 压缩效率是一个介于1－9的数值，预设值为\"6\"，指定愈大的数值，压缩效率就会愈高。 --best 此参数的效果和指定\"-9\"参数相同。 --fast 此参数的效果和指定\"-1\"参数相同。 //实例 //1.压缩 gzip * //压缩目录下的所有文件 //2.解压文件 gzip -dv * //解压文件，并列出详细信息 bz2命令详解 bzip2(选项)（参数）:用于创建和管理.bz2格式的压缩包。 //命令格式: bzip2 源文件 //压缩不保留源文件 bzip2 -k 源文件 //压缩保留源文件 //**注意** bzip2 命令不能解压目录 //常用参数: -c 将压缩与解压缩的结果送到标准输出 -d 执行解压缩 -f 在压缩或解压缩时，若输出文件与现有文件名相同，预设不会覆盖现有文件；使用该选项，可覆盖文件 -k 在压缩或解压缩后，会删除原是文件；若要保留原是文件，使用该选项 -v 压缩或解压缩文件时，显示详细的信息 -z 强制执行压缩 //实例 //1.压缩 bzip2 源文件 //压缩不保留源文件 bzip2 -k 源文件 //压缩保留源文件 //2.解压文件 bzip2 -d 源文件 //解压缩 -k 保留压缩文件 bunzip2 源文件 //解压缩 -k 保留压缩文件 compress命令详解 Linux compress命令是一个相当古老的 unix 档案压缩指令，压缩后的档案会加上一个 .Z 延伸档名以区别未压缩的档案，压缩后的档案可以以 uncompress 解压。若要将数个档案压成一个压缩档，必须先将档案 tar 起来再压缩。由于 gzip 可以产生更理想的压缩比例，一般人多已改用 gzip 为档案压缩工具。 //命令格式: compress [-dfvcV] [-b maxbits] [file ...] //常用参数: -c 输出结果至标准输出设备（一般指荧幕） -f 强迫写入档案，若目的档已经存在，则会被覆盖 (force) -v 将程序执行的讯息印在荧幕上 (verbose) -b 设定共同字串数的上限，以位元计算，可以设定的值为 9 至 16 bits 。由于值越大，能使用的共同字串就 越多，压缩比例就越大，所以一般使用预设值 16 bits (bits) -d 将压缩档解压缩 -V 列出版本讯息 //实例 //1.压缩 compress -f source.dat //将 source.dat 压缩成 source.dat.Z ，若 source.dat.Z 已经存在，内容则会被压缩档覆盖。 //2.解压文件 compress -d source.dat //将 source.dat.Z 解压成 source.dat ，若档案已经存在，使用者按 y 以确定覆盖档案，若使用 -df 程序则会自动覆盖档案。 zip 命令详解 //命令格式: zip [-AcdDfFghjJKlLmoqrSTuvVwXyz$][-b \u003c工作目录\u003e][-ll][-n \u003c字尾字符串\u003e][-t \u003c日期时间\u003e][-\u003c压缩效率\u003e][压缩文件][文件...][-i \u003c范本样式\u003e][-x \u003c范本样式\u003e] //常用参数: -m 将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中。 -o 以压缩文件内拥有最新更改时间的文件为准，将压缩文件的更改时间设成和该文件相同。 -q 不显示指令执行过程。 -r 递归处理，将指定目录下的所有文件和子目录一并处理。 -x\u003c范本样式\u003e 压缩时排除符合条件的文件。 //实例: //将 /home/html/ 这个目录下所有文件和文件夹打包为当前目录下的 html.zip： zip -q -r html.zip /home/html //如果在我们在 /home/html 目录下，可以执行以下命令: zip -q -r html.zip * //从压缩文件 cp.zip 中删除文件 a.c zip -dv cp.zip a.c unzip 命令详解 Linux unzip命令用于解压缩zip文件 unzip为.zip压缩文件的解压缩程序。 //命令格式: unzip [-cflptuvz][-agCjLMnoqsVX][-P \u003c密码\u003e][.zip文件][文件][-d \u003c目录\u003e][-x \u003c文件\u003e] 或 unzip [-Z] //常用参数: -c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。 -f 更新现有的文件。 -l 显示压缩文件内所包含的文件。 -p 与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换。 -t 检查压缩文件是否正","date":"2022-06-28 15:30:06","objectID":"/interview/:6:15","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.9 Linux查询连接数的命令是什么？ 参考回答 netstat //示例 查看Web服务器（Nginx Apache）的并发请求数及其TCP连接状态: netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' 解释: 返回结果示例: LAST_ACK 5 (正在等待处理的请求数) SYN_RECV 30 ESTABLISHED 1597 (正常数据传输状态) FIN_WAIT1 51 FIN_WAIT2 504 TIME_WAIT 1057 (处理完毕，等待超时结束的请求数) 状态:描述 CLOSED：无连接是活动的或正在进行 LISTEN：服务器在等待进入呼叫 SYN_RECV：一个连接请求已经到达，等待确认 SYN_SENT：应用已经开始，打开一个连接 ESTABLISHED：正常数据传输状态 FIN_WAIT1：应用说它已经完成 FIN_WAIT2：另一边已同意释放 ITMED_WAIT：等待所有分组死掉 CLOSING：两边同时尝试关闭 TIME_WAIT：另一边已初始化一个释放 LAST_ACK：等待所有分组死掉 答案解析 无。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:16","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.10 Linux中top命令有哪些参数？ 参考回答 top命令。显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等 参数: -d 指定每两次屏幕信息刷新之间的时间间隔。当然用户可以使用s交互命令来改变之。 -p 通过指定监控进程ID来仅仅监控某个进程的状态。 -q 该选项将使top没有任何延迟的进行刷新。如果调用程序有超级用户权限，那么top将以尽可能高的优先级运行。 -S 指定累计模式 -s 使top命令在安全模式中运行。这将去除交互命令所带来的潜在危险。 -i 使top不显示任何闲置或者僵死进程。 -c 显示整个命令行而不只是显示命令名 答案解析 前五行是当前系统情况整体的统计信息区。 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下: 00:12:54 — 当前系统时间 up ？days, 4:49 — 系统已经运行了？天4小时49分钟（在这期间系统没有重启过） 21users — 当前有1个用户登录系统 load average: 0.06, 0.02, 0.00 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 第二行，Tasks — 任务（进程），具体信息说明如下: 系统现在共有256个进程，其中处于运行中的有1个，177个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。 第三行，cpu状态信息，具体属性说明如下: 0.2%us — 用户空间占用CPU的百分比。 0.2% sy — 内核空间占用CPU的百分比。 0.0% ni — 改变过优先级的进程占用CPU的百分比 99.5% id — 空闲CPU百分比 0.0% wa — IO等待占用CPU的百分比 0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比 0.0% si — 软中断（Software Interrupts）占用CPU的百分比 第四行，内存状态，具体信息如下: 2017552 total — 物理内存总量 720188 used — 使用中的内存总量 197916 free — 空闲内存总量 1099448 cached — 缓存的总量 第五行，swap交换分区信息，具体信息说明如下: 998396 total — 交换区总量 989936 free — 空闲交换区总量 8460 used — 使用的交换区总量 1044136 cached — 缓冲的交换区总量 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:17","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.11 Linux中，如何通过端口查进程，如何通过进程查端口？ 参考回答 linux下通过进程名查看其占用端口: （1）先查看进程pid ps -ef | grep 进程名 （2）通过pid查看占用端口 netstat -nap | grep 进程pid linux通过端口查看进程: netstat -nap | grep 端口号 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:18","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.12 请你说说ping命令？ 参考回答 Linux ping命令用于检测主机。 执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 答案解析 语法: ping [-dfnqrRv][-c\u003c完成次数\u003e][-i\u003c间隔秒数\u003e][-I\u003c网络界面\u003e][-l\u003c前置载入\u003e][-p\u003c范本样式\u003e][-s\u003c数据包大小\u003e][-t\u003c存活数值\u003e][主机名称或IP地址] 参数说明: -d 使用Socket的SO_DEBUG功能。 -c\u003c完成次数\u003e 设置完成要求回应的次数。 -f 极限检测。 -i\u003c间隔秒数\u003e 指定收发信息的间隔时间。 -I\u003c网络界面\u003e 使用指定的网络接口送出数据包。 -l\u003c前置载入\u003e 设置在送出要求信息之前，先行发出的数据包。 -n 只输出数值。 -p\u003c范本样式\u003e 设置填满数据包的范本样式。 -q 不显示指令执行过程，开头和结尾的相关信息除外。 -r 忽略普通的Routing Table，直接将数据包送到远端主机上。 -R 记录路由过程。 -s\u003c数据包大小\u003e 设置数据包的大小。 -t\u003c存活数值\u003e 设置存活数值TTL的大小。 -v 详细显示指令的执行过程。 实例: 检测是否与主机连通 # ping www.w3cschool.cc //ping主机 PING aries.m.alikunlun.com (114.80.174.110) 56(84) bytes of data. 64 bytes from 114.80.174.110: icmp_seq=1 ttl=64 time=0.025 ms 64 bytes from 114.80.174.110: icmp_seq=2 ttl=64 time=0.036 ms 64 bytes from 114.80.174.110: icmp_seq=3 ttl=64 time=0.034 ms 64 bytes from 114.80.174.110: icmp_seq=4 ttl=64 time=0.034 ms 64 bytes from 114.80.174.110: icmp_seq=5 ttl=64 time=0.028 ms 64 bytes from 114.80.174.110: icmp_seq=6 ttl=64 time=0.028 ms 64 bytes from 114.80.174.110: icmp_seq=7 ttl=64 time=0.034 ms 64 bytes from 114.80.174.110: icmp_seq=8 ttl=64 time=0.034 ms 64 bytes from 114.80.174.110: icmp_seq=9 ttl=64 time=0.036 ms 64 bytes from 114.80.174.110: icmp_seq=10 ttl=64 time=0.041 ms --- aries.m.alikunlun.com ping statistics --- 10 packets transmitted, 30 received, 0% packet loss, time 29246ms rtt min/avg/max/mdev = 0.021/0.035/0.078/0.011 ms //需要手动终止Ctrl+C 指定接收包的次数 # ping -c 2 www.w3cschool.cc PING aries.m.alikunlun.com (114.80.174.120) 56(84) bytes of data. 64 bytes from 114.80.174.120: icmp_seq=1 ttl=54 time=6.18 ms 64 bytes from 114.80.174.120: icmp_seq=2 ttl=54 time=15.4 ms --- aries.m.alikunlun.com ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1016ms rtt min/avg/max/mdev = 6.185/10.824/15.464/4.640 ms //收到两次包后，自动退出 多参数使用 # ping -i 3 -s 1024 -t 255 g.cn //ping主机 PING g.cn (203.208.37.104) 1024(1052) bytes of data. 1032 bytes from bg-in-f104.1e100.net (203.208.37.104): icmp_seq=0 ttl=243 time=62.5 ms 1032 bytes from bg-in-f104.1e100.net (203.208.37.104): icmp_seq=1 ttl=243 time=63.9 ms 1032 bytes from bg-in-f104.1e100.net (203.208.37.104): icmp_seq=2 ttl=243 time=61.9 ms --- g.cn ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 6001ms rtt min/avg/max/mdev = 61.959/62.843/63.984/0.894 ms, pipe 2 [root@linux ~]# //-i 3 发送周期为 3秒 -s 设置发送包的大小 -t 设置TTL值为 255 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:19","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.13 什么是协程？ 参考回答 协程：协程是微线程，在子程序内部执行，可在子程序内部中断，转而执行别的子程序，在适当的时候再返回来接着执行。 答案解析 线程与协程的区别: （1）协程执行效率极高。协程直接操作栈基本没有内核切换的开销，所以上下文的切换非常快，切换开销比线程更小。 （2）协程不需要多线程的锁机制，因为多个协程从属于一个线程，不存在同时写变量冲突，效率比线程高。 （3）一个线程可以有多个协程。 协程的优势: （1）协程调用跟切换比线程效率高:协程执行效率极高。协程不需要多线程的锁机制，可以不加锁的访问全局变量，所以上下文的切换非常快。 （2）协程占用内存少:执行协程只需要极少的栈内存（大概是4～5KB），而默认情况下，线程栈的大小为1MB。 （3）切换开销更少:协程直接操作栈基本没有内核切换的开销，所以切换开销比线程少。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:20","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1.14 为什么协程比线程切换的开销小？ 参考回答 协程执行效率极高。协程直接操作栈基本没有内核切换的开销，所以上下文的切换非常快，切换开销比线程更小。 协程不需要多线程的锁机制，因为多个协程从属于一个线程，不存在同时写变量冲突，效率比线程高。避免了加锁解锁的开销。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:6:21","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"计网 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:0","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"1. OSI与TCP/IP 模型 OSI七层：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层 TCP/IP五层：物理层、数据链路层、网络层、传输层、应用层 七层网络体系结构各层的主要功能： 应用层：为应用程序提供交互服务。在互联网中的应用层协议很多，如域名系统DNS， 支持万维网应用的HTTP协议，支持电子邮件的SMTP协议等。 表示层：主要负责数据格式的转换，如加密解密、转换翻译、压缩解压缩等。 会话层：负责在网络中的两节点之间建立、维持和终止通信，如服务器验证用户登录便是由会话层完成的。 运输层：有时也译为传输层，向主机进程提供通用的数据传输服务。该层主要有以下两种协议: TCP：提供面向连接的、可靠的数据传输服务; UDP：提供无连接的、尽最大努力的数据传输服务，但不保证数据传输的可靠性。 网络层：选择合适的路由和交换结点，确保数据及时传送。主要包括IP协议。 数据链路层：数据链路层通常简称为链路层。将网络层传下来的IP数据包组装成帧，并再相邻节点的链路上传送帧。 物理层：实现相邻节点间比特流的透明传输，尽可能屏蔽传输介质和通信手段的差异。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:1","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"2. 常见网络服务分层 应用层：HTTP、DNS、FTP、SMTP 传输层：TCP 、UDP 网络层：IP、ICMP 、路由器、防火墙 数据链路层：网卡、网桥、交换机 物理层：中继器、集线器 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:2","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"3. TCP三次握手 三次握手过程： 客户端——发送带有SYN标志的数据包——服务端 一次握手 客户端进入syn_sent状态 服务端——发送带有SYN/ACK标志的数据包——客户端 二次握手 服务端进入syn_rcvd 客户端——发送带有ACK标志的数据包——服务端 三次握手 连接就进入Established状态 为什么三次： 主要是为了建立可靠的通信信道，保证客户端与服务端同时具备发送、接收数据的能力。 为什么两次不行？ 防止已失效的请求报文又传送到了服务端，建立了多余的链接，浪费资源。 两次握手只能保证单向连接是畅通的。（为了实现可靠数据传输， TCP 协议的通信双方， 都必须维护一个序列号， 以标识发送出去的数据包中， 哪些是已经被对方收到的。三次握手的过程即是通信双方 相互告知序列号起始值， 并确认对方已经收到了序列号起始值的必经步骤；如果只是两次握手， 至多只有连接发起方的起始序列号能被确认， 另一方选择的序列号则得不到确认）。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:3","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"4. 四次挥手过程： 客户端——发送带有FIN标志的数据包——服务端，关闭与服务端的连接 ，客户端进入FIN-WAIT-1状态 服务端收到这个 FIN，它发回⼀个 ACK，确认序号为收到的序号加1，服务端就进入了CLOSE-WAIT状态 服务端——发送⼀个FIN数据包——客户端，关闭与客户端的连接，客户端就进入FIN-WAIT-2状态 客户端收到这个 FIN，发回 ACK 报⽂确认，并将确认序号设置为收到序号加1，客户端进入TIME-WAIT状态 **为什么四次：**因为需要确保客户端与服务端的数据能够完成传输。 CLOSE-WAIT： 这种状态的含义其实是表示在等待关闭。 TIME-WAIT： 为了解决网络的丢包和网络不稳定所带来的其他问题，确保连接方能在时间范围内，关闭自己的连接。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:4","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"5. 为什么连接的时候是三次握手，关闭的时候却是四次握手? 服务器在收到客户端的FIN报文段后，可能还有一些数据要传输，所以不能马上关闭连接，但是会做出应答，返回ACK报文段。 接下来可能会继续发送数据，在数据发送完后，服务器会向客户端发送FIN报文，表示数据已经发送完毕，请求关闭连接。服务器的ACK和FIN一般都会分开发送，从而导致多了一次，因此一共需要四次挥手。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:5","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"6. 如何查看TIME-WAIT状态的链接数量？ netstat -an | grep TIME_WAIT | wc -l //查看连接数等待time_wait状态连接数 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:6","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"7. 为什么会TIME-WAIT过多？解决方法是怎样的？ **可能原因：**高并发短连接的TCP服务器上，当服务器处理完请求后立刻按照主动正常关闭连接 **解决：**负载均衡服务器；Web服务器首先关闭来自负载均衡服务器的连接 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:7","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"8. 半连接，洪泛攻击问题以及如何解决（syn_cookie） 在三次握手的过程中，服务器为了响应一个受到的SYN报文段，会分配并初始化连接变量和缓存，然后服务器发送一个SYN/ACK报文段进行响应，并等待客户端的ACK报文段。如果客户不发送ACK来完成该三次握手的第三步，最终(通常在一分多钟之后)服务器将终止该半开连接并回收资源。这种TCP连接管理协议的特性就会有这样一个漏洞，攻击者发送大量的TCP SYN报文段，而不完成第三次握手的步骤。随着这种SYN报文段的不断到来，服务器不断为这些半开连接分配资源，从而导致服务器连接资源被消耗殆尽。这种攻击就是SYN泛供攻击。 为了应对这种攻击，现在有一种有效的防御系统，称为SYN cookie。SYN cookie的工作方式如下： 当服务器接收到一个SYN报文段时，它并不知道该报文段是来自一个合法的用户，还是这种SYN洪泛攻击的一部分。因为服务器不会为该报文段生成一个半开的连接。相反，服务器生成一个初始TCP序列号，该序列号是SYN报文段的源IP地址和目的IP地址，源端口号和目的端口号以及仅有服务器知道的秘密数的复杂函数(散列函数)。这种精心制作的初始序列号称为为“cookie”。服务器则发送具有这种特殊初始序号的SYN/ACK报文分组。服务器并不记忆该cookie或任何对应于SYN的其他状态信息。 如果该客户是合法的，则它将返回一个ACK报文段。当服务器收到该ACK报文段，需要验证该ACK是与前面发送的某个SYN相对应。由于服务器并不维护有关SYN报文段的记忆，所以服务器通过使用SYN/ACK报文段中的源和目的IP地址与端口号以及秘密数运行相同的散列函数。如果这个函数的结果(cookie值)加1和在客户的ACK报文段中的确认值相同的话，那么服务器就会认为该ACK对应于较早的SYN报文段，因此它是合法的。服务器则会生成一个套接字的全开连接。 另一方面，如果客户没有返回一个ACK报文段，说明之前的SYN报文段是洪泛攻击的一部分，但是它并没有对服务器产生危害，因为服务器没有为它分配任何资源。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:8","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"9. 为什么客户端的TIME-WAIT状态必须等待2MSL ? 主要有两个原因: 为了保证客户端发送的最后一个ACK报文段能够达到服务器。 这个ACK报文段可能丢失，因而使处在LAST-ACK状态的服务器收不到确认。服务器会超时重传FIN+ACK报文段，客户端就能在2MSL时间内收到这个重传的FIN+ACK报文段，接着客户端重传一次确认，重启计时器。最好，客户端和服务器都正常进入到CLOSED状态。如果客户端在TIME-WAIT状态不等待一段时间，而是再发送完ACK报文后立即释放连接，那么就无法收到服务器重传的FIN+ACK报文段，因而也不会再发送一次确认报文。这样，服务器就无法按照正常步骤进入CLOSED状态。 防止已失效的连接请求报文段出现在本连接中。 客户端在发送完最后一个ACK确认报文段后，再经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:9","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"10. 4g切换wifi会发生什么 当移动设备的网络从4G切换到WiFi时，意味着IP地址变化了，那么必须要断开连接，然后重新连接，而建立连接的过程包含TCP三次握手和TLS四次挥手的时延，以及TCP慢启动的减速过程，给用户的感觉就是突然网络卡顿了一下，所以说，迁移的成本是很高的。 http3是怎么解决连接迁移 HTTP3中QUIC协议没有用四元组的方式来\"绑定”连接，而是通过连接ID来标记通信的两个端点，客户端和服务器可以各自选择一组ID来标记自己，因此即使移动设备的网络变化后，导致IP地址变化了，只要仍保有上下文信息(比如连接ID、TLS 密钥等)，就可以\"无缝\"地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了连接迁移的功能。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:10","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"11. TCP与UDP区别及场景 类型 特点 性能 应用过场景 首部字节 TCP 面向连接、可靠、字节流 传输效率慢、所需资源多 文件、邮件传输 20-60 UDP 无连接、不可靠、数据报文段 传输效率快、所需资源少 语音、视频、直播 8个字节 基于TCP的协议：HTTP、FTP、SMTP 基于UDP的协议： RIP、DNS、SNMP ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:11","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"12. TCP滑动窗口，拥塞控制 **TCP通过：**应用数据分割、对数据包进行编号、校验和、流量控制、拥塞控制、超时重传等措施保证数据的可靠传输。 **拥塞控制目的：**为了防止过多的数据注入到网络中，避免网络中的路由器、链路过载。 **拥塞控制过程：**TCP维护一个拥塞窗口，该窗口随着网络拥塞程度动态变化，通过慢开始、拥塞避免等算法减少网络拥塞的发生。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:12","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"13. TCP粘包原因和解决方法 TCP粘包是指：发送方发送的若干包数据到接收方接收时粘成一包 发送方原因： TCP默认使用Nagle算法（主要作用：减少网络中报文段的数量）： 收集多个小分组，在一个确认到来时一起发送、导致发送方可能会出现粘包问题 接收方原因： TCP将接收到的数据包保存在接收缓存里，如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。 传输层的UDP协议不会发生粘包或者拆包问题 UDP是基于报文发送的，在UDP首部采用了16bit来指示UDP数据报文的长度，因此在应用层能很好的将不同的数据报文区分开，从而避免粘包和拆包的问题。 传输层的TCP协议会发生粘包或者拆包问题 原因有以下两点： TCP是基于字节流的，虽然应用层和传输层之间的数据交互是大小不等的数据块，但是TCP把这些数据块仅仅看成一连串无结构的字节流，没有边界； 在TCP的首部没有表示数据长度的字段，基于上面两点，在使用TCP传输数据时，才有粘包或者拆包现象发生的可能。 **解决粘包问题：**解决问题的关键在于如何给每个数据包添加边界信息 最本质原因在与接收对等方无法分辨消息与消息之间的边界在哪，通过使用某种方案给出边界，例如： 发送定长包。每个消息的大小都是一样的，接收方只要累计接收数据，直到数据等于一个定长的数值就将它作为一个消息。 包尾加上\\r\\n标记。FTP协议正是这么做的。但问题在于如果数据正文中也含有\\r\\n，则会误判为消息的边界。 包头加上包体长度。包头是定长的4个字节，说明了包体的长度。接收对等方先接收包体长度，依据包体长度来接收包体。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:13","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"14. TCP、UDP报文格式 TCP报文格式： 源端口号和目的端口号： 用于寻找发端和收端应用进程。这两个值加上IP首部源端IP地址和目的端IP地址唯一确定一个TCP连接。 序号字段： 序号用来标识从TCP发端向TCP收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节。如果将字节流看作在两个应用程序间的单向流动，则 TCP用序号对每个字节进行计数。序号是32 bit的无符号数，序号到达 2^32-1后又从0开始。 当建立一个新的连接时，SYN标志变1。序号字段包含由这个主机选择的该连接的初始序号ISN（Initial Sequence Number）。该主机要发送数据的第一个字节序号为这个ISN加1，因为SYN标志消耗了一个序号 确认序号： 既然每个传输的字节都被计数，确认序号包含发送确认的一端所期望收到的下一个序号。因此，确认序号应当是上次已成功收到数据字节序号加 1。只有ACK标志为 1时确认序号字段才有效。发送ACK无需任何代价，因为 32 bit的确认序号字段和A C K标志一样，总是TCP首部的一部分。因此，我们看到一旦一个连接建立起来，这个字段总是被设置， ACK标志也总是被设置为1。TCP为应用层提供全双工服务。这意味数据能在两个方向上独立地进行传输。因此，连接的每一端必须保持每个方向上的传输数据序号。 首都长度： 首部长度给出首部中 32 bit字的数目。需要这个值是因为任选字段的长度是可变的。这个字段占4 bit，因此TCP最多有6 0字节的首部。然而，没有任选字段，正常的长度是 20字节。 标志字段：在TCP首部中有 6个标志比特，它们中的多个可同时被设置为1。 URG紧急指针（u rgent pointer）有效 ACK确认序号有效。 PSH接收方应该尽快将这个报文段交给应用层。 RST重建连接。 SYN同步序号用来发起一个连接。这个标志和下一个标志将在第 1 8章介绍。 FIN发端完成发送任务。 窗口大小： TCP的流量控制由连接的每一端通过声明的窗口大小来提供。窗口大小为字节数，起始于确认序号字段指明的值，这个值是接收端期望接收的字节。窗口大小是一个 16 bit字段，因而窗口大小最大为 65535字节。 检验和： 检验和覆盖了整个的 TCP报文段：TCP首部和TCP数据。这是一个强制性的字段，一定是由发端计算和存储，并由收端进行验证。 紧急指针： 只有当URG标志置1时紧急指针才有效。紧急指针是一个正的偏移量，和序号字段中的值相加表示紧急数据最后一个字节的序号。TCP的紧急方式是发送端向另一端发送紧急数据的一种方式。 选项： 最常见的可选字段是最长报文大小，又称为 MSS (Maximum Segment Size)。每个连接方通常都在通信的第一个报文段（为建立连接而设置 SYN标志的那个段）中指明这个选项。它指明本端所能接收的最大长度的报文段。 UDP报文格式： 端口号： 用来表示发送和接受进程。由于 IP层已经把I P数据报分配给TCP或 UDP（根据I P首部中协议字段值），因此TCP端口号由TCP来查看，而 UDP端口号由 UDP来查看。TCP端口号与 UDP端口号是相互独立的。 长度： UDP长度字段指的是 UDP首部和 UDP数据的字节长度。该字段的最小值为 8字节（发送一份0字节的 UDP数据报是 O K）。 检验和： UDP检验和是一个端到端的检验和。它由发送端计算，然后由接收端验证。其目的是为了发现 UDP首部和数据在发送端到接收端之间发生的任何改动。 **IP报文格式：**普通的IP首部长为20个字节，除非含有可选项字段。 4位版本：目前协议版本号是4，因此IP有时也称作IPV4. 4位首部长度： 首部长度指的是首部占32bit字的数目，包括任何选项。由于它是一个4比特字段，因此首部长度最长为60个字节。 服务类型（TOS）： 服务类型字段包括一个3bit的优先权字段（现在已经被忽略），4bit的TOS子字段和1bit未用位必须置0。4bit的TOS分别代表：最小时延，最大吞吐量，最高可靠性和最小费用。4bit中只能置其中1比特。如果所有4bit均为0，那么就意味着是一般服务。 总长度： 总长度字段是指整个IP数据报的长度，以字节为单位。利用首部长度和总长度字段，就可以知道IP数据报中数据内容的起始位置和长度。由于该字段长16bit，所以IP数据报最长可达65535字节。当数据报被分片时，该字段的值也随着变化。 标识字段： 标识字段唯一地标识主机发送的每一份数据报。通常每发送一份报文它的值就会加1。 生存时间： TTL（time-to-live）生存时间字段设置了数据报可以经过的最多路由器数。它指定了数据报的生存时间。TTL的初始值由源主机设置（通常为 3 2或6 4），一旦经过一个处理它的路由器，它的值就减去 1。当该字段的值为 0时，数据报就被丢弃，并发送 ICMP 报文通知源主机。 首部检验和： 首部检验和字段是根据 I P首部计算的检验和码。它不对首部后面的数据进行计算。ICMP、IGMP、UDP和TCP在它们各自的首部中均含有同时覆盖首部和数据检验和码。 以太网报文格式： 目的地址和源地址： 是指网卡的硬件地址（也叫MAC 地址），长度是48 位，是在网卡出厂时固化的。 数据： 以太网帧中的数据长度规定最小46 字节，最大1500 字节，ARP 和RARP 数据包的长度不够46 字节，要在后面补填充位。最大值1500 称为以太网的最大传输单元（MTU），不同的网络类型有不同的MTU，如果一个数据包从以太网路由到拨号链路上，数据包度大于拨号链路的MTU了，则需要对数据包进行分片fragmentation）。ifconfig 命令的输出中也有“MTU:1500”。注意，MTU个概念指数据帧中有效载荷的最大长度，不包括帧首部的长度。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:14","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"15. TCP协议如何保证可靠传输机制？ TCP主要提供了检验和、序列号/确认应答、超时重传、滑动窗口、拥塞控制和流量控制等方法实现了可靠性传输。 **检验和：**通过检验和的方式，接收端可以检测出来数据是否有差错和异常，假如有差错就会直接丢弃TCP段，重新发送。 序列号/确认应答： 序列号的作用不仅仅是应答的作用，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据。 TCP传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送ACK报文,这个ACK报文当中带有对应的确认序列号，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。 **超时重传：**超时重传是指发送出去的数据包到接收到确认包之间的时间，如果超过了这个时间会被认为是丢包了，需要重传。最大超时时间是动态计算的。 **滑动窗口：**滑动窗口既提高了报文传输的效率，也避免了发送方发送过多的数据而导致接收方无法正常处理的异常。 **拥塞控制：**在数据传输过程中，可能由于网络状态的问题，造成网络拥堵，此时引入拥塞控制机制，在保证TCP可靠性的同时，提高性能。 **流量控制：**如果主机A一直向主机B发送数据，不考虑主机B的接受能力，则可能导致主机B的接受缓冲区满了而无法再接受数据，从而会导致大量的数据丢包，引发重传机制。而在重传的过程中，若主机B的接收缓冲区情况仍未好转，则会将大量的时间浪费在重传数据上，降低传送数据的效率。所以引入流量控制机制，主机B通过告诉主机A自己接收缓冲区的大小，来使主机A控制发送的数据量。流量控制与TCP协议报头中的窗口大小有关。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:15","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"16. TCP的滑动窗口? 在进行数据传输时，如果传输的数据比较大，就需要拆分为多个数据包进行发送。TCP协议需要对数据进行确认后，才可以发送下一个数据包。这样一来，就会在等待确认应答包环节浪费时间。 为了避免这种情况，TCP引入了窗口概念。窗口大小指的是不需要等待确认应答包而可以继续发送数据包的最大值。 从上面的图可以看到滑动窗口左边的是已发送并且被确认的分组，滑动窗口右边是还没有轮到的分组。 滑动窗口里面也分为两块，一块是已经发送但是未被确认的分组，另一块是窗口内等待发送的分组。随着已发送的分组不断被确认，窗口内等待发送的分组也会不断被发送。整个窗口就会往右移动，让还没轮到的分组进入窗口内。 可以看到滑动窗口起到了一个限流的作用，也就是说当前滑动窗口的大小决定了当前TCP发送包的速率，而滑动窗口的大小取决于拥塞控制窗口和流量控制窗口的两者间的最小值。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:16","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"17. 详细讲一下拥塞控制? 为何要进行拥塞控制？ A在给B传输数据, A却没有收到B反馈的TCP,A就认为B发送的数据包丢失了..进而会重新传输这个丢失的数据包。然而实际情况有可能此时有太多主机正在使用信道资源，导致网络拥塞了。重传数据浪费了资源，所以要进行拥塞控制。发送发不知道一次发多少数据合适，所以设置一个拥塞窗口。 TCP拥塞控制原理是通过：慢启动、拥塞避免、快重传、快启动 发送方维持一个叫做拥塞窗口cwnd (congestion window)的状态变量。当cwndssthresh时， 改用拥塞避免算法。 慢开始: 不要一开始就发送大量的数据，由小到大逐渐增加拥塞窗口的大小。 拥塞避免: 拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1而不是加倍。这样拥塞窗口按线性规律缓慢增长。 **快重传: **我们可以剔除一些不必要的拥塞报文，提高网络吞吐量。比如接收方在收到一个失序的报文段后就立即发出重复确认，而不要等到自己发送数据时捎带确认。快重传规定:发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 **快恢复: **主要是配合快重传。当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半(为了预防网络发生拥塞)，但接下来并不执行慢开始算法，因为如果网络出现拥塞的话就不会收到好几个重复的确认，收到三个重复确认说明网络状况还可以。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:17","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"18. TCP拥塞控制4种算法 基于丢包的拥塞控制：将丢包视为出现拥塞，采取缓慢探测的方式，逐渐增大拥塞窗口，当出现丢包时，将拥塞窗口减小，如Reno、Cubic等。 基于时延的拥塞控制：将时延增加视为出现拥塞，延时增加时增大拥塞窗口，延时减小时减小拥塞窗口，如Vegas、FastTCP等。 基于链路容量的拥塞控制：实时测量网络带宽和时延，认为网络上报文总量大于带宽时延乘积时出现了拥塞，如BBR。 基于学习的拥塞控制：没有特定的拥塞信号，而是借助评价函数，基于训练数据，使用机器学习的方法形成一个控制策略，如Remy。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:18","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"HTTP协议 1. HTTP协议1.0 1.1 2.0 **HTTP1.0：**服务器处理完成后立即断开TCP连接（无连接），服务器不跟踪每个客户端也不记录过去的请求（无状态） HTTP1.1： KeepAlived 长连接避免了连接建立和释放的开销；通过Content-Length来判断当前请求数据是否已经全部接受（有状态） HTTP2.0：引入二进制数据帧和流的概念，其中帧对数据进行顺序标识；因为有了序列，服务器可以并行的传输数据。 无状态的好坏 无状态的好处，因为服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减 轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。 无状态的坏处，既然服务器没有记忆能力，这样每操作一次，都要验证信息。例如登录-\u003e添加购物车-\u003e下单-\u003e结算-\u003e支付，这系列操作都要知道用户的身份才行。但服务器不知道这些请求是有关联的，每次都要问一遍身份信息。 解决无状态的问题，解法方案有很多种，其中比较简单的方式用 Cookie 和Session技术。 2. HTTP1.0和HTTP1.1的主要区别如下： 缓存处理：1.1添加更多的缓存控制策略（如：Entity tag，If-Match） 网络连接的优化：1.1支持断点续传 错误状态码的增多：1.1新增了24个错误状态响应码，丰富的错误码更加明确各个状态 Host头处理：支持Host头域，不在以IP为请求方标志 长连接：减少了建立和关闭连接的消耗和延迟。 3. HTTP1.1和HTTP2.0的主要区别： 新的传输格式：2.0使用二进制格式，1.0依然使用基于文本格式 多路复用：连接共享，不同的request可以使用同一个连接传输（最后根据每个request上的id号组合成正常的请求） header压缩：由于1.X中header带有大量的信息，并且得重复传输，2.0使用encoder来减少需要传输的hearder大小 服务端推送：同google的SPDUY（1.0的一种升级）一样 4. HTTP和 HTTPS的区别： 最重要的区别就是安全性，HTTP 明文传输，不对数据进行加密安全性较差。HTTPS (HTTP + SSL / TLS)的数据传输过程是加密的，安全性较好。 证书：使用 HTTPS 协议需要申请 CA 证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo 、DigiCert 和 GlobalSign 等。 响应速度：HTTP 页面响应速度比 HTTPS 快，这个很好理解，由于加了一层安全层，建立连接的过程更复杂，也要交换更多的数据，难免影响速度。 加密：HTTP协议运行在TCP（三次握手）之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。 端口不同：HTTPS 和 HTTP 使用的是完全不同的连接方式，用的端口也不一样，前者是 443，后者是 80。 HTTP HTTPS 默认端口80 默认端口443 明文传输、数据未加密、安全性较差 传输协议ssl加密、安全性较好 响应速度快，消耗资源少 响应速度慢、消耗资源多，需要用到CA证书 5. HTTPS 的缺点： 在相同网络环境中，HTTPS 相比 HTTP 无论是响应时间还是耗电量都有大幅度上升。 HTTPS 的安全是有范围的，在黑客攻击、服务器劫持等情况下几乎起不到作用。 在现有的证书机制下，中间人攻击依然有可能发生。 HTTPS 需要更多的服务器资源，也会导致成本的升高。 6. HTTPS链接建立的过程： 首先客户端先给服务器发送一个请求 服务器发送一个SSL证书给客户端，内容包括：证书的发布机构、有效期、所有者、签名以及公钥 客户端对发来的公钥进行真伪校验，校验为真则使用公钥对对称加密算法以及对称密钥进行加密 服务器端使用私钥进行解密并使用对称密钥加密确认信息发送给客户端 随后客户端和服务端就使用对称密钥进行信息传输 加密流程按图中的序号分为: 客户端请求HTTPS网址，然后连接到server的443端口(HTTPS默认端口，类似于HTTP的80端口)。 采用HTTPS协议的服务器必须要有一套数字CA (Certification Authority)证书。颁发证书的同时会产生一个私钥和公钥。私钥由服务端自己保存，不可泄漏。公钥则是附带在证书的信息中，可以公开的。证书本身也附带-一个证书电子签名，这个签名用来验证证书的完整性和真实性，可以防止证书被篡改。 服务器响应客户端请求，将证书传递给客户端，证书包含公钥和大量其他信息，比如证书颁发机构信息，公司信息和证书有效期等。 客户端解析证书并对其进行验证。如果证书不是可信机构颁布，或者证书中的域名与实际域名不一致，或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。如果证书没有问题，客户端就会从服务器证书中取出服务器的公钥A。然后客户端还会生成一一个随机码KEY,并使用公钥A将其加密。 客户端把加密后的随机码KEY发送给服务器，作为后面对称加密的密钥。 服务器在收到随机码KEY之后会使用私钥B将其解密。经过以上这些步骤，客户端和服务器终于建立了安全连接，完美解决了对称加密的密钥泄露问题，接下来就可以用对称加密愉快地进行通信了。 服务器使用密钥(随机码KEY)对数据进行对称加密并发送给客户端，客户端使用相同的密钥(随机码KEY)解密数据。 双方使用对称加密愉快地传输所有数据。 7. HTTP常见响应状态码 100：Continue 一一 继续。客户端应继续其请求。 200：OK 一一 请求成功。一 般用于GET与POST请求。 301：Moved Permanently 一一 永久重定向。 302：Found 一一 暂时重定向。 400：Bad Request 一一 客户端请求的语法错误，服务器无法理解。请求没有包含host头 403：Forbideen 一一 服务器理解请求客户端的请求，但是拒绝执行此请求。禁止客户访问该资源 404：Not Found 一一 服务器无法根据客户端的请求找到资源（网页）。资源未找到 500：Internal Server Error 一一 服务器内部错误，无法完成请求。 502：Bad Gateway 一一 作为网关或者代理服务器尝试执行请求时，从远程服务器接收到了无效的响应。 8. 状态码301和302的区别是什么? 301为永久重定向，302为临时重定向 共同点: 301和302状态码都表示重定向，就是说浏览器在拿到服务器返回的这个状态码后会自动跳转到一个新的URL地址，这个地址可以从响应的Location首部中获取(用户看到的效果就是他输入的地址A瞬间变成了另一个地址B)。 不同点: 301表示旧地址A的资源已经被永久地移除了(这个资源不可访问了)，搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址;302表示旧地址A的资源还在(仍然可以访问)，这个重定向只是临时地从旧地址A跳转到地址B，搜索引擎会抓取新的内容而保存旧的网址。SEO中302好于301。 补充，重定向原因: 网站调整(如改变网页目录结构); 网页被移到一个新地址; 网页扩展名改变(如应用需要把.php改成.Html或.shtml)。 9. 对称加密算法与非对称加密算法的区别 对称加密算法： 双方持有相同的密钥，且加密速度快，典型对称加密算法：DES、AES 非对称加密算法： 密钥成对出现（私钥、公钥），私钥只有自己知道，不在网络中传输；而公钥可以公开。相比对称加密速度较慢，典型的非对称加密算法有：AES、DSA 10. HTTP请求方法： 方法 描述 GET 像特定资源发送请求，查询数据，并返回实体 POST 向指定资源提交数据进行处理，可能会导致新的资源建立、已有的资源修改 PUT 向服务器上传新的内容 HEAD 类似GET请求，返回的响应式中没有具体内容，用于获取报头 DELETE 请求服务器删除指定标识的资源 OPTIONS 可以原来向服务器发送请求来测试服务器的功能性 TRACE 回显服务器收到的请求，用于测试和诊断 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器 11. Get和Post请求区别 GET POST HTTP规范 GET用于信息获取 修改服务器上的资源的请求 可见性 数据在URL中对所有人可见 数据不会显示在URL中 安全性 与post相比，get的安全性较差，因为所发送的数据是URL的一部分 安全，因为参数不会被保存在浏览器历史或web服务器日志中 数据长度 受限制，最长2kb 无限制 编码类型 application/x-www-form-urlencoded multipart/form-data 缓存 能被缓存 不能被缓存 12. GET 和 POST 方法都是安全和幂等的吗？ 先说明下安全和幂等的概念： 在 DSA协议里，所谓的「安全」是指请求方法不会「破坏」服务器上的资源。 所谓的「幂等」，意思是多次执行相同的操作，结果都是「相同」的。 那么很明显 GET 方法就是安全且幂等的，因为它是「只读」操作，无论操作多少次，服务器上的数据 都是安全的，且每次的结果都是相同的。 POST 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据 就会创建多个资源，所以不是幂等的。 13. 重定向和转发区别 转发是服务器行为,重定向是客户端行为 重定向：redirect： 地址栏发生变化 重定向可以访问其他站","date":"2022-06-28 15:30:06","objectID":"/interview/:7:19","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"什么是网络编程 网络编程的本质是多台计算机之间的数据交换。数据传递本身没有多大的难度，不就是把一个设备 中的数据发送给其他设备，然后接受另外一个设备反馈的数据。现在的网络编程基本上都是基于请 求/响应方式的，也就是一个设备发送请求数据给另外一个，然后接收另一个设备的反馈。在网络 编程中，发起连接程序，也就是发送第一次请求的程序，被称作客户端(Client)，等待其他程序连 接的程序被称作服务器(Server)。客户端程序可以在需要的时候启动，而服务器为了能够时刻相应 连接，则需要一直启动。 例如以打电话为例，首先拨号的人类似于客户端，接听电话的人必须保持电话畅通类似于服务器。 连接一旦建立以后，就客户端和服务器端就可以进行数据传递了，而且两者的身份是等价的。在一 些程序中，程序既有客户端功能也有服务器端功能，最常见的软件就是QQ、微信这类软件了。 网络编程中两个主要的问题 一个是如何准确的定位网络上一台或多台主机， 另一个就是找到主机后如何可靠高效的进行数据传输。 在TCP/IP协议中IP层主要负责网络主机的定位，数据传输的路由，由IP地址可以唯一地确定 Internet上的一台主机。 而TCP层则提供面向应用的可靠（TCP）的或非可靠（UDP）的数据传输机制，这是网络编程的主 要对象，一般不需要关心IP层是如何处理数据的。 目前较为流行的网络编程模型是客户机/服务器（C/S）结构。即通信双方一方作为服务器等待客户 提出请求并予以响应。客户则在需要服务时向服务器提 出申请。服务器一般作为守护进程始终运 行，监听网络端口，一旦有客户请求，就会启动一个服务进程来响应该客户，同时自己继续监听服 务端口，使后来的客户也 能及时得到服务。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:20","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"网络协议是什么 在计算机网络要做到井井有条的交换数据，就必须遵守一些事先约定好的规则，比如交换数据的格 式、是否需要发送一个应答信息。这些规则被称为网络协议。 为什么要对网络协议分层 简化问题难度和复杂度。由于各层之间独立，我们可以分割大问题为小问题。 灵活性好。当其中一层的技术变化时，只要层间接口关系保持不变，其他层不受影响。 易于实现和维护。 促进标准化工作。分开后，每层功能可以相对简单地被描述 TCP/IP参考模型 TCP/IP四层协议（数据链路层、网络层、传输层、应用层） 应用层 应用层最靠近用户的一层，是为计算机用户提供应用接口，也为用户直接提供各种网 络服务。我们常见应用层的网络服务协议有：HTTP，HTTPS，FTP，TELNET等。 传输层 建立了主机端到端的链接，传输层的作用是为上层协议提供端到端的可靠和透明的数 据传输服务，包括处理差错控制和流量控制等问题。该层向高层屏蔽了下层数据通信的细 节，使高层用户看到的只是在两个传输实体间的一条主机到主机的、可由用户控制和设定 的、可靠的数据通路。我们通常说的，TCP UDP就是在这一层。端口号既是这里的“端”。 网络层 本层通过IP寻址来建立两个节点之间的连接，为源端的运输层送来的分组，选择合适 的路由和交换节点，正确无误地按照地址传送给目的端的运输层。就是通常说的IP层。这一 层就是我们经常说的IP协议层。IP协议是Internet的基础。 数据链路层 通过一些规程或协议来控制这些数据的传输，以保证被传输数据的正确性。实现 这些规程或协议的 硬件 和软件加到物理线路，这样就构成了数据链路， ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:21","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"TCP / UDP 什么是TCP/IP和UDP TCP/IP即传输控制/网络协议，是面向连接的协议，发送数据前要先建立连接(发送方和接收方的成 对的两个之间必须建 立连接)，TCP提供可靠的服务，也就是说，通过TCP连接传输的数据不会丢 失，没有重复，并且按顺序到达 UDP它是属于TCP/IP协议族中的一种。是无连接的协议，发送数据前不需要建立连接，是没有可 靠性的协议。因为不需要建立连接所以可以在在网络上以任何可能的路径传输，因此能否到达目的 地，到达目的地的时间以及内容的正确性都是不能被保证的。 TCP与UDP区别： TCP是面向连接的协议，发送数据前要先建立连接，TCP提供可靠的服务，也就是说，通过TCP连 接传输的数据不会丢失，没有重复，并且按顺序到达； UDP是无连接的协议，发送数据前不需要建立连接，是没有可靠性； TCP通信类似于于要打个电话，接通了，确认身份后，才开始进行通行； UDP通信类似于学校广播，靠着广播播报直接进行通信。 TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多； TCP是面向字节流的，UDP是面向报文的； 面向字节流是指发送数据时以字节为单位，一个数据 包可以拆分成若干组进行发送，而UDP一个报文只能一次发完。 TCP首部开销（20字节）比UDP首部开销（8字节）要大 UDP 的主机不需要维持复杂的连接状态表 TCP和UDP的应用场景 对某些实时性要求比较高的情况使用UDP，比如游戏，媒体通信，实时直播，即使出现传输错误 也可以容忍； 其它大部分情况下，HTTP都是用TCP，因为要求传输的内容可靠，不出现丢失的情 况 形容一下TCP和UDP TCP通信可看作打电话： 李三(拨了个号码)：喂，是王五吗？ 王五：哎，您谁啊？ 李三：我是李三，我想给你说点事儿， 你现在方便吗？ 王五：哦，我现在方便，你说吧。 甲：那我说了啊？ 乙：你说吧。 (连接建立 了，接下来就是说正事了…) UDP通信可看为学校里的广播： 播音室：喂喂喂！全体操场集合 运行在TCP 或UDP的应用层协议分析 运行在TCP协议上的协议： HTTP（Hypertext Transfer Protocol，超文本传输协议），主要用于普通浏览。 HTTPS（HTTP over SSL，安全超文本传输协议）,HTTP协议的安全版本。 FTP（File Transfer Protocol，文件传输协议），用于文件传输。 POP3（Post Office Protocol, version 3，邮局协议），收邮件用。 SMTP（Simple Mail Transfer Protocol，简单邮件传输协议），用来发送电子邮件。 TELNET（Teletype over the Network，网络电传），通过一个终端（terminal）登陆到网 络。 SSH（Secure Shell，用于替代安全性差的TELNET），用于加密安全登陆用。 运行在UDP协议上的协议： BOOTP（Boot Protocol，启动协议），应用于无盘设备。 NTP（Network Time Protocol，网络时间协议），用于网络同步。 DHCP（Dynamic Host Configuration Protocol，动态主机配置协议），动态配置IP地址。 运行在TCP和UDP协议上： DNS（Domain Name Service，域名服务），用于完成地址查找，邮件转发等工作。 ECHO（Echo Protocol，回绕协议），用于查错及测量应答时间（运行在TCP和UDP协议 上）。 SNMP（Simple Network Management Protocol，简单网络管理协议），用于网络信息的 收集和网络管理。 DHCP（Dynamic Host Configuration Protocol，动态主机配置协议），动态配置IP地址。 ARP（Address Resolution Protocol，地址解析协议），用于动态解析以太网硬件的地址。 什么是ARP协议 (Address Resolution Protocol)？ ARP协议完成了IP地址与物理地址的映射。每一个主机都设有一个 ARP 高速缓存，里面有所在的 局域网上的各主机和路由器的 IP 地址到硬件地址的映射表。当源主机要发送数据包到目的主机 时，会先检查自己的ARP高速缓存中有没有目的主机的MAC地址，如果有，就直接将数据包发到这 个MAC地址，如果没有，就向所在的局域网发起一个ARP请求的广播包（在发送自己的 ARP 请求 时，同时会带上自己的 IP 地址到硬件地址的映射），收到请求的主机检查自己的IP地址和目的主 机的IP地址是否一致，如果一致，则先保存源主机的映射到自己的ARP缓存，然后给源主机发送一 个ARP响应数据包。源主机收到响应数据包之后，先添加目的主机的IP地址与MAC地址的映射，再 进行数据传送。如果源主机一直没有收到响应，表示ARP查询失败。 如果所要找的主机和源主机不在同一个局域网上，那么就要通过 ARP 找到一个位于本局域网上的 某个路由器的硬件地址，然后把分组发送给这个路由器，让这个路由器把分组转发给下一个网络。 剩下的工作就由下一个网络来做。 从输入址到获得页面的过程? **过程：**DNS解析、TCP连接、发送HTTP请求、服务器处理请求并返回HTTP报文、浏览器渲染、结束 浏览器查询 DNS，获取域名对应的IP地址:具体过程包括浏览器搜索自身的DNS缓存、搜索操作系 统的DNS缓存、读取本地的Host文件和向本地DNS服务器进行查询等。对于向本地DNS服务器进 行查询，如果要查询的域名包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析 (此解析具有权威性)；如果要查询的域名不由本地DNS服务器区域解析，但该服务器已缓存了此网 址映射关系，则调用这个IP地址映射，完成域名解析（此解析不具有权威性）。如果本地域名服务 器并未缓存该网址映射关系，那么将根据其设置发起递归查询或者迭代查询； 浏览器获得域名对应的IP地址以后，浏览器向服务器请求建立链接，发起三次握手； TCP/IP链接建立起来后，浏览器向服务器发送HTTP请求； 服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应 的视图返回给浏览器； 浏览器解析并渲染视图，若遇到对js文件、css文件及图片等静态资源的引用，则重复上述步骤并 向服务器请求这些资源； 浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:22","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"TCP的三次握手 什么是TCP的三次握手 在网络数据传输中，传输层协议TCP是要建立连接的可靠传输，TCP建立连接的过程，我们称为三 次握手。 第一次握手：Client将SYN置1，随机产生一个初始序列号seq发送给Server，进入SYN_SENT状 态； 第二次握手：Server收到Client的SYN=1之后，知道客户端请求建立连接，将自己的SYN置1，ACK 置1，产生一个acknowledge number=sequence number+1，并随机产生一个自己的初始序列 号，发送给客户端；进入SYN_RCVD状态； 第三次握手：客户端检查acknowledge number是否为序列号+1，ACK是否为1，检查正确之后将 自己的ACK置为1，产生一个acknowledge number=服务器发的序列号+1，发送给服务器；进入 ESTABLISHED状态；服务器检查ACK为1和acknowledge number为序列号+1之后，也进入 ESTABLISHED状态；完成三次握手，连接建立。 用现实理解三次握手的具体细节 三次握手的目的是建立可靠的通信信道，主要的目的就是双方确认自己与对方的发送与接收机能正 常。 第一次握手：客户什么都不能确认；服务器确认了对方发送正常 第二次握手：客户确认了：自己发送、接收正常，对方发送、接收正常；服务器确认 了：自己接 收正常，对方发送正常 第三次握手：客户确认了：自己发送、接收正常，对方发送、接收正常；服务器确认 了：自己发 送、接收正常，对方发送接收正常 所以三次握手就能确认双发收发功能都正常，缺一不可。 为什么不能把服务器发送的ACK和FIN合并起来，变成三次挥手（CLOSE_WAIT状态意义是什 么）？ 因为服务器收到客户端断开连接的请求时，可能还有一些数据没有发完，这时先回复ACK，表示接 收到了断开连接的请求。等到数据发完之后再发FIN，断开服务器到客户端的数据传送。 如果第二次挥手时服务器的ACK没有送达客户端，会怎样？ 客户端没有收到ACK确认，会重新发送FIN请求。 客户端TIME_WAIT状态的意义是什么？ 第四次挥手时，客户端发送给服务器的ACK有可能丢失，TIME_WAIT状态就是用来重发可能丢失的 ACK报文。如果Server没有收到ACK，就会重发FIN，如果Client在2*MSL的时间内收到了FIN，就 会重新发送ACK并再次等待2MSL，防止Server没有收到ACK而不断重发FIN。 MSL(Maximum Segment Lifetime)，指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的 最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则 结束TCP连接。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:23","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"Socket 什么是Socket 网络上的两个程序通过一个双向的通讯连接实现数据的交换，这个双向链路的一端称为一个 Socket。Socket通常用来实现客户方和服务方的连接。Socket是TCP/IP协议的一个十分流行的编 程界面，一个Socket由一个IP地址和一个端口号唯一确定。 但是，Socket所支持的协议种类也不光TCP/IP、UDP，因此两者之间是没有必然联系的。在Java环境下，Socket编程主要是指基于TCP/IP协议的网络编程。 socket连接就是所谓的长连接，客户端和服务器需要互相连接，理论上客户端和服务器端一旦建立 起连接将不会主动断掉的，但是有时候网络波动还是有可能的 Socket偏向于底层。一般很少直接使用Socket来编程，框架底层使用Socket比较多， socket属于网络的那个层面 Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket 其实就是一个外观模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简 单的接口就是全部，让Socket去组织数据，以符合指定的协议。 Socket通讯的过程 基于TCP：服务器端先初始化Socket，然后与端口绑定(bind)，对端口进行监听(listen)，调用 accept阻塞，等待客户端连接。在这时如果有个客户端初始化一个Socket，然后连接服务器 (connect)，如果连接成功，这时客户端与服务器端的连接就建立了。客户端发送数据请求，服务 器端接收请求并处理请求，然后把回应数据发送给客户端，客户端读取数据，最后关闭连接，一次 交互结束。 基于UDP：UDP 协议是用户数据报协议的简称，也用于网络数据的传输。虽然 UDP 协议是一种不 太可靠的协议，但有时在需要较快地接收数据并且可以忍受较小错误的情况下，UDP 就会表现出 更大的优势。我客户端只需要发送，服务端能不能接收的到我不管 TCP协议Socket代码示例： 先运行服务端，在运行客户端 ， 服务端： package com.test.io; import java.io.IOException; import java.io.InputStream; import java.io.OutputStream; import java.net.ServerSocket; import java.net.Socket; //TCP协议Socket使用BIO进行通行：服务端 public class BIOServer { // 在main线程中执行下面这些代码 public static void main(String[] args) { //1单线程服务 ServerSocket server = null; Socket socket = null; InputStream in = null; OutputStream out = null; try { server = new ServerSocket(8000); System.out.println(\"服务端启动成功，监听端口为8000，等待客户端连接...\"); while (true){ socket = server.accept(); //等待客户端连接 System.out.println(\"客户连接成功，客户信息为：\" + socket.getRemoteSocketAddress()); in = socket.getInputStream(); byte[] buffer = new byte[1024]; int len = 0; //读取客户端的数据 while ((len = in.read(buffer)) \u003e 0) { System.out.println(new String(buffer, 0, len)); } //向客户端写数据 out = socket.getOutputStream(); out.write(\"hello!\".getBytes()); } } catch (IOException e) { e.printStackTrace(); } } } 客户端： package com.test.io; import java.io.IOException; import java.io.OutputStream; import java.net.Socket; import java.util.Scanner; //TCP协议Socket：客户端 public class Client01 { public static void main(String[] args) throws IOException { //创建套接字对象socket并封装ip与port Socket socket = new Socket(\"127.0.0.1\", 8000); //根据创建的socket对象获得一个输出流 OutputStream outputStream = socket.getOutputStream(); //控制台输入以IO的形式发送到服务器 System.out.println(\"TCP连接成功 \\n请输入：\"); while(true){ byte[] car = new Scanner(System.in).nextLine().getBytes(); outputStream.write(car); System.out.println(\"TCP协议的Socket发送成功\"); //刷新缓冲区 outputStream.flush(); } } } 先运行服务端，在运行客户端 。测试结果发送成功： 5 UDP协议Socket代码示例： 先运行服务端，在运行客户端 服务端： //UDP协议Socket：服务端 public class Server1 { public static void main(String[] args) { try { //DatagramSocket代表声明一个UDP协议的Socket DatagramSocket socket = new DatagramSocket(8888); //byte数组用于数据存储。 byte[] car = new byte[1024]; //DatagramPacket 类用来表示数据报包DatagramPacket DatagramPacket packet = new DatagramPacket(car, car.length); // //创建DatagramPacket的receive()方法来进行数据的接收,等待接收一个socket请 求后才执行后续操作； System.out.println(\"等待UDP协议传输数据\"); socket.receive(packet); //packet.getLength返回将要发送或者接收的数据的长度。 int length = packet.getLength(); System.out.println(\"啥东西来了：\" + new String(car, 0, length)); socket.close(); System.out.println(\"UDP协议Socket接受成功\"); } catch (IOException e) { e.printStackTrace(); } } } 客户端： //UDP协议Socket：客户端 public class Client1 { public static void main(String[] args) { try { //DatagramSocket代表声明一个UDP协议的Socket DatagramSocket socket = new DatagramSocket(2468); //字符串存储人Byte数组 byte[] car = \"UDP协议的Socket请求，有可能失败哟\".getBytes(); //InetSocketAddress类主要作用是封装端口 InetSocketAddress address = new InetSocketAddress(\"127.0.0.1\", 8888); //DatagramPacket 类用来表示数据报包DatagramPacket DatagramPacket packet = new DatagramPacket(car, car.length, address); //send() 方法发送数据包。 socket.send(packet); System.out.println(\"UDP协议的Socket发送成功\"); socket.close(); } catch (Exception e) { e.printStackTrace(); } } } 先运行服务端，在运行客户端 。测试结果成功发送成功： 6 Socket的常用类 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:24","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"3. HTTP 什么是Http协议？ Http协议是对客户端和服务器端之间数据之间实现可靠性的传输文字、图片、音频、视频等超文 本数据的规范，格式简称为“超文本传输协议” Http协议属于应用层，及用户访问的第一层就是http Socket和http的区别和应用场景 Socket连接就是所谓的长连接，理论上客户端和服务器端一旦建立起连接将不会主动断掉； Socket适用场景：网络游戏，银行持续交互，直播，在线视屏等。 http连接就是所谓的短连接，即客户端向服务器端发送一次请求，服务器端响应后连接即会断开等 待下次连接 http适用场景：公司OA服务，互联网服务，电商，办公，网站等等等等 什么是http的请求体？ HTTP请求体是我们请求数据时先发送给服务器的数据，毕竟我向服务器那数据，先要表明我要什 么吧 HTTP请求体由：请求行 、请求头、请求数据组成的， 注意：GIT请求是没有请求体的 HTTPS工作原理 一、首先HTTP请求服务端生成证书，客户端对证书的有效期、合法性、域名是否与请求的域名一 致、证书的公钥（RSA加密）等进行校验； 二、客户端如果校验通过后，就根据证书的公钥的有效， 生成随机数，随机数使用公钥进行加密 （RSA加密）； 三、消息体产生的后，对它的摘要进行MD5（或者SHA1）算法加密，此时就得到了RSA签名； 四、发送给服务端，此时只有服务端（RSA私钥）能解密。 五、解密得到的随机数，再用AES加密，作为密钥（此时的密钥只有客户端和服务端知道）。 一次完整的HTTP请求所经历几个步骤? HTTP通信机制是在一次完整的HTTP通信过程中，Web浏览器与Web服务器之间将完成下列7个步骤： 建立TCP连接 怎么建立连接的，看上面的三次捂手 Web浏览器向Web服务器发送请求行 一旦建立了TCP连接，Web浏览器就会向Web服务器发送请求命令。例如：GET /sample/hello.jsp HTTP/1.1。 Web浏览器发送请求头 浏览器发送其请求命令之后，还要以头信息的形式向Web服务器发送一些别的信息，之后浏览器发送 了一空白行来通知服务器，它已经结束了该头信息的发送。 Web服务器应答 客户机向服务器发出请求后，服务器会客户机回送应答， HTTP/1.1 200 OK ，应答的第一部分是协议 的版本号和应答状态码。 Web服务器发送应答头 类别 描述 1xx： 指示信息–表示请求已接收，正在处理 2xx： 成功–表示请求已被成功接收、理解、接受 3xx： 重定向–要完成请求必须进行更进一步的操作 4xx： 客户端错误–请求有语法错误或请求无法实现 5xx： 服务器端错误–服务器未能实现合法的请求 正如客户端会随同请求发送关于自身的信息一样，服务器也会随同应答向用户发送关于它自己的数据及 被请求的文档。 Web服务器向浏览器发送数据 Web服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此为结束，接着，它 就以Content-Type应答头信息所描述的格式发送用户所请求的实际数据。 Web服务器关闭TCP连接 常用HTTP状态码是怎么分类的，有哪些常见的状态码？ HTTP状态码表示客户端HTTP请求的返回结果、标识服务器处理是否正常、表明请求出现的错误 等。 状态码的类别： 1xx： 指示信息–表示请求已接收，正在处理 2xx： 成功–表示请求已被成功接收、理解、接受 3xx： 重定向–要完成请求必须进行更进一步的操作 4xx： 客户端错误–请求有语法错误或请求无法实现 5xx： 服务器端错误–服务器未能实现合法的请求 http版本的对比 HTTP1.0版本的特性： 早先1.0的HTTP版本，是一种无状态、无连接的应用层协议。 HTTP1.0规定浏览器和服务器保持短暂的连接，浏览器的每次请求都需要与服务器建立一个 TCP连接，服务器处理完成后立即断开TCP连接（无连接），服务器不跟踪每个客户端也不记 录过去的请求（无状态）。 HTTP1.1版本新特性 默认持久连接节省通信量，只要客户端服务端任意一端没有明确提出断开TCP连接，就一直 保持连接，可以发送多次HTTP请求 管线化，客户端可以同时发出多个HTTP请求，而不用一个个等待响应 断点续传原理 HTTP2.0版本的特性 二进制分帧（采用二进制格式的编码将其封装） 首部压缩（设置了专门的首部压缩设计的HPACK算法。） 流量控制（设置了接收某个数据流的多少字节一些流量控制） 多路复用（可以在共享TCP链接的基础上同时发送请求和响应） 请求优先级（可以通过优化这些帧的交错和传输顺序进一步优化性能） 服务器推送（就是服务器可以对一个客户端请求发送多个响应。服务器向客户端推送资 源无 需客户端明确的请求。（重大更新）） 什么是对称加密与非对称加密 对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问 题，即如何安全地将密钥发给对方； 而非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知 道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行 解密。 由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性；但是和对称加 密比起来，非常的慢 cookie和session对于HTTP有什么用？ HTTP协议本身是无法判断用户身份。所以需要cookie或者session 什么是cookie cookie是由Web服务器保存在用户浏览器上的文件（key-value格式），可以包含用户相关的信 息。客户端向服务器发起请求，就提取浏览器中的用户信息由http发送给服务器 什么是session session 是浏览器和服务器会话过程中，服务器会分配的一块储存空间给session。 服务器默认为客户浏览器的cookie中设置 sessionid，这个sessionid就和cookie对应，浏览器在向 服务器请求过程中传输的cookie 包含 sessionid ，服务器根据传输cookie 中的 sessionid 获取出 会话中存储的信息，然后确定会话的身份信息。 cookie与session区别 cookie数据存放在客户端上，安全性较差，session数据放在服务器上，安全性相对更高 单个cookie保存的数据不能超过4K，session无此限制 session一定时间内保存在服务器上，当访问增多，占用服务器性能，考虑到服务器性能方面，应 当使用cookie。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:7:25","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"面试问题 ","date":"2022-06-28 15:30:06","objectID":"/interview/:8:0","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"外包公司能不能去 外包合同和正式合同的区别: 外包合同不是和本单位签订的。所以福利可能差一些。 签的时间比较短，稳定性差。 外包的优势，就业门槛低 适合转行，失业解决温饱问题的。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:8:1","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"你如何看待加班？ 回答hr，你可以接受加班，表达一下你的意愿 告诉hr你的工作效率非常高，可以避免不必要的加班。最后追问，公司加班的原因。 正确回答： 如果出现加班，我首先会考虑是不是我的原因导致的，如果是的话，我会尽量加快提高工作效率，减少加班情况的发生。 如果是项目比较急，加班，我也会接受加班的。 但是我不是为了加班而加班，为了表现自己多努力工作而加班。我更希望提高自己的工作效率，合理安排自己的加班时间，这样可以协调工作和生活。 如果公司有需要我是可以加班，但是呢，我会尽量在周一在周五之内完成自己的工作。周六日好好学习充电以备下周更高效的工作。 追问hr：那么加班严重到什么程度，一般是为了什么而加班。 ","date":"2022-06-28 15:30:06","objectID":"/interview/:8:2","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"面试问题｜被录用后如何开展工作？ 我会按照领导的指示和需求，了解工作情况，其次的话，我会制作一个工作计划表，然后开展工作 注重工作态度和学习能力 ","date":"2022-06-28 15:30:06","objectID":"/interview/:8:3","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"面试问题｜你从上份工作中学到了什么？ 学会开发的流程，这样一来，让我的工作更加工业化和职业化 在工作上，我学会了如何从项目中分析问题和解决问题 工作之外，我还会学习和充电 ","date":"2022-06-28 15:30:06","objectID":"/interview/:8:4","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"面试问题 ｜你希望通过这份工作获得什么？满分话术分享 对于我来说，我从事的工作还是比较适合我的，我从事的工作可以充分发挥我的专长，这样的话可以带给我一种满足感，同时，我希望这一份工作具有挑战性，这样的话可以促使我提升自己 ","date":"2022-06-28 15:30:06","objectID":"/interview/:8:5","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"面试问题 ｜谈谈你失败的一次经历？满分话术分享 hr的目的：想知道你遇到挫折之后你的承受能力有多强，你的学习能力有多强，收获了什么，下一次遇到什么问题，你应该如何去做 我：举例子：。。。。。 最后，我有时候认为失败比成功更重要，失败可以让我积累一定的经验和财富，这样的话，以后遇到同样的事情，我就不会再犯同样的错误了 ","date":"2022-06-28 15:30:06","objectID":"/interview/:8:6","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"面试问题 ｜你有没有参加其他公司面试？ hr的动机： 判断求职者是否有明确的职业规划，而不是今天投这一行,明天找另外一行 根据求职者面试情况，给求职者定薪资 话术： 在投递简历之前，我已经对业内几家有口碑的公司进行了背调，并且结合我的职业规划进行筛选和投递，来您公司之前我已经去面试过其他家岗位，并且收到几家offer 。但是呢，从今天的面试来看，我觉得贵公司的企业文化及实力非常符合我的职业发展，并且我的工作履历和贵公司的岗位职责描述是相符的，所以我也真诚希望贵公司可以考虑我(不要说其他公司的细节，大忌) 再面试之前我做了很多准备，并且通过这一次面试，我对贵公司有了一个了解和认识。觉得和我的求职意向还是满符合的。我希望贵公司可以给我一个机会，让我为贵公司做出一些贡献。贵公司是我参加面试这么多 ","date":"2022-06-28 15:30:06","objectID":"/interview/:8:7","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["interview"],"content":"薪资技巧 薪资： 面试阶段不要提薪酬，等公司确定要你之后再谈；不要狮子大张口 不要工资造假 谈钱不可耻，不卑不亢的提出薪资要求 不要自爆（比如：重发展机会，不在乎工资）可以表达，本人很欣赏公司，也很开心能加入，对于薪酬也是一个重要的考虑纬度，然后与人资展开具体沟通（喷血，我都是这么说的) 不要过早亮出底牌，报出具体金额，应先给到方向性的回复，争取让对方先报数。 比如：相较于现有的工资水平，有一定合理比例的涨幅，也想了解一下公司对我offer的看法；具体询问公司的工资构成。如果对方还是不说数字的话，报出去的工资要比底线高10-20%。 能否先问一下公司的薪酬结构。比如基础薪资和绩效奖金的构成，绩效奖金的发放标准，公司有没有年终奖，项目奖金，公司有没有期权，五险一金的缴纳方式，公司有没有期权，公司上班时间，周末上班的计薪方式。 一定要争取对方先报数。 比如hr和我们介绍公司的薪酬体系，你现在是什么样的想法呢。这时候你可以说：我希望总包上，相较于上一份工作一定有合理比例的涨幅，也想听听看公司给我目前聊下给我offer提议。这时候，有两种情况： hr给你一个具体的数字。那你就给这个数字具体一个反馈，如果符合预期，就愉快的接受，如果不符合，就说，可能这距离距离我的预期还有一定的差距，看你能否给到xxx呢。or我这边的期望是不低于xxx hr继续追问，你预计的涨幅是多少呢。你期望的薪资数字是怎么样的呢。这时候会回答，比自己底线高10%-20%的数字 拿现有的去argue。 说你上一份工作中拿到额外的奖励，升职加薪等 说目前拿到的offer是多少，具体哪一个公司不说了。比如xxxx行业的公司给我offer是多少。 说你工作上，经验上的优势。比如，这个岗位，你有非常成功的经验，能够快速上手；这个岗位所要求的技术栈，你已经非常熟练的运用了。 说出自己的优势的论据 我最近刚换工作，薪资方面还比较满意。 分享几点我的经历： 1、根据岗位对标信息，认为自己绝对有信心胜任此岗位的，绝不松口，绝不给对方压价的余地，如果对方使用话术提问一些比较难刻薄的问题，那么可以拒不作答或者反问，只要别吵起来，都没有问题，HR的职责是花小钱招大能，最终决定权一般都在用人单位手中不在HR手中。 2、不要让HR知道你只面试了这一家或者圈子很小的几家公司，因为这几家公司有可能中高层互通有无，对你的信息和情况掌握得十分清楚，甚至直接电联你直属上司，非常非常被动。面试的时候HR掌握你的信息越多，越不利于你谈判。甚至可以用跨多个行业的应聘资历反过来给HR施加压力。 3、面试过程中回答问题所表达的高度和视角要高于应聘岗位，给用人单位一种：“卧槽！这是个宝才！这下捡到鬼了！”这种感觉，HR一般还是要听用人单位的意见，如果用人单位说这个人一定要拿下，那么HR是压不住你的薪资的。 4、不要给自己画条条框框，比如：看别人跳槽升1级比较多，升2级的寥寥无几。就抱着我肯定也只能升1级的想法，是完全没有道理的。只要招聘的岗位能给你实现想法的平台，能给你展现才华的舞台，就勇敢去追！有底气有信心就去搏一搏，便是失败了，也明了了自身的不足与差距在哪里。 我面试过程中，升了两级，涨了一次工资，省了差不多3年时间： P1：HR找到我，问我有没有意向加入A+1岗位（我当前处于A岗位），我表示完全没有兴趣，没有谈的必要，于是他说那么A+2的岗位有没有兴趣，我表示吸引力一般般，但可以接受面试。 P2：第一轮面试通过，面谈结果定岗A+3，薪资方便我表示低于底线X-4就不必谈了。于是第一轮薪资报价是X-2，进入第二轮面试（当对于目前年薪X，因为绩效优占比较大，所以跳槽有一定的下浮是正常的） P3：第二轮面试通过，面谈结果定岗A+3，并且表示薪资X-2已经是A+3岗位封顶水平。我表示没有谈的必要了，此offer的“岗位+薪资综合水平”对我完全没有吸引力。 P4：HR上报审批意见，最终定岗A+4，薪资X+3+期权（处于A+4岗位中上水平）。 关于不要给自己画条条框框这一点我举个例子，我室友和我是同岗位，他的薪资比我还高6个。可以讲，他跳槽之后，+2职级的情况下，没有哪家公司能给出他当前水平的薪资。他陷入了“不跳槽要命，跳槽就亏钱”的情境之中。他就是给自己画了一个“我跳槽只能升2级”的框框里，现在他新面试的岗位比原先高3级，对方公司明确表示，你这点薪资要求完全没有问题，属于该要钱的时候要少了的典型。 还有就是涨薪幅度的问题，不要受到30%、50%这种统计数字的束缚，这些数字只能代表大多数或者平均数，不跨行涨300%的，跨行涨500%的这种人虽然少，但不是没有！ 我个人的想法是，一般来说选offer钱多 事少 离家近 至少占一个。但对于应届生，你是否能学到东西才是最重要的。这里的学到东西说的不是hr许诺给你的个人成长培训资源之类的，而是会不会为你以后的跳槽提高身价。职场无非就是个工作能力和人情世故，薪资福利该问问该谈谈，千万不要不好意思，不然折磨的只会是自己。 💡💡先说下楼主本人背景：双非硕士，无具体实习经验，但项目经验比较丰富，在校也有相关的获奖情况 💈12月1号当天收到补录通过的通知，然后前天就到了和HR谈薪的环节。（偷偷说个小秘密，原来校招生也是有谈薪的余地的） · 这个环节可谓精彩，鹅厂的HR也真的是专业🎏 谈薪流程如下： 💎- HR：同学，你期望的薪资是多少呢？（第一个坑：千万别先说） 我：我已经通过您们的面试，相信贵司是一个珍惜人才的公司，已经有一个合理的薪酬给到我，您直接跟我说就好。（不入坑） 💎- HR：好的，同学你的整体面试还不错，我们这边给出了一个SP的评价，请问同学你这边还有其它公司的offer吗？（第二个坑：慎重说） 我：谢谢您们的认可，目前的话我是有几个offer在手的，但具体还没有确定接受哪个，还正在考虑中。（模糊回答） 💎- HR：那目前你这边已经接了哪些offer呢，以及对应的薪酬是如何的呢？（第三个坑：看情况砍价） 我：目前的话主要是字节、顺丰等头部企业，薪酬其实每一家都不一样，这个很难统一说。（继续打太极拳） 💎- HR：好的，那我们这边目前给你的薪酬为：18k * 16 + 4k * 12 + 2w签字费 + 6w 股票 （主动说出薪酬） 总包40W+，还有一些诸如此类的福利等等 我：明白，这个薪酬相对来说还是比较ok的，和其它几家公司也差不多，我会好好考虑一下（压住激动的心，先压一压） 💎- HR：同学你这边是还有其它的考虑吗？你可以说一说（似乎上钩了） 我：对，因为我同学也进了咱们公司，但他的薪酬似乎比我好一些。（的确存在，但我同学是算法岗） 💎- HR：哦，那你自己有没有目标的薪酬呢？（这时候还不能急） 我：哈哈，我当然希望能够高一些，我想的是，基础工资那里希望是19k，签字费有3w，这个是我的想法（明确说出自己的希望） 💎- HR：明白，这样，我跟部门经理先说一下你的情况，然后再回电你。 （过程15分钟挂机，心里比较忐忑） · 再次回电 💎- HR：同学，我这边已经沟通过了，你这边提的这个要求我们可以满足，前提是你需要尽快办理三方。 我：好的，谢谢您！ · 至此，不到10分钟的谈话，薪酬从18k * 16 + 4k * 12 + 2w签字费 + 6w 股票 = 41.6W 变成了 19k * 16 + 4k * 12 + 3w签字费 + 6w 股票 = 44.2W 💡💡也就是相当于我10分钟赚了2.6W💡💡 · ☎️回顾整体面试给我的感觉就是，没有实习经历的确是我的硬伤，但我的项目经历是我的亮点，扬长避短，而且在面试中一定是表现冷静、沉着，不断通过交流展示自己的优点。 · 🔋好了，至此，鹅厂的谈薪已经结束，也正在准备三方了。面试过程还有很多有趣的东西没来得及分享，如果想了解的可以私戳我哦。谢谢大家的喜欢，祝大家也有满意的offer！！🧧🧧 ","date":"2022-06-28 15:30:06","objectID":"/interview/:8:8","tags":["interview"],"title":"interview","uri":"/interview/"},{"categories":["Graduation project"],"content":" Hsiang-Fu Yu, Nikhil Rao, Inderjit S. Dhillon, 2016. Temporal regularized matrix factorization for high-dimensional time series prediction. 笔记中部分公式未渲染出来，文末截图可见 Temporal Regularized Matrix Factorization(TRMF) for High-dimensional Time Series Prediction ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:0:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"摘要 现代应用程序需要具有高度可扩展性的方法，并且可以处理有噪声的或有缺失值的数据。 本文提出了一个支持数据驱动的时态学习和预测的时态正则化矩阵分解（TRMF）框架。我们在学习自回归模型中的依赖关系的背景下，与图正则化方法建立了有趣的联系框架。 实验结果表明：TRMF在维数为50000的问题上比其他方法快两个数量级，并能对现实世界的数据集（如沃尔玛电子商务数据集）生成更好的预测。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:1:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"1. 介绍 现代时间序列应用程序给从业者带来了两个挑战：处理大n（数字）和T（时间帧）的可伸缩性，以及处理缺失值的灵活性。 AR和DLM侧重于低维时间序列数据，而没有处理上述两个问题。 对高维时间序列数据建模的一种自然方式是以矩阵的形式，行对应于每个一维时间序列，列对应于时间点。 鉴于n个时间序列通常高度相关，有人尝试应用低秩矩阵分解（MF）或矩阵完成（MC）技术来分析高维时间序列[2,14,16,23,26]。与上面的AR和DLM模型不同，最先进的MF方法以n为单位线性扩展，因此可以处理大型数据集。 在MF中，观测到的n维时间序列数据被组织在矩阵 $\\mathcal{Y} \\in \\mathbb{R}^{n \\times T}$ 中，矩阵 y 由维度特性矩阵 $W \\in \\mathbb{R}^{n \\times r}$ 与时间特性矩阵 $X \\in \\mathbb{R}^{r \\times T}$ 的组合进行低秩逼近，从而修补缺失数据。 $$ \\mathcal{Y} \\approx W X $$ 使用最小二乘法、梯度下降等方法求解下述最小化问题，从而对矩阵 W 与矩阵 X 进行逼近： $$ \\min {W X} \\sum{(i, t) \\in \\Omega}\\left(\\mathcal{Y}{i t}-w{i}^{T} x_{t}\\right)^{2}+\\lambda_{w} \\mathcal{R}{w}(W)+\\lambda{x} \\mathcal{R}_{x}(X) $$ 其中， $\\Omega$ 是原矩阵 $\\mathcal{Y} \\in \\mathbb{R}^{n \\times T}$ 中非零元所处位置的集合； $\\sum_{(i, t) \\in \\Omega}\\left(\\mathcal{Y}{i t}-w{i}^{T} x_{t}\\right)^{2}$ 为残差矩阵F范数的平方，用来描述 W X 矩阵 与原矩阵 Y 的差异；$R_w$ 与 $R_x$ 分别是 W 与 X 的正则项，用来防止过拟合。 $$ \\mathcal{R}{w}(W)=|W|{F}^{2}=\\sum_{i=1}^{n}\\left|\\boldsymbol{w}{i}\\right|^{2}=\\sum{i=1}^{n} w_{i}^{T} w_{i} $$ $$ \\mathcal{R}{x}(X)=|X|{F}^{2}=\\sum_{t=1}^{T}\\left|\\boldsymbol{x}{t}\\right|^{2}=\\sum{t=1}^{T} x_{t}^{T} x_{t} $$ 大多数现有的MF方法采用基于图（这个图指的是同一个特征的时序关系拼接成图，也就是X矩阵的一行）的方法来处理时间依赖性。具体来说，依赖关系由加权相似图描述，并通过拉普拉斯正则项进行约束。 图1：多重时间序列的矩阵分解模型。F捕捉矩阵Y中每个时间序列的特征，X捕捉潜在和时变变量 MF方法可以对缺失数据进行修复，但是对于预测问题则无能为力。此外，由于MF方法并没有考虑数据的时序特性，对上述的交通与天气数据的修复效果并不理想。 本文提出了一个新的时间正则化矩阵分解框架(TRMF)用于高维时间序列分析。 在TRMF中，我们考虑了一种原则性的方法来描述潜在时间嵌入之间的时间依赖性结构{$x_t$}，并设计了一个时间正则化器来将这种时间依赖性结构纳入标准MF公式。 与大多数现有的MF方法不同，我们的TRMF方法支持数据驱动的时间依赖性学习，并为矩阵分解方法带来预测未来值的能力。此外，TRMF方法继承了MF方法的属性，即使在存在许多缺失值的情况下，TRMF也可以轻松处理高维时间序列数据。 作为一个具体的例子，我们展示了一种新的自回归时间正则化器，它鼓励时间嵌入{$x_t$}之间的AR（autoregressive）结构。 我们还将提出的正则化框架与基于图的方法联系起来，其中甚至可以解释负相关。 这种连接不仅有助于更好地理解我们的框架所包含的依赖结构，而且还有助于使用现成的高效求解器(如GRALS)直接求解TRMF。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:2:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"2. 具有时间依赖性的数据的现有矩阵分解方法 标准MF公式对列的排列保持不变（列不管怎么变，权重矩阵保持不变），这不适用于具有时间依赖性的数据。 因此，对于时间依赖性{$x_t$}，大多数现有的时间MF方法都转向基于图的正则化框架，并用图编码时间依赖性。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:3:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"时间依赖性的图正则化 图2：时态依赖的基于图的正则化。 令G是一个时间依赖性{$x_t$}的图，$G_{ts}$是第t个点和第s个点之间的边权重。一种常见的正则化方式如下公式： $$ \\mathcal{R}{x}(X)=\\mathcal{G}(X \\mid G, \\eta):=\\frac{1}{2} \\sum{t \\sim s} G_{t s}\\left|\\boldsymbol{x}{t}-\\boldsymbol{x}{s}\\right|^{2}+\\frac{\\eta}{2} \\sum_{t}\\left|\\boldsymbol{x}_{t}\\right|^{2}(2) $$ 其中t~s代表了第t个点和第s个点之间的边；第二个正则化项是用来保证强凸性 一个很大的$G_{ts}$可以保证$x_t$和$x_s$在欧几里得距离上很接近 为了保证$\\mathcal{G}(X \\mid G, \\eta)$的凸性，我们让$G_{ts}$≥0 为了将基于图的正则化应用于时间依赖关系上，我们需要通过滞后集L和权值向量w重复地指定各个点之间的依赖模式，以便距离L的所有边t ~ s共享相同的权值 于是上面的公式可以改写成： $$ \\mathcal{G}(X \\mid G, \\eta)=\\frac{1}{2} \\sum_{l \\in \\mathcal{L}} \\sum_{t: t\u003el} w_{l}\\left(\\boldsymbol{x}{t}-\\boldsymbol{x}{t-l}\\right)^{2}+\\frac{\\eta}{2} \\sum_{t}\\left|\\boldsymbol{x}_{t}\\right|^{2}(3) $$ 这种直接使用基于图的方法虽然很直观，但有两个问题: 两个时间点之间可能存在负相关依赖关系; 显式的时态依赖结构通常不可用，必须使用者进行推断。 于是，很多现有的这种正则化的模型只能考虑很简单的时间依赖关系（比如滞后集L很小，L={1}），和统一的权重（比如不管两个点之间距离是多少，权重统一设置为1） 这导致现有MF方法对大规模时间序列的预测能力较差。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:3:1","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"学习时间依赖性的挑战 也许有人会想：那我权重参数w让机器自己学不就好了吗？ 在这种假设下，我们有了以下的优化方程： $$ \\min {F, X, \\boldsymbol{w} \\geq \\mathbf{0}} \\sum{(i, t) \\in \\Omega}\\left(Y_{i t}-\\boldsymbol{f}{i}^{\\top} \\boldsymbol{x}{t}\\right)^{2}+\\lambda_{f} \\mathcal{R}{f}(F)+\\frac{\\lambda{x}}{2} \\sum_{l \\in \\mathcal{L}} \\sum_{t: t-l\u003e0} w_{l}\\left(\\boldsymbol{x}{t}-\\boldsymbol{x}{t-l}\\right)^{2}+\\frac{\\lambda_{x} \\eta}{2} \\sum_{t}\\left|\\boldsymbol{x}_{t}\\right|^{2} (4)$$ 我们不难发现，最终的优化结果，是所有的w都是0，意为没有空间依赖关系的时候，目标函数达到最小值。 为了避免让所有的w都是0，有人想到可以给w的和加上一个限制，比如$\\sum_{l \\in \\mathcal{L}} w_{l}=1$ 同样地，我们不难发现，最终的优化结果是$l^{}=\\arg \\min {l \\in \\mathcal{L}} \\sum{t: t\u003el}\\left|\\boldsymbol{x}{t}-\\boldsymbol{x}{t-l}\\right|^{2}$，对应的wl是1，其他的w是0 因此，通过简单地在MF公式中插入正则化器来自动学习权重并不是一个可行的选择。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:3:2","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"3. TRMF 为了解决前面提到的限制，本文提出了时间正则化矩阵分解(TRMF)框架，这是一种将时间依赖性纳入矩阵分解模型的新方法。 与前面提到的基于图的方法不同，我们建议使用经过充分研究的时间序列模型来明确地描述{$x_t$}之间的时间依赖性。 $$\\boldsymbol{x}{t}=M{\\Theta}\\left(\\left{\\boldsymbol{x}{t-l}: l \\in \\mathcal{L}\\right}\\right)+\\boldsymbol{\\epsilon}{t} (5)$$ $\\boldsymbol{\\epsilon}_{t}$是一个高斯噪声向量 $M_{\\Theta}$是一个时间序列模型，参数是Θ和滞后集L L是一个包含滞后指标L的集合，表示t和t-l时间点之间的相关性 Θ捕捉时间相关性的权重信息(如AR模型中的转移矩阵) 基于此，我们提出了一个新的正则化项$\\mathcal{T}{\\mathrm{M}}(X \\mid \\Theta)$，这可以鼓励模型依照时间序列$M{\\Theta}$。 我们令： $$ \\mathcal{T}{\\mathrm{M}}(X \\mid \\Theta)=-\\log \\mathbb{P}\\left(\\boldsymbol{x}{1}, \\ldots, \\boldsymbol{x}_{T} \\mid \\Theta\\right) (6) $$ 当θ给定的时候，我们令$\\mathcal{T}_{\\mathrm{M}}(X \\mid \\Theta)$为矩阵分解的一个正则化项；当θ未知的时候，我们令θ为另外一部分参数，并且设计$R_θ$以作为另一个正则化项。 $$ \\min {F, X, \\Theta} \\sum{(i, t) \\in \\Omega}\\left(Y_{i t}-\\boldsymbol{f}{i}^{\\top} \\boldsymbol{x}{t}\\right)^{2}+\\lambda_{f} \\mathcal{R}{f}(F)+\\lambda{x} \\mathcal{T}{\\mathrm{M}}(X \\mid \\Theta)+\\lambda{\\theta} \\mathcal{R}_{\\theta}(\\Theta) (7) $$ 通过交替地优化更新F,X,Θ，可以解决上面的优化方程。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:4:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"TRMF中数据驱动的时间依赖性学习 在TRMF中，当F和X是固定的时候，式（7）可以简化为： $$ \\min {\\Theta} \\lambda{x} \\mathcal{T}{M}(X \\mid \\Theta)+\\lambda{\\theta} \\mathcal{R}_{\\theta}(\\Theta) (8)$$ 其中第一项可以看成：$\\min _{\\Theta} -\\log{P}(X_1,…X_T \\mid \\Theta)$，即$\\max _{\\Theta}{P}(X_1,…X_T \\mid \\Theta)$ 也就是说，后一项可以看成最大后验概率 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:4:1","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"TRMF时间序列分析 我们可以看到，TRMF可以无缝地处理在分析具有时间依赖性的数据时经常遇到的各种任务: 时间序列预测 一旦我们有了潜在的嵌入{$x_t:1,…T$}的$M_{\\Theta}$，我们可以预测未来的嵌入{$x_t:t\u003eT$}，然后使用来预测结果 缺失值补全 我们可以使用$\\boldsymbol{f}{i}^{\\top} \\boldsymbol{x}{t}$来对这些缺失的条目进行插补，就像标准矩阵补全，在推荐系统和传感器网络中很有用。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:4:2","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"4. 一种新的自回归时间正则化算法 在小节3中，我们大致介绍了TRMF的框架：正则项$\\mathcal{T}{M}(X \\mid \\Theta)$（有时间序列模型$M{\\Theta}$确定） 在这一小节中，我们将介绍一种TRMF框架：自回归模型，参数为滞后集L和权重 我们令xt是以下形式: $$ \\boldsymbol{x}{t}=\\sum{l \\in \\mathcal{L}} W^{(l)} \\boldsymbol{x}{t-l}+\\boldsymbol{\\epsilon}{t} $$ $\\boldsymbol{\\epsilon}_{t}$是一个高斯噪声向量。 为了简化，假设$\\boldsymbol{\\epsilon}{t} \\sim \\mathcal{N}\\left(0, \\sigma^{2} I{k}\\right)$ 于是，时间正则化项$\\mathcal{T}_{M}(X \\mid \\Theta)$可以写成： $$ \\mathcal{T}{\\mathrm{AR}}(X \\mid \\mathcal{L}, \\mathcal{W}, \\eta):=\\frac{1}{2} \\sum{t=m}^{T}\\left|\\boldsymbol{x}{t}-\\sum{l \\in \\mathcal{L}} W^{(l)} \\boldsymbol{x}{t-l}\\right|^{2}+\\frac{\\eta}{2} \\sum{t}\\left|\\boldsymbol{x}_{t}\\right|^{2} (9)$$ 其中：$m := 1 + L, L := max(L)$, and $\\eta \u003e 0$ 由于每个$W^{(l)} \\in R^{k*k}$所以我们有$|\\mathcal{L}| k^{2}$个参数要学习，这可能导致过拟合 为了避免过拟合，同时为了生成更可解释的结果，我们人为定义$W^{(l)} \\in R^{k*k}$为对角矩阵，这可以使得参数量减少至$|\\mathcal{L}| k$ 出于简化的考虑，我们使用W来表示这个k×L的矩阵，其中第l列表示$W^{(l)} \\in R^{k*k}$ 的对角线元素 简化后： $$ \\mathcal{T}{\\mathrm{AR}}(\\overline{\\boldsymbol{x}} \\mid \\mathcal{L}, \\overline{\\boldsymbol{w}}, \\eta)=\\frac{1}{2} \\sum{t=m}^{T}\\left(x_{t}-\\sum_{l \\in \\mathcal{L}} w_{l} x_{t-l}\\right)^{2}+\\frac{\\eta}{2}|\\overline{\\boldsymbol{x}}|^{2} (10)$$ $x_t$表示时刻t的向量。 将式10代入式7，得到式12： $$ \\min {F, X, \\mathcal{W}} \\sum{(i, t) \\in \\Omega}\\left(Y_{i t}-\\boldsymbol{f}{i}^{\\top} \\boldsymbol{x}{t}\\right)^{2}+\\lambda_{f} \\mathcal{R}{f}(F)+\\sum{r=1}^{k} \\lambda_{x} \\mathcal{T}{\\mathrm{AR}}\\left(\\overline{\\boldsymbol{x}}{r} \\mid \\mathcal{L}, \\overline{\\boldsymbol{w}}{r}, \\eta\\right)+\\lambda{w} \\mathcal{R}_{w}(\\mathcal{W}) (12)$$ 我们将式（12）命名为TRMF-AR. ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:5:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"不同时间序列之间的关联性 尽管$W^{(l)} \\in R^{k*k}$是对角阵，但是TRMF还是可以建模不同时间序列（X矩阵不同行之间）的关联性。这个关联性在特征矩阵F中体现。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:5:1","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"滞后集L的选择 TRMF中L的选择更加灵活。因此，TRMF可以提供重要的优势: 首先，因为不需要指定权重参数W，可以选择更大的L来考虑长期依赖性，这也可以产生更准确和稳健的预测。 其次，L中的时延不需要是连续的，这样就可以很容易地嵌入关于周期性或季节性的领域知识。例如，对于具有一年季节性的每周数据，可以考虑L ={1, 2, 3, 51, 52, 53}。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:5:2","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"参数优化 式10和式12 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:5:3","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"5. 实验结果 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:6:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"数据集 对于synthetic数据集，我们先随机生成一个$F \\in R^{16*4}$，，生成{$x_t$}，它满足AR过程，且滞后集L={1,8}。然后Y通过$\\boldsymbol{y}{t}=F \\boldsymbol{x}{t}+\\boldsymbol{\\epsilon}{t}$ 且 $\\boldsymbol{\\epsilon}{t} \\sim \\mathcal{N}(\\mathbf{0}, 0.1 I)$生成 电力和交通数据集从UCI存储库获得，而Walmart -1和Walmart -2是来自Walmart电子商务的两个专有数据集，其中包含每周的销售信息。由于缺货等原因，missing rate分别为55.3%和49.3%。为了评价预测性能，我们考虑了归一化偏差(ND)和归一化均方根(NRMSE)。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:6:1","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"实验结果 表3：缺失值插补结果：每种方法的ND/NRMSE。请注意，TRMF 在几乎所有情况下都优于所有竞争方法 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:6:2","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":" FILLING THE G AP S: MULTIVARIATE TIME SERIES IMPUTATION BY GRAPH NEURAL NETWORKS ","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:0:0","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Graduation project"],"content":"摘要 在处理来自实际应用程序的数据时，处理缺失值和不完整的时间序列是一项劳动密集型、乏味且不可避免的任务。 有效的时空表示将允许插补方法通过利用来自不同位置的传感器的信息来重建丢失的时间数据。 然而，标准方法在捕捉互连传感器网络中存在的非线性时间和空间依赖性方面存在不足，并且没有充分利用可用的（通常是强大的）关系信息。 值得注意的是，大多数基于深度学习的最先进的插补方法都没有明确地对关系方面进行建模，并且在任何情况下都没有利用能够充分表示结构化时空数据的处理框架。 相反，图神经网络最近作为用于处理具有关系归纳偏差的序列数据的表达性和可扩展性工具而大受欢迎。 在这项工作中，我们在多元时间序列插补的背景下首次评估了图神经网络。 特别是，我们引入了一种名为 GRIN 的新型图神经网络架构，旨在通过消息传递学习时空表示来重建多元时间序列的不同通道中的缺失数据。 实证结果表明，我们的模型在相关真实世界基准的插补任务中优于最先进的方法，平均绝对误差改进通常高于 20% ","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:1:0","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Graduation project"],"content":"介绍 从理论和实践的角度来看，缺失值的插补是多元时间序列分析 (TSA) 中的一个突出问题（Little \u0026 Rubin，2019）。 事实上，在一个复杂互联系统的世界中，例如那些表征传感器网络或物联网的系统，故障传感器和网络故障是一种普遍现象，会导致数据采集过程中断。幸运的是，这些类型的故障通常是稀疏的并且局限于单个传感器级别，即它们不会立即危及整个传感器网络。 换句话说，通常情况下，在某个时间步长，缺失数据仅出现在生成的多元时间序列的某些通道上。在这种情况下，时空插补方法 (Yi et al., 2016; Yoon et al., 2018b) 旨在通过可能利用时间和空间依赖性来重建信号的缺失部分。 特别是，有效的时空方法将通过考虑过去和未来的值以及空间上相邻传感器的同时测量来重建缺失值。 在这里，空间相似性并不一定意味着物理（例如，地理）接近，而是表明所考虑的传感器与 w.r.t. 相关。一个通用的（可量化的）函数依赖（例如，皮尔逊相关性或格兰杰因果关系——格兰杰，1969）和/或在某个潜在空间中接近。 因此，关系信息可以被解释为一组约束——连接不同的时间序列——允许用虚拟传感器替换故障传感器。 在不同的插补方法中，基于深度学习的方法（LeCun 等，2015；Schmidhuber，2015；Goodfellow 等，2016）变得越来越流行（Yoon 等，2018a；Cao 等，2018；Liu 等 等人，2019）。 然而，这些方法通常完全忽略可用的关系信息或依赖于为序列处理量身定制的标准神经架构的相当简单的修改（Hochreiter \u0026 Schmidhuber，1997；Chung 等人，2014；Bai 等人，2018；Vaswani 等人，2017 年）。 我们认为，需要更强大的、结构性的、归纳性偏差来推进时间序列插补的最新技术水平，并允许在大型和复杂传感器网络的背景下构建有效的推理引擎，就像在现实世界应用中发现的那样。 在这项工作中，我们将输入多元时间序列建模为图序列，其中边表示不同通道之间的关系。 我们提出图神经网络 (GNN) (Scarselli et al., 2008; Bronstein et al., 2017; Battaglia et al., 2018) 作为用于多元时间序列插补 (MTSI) 的新型双向循环神经网络的构建块）。 我们的方法名为 Graph Recurrent Imputation Network (GRIN)，其核心是一个循环神经网络单元，其中的门由消息传递神经网络 (MPNNs; Gilmer et al., 2017) 实现。 其中两个网络在每个节点处处理前向和后向时间方向的输入多元时间序列，而隐藏状态由消息传递插补层处理，该插补层受限于通过查看相邻节点来学习如何执行插补。 事实上，通过将每条边视为限制在相应节点观察到的值的软函数依赖，我们认为在图的上下文中操作会为 MTSI 引入正归纳偏差。 我们的贡献是多方面的： 我们引入了一个方法框架来在 MTSI 的背景下利用图神经网络， 我们为 MTSI 提出了一种新颖、实用且有效的基于 GNN 的架构实现，以及 我们在多个不同的 MTSI 基准上取得了最先进的结果。 我们的方法不依赖于对缺失值分布的任何假设（例如，瞬态动态的存在和持续时间和/或缺失序列的长度），而不是基础过程的平稳性。 本文的其余部分安排如下。在第 2 节中，我们讨论了相关的工作。然后，在第 3 节中，我们正式介绍了问题设置和 MTSI 的任务。我们在第 4 节中介绍了我们的 MTSI 方法，通过描述实现基于 GNN 的插补架构的新框架。我们在第 5 节中对所提出的方法与最先进的基线进行实证评估，最后，我们在第 6 节中得出结论。 ","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:2:0","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Graduation project"],"content":"相关工作 时间序列插补 存在大量文献解决时间序列中的缺失值插补。除了基于多项式曲线拟合的简单和标准插值方法外，流行的方法旨在利用标准预测方法和时间序列之间的相似性来填补缺失值。 例如，有几种方法依赖于 k 近邻（Troyanskaya 等人，2001；Beretta 和 Santaniello，2016）、期望最大化算法（Ghahramani 和 Jordan，1994；Nelwamondo 等人，2007）或线性预测器和状态空间模型（Durbin 和 Koopman，2012 年；Kihoro 等人，2013 年）。低秩逼近方法，例如矩阵分解 (Cichocki \u0026 Phan, 2009)，也是可以解释空间 (Cai et al., 2010; Rao et al., 2015) 和时间 (Yu et al. , 2016; Mei et al., 2017) 信息。在线性方法中，STMVL (Yi et al., 2016) 结合时间和空间插值来填补地理标记时间序列中的缺失值。 最近，已经为 MTSI 提出了几种深度学习方法。其中，基于循环神经网络 (RNN) 的深度自回归方法获得了广泛的成功（Lipton 等人，2016；Che 等人，2018；Luo 等人，2018；Yoon 等人，2018b；Cao 等人., 2018)。 GRU-D (Che et al., 2018) 学习如何通过控制门控 RNN 隐藏状态的衰减来处理缺失数据的序列。**曹等人。 (2018)**提出了 BRITS，这是一种双向 GRU-D-like RNN，用于多变量时间序列插补，考虑到不同通道之间的相关性以执行空间插补。 文献中提出了其他成功的策略，即利用对抗性训练框架生成逼真的重建序列（Yoon 等人，2018a；Fedus 等人，2018；Luo 等人，2018；2019）。值得注意的是，GAIN (Yoon et al., 2018a) 使用 GANs (Goodfellow et al., 2014) 来学习在 i.i.d. 中执行插补的模型设置。**罗等人。 （2018 年；2019 年）**的目标是学习生成真实合成序列并利用它们来填充缺失值的模型。苗等人。 (2021) 使用类似于 GAIN 的方法，但将生成器设置在目标不完整时间序列的预测标签上。 在我们工作的同时，**Kuppannagari 等人。 （2021）**开发了一种基于图的时空去噪自动编码器，用于来自具有已知拓扑的智能电网的时空数据。**刘等人。相反，(2019)**使用对抗性学习来训练多尺度模型，该模型以分层方式估算高度稀疏的时间序列。然而，我们认为上述方法都不能充分利用关系信息和非线性时空依赖性。 最重要的是，上述方法并没有充分利用在图处理上下文中操作所带来的灵活性和表现力。 Figure 1: Representation of a multivariate time series as a sequence of graphs. Red circles denotenodes with missing values, nodes are identified.图 1：将多元时间序列表示为图形序列。 红色圆圈表示具有缺失值的节点，节点已被识别。 用于 TSA 的图神经网络 图神经网络已在 TSA 中主要用于时空预测方法。文献中存在的大多数方法背后的想法是通过依赖在图域中工作的运算符来修改用于顺序数据的标准神经网络架构。例如，**Seo 等人。 （2018 年）**提出了一个 GRU 单元，其中门由光谱 GNN 实现（Defferrard 等人，2016 年）；**李等人。 （2018 年）**提出了一种类似的架构，用扩散卷积网络代替光谱 GNN（Atwood 和 Towsley，2016 年）。 请注意，这些模型是不同的 w.r.t。使用循环网络以图形方式传播信息的方法（Scarselli 等人，2008；Li 等人，2016）。**于等人。 （2017）和吴等人。 （2019；2020b）**提出了时空卷积神经网络，它在时间和空间维度上交替卷积。 在基于注意力的模型（Vaswani 等人，2017 年）和时空类 Transformer 架构（Zhang 等人，2018 年；Cai 等人，2020 年）的背景下，也研究了类似的方法。另一个特别有趣的研究方向与学习输入多元时间序列的图结构问题有关（Kipf 等人，2018；Wuet 等人，2020b；Shang 等人，2020）。 虽然前面提到的方法侧重于多变量时间序列预测，但其他方法旨在预测图拓扑的变化（Zambon 等人，2019；Paassen 等人，2020）。相反，时间图网络 (Rossi et al.,2020) 等方法专门用于学习动态图中的节点嵌入。 最后，最近的工作提出了在 i.i.d 的上下文中估算缺失特征的 GNN。数据。其中，**斯皮内利等人。 （2020）**提出了一个对抗性框架来训练 GNN 的数据重建任务，而 **You 等人。 （2020）**提出了一种用于特征插补的二部图表示。 最近，GNN 也被用于空间插值（Appleby 等人，2020；Wu 等人，2020a）——有时也称为克里金法（Stein，1999）。据我们所知，以前没有基于 GNN 的方法针对通用多元时间序列的缺失值插补。 ","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:3:0","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Graduation project"],"content":"预备 图的序列 我们考虑加权有向图序列，其中我们在每个时间步 t 观察具有 $N_t$ 个节点的图 $g_t$。 一个图是一对 $g_t = \u003cX_t,W_t\u003e$，其中 $X_t ∈ R^{N_t×d}$ 是节点属性矩阵，其第 i 行包含与第 i 个节点关联的 d 维节点属性向量 $x^i_t ∈R^d$； 邻接矩阵 $W_t ∈ R^{N_t×N_t}$ 的条目 $w^{i,j}_t$ 表示连接第 i 个和第 j 个节点的边（如果有）的标量权重。 图 1 举例说明了这个建模框架。 我们假设节点被识别，即具有唯一的 ID，可以实现时间一致的处理。 这个问题设置可以很容易地扩展到具有属性边和全局属性的更一般的图类。 在这项工作中，我们主要关注图的拓扑是固定的并且不随时间变化的问题，即在每个时间步 $W_t = W$ 和 $N_t = N$。 通过让序列的每个通道（即每个传感器）对应一个节点并使用可用的关系信息来构建邻接矩阵，任何通用的多变量时间序列都符合上述框架。 如果没有可用的关系信息，可以使用单位矩阵，但这会违背公式的目的。可以使用任何标准相似度得分（例如，Pearson 相关性）或（阈值）内核来更合适地选择 Wt。 相反，更高级的方法可以通过使用空间注意力分数或求助于图学习技术（例如，Kipf 等人）直接从数据中学习邻接。 （2018 年）。 从现在开始，我们假设输入的多元时间序列具有同质通道，即传感器属于同一类型。请注意，此假设并不意味着一般性的损失：通过添加传感器类型属性和附加维度以适应不同类型的传感器读数，始终可以标准化节点特征。或者，可以通过利用异构图直接对问题进行建模（Schlichtkrull 等人，2018 年）。 多元时间序列插补 为了对缺失值的存在进行建模，我们在每一步都考虑一个二进制掩码 $M_t ∈ {0, 1}^{N_t×d}$，其中每一行 $m^i_t$ 表示 $ 的哪些对应节点属性x^i_t$ 在 $X_t$ 中可用。因此，$m^{i,j}_t = 0$ 意味着 $x^{i,j}_t$ 缺失； 相反，如果 $m^{i,j}_t = 1$，则 $x^{i,j}_t$ 存储实际传感器读数。我们用 $\\widetilde{\\boldsymbol{X}}_t$ 表示 未知的地面实况节点属性矩阵，即没有任何缺失数据的完整节点属性矩阵。 我们假设缺失数据分布的平稳性，并且在实验中，我们主要关注随机缺失（MAR）场景（Rubin，1976）。我们既不假设并发传感器故障的数量，也不假设丢失数据块的长度，即考虑到随时间延长的多个故障。 显然，人们应该期望插补性能与并发故障的数量和丢失数据突发的时间长度成比例。 MTSI 的目标是在输入数据序列中估算缺失值。 更正式地说，给定一个长度为 T 的图序列 $G_{[t,t+T]}$，我们可以将缺失数据重构误差定义为 $$ \\mathcal{L}\\left(\\widehat{\\boldsymbol{X}}{[t, t+T]}, \\widetilde{\\boldsymbol{X}}{[t, t+T]}, \\overline{\\boldsymbol{M}}{[t, t+T]}\\right)=\\sum{h=t}^{t+T} \\sum_{i=1}^{N_t} \\frac{\\left\\langle\\overline{\\boldsymbol{m}}_h^i, \\ell\\left(\\hat{\\boldsymbol{x}}_h^i, \\tilde{\\boldsymbol{x}}_h^i\\right)\\right\\rangle}{\\left\\langle\\overline{\\boldsymbol{m}}_h^i, \\overline{\\boldsymbol{m}}_h^i\\right\\rangle} $$ 其中$\\hat{\\boldsymbol{x}}h^i$是重构的$\\tilde{\\boldsymbol{x}}h^i$； $\\overline{\\boldsymbol{M}}{[t, t+T]}$ 和 $\\overline{\\boldsymbol{m}}h^i$ 分别是 ${\\boldsymbol{M} 的逻辑二进制补码}{[t, t+T]}$ 和 ${\\boldsymbol{m}}h^i$, l( · , · ) 是元素级误差函数（例如，绝对误差或平方误差）并且 \u003c · , ·\u003e 表示标准点积。 请注意，在实践中，不可能访问 $\\widetilde{\\boldsymbol{X}}{[t, t+T]}$，因此，有必要通过以下方式定义代理优化目标，例如，使用预测损失或生成合成缺失值。在可训练、参数化、插补方法的背景下，我们考虑了两种不同的操作设置。 在第一个名为 in-sample imputation 的方法中，模型被训练以重建给定固定输入序列 ${\\boldsymbol{X}}{[t, t+T]}$ 中的缺失值 ，即模型在所有可用数据上进行训练，除了那些缺失的数据和那些已从序列中删除的数据以模拟额外的失败 评估。 不同的是，在第二个（称为样本外插补）中，模型是在不相交的序列上训练和评估的。 请注意，在这两种情况下，模型都无法访问用于最终评估的真实数据。第一个操作设置模拟了从业者直接在序列上拟合模型以填补其空白的情况。相反，第二种模拟了希望使用拟合一组历史数据的模型来估算看不见的目标序列中的缺失值的情况。 ","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:4:0","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Graduation project"],"content":"4 图形循环插补网络 在本节中，我们将介绍我们的方法，即图递归插补网络 (GRIN)，这是一种用于 MTSI 的基于图的递归神经架构。 给定一个多元时间序列 ${\\boldsymbol{X}}{[t, t+T]}$ 使用掩码 ${\\boldsymbol{M}}{[t, t+T]}$ ，我们的目标是通过结合来自时间和空间维度的信息来重建输入序列中的缺失值。 为此，我们设计了一种新颖的双向图循环神经网络，它逐步处理输入序列 通过对每个方向执行两个阶段的插补，在时间上向前和向后。 然后，前馈网络将前向和后向模型学习的表示作为输入，并对图的每个节点和序列的步骤执行最终的 - 细化 - 插补。 更准确地说，最终的插补取决于两个 GRIN 模块的输出，它们的学习表示最终由最后一个解码多层感知器 (MLP) 处理（空间和时间方面）。 图 2 给出了完整架构的概述。如图所示，这两个模块迭代地估算缺失值，在每个时间步使用先前估算的值作为输入。我们首先详细描述单向模型，然后提供双向扩展。 单向模型 每个 GRIN 模块由两个块组成，一个时空编码器和一个空间解码器，它们分两个阶段处理图的输入序列。 时空编码器将输入序列 ${\\boldsymbol{X}}{[t, t+T]}$ 映射到时空表示 $\\boldsymbol{H}{[t, t+T]} \\in \\mathbb{R}^{N_t \\times l}$ 通过利用特别设计的循环 GNN。 相反，空间解码器利用学习的表示来执行两轮连续的插补。 通过使用线性读数从表示中获得第一阶段的插补； 第二个利用时间步 t 处可用的关系、空间信息。 特别是，解码器由 MPNN 实现，该 MPNN 学习推断每个第 i 个节点的观察值 - $x^i_t$ - 通过细化第一阶段插补考虑 - 本地 - $H_{t−1}$ 和值 在相邻节点上观察到。 图 2：双向架构概述。 在这里，每个单向 GRIN 模块正在处理具有 4 维（传感器）的输入序列的第 τ 步。 在考虑的时间步长处缺少两个值。 GRIN 执行第一次插补，然后由空间解码器处理和细化。 然后使用这些第二阶段的插补继续下一步的处理。 MLP 处理学习的表示节点和时间，以获得最终的插补。 时空编码器 在编码器中，依次处理输入序列${\\boldsymbol{X}}{[t, t+T]}$和掩码${\\boldsymbol{M}}{[t, t+T]}$ 一次一步，通过带有由消息传递层实现的门的循环神经网络。 原则上可以使用任何消息传递运算符。 特别是，给定 $z^i_{t,k−1} ，即第 k−1 层的节点特征向量，我们认为 MPNN 的一般类描述为 $$ \\operatorname{MPNN}k\\left(\\boldsymbol{z}{t, k-1}^i, \\boldsymbol{W}t\\right)=\\gamma_k\\left(\\boldsymbol{z}{t, k-1}^i, \\sum_{j \\in \\mathcal{N}(i)} \\rho_k\\left(\\boldsymbol{z}{t, k-1}^i, \\boldsymbol{z}{t, k-1}^j\\right)\\right)=\\boldsymbol{z}_{t, k}^i $$ 其中 $\\mathcal{N}(i)$ 是 $g_t$ 中第 i 个节点的邻居集合，$γ_k$ 和 $ρ_k$ 是通用、可微分、更新和消息函数（例如 MLP），并且Σ 是一个置换不变、可微的聚合函数（例如，总和或均值）。请注意，邻域的几种定义是可能的，例如，可以考虑由最长为一定长度 l 的路径连接的节点。为了简单起见，从现在开始，我们用 MPNN($z^i_t$,$W_t$) 表示通用 K 层消息传递神经网络的前向传递。在下文中，我们使用 MPNN 作为时空特征提取器的构建块。为了学习系统的动态，我们利用门控循环单元（GRUs；Cho et al., 2014）。如前所述，类似于 Seo 等人。 （2018）和李等人。 （2018），我们依靠上面定义的消息传递层来实现 GRU 门。在节点级别，消息传递 GRU (MPGRU) 的元素可以描述为： $$ \\begin{aligned} \\boldsymbol{r}_t^i \u0026=\\sigma\\left(\\operatorname{MPNN}\\left(\\left[\\hat{\\boldsymbol{x}}_t^{i(2)}\\left|\\boldsymbol{m}t^i\\right| \\boldsymbol{h}{t-1}^i\\right], \\boldsymbol{W}_t\\right)\\right) (3)\\ \\boldsymbol{u}_t^i \u0026=\\sigma\\left(\\operatorname{MPNN}\\left(\\left[\\hat{\\boldsymbol{x}}_t^{i(2)}\\left|\\boldsymbol{m}t^i\\right| \\boldsymbol{h}{t-1}^i\\right], \\boldsymbol{W}_t\\right)\\right) (4)\\ \\boldsymbol{c}_t^i \u0026=\\tanh \\left(\\operatorname{MPNN}\\left(\\left[\\hat{\\boldsymbol{x}}_t^{i(2)}\\left|\\boldsymbol{m}_t^i\\right| \\boldsymbol{r}t^i \\odot \\boldsymbol{h}{t-1}^i\\right], \\boldsymbol{W}_t\\right)\\right) (5)\\ \\boldsymbol{h}_t^i \u0026=\\boldsymbol{u}t^i \\odot \\boldsymbol{h}{t-1}^i+\\left(1-\\boldsymbol{u}_t^i\\right) \\odot \\boldsymbol{c}_t^i (6) \\end{aligned} $$ 其中 $r^i_t$ ,$u^i_t$ 分别是重置门和更新门，$h^i_t$ 是第 i 个节点在时间 t 的隐藏表示，$\\boldsymbol{x}t^ {i(2)}$ 是解码块在前一个时间步的输出（见下一段）。 符号 $\\odot$ 和 || 分别表示 Hadamard 积和串联运算符。 初始表示 $H{t−1}$ 可以初始化为常数或具有可学习的嵌入。 请注意，对于缺少输入数据的步骤，编码器会收到来自解码器块的预测，如下一小节所述。 通过执行上述计算时间和节点，我们得到编码序列$H_{[t,t+T]}$。 空间解码器 作为第一个解码步骤，我们通过线性读出从 MPGRU 的隐藏表示中生成单步预测 $$ \\widehat{\\boldsymbol{Y}}t^{(1)}=\\boldsymbol{H}{t-1} \\boldsymbol{V}_h+\\boldsymbol{b}_h $$ 其中 $V_h \\in \\mathbb{R}^{l \\times d}$ 是一个可学习的权重矩阵， $b_h \\in \\mathbb{R}^{d}$ 是一个可学习的偏置向量。 然后我们将填充运算符定义为 $$ \\Phi\\left(\\boldsymbol{Y}_t\\right)=\\boldsymbol{M}_t \\odot \\boldsymbol{X}_t+\\overline{\\boldsymbol{M}}_t \\odot \\boldsymbol{Y}_t (8) $$ 直观地，填充运算符将输入 $X_t$ 中的缺失值替换为 $Y_t$ 中相同位置的值。 通过将 $\\widehat{\\boldsymbol{Y}}_t^{(1)}$ 提供给填充算子，我们得到第一阶段插补 $\\widehat{\\boldsymbol{X}}_t^{(1)}$，使得 输出是 $X_t$ ，缺失值替换为一步前预测 $\\widehat{\\boldsymbol{Y}}t^{(1)}$ 。 然后将得到的节点级预测连接到掩码 $M_t$ 和隐藏表示 $H{t−1}$，并由最终的一层 MPNN 处理，该 MPNN 为每个节点计算一个插补表示 $s^i_t$ 作为 $$ \\boldsymbol{s}t^i=\\gamma\\left(\\boldsymbol{h}{t-1}^i, \\sum_{j \\in \\mathcal{N}(i) / i} \\rho\\left(\\left[\\Phi\\left(\\hat{\\boldsymbol{x}}t^{j(1)}\\right)\\left|\\boldsymbol{h}{t-1}^j\\right| \\boldsymbol{m}_t^j\\right]\\right)\\right) $$ 请注意，如前所述，插补表示仅取决于从相邻节点接收到的消息和上一步的表示。 事实上，通过仅聚合来自一跳邻域的消息，表示 $s^i_t$ 独立于第 i 个节点本身的输入特征 $x^i_t$。 这种约束迫使模型通过考虑空间依赖性来学习如何重建目标输入：这具有正则化效果，因为模型被约束为专注于局部信息。 之后，我们将插补表示 $S_t$ 与隐藏表示 $H_{t−1}$ 连接起来，并通过使用第二个线性读数并应用填充算子来生成第二阶段插补 $$ \\widehat{\\boldsymbol{Y}}_t^{(2)}=\\left[\\boldsymbol{S}t | \\boldsymbol{H}{t-1}\\right] \\boldsymbol{V}_s+\\boldsymbol{b}_s ; \\quad \\wi","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:5:0","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Graduation project"],"content":"5 实证评估 在本节中，我们在来自三个相关应用领域的四个数据集上根据最先进的基线经验评估我们的方法。 我们的方法显着地在所有这些上实现了最先进的性能。 空气质量 (AQI)：来自中国 43 个城市的 437 个监测站的多项空气质量指数记录数据集。 我们只考虑 PM2.5 污染物。 先前的估算工作（Yi et al., 2016; Cao et al., 2018）考虑了该数据集的简化版本，仅包括 36 个传感器（以下为 AQI-36）。 我们在两个数据集上评估我们的模型。 我们使用从成对地理距离计算的阈值高斯核 (Shuman et al., 2013) 作为邻接矩阵。 表 1：空气数据集的结果。 5 次运行的平均性能 交通：我们考虑来自 Li 等人的 PEMS-BAY 和 METR-LA 数据集。 (2018)，包含来自旧金山湾区和洛杉矶县高速公路的交通传感器的数据。 我们使用与 Li 等人相同的方法。 （2018）和吴等人。 （2019）获得邻接矩阵。 智能电网：我们考虑来自爱尔兰能源监管委员会智能计量项目（CER-E；能源监管委员会，2016 年）的数据。 我们仅选择监控中小型企业 (SME) 能源消耗的可用智能电表的子集，即每 30 分钟采集一次样本的 485 个时间序列。 我们通过从通过计算时间序列之间的相关熵 (Liu et al., 2007) 构建的相似性矩阵中提取 k 最近邻图（k = 10）来构建邻接矩阵。 对于空气质量数据集，我们采用与之前工作相同的评估协议（Yi et al., 2016; Cao et al., 2018），我们展示了样本内和样本外设置的结果。对于交通和能源消耗数据集，我们仅考虑样本外场景（矩阵分解仅在样本内有效）。我们通过考虑 2 个不同的设置来模拟缺失数据的存在：1) 块缺失，即在每个步骤中，对于每个传感器，我们随机丢弃 5% 的可用数据，此外，我们以概率 pfailure = 模拟故障0.15% 并在区间 [min steps, max steps] 中均匀采样其持续时间，其中 min steps 和 max steps 是交通情况下分别对应于 1 和 4 小时以及 CER 的 2 小时和 2 天的时间步数- E; 2）点缺失，即我们只是随机屏蔽掉 25% 的可用数据。我们将所有数据集拆分为训练/验证/测试集。我们使用在插补窗口上计算的平均绝对误差 (MAE)、均方误差 (MSE) 和平均相对误差 (MRE; Cao et al., 2018) 作为性能指标。对于所有实验，我们使用 Atwood \u0026 Towsley (2016) 引入的扩散卷积作为消息传递算子。我们认为 BRITS (Cao et al., 2018) 是非对抗性深度自回归方法中的主要竞争替代方案，因为它与我们的方法具有架构相似性。作为额外的基线，我们考虑：1）MEAN，即使用节点级平均值进行插补； 2) KNN，即通过对邻接矩阵 Wt 中权重最高的 k = 10 个相邻节点的平均值进行插补； 3) MICE (White et al., 2011)，将最大迭代次数限制为 100，将最近特征数限制为 10； 4) 秩 = 10 的矩阵分解 (MF)； 5) VAR，即向量自回归单步预测器； 6) rGAIN，即 SSGAN (Miao et al., 2021) 的无监督版本，可以看作是具有双向循环编码器和解码器的 GAIN (Yoon et al., 2018a)； 7) MPGRU，一种基于 GNN 的单步预测器，类似于 DCRNN (Li et al., 2018)。 表 2：交通和智能电网数据集的结果。 5 次运行的平均性能 我们提供了关于基线和数据集的进一步评论和深入细节，以及附录中关于合成数据的额外实验。 ","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:6:0","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Graduation project"],"content":"5.1 结果 实证结果表明，GRIN 可以在多种情况下大幅提高插补性能，并提高灵活性。事实上，与其他 state-of-theart 基线不同，GRIN 可以处理具有可变维数的输入。标签。图 1 显示了空气质量数据集的实验结果。在样本内设置中，我们使用通过对所有重叠窗口的平均预测获得的值作为插补来计算指标；相反，在样本外设置中，我们只是通过平均窗口上的误差来报告结果。 GRIN 在这两种设置上都大大优于其他基线。特别是在后一种情况下，GRIN 会降低 MAE w.r.t。 AQI 中最接近的基线超过 20%。有趣的是，对于与孤立（断开）节点相对应的传感器，GRIN 在估算缺失值方面的表现始终优于 BRITS，即，与任何其他站点相距 40 公里以上的站点对应的节点（见 B.1）：这是积极的经验证据编码为 GRIN 的正则化。我们的方法在 36 维数据集中也实现了更准确的插补，我们可以预期图形表示的影响较小。交通和智能电网数据集的结果显示在选项卡中。 2. 在流量数据集中，我们的方法在所有考虑的设置中都大大优于 BRITS 和 rGAIN，同时使用的参数数量要少得多（参见 A）。在交通数据集中，平均而言，GRIN 将 MAE 降低了 ≈ 29% w.r.t。 BRITS，特别是在 PEMS-BAY 数据集的点缺失设置中，误差减半。在 CER-E 中，GRIN 始终优于其他基线。除了展示我们的方法在相关应用领域的有效性之外，该实验还表明 GRIN 可以在关系信息不易获得的环境中被利用。 最后，标签。图 3 显示了 AQI、METR-LA（在块缺失设置中）和 CER-E（点缺失设置）中样本外场景的消融研究结果 - 就 MAE 而言。特别是，我们将 GRIN 与 3 个基线进行比较，以评估空间解码器和双向架构的影响。第一个基线本质上是一个双向 MPGRU，其中值由最终 MLP 估算，将 h fwd t−1 和 h bwd t+1 作为输入，而第二个基线具有类似的架构，但使用隐藏表示和时间步 t（对于两个方向），因此，其行为类似于去噪自动编码器。作为参考，我们报告了单向 MPGRU 的结果。结果表明，我们引入的组件确实有助于显着降低插补误差。很明显，空间解码和双向架构对于获得准确的缺失数据重建很重要，尤其是在具有缺失数据块的现实环境中。有趣的是，去噪模型在 Block Missing 场景中受到影响，而正如人们所预料的那样，在 Point Missing 设置中效果很好。有关可扩展性问题的其他结果和讨论，请参阅本文的附录 表 3：消融研究。 平均超过 5 次运行。 ","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:6:1","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Graduation project"],"content":"5.2 虚拟传感 作为最后的实验，我们对所提出的虚拟传感方法进行了定量和定性评估。这个想法（通常在克里金法的背景下研究——见第 2 节）是通过添加一个没有可用数据的节点来模拟传感器的存在，然后让模型重建相应的时间序列。请注意，该方法的工作需要几个假设：1）我们必须假设被监控的物理量可以从相邻传感器的观察中重建； 2）我们应该假设传感器的高度同质性（例如，在空气质量站的情况下，我们应该假设传感器放置在相同的高度）或表征每个相邻传感器的特征（例如，放置）是可用的到模型。在这种情况下，值得注意的是，由于模型中嵌入了归纳偏差，GRIN 不仅通过最小化单个节点的重建误差来执行重建，而且通过对相邻传感器处的插补的重建值进行正则化。我们掩盖了 AQI-36 的两个节点的观测值，具有最高（1014 号站）和最低（1031 号）连通性，并像往常一样在数据的剩余部分上训练 GRIN。结果，在图 3 中，定性地表明 GRIN 可以推断出看不见的传感器的趋势和规模。在 MAE 方面，传感器 1014 的 GRIN 得分为 11.74，传感器 1031 的得分为 20.00（5 次独立运行的平均值）。 图 3：重建从训练集中移除的传感器的观察结果。 图表显示 GRIN 可能用于虚拟传感。 ","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:6:2","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Graduation project"],"content":"6 结论 我们介绍了 GRIN，这是一种利用现代图神经网络的 MTSI 新方法。 我们的方法通过利用表征传感器底层网络的关系信息以及它们之间的功能依赖关系来估算丢失的数据。 与最先进的基线相比，我们的框架提供了更高的灵活性，并在所有考虑的场景中实现了更好的重建精度。 未来的工作有几个可能的方向。 从理论的角度来看，研究能够保证准确重建的特性会很有趣。 此外，未来的工作应该研究能够处理非静止环境的扩展，并进一步评估 GRIN 在虚拟和主动传感中的应用。 本文提供的重现实验的代码与配置文件一起作为补充材料提供，以复制报告的结果。 除 CER-E 外，所有数据集都是开放的，补充材料中提供了下载链接。 CER-E 数据集可以免费获得用于研究目的（见附录）。 对于模拟失败的实验，我们使用具有固定种子的随机数生成器来生成缺失数据，以确保实验和基线之间的可重复性和一致性。 ","date":"2022-05-13 09:33:30","objectID":"/sameonlychinese/:7:0","tags":["papers"],"title":"图神经网络的多元时间序列插补","uri":"/sameonlychinese/"},{"categories":["Go"],"content":" https://go.dev/doc/effective_go ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:0:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"引言 Go 是一门全新的语言。尽管它从既有的语言中借鉴了许多理念，但其与众不同的特性，使得用 Go 编程在本质上就不同于其它语言。将现有的 C++ 或 Java 程序直译为 Go 程序并不能令人满意——毕竟 Java 程序是用 Java 编写的，而不是 Go。 另一方面，若从 Go 的角度去分析问题，你就能编写出同样可行但大不相同的程序。 换句话说，要想将 Go 程序写得好，就必须理解其特性和风格。了解命名、格式化、程序结构等既定规则也同样重要，这样你编写的程序才能更容易被其他程序员所理解。 本文档就如何编写清晰、地道的 Go 代码提供了一些技巧。它是对 语言规范、 Go 语言之旅 以及 如何使用 Go 编程 的补充说明，因此我们建议您先阅读这些文档。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:1:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"格式化 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:2:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"注释 Go 语言支持 C 风格的块注释 /* */ 和 C++ 风格的行注释 //。 行注释更为常用，而块注释则主要用作包的注释，当然也可在禁用一大段代码时使用。 每个包都应包含一段包注释，即放置在包子句前的一个块注释。对于包含多个文件的包， 包注释只需出现在其中的任一文件中即可。包注释应在整体上对该包进行介绍，并提供包的相关信息。 它将出现在 godoc 页面中的最上面，并为紧随其后的内容建立详细的文档。 /* Package regexp implements a simple library for regular expressions. The syntax of the regular expressions accepted is: regexp: concatenation { '|' concatenation } concatenation: { closure } closure: term [ '*' | '+' | '?' ] term: '^' '$' '.' character '[' [ '^' ] character-ranges ']' '(' regexp ')' */ package regexp 注释无需进行额外的格式化，如用星号来突出等。生成的输出甚至可能无法以等宽字体显示， 因此不要依赖于空格对齐，godoc 会像 gofmt 那样处理好这一切。 注释是不会被解析的纯文本，因此像 HTML 或其它类似于 这样 的东西将按照 原样 输出，因此不应使用它们。godoc 所做的调整， 就是将已缩进的文本以等宽字体显示，来适应对应的程序片段。 在包中，任何顶级声明前面的注释都将作为该声明的文档注释。 在程序中，每个可导出（首字母大写）的名称都应该有文档注释。 若注释总是以名称开头，godoc 的输出就能通过 grep 变得更加有用。假如你记不住 “Compile” 这个名称，而又在找正则表达式的解析函数， 那就可以运行 $ godoc regexp | grep parse 若包中的所有文档注释都以 “此函数…” 开头，grep 就无法帮你记住此名称。 但由于每个包的文档注释都以其名称开头，你就能看到这样的内容，它能显示你正在寻找的词语。 Go的声明语法允许成组声明。单个文档注释应介绍一组相关的常量或变量。 由于是整体声明，这种注释往往较为笼统。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:3:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"命名 当一个包被导入后，包名就会成了内容的访问器。在之后，被导入的包就能通过 bytes.Buffer 来引用了。 若所有人都能以相同的名称来引用其内容，这将大有裨益，因此，包应当有个恰当的名称：其名称应该简洁明了而易于理解。按照惯例， 包应当以小写的单个单词来命名，且不应使用下划线或驼峰记法。err 的命名就是出于简短考虑的，因为任何使用该包的人都会键入该名称。 不必担心引用次序的冲突。包名就是导入时所需的唯一默认名称， 它并不需要在所有源码中保持唯一，即便在少数发生冲突的情况下， 也可为导入的包选择一个别名来局部使用。 无论如何，通过文件名来判定使用的包，都是不会产生混淆的。 包的导入者可通过包名来引用其内容，因此包中的可导出名称可以此来避免冲突。 （请勿使用 import . 记法，它可以简化必须在被测试包外运行的测试， 除此之外应尽量避免使用。）例如，bufio 包中的缓存读取器类型叫做 Reader 而非 BufReader，因为用户将它看做 bufio.Reader，这是个清楚而简洁的名称。 此外，由于被导入的项总是通过它们的包名来确定，因此 bufio.Reader 不会与 io.Reader 发生冲突。同样，用于创建 ring.Ring 的新实例的函数（这就是 Go 中的构造函数）一般会称之为 NewRing，但由于 Ring 是该包所导出的唯一类型，且该包也叫 ring，因此它可以只叫做 New，它跟在包的后面，就像 ring.New。使用包结构可以帮助你选择好的名称。 另一个简短的例子是 once.Do，once.Do(setup) 表述足够清晰， 使用 once.DoOrWaitUntilDone(setup) 完全就是画蛇添足。 长命名并不会使其更具可读性。一份有用的说明文档通常比额外的长名更有价值。 Go 并不对获取器（getter）和设置器（setter）提供自动支持。 你应当自己提供获取器和设置器，通常很值得这样做，但若要将 Get 放到获取器的名字中，既不符合习惯，也没有必要。若你有个名为 owner （小写，未导出）的字段，其获取器应当名为 Owner（大写，可导出）而非 GetOwner。大写字母即为可导出的这种规定为区分方法和字段提供了便利。 若要提供设置器方法，SetOwner 是个不错的选择。两个命名看起来都很合理： owner := obj.Owner() if owner != user { obj.SetOwner(user) } 接口名： 按照约定，只包含一个方法的接口应当以该方法的名称加上 - er 后缀来命名，如 Reader、Writer、 Formatter、CloseNotifier 等。 诸如此类的命名有很多，遵循它们及其代表的函数名会让事情变得简单。 Read、Write、Close、Flush、 String 等都具有典型的签名和意义。为避免冲突，请不要用这些名称为你的方法命名， 除非你明确知道它们的签名和意义相同。反之，若你的类型实现了的方法， 与一个众所周知的类型的方法拥有相同的含义，那就使用相同的命名。 请将字符串转换方法命名为 String 而非 ToString。 最后，Go 中约定使用驼峰记法 MixedCaps 或 mixedCaps 而非下划线的方式来对多单词名称进行命名。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:4:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"分号 和 C 一样，Go 的正式语法使用分号来结束语句；和 C 不同的是，这些分号并不在源码中出现。 取而代之，词法分析器会使用一条简单的规则来自动插入分号，因此源码中基本就不用分号了。 规则是这样的：若在新行前的最后一个标记为标识符（包括 int 和 float64 这类的单词）、数值或字符串常量之类的基本字面或以下标记之一 break continue fallthrough return ++ -- ) } 则词法分析将始终在该标记后面插入分号。这点可以概括为： “如果新行前的最后一个标记可以结束该段语句，则插入分号”。 分号也可在闭合的大括号之前直接省略，因此像 go func() { for { dst \u003c- \u003c-src } }() 这样的语句无需分号。通常Go程序只在诸如 for 循环子句这样的地方使用分号， 以此来将初始化器、条件及增量元素分开。如果你在一行中写多个语句，也需要用分号隔开。 警告：无论如何，你都不应将一个控制结构（if、for、switch 或 select）的左大括号放在下一行。如果这样做，就会在大括号前面插入一个分号，这可能引起不需要的效果。 你应该这样写 if i \u003c f() { g() } 而不是 if i \u003c f() // wrong! { // wrong! g() } ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:5:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"控制结构 Go 中的结构控制与 C 有许多相似之处，但其不同之处才是独到之处。 Go 不再使用 do 或 while 循环，只有一个更通用的 for；switch 要更灵活一点；if 和 switch 像 for 一样可接受可选的初始化语句； 此外，还有一个包含类型选择和多路通信复用器的新控制结构：select。 其语法也有些许不同：没有圆括号，而其主体必须始终使用大括号括住。 由于 if 和 switch 可接受初始化语句， 因此用它们来设置局部变量十分常见。 在 Go 的库中，你会发现若 if 语句不会执行到下一条语句时，亦即其执行体 以 break、continue、goto 或 return 结束时，不必要的 else 会被省略。由于出错时将以 return 结束， 之后的代码也就无需 else 了。 重复声明： 在满足下列条件时，已被声明的变量 v 可出现在:= 声明中： 本次声明与已声明的 v 处于同一作用域中（若 v 已在外层作用域中声明过，则此次声明会创建一个新的变量 §）， 在初始化中与其类型相应的值才能赋予 v，且在此次声明中至少另有一个变量是新声明的。 Go 的 for 循环类似于 C，但却不尽相同。它统一了 for 和 while，不再有 do-while 了。它有三种形式，但只有一种需要分号。 若你想遍历数组、切片、字符串或者映射，或从信道中读取消息， range 子句能够帮你轻松实现循环。 对于字符串，range 能够提供更多便利。它能通过解析 UTF-8， 将每个独立的 Unicode 码点分离出来。错误的编码将占用一个字节，并以符文 U+FFFD 来代替。 （名称 “符文” 和内建类型 rune 是 Go 对单个 Unicode 码点的称谓。 详情见语言规范）。循环 for pos, char := range \"日本 \\ x80 語\" { // \\x80 is an illegal UTF-8 encoding fmt.Printf(\"character %#U starts at byte position %d\\n\", char, pos) } character U+65E5 '日' starts at byte position 0 character U+672C '本' starts at byte position 3 character U+FFFD '�' starts at byte position 6 character U+8A9E '語' starts at byte position 7 最后，Go 没有逗号操作符，而 ++ 和 – 为语句而非表达式。 因此，若你想要在 for 中使用多个变量，应采用平行赋值的方式 （因为它会拒绝 ++ 和 –）. Go 的 switch 比 C 的更通用。其表达式无需为常量或整数，case 语句会自上而下逐一进行求值直到匹配为止。若 switch 后面没有表达式，它将匹配 true，因此，我们可以将 if-else-if-else 链写成一个 switch，这也更符合 Go 的风格。 func unhex(c byte) byte { switch { case '0' \u003c= c \u0026\u0026 c \u003c= '9': return c - '0' case 'a' \u003c= c \u0026\u0026 c \u003c= 'f': return c - 'a' + 10 case 'A' \u003c= c \u0026\u0026 c \u003c= 'F': return c - 'A' + 10 } return 0 } switch 并不会自动下溯(fallthrough)，但 case 可通过逗号分隔来列举相同的处理条件。 func shouldEscape(c byte) bool { switch c { case ' ', '?', '\u0026', '=', '#', '+', '%': return true } return false } 尽管它们在 Go 中的用法和其它类 C 语言差不多，但 break 语句可以使 switch 提前终止。不仅是 switch， 有时候也必须打破层层的循环。在 Go 中，我们只需将标签放置到循环外，然后 “蹦” 到那里即可。下面的例子展示了二者的用法。 Loop: for n := 0; n \u003c len(src); n += size { switch { case src[n] \u003c sizeOne: if validateOnly { break } size = 1 update(src[n]) case src[n] \u003c sizeTwo: if n+1 \u003e= len(src) { err = errShortInput break Loop } if validateOnly { break } size = 2 update(src[n] + src[n+1]\u003c\u003cshift) } } 当然，continue 语句也能接受一个可选的标签，不过它只能在循环中使用。 类型选择 var t interface{} t = functionOfSomeType() switch t := t.(type) { default: fmt.Printf(\"unexpected type %T\", t) // %T prints whatever type t has case bool: fmt.Printf(\"boolean %t\\n\", t) // t has type bool case int: fmt.Printf(\"integer %d\\n\", t) // t has type int case *bool: fmt.Printf(\"pointer to boolean %t\\n\", *t) // t has type *bool case *int: fmt.Printf(\"pointer to integer %d\\n\", *t) // t has type *int } switch 也可用于判断接口变量的动态类型。如 类型选择 通过圆括号中的关键字 type 使用类型断言语法。若 switch 在表达式中声明了一个变量，那么该变量的每个子句中都将有该变量对应的类型。在这些 case 中重用一个名字也是符合语义的，实际上是在每个 case 里声明了一个不同类型但同名的新变量。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:6:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"函数 Go 与众不同的特性之一就是函数和方法可返回多个值。这种形式可以改善 C 中一些笨拙的习惯： 将错误值返回（例如用 -1 表示 EOF）和修改通过地址传入的实参。 在 C 中，写入操作发生的错误会用一个负数标记，而错误码会隐藏在某个不确定的位置。 而在 Go 中，Write 会返回写入的字节数以及一个错误： “是的，您写入了一些字节，但并未全部写入，因为设备已满”。 在 os 包中，File.Write 的签名为： func (file *File) Write(b []byte) (n int, err error) 正如文档所述，它返回写入的字节数，并在 n != len(b) 时返回一个非 nil 的 error 错误值。 这是一种常见的编码风格，更多示例见错误处理一节。 我们可以采用一种简单的方法。来避免为模拟引用参数而传入指针。 以下简单的函数可从字节数组中的特定位置获取其值，并返回该数值和下一个位置。 func nextInt(b []byte, i int) (int, int) { for ; i \u003c len(b) \u0026\u0026 !isDigit(b[i]); i++ { } x := 0 for ; i \u003c len(b) \u0026\u0026 isDigit(b[i]); i++ { x = x*10 + int(b[i]) - '0' } return x, i } 你可以像下面这样，通过它扫描输入的切片 b 来获取数字。 for i := 0; i \u003c len(b); { x, i = nextInt(b, i) fmt.Println(x) } 可命名结果形参 Go 函数的返回值或结果 “形参” 可被命名，并作为常规变量使用，就像传入的形参一样。 命名后，一旦该函数开始执行，它们就会被初始化为与其类型相应的零值； 若该函数执行了一条不带实参的 return 语句，则结果形参的当前值将被返回。 此名称不是强制性的，但它们能使代码更加简短清晰：它们就是文档。若我们命名了 nextInt 的结果，那么它返回的 int 就值如其意了。 func nextInt(b []byte, pos int) (value, nextPos int) { 由于被命名的结果已经初始化，且已经关联至无参数的返回，它们就能让代码简单而清晰。 下面的 io.ReadFull 就是个很好的例子： func ReadFull(r Reader, buf []byte) (n int, err error) { for len(buf) \u003e 0 \u0026\u0026 err == nil { var nr int nr, err = r.Read(buf) n += nr buf = buf[nr:] } return } Defer Go 的 defer 语句用于预设一个函数调用（即推迟执行函数）， 该函数会在执行 defer 的函数返回之前立即执行。它显得非比寻常， 但却是处理一些事情的有效方式，例如无论以何种路径返回，都必须释放资源的函数。 典型的例子就是解锁互斥和关闭文件。 // Contents returns the file's contents as a string. func Contents(filename string) (string, error) { f, err := os.Open(filename) if err != nil { return \"\", err } defer f.Close() // f.Close will run when we're finished. var result []byte buf := make([]byte, 100) for { n, err := f.Read(buf[0:]) result = append(result, buf[0:n]...) // append is discussed later. if err != nil { if err == io.EOF { break } return \"\", err // f will be closed if we return here. } } return string(result), nil // f will be closed if we return here. } 推迟诸如 Close 之类的函数调用有两点好处：第一， 它能确保你不会忘记关闭文件。如果你以后又为该函数添加了新的返回路径时， 这种情况往往就会发生。第二，它意味着 “关闭” 离 “打开” 很近， 这总比将它放在函数结尾处要清晰明了。 被推迟函数的实参（如果该函数为方法则还包括接收者）在推迟执行时就会被求值， 而不是在调用执行时才求值。这样不仅无需担心变量值在函数执行时被改变， 同时还意味着单个被推迟的调用可推迟多个函数的执行。下面是个简单的例子。 for i := 0; i \u003c 5; i++ { defer fmt.Printf(\"%d \", i) } 被推迟的函数按照后进先出（LIFO）的顺序执行，因此以上代码在函数返回时会打印 4 3 2 1 0。一个更具实际意义的例子是通过一种简单的方法， 用程序来跟踪函数的执行。我们可以编写一对简单的跟踪例程： func trace(s string) { fmt.Println(\"entering:\", s) } func untrace(s string) { fmt.Println(\"leaving:\", s) } // Use them like this: func a() { trace(\"a\") defer untrace(\"a\") // do something.... } 我们可以充分利用这个特点，即被推迟函数的实参在 defer 执行时就会被求值。 跟踪例程可针对反跟踪例程设置实参。以下例子： func trace(s string) string { fmt.Println(\"entering:\", s) return s } func un(s string) { fmt.Println(\"leaving:\", s) } func a() { defer un(trace(\"a\")) fmt.Println(\"in a\") } func b() { defer un(trace(\"b\")) fmt.Println(\"in b\") a() } func main() { b() } entering: b in b entering: a in a leaving: a leaving: b 对于习惯其它语言中块级资源管理的程序员，defer 似乎有点怪异， 但它最有趣而强大的应用恰恰来自于其基于函数而非块的特点。在 panic 和 recover 这两节中，我们将看到关于它可能性的其它例子。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:7:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"数据 new分配 Go 提供了两种分配原语，即内建函数 new 和 make。 它们所做的事情不同，所应用的类型也不同。它们可能会引起混淆，但规则却很简单。 让我们先来看看 new。这是个用来分配内存的内建函数， 但与其它语言中的同名函数不同，它不会初始化内存，只会将内存置零。 也就是说，new(T) 会为类型为 T 的新项分配已置零的内存空间， 并返回它的地址，也就是一个类型为 *T 的值。用 Go 的术语来说，它返回一个指针， 该指针指向新分配的，类型为 T 的零值。 既然 new 返回的内存已置零，那么当你设计数据结构时， 每种类型的零值就不必进一步初始化了，这意味着该数据结构的使用者只需用 new 创建一个新的对象就能正常工作。例如，bytes.Buffer 的文档中提到 “零值的 Buffer 就是已准备就绪的缓冲区。\" 同样，sync.Mutex 并没有显式的构造函数或 Init 方法， 而是零值的 sync.Mutex 就已经被定义为已解锁的互斥锁了。 “零值属性” 是传递性的。考虑以下类型声明。 type SyncedBuffer struct { lock sync.Mutex buffer bytes.Buffer } SyncedBuffer 类型的值也是在声明时就分配好内存就绪了。后续代码中， p 和 v 无需进一步处理即可正确工作。 p := new(SyncedBuffer) // type *SyncedBuffer var v SyncedBuffer // type SyncedBuffer 构造函数和复合字面量 有时零值还不够好，这时就需要一个初始化构造函数，如来自 os 包中的这段代码所示。 func NewFile(fd int, name string) *File { if fd \u003c 0 { return nil } f := new(File) f.fd = fd f.name = name f.dirinfo = nil f.nepipe = 0 return f } 这里显得代码过于冗长。我们可通过复合字面量来简化它， 该表达式在每次求值时都会创建新的实例。 func NewFile(fd int, name string) *File { if fd \u003c 0 { return nil } f := File{fd, name, nil, 0} return \u0026f } 请注意，返回一个局部变量的地址完全没有问题，这点与 C 不同。该局部变量对应的数据 在函数返回后依然有效。实际上，每当获取一个复合字面量的地址时，都将为一个新的实例分配内存， 因此我们可以将上面的最后两行代码合并： return \u0026File{fd, name, nil, 0} 复合字面量的字段必须按顺序全部列出。但如果以 字段: 值 对的形式明确地标出元素，初始化字段时就可以按任何顺序出现，未给出的字段值将赋予零值，推荐列出键值对。 因此，我们可以用如下形式： return \u0026File{fd: fd, name: name} 少数情况下，若复合字面量不包括任何字段，它将创建该类型的零值。表达式 new(File) 和 \u0026File{} 是等价的。 复合字面量同样可用于创建数组、切片以及映射，字段标签是索引还是映射键则视情况而定。 在下例初始化过程中，无论 Enone、Eio 和 Einval 的值是什么，只要它们的标签不同就行。 make分配 再回到内存分配上来。内建函数 make(T, args) 的目的不同于 new(T)。它只用于创建切片、映射和信道，并返回类型为 T（而非 *T）的一个已初始化 （而非置零）的值。 出现这种差异的原因在于，这三种类型本质上为引用数据类型，它们在使用前必须初始化。 例如，切片是一个具有三项内容的描述符，包含一个指向（数组内部）数据的指针、长度以及容量， 在这三项被初始化之前，该切片为 nil。对于切片、映射和信道，make 用于初始化其内部的数据结构并准备好将要使用的值。例如， make([]int, 10, 100) 会分配一个具有 100 个 int 的数组空间，接着创建一个长度为 10， 容量为 100 并指向该数组中前 10 个元素的切片结构。（生成切片时，其容量可以省略，更多信息见切片一节。） 与此相反，new([]int) 会返回一个指向新分配的，已置零的切片结构， 即一个指向 nil 切片值的指针。 下面的例子阐明了 new 和 make 之间的区别： var p *[]int = new([]int) // allocates slice structure; *p == nil; rarely useful var v []int = make([]int, 100) // the slice v now refers to a new array of 100 ints // Unnecessarily complex: var p *[]int = new([]int) *p = make([]int, 100, 100) // Idiomatic: v := make([]int, 100) 请记住，make 只适用于映射、切片和信道且不返回指针。若要获得明确的指针， 请使用 new 分配内存或显式地获取一个变量的地址。 数组 在详细规划内存布局时，数组是非常有用的，有时还能避免过多的内存分配， 但它们主要用作切片的构件。这是下一节的主题了，不过要先说上几句来为它做铺垫。 以下为数组在 Go 和 C 中的主要区别。在 Go 中， 数组是值。将一个数组赋予另一个数组会复制其所有元素。 特别地，若将某个数组传入某个函数，它将接收到该数组的一份副本而非指针。 数组的大小是其类型的一部分。类型 [10]int 和 [20]int 是不同的。 数组为值的属性很有用，但代价高昂；若你想要 C 那样的行为和效率，你可以传递一个指向该数组的指针。 func Sum(a *[3]float64) (sum float64) { for _, v := range *a { sum += v } return } array := [...]float64{7.0, 8.5, 9.1} x := Sum(\u0026array) // Note the explicit address-of operator 但这并不是 Go 的习惯用法，切片才是。 切片 切片通过对数组进行封装，为数据序列提供了更通用、强大而方便的接口。 除了矩阵变换这类需要明确维度的情况外，Go 中的大部分数组编程都是通过切片来完成的。 切片保存了对底层数组的引用，若你将某个切片赋予另一个切片，它们会引用同一个数组。 若某个函数将一个切片作为参数传入，则它对该切片元素的修改对调用者而言同样可见， 这可以理解为传递了底层数组的指针。因此，Read 函数可接受一个切片实参 而非一个指针和一个计数；切片的长度决定了可读取数据的上限。以下为 os 包中 File 类型的 Read 方法签名: func (file *File) Read(buf []byte) (n int, err error) 该方法返回读取的字节数和一个错误值（若有的话）。若要从更大的缓冲区 b 中读取前 32 个字节，只需对其进行切片即可。 n, err := f.Read(buf[0:32]) 这种切片的方法常用且高效。若不谈效率，以下片段同样能读取该缓冲区的前 32 个字节。 var n int var err error for i := 0; i \u003c 32; i++ { nbytes, e := f.Read(buf[i:i+1]) // Read one byte. if nbytes == 0 || e != nil { err = e break } n += nbytes } 只要切片不超出底层数组的限制，它的长度就是可变的，只需将它赋予其自身的切片即可。 切片的容量可通过内建函数 cap 获得，它将给出该切片可取得的最大长度。 以下是将数据追加到切片的函数。若数据超出其容量，则会重新分配该切片。返回值即为所得的切片。 该函数中所使用的 len 和 cap 在应用于 nil 切片时是合法的，它会返回 0. func Append(slice, data[]byte) []byte { l := len(slice) if l + len(data) \u003e cap(slice) { // reallocate // Allocate double what's needed, for future growth. newSlice := make([]byte, (l+len(data))*2) // The copy function is predeclared and works for any slice type. copy(newSlice, slice) slice = newSlice } slice = slice[0:l+len(data)] for i, c := range data { slice[l+i] = c } return slice } 最终我们必须返回切片，因为尽管 Append 可修改 slice 的元素，但切片自身（其运行时数据结构包含指针、长度和容量）是通过值传递的。 向切片追加东西的想法非常有用，因此有专门的内建函数 append。 要理解该函数的设计，我们还需要一些额外的信息，我们将稍后再介绍它。 二维切片","date":"2022-04-30 09:22:50","objectID":"/effective_go/:8:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"初始化 尽管从表面上看，Go 的初始化过程与 C 或 C++ 相比并无太大差别，但它确实更为强大。 在初始化过程中，不仅可以构建复杂的结构，还能正确处理不同包对象间的初始化顺序。 常量 Go 中的常量就是不变量。它们在编译时创建，即便它们可能是函数中定义的局部变量。 常量只能是数字、字符（符文）、字符串或布尔值。由于编译时的限制， 定义它们的表达式必须也是可被编译器求值的常量表达式。例如 1«3 就是一个常量表达式，而 math.Sin(math.Pi/4) 则不是，因为对 math.Sin 的函数调用在运行时才会发生。 在 Go 中，枚举常量使用枚举器 iota 创建。由于 iota 可为表达式的一部分，而表达式可以被隐式地重复，这样也就更容易构建复杂的值的集合了。 type ByteSize float64 const ( // 通过赋予空白标识符来忽略第一个值 _ = iota // ignore first value by assigning to blank identifier KB ByteSize = 1 \u003c\u003c (10 * iota) MB GB TB PB EB ZB YB ) 由于可将 String 之类的方法附加在用户定义的类型上， 因此它就为打印时自动格式化任意值提供了可能性，即便是作为一个通用类型的一部分。 尽管你常常会看到这种技术应用于结构体，但它对于像 ByteSize 之类的浮点数标量等类型也是有用的。 func (b ByteSize) String() string { switch { case b \u003e= YB: return fmt.Sprintf(\"%.2fYB\", b/YB) case b \u003e= ZB: return fmt.Sprintf(\"%.2fZB\", b/ZB) case b \u003e= EB: return fmt.Sprintf(\"%.2fEB\", b/EB) case b \u003e= PB: return fmt.Sprintf(\"%.2fPB\", b/PB) case b \u003e= TB: return fmt.Sprintf(\"%.2fTB\", b/TB) case b \u003e= GB: return fmt.Sprintf(\"%.2fGB\", b/GB) case b \u003e= MB: return fmt.Sprintf(\"%.2fMB\", b/MB) case b \u003e= KB: return fmt.Sprintf(\"%.2fKB\", b/KB) } return fmt.Sprintf(\"%.2fB\", b) } 表达式 YB 会打印出 1.00YB，而 ByteSize(1e13) 则会打印出 9.09TB。 在这里用 Sprintf 实现 ByteSize 的 String 方法很安全（不会无限递归），这倒不是因为类型转换，而是它以 %f 调用了 Sprintf，它并不是一种字符串格式：Sprintf 只会在它需要字符串时才调用 String 方法，而 %f 需要一个浮点数值。 变量 变量的初始化与常量类似，但其初始值也可以是在运行时才被计算的一般表达式。 var ( home = os.Getenv(\"HOME\") user = os.Getenv(\"USER\") gopath = os.Getenv(\"GOPATH\") ) init 函数 最后，每个源文件都可以通过定义自己的无参数 init 函数来设置一些必要的状态。 （其实每个文件都可以拥有多个 init 函数。）而它的结束就意味着初始化结束： 只有该包中的所有变量声明都通过它们的初始化器求值后 init 才会被调用， 而那些 init 只有在所有已导入的包都被初始化后才会被求值。 除了那些不能被表示成声明的初始化外，init 函数还常被用在程序真正开始执行前，检验或校正程序的状态。 func init() { if user == \"\" { log.Fatal(\"$USER not set\") } if home == \"\" { home = \"/home/\" + user } if gopath == \"\" { gopath = home + \"/go\" } // gopath may be overridden by --gopath flag on command line. flag.StringVar(\u0026gopath, \"gopath\", gopath, \"override default GOPATH\") } ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:9:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"方法 指针vs值 正如 ByteSize 那样，我们可以为任何已命名的类型（除了指针或接口）定义方法； 接收者可不必为结构体。 在之前讨论切片时，我们编写了一个 Append 函数。 我们也可将其定义为切片的方法。为此，我们首先要声明一个已命名的类型来绑定该方法， 然后使该方法的接收者成为该类型的值。 type ByteSlice []byte func (slice ByteSlice) Append(data []byte) []byte { // Body exactly the same as above } 我们仍然需要该方法返回更新后的切片。为了消除这种不便，我们可通过重新定义该方法， 将一个指向 ByteSlice 的指针作为该方法的接收者， 这样该方法就能重写调用者提供的切片了。 func (p *ByteSlice) Append(data []byte) { slice := *p // Body as above, without the return. *p = slice } 其实我们做得更好。若我们将函数修改为与标准 Write 类似的方法，就像这样， func (p *ByteSlice) Write(data []byte) (n int, err error) { slice := *p // Again as above. *p = slice return len(data), nil } 那么类型 *ByteSlice 就满足了标准的 io.Writer 接口，这将非常实用。 例如，我们可以通过打印将内容写入。 var b ByteSlice fmt.Fprintf(\u0026b, \"This hour has %d days\\n\", 7) 我们将 ByteSlice 的地址传入，因为只有 *ByteSlice 才满足 io.Writer。以指针或值为接收者的区别在于：值方法可通过指针和值调用， 而指针方法只能通过指针来调用。 之所以会有这条规则是因为指针方法可以修改接收者；通过值调用它们会导致方法接收到该值的副本， 因此任何修改都将被丢弃，因此该语言不允许这种错误。不过有个方便的例外：若该值是可寻址的， 那么该语言就会自动插入取址操作符来对付一般的通过值调用的指针方法。在我们的例子中，变量 b 是可寻址的，因此我们只需通过 b.Write 来调用它的 Write 方法，编译器会将它重写为 (\u0026b).Write。 顺便一提，在字节切片上使用 Write 的想法已被 bytes.Buffer 所实现。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:10:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"接口与其他类型 接口 Go 中的接口为指定对象的行为提供了一种方法：如果某样东西可以完成这个， 那么它就可以用在这里。我们已经见过许多简单的示例了；通过实现 String 方法，我们可以自定义打印函数，而通过 Write 方法，Fprintf 则能对任何对象产生输出。在 Go 代码中， 仅包含一两种方法的接口很常见，且其名称通常来自于实现它的方法， 如 io.Writer 就是实现了 Write 的一类对象。 每种类型都能实现多个接口。例如一个实现了 sort.Interface 接口的集合就可通过 sort 包中的例程进行排序。该接口包括 Len()、Less(i, j int) bool 以及 Swap(i, j int)，另外，该集合仍然可以有一个自定义的格式化器。 以下特意构建的例子 Sequence 就同时满足这两种情况。 type Sequence []int // Methods required by sort.Interface. // sort.Interface 所需的方法。 func (s Sequence) Len() int { return len(s) } func (s Sequence) Less(i, j int) bool { return s[i] \u003c s[j] } func (s Sequence) Swap(i, j int) { s[i], s[j] = s[j], s[i] } // Method for printing - sorts the elements before printing. // 用于打印的方法 - 在打印前对元素进行排序。 func (s Sequence) String() string { sort.Sort(s) str := \"[\" for i, elem := range s { if i \u003e 0 { str += \" \" } str += fmt.Sprint(elem) } return str + \"]\" } 类型转换 Sequence 的 String 方法重新实现了 Sprint 为切片实现的功能。若我们在调用 Sprint 之前将 Sequence 转换为纯粹的 []int，就能共享已实现的功能。 func (s Sequence) String() string { sort.Sort(s) return fmt.Sprint([]int(s)) } 该方法是通过类型转换技术，在 String 方法中安全调用 Sprintf 的另个一例子。若我们忽略类型名的话，这两种类型（Sequence 和 []int）其实是相同的，因此在二者之间进行转换是合法的。 转换过程并不会创建新值，它只是暂时让现有的值看起来有个新类型而已。 （还有些合法转换则会创建新值，如从整数转换为浮点数等。） 在 Go 程序中，为访问不同的方法集而进行类型转换的情况非常常见。 例如，我们可使用现有的 sort.IntSlice 类型来简化整个示例： type Sequence []int // Method for printing - sorts the elements before printing func (s Sequence) String() string { sort.IntSlice(s).Sort() return fmt.Sprint([]int(s)) } 现在，不必让 Sequence 实现多个接口（排序和打印）， 我们可通过将数据条目转换为多种类型（Sequence、sort.IntSlice 和 []int）来使用相应的功能，每次转换都完成一部分工作。 这在实践中虽然有些不同寻常，但往往却很有效。 接口转换与类型断言 类型选择 是类型转换的一种形式：它接受一个接口，在选择 （switch）中根据其判断选择对应的情况（case）， 并在某种意义上将其转换为该种类型。以下代码为 fmt.Printf 通过类型选择将值转换为字符串的简化版。若它已经为字符串，我们需要该接口中实际的字符串值； 若它有 String 方法，我们则需要调用该方法所得的结果。 type Stringer interface { String() string } var value interface{} // Value provided by caller. switch str := value.(type) { case string: return str case Stringer: return str.String() } 第一种情况获取具体的值，第二种将该接口转换为另一个接口。这种方式对于混合类型来说非常完美。 若我们只关心一种类型呢？若我们知道该值拥有一个 string 而想要提取它呢？ 只需一种情况的类型选择就行，但它需要类型断言。类型断言接受一个接口值， 并从中提取指定的明确类型的值。其语法借鉴自类型选择开头的子句，但它需要一个明确的类型， 而非 type 关键字： value.(typeName) 而其结果则是拥有静态类型 typeName 的新值。该类型必须为该接口所拥有的具体类型， 或者该值可转换成的第二种接口类型。要提取我们知道在该值中的字符串，可以这样： str := value.(string) 但若它所转换的值中不包含字符串，该程序就会以运行时错误崩溃。为避免这种情况， 需使用 “逗号, ok” 惯用法来测试它能安全地判断该值是否为字符串： str, ok := value.(string) if ok { fmt.Printf(\"string value is: %q\\n\", str) } else { fmt.Printf(\"value is not a string\\n\") } 若类型断言失败，str 将继续存在且为字符串类型，但它将拥有零值，即空字符串。 作为对这种能力的说明，这里有个 if-else 语句，它等价于本节开头的类型选择。 if str, ok := value.(string); ok { return str } else if str, ok := value.(Stringer); ok { return str.String() } 通用性 若某种现有的类型仅实现了一个接口，且除此之外并无可导出的方法，则该类型本身就无需导出。 仅导出该接口能让我们更专注于其行为而非实现，其它属性不同的实现则能反映该原始类型的行为。 这也能够避免为每个通用接口的实例重复编写文档。 在这种情况下，构造函数应当返回一个接口值而非实现的类型。例如在 hash 库中，crc32.NewIEEE 和 adler32.New 都返回接口类型 hash.Hash32。要在 Go 程序中用 Adler-32 算法替代 CRC-32， 只需修改构造函数调用即可，其余代码则不受算法改变的影响。 同样的方式能将 crypto 包中多种联系在一起的流密码算法与块密码算法分开。 crypto/cipher 包中的 Block 接口指定了块密码算法的行为， 它为单独的数据块提供加密。接着，和 bufio 包类似，任何实现了该接口的密码包都能被用于构造以 Stream 为接口表示的流密码，而无需知道块加密的细节。 crypto/cipher 接口看其来就像这样： type Block interface { BlockSize() int Encrypt(src, dst []byte) Decrypt(src, dst []byte) } type Stream interface { XORKeyStream(dst, src []byte) } 这是计数器模式 CTR 流的定义，它将块加密改为流加密，注意块加密的细节已被抽象化了。 // NewCTR returns a Stream that encrypts/decrypts using the given Block in // counter mode. The length of iv must be the same as the Block's block size. func NewCTR(block Block, iv []byte) Stream NewCTR 的应用并不仅限于特定的加密算法和数据源，它适用于任何对 Block 接口和 Stream 的实现。因为它们返回接口值， 所以用其它加密模式来代替 CTR 只需做局部的更改。构造函数的调用过程必须被修改， 但由于其周围的代码只能将它看做 Stream，因此它们不会注意到其中的区别。 接口和方法 由于几乎任何类型都能添加方法，因此几乎任何类型都能满足一个接口。一个很直观的例子就是 http 包中定义的 Handler 接口。任何实现了 Handler 的对象都能够处理 HTTP 请求。 type Handler interface { ServeHTTP(ResponseWriter, *Request) } ResponseWriter 接口提供了对方法的访问，这些方法需要响应客户端的请求。 由于这些方法包含了标准的 Write 方法，因此 http.ResponseWriter 可用于任何 io.Writer 适用的场景。Request 结构体包含已解析的客户端请求。 为简单起见，我们假设所有的 HTTP 请求都是 GET 方法，而忽略 POST 方法， 这种简化不会影响处理程序的建立方式。这里有个短小却完整的处理程序实现， 它用","date":"2022-04-30 09:22:50","objectID":"/effective_go/:11:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"空白标识符 我们在 for-range 循环和 映射 中提过几次空白标识符。 空白标识符可被赋予或声明为任何类型的任何值，而其值会被无害地丢弃。它有点像 Unix 中的 /dev/null 文件：它表示只写的值，在需要变量但不需要实际值的地方用作占位符。 我们在前面已经见过它的用法了。 多重赋值中的空白标识符 for range 循环中对空白标识符的用法是一种具体情况，更一般的情况即为多重赋值。 若某次赋值需要匹配多个左值，但其中某个变量不会被程序使用， 那么用空白标识符来代替该变量可避免创建无用的变量，并能清楚地表明该值将被丢弃。 例如，当调用某个函数时，它会返回一个值和一个错误，但只有错误很重要， 那么可使用空白标识符来丢弃无关的值。 if _, err := os.Stat(path); os.IsNotExist(err) { fmt.Printf(\"%s does not exist\\n\", path) } 你偶尔会看见为忽略错误而丢弃错误值的代码，这是种糟糕的实践。请务必检查错误返回， 它们会提供错误的理由。 // Bad! This code will crash if path does not exist. fi, _ := os.Stat(path) if fi.IsDir() { fmt.Printf(\"%s is a directory\\n\", path) } 未使用的导入和变量 若导入某个包或声明某个变量而不使用它就会产生错误。未使用的包会让程序膨胀并拖慢编译速度， 而已初始化但未使用的变量不仅会浪费计算能力，还有可能暗藏着更大的 Bug。 然而在程序开发过程中，经常会产生未使用的导入和变量。虽然以后会用到它们， 但为了完成编译又不得不删除它们才行，这很让人烦恼。空白标识符就能提供一个临时解决方案。 这个写了一半的程序有两个未使用的导入（fmt 和 io）以及一个未使用的变量（fd），因此它不能编译， 但若到目前为止代码还是正确的，我们还是很乐意看到它们的。 package main import ( \"fmt\" \"io\" \"log\" \"os\" ) func main() { fd, err := os.Open(\"test.go\") if err != nil { log.Fatal(err) } // TODO: use fd. } 要让编译器停止关于未使用导入的抱怨，需要空白标识符来引用已导入包中的符号。 同样，将未使用的变量 fd 赋予空白标识符也能关闭未使用变量错误。 该程序的以下版本可以编译。 package main import ( \"fmt\" \"io\" \"log\" \"os\" ) var _ = fmt.Printf // For debugging; delete when done. // 用于调试，结束时删除。 var _ io.Reader // For debugging; delete when done. // 用于调试，结束时删除。 func main() { fd, err := os.Open(\"test.go\") if err != nil { log.Fatal(err) } // TODO: use fd. _ = fd } 按照惯例，我们应在导入并加以注释后，再使全局声明导入错误静默，这样可以让它们更易找到， 并作为以后清理它的提醒。 为副作用而导入 像前例中 fmt 或 io 这种未使用的导入总应在最后被使用或移除： 空白赋值会将代码标识为工作正在进行中。但有时导入某个包只是为了其副作用， 而没有任何明确的使用。例如，在 net/http/pprof 包的 init 函数中记录了 HTTP 处理程序的调试信息。它有个可导出的 API， 但大部分客户端只需要该处理程序的记录和通过 Web 页面访问数据。欲导入一个只使用其副作用的包， 只需将该包重命名为空白标识符： import _ \"net/http/pprof\" 这种导入格式能明确表示该包是为其副作用而导入的，因为没有其它使用该包的可能： 在此文件中，它没有名字。（若它有名字而我们没有使用，编译器就会拒绝该程序。） 接口检查 就像我们在前面 接口 中讨论的那样， 一个类型无需显式地声明它实现了某个接口。取而代之，该类型只要实现了某个接口的方法， 其实就实现了该接口。在实践中，大部分接口转换都是静态的，因此会在编译时检测。 例如，将一个 *os.File 传入一个接收 io.Reader 的函数将不会被编译， 除非 *os.File 实现了 io.Reader 接口。 尽管如此，有些接口检查会在运行时进行。例如，encoding/json 包定义了一个 Marshaler 接口。当 JSON 编码器接收到一个实现了该接口的值，那么该编码器就会调用该值的编组方法， 将其转换为 JSON，而非进行标准的类型转换。 编码器在运行时通过 类型断言 检查其属性，就像这样： m, ok := val.(json.Marshaler) 若只需要判断某个类型是否是实现了某个接口，而不需要实际使用接口本身 （可能是错误检查部分），就使用空白标识符来忽略类型断言的值： if _, ok := val.(json.Marshaler); ok { fmt.Printf(\"value %v of type %T implements json.Marshaler\\n\", val, val) } 当需要确保某个包中实现的类型一定满足该接口时，就会遇到这种情况。 若某个类型（例如 json.RawMessage） 需要一种定制的 JSON 表现时，它应当实现 json.Marshaler， 不过现在没有静态转换可以让编译器去自动验证它。若该类型通过忽略转换失败来满足该接口， 那么 JSON 编码器仍可工作，但它却不会使用定制的实现。为确保其实现正确， 可在该包中用空白标识符声明一个全局变量： var _ json.Marshaler = (*RawMessage)(nil) 在此声明中，我们调用了一个 *RawMessage 转换并将其赋予了 Marshaler，以此来要求 *RawMessage 实现 Marshaler，这时其属性就会在编译时被检测。 若 json.Marshaler 接口被更改，此包将无法通过编译， 而我们则会注意到它需要更新。 在这种结构中出现空白标识符，即表示该声明的存在只是为了类型检查。 不过请不要为满足接口就将它用于任何类型。作为约定， 仅当代码中不存在静态类型转换时才能这种声明，毕竟这是种罕见的情况。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:12:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"内嵌 Go 并不提供典型的，类型驱动的子类化概念，但通过将类型内嵌到结构体或接口中， 它就能 “借鉴” 部分实现。 接口内嵌非常简单。我们之前提到过 io.Reader 和 io.Writer 接口，这里是它们的定义。 type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } io 包也导出了一些其它接口，以此来阐明对象所需实现的方法。 例如 io.ReadWriter 就是个包含 Read 和 Write 的接口。我们可以通过显示地列出这两个方法来指明 io.ReadWriter， 但通过将这两个接口内嵌到新的接口中显然更容易且更具启发性，就像这样： // ReadWriter is the interface that combines the Reader and Writer interfaces. type ReadWriter interface { Reader Writer } 正如它看起来那样：ReadWriter 能够做任何 Reader 和 Writer 可以做到的事情，它是内嵌接口的联合体 （它们必须是不相交的方法集）。只有接口能被嵌入到接口中。 同样的基本想法可以应用在结构体中，但其意义更加深远。bufio 包中有 bufio.Reader 和 bufio.Writer 这两个结构体类型， 它们每一个都实现了与 io 包中相同意义的接口。此外，bufio 还通过结合 reader/writer 并将其内嵌到结构体中，实现了带缓冲的 reader/writer：它在结构体中列出了这些类型，但并未给予它们字段名。 // ReadWriter stores pointers to a Reader and a Writer. // It implements io.ReadWriter. type ReadWriter struct { *Reader // *bufio.Reader *Writer // *bufio.Writer } 内嵌的元素为指向结构体的指针，当然它们在使用前必须被初始化为指向有效结构体的指针。 ReadWriter 结构体可通过如下方式定义： type ReadWriter struct { reader *Reader writer *Writer } 但为了提升该字段的方法并满足 io 接口，我们同样需要提供转发的方法， 就像这样： func (rw *ReadWriter) Read(p []byte) (n int, err error) { return rw.reader.Read(p) } 而通过直接内嵌结构体，我们就能避免如此繁琐。 内嵌类型的方法可以直接引用，这意味着 bufio.ReadWriter 不仅包括 bufio.Reader 和 bufio.Writer 的方法，它还同时满足下列三个接口： io.Reader、io.Writer 以及 io.ReadWriter。 还有种区分内嵌与子类的重要手段。当内嵌一个类型时，该类型的方法会成为外部类型的方法， 但当它们被调用时，该方法的接收者是内部类型，而非外部的。在我们的例子中，当 bufio.ReadWriter 的 Read 方法被调用时， 它与之前写的转发方法具有同样的效果；接收者是 ReadWriter 的 reader 字段，而非 ReadWriter 本身。 内嵌同样可以提供便利。这个例子展示了一个内嵌字段和一个常规的命名字段。 type Job struct { Command string *log.Logger } Job 类型现在有了 Log、Logf 和 *log.Logger 的其它方法。我们当然可以为 Logger 提供一个字段名，但完全不必这么做。现在，一旦初始化后，我们就能记录 Job 了： job.Log(\"starting now...\") Logger 是 Job 结构体的常规字段， 因此我们可在 Job 的构造函数中，通过一般的方式来初始化它，就像这样： func NewJob(command string, logger *log.Logger) *Job { return \u0026Job{command, logger} } 或通过复合字面： job := \u0026Job{command, log.New(os.Stderr, \"Job: \", log.Ldate)} 若我们需要直接引用内嵌字段，可以忽略包限定名，直接将该字段的类型名作为字段名， 就像我们在 ReaderWriter 结构体的 Read 方法中做的那样。 若我们需要访问 Job 类型的变量 job 的 *log.Logger， 可以直接写作 job.Logger。若我们想精炼 Logger 的方法时， 这会非常有用。 func (job *Job) Logf(format string, args ...interface{}) { job.Logger.Logf(\"%q: %s\", job.Command, fmt.Sprintf(format, args...)) } 内嵌类型会引入命名冲突的问题，但解决规则却很简单。首先，字段或方法 X 会隐藏该类型中更深层嵌套的其它项 X。若 log.Logger 包含一个名为 Command 的字段或方法，Job 的 Command 字段会覆盖它。 其次，若相同的嵌套层级上出现同名冲突，通常会产生一个错误。若 Job 结构体中包含名为 Logger 的字段或方法，再将 log.Logger 内嵌到其中的话就会产生错误。然而，若重名永远不会在该类型定义之外的程序中使用，那就不会出错。 这种限定能够在外部嵌套类型发生修改时提供某种保护。 因此，就算添加的字段与另一个子类型中的字段相冲突，只要这两个相同的字段永远不会被使用就没问题。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:13:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"并发 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"通过通信共享内存 并发编程是个很大的论题。但限于篇幅，这里仅讨论一些 Go 特有的东西。 在并发编程中，为实现对共享变量的正确访问需要精确的控制，这在多数环境下都很困难。 Go 语言另辟蹊径，它将共享的值通过信道传递，实际上，多个独立执行的线程从不会主动共享。 在任意给定的时间点，只有一个 goroutine 能够访问该值。数据竞争从设计上就被杜绝了。 为了提倡这种思考方式，我们将它简化为一句口号： 不要通过共享内存来通信，而应通过通信来共享内存。 这种方法意义深远。例如，引用计数通过为整数变量添加互斥锁来很好地实现。 但作为一种高级方法，通过信道来控制访问能够让你写出更简洁，正确的程序。 我们可以从典型的单线程运行在单 CPU 之上的情形来审视这种模型。它无需提供同步原语。 现在再运行一个线程，它也无需同步。现在让它们俩进行通信。若将通信过程看做同步着， 那就完全不需要其它同步了。例如，Unix 管道就与这种模型完美契合。 尽管 Go 的并发处理方式来源于 Hoare 的通信顺序处理（CSP）， 它依然可以看做是类型安全的 Unix 管道的实现。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:1","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Goroutines 我们称之为 ** goroutine ** ，是因为现有的术语—线程、协程、进程等等—无法准确传达它的含义。 Goroutine 具有简单的模型：它是与其它 goroutine 并发运行在同一地址空间的函数。它是轻量级的， 所有消耗几乎就只有栈空间的分配。而且栈最开始是非常小的，所以它们很廉价， 仅在需要时才会随着堆空间的分配（和释放）而变化。 Goroutine 在多线程操作系统上可实现多路复用，因此若一个线程阻塞，比如说等待 I/O， 那么其它的线程就会运行。Goroutine 的设计隐藏了线程创建和管理的诸多复杂性。 在函数或方法前添加 go 关键字能够在新的 goroutine 中调用它。当调用完成后， 该 goroutine 也会安静地退出。（效果有点像 Unix Shell 中的 \u0026 符号，它能让命令在后台运行。） go list.Sort() // 并发运行 list.Sort，无需等它结束。 函数字面在 goroutine 调用中非常有用。 func Announce(message string, delay time.Duration) { go func() { time.Sleep(delay) fmt.Println(message) }() // Note the parentheses - must call the function. } 在 Go 中，函数字面都是闭包：其实现在保证了函数内引用变量的生命周期与函数的活动时间相同。 这些函数没什么实用性，因为它们没有实现完成时的信号处理。因此，我们需要信道。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:2","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Channels 信道与映射一样，也需要通过 make 来分配内存。其结果值充当了对底层数据结构的引用。 若提供了一个可选的整数形参，它就会为该信道设置缓冲区大小。默认值是零，表示不带缓冲的或同步的信道。 ci := make(chan int) // unbuffered channel of integers cj := make(chan int, 0) // unbuffered channel of integers cs := make(chan *os.File, 100) // buffered channel of pointers to Files 无缓冲信道在通信时会同步交换数据，它能确保（两个 goroutine）计算处于确定状态。 信道有很多惯用法，我们从这里开始了解。在上一节中，我们在后台启动了排序操作。 信道使得启动的 goroutine 等待排序完成。 c := make(chan int) // Allocate a channel. // Start the sort in a goroutine; when it completes, signal on the channel. go func() { list.Sort() c \u003c- 1 // Send a signal; value does not matter. }() doSomethingForAWhile() \u003c-c // Wait for sort to finish; discard sent value. 接收者在收到数据前会一直阻塞。若信道是不带缓冲的，那么在接收者收到值前， 发送者会一直阻塞；若信道是带缓冲的，则发送者直到值被复制到缓冲区才开始阻塞； 若缓冲区已满，发送者会一直等待直到某个接收者取出一个值为止。 带缓冲的信道可被用作信号量，例如限制吞吐量。在此例中，进入的请求会被传递给 handle，它从信道中接收值，处理请求后将值发回该信道中，以便让该 “信号量” 准备迎接下一次请求。信道缓冲区的容量决定了同时调用 process 的数量上限。 var sem = make(chan int, MaxOutstanding) func handle(r *Request) { sem \u003c- 1 // Wait for active queue to drain. process(r) // May take a long time. \u003c-sem // Done; enable next request to run. } func Serve(queue chan *Request) { for { req := \u003c-queue go handle(req) // Don't wait for handle to finish. } } 一旦有 MaxOutstanding 个处理器进入运行状态，其他的所有处理器都会在试图发送值到信道缓冲区的时候阻塞，直到某个处理器完成处理并从缓冲区取回一个值为止。 然而，它却有个设计问题：尽管只有 MaxOutstanding 个 goroutine 能同时运行，但 Serve 还是为每个进入的请求都创建了新的 goroutine。其结果就是，若请求来得很快， 该程序就会无限地消耗资源。为了弥补这种不足，我们可以通过修改 Serve 来限制创建 Go 程，这是个明显的解决方案，但要当心我们修复后出现的 Bug。 func Serve(queue chan *Request) { for req := range queue { sem \u003c- 1 go func() { process(req) // Buggy; see explanation below. \u003c-sem }() } } Bug 出现在 Go 的 for 循环中，该循环变量在每次迭代时会被重用，因此 req 变量会在所有的 goroutine 间共享，这不是我们想要的。我们需要确保 req 对于每个 goroutine 来说都是唯一的。有一种方法能够做到，就是将 req 的值作为实参传入到该 goroutine 的闭包中： func Serve(queue chan *Request) { for req := range queue { sem \u003c- 1 go func(req *Request) { process(req) \u003c-sem }(req) } } 比较前后两个版本，观察该闭包声明和运行中的差别。 另一种解决方案就是以相同的名字创建新的变量，如例中所示： func Serve(queue chan *Request) { for req := range queue { req := req // Create new instance of req for the goroutine. sem \u003c- 1 go func() { process(req) \u003c-sem }() } } 它的写法看起来有点奇怪 req := req 但在 Go 中这样做是合法且惯用的。你用相同的名字获得了该变量的一个新的版本， 以此来局部地刻意屏蔽循环变量，使它对每个 goroutine 保持唯一。 回到编写服务器的一般问题上来。另一种管理资源的好方法就是启动固定数量的 handle goroutine，一起从请求信道中读取数据。Goroutine 的数量限制了同时调用 process 的数量。Serve 同样会接收一个通知退出的信道， 在启动所有 goroutine 后，它将阻塞并暂停从信道中接收消息。 func handle(queue chan *Request) { for r := range queue { process(r) } } func Serve(clientRequests chan *Request, quit chan bool) { // Start handlers for i := 0; i \u003c MaxOutstanding; i++ { go handle(clientRequests) } \u003c-quit // Wait to be told to exit. } ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:3","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Channels of channels Go 最重要的特性就是信道是一等值，它可以被分配并像其它值到处传递。 这种特性通常被用来实现安全、并行的多路分解。 在上一节的例子中，handle 是个非常理想化的请求处理程序， 但我们并未定义它所处理的请求类型。若该类型包含一个可用于回复的信道， 那么每一个客户端都能为其回应提供自己的路径。以下为 Request 类型的大概定义。 type Request struct { args []int f func([]int) int resultChan chan int } 客户端提供了一个函数及其实参，此外在请求对象中还有个接收应答的信道。 func sum(a []int) (s int) { for _, v := range a { s += v } return } request := \u0026Request{[]int{3, 4, 5}, sum, make(chan int)} // Send request clientRequests \u003c- request // Wait for response. fmt.Printf(\"answer: %d\\n\", \u003c-request.resultChan) 在服务端，只需改动 handler 函数。 func handle(queue chan *Request) { for req := range queue { req.resultChan \u003c- req.f(req.args) } } 要使其实际可用还有很多工作要做，这些代码仅能实现一个速率有限、并行、非阻塞 RPC 系统的框架，而且它并不包含互斥锁。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:4","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Parallelization 这些设计的另一个应用是在多 CPU 核心上实现并行计算。如果计算过程能够被分为几块可独立执行的过程，它就可以在每块计算结束时向信道发送信号，从而实现并行处理。 让我们看看这个理想化的例子。我们在对一系列向量项进行极耗资源的操作， 而每个项的值计算是完全独立的。 type Vector []float64 // Apply the operation to v[i], v[i+1] ... up to v[n-1]. func (v Vector) DoSome(i, n int, u Vector, c chan int) { for ; i \u003c n; i++ { v[i] += u.Op(v[i]) } c \u003c- 1 // signal that this piece is done } 我们在循环中启动了独立的处理块，每个 CPU 将执行一个处理。 它们有可能以乱序的形式完成并结束，但这没有关系； 我们只需在所有 goroutine 开始后接收，并统计信道中的完成信号即可。 const NCPU = 4 // number of CPU cores func (v Vector) DoAll(u Vector) { c := make(chan int, NCPU) // Buffering optional but sensible. for i := 0; i \u003c NCPU; i++ { go v.DoSome(i*len(v)/NCPU, (i+1)*len(v)/NCPU, u, c) } // Drain the channel. for i := 0; i \u003c NCPU; i++ { \u003c-c // wait for one task to complete } // All done. } 目前 Go 运行时的实现默认并不会并行执行代码，它只为用户层代码提供单一的处理核心。 任意数量的 goroutine 都可能在系统调用中被阻塞，而在任意时刻默认只有一个会执行用户层代码。 它应当变得更智能，而且它将来肯定会变得更智能。但现在，若你希望 CPU 并行执行， 就必须告诉运行时你希望同时有多少 goroutine 能执行代码。有两种途径可达到这一目的，要么 在运行你的工作时将 GOMAXPROCS 环境变量设为你要使用的核心数， 要么导入 runtime 包并调用 runtime.GOMAXPROCS(NCPU)。 runtime.NumCPU() 的值可能很有用，它会返回当前机器的逻辑 CPU 核心数。 当然，随着调度算法和运行时的改进，将来会不再需要这种方法。 注意不要混淆并发和并行的概念：并发是用可独立执行的组件构造程序的方法， 而并行则是为了效率在多 CPU 上平行地进行计算。尽管 Go 的并发特性能够让某些问题更易构造成并行计算， 但 Go 仍然是种并发而非并行的语言，且 Go 的模型并不适合所有的并行问题。 关于其中区别的讨论，见 此博文。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:5","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"A leaky buffer 并发编程的工具甚至能很容易地表达非并发的思想。这里有个提取自 RPC 包的例子。 客户端 Go 程从某些来源，可能是网络中循环接收数据。为避免分配和释放缓冲区， 它保存了一个空闲链表，使用一个带缓冲信道表示。若信道为空，就会分配新的缓冲区。 一旦消息缓冲区就绪，它将通过 serverChan 被发送到服务器。 var freeList = make(chan *Buffer, 100) var serverChan = make(chan *Buffer) func client() { for { var b *Buffer // Grab a buffer if available; allocate if not. select { case b = \u003c-freeList: // Got one; nothing more to do. default: // None free, so allocate a new one. b = new(Buffer) } load(b) // Read next message from the net. serverChan \u003c- b // Send to server. } } 服务器从客户端循环接收每个消息，处理它们，并将缓冲区返回给空闲列表。 func server() { for { b := \u003c-serverChan // Wait for work. process(b) // Reuse buffer if there's room. select { case freeList \u003c- b: // Buffer on free list; nothing more to do. default: // Free list full, just carry on. } } } 客户端试图从 freeList 中获取缓冲区；若没有缓冲区可用， 它就将分配一个新的。服务器将 b 放回空闲列表 freeList 中直到列表已满，此时缓冲区将被丢弃，并被垃圾回收器回收。（select 语句中的 default 子句在没有条件符合时执行，这也就意味着 selects 永远不会被阻塞。）依靠带缓冲的信道和垃圾回收器的记录， 我们仅用短短几行代码就构建了一个可能导致缓冲区槽位泄露的空闲列表。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:6","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Errors 库例程通常需要向调用者返回某种类型的错误提示。之前提到过，Go 语言的多值返回特性， 使得它在返回常规的值时，还能轻松地返回详细的错误描述。使用这个特性来提供详细的错误信息是一种良好的风格。 例如，我们稍后会看到， os.Open 在失败时不仅返回一个 nil 指针，还返回一个详细描述错误的 error 值。 按照约定，错误的类型通常为 error，这是一个内建的简单接口。 type error interface { Error() string } 库的编写者通过更丰富的底层模型可以轻松实现这个接口，这样不仅能看见错误，还能提供一些上下文。前已述及，除了通常的 *os.File 返回值， os.Open 还返回一个 error 值。若该文件被成功打开， error 值就是 nil ，而如果出了问题，该值就是一个 os.PathError。 // PathError records an error and the operation and // file path that caused it. type PathError struct { Op string // \"open\", \"unlink\", etc. Path string // The associated file. Err error // Returned by the system call. } func (e *PathError) Error() string { return e.Op + \" \" + e.Path + \": \" + e.Err.Error() } PathError 的 Error 会生成如下错误信息： open /etc/passwx: no such file or directory 这种错误包含了出错的文件名、操作和触发的操作系统错误，即便在产生该错误的调用和输出的错误信息相距甚远时，它也会非常有用，这比苍白的 “不存在该文件或目录” 更具说明性。 错误字符串应尽可能地指明它们的来源，例如产生该错误的包名前缀。例如在 image 包中，由于未知格式导致解码错误的字符串为 “image: unknown format”。 若调用者关心错误的完整细节，可使用类型选择或者类型断言来查看特定错误，并抽取其细节。 对于 PathErrors，它应该还包含检查内部的 Err 字段以进行可能的错误恢复。 for try := 0; try \u003c 2; try++ { file, err = os.Create(filename) if err == nil { return } if e, ok := err.(*os.PathError); ok \u0026\u0026 e.Err == syscall.ENOSPC { deleteTempFiles() // Recover some space. continue } return } 这里的第二条 if 是另一种 类型断言。若它失败， ok 将为 false，而 e 则为 nil. 若它成功，ok 将为 true，这意味着该错误属于 *os.PathError 类型，而 e 能够检测关于该错误的更多信息。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:15:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Panic 向调用者报告错误的一般方式就是将 error 作为额外的值返回。 标准的 Read 方法就是个众所周知的实例，它返回一个字节计数和一个 error。但如果错误是不可恢复的呢？有时程序就是不能继续运行。 为此，我们提供了内建的 panic 函数，它会产生一个运行时错误并终止程序 （但请继续看下一节）。该函数接受一个任意类型的实参（一般为字符串），并在程序终止时打印。 它还能表明发生了意料之外的事情，比如从无限循环中退出了。 // A toy implementation of cube root using Newton's method. func CubeRoot(x float64) float64 { z := x/3 // Arbitrary initial value for i := 0; i \u003c 1e6; i++ { prevz := z z -= (z*z*z-x) / (3*z*z) if veryClose(z, prevz) { return z } } // A million iterations has not converged; something is wrong. panic(fmt.Sprintf(\"CubeRoot(%g) did not converge\", x)) } 这仅仅是个示例，实际的库函数应避免 panic。若问题可以被屏蔽或解决， 最好就是让程序继续运行而不是终止整个程序。一个可能的反例就是初始化： 若某个库真的不能让自己工作，且有足够理由产生 Panic，那就由它去吧。 var user = os.Getenv(\"USER\") func init() { if user == \"\" { panic(\"no value for $USER\") } } ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:15:1","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Recover 当 panic 被调用后（包括不明确的运行时错误，例如切片检索越界或类型断言失败）， 程序将立刻终止当前函数的执行，并开始回溯 goroutine 的栈，运行任何被推迟的函数。 若回溯到达 goroutine 栈的顶端，程序就会终止。不过我们可以用内建的 recover 函数来重新取回 goroutine 的控制权限并使其恢复正常执行。 调用 recover 将停止回溯过程，并返回传入 panic 的实参。 由于在回溯时只有被推迟函数中的代码在运行，因此 recover 只能在被推迟的函数中才有效。 recover 的一个应用就是在服务器中终止失败的 goroutine 而无需杀死其它正在执行的 goroutine。 func server(workChan \u003c-chan *Work) { for work := range workChan { go safelyDo(work) } } func safelyDo(work *Work) { defer func() { if err := recover(); err != nil { log.Println(\"work failed:\", err) } }() do(work) } 在此例中，若 do(work) 触发了 Panic，其结果就会被记录， 而该 Go 程会被干净利落地结束，不会干扰到其它 goroutine。我们无需在推迟的闭包中做任何事情， recover 会处理好这一切。 由于直接从被推迟函数中调用 recover 时不会返回 nil， 因此被推迟的代码能够调用本身使用了 panic 和 recover 的库函数而不会失败。例如在 safelyDo 中，被推迟的函数可能在调用 recover 前先调用记录函数，而该记录函数应当不受 Panic 状态的代码的影响。 通过恰当地使用恢复模式，do 函数（及其调用的任何代码）可通过调用 panic 来避免更坏的结果。我们可以利用这种思想来简化复杂软件中的错误处理。 让我们看看 regexp 包的理想化版本，它会以局部的错误类型调用 panic 来报告解析错误。以下是一个 error 类型的 Error 方法和一个 Compile 函数的定义： // Error is the type of a parse error; it satisfies the error interface. type Error string func (e Error) Error() string { return string(e) } // error is a method of *Regexp that reports parsing errors by // panicking with an Error. func (regexp *Regexp) error(err string) { panic(Error(err)) } // Compile returns a parsed representation of the regular expression. func Compile(str string) (regexp *Regexp, err error) { regexp = new(Regexp) // doParse will panic if there is a parse error. defer func() { if e := recover(); e != nil { regexp = nil // Clear return value. err = e.(Error) // Will re-panic if not a parse error. } }() return regexp.doParse(str), nil } 若 doParse 触发了 Panic，恢复块会将返回值设为 nil —被推迟的函数能够修改已命名的返回值。在 err 的赋值过程中， 我们将通过断言它是否拥有局部类型 Error 来检查它。若它没有， 类型断言将会失败，此时会产生运行时错误，并继续栈的回溯，仿佛一切从未中断过一样。 该检查意味着若发生了一些像索引越界之类的意外，那么即便我们使用了 panic 和 recover 来处理解析错误，代码仍然会失败。 通过适当的错误处理，error 方法（由于它是个绑定到具体类型的方法， 因此即便它与内建的 error 类型名字相同也没有关系） 能让报告解析错误变得更容易，而无需手动处理回溯的解析栈： if pos == 0 { re.error(\"'*' illegal at start of expression\") } 尽管这种模式很有用，但它应当仅在包内使用。Parse 会将其内部的 panic 调用转为 error 值，它并不会向调用者暴露出 panic。这是个值得遵守的良好规则。 顺便一提，这种重新触发Panic的惯用法会在产生实际错误时改变Panic的值。 然而，不管是原始的还是新的错误都会在崩溃报告中显示，因此问题的根源仍然是可见的。 这种简单的重新触发Panic的模型已经够用了，毕竟他只是一次崩溃。 但若你只想显示原始的值，也可以多写一点代码来过滤掉不需要的问题，然后用原始值再次触发Panic。 这里就将这个练习留给读者了。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:15:2","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"A web server 让我们以一个完整的 Go 程序作为结束吧，一个 Web 服务器。该程序其实只是个 Web 服务器的重用。 Google 在 http://chart.apis.google.com 上提供了一个将表单数据自动转换为图表的服务。不过，该服务很难交互， 因为你需要将数据作为查询放到 URL 中。此程序为一种数据格式提供了更好的的接口： 给定一小段文本，它将调用图表服务器来生成二维码（QR 码），这是一种编码文本的点格矩阵。 该图像可被你的手机摄像头捕获，并解释为一个字符串，比如 URL， 这样就免去了你在狭小的手机键盘上键入 URL 的麻烦。 以下为完整的程序，随后有一段解释。 package main import ( \"flag\" \"html/template\" \"log\" \"net/http\" ) var addr = flag.String(\"addr\", \":1718\", \"http service address\") // Q=17, R=18 var templ = template.Must(template.New(\"qr\").Parse(templateStr)) func main() { flag.Parse() http.Handle(\"/\", http.HandlerFunc(QR)) err := http.ListenAndServe(*addr, nil) if err != nil { log.Fatal(\"ListenAndServe:\", err) } } func QR(w http.ResponseWriter, req *http.Request) { templ.Execute(w, req.FormValue(\"s\")) } const templateStr = ` \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eQR Link Generator\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e {{if.}}\u003cimg src=\"http://chart.apis.google.com/chart?chs=300x300\u0026cht=qr\u0026choe=UTF-8\u0026chl={{.}}\" /\u003e \u003cbr\u003e {{.}}\u003cbr\u003e \u003cbr\u003e {{end}}\u003cform action=\"/\" name=f method=\"GET\"\u003e\u003cinput maxLength=1024 size=70 name=s value=\"\" title=\"Text to QR Encode\"\u003e\u003cinput type=submit value=\"Show QR\" name=qr\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e ` main 之前的代码应该比较容易理解。我们通过一个标志为服务器设置了默认端口。 模板变量 templ 正是有趣的地方。它构建的 HTML 模版将会被服务器执行并显示在页面中。 稍后我们将详细讨论。 main 函数解析了参数标志并使用我们讨论过的机制将 QR 函数绑定到服务器的根路径。然后调用 http.ListenAndServe 启动服务器；它将在服务器运行时处于阻塞状态。 QR 仅接受包含表单数据的请求，并为表单值 s 中的数据执行模板。 模板包 html/template 非常强大；该程序只是浅尝辄止。 本质上，它通过在运行时将数据项中提取的元素（在这里是表单值）传给 templ.Execute 执行因而重写了 HTML 文本。 在模板文本（templateStr）中，双大括号界定的文本表示模板的动作。 从 {{if .}} 到 {{end}} 的代码段仅在当前数据项（这里是点 .）的值非空时才会执行。 也就是说，当字符串为空时，此部分模板段会被忽略。 其中两段 {{.}} 表示要将数据显示在模板中 （即将查询字符串显示在 Web 页面上）。HTML 模板包将自动对文本进行转义， 因此文本的显示是安全的。 余下的模板字符串只是页面加载时将要显示的 HTML。如果这段解释你无法理解，请参考 文档 获得更多有关模板包的解释。 你终于如愿以偿了：以几行代码实现的，包含一些数据驱动的HTML文本的Web服务器。 Go语言强大到能让很多事情以短小精悍的方式解决。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:16:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Coding"],"content":"查找算法 散列查找：也称哈希查找，有拉链法查找，也有线性探测法查找，拉链法使用数组链表结构，线性探测法使用数组。 树查找：有搜索二叉树，平衡查找树如：红黑树，B树，AVL树，B+等，使用链表树结构 ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:0:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"哈希表：散列查找 线性查找：在链表上线性查找键更新值。 散列查找： 空间换时间的查找算法 依赖数据结构HashTable（Hash: 指压缩映射，它将一个比较大的域空间映射到一个比较小的域空间） 关于线性探测法与拉链法在go语法里map实现原理时有讲解。（go_base_01） 哈希算法非常多，随机分布性不同，当然，让得到的哈希值越均匀随机分布越好，拉链法形成的链表不会很长。 最好时间复杂度能达到： O(1) ，最坏情况下退化到查找链表： O(n) 。均匀性很好的哈希算法以及合适空间大小的数组，在很大概率避免了最坏情况。 ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:1:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"二叉查找树 又称为二叉搜索树，二叉排序树。 二叉查找树不保证是一个平衡的二叉树，最坏情况下二叉查找树会退化成一个链表。 通用形式的二叉查找树实现甚少使用，大部分程序都使用了AVL树或红黑树。 二叉查找树中序遍历即可实现排序。 查找，添加，删除元素的时间复杂度取决于树的高度。查找，添加和删除的时间复杂度范围为 log(n)~n 。 AVL树和红黑树都是相对平衡的二叉查找树，因为特殊的旋转平衡操作，树的高度被大大压低。它们查找效率较高，添加，删除，查找操作的平均时间复杂度都为 log(n) 。 ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:2:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"AVL树（平衡二叉搜索树） Adelson-Velsky and Landis。 添加和删除元素时的调整操作比较关键且重要。 // AVL树 type AVLTree struct { Root *AVLTreeNode // 树根节点 } // AVL节点 type AVLTreeNode struct { Value int64 // 值 Times int64 // 值出现的次数 Height int64 // 该节点作为树根节点，树的高度，方便计算平衡因子 Left *AVLTreeNode // 左子树 Right *AVLTreeNode // 右字树 } // 初始化一个AVL树 func NewAVLTree() *AVLTree { return new(AVLTree) } AVL树添加元素 插入节点后，需要满足所有节点的平衡因子在 [-1，0，1] 范围内，如果不在，需要进行旋转调整。旋转有四种情况： 在右子树上插上右儿子导致失衡，左旋，转一次。 在左子树上插上左儿子导致失衡，右旋，转一次。 在左子树上插上右儿子导致失衡，先左后右旋，转两次。 在右子树上插上左儿子导致失衡，先右后左旋，转两次。 右图来自维基百科： // 单右旋操作，看图说话 func RightRotation(Root *AVLTreeNode) *AVLTreeNode { // 只有Pivot和B，Root位置变了 Pivot := Root.Left B := Pivot.Right Pivot.Right = Root Root.Left = B // 只有Root和Pivot变化了高度 Root.UpdateHeight() Pivot.UpdateHeight() return Pivot } // 单左旋操作，看图说话 func LeftRotation(Root *AVLTreeNode) *AVLTreeNode { // 只有Pivot和B，Root位置变了 Pivot := Root.Right B := Pivot.Left Pivot.Left = Root Root.Right = B // 只有Root和Pivot变化了高度 Root.UpdateHeight() Pivot.UpdateHeight() return Pivot } 复用上面代码 // 先左后右旋操作，看图说话 func LeftRightRotation(node *AVLTreeNode) *AVLTreeNode { node.Left = LeftRotation(node.Left) return RightRotation(node) } // 先右后左旋操作，看图说话 func RightLeftRotation(node *AVLTreeNode) *AVLTreeNode { node.Right = RightRotation(node.Right) return LeftRotation(node) } 完成旋转操作，可以添加元素了~ // 添加元素 func (tree *AVLTree) Add(value int64) { // 往树根添加元素，会返回新的树根 tree.Root = tree.Root.Add(value) } func (node *AVLTreeNode) Add(value int64) *AVLTreeNode { // 添加值到根节点node，如果node为空，那么让值成为新的根节点，树的高度为1 if node == nil { return \u0026AVLTreeNode{Value: value, Height: 1} } // 如果值重复，什么都不用做，直接更新次数 if node.Value == value { node.Times = node.Times + 1 return node } // 辅助变量 var newTreeNode *AVLTreeNode if value \u003e node.Value { // 插入的值大于节点值，要从右子树继续插入 node.Right = node.Right.Add(value) // 平衡因子，插入右子树后，要确保树根左子树的高度不能比右子树低一层。 factor := node.BalanceFactor() // 右子树的高度变高了，导致左子树-右子树的高度从-1变成了-2。 if factor == -2 { if value \u003e node.Right.Value { // 表示在右子树上插上右儿子导致失衡，需要单左旋： newTreeNode = LeftRotation(node) } else { //表示在右子树上插上左儿子导致失衡，先右后左旋： newTreeNode = RightLeftRotation(node) } } } else { // 插入的值小于节点值，要从左子树继续插入 node.Left = node.Left.Add(value) // 平衡因子，插入左子树后，要确保树根左子树的高度不能比右子树高一层。 factor := node.BalanceFactor() // 左子树的高度变高了，导致左子树-右子树的高度从1变成了2。 if factor == 2 { if value \u003c node.Left.Value { // 表示在左子树上插上左儿子导致失衡，需要单右旋： newTreeNode = RightRotation(node) } else { //表示在左子树上插上右儿子导致失衡，先左后右旋： newTreeNode = LeftRightRotation(node) } } } if newTreeNode == nil { // 表示什么旋转都没有，根节点没变，直接刷新树高度 node.UpdateHeight() return node } else { // 旋转了，树根节点变了，需要刷新新的树根高度 newTreeNode.UpdateHeight() return newTreeNode } } 由于树的高度最高为 1.44log(n)，查找元素插入位置，最坏次数为 1.44log(n) 次。逐层更新子树高度并判断平衡是否被破坏，最坏需要 1.44log(n) 次，因此可以得知添加元素最坏时间复杂度为：2.88log(n) 当插入节点后，某子树不平衡时最多旋转 2次，也就是双旋该子树即可恢复平衡，该调整为局部特征，调整完后其父层不再需要旋转。也就是说，插入操作最坏旋转两次即可 AVL树查找元素与普通二叉查找树一致。 删除元素有四种情况： 删除的节点是叶子节点，没有儿子，直接删除后看离它最近的父亲节点是否失衡，做调整操作。 删除的节点下有两个子树，选择高度更高的子树下的节点来替换被删除的节点，如果左子树更高，选择左子树中最大的节点，也就是左子树最右边的叶子节点，如果右子树更高，选择右子树中最小的节点，也就是右子树最左边的叶子节点。最后，删除这个叶子节点，也就是变成情况1。 删除的节点只有左子树，可以知道左子树其实就只有一个节点，被删除节点本身（假设左子树多于2个节点，那么高度差就等于2了，不符合AVL树定义），将左节点替换被删除的节点，最后删除这个左节点，变成情况1。 删除的节点只有右子树，可以知道右子树其实就只有一个节点，被删除节点本身（假设右子树多于2个节点，那么高度差就等于2了，不符合AVL树定义），将右节点替换被删除的节点，最后删除这个右节点，变成情况1。 后面三种情况最后都变成情况1，就是将删除的节点变成叶子节点，然后可以直接删除该叶子节点，然后看其最近的父亲节点是否失衡，失衡时对树进行平衡。 实现代码： func (node *AVLTreeNode) Delete(value int64) *AVLTreeNode { if node == nil { // 如果是空树，直接返回 return nil } if value \u003c node.Value { // 从左子树开始删除 node.Left = node.Left.Delete(value) // 删除后要更新该子树高度 node.Left.UpdateHeight() } else if value \u003e node.Value { // 从右子树开始删除 node.Right = node.Right.Delete(value) // 删除后要更新该子树高度 node.Right.UpdateHeight() } else { // 找到该值对应的节点 // 该节点没有左右子树 // 第一种情况，删除的节点没有儿子，直接删除即可。 if node.Left == nil \u0026\u0026 node.Right == nil { return nil // 直接返回nil，表示直接该值删除 } // 该节点有两棵子树，选择更高的哪个来替换 // 第二种情况，删除的节点下有两个子树，选择高度更高的子树下的节点来替换被删除的节点，如果左子树更高，选择左子树中最大的节点，也就是左子树最右边的叶子节点，如果右子树更高，选择右子树中最小的节点，也就是右子树最左边的叶子节点。最后，删除这个叶子节点。 if node.Left != nil \u0026\u0026 node.Right != nil { // 左子树更高，拿左子树中最大值的节点替换 if node.Left.Height \u003e node.Right.Height { maxNode := node.","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:3:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"2-3树和左倾红黑树 左倾红黑树比普通红黑树实现更简单。 红黑树是一种近似平衡的二叉查找树，从 2-3 树或 2-3-4 树衍生而来。通过对二叉树节点进行染色，染色为红或黑节点，来模仿 2-3 树或 2-3-4 树的3节点和4节点，从而让树的高度减小。2-3-4 树对照实现的红黑树是普通的红黑树，而 2-3 树对照实现的红黑树是一种变种，称为左倾红黑树。 2-3树：它不是一棵二叉树，是一棵三叉树。具有以下特征： 内部节点要么有1个数据元素和2个孩子，要么有2个数据元素和3个孩子，叶子节点没有孩子，但有1或2个数据元素。 所有叶子节点到根节点的长度一致。这个特征保证了完全平衡，非常完美的平衡。 每个节点的数据元素保持从小到大排序，两个数据元素之间的子树的所有值大小介于两个数据元素之间。 2-3树插入元素： 先二分查找到要插入的位置 插入元素到一个2节点，直接插入即可，这样节点变成3节点。 插入元素到一个3节点，该3节点的父亲是一个2节点，先将节点变成临时的4节点，然后向上分裂调整一次。 插入元素到一个3节点，该3节点的父亲是一个3节点，先将节点变成临时的4节点，然后向上分裂调整，此时父亲节点变为临时4节点，继续向上分裂调整。 核心在于插入3节点后，该节点变为临时4节点，然后进行分裂恢复树的特征。最坏情况为插入节点后，每一次分裂后都导致上一层变为临时4节点，直到树根节点，这样需要不断向上分裂。 临时4节点的分裂，细分有六种情况： 2-3树删除元素： 情况1：删除中间节点 删除的是非叶子节点，该节点一定是有两棵或者三棵子树的，那么从子树中找到其最小后继节点，该节点是叶子节点，用该节点替换被删除的非叶子节点，然后再删除这个叶子节点，进入情况2。 如何找到最小后继节点，当有两棵子树时，那么从右子树一直往左下方找，如果有三棵子树，被删除节点在左边，那么从中子树一直往左下方找，否则从右子树一直往左下方找。 情况2：删除叶子节点 删除的是叶子节点，这时如果叶子节点是3节点，那么直接变为2节点即可，不影响平衡。但是，如果叶子节点是2节点，那么删除后，其父节点将会缺失一个儿子，破坏了满孩子的 2-3 树特征，需要进行调整后才能删除。 针对情况2，删除一个2节点的叶子节点，会导致父节点缺失一个儿子，破坏了 2-3 树的特征，我们可以进行调整变换，主要有两种调整： 重新分布：尝试从兄弟节点那里借值，然后重新调整节点。 合并：如果兄弟借不到值，合并节点（与父亲的元素），再向上递归处理。 左倾红黑树： 可以由2-3树实现 根节点的链接是黑色的。 红链接均为左链接。 没有任何一个结点同时和两条红链接相连 任意一个节点到达叶子节点的所有路径，经过的黑链接数量相同，也就是该树是完美黑色平衡的。比如，某一个节点，它可以到达5个叶子节点，那么这5条路径上的黑链接数量一样。 // 定义颜色 const ( RED = true BLACK = false ) // 左倾红黑树 type LLRBTree struct { Root *LLRBTNode // 树根节点 } // 左倾红黑树节点 type LLRBTNode struct { Value int64 // 值 Times int64 // 值出现的次数 Left *LLRBTNode // 左子树 Right *LLRBTNode // 右子树 Color bool // 父亲指向该节点的链接颜色 } // 新建一棵空树 func NewLLRBTree() *LLRBTree { return \u0026LLRBTree{} } // 节点的颜色 func IsRed(node *LLRBTNode) bool { if node == nil { return false } return node.Color == RED } 在元素添加和实现的过程中，需要做调整操作，有两种旋转操作，对某节点的右链接进行左旋转，或者左链接进行右旋转。 // 左旋转 func RotateLeft(h *LLRBTNode) *LLRBTNode { if h == nil { return nil } x := h.Right h.Right = x.Left x.Left = h x.Color = h.Color h.Color = RED return x } // 右旋转 func RotateRight(h *LLRBTNode) *LLRBTNode { if h == nil { return nil } x := h.Left h.Left = x.Right x.Right = h x.Color = h.Color h.Color = RED return x } // 颜色转换 func ColorChange(h *LLRBTNode) { if h == nil { return } h.Color = !h.Color h.Left.Color = !h.Left.Color h.Right.Color = !h.Right.Color } 添加元素实现： 每次添加元素节点时，都将该节点 Color 字段，也就是父亲指向它的链接设置为 RED 红色。接着判断其父亲是否有两个红链接（如连续的两个左红链接或者左右红色链接），或者有右红色链接，进行颜色变换或旋转操作。 几种情况： 插入元素到2节点，直接让节点变为3节点，不过当右插入时需要左旋使得红色链接在左边 插入元素到3节点，需要做旋转和颜色转换操作 // 左倾红黑树添加元素 func (tree *LLRBTree) Add(value int64) { // 跟节点开始添加元素，因为可能调整，所以需要将返回的节点赋值回根节点 tree.Root = tree.Root.Add(value) // 根节点的链接永远都是黑色的 tree.Root.Color = BLACK } // 往节点添加元素 func (node *LLRBTNode) Add(value int64) *LLRBTNode { // 插入的节点为空，将其链接颜色设置为红色，并返回 if node == nil { return \u0026LLRBTNode{ Value: value, Color: RED, } } // 插入的元素重复 if value == node.Value { node.Times = node.Times + 1 } else if value \u003e node.Value { // 插入的元素比节点值大，往右子树插入 node.Right = node.Right.Add(value) } else { // 插入的元素比节点值小，往左子树插入 node.Left = node.Left.Add(value) } // 辅助变量 nowNode := node // 右链接为红色，那么进行左旋，确保树是左倾的 // 这里做完操作后就可以结束了，因为插入操作，新插入的右红链接左旋后，nowNode节点不会出现连续两个红左链接，因为它只有一个左红链接 if IsRed(nowNode.Right) \u0026\u0026 !IsRed(nowNode.Left) { nowNode = RotateLeft(nowNode) } else { // 连续两个左链接为红色，那么进行右旋 if IsRed(nowNode.Left) \u0026\u0026 IsRed(nowNode.Left.Left) { nowNode = RotateRight(nowNode) } // 旋转后，可能左右链接都为红色，需要变色 if IsRed(nowNode.Left) \u0026\u0026 IsRed(nowNode.Right) { ColorChange(nowNode) } } return nowNode } 算法分析： 左倾红黑树的最坏树高度为 2log(n)，其中 n 为树的节点数量。为什么呢，我们先把左倾红黑树当作 2-3 树，也就是说最坏情况下沿着 2-3 树左边的节点都是3节点，其他节点都是2节点，这时树高近似 log(n)，再从 2-3 树转成左倾红黑树，当3节点不画平时，可以知道树高变成原来 2-3 树树高的两倍。虽然如此，构造一棵最坏的左倾红黑树很难。 AVL 树的最坏树高度为 1.44log(n)。由于左倾红黑树是近似平衡的二叉树，没有 AVL 树的严格平衡，树的高度会更高一点，因此查找操作效率比 AVL 树低，但时间复杂度只在于常数项的差别，去掉常数项，时间复杂度仍然是 log(n)。 我们的代码实现中，左倾红黑树的插入，需要逐层判断是否需要旋转和变色，复杂度为 log(n)，当旋转变色后导致上层存在连续的红左链接或者红色左右链接，那么需要继续旋转和变色，可能有多次这种调整操作，如图在箭头处添加新节点，出现了右红链接，要一直向上变色到根节点（实际上穿投到根节点的情况极少发生）：我们可以优化代码，使得在某一层旋转变色后，如果其父层没有连续的左红链接或者不需要变色，那么可以直接退出，不需要逐层判断是否需要旋转和变色。 对于 AVL 树来说，插入最多旋转两次，但其需要逐层更新树高度，复杂度也是为 log(n)。 按照插入效率来说，很多教程都说左倾红黑树会比 AVL 树好一点，因为其不要求严格的平衡，会插入得更快点，但根据我们实际上的递归代码，两者都需要逐层向上判断是否需要调整，只不过 AVL 树多了更新树高度的操作，此操作影响了一点点效率，但我觉得两种树的插入效率都差不多。 在此，我们不再纠结两种平衡树哪种更好","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:4:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"2-3-4树和普通红黑树 ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:5:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"参考 https://www.cs.princeton.edu/~rs/talks/LLRB/LLRB.pdf ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:6:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"排序算法 稳定性概念 定义：能保证两个相等的数，经过排序之后，其在序列的前后位置顺序不变。（A1=A2，排序前A1在A2前面，排序后A1还在A2前面） 意义：稳定性本质是维持具有相同属性的数据的插入顺序，如果后面需要使用该插入顺序排序，则稳定性排序可以避免这次排序。 冒泡排序，直接选择排序，直接插入排序被认为是初级的排序算法。中等规模用希尔排序，大规模排序使用快排、归并、堆排序这些高级排序算法。快排综合性能最好，甚至成为了很多编程库内置的排序算法。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:0:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"冒泡 自己实现时忘记考虑某一轮两两比较时已经排好序的情况，属实生疏了。时间复杂度一般是$O(n^2)$。冒泡排序是效率较低的排序算法，可以说是最慢的排序算法了，我们只需知道它是什么，在实际编程中一定不能使用如此之慢的排序算法! ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:1:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"选择 效率同样低下的排序算法。可以在每次循环选择时既选择最小的数，也选择最大的数，以减少循环次数达到优化的目的。工程上同样避免使用。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:2:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"插入 时间复杂度：$O(n)-O(n^2)$ 数组规模 n 较小的大多数情况下，我们可以使用插入排序，它比冒泡排序，选择排序都快，甚至比任何的排序算法都快。 数列中的有序性越高，插入排序的性能越高，因为待排序数组有序性越高，插入排序比较的次数越少。 数据规模较大时，效率也低。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:3:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"希尔 一个美国人1959年发明的，希尔排序是直接插入排序的改进版本。因为直接插入排序对那些几乎已经排好序的数列来说，排序效率极高，达到了 O(n) 的线性复杂度，但是每次只能将数据移动一位。希尔排序创造性的可以将数据移动 n 位，然后将 n 一直缩小，缩到与直接插入排序一样为 1 先取一个小于 N 的整数 d1 ，将位置是 d1 整数倍的数们分成一组，对这些数进行直接 插入排序。接着取一个小于 d1 的整数 d2 ，将位置是 d2 整数倍的数们分成一组，对这些数进行直接插入排序。接着取一个小于 d2 的整数 d3 ，将位置是 d3 整数倍的数们分成一组，对这些数进行直接插入排序。…直到取到的整数 d=1 ，接着使用直接插入排序。 这是一种分组插入方法，最后一次迭代就相当于是直接插入排序，其他迭代相当于每次移动 n 个距离的直接插入排序，这些整数是两个数之间的距离，我们称它们为增量。 我们取数列长度的一半为增量，以后每次减半，直到增量为1。 希尔排序通过分组使用直接插入排序，因为步长比 1大，在一开始可以很快将无序的数列变得不那么无序，比较和交换的次数也减少，直到最后使用步长为 1 的直接插入排序，数列已经是相对有序了，所以时间复杂度会稍好一点。 在最好情况下，也就是数列是有序时，希尔排序需要进行 logn 次增量的直接插入排序，因为每次直接插入排序最佳时间复杂度都为： O(n) ，因此希尔排序的最佳时间复杂度为： O(nlogn) 。 在最坏情况下，每一次迭代都是最坏的，假设增量序列为： d8 d7 d6 … d3 d2 1 ，那么每一轮直接插入排序的元素数量为： n/d8 n/d7 n/d6 …. n/d3 n/d2 n ，那么时间复杂度按照直接插入的最坏复杂度来计算为：$O( (n/d8)^2 + (n/d7)^2 + (n/d6)^2 + … + (n/d2)^2 + n^2) = O( \u003c 2 ) * O(n^2)$ 不同的分组增量序列，有不同的时间复杂度。Hibbard 增量序列： 1，3，7，···，2n−1 是被证明可广泛应用的分组序列，时间复杂度为： Θ(n^1.5) 。 希尔排序的时间复杂度大约在这个范围： O(n^1.3)~O(n^2) ，具体还无法用数学来严格证明它。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:4:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"归并 分治法 将两个有序数组进行合并，最多进行 N 次比较就可以生成一个新的有序数组， N 是两个数组长度之和。 归并操作最坏的时间复杂度为： O(n) ，其中 n 是较长数组的长度（因为$N\u003c2n$）。归并操作最好的时间复杂度为： O(n) ，其中 n 是较短数组的长度。正是利用这个特点，归并排序先排序较小的数组，再将有序的小数组合并形成更大有序的数组。 归并排序有两种递归做法，一种是自顶向下，一种是自底向上。 自顶向下：不断二分大数组直到无法切分，排序，不断两两合并，得到排序后数组。 每次都是一分为二，特别均匀，所以最差和最坏时间复杂度都一样。归并操作的时间复杂度为： O(n) ，因此总的时间复杂度为： T(n)=2T(n/2)+O(n) ，根据主定理公式可以知道时间复杂度为： O(nlogn) 因为不断地递归，程序栈层数会有 logn 层，所以递归栈的空间复杂度为： O(logn) ，对于排序十亿个整数，也只要： log(100 0000 0000)=29.897 ，占用的堆栈层数最多 30 层 自底向上：小数组排序合并成大数组。 时间复杂度同上 因不需要递归，没有程序栈占用，空间复杂度为O(1) 归并排序归并操作占用了额外的辅助数组，且归并操作是从一个元素的数组开始。 改进： 对于小规模数组，使用直接插入排序。 原地排序，节约掉辅助数组空间的占用。 建议使用自底向上非递归排序，不会有程序栈空间损耗 手摇算法（翻转算法）： 主要用来对数组两部分进行位置互换 eg:将字符串 abcde1234567 的前 5 个字符与后面的字符交换位置，那么手摇后变成： 1234567abcde 。 如何翻转： 将前部分逆序 将后部分逆序 对整体逆序 归并原地排序利用了手摇算法的特征，不需要额外的辅助数组。 我们自底开始，将元素按照数量为 blockSize 进行小数组排序，使用直接插入排序，然后我们对这些有序的数组向上进行原地归并操作。 归并排序是唯一一个有稳定性保证的高级排序算法，某些时候，为了寻求大规模数据下排序前后，相同元素位置不变，可以使用归并排序。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:5:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"优先队列及堆 堆排序属于选择类排序算法。 优先队列是一种能完成以下任务的队列：插入一个数值，取出最小或最大的数值（获取数值，并且删除）。 优先队列可以用二叉树来实现，我们称这种结构为二叉堆。 最小堆和最大堆是二叉堆的一种，是一棵完全二叉树（一种平衡树）。 最小堆的性质： 父节点的值都小于左右儿子节点。 这是一个递归的性质。 最大堆的性质： 父节点的值都大于左右儿子节点。 这是一个递归的性质。 最大堆和最小堆实现方式一样，只不过根节点一个是最大的，一个是最小的 最大堆特征： 最大堆实现细节(两个操作)： push：向堆中插入数据时，首先在堆的末尾插入数据，如果该数据比父亲节点还大，那么交换，然后不断向上提升，直到没有大小颠倒为止。 pop：从堆中删除最大值时，首先把最后一个值复制到根节点上，并且删除最后一个数值，然后和儿子节点比较，如果值小于儿子，与儿子节点交换，然后不断向下交换， 直到没有大小颠倒为止。在向下交换过程中，如果有两个子儿子都大于自己，就选择较大的。 最大堆有两个核心操作，一个是上浮，一个是下沉，分别对应 push 和 pop 。 上浮操作 操作一次 push 的最好时间复杂度为： O(1) ，因为第一次上浮时如果不大于父亲，那么就结束了。最坏的时间复杂度为： O(logn) ，相当于每次都大于父亲，会一直往上浮到根节点，翻转次数等于树的高度，而树的高度等于元素个数的对数： log(n) 。 构建一个最大堆，从空堆开始，每次添加元素到尾部后，需要向上翻转，最坏翻转次数是：近似 = log(1)+log(2)+log(3)+…+log(n) = log(n!) log(n!) 和 nlog(n) 是同阶的，故最坏时间复杂度便得到了。元素不全相同的情况下最好时间复杂度也是这个。 下沉操作 操作一次 pop 最好的时间复杂度也是： O(1) ，因为第一次比较时根节点就是最大的。最坏时间复杂度仍然是树的高度： O(logn) 。 从一个最大堆，逐一移除堆顶元素，然后将堆尾元素置于堆顶后，向下翻转恢复堆特征，最坏翻转次数是:近似 = log(1)+log(2)+log(3)+…+log(n) = log(n!)，同上浮。元素不全相同的情况下最好时间复杂度也是O(nlog(n)) 如果所有的元素都一样的情况下，建堆和移除堆的每一步都不需要翻转，最好时间复杂度 为： O(n) ，复杂度主要在于遍历元素。 根据最大堆，可以实现堆排序。 普通堆排序：先构建一个最小堆，然后依次把根节点元素 pop 出即可： 因为一开始会认为堆是空的，每次添加元素都需要添加到尾部，然后向上翻转，需要用 Heap.Size来记录堆的大小增长，这种堆构建，可以认为是非原地的构建，影响了效率 改进的原地自底向上的堆排序，不会从空堆开始，而是把待排序的数列当成一个混乱的最大堆，从底层逐层开始，对元素进行下沉操作，一直恢复最大堆的特征，直到根节点。 将构建堆的时间复杂度从 O(nlogn) 降为 O(n) ，总的堆排序时间复杂度从 O(2nlogn) 改进到 O(n+nlogn) 。 自底向上堆排序： 构建最大堆步骤： 先对最底部的所有非叶子节点进行下沉，即这些非叶子节点与它们的儿子节点比较，较大的儿子和父亲交换位置。 接着从次二层开始的非叶子节点重复这个操作，直到到达根节点最大堆就构建好了 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:6:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"快速 亦使用了分治法。 对冒泡排序的改进。 快速排序通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 步骤： 先从数列中取出一个数作为基准数。一般取第一个数。 分区过程，将比这个数大的数全放到它的右边，小于或等于它的数全放到它的左边。 再对左右区间重复第二步，直到各区间只有一个数。 在最好情况下，每一轮都能平均切分，这样遍历元素只要 n/2 次就可以把数列分成两部分，每一轮的时间复杂度都是： O(n) 。因为问题规模每次被折半，折半的数列继续递归进行切分，也就是总的时间复杂度计算公式为： T(n) = 2*T(n/2) + O(n) 。按照主定理公式计算，我们可以知道时间复杂度为： O(nlogn) 最差的情况下，每次都不能平均地切分，每次切分都因为基准数是最大的或者最小的，不能分成两个数列，这样时间复杂度变为了 T(n) = T(n-1) + O(n) ，按照主定理计算可以知道时间复杂度为： O(n^2) 数据规模越大越难以出现最差的情况，在综合情况下，快速排序的平均时间复杂度为： O(nlogn) 。对比之前介绍的排序算法，快速排序比那些动不动就是平方级别的初级排序算法更佳。 为了避免切分不均匀情况的发生，有几种方法改进： 每次进行快速排序切分时，先将数列随机打乱，再进行切分，这样随机加了个震荡，减少不均匀的情况。当然，也可以随机选择一个基准数，而不是选第一个数。 每次取数列头部，中部，尾部三个数，取三个数的中位数为基准数进行切分。 快速排序使用原地排序，存储空间复杂度为： O(1) 。而因为递归栈的影响，递归的程序栈开辟的层数范围在 $logn-n$ ，所以递归栈的空间复杂度为： $O(logn)-log(n)$ ，最坏为： log(n) ，当元素较多时，程序栈可能溢出。通过改进算法，使用伪尾递归进行优化，递归栈的空间复杂度可以减小到 O(logn) 改进： 在小规模数组的情况下，直接插入排序的效率最好，当快速排序递归部分进入小数组范围，可以 切换成直接插入排序。 排序数列可能存在大量重复值，使用三向切分快速排序，将数组分成三部分，大于基准数，等于 基准数，小于基准数，这个时候需要维护三个下标。 使用伪尾递归减少程序栈空间占用，使得栈空间复杂度从 O(logn)~log(n) 变 为： O(logn) 。 伪尾递归优化： // 伪尾递归快速排序 func QuickSort3(array []int, begin, end int) { for begin \u003c end { // 进行切分 loc := partition(array, begin, end) // 那边元素少先排哪边 if loc-begin \u003c end-loc { // 先排左边 QuickSort3(array, begin, loc-1) begin = loc + 1 } else { // 先排右边 QuickSort3(array, loc+1, end) end = loc - 1 } } } 解析：很多人以为这样子是尾递归。其实这样的快排写法是伪装的尾递归，不是真正的尾递归，因为有for 循环，不是直接 return QuickSort ，递归还是不断地压栈，栈的层次仍然不断地增长。但是，因为先让规模小的部分排序，栈的深度大大减少，程序栈最深不会超过 logn 层，这样堆栈最坏空间复杂度从 O(n) 降为 O(logn) 。这种优化也是一种很好的优化，因为栈的层数减少了，对于排序十亿个整数，也只要： log(100 00000000)=29.897 ，占用的堆栈层数最多 30 层，比不进行优化，可能出现的 O(n) 常数层好很多。 非递归写法仅仅是将之前的递归栈转化为自己维持的手工栈。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:7:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"内置库使用快排的原因 首先堆排序，归并排序最好最坏时间复杂度都是： O(nlogn) ，而快速排序最坏的时间复杂度是： O(n^2) ，但是很多编程语言内置的排序算法使用的仍然是快速排序，这是为什么？ 这个问题有偏颇，选择排序算法要看具体的场景， Linux 内核用的排序算法就是堆排序，而Java 对于数量比较多的复杂对象排序，内置排序使用的是归并排序，只是一般情况下，快速排序更快。 归并排序有两个稳定，第一个稳定是排序前后相同的元素位置不变，第二个稳定是，每次都是很平均地进行排序，读取数据也是顺序读取，能够利用存储器缓存的特征，比如从磁盘读取数据进行排序。因为排序过程需要占用额外的辅助数组空间，所以这部分有代价损耗，但是原地手摇的归并排序克服了这个缺陷。 复杂度中，大 O 有一个常数项被省略了，堆排序每次取最大的值之后，都需要进行节点翻转，重新恢复堆的特征，做了大量无用功，常数项比快速排序大，大部分情况下比快速排序慢很多。但是堆排序时间较稳定，不会出现快排最坏 O(n^2) 的情况，且省空间，不需要额外的存储空间和栈空间。 当待排序数量大于16000个元素时，使用自底向上的堆排序比快速排序还快，可见此：https://core.ac.uk/download/pdf/82350265.pdf。 快速排序最坏情况下复杂度高，主要在于切分不像归并排序一样平均，而是很依赖基准数的现在，我们通过改进，比如随机数，三切分等，这种最坏情况的概率极大的降低。大多数情况下，它并不会那么地坏，大多数快才是真的块。 归并排序和快速排序都是分治法，排序的数据都是相邻的，而堆排序比较的数可能跨越很大的范围，导致局部性命中率降低，不能利用现代存储器缓存的特征，加载数据过程会损失性能。 对稳定性有要求的，要求排序前后相同元素位置不变，可以使用归并排序， Java 中的复杂对象类型，要求排序前后位置不能发生变化，所以小规模数据下使用了直接插入排序，大规模数据下使用了归并排序。 对栈，存储空间有要求的可以使用堆排序，比如 Linux 内核栈小，快速排序占用程序栈太大了，使用快速排序可能栈溢出，所以使用了堆排序。 在 Golang 中，标准库 sort 中对切片进行稳定排序：会先按照 20 个元素的范围，对整个切片分段进行插入排序，因为小数组插入排序效率高，然后再对这些已排好序的小数组进行归并排序。其中归并排序还使用了原地排序，节约了辅助空间。 快速排序限制程序栈的层数为： 2*ceil(log(n+1)) ，当递归超过该层时表示程序栈过深，那么转为堆排序。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:8:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"结构型设计模式 常用的有：代理模式、桥接模式、装饰者模式、适配器模式。不常用的有：门面模式、组合模式、享元模式。 代理模式：代理模式在不改变原始类接口的条件下，为原始类定义一个代理类，主要目的是控制访问，而非加强功能，这是它跟装饰器模式最大的不同。 桥接模式：桥接模式的目的是将接口部分和实现部分分离，从而让它们可以较为容易、也相对独立地加以改变。 装饰器模式：装饰者模式在不改变原始类接口的情况下，对原始类功能进行增强，并且支持多个装饰器的嵌套使用。 适配器模式：适配器模式是一种事后的补救策略。适配器提供跟原始类不同的接口，而代理模式、装饰器模式提供的都是跟原始类相同的接口。 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:0:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"adapter 适配器模式的英文翻译是 Adapter Design Pattern。顾名思义，这个模式就是用来做适配的，它将不兼容的接口转换为可兼容的接口，让原本由于接口不兼容而不能一起工作的类可以一起工作。对于这个模式，有一个经常被拿来解释它的例子，就是 USB 转接头充当适配器，把两种不兼容的接口，通过转接变得可以一起工作。 适配器模式用于转换一种接口适配另一种接口。 实际使用中Adaptee一般为接口，并且使用工厂函数生成实例。 在Adapter中匿名组合Adaptee接口，所以Adapter类也拥有SpecificRequest实例方法，又因为Go语言中非入侵式接口特征，其实Adapter也适配Adaptee接口。 两种实现方式，类适配器和对象适配器 其中，类适配器使用继承关系来实现，对象适配器使用组合关系来实现。 一般来说，适配器模式可以看作一种“补偿模式”，用来补救设计上的缺陷。应用这种模式算是“无奈之举”。如果在设计初期，我们就能协调规避接口不兼容的问题，那这种模式就没有应用的机会了。 封装有缺陷的接口设计 假设我们依赖的外部系统在接口设计方面有缺陷（比如包含大量静态方法），引入之后会影响到我们自身代码的可测试性。为了隔离设计上的缺陷，我们希望对外部系统提供的接口进行二次封装，抽象出更好的接口设计，这个时候就可以使用适配器模式了。 统一多个类的接口设计 某个功能的实现依赖多个外部系统（或者说类）。通过适配器模式，将它们的接口适配为统一的接口定义，然后我们就可以使用多态的特性来复用代码逻辑。 替换依赖的外部系统 当我们把项目中依赖的一个外部系统替换为另一个外部系统的时候，利用适配器模式，可以减少对代码的改动。 兼容老版本接口 在做版本升级的时候，对于一些要废弃的接口，我们不直接将其删除，而是暂时保留，并且标注为 deprecated，并将内部实现逻辑委托为新的接口实现。这样做的好处是，让使用它的项目有个过渡期，而不是强制进行代码修改。这也可以粗略地看作适配器模式的一个应用场景。 适配不同格式的数据 前面我们讲到，适配器模式主要用于接口的适配，实际上，它还可以用在不同格式的数据之间的适配。比如，把从不同征信系统拉取的不同格式的征信数据，统一为相同的格式，以方便存储和使用。再比如，Java 中的 Arrays.asList() 也可以看作一种数据适配器，将数组类型的数据转化为集合容器类型。 package adapter //Target 是适配的目标接口 type Target interface { Request() string } //Adaptee 是被适配的目标接口 type Adaptee interface { SpecificRequest() string } //NewAdaptee 是被适配接口的工厂函数 func NewAdaptee() Adaptee { return \u0026adapteeImpl{} } //AdapteeImpl 是被适配的目标类 type adapteeImpl struct{} //SpecificRequest 是目标类的一个方法 func (*adapteeImpl) SpecificRequest() string { return \"adaptee method\" } //NewAdapter 是Adapter的工厂函数 func NewAdapter(adaptee Adaptee) Target { return \u0026adapter{ Adaptee: adaptee, } } //Adapter 是转换Adaptee为Target接口的适配器 type adapter struct { Adaptee } //Request 实现Target接口 func (a *adapter) Request() string { return a.SpecificRequest() } package adapter import \"testing\" var expect = \"adaptee method\" func TestAdapter(t *testing.T) { adaptee := NewAdaptee() target := NewAdapter(adaptee) res := target.Request() if res != expect { t.Fatalf(\"expect: %s, actual: %s\", expect, res) } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:1:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"proxy 代理模式用于延迟处理操作或者在进行实际操作前后进行其它处理。 代理模式的常见用法有 虚代理 COW代理 远程代理 保护代理 Cache 代理 防火墙代理 同步代理 智能指引 等。。。 package proxy type Subject interface { Do() string } type RealSubject struct{} func (RealSubject) Do() string { return \"real\" } type Proxy struct { real RealSubject } func (p Proxy) Do() string { var res string // 在调用真实对象之前的工作，检查缓存，判断权限，实例化真实对象等。。 res += \"pre:\" // 调用真实对象 res += p.real.Do() // 调用之后的操作，如缓存结果，对结果进行处理等。。 res += \":after\" return res } package proxy import \"testing\" func TestProxy(t *testing.T) { var sub Subject sub = \u0026Proxy{} res := sub.Do() if res != \"pre:real:after\" { t.Fail() } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:2:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"代理在RPC、缓存、监控等场景中的应用 代理模式的原理解析 代理模式（Proxy Design Pattern）的原理和代码实现都不难掌握。它在不改变原始类（或叫被代理类）代码的情况下，通过引入代理类来给原始类附加功能。 动态代理的原理解析 所谓动态代理（Dynamic Proxy），就是我们不事先为每个原始类编写代理类，而是在运行的时候，动态地创建原始类对应的代理类，然后在系统中用代理类替换掉原始类。 代理模式的应用场景 代理模式的应用场景非常多，我这里列举一些比较常见的用法，希望你能举一反三地应用在你的项目开发中。 业务系统的非功能性需求开发代理模式最常用的一个应用场景就是，在业务系统中开发一些非功能性需求，比如：监控、统计、鉴权、限流、事务、幂等、日志。我们将这些附加功能与业务功能解耦，放到代理类中统一处理，让程序员只需要关注业务方面的开发。实际上，前面举的搜集接口请求信息的例子，就是这个应用场景的一个典型例子。如果你熟悉 Java 语言和 Spring 开发框架，这部分工作都是可以在 Spring AOP 切面中完成的。前面我们也提到，Spring AOP 底层的实现原理就是基于动态代理。 代理模式在 RPC、缓存中的应用实际上，RPC 框架也可以看作一种代理模式，GoF 的《设计模式》一书中把它称作远程代理。通过远程代理，将网络通信、数据编解码等细节隐藏起来。客户端在使用 RPC 服务的时候，就像使用本地函数一样，无需了解跟服务器交互的细节。除此之外，RPC 服务的开发者也只需要开发业务逻辑，就像开发本地使用的函数一样，不需要关注跟客户端的交互细节。关于远程代理的代码示例，我自己实现了一个简单的 RPC 框架 Demo，放到了 GitHub 中，你可以点击这里的链接查看。 我们再来看代理模式在缓存中的应用。假设我们要开发一个接口请求的缓存功能，对于某些接口请求，如果入参相同，在设定的过期时间内，直接返回缓存结果，而不用重新进行逻辑处理。比如，针对获取用户个人信息的需求，我们可以开发两个接口，一个支持缓存，一个支持实时查询。对于需要实时数据的需求，我们让其调用实时查询接口，对于不需要实时数据的需求，我们让其调用支持缓存的接口。那如何来实现接口请求的缓存功能呢？最简单的实现方法就是刚刚我们讲到的，给每个需要支持缓存的查询需求都开发两个不同的接口，一个支持缓存，一个支持实时查询。但是，这样做显然增加了开发成本，而且会让代码看起来非常臃肿（接口个数成倍增加），也不方便缓存接口的集中管理（增加、删除缓存接口）、集中配置（比如配置每个接口缓存过期时间）。针对这些问题，代理模式就能派上用场了，确切地说，应该是动态代理。如果是基于 Spring 框架来开发的话，那就可以在 AOP 切面中完成接口缓存的功能。在应用启动的时候，我们从配置文件中加载需要支持缓存的接口，以及相应的缓存策略（比如过期时间）等。当请求到来的时候，我们在 AOP 切面中拦截请求，如果请求中带有支持缓存的字段（比如 http://…?..\u0026cached=true），我们便从缓存（内存缓存或者 Redis 缓存等）中获取数据直接返回。 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:2:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"decorator 装饰模式使用对象组合的方式动态改变或增加对象行为。 Go语言借助于匿名组合和非入侵式接口可以很方便实现装饰模式。 使用匿名组合，在装饰器中不必显式定义转调原对象方法。 装饰器模式主要解决继承关系过于复杂的问题，通过组合来替代继承。它主要的作用是给原始类添加增强功能。这也是判断是否该用装饰器模式的一个重要的依据。除此之外，装饰器模式还有一个特点，那就是可以对原始类嵌套使用多个装饰器。为了满足这个应用场景，在设计的时候，装饰器类需要跟原始类继承相同的抽象类或者接口。 package decorator type Component interface { Calc() int } type ConcreteComponent struct{} func (*ConcreteComponent) Calc() int { return 0 } type MulDecorator struct { Component num int } func WarpMulDecorator(c Component, num int) Component { return \u0026MulDecorator{ Component: c, num: num, } } func (d *MulDecorator) Calc() int { return d.Component.Calc() * d.num } type AddDecorator struct { Component num int } func WarpAddDecorator(c Component, num int) Component { return \u0026AddDecorator{ Component: c, num: num, } } func (d *AddDecorator) Calc() int { return d.Component.Calc() + d.num } package decorator import \"fmt\" func ExampleDecorator() { var c Component = \u0026ConcreteComponent{} c = WarpAddDecorator(c, 10) c = WarpMulDecorator(c, 8) res := c.Calc() fmt.Printf(\"res %d\\n\", res) // Output: // res 80 } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:3:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"bridge 桥接模式分离抽象部分和实现部分。使得两部分独立扩展。 桥接模式类似于策略模式，区别在于策略模式封装一系列算法使得算法可以互相替换。 策略模式使抽象部分和实现部分分离，可以独立变化。 桥接模式有两种理解方式。第一种理解方式是“将抽象和实现解耦，让它们能独立开发”。这种理解方式比较特别，应用场景也不多。另一种理解方式更加简单，类似“组合优于继承”设计原则，这种理解方式更加通用，应用场景比较多。不管是哪种理解方式，它们的代码结构都是相同的，都是一种类之间的组合关系。 package bridge import \"fmt\" type AbstractMessage interface { SendMessage(text, to string) } type MessageImplementer interface { Send(text, to string) } type MessageSMS struct{} func ViaSMS() MessageImplementer { return \u0026MessageSMS{} } func (*MessageSMS) Send(text, to string) { fmt.Printf(\"send %s to %s via SMS\", text, to) } type MessageEmail struct{} func ViaEmail() MessageImplementer { return \u0026MessageEmail{} } func (*MessageEmail) Send(text, to string) { fmt.Printf(\"send %s to %s via Email\", text, to) } type CommonMessage struct { method MessageImplementer } func NewCommonMessage(method MessageImplementer) *CommonMessage { return \u0026CommonMessage{ method: method, } } func (m *CommonMessage) SendMessage(text, to string) { m.method.Send(text, to) } type UrgencyMessage struct { method MessageImplementer } func NewUrgencyMessage(method MessageImplementer) *UrgencyMessage { return \u0026UrgencyMessage{ method: method, } } func (m *UrgencyMessage) SendMessage(text, to string) { m.method.Send(fmt.Sprintf(\"[Urgency] %s\", text), to) } package bridge func ExampleCommonSMS() { m := NewCommonMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via SMS } func ExampleCommonEmail() { m := NewCommonMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via Email } func ExampleUrgencySMS() { m := NewUrgencyMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via SMS } func ExampleUrgencyEmail() { m := NewUrgencyMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via Email } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:4:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"如何实现支持不同类型和渠道的消息推送系统？ 相对于代理模式来说，桥接模式在实际的项目中并没有那么常用，你只需要简单了解 桥接模式的原理解析 桥接模式，也叫作桥梁模式，英文是 Bridge Design Pattern。这个模式可以说是 23 种设计模式中最难理解的模式之一了。我查阅了比较多的书籍和资料之后发现，对于这个模式有两种不同的理解方式。当然，这其中“最纯正”的理解方式，当属 GoF 的《设计模式》一书中对桥接模式的定义。毕竟，这 23 种经典的设计模式，最初就是由这本书总结出来的。在 GoF 的《设计模式》一书中，桥接模式是这么定义的：“Decouple an abstraction from its implementation so that the two can vary independently。”翻译成中文就是：“将抽象和实现解耦，让它们可以独立变化。”关于桥接模式，很多书籍、资料中，还有另外一种理解方式：“一个类存在两个（或多个）独立变化的维度，我们通过组合的方式，让这两个（或多个）维度可以独立进行扩展。”通过组合关系来替代继承关系，避免继承层次的指数级爆炸。这种理解方式非常类似于，我们之前讲过的“组合优于继承”设计原则，所以，这里我就不多解释了。我们重点看下 GoF 的理解方式。GoF 给出的定义非常的简短，单凭这一句话，估计没几个人能看懂是什么意思。所以，我们通过 JDBC 驱动的例子来解释一下。JDBC 驱动是桥接模式的经典应用。我们先来看一下，如何利用 JDBC 驱动来查询数据库。具体的代码如下所示： Class.forName(\"com.mysql.jdbc.Driver\");//加载及注册JDBC驱动程序 String url = \"jdbc:mysql://localhost:3306/sample_db?user=root\u0026password=your_password\"; Connection con = DriverManager.getConnection(url); Statement stmt = con.createStatement()； String query = \"select * from test\"; ResultSet rs=stmt.executeQuery(query); while(rs.next()) { rs.getString(1); rs.getInt(2); } 如果我们想要把 MySQL 数据库换成 Oracle 数据库，只需要把第一行代码中的 com.mysql.jdbc.Driver 换成 oracle.jdbc.driver.OracleDriver 就可以了。当然，也有更灵活的实现方式，我们可以把需要加载的 Driver 类写到配置文件中，当程序启动的时候，自动从配置文件中加载，这样在切换数据库的时候，我们都不需要修改代码，只需要修改配置文件就可以了。不管是改代码还是改配置，在项目中，从一个数据库切换到另一种数据库，都只需要改动很少的代码，或者完全不需要改动代码，那如此优雅的数据库切换是如何实现的呢？源码之下无秘密。要弄清楚这个问题，我们先从 com.mysql.jdbc.Driver 这个类的代码看起。我摘抄了部分相关代码，放到了这里，你可以看一下。 java package com.mysql.jdbc; import java.sql.SQLException; public class Driver extends NonRegisteringDriver implements java.sql.Driver { static { try { java.sql.DriverManager.registerDriver(new Driver()); } catch (SQLException E) { throw new RuntimeException(\"Can't register driver!\"); } } /** * Construct a new driver and register it with DriverManager * @throws SQLException if a database error occurs. */ public Driver() throws SQLException { // Required for Class.forName().newInstance() } } 结合 com.mysql.jdbc.Driver 的代码实现，我们可以发现，当执行 Class.forName(“com.mysql.jdbc.Driver”) 这条语句的时候，实际上是做了两件事情。第一件事情是要求 JVM 查找并加载指定的 Driver 类，第二件事情是执行该类的静态代码，也就是将 MySQL Driver 注册到 DriverManager 类中。现在，我们再来看一下，DriverManager 类是干什么用的。具体的代码如下所示。当我们把具体的 Driver 实现类（比如，com.mysql.jdbc.Driver）注册到 DriverManager 之后，后续所有对 JDBC 接口的调用，都会委派到对具体的 Driver 实现类来执行。而 Driver 实现类都实现了相同的接口（java.sql.Driver ），这也是可以灵活切换 Driver 的原因。 java public class DriverManager { private final static CopyOnWriteArrayList\u003cDriverInfo\u003e registeredDrivers = new CopyOnWriteArrayList\u003cDriverInfo\u003e(); //... static { loadInitialDrivers(); println(\"JDBC DriverManager initialized\"); } //... public static synchronized void registerDriver(java.sql.Driver driver) throws SQLException { if (driver != null) { registeredDrivers.addIfAbsent(new DriverInfo(driver)); } else { throw new NullPointerException(); } } public static Connection getConnection(String url, String user, String password) throws SQLException { java.util.Properties info = new java.util.Properties(); if (user != null) { info.put(\"user\", user); } if (password != null) { info.put(\"password\", password); } return (getConnection(url, info, Reflection.getCallerClass())); } //... } 桥接模式的定义是“将抽象和实现解耦，让它们可以独立变化”。那弄懂定义中“抽象”和“实现”两个概念，就是理解桥接模式的关键。那在 JDBC 这个例子中，什么是“抽象”？什么是“实现”呢？实际上，JDBC 本身就相当于“抽象”。注意，这里所说的“抽象”，指的并非“抽象类”或“接口”，而是跟具体的数据库无关的、被抽象出来的一套“类库”。具体的 Driver（比如，com.mysql.jdbc.Driver）就相当于“实现”。注意，这里所说的“实现”，也并非指“接口的实现类”，而是跟具体数据库相关的一套“类库”。JDBC 和 Driver 独立开发，通过对象之间的组合关系，组装在一起。JDBC 的所有逻辑操作，最终都委托给 Driver 来执行。我画了一张图帮助你理解，你可以结合着我刚才的讲解一块看。 桥接模式的应用举例 一个 API 接口监控告警的例子：根据不同的告警规则，触发不同类型的告警。告警支持多种通知渠道，包括：邮件、短信、微信、自动语音电话。通知的紧急程度有多种类型，包括：SEVERE（严重）、URGENCY（紧急）、NORMAL（普通）、TRIVIAL（无关紧要）。不同的紧急程度对应不同的通知渠道。比如，SERVE（严重）级别的消息会通过“自动语音电话”告知相关人员。在当时的代码实现中，关于发送告警信息那部分代码，我们只给出了粗略的设计，现在我们来一块实现一下。我们先来看最简单、最直接的一种实现方式。代码如下所示： java public enum NotificationEmergencyLevel { SEVERE, URGENCY, NORMAL, TRIVIAL } public class Notification { private List\u003cString\u003e emailAddresses; private List\u003cString\u003e telephones; private List\u003cString\u003e wechatIds; public Notification() {} public void setEmailAddress(List\u003cString\u003e emailAddress) { this.em","date":"2022-01-22 09:21:00","objectID":"/structural_type/:4:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"facade 门面模式，一个很典型的例子，Socket其实就是应用层与TCP/IP协议族通信的中间软件抽象层，就是一个门面模式，把复杂的TCP/IP协议族隐藏在Socket后面，对用户来说，只需要调用Socket规定的相关函数，让Socket去组织符合制定的协议数据然后进行通信。 API 为facade 模块的外观接口，大部分代码使用此接口简化对facade类的访问。 facade模块同时暴露了a和b 两个Module 的NewXXX和interface，其它代码如果需要使用细节功能时可以直接调用。 package facade import \"fmt\" func NewAPI() API { return \u0026apiImpl{ a: NewAModuleAPI(), b: NewBModuleAPI(), } } //API is facade interface of facade package type API interface { Test() string } //facade implement type apiImpl struct { a AModuleAPI b BModuleAPI } func (a *apiImpl) Test() string { aRet := a.a.TestA() bRet := a.b.TestB() return fmt.Sprintf(\"%s\\n%s\", aRet, bRet) } //NewAModuleAPI return new AModuleAPI func NewAModuleAPI() AModuleAPI { return \u0026aModuleImpl{} } //AModuleAPI ... type AModuleAPI interface { TestA() string } type aModuleImpl struct{} func (*aModuleImpl) TestA() string { return \"A module running\" } //NewBModuleAPI return new BModuleAPI func NewBModuleAPI() BModuleAPI { return \u0026bModuleImpl{} } //BModuleAPI ... type BModuleAPI interface { TestB() string } type bModuleImpl struct{} func (*bModuleImpl) TestB() string { return \"B module running\" } package facade import \"testing\" var expect = \"A module running\\nB module running\" // TestFacadeAPI ... func TestFacadeAPI(t *testing.T) { api := NewAPI() ret := api.Test() if ret != expect { t.Fatalf(\"expect %s, return %s\", expect, ret) } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:5:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"如何设计合理的接口粒度以兼顾接口的易用性和通用性？ 门面模式原理和实现都特别简单，应用场景也比较明确，主要在接口设计方面使用。 如果你平时的工作涉及接口开发，不知道你有没有遇到关于接口粒度的问题呢？ 为了保证接口的可复用性（或者叫通用性），我们需要将接口尽量设计得细粒度一点，职责单一一点。但是，如果接口的粒度过小，在接口的使用者开发一个业务功能时，就会导致需要调用 n 多细粒度的接口才能完成。调用者肯定会抱怨接口不好用。 相反，如果接口粒度设计得太大，一个接口返回 n 多数据，要做 n 多事情，就会导致接口不够通用、可复用性不好。接口不可复用，那针对不同的调用者的业务需求，我们就需要开发不同的接口来满足，这就会导致系统的接口无限膨胀。 门面模式的原理与实现 门面模式，也叫外观模式，英文全称是 Facade Design Pattern。在 GoF 的《设计模式》一书中，门面模式是这样定义的： Provide a unified interface to a set of interfaces in a subsystem. Facade Pattern defines a higher-level interface that makes the subsystem easier to use.翻译成中文就是：门面模式为子系统提供一组统一的接口，定义一组高层接口让子系统更易用。 门面模式的应用场景举例 解决易用性问题门面模式可以用来封装系统的底层实现，隐藏系统的复杂性，提供一组更加简单易用、更高层的接口。比如，**Linux 系统调用函数就可以看作一种“门面”。**它是 Linux 操作系统暴露给开发者的一组“特殊”的编程接口，它封装了底层更基础的 Linux 内核调用。再比如，Linux 的 Shell 命令，实际上也可以看作一种门面模式的应用。它继续封装系统调用，提供更加友好、简单的命令，让我们可以直接通过执行命令来跟操作系统交互。我们前面也多次讲过，设计原则、思想、模式很多都是相通的，是同一个道理不同角度的表述。实际上，从隐藏实现复杂性，提供更易用接口这个意图来看，门面模式有点类似之前讲到的迪米特法则（最少知识原则）和接口隔离原则：两个有交互的系统，只暴露有限的必要的接口。除此之外，门面模式还有点类似之前提到封装、抽象的设计思想，提供更抽象的接口，封装底层实现细节。 解决性能问题关于利用门面模式解决性能问题这一点，刚刚我们已经讲过了。我们通过将多个接口调用替换为一个门面接口调用，减少网络通信成本，提高 App 客户端的响应速度。所以，关于这点，我就不再举例说明了。我们来讨论一下这样一个问题：从代码实现的角度来看，该如何组织门面接口和非门面接口？如果门面接口不多，我们完全可以将它跟非门面接口放到一块，也不需要特殊标记，当作普通接口来用即可。如果门面接口很多，我们可以在已有的接口之上，再重新抽象出一层，专门放置门面接口，从类、包的命名上跟原来的接口层做区分。如果门面接口特别多，并且很多都是跨多个子系统的，我们可以将门面接口放到一个新的子系统中。 解决分布式事务问题关于利用门面模式来解决分布式事务问题，我们通过一个例子来解释一下。在一个金融系统中，有两个业务领域模型，用户和钱包。这两个业务领域模型都对外暴露了一系列接口，比如用户的增删改查接口、钱包的增删改查接口。假设有这样一个业务场景：在用户注册的时候，我们不仅会创建用户（在数据库 User 表中），还会给用户创建一个钱包（在数据库的 Wallet 表中）。对于这样一个简单的业务需求，我们可以通过依次调用用户的创建接口和钱包的创建接口来完成。但是，用户注册需要支持事务，也就是说，创建用户和钱包的两个操作，要么都成功，要么都失败，不能一个成功、一个失败。要支持两个接口调用在一个事务中执行，是比较难实现的，这涉及分布式事务问题。虽然我们可以通过引入分布式事务框架或者事后补偿的机制来解决，但代码实现都比较复杂。而最简单的解决方案是，利用数据库事务或者 Spring 框架提供的事务（如果是 Java 语言的话），在一个事务中，执行创建用户和创建钱包这两个 SQL 操作。这就要求两个 SQL 操作要在一个接口中完成，所以，我们可以借鉴门面模式的思想，再设计一个包裹这两个操作的新接口，让新接口在一个事务中执行两个 SQL 操作。 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:5:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"composite k8s的Job对象有应用。 组合模式统一对象和对象集，使得使用相同接口使用对象和对象集。 组合模式常用于树状结构，用于统一叶子节点和树节点的访问，并且可以用于应用某一操作到所有子节点。 package composite import \"fmt\" type Component interface { Parent() Component SetParent(Component) Name() string SetName(string) AddChild(Component) Print(string) } const ( LeafNode = iota CompositeNode ) func NewComponent(kind int, name string) Component { var c Component switch kind { case LeafNode: c = NewLeaf() case CompositeNode: c = NewComposite() } c.SetName(name) return c } type component struct { parent Component name string } func (c *component) Parent() Component { return c.parent } func (c *component) SetParent(parent Component) { c.parent = parent } func (c *component) Name() string { return c.name } func (c *component) SetName(name string) { c.name = name } func (c *component) AddChild(Component) {} func (c *component) Print(string) {} type Leaf struct { component } func NewLeaf() *Leaf { return \u0026Leaf{} } func (c *Leaf) Print(pre string) { fmt.Printf(\"%s-%s\\n\", pre, c.Name()) } type Composite struct { component childs []Component } func NewComposite() *Composite { return \u0026Composite{ childs: make([]Component, 0), } } func (c *Composite) AddChild(child Component) { child.SetParent(c) c.childs = append(c.childs, child) } func (c *Composite) Print(pre string) { fmt.Printf(\"%s+%s\\n\", pre, c.Name()) pre += \" \" for _, comp := range c.childs { comp.Print(pre) } } package composite func ExampleComposite() { root := NewComponent(CompositeNode, \"root\") c1 := NewComponent(CompositeNode, \"c1\") c2 := NewComponent(CompositeNode, \"c2\") c3 := NewComponent(CompositeNode, \"c3\") l1 := NewComponent(LeafNode, \"l1\") l2 := NewComponent(LeafNode, \"l2\") l3 := NewComponent(LeafNode, \"l3\") root.AddChild(c1) root.AddChild(c2) c1.AddChild(c3) c1.AddChild(l1) c2.AddChild(l2) c2.AddChild(l3) root.Print(\"\") // Output: // +root // +c1 // +c3 // -l1 // +c2 // -l2 // -l3 } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:6:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"如何设计实现支持递归遍历的文件系统目录树结构？ 组合模式跟我们之前讲的面向对象设计中的“组合关系（通过组合来组装两个类）”，完全是两码事。这里讲的“组合模式”，主要是用来处理树形结构数据。这里的“数据”，你可以简单理解为一组对象集合，待会我们会详细讲解。正因为其应用场景的特殊性，数据必须能表示成树形结构，这也导致了这种模式在实际的项目开发中并不那么常用。但是，一旦数据满足树形结构，应用这种模式就能发挥很大的作用，能让代码变得非常简洁。 组合模式的原理与实现 在 GoF 的《设计模式》一书中，组合模式是这样定义的：Compose objects into tree structure to represent part-whole hierarchies.Composite lets client treat individual objects and compositions of objects uniformly.翻译成中文就是：将一组对象组织（Compose）成树形结构，以表示一种“部分 - 整体”的层次结构。组合让客户端（在很多设计模式书籍中，“客户端”代指代码的使用者。）可以统一单个对象和组合对象的处理逻辑。 “将一组对象（文件和目录）组织成树形结构，以表示一种‘部分 - 整体’的层次结构（目录与子目录的嵌套结构）。组合模式让客户端可以统一单个对象（文件）和组合对象（目录）的处理逻辑（递归遍历）。” 实际上，刚才讲的这种组合模式的设计思路，与其说是一种设计模式，倒不如说是对业务场景的一种数据结构和算法的抽象。其中，数据可以表示成树这种数据结构，业务需求可以通过在树上的递归遍历算法来实现。 组合模式的应用场景举例 在实际的项目中，遇到类似的可以表示成树形结构的业务场景，你只要“照葫芦画瓢”去设计就可以了。 假设我们在开发一个 OA 系统（办公自动化系统）。公司的组织结构包含部门和员工两种数据类型。其中，部门又可以包含子部门和员工。 我们希望在内存中构建整个公司的人员架构图（部门、子部门、员工的隶属关系），并且提供接口计算出部门的薪资成本（隶属于这个部门的所有员工的薪资和）。部门包含子部门和员工，这是一种嵌套结构，可以表示成树这种数据结构。计算每个部门的薪资开支这样一个需求，也可以通过在树上的遍历算法来实现。所以，从这个角度来看，这个应用场景可以使用组合模式来设计和实现。 java public abstract class HumanResource { protected long id; protected double salary; public HumanResource(long id) { this.id = id; } public long getId() { return id; } public abstract double calculateSalary(); } public class Employee extends HumanResource { public Employee(long id, double salary) { super(id); this.salary = salary; } @Override public double calculateSalary() { return salary; } } public class Department extends HumanResource { private List\u003cHumanResource\u003e subNodes = new ArrayList\u003c\u003e(); public Department(long id) { super(id); } @Override public double calculateSalary() { double totalSalary = 0; for (HumanResource hr : subNodes) { totalSalary += hr.calculateSalary(); } this.salary = totalSalary; return totalSalary; } public void addSubNode(HumanResource hr) { subNodes.add(hr); } } // 构建组织架构的代码 public class Demo { private static final long ORGANIZATION_ROOT_ID = 1001; private DepartmentRepo departmentRepo; // 依赖注入 private EmployeeRepo employeeRepo; // 依赖注入 public void buildOrganization() { Department rootDepartment = new Department(ORGANIZATION_ROOT_ID); buildOrganization(rootDepartment); } private void buildOrganization(Department department) { List\u003cLong\u003e subDepartmentIds = departmentRepo.getSubDepartmentIds(department.getId()); for (Long subDepartmentId : subDepartmentIds) { Department subDepartment = new Department(subDepartmentId); department.addSubNode(subDepartment); buildOrganization(subDepartment); } List\u003cLong\u003e employeeIds = employeeRepo.getDepartmentEmployeeIds(department.getId()); for (Long employeeId : employeeIds) { double salary = employeeRepo.getEmployeeSalary(employeeId); department.addSubNode(new Employee(employeeId, salary)); } } } 组合模式的设计思路，与其说是一种设计模式，倒不如说是对业务场景的一种数据结构和算法的抽象。其中，数据可以表示成树这种数据结构，业务需求可以通过在树上的递归遍历算法来实现。组合模式，将一组对象组织成树形结构，将单个对象和组合对象都看做树中的节点，以统一处理逻辑，并且它利用树形结构的特点，递归地处理每个子树，依次简化代码实现。使用组合模式的前提在于，你的业务场景必须能够表示成树形结构。所以，组合模式的应用场景也比较局限，它并不是一种很常用的设计模式。 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:6:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"flyweight 享元模式从对象中剥离出不发生改变且多个实例需要的重复数据，独立出一个享元，使多个对象共享，从而节省内存以及减少对象数量。 package flyweight import \"fmt\" type ImageFlyweightFactory struct { maps map[string]*ImageFlyweight } var imageFactory *ImageFlyweightFactory func GetImageFlyweightFactory() *ImageFlyweightFactory { if imageFactory == nil { imageFactory = \u0026ImageFlyweightFactory{ maps: make(map[string]*ImageFlyweight), } } return imageFactory } func (f *ImageFlyweightFactory) Get(filename string) *ImageFlyweight { image := f.maps[filename] if image == nil { image = NewImageFlyweight(filename) f.maps[filename] = image } return image } type ImageFlyweight struct { data string } func NewImageFlyweight(filename string) *ImageFlyweight { // Load image file data := fmt.Sprintf(\"image data %s\", filename) return \u0026ImageFlyweight{ data: data, } } func (i *ImageFlyweight) Data() string { return i.data } type ImageViewer struct { *ImageFlyweight } func NewImageViewer(filename string) *ImageViewer { image := GetImageFlyweightFactory().Get(filename) return \u0026ImageViewer{ ImageFlyweight: image, } } func (i *ImageViewer) Display() { fmt.Printf(\"Display: %s\\n\", i.Data()) } package flyweight import \"testing\" func ExampleFlyweight() { viewer := NewImageViewer(\"image1.png\") viewer.Display() // Output: // Display: image data image1.png } func TestFlyweight(t *testing.T) { viewer1 := NewImageViewer(\"image1.png\") viewer2 := NewImageViewer(\"image1.png\") if viewer1.ImageFlyweight != viewer2.ImageFlyweight { t.Fail() } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:7:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"如何利用享元模式优化文本编辑器的内存占用？ 跟其他所有的设计模式类似，享元模式的原理和实现也非常简单。今天，我会通过棋牌游戏和文本编辑器两个实际的例子来讲解。除此之外，我还会讲到它跟单例、缓存、对象池的区别和联系 享元模式原理与实现 所谓“享元”，顾名思义就是被共享的单元。享元模式的意图是复用对象，节省内存，前提是享元对象是不可变对象。 具体来讲，当一个系统中存在大量重复对象的时候，如果这些重复的对象是不可变对象，我们就可以利用享元模式将对象设计成享元，在内存中只保留一份实例，供多处代码引用。这样可以减少内存中对象的数量，起到节省内存的目的。实际上，不仅仅相同对象可以设计成享元，对于相似对象，我们也可以将这些对象中相同的部分（字段）提取出来，设计成享元，让这些大量相似对象引用这些享元。 这里我稍微解释一下，定义中的“不可变对象”指的是，一旦通过构造函数初始化完成之后，它的状态（对象的成员变量或者属性）就不会再被修改了。所以，不可变对象不能暴露任何 set() 等修改内部状态的方法。之所以要求享元是不可变对象，那是因为它会被多处代码共享使用，避免一处代码对享元进行了修改，影响到其他使用它的代码。 享元模式 vs 单例、缓存、对象池 在上面的讲解中，我们多次提到“共享”“缓存”“复用”这些字眼，那它跟单例、缓存、对象池这些概念有什么区别呢？我们来简单对比一下。 我们先来看享元模式跟单例的区别。在单例模式中，一个类只能创建一个对象，而在享元模式中，一个类可以创建多个对象，每个对象被多处代码引用共享。实际上，享元模式有点类似于之前讲到的单例的变体：多例。我们前面也多次提到，区别两种设计模式，不能光看代码实现，而是要看设计意图，也就是要解决的问题。尽管从代码实现上来看，享元模式和多例有很多相似之处，但从设计意图上来看，它们是完全不同的。应用享元模式是为了对象复用，节省内存，而应用多例模式是为了限制对象的个数。 我们再来看享元模式跟缓存的区别。在享元模式的实现中，我们通过工厂类来“缓存”已经创建好的对象。这里的“缓存”实际上是“存储”的意思，跟我们平时所说的“数据库缓存”“CPU 缓存”“MemCache 缓存”是两回事。我们平时所讲的缓存，主要是为了提高访问效率，而非复用。 最后我们来看享元模式跟对象池的区别。对象池、连接池（比如数据库连接池）、线程池等也是为了复用，那它们跟享元模式有什么区别呢？你可能对连接池、线程池比较熟悉，对对象池比较陌生，所以，这里我简单解释一下对象池。像 C++ 这样的编程语言，内存的管理是由程序员负责的。为了避免频繁地进行对象创建和释放导致内存碎片，我们可以预先申请一片连续的内存空间，也就是这里说的对象池。每次创建对象时，我们从对象池中直接取出一个空闲对象来使用，对象使用完成之后，再放回到对象池中以供后续复用，而非直接释放掉。虽然对象池、连接池、线程池、享元模式都是为了复用，但是，如果我们再细致地抠一抠“复用”这个字眼的话，对象池、连接池、线程池等池化技术中的“复用”和享元模式中的“复用”实际上是不同的概念。池化技术中的“复用”可以理解为“重复使用”，主要目的是节省时间（比如从数据库池中取一个连接，不需要重新创建）。在任意时刻，每一个对象、连接、线程，并不会被多处使用，而是被一个使用者独占，当使用完成之后，放回到池中，再由其他使用者重复利用。享元模式中的“复用”可以理解为“共享使用”，在整个生命周期中，都是被所有使用者共享的，主要目的是节省空间。 总结 享元模式的原理所谓“享元”，顾名思义就是被共享的单元。享元模式的意图是复用对象，节省内存，前提是享元对象是不可变对象。具体来讲，当一个系统中存在大量重复对象的时候，我们就可以利用享元模式，将对象设计成享元，在内存中只保留一份实例，供多处代码引用，这样可以减少内存中对象的数量，以起到节省内存的目的。实际上，不仅仅相同对象可以设计成享元，对于相似对象，我们也可以将这些对象中相同的部分（字段），提取出来设计成享元，让这些大量相似对象引用这些享元。 享元模式的实现享元模式的代码实现非常简单，主要是通过工厂模式，在工厂类中，通过一个 Map 或者 List 来缓存已经创建好的享元对象，以达到复用的目的。 享元模式 VS 单例、缓存、对象池我们前面也多次提到，区别两种设计模式，不能光看代码实现，而是要看设计意图，也就是要解决的问题。这里的区别也不例外。我们可以用简单几句话来概括一下它们之间的区别。应用单例模式是为了保证对象全局唯一。应用享元模式是为了实现对象复用，节省内存。缓存是为了提高访问效率，而非复用。池化技术中的“复用”理解为“重复使用”，主要是为了节省时间。 参考 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:7:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"行为型设计模式 常用的有：观察者模式、模板模式、策略模式、职责链模式、迭代器模式、状态模式。 不常用的有：命令模式、备忘录模式、解释器模式、访问者模式、中介模式。 ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:0:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"observer 观察者模式用于触发联动。 一个对象的改变会触发其它观察者的相关动作，而此对象无需关心连动对象的具体实现。 package observer import \"fmt\" type Subject struct { observers []Observer context string } func NewSubject() *Subject { return \u0026Subject{ observers: make([]Observer, 0), } } func (s *Subject) Attach(o Observer) { s.observers = append(s.observers, o) } func (s *Subject) notify() { for _, o := range s.observers { o.Update(s) } } func (s *Subject) UpdateContext(context string) { s.context = context s.notify() } type Observer interface { Update(*Subject) } type Reader struct { name string } func NewReader(name string) *Reader { return \u0026Reader{ name: name, } } func (r *Reader) Update(s *Subject) { fmt.Printf(\"%s receive %s\\n\", r.name, s.context) } package observer func ExampleObserver() { subject := NewSubject() reader1 := NewReader(\"reader1\") reader2 := NewReader(\"reader2\") reader3 := NewReader(\"reader3\") subject.Attach(reader1) subject.Attach(reader2) subject.Attach(reader3) subject.UpdateContext(\"observer mode\") // Output: // reader1 receive observer mode // reader2 receive observer mode // reader3 receive observer mode } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:1:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"template method 模版方法模式使用继承机制，把通用步骤和通用方法放到父类中，把具体实现延迟到子类中实现。使得实现符合开闭原则。 如实例代码中通用步骤在父类中实现（准备、下载、保存、收尾）下载和保存的具体实现留到子类中，并且提供 保存方法的默认实现。 因为Golang不提供继承机制，需要使用匿名组合模拟实现继承。 此处需要注意：因为父类需要调用子类方法，所以子类需要匿名组合父类的同时，父类需要持有子类的引用。 package templatemethod import \"fmt\" type Downloader interface { Download(uri string) } type template struct { implement uri string } type implement interface { download() save() } func newTemplate(impl implement) *template { return \u0026template{ implement: impl, } } func (t *template) Download(uri string) { t.uri = uri fmt.Print(\"prepare downloading\\n\") t.implement.download() t.implement.save() fmt.Print(\"finish downloading\\n\") } func (t *template) save() { fmt.Print(\"default save\\n\") } type HTTPDownloader struct { *template } func NewHTTPDownloader() Downloader { downloader := \u0026HTTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *HTTPDownloader) download() { fmt.Printf(\"download %s via http\\n\", d.uri) } func (*HTTPDownloader) save() { fmt.Printf(\"http save\\n\") } type FTPDownloader struct { *template } func NewFTPDownloader() Downloader { downloader := \u0026FTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *FTPDownloader) download() { fmt.Printf(\"download %s via ftp\\n\", d.uri) } package templatemethod func ExampleHTTPDownloader() { var downloader Downloader = NewHTTPDownloader() downloader.Download(\"http://example.com/abc.zip\") // Output: // prepare downloading // download http://example.com/abc.zip via http // http save // finish downloading } func ExampleFTPDownloader() { var downloader Downloader = NewFTPDownloader() downloader.Download(\"ftp://example.com/abc.zip\") // Output: // prepare downloading // download ftp://example.com/abc.zip via ftp // default save // finish downloading } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:2:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"strategy 定义一系列算法，让这些算法在运行时可以互换，使得分离算法，符合开闭原则。 package strategy import \"fmt\" type Payment struct { context *PaymentContext strategy PaymentStrategy } type PaymentContext struct { Name, CardID string Money int } func NewPayment(name, cardid string, money int, strategy PaymentStrategy) *Payment { return \u0026Payment{ context: \u0026PaymentContext{ Name: name, CardID: cardid, Money: money, }, strategy: strategy, } } func (p *Payment) Pay() { p.strategy.Pay(p.context) } type PaymentStrategy interface { Pay(*PaymentContext) } type Cash struct{} func (*Cash) Pay(ctx *PaymentContext) { fmt.Printf(\"Pay $%d to %s by cash\", ctx.Money, ctx.Name) } type Bank struct{} func (*Bank) Pay(ctx *PaymentContext) { fmt.Printf(\"Pay $%d to %s by bank account %s\", ctx.Money, ctx.Name, ctx.CardID) } package strategy func ExamplePayByCash() { payment := NewPayment(\"Ada\", \"\", 123, \u0026Cash{}) payment.Pay() // Output: // Pay $123 to Ada by cash } func ExamplePayByBank() { payment := NewPayment(\"Bob\", \"0002\", 888, \u0026Bank{}) payment.Pay() // Output: // Pay $888 to Bob by bank account 0002 } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:3:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"chain of responsibility 职责链模式用于分离不同职责，并且动态组合相关职责。 Golang实现职责链模式时候，因为没有继承的支持，使用链对象包涵职责的方式，即： 链对象包含当前职责对象以及下一个职责链。 职责对象提供接口表示是否能处理对应请求。 职责对象提供处理函数处理相关职责。 同时可在职责链类中实现职责接口相关函数，使职责链对象可以当做一般职责对象是用。 package chain import \"fmt\" type Manager interface { HaveRight(money int) bool HandleFeeRequest(name string, money int) bool } type RequestChain struct { Manager successor *RequestChain } func (r *RequestChain) SetSuccessor(m *RequestChain) { r.successor = m } func (r *RequestChain) HandleFeeRequest(name string, money int) bool { if r.Manager.HaveRight(money) { return r.Manager.HandleFeeRequest(name, money) } if r.successor != nil { return r.successor.HandleFeeRequest(name, money) } return false } func (r *RequestChain) HaveRight(money int) bool { return true } type ProjectManager struct{} func NewProjectManagerChain() *RequestChain { return \u0026RequestChain{ Manager: \u0026ProjectManager{}, } } func (*ProjectManager) HaveRight(money int) bool { return money \u003c 500 } func (*ProjectManager) HandleFeeRequest(name string, money int) bool { if name == \"bob\" { fmt.Printf(\"Project manager permit %s %d fee request\\n\", name, money) return true } fmt.Printf(\"Project manager don't permit %s %d fee request\\n\", name, money) return false } type DepManager struct{} func NewDepManagerChain() *RequestChain { return \u0026RequestChain{ Manager: \u0026DepManager{}, } } func (*DepManager) HaveRight(money int) bool { return money \u003c 5000 } func (*DepManager) HandleFeeRequest(name string, money int) bool { if name == \"tom\" { fmt.Printf(\"Dep manager permit %s %d fee request\\n\", name, money) return true } fmt.Printf(\"Dep manager don't permit %s %d fee request\\n\", name, money) return false } type GeneralManager struct{} func NewGeneralManagerChain() *RequestChain { return \u0026RequestChain{ Manager: \u0026GeneralManager{}, } } func (*GeneralManager) HaveRight(money int) bool { return true } func (*GeneralManager) HandleFeeRequest(name string, money int) bool { if name == \"ada\" { fmt.Printf(\"General manager permit %s %d fee request\\n\", name, money) return true } fmt.Printf(\"General manager don't permit %s %d fee request\\n\", name, money) return false } package chain func ExampleChain() { c1 := NewProjectManagerChain() c2 := NewDepManagerChain() c3 := NewGeneralManagerChain() c1.SetSuccessor(c2) c2.SetSuccessor(c3) var c Manager = c1 c.HandleFeeRequest(\"bob\", 400) c.HandleFeeRequest(\"tom\", 1400) c.HandleFeeRequest(\"ada\", 10000) c.HandleFeeRequest(\"floar\", 400) // Output: // Project manager permit bob 400 fee request // Dep manager permit tom 1400 fee request // General manager permit ada 10000 fee request // Project manager don't permit floar 400 fee request } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:4:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"iterator 迭代器模式用于使用相同方式送代不同类型集合或者隐藏集合类型的具体实现。 可以使用迭代器模式使遍历同时应用送代策略，如请求新对象、过滤、处理对象等。 package iterator import \"fmt\" type Aggregate interface { Iterator() Iterator } type Iterator interface { First() IsDone() bool Next() interface{} } type Numbers struct { start, end int } func NewNumbers(start, end int) *Numbers { return \u0026Numbers{ start: start, end: end, } } func (n *Numbers) Iterator() Iterator { return \u0026NumbersIterator{ numbers: n, next: n.start, } } type NumbersIterator struct { numbers *Numbers next int } func (i *NumbersIterator) First() { i.next = i.numbers.start } func (i *NumbersIterator) IsDone() bool { return i.next \u003e i.numbers.end } func (i *NumbersIterator) Next() interface{} { if !i.IsDone() { next := i.next i.next++ return next } return nil } func IteratorPrint(i Iterator) { for i.First(); !i.IsDone(); { c := i.Next() fmt.Printf(\"%#v\\n\", c) } } package iterator func ExampleIterator() { var aggregate Aggregate aggregate = NewNumbers(1, 10) IteratorPrint(aggregate.Iterator()) // Output: // 1 // 2 // 3 // 4 // 5 // 6 // 7 // 8 // 9 // 10 } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:5:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"state 状态模式用于分离状态和行为。 package state import \"fmt\" type Week interface { Today() Next(*DayContext) } type DayContext struct { today Week } func NewDayContext() *DayContext { return \u0026DayContext{ today: \u0026Sunday{}, } } func (d *DayContext) Today() { d.today.Today() } func (d *DayContext) Next() { d.today.Next(d) } type Sunday struct{} func (*Sunday) Today() { fmt.Printf(\"Sunday\\n\") } func (*Sunday) Next(ctx *DayContext) { ctx.today = \u0026Monday{} } type Monday struct{} func (*Monday) Today() { fmt.Printf(\"Monday\\n\") } func (*Monday) Next(ctx *DayContext) { ctx.today = \u0026Tuesday{} } type Tuesday struct{} func (*Tuesday) Today() { fmt.Printf(\"Tuesday\\n\") } func (*Tuesday) Next(ctx *DayContext) { ctx.today = \u0026Wednesday{} } type Wednesday struct{} func (*Wednesday) Today() { fmt.Printf(\"Wednesday\\n\") } func (*Wednesday) Next(ctx *DayContext) { ctx.today = \u0026Thursday{} } type Thursday struct{} func (*Thursday) Today() { fmt.Printf(\"Thursday\\n\") } func (*Thursday) Next(ctx *DayContext) { ctx.today = \u0026Friday{} } type Friday struct{} func (*Friday) Today() { fmt.Printf(\"Friday\\n\") } func (*Friday) Next(ctx *DayContext) { ctx.today = \u0026Saturday{} } type Saturday struct{} func (*Saturday) Today() { fmt.Printf(\"Saturday\\n\") } func (*Saturday) Next(ctx *DayContext) { ctx.today = \u0026Sunday{} } package state func ExampleWeek() { ctx := NewDayContext() todayAndNext := func() { ctx.Today() ctx.Next() } for i := 0; i \u003c 8; i++ { todayAndNext() } // Output: // Sunday // Monday // Tuesday // Wednesday // Thursday // Friday // Saturday // Sunday } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:6:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"不常用 ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:7:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"command 命令模式本质是把某个对象的方法调用封装到对象中，方便传递、存储、调用。 示例中把主板单中的启动(start)方法和重启(reboot)方法封装为命令对象，再传递到主机(box)对象中。于两个按钮进行绑定： 第一个机箱(box1)设置按钮1(button1) 为开机按钮2(button2)为重启。 第二个机箱(box1)设置按钮2(button2) 为开机按钮1(button1)为重启。 从而得到配置灵活性。 除了配置灵活外，使用命令模式还可以用作： 批处理 任务队列 undo, redo 等把具体命令封装到对象中使用的场合 package command import \"fmt\" type Command interface { Execute() } type StartCommand struct { mb *MotherBoard } func NewStartCommand(mb *MotherBoard) *StartCommand { return \u0026StartCommand{ mb: mb, } } func (c *StartCommand) Execute() { c.mb.Start() } type RebootCommand struct { mb *MotherBoard } func NewRebootCommand(mb *MotherBoard) *RebootCommand { return \u0026RebootCommand{ mb: mb, } } func (c *RebootCommand) Execute() { c.mb.Reboot() } type MotherBoard struct{} func (*MotherBoard) Start() { fmt.Print(\"system starting\\n\") } func (*MotherBoard) Reboot() { fmt.Print(\"system rebooting\\n\") } type Box struct { button1 Command button2 Command } func NewBox(button1, button2 Command) *Box { return \u0026Box{ button1: button1, button2: button2, } } func (b *Box) PressButton1() { b.button1.Execute() } func (b *Box) PressButton2() { b.button2.Execute() } package command func ExampleCommand() { mb := \u0026MotherBoard{} startCommand := NewStartCommand(mb) rebootCommand := NewRebootCommand(mb) box1 := NewBox(startCommand, rebootCommand) box1.PressButton1() box1.PressButton2() box2 := NewBox(rebootCommand, startCommand) box2.PressButton1() box2.PressButton2() // Output: // system starting // system rebooting // system rebooting // system starting } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:8:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"memento 备忘录模式用于保存程序内部状态到外部，又不希望暴露内部状态的情形。 程序内部状态使用窄接口传递给外部进行存储，从而不暴露程序实现细节。 备忘录模式同时可以离线保存内部状态，如保存到数据库，文件等。 package memento import \"fmt\" type Memento interface{} type Game struct { hp, mp int } type gameMemento struct { hp, mp int } func (g *Game) Play(mpDelta, hpDelta int) { g.mp += mpDelta g.hp += hpDelta } func (g *Game) Save() Memento { return \u0026gameMemento{ hp: g.hp, mp: g.mp, } } func (g *Game) Load(m Memento) { gm := m.(*gameMemento) g.mp = gm.mp g.hp = gm.hp } func (g *Game) Status() { fmt.Printf(\"Current HP:%d, MP:%d\\n\", g.hp, g.mp) } package memento func ExampleGame() { game := \u0026Game{ hp: 10, mp: 10, } game.Status() progress := game.Save() game.Play(-2, -3) game.Status() game.Load(progress) game.Status() // Output: // Current HP:10, MP:10 // Current HP:7, MP:8 // Current HP:10, MP:10 } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:9:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"interpreter 解释器模式定义一套语言文法，并设计该语言解释器，使用户能使用特定文法控制解释器行为。 解释器模式的意义在于，它分离多种复杂功能的实现，每个功能只需关注自身的解释。 对于调用者不用关心内部的解释器的工作，只需要用简单的方式组合命令就可以。 package interpreter import ( \"strconv\" \"strings\" ) type Node interface { Interpret() int } type ValNode struct { val int } func (n *ValNode) Interpret() int { return n.val } type AddNode struct { left, right Node } func (n *AddNode) Interpret() int { return n.left.Interpret() + n.right.Interpret() } type MinNode struct { left, right Node } func (n *MinNode) Interpret() int { return n.left.Interpret() - n.right.Interpret() } type Parser struct { exp []string index int prev Node } func (p *Parser) Parse(exp string) { p.exp = strings.Split(exp, \" \") for { if p.index \u003e= len(p.exp) { return } switch p.exp[p.index] { case \"+\": p.prev = p.newAddNode() case \"-\": p.prev = p.newMinNode() default: p.prev = p.newValNode() } } } func (p *Parser) newAddNode() Node { p.index++ return \u0026AddNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newMinNode() Node { p.index++ return \u0026MinNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newValNode() Node { v, _ := strconv.Atoi(p.exp[p.index]) p.index++ return \u0026ValNode{ val: v, } } func (p *Parser) Result() Node { return p.prev } package interpreter import \"testing\" func TestInterpreter(t *testing.T) { p := \u0026Parser{} p.Parse(\"1 + 2 + 3 - 4 + 5 - 6\") res := p.Result().Interpret() expect := 1 if res != expect { t.Fatalf(\"expect %d got %d\", expect, res) } } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:10:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"visitor 访问者模式可以给一系列对象透明的添加功能，并且把相关代码封装到一个类中。 对象只要预留访问者接口Accept则后期为对象添加功能的时候就不需要改动对象。 package visitor import \"fmt\" type Customer interface { Accept(Visitor) } type Visitor interface { Visit(Customer) } type EnterpriseCustomer struct { name string } type CustomerCol struct { customers []Customer } func (c *CustomerCol) Add(customer Customer) { c.customers = append(c.customers, customer) } func (c *CustomerCol) Accept(visitor Visitor) { for _, customer := range c.customers { customer.Accept(visitor) } } func NewEnterpriseCustomer(name string) *EnterpriseCustomer { return \u0026EnterpriseCustomer{ name: name, } } func (c *EnterpriseCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type IndividualCustomer struct { name string } func NewIndividualCustomer(name string) *IndividualCustomer { return \u0026IndividualCustomer{ name: name, } } func (c *IndividualCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type ServiceRequestVisitor struct{} func (*ServiceRequestVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\"serving enterprise customer %s\\n\", c.name) case *IndividualCustomer: fmt.Printf(\"serving individual customer %s\\n\", c.name) } } // only for enterprise type AnalysisVisitor struct{} func (*AnalysisVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\"analysis enterprise customer %s\\n\", c.name) } } package visitor func ExampleRequestVisitor() { c := \u0026CustomerCol{} c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Accept(\u0026ServiceRequestVisitor{}) // Output: // serving enterprise customer A company // serving enterprise customer B company // serving individual customer bob } func ExampleAnalysis() { c := \u0026CustomerCol{} c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Accept(\u0026AnalysisVisitor{}) // Output: // analysis enterprise customer A company // analysis enterprise customer B company } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:11:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"mediator 中介者模式封装对象之间互交，使依赖变的简单，并且使复杂互交简单化，封装在中介者中。 例子中的中介者使用单例模式生成中介者。 中介者的change使用switch判断类型。 package mediator import ( \"fmt\" \"strings\" ) type CDDriver struct { Data string } func (c *CDDriver) ReadData() { c.Data = \"music,image\" fmt.Printf(\"CDDriver: reading data %s\\n\", c.Data) GetMediatorInstance().changed(c) } type CPU struct { Video string Sound string } func (c *CPU) Process(data string) { sp := strings.Split(data, \",\") c.Sound = sp[0] c.Video = sp[1] fmt.Printf(\"CPU: split data with Sound %s, Video %s\\n\", c.Sound, c.Video) GetMediatorInstance().changed(c) } type VideoCard struct { Data string } func (v *VideoCard) Display(data string) { v.Data = data fmt.Printf(\"VideoCard: display %s\\n\", v.Data) GetMediatorInstance().changed(v) } type SoundCard struct { Data string } func (s *SoundCard) Play(data string) { s.Data = data fmt.Printf(\"SoundCard: play %s\\n\", s.Data) GetMediatorInstance().changed(s) } type Mediator struct { CD *CDDriver CPU *CPU Video *VideoCard Sound *SoundCard } var mediator *Mediator func GetMediatorInstance() *Mediator { if mediator == nil { mediator = \u0026Mediator{} } return mediator } func (m *Mediator) changed(i interface{}) { switch inst := i.(type) { case *CDDriver: m.CPU.Process(inst.Data) case *CPU: m.Sound.Play(inst.Sound) m.Video.Display(inst.Video) } } package mediator import \"testing\" func TestMediator(t *testing.T) { mediator := GetMediatorInstance() mediator.CD = \u0026CDDriver{} mediator.CPU = \u0026CPU{} mediator.Video = \u0026VideoCard{} mediator.Sound = \u0026SoundCard{} //Tiggle mediator.CD.ReadData() if mediator.CD.Data != \"music,image\" { t.Fatalf(\"CD unexpect data %s\", mediator.CD.Data) } if mediator.CPU.Sound != \"music\" { t.Fatalf(\"CPU unexpect sound data %s\", mediator.CPU.Sound) } if mediator.CPU.Video != \"image\" { t.Fatalf(\"CPU unexpect video data %s\", mediator.CPU.Video) } if mediator.Video.Data != \"image\" { t.Fatalf(\"VidoeCard unexpect data %s\", mediator.Video.Data) } if mediator.Sound.Data != \"music\" { t.Fatalf(\"SoundCard unexpect data %s\", mediator.Sound.Data) } } 参考 ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:12:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"设计模式七大原则 我们在设计一些设计模式时，一般遵循如下七项基本原则，它们分别是: 单一职责原则 (Single Responsibility Principle) 开放-关闭原则 (Open-Closed Principle) 里氏替换原则 (Liskov Substitution Principle) 依赖倒转原则 (Dependence Inversion Principle) 接口隔离原则 (Interface Segregation Principle) 迪米特法则（Law Of Demeter） 组合/聚合复用原则 (Composite/Aggregate Reuse Principle) ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:13:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"创建型设计模式 常用的有：单例模式、工厂模式（工厂方法和抽象工厂）、建造者模式。不常用的有：原型模式。 创建型模式主要解决对象的创建问题，封装复杂的创建过程，解耦对象的创建代码和使用代码。其中，单例模式用来创建全局唯一的对象。工厂模式用来创建不同但是相关类型的对象（继承同一父类或者接口的一组子类），由给定的参数来决定创建哪种类型的对象。建造者模式是用来创建复杂对象，可以通过设置不同的可选参数，“定制化”地创建不同的对象。原型模式针对创建成本比较大的对象，利用对已有对象进行复制的方式进行创建，以达到节省创建时间的目的。 设计模式总览：http://c.biancheng.net/view/1320.html ","date":"2022-01-22 09:19:30","objectID":"/create_type/:0:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"simple factory go 语言没有构造函数一说，所以一般会定义NewXXX函数来初始化相关类。 NewXXX 函数根据参数返回不同接口时就是简单工厂模式。 在这个simplefactory包中只有API 接口和NewAPI函数为包外可见，封装了实现细节。 package simplefactory import \"fmt\" //API is interface type API interface { Say(name string) string } //NewAPI return Api instance by type func NewAPI(t int) API { if t == 1 { return \u0026hiAPI{} } else if t == 2 { return \u0026helloAPI{} } return nil } //hiAPI is one of API implement type hiAPI struct{} //Say hi to name func (*hiAPI) Say(name string) string { return fmt.Sprintf(\"Hi, %s\", name) } //HelloAPI is another API implement type helloAPI struct{} //Say hello to name func (*helloAPI) Say(name string) string { return fmt.Sprintf(\"Hello, %s\", name) } package simplefactory import \"testing\" //TestType1 test get hiapi with factory func TestType1(t *testing.T) { api := NewAPI(1) s := api.Say(\"Tom\") if s != \"Hi, Tom\" { t.Fatal(\"Type1 test fail\") } } func TestType2(t *testing.T) { api := NewAPI(2) s := api.Say(\"Tom\") if s != \"Hello, Tom\" { t.Fatal(\"Type2 test fail\") } } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:1:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"factory method 工厂方法模式使用子类的方式延迟生成对象到子类中实现。 Go中不存在继承 所以使用匿名组合来实现 package factorymethod //Operator 是被封装的实际类接口 type Operator interface { SetA(int) SetB(int) Result() int } //OperatorFactory 是工厂接口 type OperatorFactory interface { Create() Operator } //OperatorBase 是Operator 接口实现的基类，封装公用方法 type OperatorBase struct { a, b int } //SetA 设置 A func (o *OperatorBase) SetA(a int) { o.a = a } //SetB 设置 B func (o *OperatorBase) SetB(b int) { o.b = b } //PlusOperatorFactory 是 PlusOperator 的工厂类 type PlusOperatorFactory struct{} func (PlusOperatorFactory) Create() Operator { return \u0026PlusOperator{ OperatorBase: \u0026OperatorBase{}, } } //PlusOperator Operator 的实际加法实现 type PlusOperator struct { *OperatorBase } //Result 获取结果 func (o PlusOperator) Result() int { return o.a + o.b } //MinusOperatorFactory 是 MinusOperator 的工厂类 type MinusOperatorFactory struct{} func (MinusOperatorFactory) Create() Operator { return \u0026MinusOperator{ OperatorBase: \u0026OperatorBase{}, } } //MinusOperator Operator 的实际减法实现 type MinusOperator struct { *OperatorBase } //Result 获取结果 func (o MinusOperator) Result() int { return o.a - o.b } package factorymethod import \"testing\" func compute(factory OperatorFactory, a, b int) int { op := factory.Create() op.SetA(a) op.SetB(b) return op.Result() } func TestOperator(t *testing.T) { var ( factory OperatorFactory ) factory = PlusOperatorFactory{} if compute(factory, 1, 2) != 3 { t.Fatal(\"error with factory method pattern\") } factory = MinusOperatorFactory{} if compute(factory, 4, 2) != 2 { t.Fatal(\"error with factory method pattern\") } } 一般情况下，工厂模式分为三种更加细分的类型：简单工厂、工厂方法和抽象工厂。不过，在 GoF 的《设计模式》一书中，它将简单工厂模式看作是工厂方法模式的一种特例，所以工厂模式只被分成了工厂方法和抽象工厂两类。实际上，前面一种分类方法更加常见，所以，在今天的讲解中，我们沿用第一种分类方法。在这三种细分的工厂模式中，简单工厂、工厂方法原理比较简单，在实际的项目中也比较常用。而抽象工厂的原理稍微复杂点，在实际的项目中相对也不常用。所以，我们今天讲解的重点是前两种工厂模式。对于抽象工厂，你稍微了解一下即可。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:2:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"工厂方法 工厂方法模式比起简单工厂模式更加符合开闭原则（开闭原则，在面向对象编程领域中，规定“软件中的对象（类，模块，函数等等）应该对于扩展是开放的，但是对于修改是封闭的”，这意味着一个实体是允许在不改变它的源代码的前提下变更它的行为。该特性在产品化的环境中是特别有价值的，在这种环境中，改变源代码需要代码审查，单元测试以及诸如此类的用以确保产品使用质量的过程。遵循这种原则的代码在扩展时并不发生改变，因此无需上述的过程。——–百度百科）。 我们可以为工厂类再创建一个简单工厂，也就是工厂的工厂，用来创建工厂类对象 实际上，对于规则配置文件解析这个应用场景来说，工厂模式需要额外创建诸多 Factory 类，也会增加代码的复杂性，而且，每个 Factory 类只是做简单的 new 操作，功能非常单薄（只有一行代码），也没必要设计成独立的类，所以，在这个应用场景下，简单工厂模式简单好用，比工厂方法模式更加合适。 那什么时候该用工厂方法模式，而非简单工厂模式呢？ 我们前面提到，之所以将某个代码块剥离出来，独立为函数或者类，原因是这个代码块的逻辑过于复杂，剥离之后能让代码更加清晰，更加可读、可维护。但是，如果代码块本身并不复杂，就几行代码而已，我们完全没必要将它拆分成单独的函数或者类。 基于这个设计思想，当对象的创建逻辑比较复杂，不只是简单的 new 一下就可以，而是要组合其他类对象，做各种初始化操作的时候，我们推荐使用工厂方法模式，将复杂的创建逻辑拆分到多个工厂类中，让每个工厂类都不至于过于复杂。而使用简单工厂模式，将所有的创建逻辑都放到一个工厂类中，会导致这个工厂类变得很复杂。 除此之外，在某些场景下，如果对象不可复用，那工厂类每次都要返回不同的对象。如果我们使用简单工厂模式来实现，就只能选择第一种包含 if 分支逻辑的实现方式。如果我们还想避免烦人的 if-else 分支逻辑，这个时候，我们就推荐使用工厂方法模式。 总结 当创建逻辑比较复杂，是一个“大工程”的时候，我们就考虑使用工厂模式，封装对象的创建过程，将对象的创建和使用相分离。何为创建逻辑比较复杂呢？我总结了下面两种情况。第一种情况：类似规则配置解析的例子，代码中存在 if-else 分支判断，动态地根据不同的类型创建不同的对象。针对这种情况，我们就考虑使用工厂模式，将这一大坨 if-else 创建对象的代码抽离出来，放到工厂类中。还有一种情况，尽管我们不需要根据不同的类型创建不同的对象，但是，单个对象本身的创建过程比较复杂，比如前面提到的要组合其他类对象，做各种初始化操作。在这种情况下，我们也可以考虑使用工厂模式，将对象的创建过程封装到工厂类中。 对于第一种情况，当每个对象的创建逻辑都比较简单的时候，我推荐使用简单工厂模式，将多个对象的创建逻辑放到一个工厂类中。当每个对象的创建逻辑都比较复杂的时候，为了避免设计一个过于庞大的简单工厂类，我推荐使用工厂方法模式，将创建逻辑拆分得更细，每个对象的创建逻辑独立到各自的工厂类中。同理，对于第二种情况，因为单个对象本身的创建逻辑就比较复杂，所以，我建议使用工厂方法模式。 除了刚刚提到的这几种情况之外，如果创建对象的逻辑并不复杂，那我们就直接通过 new 来创建对象就可以了，不需要使用工厂模式。 现在，我们上升一个思维层面来看工厂模式，它的作用无外乎下面这四个。这也是判断要不要使用工厂模式的最本质的参考标准。 封装变化：创建逻辑有可能变化，封装成工厂类之后，创建逻辑的变更对调用者透明。 代码复用：创建代码抽离到独立的工厂类之后可以复用。 隔离复杂性：封装复杂的创建逻辑，调用者无需了解如何创建对象。 控制复杂度：将创建代码抽离出来，让原本的函数或类职责更单一，代码更简洁。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:2:1","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"如何设计实现一个Dependency Injection框架？ 总结 DI 容器在一些软件开发中已经成为了标配，比如 Spring IOC、Google Guice。但是，大部分人可能只是把它当作一个黑盒子来使用，并未真正去了解它的底层是如何实现的。当然，如果只是做一些简单的小项目，简单会用就足够了，但是，如果我们面对的是非常复杂的系统，当系统出现问题的时候，对底层原理的掌握程度，决定了我们排查问题的能力，直接影响到我们排查问题的效率。今天，我们讲解了一个简单的 DI 容器的实现原理，其核心逻辑主要包括：配置文件解析，以及根据配置文件通过“反射”语法来创建对象。其中，创建对象的过程就应用到了我们在学的工厂模式。对象创建、组装、管理完全有 DI 容器来负责，跟具体业务代码解耦，让程序员聚焦在业务代码的开发上。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:2:2","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"abstract factory 抽象工厂模式用于生成产品族的工厂，所生成的对象是有关联的。 如果抽象工厂退化成生成的对象无关联则成为工厂函数模式。 比如本例子中使用RDB和XML存储订单信息，抽象工厂分别能生成相关的主订单信息和订单详情信息。 如果业务逻辑中需要替换使用的时候只需要改动工厂函数相关的类就能替换使用不同的存储方式了。 package abstractfactory import \"fmt\" //OrderMainDAO 为订单主记录 type OrderMainDAO interface { SaveOrderMain() } //OrderDetailDAO 为订单详情纪录 type OrderDetailDAO interface { SaveOrderDetail() } //DAOFactory DAO 抽象模式工厂接口 type DAOFactory interface { CreateOrderMainDAO() OrderMainDAO CreateOrderDetailDAO() OrderDetailDAO } //RDBMainDAP 为关系型数据库的OrderMainDAO实现 type RDBMainDAO struct{} //SaveOrderMain ... func (*RDBMainDAO) SaveOrderMain() { fmt.Print(\"rdb main save\\n\") } //RDBDetailDAO 为关系型数据库的OrderDetailDAO实现 type RDBDetailDAO struct{} // SaveOrderDetail ... func (*RDBDetailDAO) SaveOrderDetail() { fmt.Print(\"rdb detail save\\n\") } //RDBDAOFactory 是RDB 抽象工厂实现 type RDBDAOFactory struct{} func (*RDBDAOFactory) CreateOrderMainDAO() OrderMainDAO { return \u0026RDBMainDAO{} } func (*RDBDAOFactory) CreateOrderDetailDAO() OrderDetailDAO { return \u0026RDBDetailDAO{} } //XMLMainDAO XML存储 type XMLMainDAO struct{} //SaveOrderMain ... func (*XMLMainDAO) SaveOrderMain() { fmt.Print(\"xml main save\\n\") } //XMLDetailDAO XML存储 type XMLDetailDAO struct{} // SaveOrderDetail ... func (*XMLDetailDAO) SaveOrderDetail() { fmt.Print(\"xml detail save\") } //XMLDAOFactory 是XML 抽象工厂实现 type XMLDAOFactory struct{} func (*XMLDAOFactory) CreateOrderMainDAO() OrderMainDAO { return \u0026XMLMainDAO{} } func (*XMLDAOFactory) CreateOrderDetailDAO() OrderDetailDAO { return \u0026XMLDetailDAO{} } package abstractfactory func getMainAndDetail(factory DAOFactory) { factory.CreateOrderMainDAO().SaveOrderMain() factory.CreateOrderDetailDAO().SaveOrderDetail() } func ExampleRdbFactory() { var factory DAOFactory factory = \u0026RDBDAOFactory{} getMainAndDetail(factory) // Output: // rdb main save // rdb detail save } func ExampleXmlFactory() { var factory DAOFactory factory = \u0026XMLDAOFactory{} getMainAndDetail(factory) // Output: // xml main save // xml detail save } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:3:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"builder package builder //Builder 是生成器接口 type Builder interface { Part1() Part2() Part3() } type Director struct { builder Builder } // NewDirector ... func NewDirector(builder Builder) *Director { return \u0026Director{ builder: builder, } } //Construct Product func (d *Director) Construct() { d.builder.Part1() d.builder.Part2() d.builder.Part3() } type Builder1 struct { result string } func (b *Builder1) Part1() { b.result += \"1\" } func (b *Builder1) Part2() { b.result += \"2\" } func (b *Builder1) Part3() { b.result += \"3\" } func (b *Builder1) GetResult() string { return b.result } type Builder2 struct { result int } func (b *Builder2) Part1() { b.result += 1 } func (b *Builder2) Part2() { b.result += 2 } func (b *Builder2) Part3() { b.result += 3 } func (b *Builder2) GetResult() int { return b.result } package builder import \"testing\" func TestBuilder1(t *testing.T) { builder := \u0026Builder1{} director := NewDirector(builder) director.Construct() res := builder.GetResult() if res != \"123\" { t.Fatalf(\"Builder1 fail expect 123 acture %s\", res) } } func TestBuilder2(t *testing.T) { builder := \u0026Builder2{} director := NewDirector(builder) director.Construct() res := builder.GetResult() if res != 6 { t.Fatalf(\"Builder2 fail expect 6 acture %d\", res) } } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:4:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"详解构造函数、set方法、建造者模式三种对象创建方式 实际上，建造者模式的原理和代码实现非常简单，掌握起来并不难，难点在于应用场景。比如，你有没有考虑过这样几个问题：直接使用构造函数或者配合 set 方法就能创建对象，为什么还需要建造者模式来创建呢？建造者模式和工厂模式都可以创建对象，那它们两个的区别在哪里呢？ ","date":"2022-01-22 09:19:30","objectID":"/create_type/:4:1","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"为什么需要建造者模式？ 在平时的开发中，创建一个对象最常用的方式是，使用 new 关键字调用类的构造函数来完成。我的问题是，什么情况下这种方式就不适用了，就需要采用建造者模式来创建对象呢？你可以先思考一下，下面我通过一个例子来带你看一下。假设有这样一道设计面试题：我们需要定义一个资源池配置类 ResourcePoolConfig。这里的资源池，你可以简单理解为线程池、连接池、对象池等。在这个资源池配置类中，有以下几个成员变量，也就是可配置项。现在，请你编写代码实现这个 ResourcePoolConfig 类。 只要你稍微有点开发经验，那实现这样一个类对你来说并不是件难事。最常见、最容易想到的实现思路如下代码所示。因为 maxTotal、maxIdle、minIdle 不是必填变量，所以在创建 ResourcePoolConfig 对象的时候，我们通过往构造函数中，给这几个参数传递 null 值，来表示使用默认值。 java public class ResourcePoolConfig { private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig(String name, Integer maxTotal, Integer maxIdle, Integer minIdle) { if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(\"name should not be empty.\"); } this.name = name; if (maxTotal != null) { if (maxTotal \u003c= 0) { throw new IllegalArgumentException(\"maxTotal should be positive.\"); } this.maxTotal = maxTotal; } if (maxIdle != null) { if (maxIdle \u003c 0) { throw new IllegalArgumentException(\"maxIdle should not be negative.\"); } this.maxIdle = maxIdle; } if (minIdle != null) { if (minIdle \u003c 0) { throw new IllegalArgumentException(\"minIdle should not be negative.\"); } this.minIdle = minIdle; } } //...省略getter方法... } 现在，ResourcePoolConfig 只有 4 个可配置项，对应到构造函数中，也只有 4 个参数，参数的个数不多。但是，如果可配置项逐渐增多，变成了 8 个、10 个，甚至更多，那继续沿用现在的设计思路，构造函数的参数列表会变得很长，代码在可读性和易用性上都会变差。在使用构造函数的时候，我们就容易搞错各参数的顺序，传递进错误的参数值，导致非常隐蔽的 bug。 // 参数太多，导致可读性差、参数可能传递错误 ResourcePoolConfig config = new ResourcePoolConfig(\"dbconnectionpool\", 16, null, 8, null, false , true, 10, 20，false， true); 解决这个问题的办法你应该也已经想到了，那就是用 set() 函数来给成员变量赋值，以替代冗长的构造函数。我们直接看代码，具体如下所示。其中，配置项 name 是必填的，所以我们把它放到构造函数中设置，强制创建类对象的时候就要填写。其他配置项 maxTotal、maxIdle、minIdle 都不是必填的，所以我们通过 set() 函数来设置，让使用者自主选择填写或者不填写。 java public class ResourcePoolConfig { private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig(String name) { if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(\"name should not be empty.\"); } this.name = name; } public void setMaxTotal(int maxTotal) { if (maxTotal \u003c= 0) { throw new IllegalArgumentException(\"maxTotal should be positive.\"); } this.maxTotal = maxTotal; } public void setMaxIdle(int maxIdle) { if (maxIdle \u003c 0) { throw new IllegalArgumentException(\"maxIdle should not be negative.\"); } this.maxIdle = maxIdle; } public void setMinIdle(int minIdle) { if (minIdle \u003c 0) { throw new IllegalArgumentException(\"minIdle should not be negative.\"); } this.minIdle = minIdle; } //...省略getter方法... } 接下来，我们来看新的 ResourcePoolConfig 类该如何使用。我写了一个示例代码，如下所示。没有了冗长的函数调用和参数列表，代码在可读性和易用性上提高了很多。 // ResourcePoolConfig使用举例 ResourcePoolConfig config = new ResourcePoolConfig(\"dbconnectionpool\"); config.setMaxTotal(16); config.setMaxIdle(8); 至此，我们仍然没有用到建造者模式，通过构造函数设置必填项，通过 set() 方法设置可选配置项，就能实现我们的设计需求。如果我们把问题的难度再加大点，比如，还需要解决下面这三个问题，那现在的设计思路就不能满足了。 我们刚刚讲到，name 是必填的，所以，我们把它放到构造函数中，强制创建对象的时候就设置。如果必填的配置项有很多，把这些必填配置项都放到构造函数中设置，那构造函数就又会出现参数列表很长的问题。如果我们把必填项也通过 set() 方法设置，那校验这些必填项是否已经填写的逻辑就无处安放了。除此之外，假设配置项之间有一定的依赖关系，比如，如果用户设置了 maxTotal、maxIdle、minIdle 其中一个，就必须显式地设置另外两个；或者配置项之间有一定的约束条件，比如，maxIdle 和 minIdle 要小于等于 maxTotal。如果我们继续使用现在的设计思路，那这些配置项之间的依赖关系或者约束条件的校验逻辑就无处安放了。如果我们希望 ResourcePoolConfig 类对象是不可变对象，也就是说，对象在创建好之后，就不能再修改内部的属性值。要实现这个功能，我们就不能在 ResourcePoolConfig 类中暴露 set() 方法。 为了解决这些问题，建造者模式就派上用场了。 我们可以把校验逻辑放置到 Builder 类中，先创建建造者，并且通过 set() 方法设置建造者的变量值，然后在使用 build() 方法真正创建对象之前，做集中的校验，校验通过之后才会创建对象。除此之外，我们把 ResourcePoolConfig 的构造函数改为 private 私有权限。这样我们就只能通过建造者来创建 ResourcePoolConfig 类对象。并且，ResourcePoolConfig 没有提供任何 set() 方法，这样我们创建出来的对象就是不可变对象了。我们用建造者模式重新实现了上面的需求，具体的代码如下所示： java public class ResourcePoolConfig { pr","date":"2022-01-22 09:19:30","objectID":"/create_type/:4:2","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"与工厂模式有何区别？ 从上面的讲解中，我们可以看出，建造者模式是让建造者类来负责对象的创建工作。上一节课中讲到的工厂模式，是由工厂类来负责对象创建的工作。那它们之间有什么区别呢？实际上，工厂模式是用来创建不同但是相关类型的对象（继承同一父类或者接口的一组子类），由给定的参数来决定创建哪种类型的对象。建造者模式是用来创建一种类型的复杂对象，通过设置不同的可选参数，“定制化”地创建不同的对象。网上有一个经典的例子很好地解释了两者的区别。顾客走进一家餐馆点餐，我们利用工厂模式，根据用户不同的选择，来制作不同的食物，比如披萨、汉堡、沙拉。对于披萨来说，用户又有各种配料可以定制，比如奶酪、西红柿、起司，我们通过建造者模式根据用户选择的不同配料来制作披萨。实际上，我们也不要太学院派，非得把工厂模式、建造者模式分得那么清楚，我们需要知道的是，每个模式为什么这么设计，能解决什么问题。只有了解了这些最本质的东西，我们才能不生搬硬套，才能灵活应用，甚至可以混用各种模式创造出新的模式，来解决特定场景的问题。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:4:3","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"prototype 原型模式使对象能复制自身，并且暴露到接口中，使客户端面向接口编程时，不知道接口实际对象的情况下生成新的对象。 原型模式配合原型管理器使用，使得客户端在不知道具体类的情况下，通过接口管理器得到新的实例，并且包含部分预设定配置。 package prototype //Cloneable 是原型对象需要实现的接口 type Cloneable interface { Clone() Cloneable } type PrototypeManager struct { prototypes map[string]Cloneable } func NewPrototypeManager() *PrototypeManager { return \u0026PrototypeManager{ prototypes: make(map[string]Cloneable), } } func (p *PrototypeManager) Get(name string) Cloneable { return p.prototypes[name].Clone() } func (p *PrototypeManager) Set(name string, prototype Cloneable) { p.prototypes[name] = prototype } package prototype import \"testing\" var manager *PrototypeManager type Type1 struct { name string } func (t *Type1) Clone() Cloneable { tc := *t return \u0026tc } type Type2 struct { name string } func (t *Type2) Clone() Cloneable { tc := *t return \u0026tc } func TestClone(t *testing.T) { t1 := manager.Get(\"t1\") t2 := t1.Clone() if t1 == t2 { t.Fatal(\"error! get clone not working\") } } func TestCloneFromManager(t *testing.T) { c := manager.Get(\"t1\").Clone() t1 := c.(*Type1) if t1.name != \"type1\" { t.Fatal(\"error\") } } func init() { manager = NewPrototypeManager() t1 := \u0026Type1{ name: \"type1\", } manager.Set(\"t1\", t1) } 对于熟悉 JavaScript 语言的前端程序员来说，原型模式是一种比较常用的开发模式。这是因为，有别于 Java、C++ 等基于类的面向对象编程语言，JavaScript 是一种基于原型的面向对象编程语言。即便 JavaScript 现在也引入了类的概念，但它也只是基于原型的语法糖而已。不过，如果你熟悉的是 Java、C++ 等这些编程语言，那在实际的开发中，就很少用到原型模式了。 通过一个 clone 散列表的例子带你搞清楚：原型模式的应用场景，以及它的两种实现方式：深拷贝和浅拷贝。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:5:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"原型模式的原理与应用 如果对象的创建成本比较大，而同一个类的不同对象之间差别不大（大部分字段都相同），在这种情况下，我们可以利用对已有对象（原型）进行复制（或者叫拷贝）的方式来创建新对象，以达到节省创建时间的目的。这种基于原型来创建对象的方式就叫作原型设计模式（Prototype Design Pattern），简称原型模式。 …… ","date":"2022-01-22 09:19:30","objectID":"/create_type/:5:1","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"原型模式的实现方式：深拷贝和浅拷贝 我们来看，在内存中，用散列表组织的搜索关键词信息是如何存储的。我画了一张示意图，大致结构如下所示。从图中我们可以发现，散列表索引中，每个结点存储的 key 是搜索关键词，value 是 SearchWord 对象的内存地址。SearchWord 对象本身存储在散列表之外的内存空间中。 浅拷贝和深拷贝的区别在于，浅拷贝只会复制图中的索引（散列表），不会复制数据（SearchWord 对象）本身。相反，深拷贝不仅仅会复制索引，还会复制数据本身。浅拷贝得到的对象（newKeywords）跟原始对象（currentKeywords）共享数据（SearchWord 对象），而深拷贝得到的是一份完完全全独立的对象。具体的对比如下图所示： …… ","date":"2022-01-22 09:19:30","objectID":"/create_type/:5:2","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"singleton 使用懒汉模式的单例模式，使用双重检查加锁保证线程安全 package singleton import \"sync\" // Singleton 是单例模式接口，导出的 // 通过该接口可以避免 GetInstance 返回一个包私有类型的指针 type Singleton interface { foo() } // singleton 是单例模式类，包私有的 type singleton struct{} func (s singleton) foo() {} var ( instance *singleton once sync.Once ) //GetInstance 用于获取单例模式对象 func GetInstance() Singleton { once.Do(func() { instance = \u0026singleton{} }) return instance } package singleton import ( \"sync\" \"testing\" ) const parCount = 100 func TestSingleton(t *testing.T) { ins1 := GetInstance() ins2 := GetInstance() if ins1 != ins2 { t.Fatal(\"instance is not equal\") } } func TestParallelSingleton(t *testing.T) { start := make(chan struct{}) wg := sync.WaitGroup{} wg.Add(parCount) instances := [parCount]Singleton{} for i := 0; i \u003c parCount; i++ { go func(index int) { //协程阻塞，等待channel被关闭才能继续运行 \u003c-start instances[index] = GetInstance() wg.Done() }(i) } //关闭channel，所有协程同时开始运行，实现并行(parallel) close(start) wg.Wait() for i := 1; i \u003c parCount; i++ { if instances[i] != instances[i-1] { t.Fatal(\"instance is not equal\") } } } 单例存在哪些问题？单例与静态类的区别？有何替代的解决方案？ ","date":"2022-01-22 09:19:30","objectID":"/create_type/:6:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"为什么要使用单例？ 单例设计模式（Singleton Design Pattern）理解起来非常简单。一个类只允许创建一个对象（或者实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。 除了使用类级别锁之外，实际上，解决资源竞争问题的办法还有很多，分布式锁是最常听到的一种解决方案。不过，实现一个安全可靠、无 bug、高性能的分布式锁，并不是件容易的事情。除此之外，并发队列（比如 Java 中的 BlockingQueue）也可以解决这个问题：多个线程同时往并发队列里写日志，一个单独的线程负责将并发队列中的数据，写入到日志文件。这种方式实现起来也稍微有点复杂。 相对于这两种解决方案，单例模式的解决思路就简单一些了。单例模式相对于之前类级别锁的好处是，不用创建那么多 Logger 对象，一方面节省内存空间，另一方面节省系统文件句柄（对于操作系统来说，文件句柄也是一种资源，不能随便浪费）。我们将 Logger 设计成一个单例类，程序中只允许创建一个 Logger 对象，所有的线程共享使用的这一个 Logger 对象，共享一个 FileWriter 对象，而 FileWriter 本身是对象级别线程安全的，也就避免了多线程情况下写日志会互相覆盖的问题。 我们将 Logger 设计成一个单例类，程序中只允许创建一个 Logger 对象，所有的线程共享使用的这一个 Logger 对象，共享一个 FileWriter 对象，而 FileWriter 本身是对象级别线程安全的，也就避免了多线程情况下写日志会互相覆盖的问题。 表示全局唯一类（或者只保存一份的数据） 比如：配置信息类、唯一递增ID号码生成器（如果程序中有两个对象，那就会存在生成重复 ID 的情况） 如何实现一个单例 要实现一个单例，我们需要关注的点无外乎下面几个：构造函数需要是 private 访问权限的，这样才能避免外部通过 new 创建实例；考虑对象创建时的线程安全问题；考虑是否支持延迟加载；考虑 getInstance() 性能是否高（是否加锁）。 饿汉式（一开始就很饿） 饿汉式的实现方式比较简单。在类加载的时候，instance 静态实例就已经创建并初始化好了，所以，instance 实例的创建过程是线程安全的。不过，这样的实现方式不支持延迟加载（在真正用到 IdGenerator 的时候，再创建实例），从名字中我们也可以看出这一点。具体的代码实现如下所示： java public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static final IdGenerator instance = new IdGenerator(); private IdGenerator() {} public static IdGenerator getInstance() { return instance; } public long getId() { return id.incrementAndGet(); } } 有人觉得这种实现方式不好，因为不支持延迟加载，如果实例占用资源多（比如占用内存多）或初始化耗时长（比如需要加载各种配置文件），提前初始化实例是一种浪费资源的行为。最好的方法应该在用到的时候再去初始化。不过，我个人并不认同这样的观点。 如果初始化耗时长，那我们最好不要等到真正要用它的时候，才去执行这个耗时长的初始化过程，这会影响到系统的性能（比如，在响应客户端接口请求的时候，做这个初始化操作，会导致此请求的响应时间变长，甚至超时）。采用饿汉式实现方式，将耗时的初始化操作，提前到程序启动的时候完成，这样就能避免在程序运行的时候，再去初始化导致的性能问题。 懒汉式 有饿汉式，对应的，就有懒汉式。懒汉式相对于饿汉式的优势是支持延迟加载。具体的代码实现如下所示： java public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() {} public static synchronized IdGenerator getInstance() { if (instance == null) { instance = new IdGenerator(); } return instance; } public long getId() { return id.incrementAndGet(); } } 不过懒汉式的缺点也很明显，我们给 getInstance() 这个方法加了一把大锁（synchronzed），导致这个函数的并发度很低。量化一下的话，并发度是 1，也就相当于串行操作了。而这个函数是在单例使用期间，一直会被调用。如果这个单例类偶尔会被用到，那这种实现方式还可以接受。但是，如果频繁地用到，那频繁加锁、释放锁及并发度低等问题，会导致性能瓶颈，这种实现方式就不可取了。 双重检测 饿汉式不支持延迟加载，懒汉式有性能问题，不支持高并发。那我们再来看一种既支持延迟加载、又支持高并发的单例实现方式，也就是双重检测实现方式。在这种实现方式中，只要 instance 被创建之后，即便再调用 getInstance() 函数也不会再进入到加锁逻辑中了。所以，这种实现方式解决了懒汉式并发度低的问题。具体的代码实现如下所示： java public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() {} public static IdGenerator getInstance() { if (instance == null) { synchronized(IdGenerator.class) { // 此处为类级别的锁 if (instance == null) { instance = new IdGenerator(); } } } return instance; } public long getId() { return id.incrementAndGet(); } } 网上有人说，这种实现方式有些问题。因为指令重排序，可能会导致 IdGenerator 对象被 new 出来，并且赋值给 instance 之后，还没来得及初始化（执行构造函数中的代码逻辑），就被另一个线程使用了。 要解决这个问题，我们需要给 instance 成员变量加上 volatile 关键字，禁止指令重排序才行。实际上，只有很低版本的 Java 才会有这个问题。我们现在用的高版本的 Java 已经在 JDK 内部实现中解决了这个问题（解决的方法很简单，只要把对象 new 操作和初始化操作设计为原子操作，就自然能禁止重排序）。 静态内部类 我们再来看一种比双重检测更加简单的实现方法，那就是利用 Java 的静态内部类。它有点类似饿汉式，但又能做到了延迟加载。具体是怎么做到的呢？我们先来看它的代码实现。 java public class IdGenerator { private AtomicLong id = new AtomicLong(0); private IdGenerator() {} private static class SingletonHolder{ private static final IdGenerator instance = new IdGenerator(); } public static IdGenerator getInstance() { return SingletonHolder.instance; } public long getId() { return id.incrementAndGet(); } } SingletonHolder 是一个静态内部类，当外部类 IdGenerator 被加载的时候，并不会创建 SingletonHolder 实例对象。只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance。instance 的唯一性、创建过程的线程安全性，都由 JVM 来保证。所以，这种实现方法既保证了线程安全，又能做到延迟加载。 枚举 最后，我们介绍一种最简单的实现方式，基于枚举类型的单例实现。这种实现方式通过 Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性。具体的代码如下所示： java public enum IdGenerator { INSTANCE; private AtomicLong id = new AtomicLong(0); public long getId() { return id.incrementAndGet(); } } 总结 单例有下面几种经典的实现方式。 饿汉式 饿汉式的实现方式，在类加载的期间，就已经将 instance 静态实例初始化好了，所以，instance 实例的创建是线程安全的。不过，这样的实现方式不支持延迟加载实例。 懒汉式 懒汉式相对于饿汉式的优势是支持延迟加载。这种实现方式会导致频繁加锁、释放锁，以及并发度低等","date":"2022-01-22 09:19:30","objectID":"/create_type/:6:1","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"单例这种设计模式存在哪些问题？ 为什么会被称为反模式？如果不用单例，该如何表示全局唯一类？有何替代的解决方案？ 大部分情况下，我们在项目中使用单例，都是用它来表示一些全局唯一类，比如配置信息类、连接池类、ID 生成器类。单例模式书写简洁、使用方便，在代码中，我们不需要创建对象，直接通过类似 IdGenerator.getInstance().getId() 这样的方法来调用就可以了。但是，这种使用方法有点类似硬编码（hard code），会带来诸多问题。 单例对 OOP 特性的支持不友好我们知道，OOP 的四大特性是封装、抽象、继承、多态。单例这种设计模式对于其中的抽象、继承、多态都支持得不好。 java public class Order { public void create(...) { //... long id = IdGenerator.getInstance().getId(); //... } } public class User { public void create(...) { // ... long id = IdGenerator.getInstance().getId(); //... } } IdGenerator 的使用方式违背了基于接口而非实现的设计原则，也就违背了广义上理解的 OOP 的抽象特性。如果未来某一天，我们希望针对不同的业务采用不同的 ID 生成算法。比如，订单 ID 和用户 ID 采用不同的 ID 生成器来生成。为了应对这个需求变化，我们需要修改所有用到 IdGenerator 类的地方，这样代码的改动就会比较大。 java public class Order { public void create(...) { //... long id = IdGenerator.getInstance().getId(); // 需要将上面一行代码，替换为下面一行代码 long id = OrderIdGenerator.getIntance().getId(); //... } } public class User { public void create(...) { // ... long id = IdGenerator.getInstance().getId(); // 需要将上面一行代码，替换为下面一行代码 long id = UserIdGenerator.getIntance().getId(); } } 除此之外，单例对继承、多态特性的支持也不友好。这里我之所以会用“不友好”这个词，而非“完全不支持”，是因为从理论上来讲，单例类也可以被继承、也可以实现多态，只是实现起来会非常奇怪，会导致代码的可读性变差。不明白设计意图的人，看到这样的设计，会觉得莫名其妙。所以，一旦你选择将某个类设计成到单例类，也就意味着放弃了继承和多态这两个强有力的面向对象特性，也就相当于损失了可以应对未来需求变化的扩展性。 单例会隐藏类之间的依赖关系 我们知道，代码的可读性非常重要。在阅读代码的时候，我们希望一眼就能看出类与类之间的依赖关系，搞清楚这个类依赖了哪些外部类。通过构造函数、参数传递等方式声明的类之间的依赖关系，我们通过查看函数的定义，就能很容易识别出来。但是，单例类不需要显示创建、不需要依赖参数传递，在函数中直接调用就可以了。如果代码比较复杂，这种调用关系就会非常隐蔽。在阅读代码的时候，我们就需要仔细查看每个函数的代码实现，才能知道这个类到底依赖了哪些单例类。 单例对代码的扩展性不友好 我们知道，单例类只能有一个对象实例。如果未来某一天，我们需要在代码中创建两个实例或多个实例，那就要对代码有比较大的改动。你可能会说，会有这样的需求吗？既然单例类大部分情况下都用来表示全局类，怎么会需要两个或者多个实例呢？实际上，这样的需求并不少见。我们拿数据库连接池来举例解释一下。在系统设计初期，我们觉得系统中只应该有一个数据库连接池，这样能方便我们控制对数据库连接资源的消耗。所以，我们把数据库连接池类设计成了单例类。但之后我们发现，系统中有些 SQL 语句运行得非常慢。这些 SQL 语句在执行的时候，长时间占用数据库连接资源，导致其他 SQL 请求无法响应。为了解决这个问题，我们希望将慢 SQL 与其他 SQL 隔离开来执行。为了实现这样的目的，我们可以在系统中创建两个数据库连接池，慢 SQL 独享一个数据库连接池，其他 SQL 独享另外一个数据库连接池，这样就能避免慢 SQL 影响到其他 SQL 的执行。如果我们将数据库连接池设计成单例类，显然就无法适应这样的需求变更，也就是说，单例类在某些情况下会影响代码的扩展性、灵活性。所以，数据库连接池、线程池这类的资源池，最好还是不要设计成单例类。实际上，一些开源的数据库连接池、线程池也确实没有设计成单例类。 单例对代码的可测试性不友好 单例模式的使用会影响到代码的可测试性。如果单例类依赖比较重的外部资源，比如 DB，我们在写单元测试的时候，希望能通过 mock 的方式将它替换掉。而单例类这种硬编码式的使用方式，导致无法实现 mock 替换。除此之外，如果单例类持有成员变量（比如 IdGenerator 中的 id 成员变量），那它实际上相当于一种全局变量，被所有的代码共享。如果这个全局变量是一个可变全局变量，也就是说，它的成员变量是可以被修改的，那我们在编写单元测试的时候，还需要注意不同测试用例之间，修改了单例类中的同一个成员变量的值，从而导致测试结果互相影响的问题。 单例不支持有参数的构造函数 单例不支持有参数的构造函数，比如我们创建一个连接池的单例对象，我们没法通过参数来指定连接池的大小。针对这个问题，我们来看下都有哪些解决方案。第一种解决思路是：创建完实例之后，再调用 init() 函数传递参数。需要注意的是，我们在使用这个单例类的时候，要先调用 init() 方法，然后才能调用 getInstance() 方法，否则代码会抛出异常。具体的代码实现如下所示： java public class Singleton { private static Singleton instance = null; private final int paramA; private final int paramB; private Singleton(int paramA, int paramB) { this.paramA = paramA; this.paramB = paramB; } public static Singleton getInstance() { if (instance == null) { throw new RuntimeException(\"Run init() first.\"); } return instance; } public synchronized static Singleton init(int paramA, int paramB) { if (instance != null){ throw new RuntimeException(\"Singleton has been created!\"); } instance = new Singleton(paramA, paramB); return instance; } } Singleton.init(10, 50); // 先init，再使用 Singleton singleton = Singleton.getInstance(); 第二种解决思路是：将参数放到 getIntance() 方法中。具体的代码实现如下所示： java public class Singleton { private static Singleton instance = null; private final int paramA; private final int paramB; private Singleton(int paramA, int paramB) { this.paramA = paramA; this.paramB = paramB; } public synchronized static Singleton getInstance(int paramA, int paramB) { if (instance == null) { instance = new Singleton(paramA, paramB); } return instance; } } Singleton singleton = Singleton.getInstance(10, 50); 不知道你有没有发现，上面的代码实现稍微有点问题。如果我们如下两次执行 getInstance() 方法，那获取到的 singleton1 和 signleton2 的 paramA 和 paramB 都是 10 和 50。也就是说，第二次的参数（20，30）没有起作用，而构建的过程也没有给与提示，这样就会误导用户。 java Singleton singleton1 = Singleton.getInstance(10, 50); Singleton singleton2 = Singleton.getInstance(20, 30); 第三种解决思路是：将参数放到另外一个全局变量中。具体的代码实现如下。Config 是一个存储了 paramA 和 paramB 值的全局变量。里面的值既可以像下面的代码那","date":"2022-01-22 09:19:30","objectID":"/create_type/:6:2","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"如何设计实现一个集群环境下的分布式单例模式？ 参考 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:6:3","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Go"],"content":"常用标准库 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:0:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"前言 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:1:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"包复用 package: 基本复⽤模块单元：以⾸字⺟⼤写来表明可被包外代码访问 代码的 package 可以和所在的⽬录不⼀致 同⼀⽬录⾥的 Go 代码的 package 要保持⼀致 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:1:1","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"依赖管理 Go 未解决的依赖问题： 同⼀环境下，不同项⽬使⽤同⼀包的不同版本 ⽆法管理对包的特定版本的依赖 vendor 路径 随着 Go 1.5 release 版本的发布，vendor ⽬录被添加到除了 GOPATH 和GOROOT 之外的依赖⽬录查找的解决⽅案。在 Go 1.6 之前，你需要⼿动的设置环境变量查找依赖包路径的解决⽅案如下： 当前包下的 vendor ⽬录 向上级⽬录查找，直到找到 src 下的 vendor ⽬录 在 GOPATH 下⾯查找依赖包 在 GOROOT ⽬录下查找 常用依赖管理工具： godep glide dep ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:1:2","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"fmt ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:2:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Time ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:3:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Flag ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:4:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Log ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:5:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"IO操作 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:6:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Strconv ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:7:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Template ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:8:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Http ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:9:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Context ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:10:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"数据格式 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:11:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"反射和unsafe ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:12:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"反射编程 反射是指在程序运行期对程序本身进行访问和修改的能力 变量的内在机制 变量包含类型信息和值信息 var arr [10]int arr[0] = 10 类型信息：是静态的元信息，是预先定义好的 值信息：是程序运行过程中动态改变的 反射的使用 reflect包封装了反射相关的方法 获取类型信息：reflect.TypeOf，是静态的 获取值信息：reflect.ValueOf，是动态的 空接口与反射 反射可以在运行时动态获取程序的各种详细信息 反射获取interface类型信息 package main import ( \"fmt\" \"reflect\" ) //反射获取interface类型信息 func reflect_type(a interface{}) { t := reflect.TypeOf(a) fmt.Println(\"类型是：\", t) // kind()可以获取具体类型 k := t.Kind() fmt.Println(k) switch k { case reflect.Float64: fmt.Printf(\"a is float64\\n\") case reflect.String: fmt.Println(\"string\") } } func main() { var x float64 = 3.4 reflect_type(x) } 反射获取interface值信息 package main import ( \"fmt\" \"reflect\" ) //反射获取interface值信息 func reflect_value(a interface{}) { v := reflect.ValueOf(a) fmt.Println(v) k := v.Kind() fmt.Println(k) switch k { case reflect.Float64: fmt.Println(\"a是：\", v.Float()) } } func main() { var x float64 = 3.4 reflect_value(x) } 反射修改值信息 package main import ( \"fmt\" \"reflect\" ) //反射修改值 func reflect_set_value(a interface{}) { v := reflect.ValueOf(a) k := v.Kind() switch k { case reflect.Float64: // 反射修改值 v.SetFloat(6.9) fmt.Println(\"a is \", v.Float()) case reflect.Ptr: // Elem()获取地址指向的值 v.Elem().SetFloat(7.9) fmt.Println(\"case:\", v.Elem().Float()) // 地址 fmt.Println(v.Pointer()) } } func main() { var x float64 = 3.4 // 反射认为下面是指针类型，不是float类型 reflect_set_value(\u0026x) fmt.Println(\"main:\", x) } 结构体与反射 查看类型字段和方法 package main import ( \"fmt\" \"reflect\" ) // 定义结构体 type User struct { Id int Name string Age int } // 绑方法 func (u User) Hello() { fmt.Println(\"Hello\") } // 传入interface{} func Poni(o interface{}) { t := reflect.TypeOf(o) fmt.Println(\"类型：\", t) fmt.Println(\"字符串类型：\", t.Name()) // 获取值 v := reflect.ValueOf(o) fmt.Println(v) // 可以获取所有属性 // 获取结构体字段个数：t.NumField() for i := 0; i \u003c t.NumField(); i++ { // 取每个字段 f := t.Field(i) fmt.Printf(\"%s : %v\", f.Name, f.Type) // 获取字段的值信息 // Interface()：获取字段对应的值 val := v.Field(i).Interface() fmt.Println(\"val :\", val) } fmt.Println(\"=================方法====================\") for i := 0; i \u003c t.NumMethod(); i++ { m := t.Method(i) fmt.Println(m.Name) fmt.Println(m.Type) } } func main() { u := User{1, \"zs\", 20} Poni(u) } 查看匿名字段 package main import ( \"fmt\" \"reflect\" ) // 定义结构体 type User struct { Id int Name string Age int } // 匿名字段 type Boy struct { User Addr string } func main() { m := Boy{User{1, \"zs\", 20}, \"bj\"} t := reflect.TypeOf(m) fmt.Println(t) // Anonymous：匿名 fmt.Printf(\"%#v\\n\", t.Field(0)) // 值信息 fmt.Printf(\"%#v\\n\", reflect.ValueOf(m).Field(0)) } 修改结构体的值： package main import ( \"fmt\" \"reflect\" ) // 定义结构体 type User struct { Id int Name string Age int } // 修改结构体值 func SetValue(o interface{}) { v := reflect.ValueOf(o) // 获取指针指向的元素 v = v.Elem() // 取字段 f := v.FieldByName(\"Name\") if f.Kind() == reflect.String { f.SetString(\"kuteng\") } } func main() { u := User{1, \"5lmh.com\", 20} SetValue(\u0026u) fmt.Println(u) } 调用方法 package main import ( \"fmt\" \"reflect\" ) // 定义结构体 type User struct { Id int Name string Age int } func (u User) Hello(name string) { fmt.Println(\"Hello：\", name) } func main() { u := User{1, \"5lmh.com\", 20} v := reflect.ValueOf(u) // 获取方法 m := v.MethodByName(\"Hello\") // 构建一些参数 args := []reflect.Value{reflect.ValueOf(\"6666\")} // 没参数的情况下：var args2 []reflect.Value // 调用方法，需要传入方法的参数 m.Call(args) } 获取字段的tag package main import ( \"fmt\" \"reflect\" ) type Student struct { Name string `json:\"name1\" db:\"name2\"` } func main() { var s Student v := reflect.ValueOf(\u0026s) // 类型 t := v.Type() // 获取字段 f := t.Elem().Field(0) fmt.Println(f.Tag.Get(\"json\")) fmt.Println(f.Tag.Get(\"db\")) } 万能程序 DeepEqual比较切片和map func TestDeepEqual(t *testing.T) { a := map[int]string{1: \"one\", 2: \"two\", 3: \"three\"} b := map[int]string{1: \"one\", 2: \"two\", 3: \"three\"} // t.Log(a == b) t.Log(\"a==b?\", reflect.DeepEqual(a, b)) s1 := []int{1, 2, 3} s2 := []int{1, 2, 3} s3 := []int{2, 3, 1} t.Log(\"s1 == s2?\", reflect.DeepEqual(s1, s2)) t.Log(\"s1 == s3?\", reflect.DeepEqual(s1, s3)) c1 := Customer{\"1\", \"Mike\", 40} c2 := Customer{\"1\", \"Mike\", 40} fmt.Println(c1 == c2) fmt.Println(reflect.DeepEqual(c1, c2","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:12:1","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"文件操作 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:13:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"go module ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:14:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"String ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:15:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:0:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"前言 计算机里的数据都是以字节形式进行存储和处理，从而需要编码来表达信息。ASCII是简单字符集编码模型，定义了这个字符集里包含的字符以及其映射成的8位比特值。 关于现代编码模型： 一个字符如何映射成有限长度的比特值？ 需要表示字符的范围–字符表（character repertoire） CR映射到一个整数集，称映射为编码字符集（coded character set），也就是Unicode的概念，那些整数称为码点（code point） 将CCS里的整数映射成有限长度的比特值，这个对应关系称为字符编码方式或字符编码表（character encoding form），比如UTF-8，UTF-16。（Unicode Transformation Format，8或者16是指码元的大小，码元是一个已编码文本中具有最短的比特组合的单元，即最小单位是一个字节或者两个字节） UTF-8是完全兼容ASCII的，多字节表示一个字符时Unicode码点范围以及对应的bit组合： 一字节：U+00~U+7F————–UTF-8字节流（二进制）：0xxxxxxx 二字节：U+80~U+7FF————-UTF-8字节流（二进制）：110xxxxx 10xxxxxx 三字节：U+800~U+7FFF———–UTF-8字节流（二进制）：1110xxxx 10xxxxxx 10xxxxxx 四字节：U+10000~U+10FFFF——-UTF-8字节流（二进制）：11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 汉字大多是三字节 关于Unicode和UTF-8： func TestString(t *testing.T) { var s string t.Log(s) //初始化为默认零值“” s = \"hello\" t.Log(len(s)) //s[1] = '3' //string是不可变的byte slice //s = \"\\xE4\\xB8\\xA5\" //可以存储任何二进制数据 s = \"\\xE4\\xBA\\xBB\\xFF\" t.Log(s) t.Log(len(s)) s = \"中\" t.Log(len(s)) //是byte数 c := []rune(s) t.Log(len(c)) // t.Log(\"rune size:\", unsafe.Sizeof(c[0])) t.Logf(\"中 unicode %x\", c[0]) t.Logf(\"中 UTF8 %x\", s) } Running tool: D:\\go\\bin\\go.exe test -timeout 30s -run ^TestString$ code/code/ch9/string === RUN TestString d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:9: d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:11: 5 d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:15: 亻� d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:16: 4 d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:18: 3 d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:21: 1 d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:23: 中 unicode 4e2d d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:24: 中 UTF8 e4b8ad --- PASS: TestString (0.00s) PASS ok code/code/ch9/string 0.514s ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:1:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go起源 07年 三位大牛 解决三个困难：多核硬件架构、超大规模的分布式计算集群、如今使用的web开发模式导致的前所未有的开发规模和更新速度 Go 官方下载站点是 golang.org/dl，但我们可以用针对中国大陆的镜像站点 golang.google.cn/dl 来下载 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:2:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"主要特征 自动立即回收。(自带GC) 更丰富的内置类型。 函数多返回值。 错误处理。 匿名函数和闭包。 类型和接口。 并发编程。 反射。 有复合，无继承（因为复合大于继承，干脆不要继承） Go的函数、变量、常量、自定义类型、包(package)的命名方式遵循以下规则： 1）首字符可以是任意的Unicode字符或者下划线 2）剩余字符可以是Unicode字符、下划线、数字 3）字符长度不限 25关键字： break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var select和switch:select只能应用于channel的操作，既可以用于channel的数据接收，也可以用于channel的数据发送。如果select的多个分支都满足条件，则会随机的选取其中一个满足条件的分支。而switch用于一般的分支判断，顺序执行。 continue 如果循环体中的代码执行到一半，要中断当前迭代，忽略此迭代循环体中的后续代码（循环后置语句eg:x++不会被忽略），并回到 for 循环条件判断，尝试开启下一次迭代,使用continue关键字。 看上去好像和C没区别，其实Go的continue增加了对lable的支持，label 语句的作用，是标记跳转的目标。在中断内层 for 循环，回到外层 for 循环继续执行的场景应用比较合适 func main() { var sum int var sl = []int{1, 2, 3, 4, 5, 6} loop: for i := 0; i \u003c len(sl); i++ { if sl[i]%2 == 0 { // 忽略切片中值为偶数的元素 continue loop } sum += sl[i] } println(sum) // 9 } 注意与goto的区别： 一旦使用 goto 跳转，那么不管是内层循环还是外层循环都会被终结，代码将会从 outerloop 这个 label 处，开始重新执行我们的嵌套循环语句，这与带 label 的 continue 的跳转语义是完全不同的。 goto 是一种公认的、难于驾驭的语法元素，应用 goto 的代码可读性差、代码难于维护还易错。而 Go 语言保留了 goto，具体我不得而知 break Go 语言规范中明确规定，不带 label 的 break 语句中断执行并跳出的，是同一函数内 break 语句所在的最内层的 for、switch 或 select package main import \"time\" import \"fmt\" func main() { c1 := make(chan string) c2 := make(chan string) go func() { time.Sleep(time.Second * 1) c1 \u003c- \"one\" }() go func() { time.Sleep(time.Second * 2) c2 \u003c- \"two\" }() for i := 0; i \u003c 2; i++ { select { case msg1 := \u003c-c1: fmt.Println(\"received\", msg1) case msg2 := \u003c-c2: fmt.Println(\"received\", msg2) } } } fallthrough:可以使用fallthrough强制执行该case执行完下一条case代码，fallthrough不会判断下一条case的判断结果是否为true。 37个保留字： Constants: true false iota nil Types: int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64 bool byte rune string error Functions: make len cap new append copy close delete complex real imag panic recover new\u0026make func new(Type) *Type func make(t Type, size …IntegerType) Type go声明：var,const,type,func ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:3:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"内置类型与函数 值类型： bool int(32 or 64), int8, int16, int32, int64 uint(32 or 64), uint8(byte), uint16, uint32, uint64 float32, float64 string complex64, complex128 array 引用类型（声明的同时需要分配内存空间，不然会引发panic，分配内存可以用new或者make）： slice map chan …… 内置函数（无需导入即可使用）: append -- 用来追加元素到数组、slice中,返回修改后的数组、slice close -- 主要用来关闭channel delete -- 从map中删除key对应的value panic -- 停止常规的goroutine （panic和recover：用来做错误处理） recover -- 允许程序定义goroutine的panic动作 real -- 返回complex的实部 （complex、real imag：用于创建和操作复数） imag -- 返回complex的虚部 make -- 用来分配内存，返回Type本身(只能应用于slice, map, channel) make 函数允许在运行期动态指定数组长度，绕开了数组类型必须使用编译期常量的限制。 new -- 用来分配内存，主要用来分配值类型，比如int、struct。返回指向Type的指针 cap -- capacity用于返回某个类型的最大容量（只能用于切片和 map） copy -- 用于复制和连接slice，返回复制的数目，copy(a,b) 只有 min(len(a),len(b))个元素会被成功拷贝。 len -- 用来求长度，比如string、array、slice、map、channel ，返回长度 print、println -- 底层打印函数，在部署环境中建议使用 fmt 包 内置接口error： type error interface { //只要实现了Error()函数，返回值为String的都实现了err接口（鸭子类型） Error() String } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:4:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"init \u0026 main 以及Go包的初始化顺序 go语言中init函数用于包(package)的初始化，该函数是go语言的一个重要特性。有下面的特征： init函数是用于程序执行前做包的初始化的函数，比如初始化包里的变量等 每个包可以拥有多个init函数 包的每个源文件也可以拥有多个init函数 同一个包中多个init函数的执行顺序go语言没有明确的定义(说明) 不同包的init函数按照包导入的依赖关系决定该初始化函数的执行顺序 init函数不能被其他函数调用，而是在main函数执行之前，自动被调用 init()函数的用途： 重置包级变量值，对包内部以及暴露到外部的包级数据（主要是包级变量）的初始状态进行检查 实现对包级变量的复杂初始化，有些包级变量需要一个比较复杂的初始化过程，使用init()比较合适 在 init 函数中实现“注册模式”，如下 import ( \"database/sql\" _ \"github.com/lib/pq\" ) func main() { db, err := sql.Open(\"postgres\", \"user=pqgotest dbname=pqgotest sslmode=verify-full\") if err != nil { log.Fatal(err) } age := 21 rows, err := db.Query(\"SELECT name FROM users WHERE age = $1\", age) ... } //pq包的init() func init() { sql.Register(\"postgres\", \u0026Driver{}) } pq 包将自己实现的 sql 驱动注册到了 sql 包中。这样只要应用层代码在 Open 数据库的时候，传入驱动的名字（这里是“postgres”)，那么通过 sql.Open 函数，返回的数据库实例句柄对数据库进行的操作，实际上调用的都是 pq 包中相应的驱动实现。 从标准库 database/sql 包的角度来看，这种“注册模式”实质是一种工厂设计模式的实现，sql.Open 函数就是这个模式中的工厂方法，它根据外部传入的驱动名称“生产”出不同类别的数据库实例句柄。 这种“注册模式”在标准库的其他包中也有广泛应用，比如说，使用标准库 image 包获取各种格式图片的宽和高： package main import ( \"fmt\" \"image\" _ \"image/gif\" // 以空导入方式注入gif图片格式驱动 _ \"image/jpeg\" // 以空导入方式注入jpeg图片格式驱动 _ \"image/png\" // 以空导入方式注入png图片格式驱动 \"os\" ) func main() { // 支持png, jpeg, gif width, height, err := imageSize(os.Args[1]) // 获取传入的图片文件的宽与高 if err != nil { fmt.Println(\"get image size error:\", err) return } fmt.Printf(\"image size: [%d, %d]\\n\", width, height) } func imageSize(imageFile string) (int, int, error) { f, _ := os.Open(imageFile) // 打开图文文件 defer f.Close() img, _, err := image.Decode(f) // 对文件进行解码，得到图片实例 if err != nil { return 0, 0, err } b := img.Bounds() // 返回图片区域 return b.Max.X, b.Max.Y, nil } // $GOROOT/src/image/png/reader.go func init() { image.RegisterFormat(\"png\", pngHeader, Decode, DecodeConfig) } // $GOROOT/src/image/jpeg/reader.go func init() { image.RegisterFormat(\"jpeg\", \"\\xff\\xd8\", Decode, DecodeConfig) } // $GOROOT/src/image/gif/reader.go func init() { image.RegisterFormat(\"gif\", \"GIF8?a\", Decode, DecodeConfig) } Go语言程序的默认入口函数(主函数)： func main(){ …… //通过os.Args获取参数eg:os.Args[0] //不支持返回值，可以通过os.Exit()来返回状态 } 在启动了多个 Goroutine 的 Go 应用中，main.main 函数将在 Go 应用的主 Goroutine 中执行。 init函数和main函数的异同： 同 两个函数在定义时不能有任何的参数和返回值，且Go程序自动调用。 异 init可以应用于任意包中，且可以重复定义多个。 main函数只能用于main包中，且只能定义一个。 init()执行顺序： 对同一个go文件的init()调用顺序是从上到下的。 对同一个package中不同文件是按文件名字符串比较“从小到大”顺序调用各文件中的init()函数。 对于不同的package，如果不相互依赖的话，按照main包中”先import的后调用”的顺序调用其包中的init()，如果package存在依赖，则先调用最早被依赖的package中的init()，最后调用main函数。 同一个包被多次调用只会执行一次init()函数 如果init函数中使用了println()或者print()你会发现在执行过程中这两个不会按照你想象中的顺序执行。这两个函数官方只推荐在测试环境中使用，对于正式环境不要使用。 Go包初始化：从main包开始按照深度优先初始化main包的依赖包，初始化一个包时的顺序是初始化依赖包、常量、变量、init()，回到main包时同样初始化常量、变量、init()，再执行main()函数。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:5:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go命令 PS D:\\Blog\\JF-011101.github.io-blog\\content\\posts\u003e go Go is a tool for managing Go source code. Usage: go \u003ccommand\u003e [arguments] The commands are: bug start a bug report build compile packages and dependencies clean remove object files and cached files doc show documentation for package or symbol env print Go environment information fix update packages to use new APIs fmt gofmt (reformat) package sources generate generate Go files by processing source get add dependencies to current module and install them install compile and install packages and dependencies list list packages or modules mod module maintenance run compile and run Go program test test packages tool run specified go tool version print Go version vet report likely mistakes in packages Use \"go help \u003ccommand\u003e\" for more information about a command. Additional help topics: buildconstraint build constraints buildmode build modes c calling between Go and C cache build and test caching environment environment variables filetype file types go.mod the go.mod file gopath GOPATH environment variable gopath-get legacy GOPATH go get goproxy module proxy protocol importpath import path syntax modules modules, module versions, and more module-get module-aware go get module-auth module authentication using go.sum packages package lists and patterns private configuration for downloading non-public code testflag testing flags testfunc testing functions vcs controlling version control with GOVCS Use \"go help \u003ctopic\u003e\" for more information about that topic. ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go env 用于打印Go语言的环境信息。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:1","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go run命令 可以编译并运行命令源码文件。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:2","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go get 可以根据要求和实际情况从互联网上下载或更新指定的代码包及其依赖包，并对它们进行编译和安装。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:3","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go build命令 用于编译我们指定的源码文件或代码包以及它们的依赖包。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:4","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go install 用于编译并安装指定的代码包及它们的依赖包。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:5","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go clean命令 会删除掉执行其它命令时产生的一些文件和目录。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:6","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go doc命令 可以打印附于Go语言程序实体上的文档。我们可以通过把程序实体的标识符作为该命令的参数来达到查看其文档的目的。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:7","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go test命令 用于对Go语言编写的程序进行测试。 在命名文件时需要让文件必须以_test结尾。默认的情况下，go test命令不需要任何的参数，它会自动把你源码包下面所有 test 文件测试完毕，当然你也可以带上参数。 这里介绍几个常用的参数： -bench regexp 执行相应的 benchmarks，例如 -bench=.； -cover 开启测试覆盖率； -run regexp 只运行 regexp 匹配的函数，例如 -run=Array 那么就执行包含有 Array 开头的函数； -v 显示测试的详细命令。 单元测试源码文件可以由多个测试用例组成，每个测试用例函数需要以Test为前缀。**测试用例文件不会参与正常源码编译，不会被包含到可执行文件中。**测试用例文件使用go test指令来执行，没有也不需要 main() 作为函数入口。所有在以_test结尾的源码内以Test开头的函数会自动被执行。测试用例可以不传入 *testing.T 参数。单元（功能）测试以testing.T为参数，性能（压力）测试以testing.B为参数。 运行指定示例： PS D:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch2\\constant_test\u003e go test -v -run TestTest test_test.go === RUN TestTest test_test.go:6: kk --- PASS: TestTest (0.00s) PASS ok command-line-arguments 0.425s t.FailNow()———–标记错误并终止当前测试用例 t.Fail()————–仅标记错误 每个测试用例可能并发执行，使用 testing.T 提供的日志输出可以保证日志跟随这个测试上下文一起打印输出。testing.T 提供了几种日志输出方法： Log 打印日志 Logf 格式化打印日志 Error 打印错误日志 Errorf 格式化打印错误日志 Fatal 打印致命日志 Fatalf 格式化打印致命日志 单元测试使用示例： //demo.go package demo // 冒泡排序 func BubbleSort(list []int) []int { n := len(list) for i := n - 1; i \u003e 0; i-- { for j := 0; j \u003c i; j++ { if list[j] \u003e list[j+1] { list[j], list[j+1] = list[j+1], list[j] } } } return list } //demo_test.go package demo import \"testing\" func TestBubbleSort(t *testing.T) { area := BubbleSort([]list{2,1,3,4,65,13,22}) if area != {1,2,3,4,13,22,65} { t.Error(\"测试失败\") } } 执行： PS D:\\code\u003e go test -v === RUN TestGetArea --- PASS: TestGetArea (0.00s) PASS ok _/D_/code 0.435s 基准测试——获得代码内存占用和运行效率的性能数据 使用者无须准备高精度的计时器和各种分析工具，基准测试本身即可以打印出非常标准的测试报告。 package code import \"testing\" func Benchmark_Add(b *testing.B) { var n int for i := 0; i \u003c b.N; i++ { n++ } } 这段代码使用基准测试框架测试加法性能。第 7 行中的 b.N 由基准测试框架提供。测试代码需要保证函数可重入性及无状态，也就是说，测试代码不使用全局变量等带有记忆性质的数据结构。避免多次运行同一段代码时的环境不一致，不能假设 N 值范围。 //-bench=.相当于-run。在windows下使用-bench=\".\" $ go test -v -bench=. benchmark_test.go goos: linux goarch: amd64 Benchmark_Add-4 20000000 0.33 ns/op // 20000000指的是测试执行次数 PASS ok command-line-arguments 0.700s 基准测试原理：基准测试框架对一个测试用例的默认测试时间是 1 秒。开始测试时，当以 Benchmark 开头的基准测试用例函数返回时还不到 1 秒，那么 testing.B 中的 N 值将按 1、2、5、10、20、50……递增，同时以递增后的值重新调用基准测试用例函数。 通过-benchtime参数可以自定义测试时间，例如： $ go test -v -bench=. -benchtime=5s benchmark_test.go goos: linux goarch: amd64 Benchmark_Add-4 10000000000 0.33 ns/op PASS ok command-line-arguments 3.380s 基准测试可以对一段代码可能存在的内存分配进行统计，下面是一段使用字符串格式化的函数，内部会进行一些分配操作。 func Benchmark_Alloc(b *testing.B) { for i := 0; i \u003c b.N; i++ { fmt.Sprintf(\"%d\", i) } } $ go test -v -bench=Alloc -benchmem benchmark_test.go goos: linux goarch: amd64 Benchmark_Alloc-4 20000000 109 ns/op 16 B/op 2 allocs/op PASS ok command-line-arguments 2.311s 第 1 行的代码中-bench后添加了 Alloc，指定只测试 Benchmark_Alloc() 函数。 第 4 行代码的“16 B/op”表示每一次调用需要分配 16 个字节，“2 allocs/op”表示每一次调用有两次分配 开发者根据这些信息可以迅速找到可能的分配点，进行优化和调整。 控制计时器：有些测试需要一定的启动和初始化时间，如果从 Benchmark() 函数开始计时会很大程度上影响测试结果的精准性。testing.B 提供了一系列的方法可以方便地控制计时器，从而让计时器只在需要的区间进行测试。我们通过下面的代码来了解计时器的控制。 func Benchmark_Add_TimerControl(b *testing.B) { // 重置计时器 b.ResetTimer() // 停止计时器 b.StopTimer() // 开始计时器 b.StartTimer() var n int for i := 0; i \u003c b.N; i++ { n++ } } 从 Benchmark() 函数开始，Timer 就开始计数。计数器内部不仅包含耗时数据，还包括内存分配的数据。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:8","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go list命令 作用是列出指定的代码包的信息。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:9","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go fix 会把指定代码包的所有Go语言源码文件中的旧版本代码修正为新版本的代码。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:10","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go vet 是一个用于检查Go语言源码中静态错误的简单工具。比如是否存在变量遮蔽。 $go install golang.org/x/tools/go/analysis/passes/shadow/cmd/shadow@latest go: downloading golang.org/x/tools v0.1.5 go: downloading golang.org/x/mod v0.4.2 $go vet -vettool=$(which shadow) -strict complex.go ./complex.go:13:12: declaration of \"err\" shadows declaration at line 11 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:11","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go tool pprof命令 交互式的访问概要文件的内容。监测进程的运行数据，用于监控程序的性能，对内存使用和CPU使用的情况统信息进行分析。官方提供了两个包**：runtime/pprof和net/http/pprof**，前者用于普通代码的性能分析，后者用于web服务器的性能分析。 runtime/pprof 示例 PS D:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch46\\tools\\file\u003e go tool pprof prof cpu.prof prof: open prof: The system cannot find the file specified. Fetched 1 source profiles out of 2 Type: cpu Time: Mar 10, 2022 at 10:41am (CST) Duration: 2.77s, Total samples = 1.70s (61.39%) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 1.67s, 98.24% of 1.70s total Showing top 10 nodes out of 34 flat flat% sum% cum cum% 0.49s 28.82% 28.82% 1.62s 95.29% main.fillMatrix 0.35s 20.59% 49.41% 1.13s 66.47% math/rand.(*Rand).Intn 0.33s 19.41% 68.82% 0.78s 45.88% math/rand.(*Rand).Int31n 0.14s 8.24% 77.06% 0.14s 8.24% math/rand.(*rngSource).Uint64 (inline) 0.13s 7.65% 84.71% 0.45s 26.47% math/rand.(*Rand).Int31 (inline) 0.10s 5.88% 90.59% 0.24s 14.12% math/rand.(*rngSource).Int63 0.08s 4.71% 95.29% 0.32s 18.82% math/rand.(*Rand).Int63 0.02s 1.18% 96.47% 0.02s 1.18% main.calculate 0.02s 1.18% 97.65% 0.02s 1.18% runtime.stdcall1 0.01s 0.59% 98.24% 0.01s 0.59% runtime.lock2 (pprof) list fillMatrix Total: 1.70s ROUTINE ======================== main.fillMatrix in D:\\go\\Go_WorkSpace\\go_learning-master\\code\\ch46\\tools\\file\\prof.go 490ms 1.62s (flat, cum) 95.29% of Total . . 16: . . 17:func fillMatrix(m *[row][col]int) { . . 18: s := rand.New(rand.NewSource(time.Now().UnixNano())) . . 19: . . 20: for i := 0; i \u003c row; i++ { 20ms 20ms 21: for j := 0; j \u003c col; j++ { 470ms 1.60s 22: m[i][j] = s.Intn(100000) . . 23: } . . 24: } . . 25:} . . 26: . . 27:func calculate(m *[row][col]int) { (pprof) top/tree/web top [n]，查看排名前n个数据，默认为10。（这个函数本身占用的时间，以及这个函数包括调用其他函数的时间） tree [n]，以树状图形式显示，默认显示10个。 net/http/pprof ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:12","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go modules go modules 是 golang 1.11 新加的特性 go mod download download modules to local cache(下载依赖包) edit edit go.mod from tools or scripts（编辑go.mod） graph print module requirement graph (打印模块依赖图) init initialize new module in current directory（在当前目录初始化mod） tidy add missing and remove unused modules(拉取缺少的模块，移除不用的模块,常用) vendor make vendored copy of dependencies(将依赖复制到vendor下) verify verify dependencies have expected content (验证依赖是否正确 why explain why packages or modules are needed(解释为什么需要依赖) go.mod文件一旦创建后，它的内容将会被go toolchain全面掌控。go toolchain会在各类命令执行时，比如go get、go build、go mod等修改和维护go.mod文件。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:13","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"运算符 Go 语言内置的运算符有： 算术运算符 ++ 和 – 只有后置的，没有前置的，只作为语句，不作为表达式 关系运算符 逻辑运算符 位运算符 按位置零运算符 x \u0026^ y ——如果y非零，则z为0；如果y为零，则z为x（先非再与），注意按位运算 赋值运算符（一个赋值语句可以给多个变量进行赋值，多重赋值时，变量的左值和右值按从左到右的顺序赋值。多重赋值在 Go 语言的错误处理和函数返回值中会大量地使用。） = 简单的赋值运算符，将一个表达式的值赋给一个左值 += 相加后再赋值 -= 相减后再赋值 *= 相乘后再赋值 /= 相除后再赋值 %= 求余后再赋值 «= 左移后赋值 \u003e\u003e= 右移后赋值 \u0026= 按位与后赋值 l= 按位或后赋值 ^= 按位异或后赋值 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:7:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"下划线 “_”是特殊标识符，用来忽略结果。 下划线在import中 import 下划线（如：import _ hello/imp）的作用：当导入一个包时，该包下的文件里所有init()函数都会被执行，然而，有些时候我们并不需要把整个包都导入进来，仅仅是是希望它执行init()函数而已。这个时候就可以使用 import _ 引用该包。即使用【import _ 包路径】只是引用该包，仅仅是为了调用init()函数，所以无法通过包名来调用包中的其他函数。 注意区别于.在import中，它是指import进来的package里的所有的方法是在当前的名字空间的，使用其方法时直接使用即可。（请勿使用 import . 记法，它可以简化必须在被测试包外运行的测试， 除此之外应尽量避免使用。—-effective go） 下划线在代码中 作为占位符 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:8:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"格式占位符%… 普通占位符 占位符 说明 举例 输出 %v 相应值的默认格式。 Printf(\"%v\", people) {zhangsan}， %+v 打印结构体时，会添加字段名 Printf(\"%+v\", people) {Name:zhangsan} %#v 相应值的Go语法表示 Printf(\"#v\", people) main.Human{Name:\"zhangsan\"} %T 相应值的类型的Go语法表示 Printf(\"%T\", people) main.Human %% 字面上的百分号，并非值的占位符 Printf(\"%%\") % 布尔占位符 占位符 说明 举例 输出 %t true 或 false。 Printf(\"%t\", true) true 整数占位符 占位符 说明 举例 输出 %b 二进制表示 Printf(\"%b\", 5) 101 %c 相应Unicode码点所表示的字符 Printf(\"%c\", 0x4E2D) 中 %d 十进制表示 Printf(\"%d\", 0x12) 18 %o 八进制表示 Printf(\"%o\", 10) 12 %q 单引号围绕的字符字面值，由Go语法安全地转义 Printf(\"%q\", 0x4E2D) '中' %x 十六进制表示，字母形式为小写 a-f Printf(\"%x\", 13) d %X 十六进制表示，字母形式为大写 A-F Printf(\"%x\", 13) D %U Unicode格式：U+1234，等同于 \"U+%04X\" Printf(\"%U\", 0x4E2D) U+4E2D 浮点数和复数的组成部分（实部和虚部） 占位符 说明 举例 输出 %b 无小数部分的，指数为二的幂的科学计数法， 与 strconv.FormatFloat 的 'b' 转换格式一致。例如 -123456p-78 %e 科学计数法，例如 -1234.456e+78 Printf(\"%e\", 10.2) 1.020000e+01 %E 科学计数法，例如 -1234.456E+78 Printf(\"%e\", 10.2) 1.020000E+01 %f 有小数点而无指数，例如 123.456 Printf(\"%f\", 10.2) 10.200000 %g 根据情况选择 %e 或 %f 以产生更紧凑的（无末尾的0）输出 Printf(\"%g\", 10.20) 10.2 %G 根据情况选择 %E 或 %f 以产生更紧凑的（无末尾的0）输出 Printf(\"%G\", 10.20+2i) (10.2+2i) 字符串与字节切片 占位符 说明 举例 输出 %s 输出字符串表示（string类型或[]byte) Printf(\"%s\", []byte(\"Go语言\")) Go语言 %q 双引号围绕的字符串，由Go语法安全地转义 Printf(\"%q\", \"Go语言\") \"Go语言\" %x 十六进制，小写字母，每字节两个字符 Printf(\"%x\", \"golang\") 676f6c616e67 %X 十六进制，大写字母，每字节两个字符 Printf(\"%X\", \"golang\") 676F6C616E67 指针 占位符 说明 举例 输出 %p 十六进制表示，前缀 0x Printf(\"%p\", \u0026people) 0x4f57f0 其它标记 占位符 说明 举例 输出 + 总打印数值的正负号；对于%q（%+q）保证只输出ASCII编码的字符。 Printf(\"%+q\", \"中文\") \"\\u4e2d\\u6587\" - 在右侧而非左侧填充空格（左对齐该区域） # 备用格式：为八进制添加前导 0（%#o） Printf(\"%#U\", '中') U+4E2D 为十六进制添加前导 0x（%#x）或 0X（%#X），为 %p（%#p）去掉前导 0x； 如果可能的话，%q（%#q）会打印原始 （即反引号围绕的）字符串； 如果是可打印字符，%U（%#U）会写出该字符的 Unicode 编码形式（如字符 x 会被打印成 U+0078 'x'）。 ' ' (空格)为数值中省略的正负号留出空白（% d）； 以十六进制（% x, % X）打印字符串或切片时，在字节之间用空格隔开 0 填充前导的0而非空格；对于数字，这会将填充移到正负号之后 golang没有 ‘%u’ 点位符，若整数为无符号类型，默认就会被打印成无符号的。 宽度与精度的控制格式以Unicode码点为单位。宽度为该数值占用区域的最小宽度；精度为小数点之后的位数。 操作数的类型为int时，宽度与精度都可用字符 ‘*’ 表示。 对于 %g/%G 而言，精度为所有数字的总数，例如：123.45，%.4g 会打印123.5，（而 %6.2f 会打印123.45）。 %e 和 %f 的默认精度为6 对大多数的数值类型而言，宽度为输出的最小字符数，如果必要的话会为已格式化的形式填充空格。 而以字符串类型，精度为输出的最大字符数，如果必要的话会直接截断。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:9:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"变量和常量 变量： 为什么要有变量：程序运行过程中的数据都是保存在内存中，我们想要在代码中操作某个数据时就需要去内存上找到这个变量，但是如果我们直接在代码中通过内存地址去操作变量的话，代码的可读性会非常差而且还容易出错，所以我们就利用变量将这个数据的内存地址保存起来，以后直接通过这个变量就能找到内存上对应的数据了。 Go语言中的变量需要声明后才能使用，同一作用域内不支持重复声明。并且Go语言的变量声明后必须使用。 批量声明变量：（“声明聚类”） var ( a string b int c bool d float32 ) 建议将延迟初始化的变量声明放在一个 var 声明块，将声明且显式初始化的变量放在另一个 var 块中 变量声明咱们一般采用就近原则，以实现变量的作用域最小化 在函数内部，可以使用更简略的 := 方式声明并初始化变量。 匿名变量_ 常量: const ( pi = 3.1415 e = 2.7182 ) 如果省略了值则表示和上面一行的值相同 iota是go语言的常量计数器，只能在常量的表达式中使用。iota在const关键字出现时将被重置为0。const中每新增一行常量声明将使iota计数一次(iota可理解为const语句块中的行索引)。 使用iota能简化定义，在定义枚举时很有用。 可以使用_跳过某些值 const ( _ = iota KB = 1 \u003c\u003c (10 * iota) MB = 1 \u003c\u003c (10 * iota) GB = 1 \u003c\u003c (10 * iota) TB = 1 \u003c\u003c (10 * iota) PB = 1 \u003c\u003c (10 * iota) ) const ( a, b = iota + 1, iota + 2 //1,2 c, d //2,3 e, f //3,4 ) go在常量上还是有一定创新的： 与C对比： C 语言中，原生不支持常量，字面值担负着常量的角色，C 语言的常用实践是使用宏（macro）定义记号来指代这些字面值，但它是一种仅在预编译阶段进行替换的字面值，继承了宏替换的复杂性和易错性，而且还有类型不安全、无法在调试时通过宏名字输出常量的值，等等问题。后续 C 标准中提供的 const 关键字修饰的标识符也不够完美，因为 const 关键字修饰的标识符本质上依旧是变量，它甚至无法用作数组变量声明中的初始长度（除非用 GNU 扩展 C）。 Go 原生提供的用 const 关键字定义的常量，整合了 C 语言中宏定义常量、const 修饰的“只读变量”，以及枚举常量这三种形式，并消除了每种形式的不足，使得 Go 常量是类型安全的，而且对编译器优化友好。 支持无类型常量； eg: const n = 13 常量 n 在声明时并没有显式地被赋予类型（但并非真的无类型，由初始值给予默认类型），在 Go 中，这样的常量就被称为无类型常量（Untyped Constant） 但下面的例子里边为啥编译器不报错？ type myInt int const n = 13 func main() { var a myInt = 5 fmt.Println(a + n) // 输出：18 } 支持常量隐式自动转型； 对于无类型常量参与的表达式求值，Go 编译器会根据上下文中的类型信息，把无类型常量自动转换为相应的类型后，再参与求值计算，这一转型动作是隐式进行的。但由于转型的对象是一个常量，所以这并不会引发类型安全问题，Go 编译器会保证这一转型的安全性。 这就很好地解释了上面的问题。 注意：如果 Go 编译器在做隐式转型时，发现无法将常量转换为目标类型，Go 编译器也会报错，比如转型后溢出了。 以及前面有提到的可用于实现枚举。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:10:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"基本类型 类型 长度(字节) 默认值 说明 bool 1 false byte 1 0 uint8，一个ASCII字符 rune 4 0 Unicode Code Point，int32，一个utf-8字符,c:=[]rune(s)//指将字符串s转化为rune的切片 int, uint 4或8 0 由操作系统位数(32/64)决定 int8, uint8 1 0 -128 ~ 127, 0 ~ 255，byte是uint8 的别名 int16, uint16 2 0 -32768 ~ 32767, 0 ~ 65535 int32, uint32 4 0 -21亿~ 21亿, 0 ~ 42亿，rune是int32 的别名 int64, uint64 8 0 float32 4 0.0 float64 8 0.0 complex64 8 complex128 16 uintptr 4或8 以存储指针的 uint32 或 uint64 整数 array 值类型 struct 值类型 string “” UTF-8 字符串 slice nil 引用类型 map nil 引用类型 channel nil 引用类型 interface nil 接口 function nil 函数 uintptr 实际上就是一个 uint 用来表示地址，go 的指针和 c 不一样不能进行偏移操作，如果非要偏移的话就需要 unsafe.Pointer 和 uintptr 配合来实现。uintptr 不是一个指针 所以 GC 时也不会处理 uintptr 的引用。如果不涉及地址偏移时没有必要使用 uintptr 。——来自知乎回答 标准库 math 定义了各数字类型取值范围。 空指针值 nil，而非C/C++ NULL。golang中有多种引用类型：pointer、interface、slice、map、channel、function。go作为一个强类型语言（类型是定义好的无法改变，不像c，你定义一个short可以当成char用，因为可以直接操作内存），不同引用类型的判空（nil）规则是不同的；比如：interface的判空规则是，需要判断类型和值是否都为nil(interface的底层是有类型和值构成的)slice的判空，需要判断slice引用底层数组的指针为空，容量和size均为0。 不允许将整型强制转换为布尔型。 字符串的内部实现使用UTF-8编码。（UTF-8是Unicode的存储实现，转化为有限长度比特组合的规则） 只有显示类型转化。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:11:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"string 值类型，空值是空字符串而不是nil。本质就是个只读的（不可变的）byte切片。 // $GOROOT/src/reflect/value.go // StringHeader是一个string的运行时表示 type StringHeader struct { Data uintptr Len int } string 类型其实是一个“描述符”，它本身并不真正存储字符串数据，而仅是由一个指向底层存储的指针和字符串的长度字段组成的 多行字符串，用反引号` s1 := `第一行 第二行 第三行 ` fmt.Println(s1) 方法 介绍 len(str) 求长度 +或fmt.Sprintf 拼接字符串 strings.Split 分割 strings.Contains 判断是否包含 strings.HasPrefix,strings.HasSuffix 前缀/后缀判断 strings.Index(),strings.LastIndex() 子串出现的位置 strings.Join(a[]string, sep string) join操作 遍历字符串： // Traversal strings func traversalString() { s := \"JeFo的博客\" for i := 0; i \u003c len(s); i++ { //Traversal by byte fmt.Printf(\"%v(%c) \", s[i], s[i]) } fmt.Println() for _, r := range s { //Traversal by rune fmt.Printf(\"%v(%c) \", r, r) } fmt.Println() } 74(J) 101(e) 70(F) 111(o) 231(ç) 154() 132( ) 229(å) 141() 154() 229(å) 174(®) 162(¢) 74(J) 101(e) 70(F) 111(o) 30340(的) 21338(博) 23458(客) 可以调用标准库 UTF-8 包中的 RuneCountInString 函数获取字符串字符个数。（len只能获得字节个数） Go 原生支持通过 +/+= 操作符进行字符串连接。以及，Go 还提供了 strings.Builder、strings.Join、fmt.Sprintf 等函数来进行字符串连接操作。 如果能知道拼接字符串的个数，那么使用bytes.Buffer和strings.Builder的Grows申请空间后，性能是最好的；如果不能确定长度，那么bytes.Buffer和strings.Builder也比“+”和fmt.Sprintf性能好很多。 bytes.Buffer与strings.Builder，strings.Builder更合适，因为bytes.Buffer 转化为字符串时重新申请了一块空间，存放生成的字符串变量，而 strings.Builder 直接将底层的 []byte 转换成了字符串类型返回了回来。 字符串比较： Go 字符串类型支持各种比较关系操作符，包括 = =、!= 、\u003e=、\u003c=、\u003e 和 \u003c。在字符串的比较上，Go 采用字典序的比较策略，分别从每个字符串的起始处，开始逐个字节地对两个字符串类型变量进行比较。 Go 支持字符串与字节切片、字符串与 rune 切片的双向转换，并且这种转换无需调用任何函数，只需使用显式类型转换就可以了 修改字符串： 要修改字符串，需要先将其转换成[]rune或[]byte，完成后再转换为string。无论哪种转换，都会重新分配内存，并复制字节数组。 为什么go要原生支持字符串类型？ c语言并没有内置字符串类型 不是原生类型，编译器不会对它进行类型校验，导致类型安全性差； 字符串操作时要时刻考虑结尾的’\\0’，防止缓冲区溢出； 以字符数组形式定义的“字符串”，它的值是可变的，在并发场景中需要考虑同步问题； 获取一个字符串的长度代价较大，通常是 O(n) 时间复杂度； C 语言没有内置对非 ASCII 字符（如中文字符）的支持。 go内置了字符串类型的好处 string 类型的数据是不可变的，提高了字符串的并发安全性和存储利用率。 没有结尾’\\0’，而且获取长度的时间复杂度是常数时间，消除了获取字符串长度的开销。 原生支持“所见即所得”的原始字符串，大大降低构造多行字符串时的心智负担。 在 C 语言中构造多行字符串，一般就是两个方法：要么使用多个字符串的自然拼接，要么需要结合续行符\"\"。但因为有转义字符的存在，我们很难控制好格式。Go 语言就简单多了，通过一对反引号原生支持构造“所见即所得”的原始字符串（Raw String）。而且，Go 语言原始字符串中的任意转义字符都不会起到转义的作用 对非 ASCII 字符提供原生支持，消除了源码在不同环境下显示乱码的可能。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:12:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"数组 数组可以通过下标进行访问，下标是从0开始，最后一个元素下标是：len-1 支持 “==\"、”!=\" 操作符，因为内存总是被初始化过的。 相同类型的数组之间可以使用 == 或 != 进行比较，但不可以使用 \u003c 或 \u003e，也可以相互赋值。 长度不同类型也不同。 指针数组 [n]*T，数组指针 *[n]T。 多维数组除了第一维，初始化时都不能用[…]省略长度声明。 值拷贝行为会造成性能问题，通常会建议使用 slice，或数组指针。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:13:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"列表list 初始化： 通过 container/list 包的 New() 函数初始化 list。变量名 := list.New() 通过 var 关键字声明初始化 list 。var 变量名 list.List 列表与切片和 map 不同的是，列表并没有具体元素类型的限制，因此，列表的元素可以是任意类型，这既带来了便利，也引来一些问题，例如给列表中放入了一个 interface{} 类型的值，取出值后，如果要将 interface{} 转换为其他类型将会发生宕机。 源码数据结构： type Element struct { // Next and previous pointers in the doubly-linked list of elements. // To simplify the implementation, internally a list l is implemented // as a ring, such that \u0026l.root is both the next element of the last // list element (l.Back()) and the previous element of the first list // element (l.Front()). next, prev *Element // The list to which this element belongs. list *List // The value stored with this element. Value interface{} } type List struct { root Element // sentinel list element, only \u0026root, root.prev, and root.next are used len int // current list length excluding (this) sentinel element } 其他源码列出稍冗长，root.prev可以视作尾节点，root.next相当于头节点，不过这些是透明的，被封装好的。 在列表中插入元素删除元素都有简便方法。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:14:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"切片 只能和nil进行比较。 //用make()初始化 var s3 []int = make([]int, 0, 10)//len=0,cap=10 var s4 []int = make([]int, 5)//len=5 //先初始化一个数组，再截取相应部分得到切片 arr := [5]int{1, 2, 3, 4, 5} var s6 []int //数组的切片化 s6 = arr[1:4] // 左闭右开 s7 = arr[1:3:5]//arr[low,high,max]，len=high-low cap=max-low 切片追加append（内置函数）： // The append built-in function appends elements to the end of a slice. If // it has sufficient capacity, the destination is resliced to accommodate the // new elements. If it does not, a new underlying array will be allocated. // Append returns the updated slice. It is therefore necessary to store the // result of append, often in the variable holding the slice itself: // slice = append(slice, elem1, elem2) // slice = append(slice, anotherSlice...) // As a special case, it is legal to append a string to a byte slice, like this: // slice = append([]byte(\"hello \"), \"world\"...) var a = []int{1, 2, 3} fmt.Printf(\"slice a : %v\\n\", a) var b = []int{4, 5, 6} fmt.Printf(\"slice b : %v\\n\", b) c := append(a, b...) fmt.Printf(\"slice c : %v\\n\", c) d := append(c, 7) fmt.Printf(\"slice d : %v\\n\", d) e := append(d, 8, 9, 10) fmt.Printf(\"slice e : %v\\n\", e) slice := append([]byte(\"hello \"), \"world\"...)//**注意**是字节数组和字符串。 fmt.Printf(\"slice slice : %v\\n\", slice) func TestOne(t *testing.T) { q := make([]int, 3, 10) w := append(q, 1) t.Log(len(w), len(q)) e := append(q, 2) //append是加在该切片的len后面，但不是最后一个元素后面，因为底层数组会被改变，而切片变量的结构体所记录的信息是固定的。 t.Log(q, w, e) } 超出原 slice.cap 限制，就会重新分配底层数组，即便原数组并未填满。 通常以 2 倍容量重新分配底层数组。在大批量添加数据时，建议一次性分配足够大的空间，以减少内存分配和数据复制开销。或初始化足够长的 len 属性，改用索引号进行操作。 及时释放不再使用的 slice 对象，避免持有过期数组，造成 GC 无法回收。 切片resize: package main import ( \"fmt\" ) func main() { var a = []int{1, 3, 4, 5} fmt.Printf(\"slice a : %v , len(a) : %v\\n\", a, len(a)) b := a[1:2] fmt.Printf(\"slice b : %v , len(b) : %v\\n\", b, len(b)) c := b[1:3] fmt.Printf(\"slice c : %v , len(c) : %v\\n\", c, len(c)) } slice a : [1 3 4 5] , len(a) : 4 slice b : [3] , len(b) : 1 slice c : [4 5] , len(c) : 2 string \u0026 slice : string底层就是一个byte的数组，因此，也可以进行切片操作。 二维切片： …… a := make([][]int,dy) for i = range a { a[i] = make([]int, dx) } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:15:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"slice切片底层实现 切片的设计想法是由动态数组概念而来，为了开发者可以更加方便的使一个数据结构可以自动增加和减少。但是切片本身并不是动态数据或者数组指针。切片常见的操作有 reslice、append、copy。与此同时，切片还具有可索引，可迭代的优秀特性。 func main() { arrayA := []int{1, 2} testArrayPoint(\u0026arrayA) // 1.传数组指针 arrayB := arrayA[:] testArrayPoint(\u0026arrayB) // 2.传切片 fmt.Printf(\"arrayA : %p , %v\\n\", \u0026arrayA, arrayA) } func testArrayPoint(x *[]int) { fmt.Printf(\"func Array : %p , %v\\n\", x, *x) (*x)[1] += 1 } func Array : 0xc4200b0140 , [1 2] func Array : 0xc4200b0180 , [1 3] arrayA : 0xc4200b0140 , [1 4] 传指针会有一个弊端，从打印结果可以看到，第一行和第三行指针地址都是同一个，万一原数组的指针指向更改了，那么函数里面的指针指向都会跟着更改。 用切片传数组参数，既可以达到节约内存的目的，也可以达到合理处理好共享内存的问题。打印结果第二行就是切片，切片的指针和原来数组的指针是不同的。 slice数据结构源码： // runtime/slice.go type slice struct { array unsafe.Pointer // 指向一个数组的指针 len int // 切片长度 cap int // 切片容量 } 注意：Golang 语言是没有操作原始内存的指针的，所以 unsafe 包提供相关的对内存指针的操作，一般情况下非专业人员勿用 如果想从 slice 中得到一块内存地址，可以这样做： s := make([]byte, 200) ptr := unsafe.Pointer(\u0026s[0]) 自己构造一个slice: var ptr unsafe.Pointer var s1 = struct { addr uintptr len int cap int }{ptr, length, length} s := *(*[]byte)(unsafe.Pointer(\u0026s1)) 在 Go 的反射中就存在一个与之对应的数据结构 SliceHeader，我们可以用它来构造一个 slice： var o []byte sliceHeader := (*reflect.SliceHeader)((unsafe.Pointer(\u0026o))) sliceHeader.Cap = length sliceHeader.Len = length sliceHeader.Data = uintptr(ptr) 此外，unsafe的Sizeof函数： var a, b = int(5), uint(6) var p uintptr = 0x12345678 fmt.Println(\"signed integer a's length is\", unsafe.Sizeof(a)) // 8 fmt.Println(\"unsigned integer b's length is\", unsafe.Sizeof(b)) // 8 fmt.Println(\"uintptr's length is\", unsafe.Sizeof(p)) // 8 并非所有时候都适合用切片代替数组：因为切片底层数组可能会在堆上分配内存，而且小数组在栈上拷贝的消耗也未必比make 消耗大。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:15:1","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"创建切片 make 函数允许在运行期动态指定数组长度，绕开了数组类型必须使用编译期常量的限制。 创建切片有两种形式，make 创建切片，字面量创建切片（既可以初始化一个新的，也可以截取一个数组,截取一个数组的时候cap未声明时为数组容量）。 空切片和nil切片 nil切片：| nil (Pointer) | Len(int) | Cap(int) | var slice []int nil 切片被用在很多标准库和内置函数中，描述一个不存在的切片的时候，就需要用到 nil 切片。比如函数在发生异常的时候，返回的切片就是 nil 切片。nil 切片的指针指向 nil。 空切片： | Array (Pointer) | Len(int) | Cap(int) | silce := make( []int , 0 ) slice := []int{ } 空切片一般会用来表示一个空的集合。比如数据库查询，一条结果也没有查到，那么就可以返回一个空切片。 空切片和 nil 切片的区别在于，空切片指向的地址不是nil，指向的是一个内存地址，但是它没有分配任何内存空间，即底层元素包含0个元素。 不管是使用 nil 切片还是空切片，对其调用内置函数 append，len 和 cap 的效果都是一样的。 扩容策略 如果切片的容量小于 1024 个元素，于是扩容的时候就翻倍增加容量。 一旦元素个数超过 1024 个元素，那么增长因子就变成 1.25 ，即每次增加原来容量的四分之一。 注意：扩容扩大的容量都是针对原来的容量而言的，而不是针对原来数组的长度而言的。 扩容后的数组是新数组还是老数组？ 如果如果切片扩容后容量比原来数组的容量最大值还大，扩容切片需要另开一片内存区域，把原来的值拷贝过来，再执行append()操作。 否则，不会开辟新数组，这种情况很危险，因为这种情况下，扩容以后的数组还是指向原来的数组,多个原来的数组上的切片会受新切片所影响！ ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:15:2","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"切片拷贝 slicecopy 方法会把源切片值(即 fm Slice )中的元素复制到目标切片(即 to Slice )中，并返回被复制的元素个数，copy 的两个类型必须一致。slicecopy 方法最终的复制结果取决于较短的那个切片，当较短的切片复制完成，整个复制过程就全部完成了。 如果用 range 的方式去遍历一个切片，拿到的 Value 其实是切片里面的值拷贝。所以每次打印 Value 的地址都不变。由于 Value 是值拷贝的，并非引用传递，所以直接改 Value 是达不到更改原切片值的目的的，需要通过 \u0026slice[index] 获取真实的地址。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:15:3","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"指针 区别于C/C++中的指针，Go语言中的指针不能进行偏移和运算，是安全指针。 首先需要知道指针地址、指针类型和指针取值。 指针地址和指针类型： Go语言中的值类型（int、float、bool、string、array、struct）都有对应的指针类型 指针取值：* 空指针： 当一个指针被定义后没有分配到任何变量时，它的值为 nil new和make: 在Go语言中对于引用类型的变量，我们在使用的时候不仅要声明它，还要为它分配内存空间，否则我们的值就没办法存储。 new 函数签名和举例 func new(Type) *Type var a *int a = new(int) *a = 10 fmt.Println(*a) } new函数不太常用，使用new函数得到的是一个类型的指针，并且该指针对应的值为该类型的零值。 make 只用于slice、map以及chan的内存创建，而且它返回的类型就是这三个类型本身,因为这三种类型就是引用类型，所以就没有必要返回他们的指针了。 以map举例： func main() { var b map[string]int b = make(map[string]int, 10) b[\"测试\"] = 100 fmt.Println(b) } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:16:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"map map[KeyType]ValueType 初始化时用make申请内存（或者直接填充元素）： make(map[KeyType]ValueType, [cap])//cap不是必须的，但最好一开始就申请一个合适的容量 // // 初始化 + 赋值一体化 m3 := map[string]string{ \"a\": \"aa\", \"b\": \"bb\", } 判断某个key是否存在： //map[key]会返回两个值，第二个是该键是否存在 value, ok := map[key] map的遍历还是正常用for range，但有一点需要注意：遍历map时元素顺序与添加键值对的顺序无关。 按照指定顺序遍历map:思路是将map的key取出另存为切片再排序，再按照切片的顺序进行遍历即可。 因为 map 类型要保证 key 的唯一性。key 的类型必须支持“==”和“!=”两种比较操作符，比如函数类型、map 类型自身，以及切片类型是不能作为 map 的 key 类型的。value类型则没有限制。 map \u0026 切片： 元素为map的切片： func main() { var mapSlice = make([]map[string]string, 3) for index, value := range mapSlice { fmt.Printf(\"index:%d value:%v\\n\", index, value) } fmt.Println(\"after init\") // 对切片中的map元素进行初始化 mapSlice[0] = make(map[string]string, 10) mapSlice[0][\"name\"] = \"王五\" mapSlice[0][\"password\"] = \"123456\" mapSlice[0][\"address\"] = \"红旗大街\" } value为切片的map: func main() { var sliceMap = make(map[string][]string, 3) fmt.Println(sliceMap) fmt.Println(\"after init\") key := \"中国\" value, ok := sliceMap[key] if !ok { value = make([]string, 0, 2) } value = append(value, \"北京\", \"上海\") sliceMap[key] = value fmt.Println(sliceMap) } 获取键值对数量： m := map[string]int { \"key1\" : 1, \"key2\" : 2, } fmt.Println(len(m)) // 2 m[\"key3\"] = 3 fmt.Println(len(m)) // 3 注意：不能对 map 类型变量调用 cap，来获取当前容量 map删除键值对：（即便传给 delete 的键在 map 中并不存在，delete 函数的执行也不会失败，更不会抛出运行时的异常。） delete(map,key) ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:17:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"map实现原理 map底层存储方式为（结构体）数组，在存储时key不能重复，当key重复时，value进行覆盖，我们通过key进行hash运算（可以简单理解为把key转化为一个整形数字）然后对数组的长度取余，得到key存储在数组的哪个下标位置，最后将key和value组装为一个结构体，放入数组下标处。 哈希冲突：即不同key经哈希映射后得到相同的数组下标。 解决办法：开放定址法： 发现hashkey(key)的下标已经被别key占用的时候，在这个数组中空间中重新找一个没被占用的存储这个冲突的key。寻找方式有很多。常见的有线性探测法，线性补偿探测法，随机探测法。 线性探测法： 从冲突的下标处开始往后探测，到达数组末尾时，从数组开始处探测，直到找到一个空位置存储这个key，当数组都找不到的情况下会扩容（事实上当数组容量快满的时候就会扩容了） 查找某一个key的时候，找到key对应的下标，比较key是否相等，如果相等直接取出来，否则按照顺序探测直到碰到一个空位置，说明key不存在。 拉链法： 当key的hash冲突时，我们在冲突位置的元素上形成一个链表，通过指针互连接。 当查找时，发现key冲突，顺着链表一直往下找，直到链表的尾节点，找不到则返回空 开放定址法的优缺点： 由上面可以看出拉链法比线性探测处理简单 线性探测查找是会被拉链法会更消耗时间 线性探测会更加容易导致扩容，而拉链不会 拉链存储了指针，所以空间上会比线性探测占用多一点 拉链是动态申请存储空间的，所以更适合链长不确定的 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:17:1","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go中map的实现原理 go里面map并不是线程安全的，在1.9版本之前用map加互斥锁来解决此问题，之后加了sync.map性能稍微高了一些，因为它有一块只读的buffer，相当于由两个buffer组成，一块只读，一块读写。sync.map更适合于读非常多，能够占到90%以上的情况。如果读写差不多或者说写更多的话，sync.map的性能就比较差了。 在读写对半的情况下，可以考虑引入concurrent_map包。（go get -u +包地址） 在go1.16中，map也是数组存储的的，每个数组下标处存储的是一个bucket,这个bucket的类型见下面代码，每个bucket中可以存储8个kv键值对，当每个bucket存储的kv对到达8个之后，会通过overflow指针指向一个新的bucket，从而形成一个链表,看bmap和hmap的结构 //go 1.18 // A bucket for a Go map. type bmap struct { // tophash generally contains the top byte of the hash value // for each key in this bucket. If tophash[0] \u003c minTopHash, // tophash[0] is a bucket evacuation state instead.即桶疏散状态 tophash [bucketCnt]uint8 // Followed by bucketCnt keys and then bucketCnt elems. // NOTE: packing all the keys together and then all the elems together makes the // code a bit more complicated than alternating key/elem/key/elem/... but it allows // us to eliminate padding which would be needed for, e.g., map[int64]int8. // Followed by an overflow pointer. } // A header for a Go map. type hmap struct { // Note: the format of the hmap is also encoded in cmd/compile/internal/reflectdata/reflect.go. // Make sure this stays in sync with the compiler's definition. count int // # live cells == size of map. Must be first (used by len() builtin) flags uint8 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) noverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details hash0 uint32 // hash seed buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing nevacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated) extra *mapextra // optional fields } B 是 buckets 数组的长度的对数，也就是说 buckets 数组的长度就是 2^B。 tophash用来快速查找key值是否在该bucket中，而不同每次都通过真值进行比较。 map[int64]int8,key是int64（8个字节），value是int8（一个字节），kv的长度不同，如果按照kv格式存放，则考虑内存对齐v也会占用int64，而按照后者存储时，8个v刚好占用一个int64 当往map中存储一个kv对时，通过k获取hash值，hash值的低八位和bucket数组长度取余，定位到在数组中的那个下标，hash值的高八位存储在bucket中的tophash中，用来快速判断key是否存在，当一个bucket满时，通过overfolw指针链接到下一个bucket。 map的存储源码： // Like mapaccess, but allocates a slot for the key if it is not present in the map.如果key不在map里为其分配一个插槽（狭槽） func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if h == nil { panic(plainError(\"assignment to entry in nil map\")) } if raceenabled { callerpc := getcallerpc() pc := funcPC(mapassign) racewritepc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled { //获取hash算法 msanread(key, t.key.size) } if h.flags\u0026hashWriting != 0 { throw(\"concurrent map writes\") } //计算哈希值 hash := t.hasher(key, uintptr(h.hash0)) // Set hashWriting after calling t.hasher, since t.hasher may panic, // in which case we have not actually done a write. h.flags ^= hashWriting //如果bucket数组一开始为空，则初始化 if h.buckets == nil { h.buckets = newobject(t.bucket) // newarray(t.bucket, 1) } again: //定位在哪一个bucket中 bucket := hash \u0026 bucketMask(h.B) if h.growing() { growWork(t, h, bucket) } //得到bucket的结构体 b := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize))) //获取高八位的哈希值 top := tophash(hash) var inserti *uint8 var insertk unsafe.Pointer var elem unsafe.Pointer bucketloop: //死循环 for { //循环bucket中的tophash数组 for i := uintptr(0); i \u003c bucketCnt; i++ { //如果hash不相等 if b.tophash[i] != top { //判断是否为空，为空则插入 if isEmpty(b.tophash[i]) \u0026\u0026 inserti == nil { inserti = \u0026b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) } //插入成功，终止外层循环 if b.tophash[i] == emptyRest { break bucketloop } continue } //高八位哈希值一样，获取已存在的kay k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } //判断两个key是否相等，不相等就循环下一个 if !t.key.equal(key, k) { continue } // already have a mapping for key. Update it. if t.needkeyupdate() { typedmemmove(t.key, k, key) } //获取已存在的value elem = add(unsafe.Pointer(b), dataOffset","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:17:2","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"map与工厂模式 map的value可以是一个方法。 与 Go 的 Dock type 接⼝⽅式⼀起，可以⽅便的实现单⼀⽅法对象的⼯⼚模式 func TestMapWithFunValue(t *testing.T) { m := map[int]func(op int) int{} m[1] = func(op int) int { return op } m[2] = func(op int) int { return op * op } m[3] = func(op int) int { return op * op * op } t.Log(m[1](2), m[2](2), m[3](2)) } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:17:3","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"set Go 的内置集合中没有 Set 实现， 可以 map[type]bool。 map可以保证添加元素的唯⼀性，方便判断唯一元素的个数 基本操作 添加元素 判断元素是否存在 删除元素 元素个数 func TestMapForSet(t *testing.T) { mySet := map[int]bool{} mySet[1] = true n := 3 if mySet[n] { t.Logf(\"%d is existing\", n) } else { t.Logf(\"%d is not existing\", n) } mySet[3] = true t.Log(len(mySet)) delete(mySet, 1) n = 1 if mySet[n] { t.Logf(\"%d is existing\", n) } else { t.Logf(\"%d is not existing\", n) } } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:18:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"结构体 Go语言中通过结构体的内嵌再配合接口比面向对象具有更高的扩展性和灵活性。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:19:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"类型别名和自定义类型 自定义类型（新类型）： //将MyInt定义为int类型 type MyInt int 可以基于内置的基本类型定义，也可以通过struct定义。 类型别名（Go1.9添加的新功能，注意编译后是原来的类型）： //将MyInt作为为int类型的昵称 type MyInt = int ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:19:1","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"结构体 本质上是一种聚合型的数据类型。 通过struct可以实现面向对象。 定义的时候，同样类型的字段也可以写在一行。 只有结构体被实例化时，才会真正被分配内存。 匿名结构体：定义临时数据结构时可能会用到。 语法糖：Go语言中支持对结构体指针直接使用.来访问结构体的成员，在使用new分配内存后得到的便是结构体指针。 对于包含结构体类型字段的结构体类型来说,可以无需提供字段的名字: 以这种方式定义的结构体字段，我们叫做嵌入字段（Embedded Field）。我们也可以将这种字段称为匿名字段，或者把类型名看作是这个字段的名字 type Book struct { Title string Person ... ... } func main(){ var book Book println(book.Person.Phone) // 将类型名当作嵌入字段的名字 println(book.Phone) // 支持直接访问嵌入字段所属类型中字段 } 注意： 结构体类型 T 定义中，不能有以自身类型 T 定义的字段，但却可以拥有自身类型的指针类型、以自身类型为元素类型的切片类型，以及以自身类型作为 value 类型的 map 类型的字段，因为指针、map、切片的变量元数据的内存占用大小是固定的。 type T struct { a T // wrong t *T // ok st []T // ok m map[string]T // ok } sync.Mutex和bytes.Buffer的“零值可用”： var mu sync.Mutex mu.Lock() mu.Unlock() var b bytes.Buffer b.Write([]byte(\"Hello, Go\")) fmt.Println(b.String()) // 输出：Hello, Go 使用\u0026对结构体进行取地址操作相当于对该结构体类型进行了一次new实例化操作。 p := \u0026person{} 初始化（没有指定初始值的字段的值就是该字段类型的零值）： p := person{ a: \"1a\", b: \"2b\", } //结构体指针 q := \u0026ss{ a: \"1a\", b: \"2b\", } //简写需**注意**三点：1.必须初始化结构体的所有字段。2.初始值的填充顺序必须与字段在结构体中的声明顺序一致。3.该方式不能和键值初始化方式混用。 s := \u0026d{ \"aq\", \"sw\", } go 语言并不推荐我们按字段顺序对一个结构体类型变量进行显式初始化，甚至 Go 官方还在提供的 go vet 工具中专门内置了一条检查规则：“composites”，用来静态检查代码中结构体变量初始化是否使用了这种方法，一旦发现，就会给出警告。 Go 推荐我们用“field:value”形式的复合字面值，对结构体类型变量进行显式初始化，这种方式可以降低结构体类型使用者和结构体类型设计者之间的耦合 var t = T{ F2: \"hello\", F1: 11, F4: 14, } 空结构体的作用： 空结构体类型变量内存占用为0 使用空结构体类型元素，作为一种“事件”信息进行 Goroutine 之间的通信 var c = make(chan Empty) // 声明一个元素类型为Empty的channel c\u003c-Empty{} // 向channel写入一个“事件” 这种以空结构体为元素类建立的 channel，是目前能实现的、内存占用最小的 Goroutine 间通信方式。 空标识符“_”作为结构体类型定义中的字段名称的作用： 自己实现一个结构体构造函数： //值拷贝开销太大，返回结构体指针 func newPerson(name, city string, age int8) *person { return \u0026person{ name: name, city: city, age: age, } } 如果一个结构体类型中包含未导出字段，并且这个字段的零值还不可用时,又或是一个结构体类型中的某些字段，需要一个复杂的初始化逻辑时：需要使用一个特定的构造函数，来创建并初始化结构体变量了。 结构体类型的内存布局： 在真实情况下，虽然 Go 编译器没有在结构体变量占用的内存空间中插入额外字段，但结构体字段实际上可能并不是紧密相连的，中间可能存在“缝隙”。这些“缝隙”同样是结构体变量占用的内存空间的一部分，它们是 Go 编译器插入的“填充物（Padding）” 这是为了内存对齐。 方法和接收者 Go语言中的方法（Method）是一种作用于特定类型变量的函数。这种特定类型变量叫做接收者（Receiver）。接收者的概念就类似于其他语言中的this或者 self。 //接收者变量：接收者中的参数变量名在命名时，官方建议使用接收者类型名的第一个小写字母，而不是self、this之类的命名。例如，Person类型的接收者变量应该命名为 p，Connector类型的接收者变量应该命名为c等。 func (接收者变量 接收者类型) 方法名(参数列表) (返回参数) { 函数体 } 指针类型的接收者： 指针类型的接收者由一个结构体的指针组成，由于指针的特性，调用方法时修改接收者指针的任意成员变量，在方法结束后，修改都是有效的。这种方式就十分接近于其他语言中面向对象中的this或者self。 例如我们为Person添加一个SetAge方法，来修改实例变量的年龄。 // SetAge 设置p的年龄 // 使用指针接收者 func (p *Person) SetAge(newAge int8) { p.age = newAge } //调用该方法 func main() { p1 := NewPerson(\"测试\", 25) fmt.Println(p1.age) // 25 p1.SetAge(30) fmt.Println(p1.age) // 30 } 值类型的接收者： // SetAge2 设置p的年龄 // 使用值接收者 func (p Person) SetAge2(newAge int8) { p.age = newAge } func main() { p1 := NewPerson(\"测试\", 25) p1.Dream() fmt.Println(p1.age) // 25 p1.SetAge2(30) // (*p1).SetAge2(30) fmt.Println(p1.age) // 25 } 什么时候应该使用指针类型接收者： 需要修改接收者中的值 接收者是拷贝代价比较大的大对象 保证一致性，如果有某个方法使用了指针接收者，那么其他的方法也应该使用指针接收者。 为任意类型添加方法： 在Go语言中，接收者的类型可以是任何类型，不仅仅是结构体，任何类型都可以拥有方法。 举个例子，我们基于内置的int类型使用type关键字可以定义新的自定义类型，然后为我们的自定义类型添加方法。 注意事项：非本地类型不能定义方法，也就是说我们不能给别的包的类型定义方法。 结构体的匿名字段： 结构体允许其成员字段在声明时没有字段名而只有类型，这种没有名字的字段就称为匿名字段。 匿名字段默认采用类型名作为字段名，结构体要求字段名称必须唯一，因此一个结构体中同种类型的匿名字段只能有一个。 嵌套结构体 一个结构体中可以嵌套包含另一个结构体或结构体指针。 嵌套结构体内部可能存在相同的字段名。这个时候为了避免歧义需要指定具体的内嵌结构体的字段。 类型的“继承”： 即通过类型嵌入，包括接口类型的类型嵌入和结构体类型的类型嵌入。 结构体类型中嵌入接口类型：结构体类型的方法集合，包含嵌入的接口类型的方法集合。 结构体类型嵌入接口类型在日常编码中有一个妙用，就是可以简化单元测试的编写：见下面例子： 由于嵌入某接口类型的结构体类型的方法集合包含了这个接口类型的方法集合，这就意味着，这个结构体类型也是它嵌入的接口类型的一个实现 package employee type Result struct { Count int } func (r Result) Int() int { return r.Count } type Rows []struct{} type Stmt interface { Close() error NumInput() int Exec(stmt string, args ...string) (Result, error) Query(args []string) (Rows, error) } // 返回男性员工总数 func MaleCount(s Stmt) (int, error) { result, err := s.Exec(\"select count(*) from employee_tab where gender=?\", \"1\") if err != nil { return 0, err } return result.Int(), nil } package employee import \"testing\" type fakeStmtForMaleCount struct { Stmt } func (fakeStmtForMaleCount) Exec(stmt string, args ...string) (Result, error) { return Result{Count: 5}, nil } func TestEmployeeMaleCount(t *testing.T) { f :=","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:19:2","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go项目的标准布局演进 官方并没有给标准布局，但社区还是有的。随着go版本的不断更新，Go源码比例不断增大，Go 1.0时还占比32%的C语言现在也只不过占比不到1%。而项目布局一直保持了下来。 Go 1.3 src 目录下面的结构： 以 all.bash 为代表的代码构建的脚本源文件放在了 src 下面的顶层目录下。 src 下的二级目录 cmd 下面存放着 Go 相关可执行文件的相关目录 每个子目录都是一个 Go 工具链命令或子命令对应的可执行文件 src 下的二级目录 pkg 下面存放着运行时实现、标准库包实现，这些包既可以被上面 cmd 下各程序所导入，也可以被 Go 语言项目之外的 Go 程序依赖并导入。 Go 1.4 版本删除 pkg 这一中间层目录并引入 internal 目录。“src/pkg/xxx”-\u003e“src/xxx” 根据 internal 机制的定义，一个 Go 项目里的 internal 目录下的 Go 包，只可以被本项目内部的包导入。项目外部是无法导入这个 internal 目录下面的包的。 internal 目录的引入，让一个 Go 项目中 Go 包的分类与用途变得更加清晰 Go 1.6 版本增加 vendor 目录 为了解决 Go 包依赖版本管理的问题，Go 核心团队在 Go 1.5 版本中做了第一次改进。增加了 vendor 构建机制，也就是 Go 源码的编译可以不在 GOPATH 环境变量下面搜索依赖包的路径，而在 vendor 目录下查找对应的依赖包 Go 1.7 版本，Go 在 vendor 下缓存了其依赖的外部包。这些依赖包主要是 golang.org/x 下面的包 vendor 机制与目录的引入，让 Go 项目第一次具有了可重现构建（Reproducible Build）的能力。 Go 1.13 版本引入 go.mod 和 go.sum 在 Go 1.11 版本中，Go 核心团队做出了第二次改进尝试：引入了 Go Module 构建机制，也就是在项目引入 go.mod 以及在 go.mod 中明确项目所依赖的第三方包和版本，项目的构建就将摆脱 GOPATH 的束缚，实现精准的可重现构建。 Go 语言项目自身在 Go 1.13 版本引入 go.mod 和 go.sum 以支持 Go Module 构建机制 Go 可执行程序项目的典型结构布局： ├── cmd/ │ ├── app1/ │ │ └── main.go │ └── app2/ │ └── main.go ├── go.mod ├── go.sum ├── internal/ │ ├── pkga/ │ │ └── pkg_a.go │ └── pkgb/ │ └── pkg_b.go ├── pkg1/ │ └── pkg1.go ├── pkg2/ │ └── pkg2.go └── vendor/ cmd（也可以是app或者其他名字） 存放项目要编译构建的可执行文件对应的 main 包的源文件。如果你的项目中有多个可执行文件需要构建，每个可执行文件的 main 包单独放在一个子目录中，cmd 目录下的各 app 的 main 包将整个项目的依赖连接在一起 通常来说，main 包应该很简洁。我们在 main 包中会做一些命令行参数解析、资源初始化、日志设施初始化、数据库连接初始化等工作，之后就会将程序的执行权限交给更高级的执行控制对象 pkgN 一个存放项目自身要使用、同样也是可执行文件对应 main 包所要依赖的库文件，同时这些目录下的包还可以被外部项目引用。 go.mod和go.sum 包依赖管理的配置文件 vendor（可选） 前面有说，vendor 是 Go 1.5 版本引入的用于在项目本地缓存特定版本依赖包的机制 Go Module 机制也保留了 vendor 目录（通过 go mod vendor 可以生成 vendor 下的依赖包，通过 go build -mod=vendor 可以实现基于 vendor 的构建）。一般我们仅保留项目根目录下的 vendor 目录，否则会造成不必要的依赖选择的复杂性。 etc 如若喜欢借助一些第三方的构建工具辅助构建，比如：make、bazel 等。你可以将这类外部辅助构建工具涉及的诸多脚本文件（比如 Makefile）放置在项目的顶层目录下，就像 Go 1.3中的 all.bash 那样。 如果app1，app2的发布版本不总是同步的，建议将每个项目单独作为一个 module 进行单独的版本管理和演进，避免版本管理的“分歧”带来更大的复杂性。当然新版Go命令较好地解决了这一点，可以采用如下结构： ├── go.mod // mainmodule ├── module1 │ └── go.mod // module1 └── module2 └── go.mod // module2 可以通过 git tag 名字来区分不同 module 的版本。其中 vX.Y.Z 形式的 tag 名字用于代码仓库下的 mainmodule；而 module1/vX.Y.Z 形式的 tag 名字用于指示 module1 的版本。 Go 库项目的典型结构布局：Go 库项目主要作用还是对外暴露 API。 ├── go.mod ├── internal/ │ ├── pkga/ │ │ └── pkg_a.go │ └── pkgb/ │ └── pkg_b.go ├── pkg1/ │ └── pkg1.go └── pkg2/ └── pkg2.go 不需要构建可执行程序 仅限项目内部使用而不想暴露到外部的包，可以放在项目顶层的 internal 目录下面。当然 internal 也可以有多个并存在于项目结构中的任一目录层级中，关键是项目结构设计人员要明确各级 internal 包的应用层次和范围。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:20:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go应用构建模式的演进 Go 程序的构建过程就是确定包版本、编译包以及将编译后得到的目标文件链接在一起的过程。 包依赖管理演进： GOPATH模式。 Go 编译器可以在本地 GOPATH 环境变量配置的路径下，搜寻 Go 程序依赖的第三方包。如果存在，就使用这个本地包进行编译；如果不存在，就会报编译错误 如果你没有显式设置 GOPATH 环境变量，Go 会将 GOPATH 设置为默认值，不同操作系统下默认值的路径不同 可以通过 go get 命令将本地缺失的第三方依赖包下载到本地。不仅能将包下载到 GOPATH 环境变量配置的目录下，它还会检查该包的依赖包在本地是否存在，如果不存在，go get 也会一并将它们下载到本地。但是，go get只能得到最新主线版本的依赖包，不能保证Reproduceable Build 总之，在 GOPATH 构建模式下，Go 编译器实质上并没有关注 Go 项目所依赖的第三方包的版本。从而有了Vendor机制控制依赖包版本。 Vendor机制 本质上就是在 Go 项目的某个特定目录下，将项目的所有依赖包缓存起来，这个特定目录名就是 vendor。 Go 编译器会优先感知和使用 vendor 目录下缓存的第三方包版本，而不是 GOPATH 环境变量所配置的路径下的第三方包版本。 目录示例： ├── main.go └── vendor/ ├── github.com/ │ └── sirupsen/ │ └── logrus/ └── golang.org/ └── x/ └── sys/ └── unix/ 开启 vendor 机制，你的 Go 项目必须位于 GOPATH 环境变量配置的某个路径的 src 目录下面 不足之处主要在于需要手工管理 vendor 下面的 Go 依赖包（Go 社区先后开发了诸如 gb、glide、dep 等工具，都是用来进行依赖分析管理的，自身却都存在某些问题）、庞大的vendor目录也得提交到代码仓库，以及项目路径的限制。 Go Module 如何创建？ go mod init go mod tidy go build go.sum是由 go mod 相关命令维护的一个文件，它存放了特定版本 module 内容的哈希值。这是 Go Module 的一个安全措施。当将来这里的某个 module 的特定版本被再次下载的时候，go 命令会使用 go.sum 文件中对应的哈希值，和新下载的内容的哈希值进行比对，只有哈希值比对一致才是合法的，这样可以确保你的项目所依赖的 module 内容，不会被恶意或意外篡改。 项目所依赖的包有很多版本，Go Module 是如何选出最适合的那个版本？ Go Module 的语义导入版本机制 go.mod 的 require 段中依赖的版本号，都符合 vX.Y.Z 的格式。语义版本号分成 3 部分：主版本号 (major)、次版本号 (minor) 和补丁版本号 (patch)。分别对应XYZ。 按照语义版本规范，主版本号不同的两个版本是相互不兼容的。而且，在主版本号相同的情况下，次版本号大都是向后兼容次版本号小的版本。补丁版本号也不影响兼容性 Go Module 规定：如果同一个包的新旧版本是兼容的，那么它们的包导入路径应该是相同的。 如果不兼容，Go Module 创新性地给出了一个方法：将包主版本号引入到包导入路径中，我们可以像下面这样导入 logrus v2.0.0 版本依赖包： import \"github.com/sirupsen/logrus/v2\" 关于主版本号为0时，按照语义版本规范的说法，v0.y.z 这样的版本号是用于项目初始开发阶段的版本号。在这个阶段任何事情都有可能发生，其 API 也不应该被认为是稳定的。Go Module 将这样的版本 (v0) 与主版本号 v1 做同等对待，也就是采用不带主版本号的包导入路径 总之，通过在包导入路径中引入主版本号的方式，来区别同一个包的不兼容版本。 最小版本选择原则 包依赖关系比较复杂时，一个包A的两个依赖包BC可能依赖于不同版本的某个包D，那A依赖于D的哪个版本？ Go 会在该项目依赖项的所有版本中，选出符合项目整体要求的“最小版本”。 明确具体版本下 Go Module 的实际表现行为还是比较重要的，方便应用时选择切换。 Go 各版本构建模式机制和切换： Go 1.11后一段时间GOPATH 构建模式与 Go Modules 构建模式各自独立工作，我们可以通过设置环境变量 GO111MODULE 的值在两种构建模式间切换。 Go 1.16 版本，Go Module 构建模式成为了默认模式。 目前我觉得GOPATH似乎可以抛弃了 Go Module常规操作： 为当前 module 添加一个依赖： go get 命令将我们新增的依赖包下载到了本地 module 缓存里，并在 go.mod 文件的 require 段中新增相关内容。 使用 go mod tidy 命令，在执行构建前自动分析源码中的依赖变化，识别新增依赖项并下载它们 两种方法都行，复杂的项目用go mod tidy/go get .（注意二者还是有区别的,看到后面你就知道了） 升级、降级依赖版本： 查询某个依赖包的版本 PS D:\\Blog\\JF-011101.github.io-blog\\content\\posts\u003e go list -m -versions github.com/sirupsen/logrus github.com/sirupsen/logrus v0.1.0 v0.1.1 v0.2.0 v0.3.0 v0.4.0 v0.4.1 v0.5.0 v0.5.1 v0.6.0 v0.6.1 v0.6.2 v0.6.3 v0.6.4 v0.6.5 v0.6.6 v0.7.0 v0.7.1 v0.7.2 v0.7.3 v0.8.0 v0.8.1 v0.8.2 v0.8.3 v0.8.4 v0.8.5 v0.8.6 v0.8.7 v0.9.0 v0.10.0 v0.11.0 v0.11.1 v0.11.2 v0.11.3 v0.11.4 v0.11.5 v1.0.0 v1.0.1 v1.0.3 v1.0.4 v1.0.5 v1.0.6 v1.1.0 v1.1.1 v1.2.0 v1.3.0 v1.4.0 v1.4.1 v1.4.2 v1.5.0 v1.6.0 v1.7.0 v1.7.1 v1.8.0 v1.8.1 //go mod tidy帮我们选择了v1.8.1 我们可以在项目的 module 根目录下，执行带有版本号的 go get 命令eg: go get github.com/sirupsen/logrus@v1.7.0 或者，用 go mod edit 命令，明确告知我们要依赖 v1.7.0 版本，而不是 v1.8.1。执行go mod edit -require=github.com/sirupsen/logrus@v1.7.0再go mod tidy。 添加一个主版本号大于 1 的依赖： 在声明它的导入路径的基础上，加上版本号信息 升级依赖版本到一个不兼容版本： 需要注意一点，可能需要移除对某个包的依赖 移除一个依赖 要想彻底从项目中移除 go.mod 中的依赖项，仅从源码中删除对依赖项的导入语句还不够。 还得用 go mod tidy 命令，将这个依赖项彻底从 Go Module 构建上下文中清除掉。go mod tidy 会自动分析源码依赖，而且将不再使用的依赖从 go.mod 和 go.sum 中移除。 特殊情况：借用Vendor Vendor机制其实可以作为Go Module的一个很好的补充。 在一些不方便访问外部网络，并且对 Go 应用构建性能敏感的环境，比如在一些内部的持续集成或持续交付环境（CI/CD）中，使用 vendor 机制可以实现与 Go Module 等价的构建。 Go 提供了可以快速建立和更新 vendor 的命令： go mod vendor make vendored copy of dependencies(将依赖复制到vendor下) 在 go build 后面加上 -mod=vendor 参数，可以快速基于 vendor 构建项目。 在 Go 1.14 及以后版本中，如果 Go 项目的顶层目录下存在 vendor 目录，那么 go build 默认也会优先基于 vendor 构建，除非你给 go build 传入 -mod=mod 的参数 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:21:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"参考 https://cloud.tencent.com/developer/article/1526095 https://www.topgoer.cn/docs/golang/chapter02 https://time.geekbang.org/column/article/429143 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:22:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Catalogue","Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 go_base_01 go起源 环境相关 主要特征 内置类型与函数 init \u0026 main以及Go包的初始化顺序 go命令 go env go run命令 go get go build命令 go install go clean命令 go doc命令 go test命令 go list命令 go fix go vet go tool pprof命令 go modules 运算符 下划线 格式占位符%…… 变量和常量 基本类型 string： 数组 切片 切片底层实现 创建切片 切片拷贝 指针 map map实现原理 go中map的实现原理 map与工厂模式 set 结构体 类型别名和自定义类型 结构体 方法和接收者 go项目的标准布局演进 go应用构建模式的演进 参考 go_base_02 if switch Type Switch select 基本使用 典型用法 for range Goto Break Continue go_base_03 函数定义 参数 不定参 返回值 匿名函数 闭包、递归 闭包 go递归函数 延迟调用（defer） defer陷阱 异常处理，错误处理 单元测试 go test工具 测试函数 测试组 子测试 测试覆盖率 基准测试 x性能比较函数 重置时间 并行测试 Setup与TearDown 示例函数 func ToUpper 压力测试 Go怎么写测试用例 如何编写测试用例 如何编写压力测试 小结 BDD go_base_04 方法定义 匿名字段 方法集 表达式 自定义error 抛异常和处理异常 系统抛 返回异常 自定义error go_base_05 匿名字段 接口 接口 类型与接口的关系 空接口 空接口的应用 类型断言 go_base_06 互联网协议介绍 互联网分层模型 socket编程 http编程 WebSocket编程 go_base_07 并发介绍 Goroutine runtime包 Channel Goroutine池 定时器 select 并发安全和锁 Sync 原子操作 GMP原理和调度 爬虫小案例 ","date":"2022-01-06 09:17:19","objectID":"/go_grammar_catalogue/:0:0","tags":["catalogue","go grammar"],"title":"Go_grammar_catalogue","uri":"/go_grammar_catalogue/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 并发编程 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:0:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"并发介绍 进程和线程 进程是程序在操作系统中的一次执行过程，系统进行资源分配和调度的一个独立单位。 线程是进程的一个执行实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位。 一个进程可以创建和撤销多个线程;同一个进程中的多个线程之间可以并发执行。 并发和并行 多线程程序在一个核的cpu上运行，就是并发。（一段时间内都有运行） 多线程程序在多个核的cpu上运行，就是并行。（同时运行） 协程和线程 协程：独立的栈空间，共享堆空间，调度由用户自己控制，本质上有点类似于用户级线程，这些用户级线程的调度也是自己实现的。 线程：一个线程上可以跑多个协程，协程是轻量级的线程。 goroutine 只是由官方实现的超级”线程池”。 每个实力4~5KB的栈内存占用和由于实现机制而大幅减少的创建和销毁开销是go高并发的根本原因。 并发不是并行： 并发主要由切换时间片来实现”同时”运行，并行则是直接利用多核实现多线程的运行，go可以设置使用核数，以发挥多核计算机的能力。 goroutine 奉行通过通信来共享内存，而不是共享内存来通信。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:1:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"Goroutine 在java/c++中我们要实现并发编程的时候，我们通常需要自己维护一个线程池，并且需要自己去包装一个又一个的任务，同时需要自己去调度线程执行任务并维护上下文切换，这一切通常会耗费程序员大量的心智。那么能不能有一种机制，程序员只需要定义很多个任务，让系统去帮助我们把这些任务分配到CPU上实现并发执行呢？ Go语言中的goroutine就是这样一种机制，goroutine的概念类似于线程，但 goroutine是由Go的运行时（runtime）调度和管理的。Go程序会智能地将 goroutine 中的任务合理地分配给每个CPU。Go语言之所以被称为现代化的编程语言，就是因为它在语言层面已经内置了调度和上下文切换的机制。 在Go语言编程中你不需要去自己写进程、线程、协程，你的技能包里只有一个技能–goroutine，当你需要让某个任务并发执行的时候，你只需要把这个任务包装成一个函数，开启一个goroutine去执行这个函数就可以了，就是这么简单粗暴。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:2:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"使用goroutine Go语言中使用goroutine非常简单，只需要在调用函数的时候在前面加上go关键字，就可以为一个函数创建一个goroutine。 一个goroutine必定对应一个函数，可以创建多个goroutine去执行相同的函数。 启动单个goroutine 启动goroutine的方式非常简单，只需要在调用的函数（普通函数和匿名函数）前面加上一个go关键字。 举个例子如下： func hello() { fmt.Println(\"Hello Goroutine!\") } func main() { hello() fmt.Println(\"main goroutine done!\") } 这个示例中hello函数和下面的语句是串行的，执行的结果是打印完Hello Goroutine!后打印main goroutine done!。 接下来我们在调用hello函数前面加上关键字go，也就是启动一个goroutine去执行hello这个函数。 func main() { go hello() // 启动另外一个goroutine去执行hello函数 fmt.Println(\"main goroutine done!\") } 这一次的执行结果只打印了main goroutine done!，并没有打印Hello Goroutine!。为什么呢？ 在程序启动时，Go程序就会为main()函数创建一个默认的goroutine。 当main()函数返回的时候该goroutine就结束了，所有在main()函数中启动的goroutine会一同结束，main函数所在的goroutine就像是权利的游戏中的夜王，其他的goroutine都是异鬼，夜王一死它转化的那些异鬼也就全部GG了。 所以我们要想办法让main函数等一等hello函数，最简单粗暴的方式就是time.Sleep了。 func main() { go hello() // 启动另外一个goroutine去执行hello函数 fmt.Println(\"main goroutine done!\") time.Sleep(time.Second) } 执行上面的代码你会发现，这一次先打印main goroutine done!，然后紧接着打印Hello Goroutine!。 首先为什么会先打印main goroutine done!是因为我们在创建新的goroutine的时候需要花费一些时间，而此时main函数所在的goroutine是继续执行的。 启动多个goroutine 在Go语言中实现并发就是这样简单，我们还可以启动多个goroutine。让我们再来一个例子： （这里使用了sync.WaitGroup来实现goroutine的同步） var wg sync.WaitGroup func hello(i int) { defer wg.Done() // goroutine结束就登记-1 fmt.Println(\"Hello Goroutine!\", i) } func main() { for i := 0; i \u003c 10; i++ { wg.Add(1) // 启动一个goroutine就登记+1 go hello(i) } wg.Wait() // 等待所有登记的goroutine都结束 } 多次执行上面的代码，会发现每次打印的数字的顺序都不一致。这是因为10个goroutine是并发执行的，而goroutine的调度是随机的。 注意 如果主协程退出了，其他任务还执行吗（运行下面的代码测试一下吧） package main import ( \"fmt\" \"time\" ) func main() { // 合起来写 go func() { i := 0 for { i++ fmt.Printf(\"new goroutine: i = %d\\n\", i) time.Sleep(time.Second) } }() i := 0 for { i++ fmt.Printf(\"main goroutine: i = %d\\n\", i) time.Sleep(time.Second) if i == 2 { break } } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:2:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"goroutine与线程 可增长的栈 OS线程（操作系统线程）一般都有固定的栈内存（通常为2MB）,一个goroutine的栈在其生命周期开始时只有很小的栈（典型情况下2KB），goroutine的栈不是固定的，他可以按需增大和缩小，goroutine的栈大小限制可以达到1GB，虽然极少会用到这么大。所以在Go语言中一次创建十万左右的goroutine也是可以的。 goroutine调度 GPM是Go语言运行时（runtime）层面的实现，是go语言自己实现的一套调度系统。区别于操作系统调度OS线程。 G很好理解，就是个goroutine的，里面除了存放本goroutine信息外 还有与所在P的绑定等信息。 P是go语言实现的协程处理器，管理着一组goroutine队列，P里面会存储当前goroutine运行的上下文环境（函数指针，堆栈地址及地址边界），P会对自己管理的goroutine队列做一些调度（比如把占用CPU时间较长的goroutine暂停，或者阻塞的协程进行跳过（有一个守护线程会记录每个processor完成的协程的数量，如果有个P完成的协程的数量一直不变，说明被阻塞了，守护线程会往这个协程的用户栈里插入一个特殊的标记，当协程运行内敛函数时会读到这个标记，会把自己中断下来，然后查找等候协程队列的队尾，然后切换成别的线程进一步运行）、运行后续的goroutine等等）当自己的队列消费完了就去全局队列里取，如果全局队列里也消费完了会去其他P的队列里抢任务。 另一个提高整个并发能力的机制：当某一个协程被系统中断了，比如一些IO操作，需要等待的时候，为了提高整体的并发，processor会把自己移到另一个可使用的系统进程当中，继续执行它所挂的队列里的其他的协程，当被中断的协程被唤醒，会把自己加入到某一个pocessor的协程等待队列里，或者加入到全局等待队列当中。需要注意的一点：当一个协程被中断的时候，其在寄存器里的运行状态也会保存到这个协程对象里，当协程再次获得运行的机会，这些又会重新写入寄存器继续运行。 go的协程机制与系统线程的这种多对多的关系以及它是如何来高效地利用系统线程来尽量多的运行并发的协程任务的。 M（machine）是Go运行时（runtime）对操作系统内核线程的虚拟， M与内核线程一般是一一映射的关系，也可以是多对一， 一个groutine最终是要放到M上执行的； P与M一般也是一一对应的。他们关系是： P管理着一组G挂载在M上运行。当一个G长久阻塞在一个M上时，runtime会新建一个M，阻塞G所在的P会把其他的G 挂载在新建的M上。当旧的G阻塞完成或者认为其已经死掉时回收旧的M。 P的个数是通过runtime.GOMAXPROCS设定（最大256），Go1.5版本之后默认为物理线程数。 在并发量大的时候会增加一些P和M，但不会太多，切换太频繁的话得不偿失。 单从线程调度讲，Go语言相比起其他语言的优势在于OS线程是由OS内核来调度的，goroutine则是由Go运行时（runtime）自己的调度器调度的，这个调度器使用一个称为m:n调度的技术（复用/调度m个goroutine到n个OS线程）。 其一大特点是goroutine的调度是在用户态下完成的， 不涉及内核态与用户态之间的频繁切换，包括内存的分配与释放，都是在用户态维护着一块大的内存池， 不直接调用系统的malloc函数（除非内存池需要改变），成本比调度OS线程低很多。 另一方面充分利用了多核的硬件资源，近似的把若干goroutine均分在物理线程上， 再加上本身goroutine的超轻量，以上种种保证了go调度方面的性能。 kernel space entity 系统线程或者说内核对象，内核对象切换的消耗很大。 多个协程对应于一个内核线程时，这些协程切换时消耗较少。 package groutine_test import ( \"fmt\" \"testing\" \"time\" ) func TestGroutine(t *testing.T) { for i := 0; i \u003c 10; i++ { go func(i int) { time.Sleep(time.Second * 1) fmt.Println(i) }(i) } time.Sleep(time.Millisecond * 50) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:2:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"runtime包 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:3:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"runtime.Gosched() 让出CPU时间片，重新等待安排任务(大概意思就是本来计划的好好的周末出去烧烤，但是你妈让你去相亲,两种情况第一就是你相亲速度非常快，见面就黄不耽误你继续烧烤，第二种情况就是你相亲速度特别慢，见面就是你侬我侬的，耽误了烧烤，但是还馋就是耽误了烧烤你还得去烧烤) package main import ( \"fmt\" \"runtime\" ) func main() { go func(s string) { for i := 0; i \u003c 2; i++ { fmt.Println(s) } }(\"world\") // 主协程 for i := 0; i \u003c 2; i++ { // 切一下，再次分配任务 runtime.Gosched() fmt.Println(\"hello\") } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:3:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"runtime.Goexit() 退出当前协程(一边烧烤一边相亲，突然发现相亲对象太丑影响烧烤，果断让她滚蛋，然后也就没有然后了) package main import ( \"fmt\" \"runtime\" ) func main() { go func() { defer fmt.Println(\"A.defer\") func() { defer fmt.Println(\"B.defer\") // 结束协程 runtime.Goexit() defer fmt.Println(\"C.defer\") fmt.Println(\"B\") }() fmt.Println(\"A\") }() for { } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:3:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"runtime.GOMAXPROCS Go运行时的调度器使用GOMAXPROCS参数来确定需要使用多少个OS线程来同时执行Go代码。默认值是机器上的CPU核心数。例如在一个8核心的机器上，调度器会把Go代码同时调度到8个OS线程上（GOMAXPROCS是m:n调度中的n）。 Go语言中可以通过runtime.GOMAXPROCS()函数设置当前程序并发时占用的CPU逻辑核心数。 Go1.5版本之前，默认使用的是单核心执行。Go1.5版本之后，默认使用全部的CPU逻辑核心数。 我们可以通过将任务分配到不同的CPU逻辑核心上实现并行的效果，这里举个例子： func a() { for i := 1; i \u003c 10; i++ { fmt.Println(\"A:\", i) } } func b() { for i := 1; i \u003c 10; i++ { fmt.Println(\"B:\", i) } } func main() { runtime.GOMAXPROCS(1) go a() go b() time.Sleep(time.Second) } 两个任务只有一个逻辑核心，此时是做完一个任务再做另一个任务。 将逻辑核心数设为2，此时两个任务并行执行，代码如下。 func a() { for i := 1; i \u003c 10; i++ { fmt.Println(\"A:\", i) } } func b() { for i := 1; i \u003c 10; i++ { fmt.Println(\"B:\", i) } } func main() { runtime.GOMAXPROCS(2) go a() go b() time.Sleep(time.Second) } go语言中的操作系统线程和goroutine的关系： 一个操作系统线程对应用户态多个goroutine。 go程序可以同时使用多个操作系统线程。 goroutine和OS线程是多对多的关系，即m:n。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:3:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"共享内存机制 使用锁来实现并发控制 package share_mem import ( \"sync\" \"testing\" \"time\" ) func TestCounter(t *testing.T) { counter := 0 for i := 0; i \u003c 5000; i++ { go func() { counter++ }() } time.Sleep(1 * time.Second) t.Logf(\"counter = %d\", counter) } //使用锁实现线程安全 func TestCounterThreadSafe(t *testing.T) { var mut sync.Mutex counter := 0 for i := 0; i \u003c 5000; i++ { go func() { defer func() { mut.Unlock() }() mut.Lock() counter++ }() } time.Sleep(1 * time.Second) t.Logf(\"counter = %d\", counter) } //是要弄WaitGroup实现 func TestCounterWaitGroup(t *testing.T) { var mut sync.Mutex var wg sync.WaitGroup counter := 0 for i := 0; i \u003c 5000; i++ { wg.Add(1) go func() { defer func() { mut.Unlock() }() mut.Lock() counter++ wg.Done() }() } wg.Wait() t.Logf(\"counter = %d\", counter) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:4:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"CSP 并发机制 communicating sequential processes通信顺序进程。 依赖于通道来完成两个通信实体之间的协调。 CSP VS Actor Model 和Actor的直接通讯不同，CSP模式则是通过Channel进⾏通讯的，更松耦合⼀些。 Go中channel是有容量限制并且独⽴于处理Groutine，⽽如Erlang，Actor模式中的mailbox容量是⽆限的，接收进程也总是被动地处理消息 可利用channel实现异步返回。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:5:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"Channel ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"channel 单纯地将函数并发执行是没有意义的。函数与函数间需要交换数据才能体现并发执行函数的意义。 虽然可以使用共享内存进行数据交换，但是共享内存在不同的goroutine中容易发生竞态问题。为了保证数据交换的正确性，必须使用互斥量对内存进行加锁，这种做法势必造成性能问题。 Go语言的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。 如果说goroutine是Go程序并发的执行体，channel就是它们之间的连接。channel是可以让一个goroutine发送特定值到另一个goroutine的通信机制。 Go 语言中的通道（channel）是一种特殊的类型。通道像一个传送带或者队列，总是遵循先入先出（First In First Out）的规则，保证收发数据的顺序。每一个通道都是一个具体类型的导管，也就是声明channel的时候需要为其指定元素类型。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"channel类型 channel是一种类型，一种引用类型。声明通道类型的格式如下： var 变量 chan 元素类型 举几个例子： var ch1 chan int // 声明一个传递整型的通道 var ch2 chan bool // 声明一个传递布尔型的通道 var ch3 chan []int // 声明一个传递int切片的通道 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"创建channel 通道是引用类型，通道类型的空值是nil。 var ch chan int fmt.Println(ch) // \u003cnil\u003e 声明的通道后需要使用make函数初始化之后才能使用。 创建channel的格式如下： make(chan 元素类型, [缓冲大小]) channel的缓冲大小是可选的。 举几个例子： ch4 := make(chan int) ch5 := make(chan bool) ch6 := make(chan []int) ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"channel操作 通道有发送（send）、接收(receive）和关闭（close）三种操作。 发送和接收都使用\u003c-符号。 现在我们先使用以下语句定义一个通道： ch := make(chan int) 发送 将一个值发送到通道中。 ch \u003c- 10 // 把10发送到ch中 接收 从一个通道中接收值。 x := \u003c- ch // 从ch中接收值并赋值给变量x \u003c-ch // 从ch中接收值，忽略结果 关闭 我们通过调用内置的close函数来关闭通道。 close(ch) 关于关闭通道需要注意的事情是，只有在通知接收方goroutine所有的数据都发送完毕的时候才需要关闭通道。通道是可以被垃圾回收机制回收的，它和关闭文件是不一样的，在结束操作之后关闭文件是必须要做的，但关闭通道不是必须的。 关闭后的通道有以下特点： 对一个关闭的通道再发送值就会导致panic。 对一个关闭的通道进行接收会一直获取值直到通道为空。 对一个关闭的并且没有值的通道执行接收操作会得到对应类型的零值。 关闭一个已经关闭的通道会导致panic。 v, ok \u003c-ch; ok 为 bool 值，true 表示正常接受，false 表示通道关闭 所有的 channel 接收者都会在 channel 关闭时，⽴刻从阻塞等待中返回且上 述 ok 值为 false。这个⼴播机制常被利⽤，进⾏向多个订阅者同时发送信号。如：退出信号 package channel_close import ( \"fmt\" \"sync\" \"testing\" ) func dataProducer(ch chan int, wg *sync.WaitGroup) { go func() { for i := 0; i \u003c 10; i++ { ch \u003c- i } close(ch) wg.Done() }() } func dataReceiver(ch chan int, wg *sync.WaitGroup) { go func() { for { if data, ok := \u003c-ch; ok { fmt.Println(data) } else { break } } wg.Done() }() } func TestCloseChannel(t *testing.T) { var wg sync.WaitGroup ch := make(chan int) wg.Add(1) dataProducer(ch, \u0026wg) wg.Add(1) dataReceiver(ch, \u0026wg) // wg.Add(1) // dataReceiver(ch, \u0026wg) wg.Wait() } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:4","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"任务的返回 package concurrency import ( \"fmt\" \"testing\" \"time\" ) func isCancelled(cancelChan chan struct{}) bool { select { case \u003c-cancelChan: return true default: return false } } func cancel_1(cancelChan chan struct{}) { cancelChan \u003c- struct{}{} } func cancel_2(cancelChan chan struct{}) { close(cancelChan) } func TestCancel(t *testing.T) { cancelChan := make(chan struct{}, 0) for i := 0; i \u003c 5; i++ { go func(i int, cancelCh chan struct{}) { for { if isCancelled(cancelCh) { break } time.Sleep(time.Millisecond * 5) } fmt.Println(i, \"Cancelled\") }(i, cancelChan) } cancel_2(cancelChan) time.Sleep(time.Second * 1) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:5","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"context与任务的取消 关联任务的取消。 go1.9以后，把context正式并入到golang正式的一个包里面了。 context： 根 Context：通过 context.Background () 创建 ⼦ Context：context.WithCancel(parentContext) 创建 ctx, cancel := context.WithCancel(context.Background()) 当前 Context 被取消时，基于他的⼦ context 都会被取消 接收取消通知 \u003c-ctx.Done() package cancel import ( \"context\" \"fmt\" \"testing\" \"time\" ) func isCancelled(ctx context.Context) bool { select { case \u003c-ctx.Done(): return true default: return false } } func TestCancel(t *testing.T) { ctx, cancel := context.WithCancel(context.Background()) for i := 0; i \u003c 5; i++ { go func(i int, ctx context.Context) { for { if isCancelled(ctx) { break } time.Sleep(time.Millisecond * 5) } fmt.Println(i, \"Cancelled\") }(i, ctx) } cancel() time.Sleep(time.Second * 1) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:6","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"无缓冲的通道 无缓冲的通道又称为阻塞的通道。我们来看一下下面的代码： func main() { ch := make(chan int) ch \u003c- 10 fmt.Println(\"发送成功\") } 上面这段代码能够通过编译，但是执行的时候会出现以下错误： fatal error: all goroutines are asleep - deadlock! goroutine 1 [chan send]: main.main() .../src/github.com/pprof/studygo/day06/channel02/main.go:8 +0x54 为什么会出现deadlock错误呢？ 因为我们使用ch := make(chan int)创建的是无缓冲的通道，无缓冲的通道只有在有人接收值的时候才能发送值。就像你住的小区没有快递柜和代收点，快递员给你打电话必须要把这个物品送到你的手中，简单来说就是无缓冲的通道必须有接收才能发送。 上面的代码会阻塞在ch \u003c- 10这一行代码形成死锁，那如何解决这个问题呢？ 一种方法是启用一个goroutine去接收值，例如： func recv(c chan int) { ret := \u003c-c fmt.Println(\"接收成功\", ret) } func main() { ch := make(chan int) go recv(ch) // 启用goroutine从通道接收值 ch \u003c- 10 fmt.Println(\"发送成功\") } 无缓冲通道上的发送操作会阻塞，直到另一个goroutine在该通道上执行接收操作，这时值才能发送成功，两个goroutine将继续执行。相反，如果接收操作先执行，接收方的goroutine将阻塞，直到另一个goroutine在该通道上发送一个值。 使用无缓冲通道进行通信将导致发送和接收的goroutine同步化。因此，无缓冲通道也被称为同步通道。 有缓冲的通道 解决上面问题的方法还有一种就是使用有缓冲区的通道。 我们可以在使用make函数初始化通道的时候为其指定通道的容量，例如： func main() { ch := make(chan int, 1) // 创建一个容量为1的有缓冲区通道 ch \u003c- 10 fmt.Println(\"发送成功\") } 只要通道的容量大于零，那么该通道就是有缓冲的通道，通道的容量表示通道中能存放元素的数量。就像你小区的快递柜只有那么个多格子，格子满了就装不下了，就阻塞了，等到别人取走一个快递员就能往里面放一个。 我们可以使用内置的len函数获取通道内元素的数量，使用cap函数获取通道的容量，虽然我们很少会这么做。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:7","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"close() 可以通过内置的close()函数关闭channel（如果你的管道不往里存值或者取值的时候一定记得关闭管道） package main import \"fmt\" func main() { c := make(chan int) go func() { for i := 0; i \u003c 5; i++ { c \u003c- i } close(c) }() for { if data, ok := \u003c-c; ok { fmt.Println(data) } else { break } } fmt.Println(\"main结束\") } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:8","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"如何优雅的从通道循环取值 当通过通道发送有限的数据时，我们可以通过close函数关闭通道来告知从该通道接收值的goroutine停止等待。当通道被关闭时，往该通道发送值会引发panic，从该通道里接收的值一直都是类型零值。那如何判断一个通道是否被关闭了呢？ 我们来看下面这个例子： // channel 练习 func main() { ch1 := make(chan int) ch2 := make(chan int) // 开启goroutine将0~100的数发送到ch1中 go func() { for i := 0; i \u003c 100; i++ { ch1 \u003c- i } close(ch1) }() // 开启goroutine从ch1中接收值，并将该值的平方发送到ch2中 go func() { for { i, ok := \u003c-ch1 // 通道关闭后再取值ok=false if !ok { break } ch2 \u003c- i * i } close(ch2) }() // 在主goroutine中从ch2中接收值打印 for i := range ch2 { // 通道关闭后会退出for range循环 fmt.Println(i) } } 从上面的例子中我们看到有两种方式在接收值的时候判断通道是否被关闭，我们通常使用的是for range的方式。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:9","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"单向通道 有的时候我们会将通道作为参数在多个任务函数间传递，很多时候我们在不同的任务函数中使用通道都会对其进行限制，比如限制通道在函数中只能发送或只能接收。 Go语言中提供了单向通道来处理这种情况。例如，我们把上面的例子改造如下： func counter(out chan\u003c- int) { for i := 0; i \u003c 100; i++ { out \u003c- i } close(out) } func squarer(out chan\u003c- int, in \u003c-chan int) { for i := range in { out \u003c- i * i } close(out) } func printer(in \u003c-chan int) { for i := range in { fmt.Println(i) } } func main() { ch1 := make(chan int) ch2 := make(chan int) go counter(ch1) go squarer(ch2, ch1) printer(ch2) } 其中， chan\u003c- int是一个只能发送的通道，可以发送但是不能接收； \u003c-chan int是一个只能接收的通道，可以接收但是不能发送。 在函数传参及任何赋值操作中将双向通道转换为单向通道是可以的，但反过来是不可以的。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:10","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"通道总结 channel常见的异常总结，如下图： 注意:关闭已经关闭的channel也会引发panic。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:11","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"用多路选择实现超时控制 多路选择select package select_test import ( \"fmt\" \"testing\" \"time\" ) func service() string { time.Sleep(time.Millisecond * 500) return \"Done\" } func AsyncService() chan string { retCh := make(chan string, 1) //retCh := make(chan string, 1) go func() { ret := service() fmt.Println(\"returned result.\") retCh \u003c- ret fmt.Println(\"service exited.\") }() return retCh } func TestSelect(t *testing.T) { select { case ret := \u003c-AsyncService(): t.Log(ret) case \u003c-time.After(time.Millisecond * 100): t.Error(\"time out\") } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:7:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"Goroutine池 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:8:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"worker pool（goroutine池） 本质上是生产者消费者模型 可以有效控制goroutine数量，防止暴涨 需求： 计算一个数字的各个位数之和，例如数字123，结果为1+2+3=6 随机生成数字进行计算 控制台输出结果如下： package main import ( \"fmt\" \"math/rand\" ) type Job struct { // id Id int // 需要计算的随机数 RandNum int } type Result struct { // 这里必须传对象实例 job *Job // 求和 sum int } func main() { // 需要2个管道 // 1.job管道 jobChan := make(chan *Job, 128) // 2.结果管道 resultChan := make(chan *Result, 128) // 3.创建工作池 createPool(64, jobChan, resultChan) // 4.开个打印的协程 go func(resultChan chan *Result) { // 遍历结果管道打印 for result := range resultChan { fmt.Printf(\"job id:%v randnum:%v result:%d\\n\", result.job.Id, result.job.RandNum, result.sum) } }(resultChan) var id int // 循环创建job，输入到管道 for { id++ // 生成随机数 r_num := rand.Int() job := \u0026Job{ Id: id, RandNum: r_num, } jobChan \u003c- job } } // 创建工作池 // 参数1：开几个协程 func createPool(num int, jobChan chan *Job, resultChan chan *Result) { // 根据开协程个数，去跑运行 for i := 0; i \u003c num; i++ { go func(jobChan chan *Job, resultChan chan *Result) { // 执行运算 // 遍历job管道所有数据，进行相加 for job := range jobChan { // 随机数接过来 r_num := job.RandNum // 随机数每一位相加 // 定义返回值 var sum int for r_num != 0 { tmp := r_num % 10 sum += tmp r_num /= 10 } // 想要的结果是Result r := \u0026Result{ job: job, sum: sum, } //运算结果扔到管道 resultChan \u003c- r } }(jobChan, resultChan) } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:8:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"定时器 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:9:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"定时器 Timer：时间到了，执行只执行1次 package main import ( \"fmt\" \"time\" ) func main() { // 1.timer基本使用 //timer1 := time.NewTimer(2 * time.Second) //t1 := time.Now() //fmt.Printf(\"t1:%v\\n\", t1) //t2 := \u003c-timer1.C //fmt.Printf(\"t2:%v\\n\", t2) // 2.验证timer只能响应1次 //timer2 := time.NewTimer(time.Second) //for { // \u003c-timer2.C // fmt.Println(\"时间到\") //} // 3.timer实现延时的功能 //(1) //time.Sleep(time.Second) //(2) //timer3 := time.NewTimer(2 * time.Second) //\u003c-timer3.C //fmt.Println(\"2秒到\") //(3) //\u003c-time.After(2*time.Second) //fmt.Println(\"2秒到\") // 4.停止定时器 //timer4 := time.NewTimer(2 * time.Second) //go func() { // \u003c-timer4.C // fmt.Println(\"定时器执行了\") //}() //b := timer4.Stop() //if b { // fmt.Println(\"timer4已经关闭\") //} // 5.重置定时器 timer5 := time.NewTimer(3 * time.Second) timer5.Reset(1 * time.Second) fmt.Println(time.Now()) fmt.Println(\u003c-timer5.C) for { } } Ticker：时间到了，多次执行 package main import ( \"fmt\" \"time\" ) func main() { // 1.获取ticker对象 ticker := time.NewTicker(1 * time.Second) i := 0 // 子协程 go func() { for { //\u003c-ticker.C i++ fmt.Println(\u003c-ticker.C) if i == 5 { //停止 ticker.Stop() } } }() for { } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:9:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"select ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:10:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"select多路复用 在某些场景下我们需要同时从多个通道接收数据。通道在接收数据时，如果没有数据可以接收将会发生阻塞。你也许会写出如下代码使用遍历的方式来实现： for{ // 尝试从ch1接收值 data, ok := \u003c-ch1 // 尝试从ch2接收值 data, ok := \u003c-ch2 … } 这种方式虽然可以实现从多个通道接收值的需求，但是运行性能会差很多。为了应对这种场景，Go内置了select关键字，可以同时响应多个通道的操作。 select的使用类似于switch语句，它有一系列case分支和一个默认的分支。每个case会对应一个通道的通信（接收或发送）过程。select会一直等待，直到某个case的通信操作完成时，就会执行case分支对应的语句。具体格式如下： select { case \u003c-chan1: // 如果chan1成功读到数据，则进行该case处理语句 case chan2 \u003c- 1: // 如果成功向chan2写入数据，则进行该case处理语句 default: // 如果上面都没有成功，则进入default处理流程 } select可以同时监听一个或多个channel，直到其中一个channel ready package main import ( \"fmt\" \"time\" ) func test1(ch chan string) { time.Sleep(time.Second * 5) ch \u003c- \"test1\" } func test2(ch chan string) { time.Sleep(time.Second * 2) ch \u003c- \"test2\" } func main() { // 2个管道 output1 := make(chan string) output2 := make(chan string) // 跑2个子协程，写数据 go test1(output1) go test2(output2) // 用select监控 select { case s1 := \u003c-output1: fmt.Println(\"s1=\", s1) case s2 := \u003c-output2: fmt.Println(\"s2=\", s2) } } 如果多个channel同时ready，则随机选择一个执行 package main import ( \"fmt\" ) func main() { // 创建2个管道 int_chan := make(chan int, 1) string_chan := make(chan string, 1) go func() { //time.Sleep(2 * time.Second) int_chan \u003c- 1 }() go func() { string_chan \u003c- \"hello\" }() select { case value := \u003c-int_chan: fmt.Println(\"int:\", value) case value := \u003c-string_chan: fmt.Println(\"string:\", value) } fmt.Println(\"main结束\") } 可以用于判断管道是否存满 package main import ( \"fmt\" \"time\" ) // 判断管道有没有存满 func main() { // 创建管道 output1 := make(chan string, 10) // 子协程写数据 go write(output1) // 取数据 for s := range output1 { fmt.Println(\"res:\", s) time.Sleep(time.Second) } } func write(ch chan string) { for { select { // 写数据 case ch \u003c- \"hello\": fmt.Println(\"write hello\") default: fmt.Println(\"channel full\") } time.Sleep(time.Millisecond * 500) } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:10:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"并发安全和锁 有时候在Go代码中可能会存在多个goroutine同时操作一个资源（临界区），这种情况会发生竞态问题（数据竞态）。类比现实生活中的例子有十字路口被各个方向的的汽车竞争；还有火车上的卫生间被车厢里的人竞争。 举个例子： var x int64 var wg sync.WaitGroup func add() { for i := 0; i \u003c 5000; i++ { x = x + 1 } wg.Done() } func main() { wg.Add(2) go add() go add() wg.Wait() fmt.Println(x) } 上面的代码中我们开启了两个goroutine去累加变量x的值，这两个goroutine在访问和修改x变量的时候就会存在数据竞争，导致最后的结果与期待的不符。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:11:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"互斥锁 互斥锁是一种常用的控制共享资源访问的方法，它能够保证同时只有一个goroutine可以访问共享资源。Go语言中使用sync包的Mutex类型来实现互斥锁。 使用互斥锁来修复上面代码的问题： var x int64 var wg sync.WaitGroup var lock sync.Mutex func add() { for i := 0; i \u003c 5000; i++ { lock.Lock() // 加锁 x = x + 1 lock.Unlock() // 解锁 } wg.Done() } func main() { wg.Add(2) go add() go add() wg.Wait() fmt.Println(x) } 使用互斥锁能够保证同一时间有且只有一个goroutine进入临界区，其他的goroutine则在等待锁；当互斥锁释放后，等待的goroutine才可以获取锁进入临界区，多个goroutine同时等待一个锁时，唤醒的策略是随机的。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:11:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"读写互斥锁 互斥锁是完全互斥的，但是有很多实际的场景下是读多写少的，当我们并发的去读取一个资源不涉及资源修改的时候是没有必要加锁的，这种场景下使用读写锁是更好的一种选择。读写锁在Go语言中使用sync包中的RWMutex类型。 读写锁分为两种：读锁和写锁。当一个goroutine获取读锁之后，其他的goroutine如果是获取读锁会继续获得锁，如果是获取写锁就会等待；当一个goroutine获取写锁之后，其他的goroutine无论是获取读锁还是写锁都会等待。 读写锁示例： var ( x int64 wg sync.WaitGroup lock sync.Mutex rwlock sync.RWMutex ) func write() { // lock.Lock() // 加互斥锁 rwlock.Lock() // 加写锁 x = x + 1 time.Sleep(10 * time.Millisecond) // 假设读操作耗时10毫秒 rwlock.Unlock() // 解写锁 // lock.Unlock() // 解互斥锁 wg.Done() } func read() { // lock.Lock() // 加互斥锁 rwlock.RLock() // 加读锁 time.Sleep(time.Millisecond) // 假设读操作耗时1毫秒 rwlock.RUnlock() // 解读锁 // lock.Unlock() // 解互斥锁 wg.Done() } func main() { start := time.Now() for i := 0; i \u003c 10; i++ { wg.Add(1) go write() } for i := 0; i \u003c 1000; i++ { wg.Add(1) go read() } wg.Wait() end := time.Now() fmt.Println(end.Sub(start)) } 需要注意的是读写锁非常适合读多写少的场景，如果读和写的操作差别不大，读写锁的优势就发挥不出来。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:11:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"Sync ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"sync.WaitGroup 在代码中生硬的使用time.Sleep肯定是不合适的，Go语言中可以使用sync.WaitGroup来实现并发任务的同步。 sync.WaitGroup有以下几个方法： 方法名 功能 (wg * WaitGroup) Add(delta int) 计数器+delta (wg *WaitGroup) Done() 计数器-1 (wg *WaitGroup) Wait() 阻塞直到计数器变为0 sync.WaitGroup内部维护着一个计数器，计数器的值可以增加和减少。例如当我们启动了N 个并发任务时，就将计数器值增加N。每个任务完成时通过调用Done()方法将计数器减1。通过调用Wait()来等待并发任务执行完，当计数器值为0时，表示所有并发任务已经完成。 我们利用sync.WaitGroup将上面的代码优化一下： var wg sync.WaitGroup func hello() { defer wg.Done() fmt.Println(\"Hello Goroutine!\") } func main() { wg.Add(1) go hello() // 启动另外一个goroutine去执行hello函数 fmt.Println(\"main goroutine done!\") wg.Wait() } 需要注意sync.WaitGroup是一个结构体，传递的时候要传递指针。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"sync.Once 说在前面的话：这是一个进阶知识点。 在编程的很多场景下我们需要确保某些操作在高并发的场景下只执行一次，例如只加载一次配置文件、只关闭一次通道等。 Go语言中的sync包中提供了一个针对只执行一次场景的解决方案–sync.Once。 sync.Once只有一个Do方法，其签名如下： func (o *Once) Do(f func()) {} 注意：如果要执行的函数f需要传递参数就需要搭配闭包来使用。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"加载配置文件示例 延迟一个开销很大的初始化操作到真正用到它的时候再执行是一个很好的实践。因为预先初始化一个变量（比如在init函数中完成初始化）会增加程序的启动耗时，而且有可能实际执行过程中这个变量没有用上，那么这个初始化操作就不是必须要做的。我们来看一个例子： var icons map[string]image.Image func loadIcons() { icons = map[string]image.Image{ \"left\": loadIcon(\"left.png\"), \"up\": loadIcon(\"up.png\"), \"right\": loadIcon(\"right.png\"), \"down\": loadIcon(\"down.png\"), } } // Icon 被多个goroutine调用时不是并发安全的 func Icon(name string) image.Image { if icons == nil { loadIcons() } return icons[name] } 多个goroutine并发调用Icon函数时不是并发安全的，现代的编译器和CPU可能会在保证每个goroutine都满足串行一致的基础上自由地重排访问内存的顺序。loadIcons函数可能会被重排为以下结果： func loadIcons() { icons = make(map[string]image.Image) icons[\"left\"] = loadIcon(\"left.png\") icons[\"up\"] = loadIcon(\"up.png\") icons[\"right\"] = loadIcon(\"right.png\") icons[\"down\"] = loadIcon(\"down.png\") } 在这种情况下就会出现即使判断了icons不是nil也不意味着变量初始化完成了。考虑到这种情况，我们能想到的办法就是添加互斥锁，保证初始化icons的时候不会被其他的goroutine操作，但是这样做又会引发性能问题。 使用sync.Once改造的示例代码如下： var icons map[string]image.Image var loadIconsOnce sync.Once func loadIcons() { icons = map[string]image.Image{ \"left\": loadIcon(\"left.png\"), \"up\": loadIcon(\"up.png\"), \"right\": loadIcon(\"right.png\"), \"down\": loadIcon(\"down.png\"), } } // Icon 是并发安全的 func Icon(name string) image.Image { loadIconsOnce.Do(loadIcons) return icons[name] } sync.Once其实内部包含一个互斥锁和一个布尔值，互斥锁保证布尔值和数据的安全，而布尔值用来记录初始化是否完成。这样设计就能保证初始化操作的时候是并发安全的并且初始化操作也不会被执行多次。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"sync.Map Go语言中内置的map不是并发安全的。请看下面的示例： var m = make(map[string]int) func get(key string) int { return m[key] } func set(key string, value int) { m[key] = value } func main() { wg := sync.WaitGroup{} for i := 0; i \u003c 20; i++ { wg.Add(1) go func(n int) { key := strconv.Itoa(n) set(key, n) fmt.Printf(\"k=:%v,v:=%v\\n\", key, get(key)) wg.Done() }(i) } wg.Wait() } 上面的代码开启少量几个goroutine的时候可能没什么问题，当并发多了之后执行上面的代码就会报fatal error: concurrent map writes错误。 像这种场景下就需要为map加锁来保证并发的安全性了，Go语言的sync包中提供了一个开箱即用的并发安全版map–sync.Map。开箱即用表示不用像内置的map一样使用make函数初始化就能直接使用。同时sync.Map内置了诸如Store、Load、LoadOrStore、Delete、Range等操作方法。 var m = sync.Map{} func main() { wg := sync.WaitGroup{} for i := 0; i \u003c 20; i++ { wg.Add(1) go func(n int) { key := strconv.Itoa(n) m.Store(key, n) value, _ := m.Load(key) fmt.Printf(\"k=:%v,v:=%v\\n\", key, value) wg.Done() }(i) } wg.Wait() } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:4","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"原子操作 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:13:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"原子操作 代码中的加锁操作因为涉及内核态的上下文切换会比较耗时、代价比较高。针对基本数据类型我们还可以使用原子操作来保证并发安全，因为原子操作是Go语言提供的方法它在用户态就可以完成，因此性能比加锁操作更好。Go语言中原子操作由内置的标准库sync/atomic提供。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:13:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"atomic包 方法 解释 func LoadInt32(addr int32) (val int32) func LoadInt64(addr int64) (val int64)\u003cbr\u003efunc LoadUint32(addruint32) (val uint32)\u003cbr\u003efunc LoadUint64(addruint64) (val uint64)\u003cbr\u003efunc LoadUintptr(addruintptr) (val uintptr)\u003cbr\u003efunc LoadPointer(addrunsafe.Pointer) (val unsafe.Pointer) 读取操作 func StoreInt32(addr *int32, val int32) func StoreInt64(addr *int64, val int64) func StoreUint32(addr *uint32, val uint32) func StoreUint64(addr *uint64, val uint64) func StoreUintptr(addr *uintptr, val uintptr) func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer) 写入操作 func AddInt32(addr *int32, delta int32) (new int32) func AddInt64(addr *int64, delta int64) (new int64) func AddUint32(addr *uint32, delta uint32) (new uint32) func AddUint64(addr *uint64, delta uint64) (new uint64) func AddUintptr(addr *uintptr, delta uintptr) (new uintptr) 修改操作 func SwapInt32(addr *int32, new int32) (old int32) func SwapInt64(addr *int64, new int64) (old int64) func SwapUint32(addr *uint32, new uint32) (old uint32) func SwapUint64(addr *uint64, new uint64) (old uint64) func SwapUintptr(addr *uintptr, new uintptr) (old uintptr) func SwapPointer(addr *unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer) 交换操作 func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) func CompareAndSwapInt64(addr *int64, old, new int64) (swapped bool) func CompareAndSwapUint32(addr *uint32, old, new uint32) (swapped bool) func CompareAndSwapUint64(addr *uint64, old, new uint64) (swapped bool) func CompareAndSwapUintptr(addr *uintptr, old, new uintptr) (swapped bool) func CompareAndSwapPointer(addr *unsafe.Pointer, old, new unsafe.Pointer) (swapped bool) 比较并交换操作 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:13:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"示例 我们填写一个示例来比较下互斥锁和原子操作的性能。 var x int64 var l sync.Mutex var wg sync.WaitGroup // 普通版加函数 func add() { // x = x + 1 x++ // 等价于上面的操作 wg.Done() } // 互斥锁版加函数 func mutexAdd() { l.Lock() x++ l.Unlock() wg.Done() } // 原子操作版加函数 func atomicAdd() { atomic.AddInt64(\u0026x, 1) wg.Done() } func main() { start := time.Now() for i := 0; i \u003c 10000; i++ { wg.Add(1) // go add() // 普通版add函数 不是并发安全的 // go mutexAdd() // 加锁版add函数 是并发安全的，但是加锁性能开销大 go atomicAdd() // 原子操作版add函数 是并发安全，性能优于加锁版 } wg.Wait() end := time.Now() fmt.Println(x) fmt.Println(end.Sub(start)) } atomic包提供了底层的原子级内存操作，对于同步算法的实现很有用。这些函数必须谨慎地保证正确使用。除了某些特殊的底层应用，使用通道或者sync包的函数/类型实现同步更好。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:13:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"GMP原理和调度 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"一、Golang “调度器” 的由来？ 单进程时代不需要调度器 我们知道，一切的软件都是跑在操作系统上，真正用来干活 (计算) 的是 CPU。早期的操作系统每个程序就是一个进程，知道一个程序运行完，才能进行下一个进程，就是 “单进程时代” 一切的程序只能串行发生。 早期的单进程操作系统，面临 2 个问题： 单一的执行流程，计算机只能一个任务一个任务处理。 进程阻塞所带来的 CPU 时间浪费。 那么能不能有多个进程来宏观一起来执行多个任务呢？ 后来操作系统就具有了最早的并发能力：多进程并发，当一个进程阻塞的时候，切换到另外等待执行的进程，这样就能尽量把 CPU 利用起来，CPU 就不浪费了。 多进程 / 线程时代有了调度器需求 在多进程 / 多线程的操作系统中，就解决了阻塞的问题，因为一个进程阻塞 cpu 可以立刻切换到其他进程中去执行，而且调度 cpu 的算法可以保证在运行的进程都可以被分配到 cpu 的运行时间片。这样从宏观来看，似乎多个进程是在同时被运行。 但新的问题就又出现了，进程拥有太多的资源，进程的创建、切换、销毁，都会占用很长的时间，CPU 虽然利用起来了，但如果进程过多，CPU 有很大的一部分都被用来进行进程调度了。 怎么才能提高 CPU 的利用率呢？ 但是对于 Linux 操作系统来讲，cpu 对进程的态度和线程的态度是一样的。 很明显，CPU 调度切换的是进程和线程。尽管线程看起来很美好，但实际上多线程开发设计会变得更加复杂，要考虑很多同步竞争等问题，如锁、竞争冲突等。 协程来提高 CPU 利用率 多进程、多线程已经提高了系统的并发能力，但是在当今互联网高并发场景下，为每个任务都创建一个线程是不现实的，因为会消耗大量的内存 (进程虚拟内存会占用 4GB [32 位操作系统], 而线程也要大约 4MB)。 大量的进程 / 线程出现了新的问题 高内存占用 调度的高消耗 CPU 好了，然后工程师们就发现，其实一个线程分为 “内核态 “线程和” 用户态 “线程。 一个 “用户态线程” 必须要绑定一个 “内核态线程”，但是 CPU 并不知道有 “用户态线程” 的存在，它只知道它运行的是一个 “内核态线程”(Linux 的 PCB 进程控制块)。 这样，我们再去细化去分类一下，内核线程依然叫 “线程 (thread)”，用户线程叫 “协程 (co-routine)”. 看到这里，我们就要开脑洞了，既然一个协程 (co-routine) 可以绑定一个线程 (thread)，那么能不能多个协程 (co-routine) 绑定一个或者多个线程 (thread) 上呢。 之后，我们就看到了有 3 中协程和线程的映射关系： s=\"default\"\u003e N:1 关系 N 个协程绑定 1 个线程，优点就是协程在用户态线程即完成切换，不会陷入到内核态，这种切换非常的轻量快速。但也有很大的缺点，1 个进程的所有协程都绑定在 1 个线程上 缺点： 某个程序用不了硬件的多核加速能力 一旦某协程阻塞，造成线程阻塞，本进程的其他协程都无法执行了，根本就没有并发的能力了。 s=\"default\"\u003e 1:1 关系 1 个协程绑定 1 个线程，这种最容易实现。协程的调度都由 CPU 完成了，不存在 N:1 缺点， 缺点： 协程的创建、删除和切换的代价都由 CPU 完成，有点略显昂贵了。 s=\"default\"\u003e M:N 关系 M 个协程绑定 1 个线程，是 N:1 和 1:1 类型的结合，克服了以上 2 种模型的缺点，但实现起来最为复杂。 协程跟线程是有区别的，线程由 CPU 调度是抢占式的，协程由用户态调度是协作式的，一个协程让出 CPU 后，才执行下一个协程。 Go 语言的协程 goroutine Go 为了提供更容易使用的并发方法，使用了 goroutine 和 channel。goroutine 来自协程的概念，让一组可复用的函数运行在一组线程之上，即使有协程阻塞，该线程的其他协程也可以被 runtime 调度，转移到其他可运行的线程上。最关键的是，程序员看不到这些底层的细节，这就降低了编程的难度，提供了更容易的并发。 Go 中，协程被称为 goroutine，它非常轻量，一个 goroutine 只占几 KB，并且这几 KB 就足够 goroutine 运行完，这就能在有限的内存空间内支持大量 goroutine，支持了更多的并发。虽然一个 goroutine 的栈只占几 KB，但实际是可伸缩的，如果需要更多内容，runtime 会自动为 goroutine 分配。 Goroutine 特点： 占用内存更小（几 kb） 调度更灵活 (runtime 调度) 被废弃的 goroutine 调度器 好了，既然我们知道了协程和线程的关系，那么最关键的一点就是调度协程的调度器的实现了。 Go 目前使用的调度器是 2012 年重新设计的，因为之前的调度器性能存在问题，所以使用 4 年就被废弃了，那么我们先来分析一下被废弃的调度器是如何运作的？ 大部分文章都是会用 G 来表示 Goroutine，用 M 来表示线程，那么我们也会用这种表达的对应关系。 下面我们来看看被废弃的 golang 调度器是如何实现的？ M 想要执行、放回 G 都必须访问全局 G 队列，并且 M 有多个，即多线程访问同一资源需要加锁进行保证互斥 / 同步，所以全局 G 队列是有互斥锁进行保护的。 老调度器有几个缺点： 创建、销毁、调度 G 都需要每个 M 获取锁，这就形成了激烈的锁竞争。 M 转移 G 会造成延迟和额外的系统负载。比如当 G 中包含创建新协程的时候，M 创建了 G’，为了继续执行 G，需要把 G’交给 M’执行，也造成了很差的局部性，因为 G’和 G 是相关的，最好放在 M 上执行，而不是其他 M’。 系统调用 (CPU 在 M 之间的切换) 导致频繁的线程阻塞和取消阻塞操作增加了系统开销。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"二、Goroutine 调度器的 GMP 模型的设计思想 面对之前调度器的问题，Go 设计了新的调度器。 在新调度器中，出列 M (thread) 和 G (goroutine)，又引进了 P (Processor)。 Processor，它包含了运行 goroutine 的资源，如果线程想运行 goroutine，必须先获取 P，P 中还包含了可运行的 G 队列。 GMP 模型 在 Go 中，线程是运行 goroutine 的实体，调度器的功能是把可运行的 goroutine 分配到工作线程上。 全局队列（Global Queue）：存放等待运行的 G。 P 的本地队列：同全局队列类似，存放的也是等待运行的 G，存的数量有限，不超过 256 个。新建 G’时，G’优先加入到 P 的本地队列，如果队列满了，则会把本地队列中一半的 G 移动到全局队列。 P 列表：所有的 P 都在程序启动时创建，并保存在数组中，最多有 GOMAXPROCS(可配置) 个。 M：线程想运行任务就得获取 P，从 P 的本地队列获取 G，P 队列为空时，M 也会尝试从全局队列拿一批 G 放到 P 的本地队列，或从其他 P 的本地队列偷一半放到自己 P 的本地队列。M 运行 G，G 执行之后，M 会从 P 获取下一个 G，不断重复下去。 Goroutine 调度器和 OS 调度器是通过 M 结合起来的，每个 M 都代表了 1 个内核线程，OS 调度器负责把内核线程分配到 CPU 的核上执行。 有关 P 和 M 的个数问题： P 的数量： 由启动时环境变量 $GOMAXPROCS 或者是由 runtime 的方法 GOMAXPROCS() 决定。这意味着在程序执行的任意时刻都只有 $GOMAXPROCS 个 goroutine 在同时运行。 M 的数量: go 语言本身的限制：go 程序启动时，会设置 M 的最大数量，默认 10000. 但是内核很难支持这么多的线程数，所以这个限制可以忽略。 runtime/debug 中的 SetMaxThreads 函数，设置 M 的最大数量 一个 M 阻塞了，会创建新的 M。 M 与 P 的数量没有绝对关系，一个 M 阻塞，P 就会去创建或者切换另一个 M，所以，即使 P 的默认数量是 1，也有可能会创建很多个 M 出来。 P 和 M 何时会被创建： P 何时创建：在确定了 P 的最大数量 n 后，运行时系统会根据这个数量创建 n 个 P。 M 何时创建：没有足够的 M 来关联 P 并运行其中的可运行的 G。比如所有的 M 此时都阻塞住了，而 P 中还有很多就绪任务，就会去寻找空闲的 M，而没有空闲的，就会去创建新的 M。 调度器的设计策略 复用线程：避免频繁的创建、销毁线程，而是对线程的复用。 1）work stealing 机制 当本线程无可运行的 G 时，尝试从其他线程绑定的 P 偷取 G，而不是销毁线程。 2）hand off 机制 当本线程因为 G 进行系统调用阻塞时，线程释放绑定的 P，把 P 转移给其他空闲的线程执行。 利用并行：GOMAXPROCS 设置 P 的数量，最多有 GOMAXPROCS 个线程分布在多个 CPU 上同时运行。GOMAXPROCS 也限制了并发的程度，比如 GOMAXPROCS = 核数/2，则最多利用了一半的 CPU 核进行并行。 抢占：在 coroutine 中要等待一个协程主动让出 CPU 才执行下一个协程，在 Go 中，一个 goroutine 最多占用 CPU 10ms，防止其他 goroutine 被饿死，这就是 goroutine 不同于 coroutine 的一个地方。 全局 G 队列：在新的调度器中依然有全局 G 队列，但功能已经被弱化了，当 M 执行 work stealing 从其他 P 偷不到 G 时，它可以从全局 G 队列获取 G。 go func () 调度流程 从上图我们可以分析出几个结论： 我们通过 go func () 来创建一个 goroutine； 有两个存储 G 的队列，一个是局部调度器 P 的本地队列、一个是全局 G 队列。新创建的 G 会先保存在 P 的本地队列中，如果 P 的本地队列已经满了就会保存在全局的队列中； G 只能运行在 M 中，一个 M 必须持有一个 P，M 与 P 是 1：1 的关系。M 会从 P 的本地队列弹出一个可执行状态的 G 来执行，如果 P 的本地队列为空，就会想其他的 MP 组合偷取一个可执行的 G 来执行； 一个 M 调度 G 执行的过程是一个循环机制； 当 M 执行某一个 G 时候如果发生了 syscall 或则其余阻塞操作，M 会阻塞，如果当前有一些 G 在执行，runtime 会把这个线程 M 从 P 中摘除 (detach)，然后再创建一个新的操作系统的线程 (如果有空闲的线程可用就复用空闲线程) 来服务于这个 P； 当 M 系统调用结束时候，这个 G 会尝试获取一个空闲的 P 执行，并放入到这个 P 的本地队列。如果获取不到 P，那么这个线程 M 变成休眠状态， 加入到空闲线程中，然后这个 G 会被放入全局队列中。 调度器的生命周期 特殊的 M0 和 G0 M0 M0 是启动程序后的编号为 0 的主线程，这个 M 对应的实例会在全局变量 runtime.m0 中，不需要在 heap 上分配，M0 负责执行初始化操作和启动第一个 G， 在之后 M0 就和其他的 M 一样了。 G0 G0 是每次启动一个 M 都会第一个创建的 goroutine，G0 仅用于负责调度的 G，G0 不指向任何可执行的函数，每个 M 都会有一个自己的 G0。在调度或系统调用时会使用 G0 的栈空间，全局变量的 G0 是 M0 的 G0。 我们来跟踪一段代码 package main import \"fmt\" func main() { fmt.Println(\"Hello world\") } 接下来我们来针对上面的代码对调度器里面的结构做一个分析。 也会经历如上图所示的过程： runtime 创建最初的线程 m0 和 goroutine g0，并把 2 者关联。 调度器初始化：初始化 m0、栈、垃圾回收，以及创建和初始化由 GOMAXPROCS 个 P 构成的 P 列表。 示例代码中的 main 函数是 main.main，runtime 中也有 1 个 main 函数 ——runtime.main，代码经过编译后，runtime.main 会调用 main.main，程序启动时会为 runtime.main 创建 goroutine，称它为 main goroutine 吧，然后把 main goroutine 加入到 P 的本地队列。 启动 m0，m0 已经绑定了 P，会从 P 的本地队列获取 G，获取到 main goroutine。 G 拥有栈，M 根据 G 中的栈信息和调度信息设置运行环境 M 运行 G G 退出，再次回到 M 获取可运行的 G，这样重复下去，直到 main.main 退出，runtime.main 执行 Defer 和 Panic 处理，或调用 runtime.exit 退出程序。 调度器的生命周期几乎占满了一个 Go 程序的一生，runtime.main 的 goroutine 执行之前都是为调度器做准备工作，runtime.main 的 goroutine 运行，才是调度器的真正开始，直到 runtime.main 结束而结束。 可视化 GMP 编程 有 2 种方式可以查看一个程序的 GMP 的数据。 方式 1：go tool trace trace 记录了运行时的信息，能提供可视化的 Web 页面。 简单测试代码：main 函数创建 trace，trace 会运行在单独的 goroutine 中，然后 main 打印”Hello World” 退出。 trace.go package main import ( \"os\" \"fmt\" \"runtime/trace\" ) func main() { //创建trace文件 f, err := os.Create(\"trace.out\") if err != nil { panic(err) } defer f.Close() //启动trace goroutine err = trace.Start(f) if err != nil { panic(err) } defer trace.Stop() //main fmt.Println(\"Hello World\") } 运行程序 $ go run trace.go Hello World 会得到一个 trace.out 文件，然后我们可以用一个工具打开，来分析这个文件。 $ go tool trace trace.out 2020/02/23 10:44:11 Parsing trace... 2020/02/23 10:44:11 Splitting trace... 2020/02/23 10:44:11 Opening browser. Trace viewer is listening on http://127.0.0.1:33479 我们可以通过浏览器打开 http://127.0.0.1:33479 网址，点击 view trace 能够看见可视化的调度流程。 G 信息 点击 Goroutines 那一行可视化的数据条，我们会看到一些详细的信息。 一共有两个G在程序中，一个是特殊的G0，是每个M必","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"三、Go 调度器调度场景过程全解析 场景 1 P 拥有 G1，M1 获取 P 后开始运行 G1，G1 使用 go func() 创建了 G2，为了局部性 G2 优先加入到 P1 的本地队列。 场景 2 G1 运行完成后 (函数：goexit)，M 上运行的 goroutine 切换为 G0，G0 负责调度时协程的切换（函数：schedule）。从 P 的本地队列取 G2，从 G0 切换到 G2，并开始运行 G2 (函数：execute)。实现了线程 M1 的复用。 场景 3 假设每个 P 的本地队列只能存 3 个 G。G2 要创建了 6 个 G，前 3 个 G（G3, G4, G5）已经加入 p1 的本地队列，p1 本地队列满了。 场景 4 G2 在创建 G7 的时候，发现 P1 的本地队列已满，需要执行负载均衡 (把 P1 中本地队列中前一半的 G，还有新创建 G 转移到全局队列) （实现中并不一定是新的 G，如果 G 是 G2 之后就执行的，会被保存在本地队列，利用某个老的 G 替换新 G 加入全局队列） 这些 G 被转移到全局队列时，会被打乱顺序。所以 G3,G4,G7 被转移到全局队列。 场景 5 G2 创建 G8 时，P1 的本地队列未满，所以 G8 会被加入到 P1 的本地队列。 G8 加入到 P1 点本地队列的原因还是因为 P1 此时在与 M1 绑定，而 G2 此时是 M1 在执行。所以 G2 创建的新的 G 会优先放置到自己的 M 绑定的 P 上。 场景 6 规定：在创建 G 时，运行的 G 会尝试唤醒其他空闲的 P 和 M 组合去执行。 假定 G2 唤醒了 M2，M2 绑定了 P2，并运行 G0，但 P2 本地队列没有 G，M2 此时为自旋线程（没有 G 但为运行状态的线程，不断寻找 G）。 (7) 场景 7 M2 尝试从全局队列 (简称 “GQ”) 取一批 G 放到 P2 的本地队列（函数：findrunnable()）。M2 从全局队列取的 G 数量符合下面的公式： n = min(len(GQ)/GOMAXPROCS + 1, len(GQ/2)) 至少从全局队列取 1 个 g，但每次不要从全局队列移动太多的 g 到 p 本地队列，给其他 p 留点。这是从全局队列到 P 本地队列的负载均衡。 假定我们场景中一共有 4 个 P（GOMAXPROCS 设置为 4，那么我们允许最多就能用 4 个 P 来供 M 使用）。所以 M2 只从能从全局队列取 1 个 G（即 G3）移动 P2 本地队列，然后完成从 G0 到 G3 的切换，运行 G3。 场景 8 假设 G2 一直在 M1 上运行，经过 2 轮后，M2 已经把 G7、G4 从全局队列获取到了 P2 的本地队列并完成运行，全局队列和 P2 的本地队列都空了，如场景 8 图的左半部分。 全局队列已经没有 G，那 m 就要执行 work stealing (偷取)：从其他有 G 的 P 哪里偷取一半 G 过来，放到自己的 P 本地队列。P2 从 P1 的本地队列尾部取一半的 G，本例中一半则只有 1 个 G8，放到 P2 的本地队列并执行。 场景 9 G1 本地队列 G5、G6 已经被其他 M 偷走并运行完成，当前 M1 和 M2 分别在运行 G2 和 G8，M3 和 M4 没有 goroutine 可以运行，M3 和 M4 处于自旋状态，它们不断寻找 goroutine。 为什么要让 m3 和 m4 自旋，自旋本质是在运行，线程在运行却没有执行 G，就变成了浪费 CPU. 为什么不销毁现场，来节约 CPU 资源。因为创建和销毁 CPU 也会浪费时间，我们希望当有新 goroutine 创建时，立刻能有 M 运行它，如果销毁再新建就增加了时延，降低了效率。当然也考虑了过多的自旋线程是浪费 CPU，所以系统中最多有 GOMAXPROCS 个自旋的线程 (当前例子中的 GOMAXPROCS=4，所以一共 4 个 P)，多余的没事做线程会让他们休眠。 场景 10 假定当前除了 M3 和 M4 为自旋线程，还有 M5 和 M6 为空闲的线程 (没有得到 P 的绑定，注意我们这里最多就只能够存在 4 个 P，所以 P 的数量应该永远是 M\u003e=P, 大部分都是 M 在抢占需要运行的 P)，G8 创建了 G9，G8 进行了阻塞的系统调用，M2 和 P2 立即解绑，P2 会执行以下判断：如果 P2 本地队列有 G、全局队列有 G 或有空闲的 M，P2 都会立马唤醒 1 个 M 和它绑定，否则 P2 则会加入到空闲 P 列表，等待 M 来获取可用的 p。本场景中，P2 本地队列有 G9，可以和其他空闲的线程 M5 绑定。 (11) 场景 11 G8 创建了 G9，假如 G8 进行了非阻塞系统调用。 M2 和 P2 会解绑，但 M2 会记住 P2，然后 G8 和 M2 进入系统调用状态。当 G8 和 M2 退出系统调用时，会尝试获取 P2，如果无法获取，则获取空闲的 P，如果依然没有，G8 会被记为可运行状态，并加入到全局队列，M2 因为没有 P 的绑定而变成休眠状态 (长时间休眠等待 GC 回收销毁)。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"四、小结 总结，Go 调度器很轻量也很简单，足以撑起 goroutine 的调度工作，并且让 Go 具有了原生（强大）并发的能力。Go 调度本质是把大量的 goroutine 分配到少量线程上去执行，并利用多核并行，实现更强大的并发。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:4","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"典型并发任务 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"只运行一次 在多线程的环境下，某一段代码只执行一次。 也就是单例模式。 在go语言里有个专门的方法 sync.Once package once_test import ( \"fmt\" \"sync\" \"testing\" \"unsafe\" ) type Singleton struct { data string } var singleInstance *Singleton var once sync.Once func GetSingletonObj() *Singleton { once.Do(func() { fmt.Println(\"Create Obj\") singleInstance = new(Singleton) }) return singleInstance } func TestGetSingletonObj(t *testing.T) { var wg sync.WaitGroup for i := 0; i \u003c 10; i++ { wg.Add(1) go func() { obj := GetSingletonObj() fmt.Printf(\"%X\\n\", unsafe.Pointer(obj)) wg.Done() }() } wg.Wait() } PS D:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch23\\singleton\u003e go test -v -run TestGetSingletonObj once_test.go === RUN TestGetSingletonObj Create Obj C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 --- PASS: TestGetSingletonObj (0.00s) PASS ok command-line-arguments 0.550s ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"仅需任意任务完成 eg:并行执行很多任务，当一个任务返回的时候就可以返回给用户了。像搜索引擎返回搜索结果。 go的CSP的并发控制机制能够简单快速地实现这样的模式。 package concurrency import ( \"fmt\" \"runtime\" \"testing\" \"time\" ) func runTask(id int) string { time.Sleep(10 * time.Millisecond) return fmt.Sprintf(\"The result is from %d\", id) } func FirstResponse() string { numOfRunner := 10 ch := make(chan string, numOfRunner) for i := 0; i \u003c numOfRunner; i++ { go func(i int) { ret := runTask(i) ch \u003c- ret }(i) } return \u003c-ch } func TestFirstResponse(t *testing.T) { t.Log(\"Before:\", runtime.NumGoroutine()) t.Log(FirstResponse()) time.Sleep(time.Second * 1) t.Log(\"After:\", runtime.NumGoroutine()) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"所有任务完成 sync package 里的WaitGroup即可实现。 另一种方式：在CSP模式下如何利用channel实现？ package util_all_done import ( \"fmt\" \"runtime\" \"testing\" \"time\" ) func runTask(id int) string { time.Sleep(10 * time.Millisecond) return fmt.Sprintf(\"The result is from %d\", id) } func AllResponse() string { numOfRunner := 10 ch := make(chan string, numOfRunner) for i := 0; i \u003c numOfRunner; i++ { go func(i int) { ret := runTask(i) ch \u003c- ret }(i) } finalRet := \"\" for j := 0; j \u003c numOfRunner; j++ { finalRet += \u003c-ch + \"\\n\" } return finalRet } func TestAllResponse(t *testing.T) { t.Log(\"Before:\", runtime.NumGoroutine()) t.Log(AllResponse()) time.Sleep(time.Second * 1) t.Log(\"After:\", runtime.NumGoroutine()) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"对象池 很多场景可能会遇到对象池，比如创建一些代价比较高的对象。数据库连接、网络连接。通常会将这些对象进行池化以避免重复创建 简单地可以使用buffered channel实现对象池。在select时需要有一个超时控制（高可用系统里有个金句：slow response比quick failure更糟糕） 当然可以用空接口来实现对象池里有不同类型的对象，但每次取出对象时需要断言来确认对象的类型。实际运用时建议不同类型用不同缓冲池。 package object_pool import ( \"errors\" \"time\" ) type ReusableObj struct { } type ObjPool struct { bufChan chan *ReusableObj //用于缓冲可重用对象 } func NewObjPool(numOfObj int) *ObjPool { objPool := ObjPool{} objPool.bufChan = make(chan *ReusableObj, numOfObj) for i := 0; i \u003c numOfObj; i++ { objPool.bufChan \u003c- \u0026ReusableObj{} } return \u0026objPool } func (p *ObjPool) GetObj(timeout time.Duration) (*ReusableObj, error) { select { case ret := \u003c-p.bufChan: return ret, nil case \u003c-time.After(timeout): //超时控制 return nil, errors.New(\"time out\") } } func (p *ObjPool) ReleaseObj(obj *ReusableObj) error { select { //channel被阻塞会立即返回default case p.bufChan \u003c- obj: return nil default: return errors.New(\"overflow\") } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:4","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"sync.pool对象缓存 注意与buffered channel区别。 sync.Pool对象获取： 尝试从私有对象获取 私有对象不存在，尝试从当前 Processor 的共享池获取 如果当前 Processor 共享池也是空的，那么就尝试去其他Processor 的共享池获取 如果所有⼦池都是空的，最后就⽤⽤户指定的 New 函数产⽣⼀个新的对象返回 对象放回： 如果私有对象不存在则保存为私有对象 如果私有对象存在，放⼊当前 Processor ⼦池的共享池中 …… pool := \u0026sync.Pool{ New: func() interface{} { return 0 }, } arry := pool.Get().(int) … pool.Put(10) 为什么sync.Pool不能拿来当对象池用？ sync.Pool对象的生命周期 GC 会清除 sync.pool 缓存的对象（GC是通过系统来调度的，没办法去干预，如果要长时间的去控制一个连接的生命周期就难以做到） 对象的缓存有效期为下⼀次GC 之前 package object_pool import ( \"fmt\" \"runtime\" \"sync\" \"testing\" ) func TestSyncPool(t *testing.T) { pool := \u0026sync.Pool{ New: func() interface{} { fmt.Println(\"Create a new object.\") return 100 }, } v := pool.Get().(int) fmt.Println(v) pool.Put(3) runtime.GC() //GC 会清除sync.pool中缓存的对象 v1, _ := pool.Get().(int) fmt.Println(v1) } func TestSyncPoolInMultiGroutine(t *testing.T) { pool := \u0026sync.Pool{ New: func() interface{} { fmt.Println(\"Create a new object.\") return 10 }, } pool.Put(100) pool.Put(100) pool.Put(100) var wg sync.WaitGroup for i := 0; i \u003c 10; i++ { wg.Add(1) go func(id int) { fmt.Println(pool.Get()) wg.Done() }(i) } wg.Wait() } sync.Pool总结： 适合于通过复⽤，降低复杂对象的创建和 GC 代价 协程安全，会有锁的开销 ⽣命周期受 GC 影响，不适合于做连接池等，需⾃⼰管理⽣命周期的资源的池化 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:5","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"爬虫小案例 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:16:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"爬虫步骤 明确目标（确定在哪个网站搜索） 爬（爬下内容） 取（筛选想要的） 处理数据（按照你的想法去处理） package main import ( \"fmt\" \"io/ioutil\" \"net/http\" \"regexp\" ) //这个只是一个简单的版本只是获取QQ邮箱并且没有进行封装操作，另外爬出来的数据也没有进行去重操作 var ( // \\d是数字 reQQEmail = `(\\d+)@qq.com` ) // 爬邮箱 func GetEmail() { // 1.去网站拿数据 resp, err := http.Get(\"https://tieba.baidu.com/p/6051076813?red_tag=1573533731\") HandleError(err, \"http.Get url\") defer resp.Body.Close() // 2.读取页面内容 pageBytes, err := ioutil.ReadAll(resp.Body) HandleError(err, \"ioutil.ReadAll\") // 字节转字符串 pageStr := string(pageBytes) //fmt.Println(pageStr) // 3.过滤数据，过滤qq邮箱 re := regexp.MustCompile(reQQEmail) // -1代表取全部 results := re.FindAllStringSubmatch(pageStr, -1) //fmt.Println(results) // 遍历结果 for _, result := range results { fmt.Println(\"email:\", result[0]) fmt.Println(\"qq:\", result[1]) } } // 处理异常 func HandleError(err error, why string) { if err != nil { fmt.Println(why, err) } } func main() { GetEmail() } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:16:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"正则表达式 文档：https://studygolang.com/pkgdoc API re := regexp.MustCompile(reStr)，传入正则表达式，得到正则表达式对象 ret := re.FindAllStringSubmatch(srcStr,-1)：用正则对象，获取页面页面，srcStr是页面内容，-1代表取全部 爬邮箱 方法抽取 爬超链接 爬手机号 http://www.zhaohaowang.com/ 如果连接失效了自己找一个有手机号的就好了 爬身份证号 http://henan.qq.com/a/20171107/069413.htm 如果连接失效了自己找一个就好了 爬图片链接 package main import ( \"fmt\" \"io/ioutil\" \"net/http\" \"regexp\" ) var ( // w代表大小写字母+数字+下划线 reEmail = `\\w+@\\w+\\.\\w+` // s?有或者没有s // +代表出1次或多次 //\\s\\S各种字符 // +?代表贪婪模式 reLinke = `href=\"(https?://[\\s\\S]+?)\"` rePhone = `1[3456789]\\d\\s?\\d{4}\\s?\\d{4}` reIdcard = `[123456789]\\d{5}((19\\d{2})|(20[01]\\d))((0[1-9])|(1[012]))((0[1-9])|([12]\\d)|(3[01]))\\d{3}[\\dXx]` reImg = `https?://[^\"]+?(\\.((jpg)|(png)|(jpeg)|(gif)|(bmp)))` ) // 处理异常 func HandleError(err error, why string) { if err != nil { fmt.Println(why, err) } } func GetEmail2(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reEmail) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result) } } // 抽取根据url获取内容 func GetPageStr(url string) (pageStr string) { resp, err := http.Get(url) HandleError(err, \"http.Get url\") defer resp.Body.Close() // 2.读取页面内容 pageBytes, err := ioutil.ReadAll(resp.Body) HandleError(err, \"ioutil.ReadAll\") // 字节转字符串 pageStr = string(pageBytes) return pageStr } func main() { // 2.抽取的爬邮箱 // GetEmail2(\"https://tieba.baidu.com/p/6051076813?red_tag=1573533731\") // 3.爬链接 //GetLink(\"http://www.baidu.com/s?wd=%E8%B4%B4%E5%90%A7%20%E7%95%99%E4%B8%8B%E9%82%AE%E7%AE%B1\u0026rsv_spt=1\u0026rsv_iqid=0x98ace53400003985\u0026issp=1\u0026f=8\u0026rsv_bp=1\u0026rsv_idx=2\u0026ie=utf-8\u0026tn=baiduhome_pg\u0026rsv_enter=1\u0026rsv_dl=ib\u0026rsv_sug2=0\u0026inputT=5197\u0026rsv_sug4=6345\") // 4.爬手机号 //GetPhone(\"https://www.zhaohaowang.com/\") // 5.爬身份证号 //GetIdCard(\"https://henan.qq.com/a/20171107/069413.htm\") // 6.爬图片 // GetImg(\"http://image.baidu.com/search/index?tn=baiduimage\u0026ps=1\u0026ct=201326592\u0026lm=-1\u0026cl=2\u0026nc=1\u0026ie=utf-8\u0026word=%E7%BE%8E%E5%A5%B3\") } func GetIdCard(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reIdcard) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result) } } // 爬链接 func GetLink(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reLinke) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result[1]) } } //爬手机号 func GetPhone(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(rePhone) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result) } } func GetImg(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reImg) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result[0]) } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:16:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"并发爬取美图 下面的两个是即将要爬的网站，如果网址失效自己换一个就好了 https://www.bizhizu.cn/shouji/tag-%E5%8F%AF%E7%88%B1/1.html package main import ( \"fmt\" \"io/ioutil\" \"net/http\" \"regexp\" \"strconv\" \"strings\" \"sync\" \"time\" ) func HandleError(err error, why string) { if err != nil { fmt.Println(why, err) } } // 下载图片，传入的是图片叫什么 func DownloadFile(url string, filename string) (ok bool) { resp, err := http.Get(url) HandleError(err, \"http.get.url\") defer resp.Body.Close() bytes, err := ioutil.ReadAll(resp.Body) HandleError(err, \"resp.body\") filename = \"E:/topgoer.com/src/github.com/student/3.0/img/\" + filename // 写出数据 err = ioutil.WriteFile(filename, bytes, 0666) if err != nil { return false } else { return true } } // 并发爬思路： // 1.初始化数据管道 // 2.爬虫写出：26个协程向管道中添加图片链接 // 3.任务统计协程：检查26个任务是否都完成，完成则关闭数据管道 // 4.下载协程：从管道里读取链接并下载 var ( // 存放图片链接的数据管道 chanImageUrls chan string waitGroup sync.WaitGroup // 用于监控协程 chanTask chan string reImg = `https?://[^\"]+?(\\.((jpg)|(png)|(jpeg)|(gif)|(bmp)))` ) func main() { // myTest() // DownloadFile(\"http://i1.shaodiyejin.com/uploads/tu/201909/10242/e5794daf58_4.jpg\", \"1.jpg\") // 1.初始化管道 chanImageUrls = make(chan string, 1000000) chanTask = make(chan string, 26) // 2.爬虫协程 for i := 1; i \u003c 27; i++ { waitGroup.Add(1) go getImgUrls(\"https://www.bizhizu.cn/shouji/tag-%E5%8F%AF%E7%88%B1/\" + strconv.Itoa(i) + \".html\") } // 3.任务统计协程，统计26个任务是否都完成，完成则关闭管道 waitGroup.Add(1) go CheckOK() // 4.下载协程：从管道中读取链接并下载 for i := 0; i \u003c 5; i++ { waitGroup.Add(1) go DownloadImg() } waitGroup.Wait() } // 下载图片 func DownloadImg() { for url := range chanImageUrls { filename := GetFilenameFromUrl(url) ok := DownloadFile(url, filename) if ok { fmt.Printf(\"%s 下载成功\\n\", filename) } else { fmt.Printf(\"%s 下载失败\\n\", filename) } } waitGroup.Done() } // 截取url名字 func GetFilenameFromUrl(url string) (filename string) { // 返回最后一个/的位置 lastIndex := strings.LastIndex(url, \"/\") // 切出来 filename = url[lastIndex+1:] // 时间戳解决重名 timePrefix := strconv.Itoa(int(time.Now().UnixNano())) filename = timePrefix + \"_\" + filename return } // 任务统计协程 func CheckOK() { var count int for { url := \u003c-chanTask fmt.Printf(\"%s 完成了爬取任务\\n\", url) count++ if count == 26 { close(chanImageUrls) break } } waitGroup.Done() } // 爬图片链接到管道 // url是传的整页链接 func getImgUrls(url string) { urls := getImgs(url) // 遍历切片里所有链接，存入数据管道 for _, url := range urls { chanImageUrls \u003c- url } // 标识当前协程完成 // 每完成一个任务，写一条数据 // 用于监控协程知道已经完成了几个任务 chanTask \u003c- url waitGroup.Done() } // 获取当前页图片链接 func getImgs(url string) (urls []string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reImg) results := re.FindAllStringSubmatch(pageStr, -1) fmt.Printf(\"共找到%d条结果\\n\", len(results)) for _, result := range results { url := result[0] urls = append(urls, url) } return } // 抽取根据url获取内容 func GetPageStr(url string) (pageStr string) { resp, err := http.Get(url) HandleError(err, \"http.Get url\") defer resp.Body.Close() // 2.读取页面内容 pageBytes, err := ioutil.ReadAll(resp.Body) HandleError(err, \"ioutil.ReadAll\") // 字节转字符串 pageStr = string(pageBytes) return pageStr } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:16:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 网络编程 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:0:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"互联网协议介绍 互联网的核心是一系列协议，总称为”互联网协议”（Internet Protocol Suite），正是这一些协议规定了电脑如何连接和组网。我们理解了这些协议，就理解了互联网的原理。由于这些协议太过庞大和复杂，没有办法在这里一概而全，只能介绍一下我们日常开发中接触较多的几个协议。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:1:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"互联网分层模型 互联网的逻辑实现被分为好几层。每一层都有自己的功能，就像建筑物一样，每一层都靠下一层支持。用户接触到的只是最上面的那一层，根本不会感觉到下面的几层。要理解互联网就需要自下而上理解每一层的实现的功能。 如上图所示，互联网按照不同的模型划分会有不用的分层，但是不论按照什么模型去划分，越往上的层越靠近用户，越往下的层越靠近硬件。在软件开发中我们使用最多的是上图中将互联网划分为五个分层的模型。 接下来我们一层一层的自底向上介绍一下每一层。 物理层 我们的电脑要与外界互联网通信，需要先把电脑连接网络，我们可以用双绞线、光纤、无线电波等方式。这就叫做”实物理层”，它就是把电脑连接起来的物理手段。它主要规定了网络的一些电气特性，作用是负责传送0和1的电信号。 数据链路层 单纯的0和1没有任何意义，所以我们使用者会为其赋予一些特定的含义，规定解读电信号的方式：例如：多少个电信号算一组？每个信号位有何意义？这就是”数据链接层”的功能，它在”物理层”的上方，确定了物理层传输的0和1的分组方式及代表的意义。早期的时候，每家公司都有自己的电信号分组方式。逐渐地，一种叫做”以太网”（Ethernet）的协议，占据了主导地位。 以太网规定，一组电信号构成一个数据包，叫做”帧”（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。其中”标头”包含数据包的一些说明项，比如发送者、接受者、数据类型等等；”数据”则是数据包的具体内容。”标头”的长度，固定为18字节。”数据”的长度，最短为46字节，最长为1500字节。因此，整个”帧”最短为64字节，最长为1518字节。如果数据很长，就必须分割成多个帧进行发送。 那么，发送者和接受者是如何标识呢？以太网规定，连入网络的所有设备都必须具有”网卡”接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做MAC地址。每块网卡出厂的时候，都有一个全世界独一无二的MAC地址，长度是48个二进制位，通常用12个十六进制数表示。前6个十六进制数是厂商编号，后6个是该厂商的网卡流水号。有了MAC地址，就可以定位网卡和数据包的路径了。 我们会通过ARP协议来获取接受方的MAC地址，有了MAC地址之后，如何把数据准确的发送给接收方呢？其实这里以太网采用了一种很”原始”的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机都发送，让每台计算机读取这个包的”标头”，找到接收方的MAC地址，然后与自身的MAC地址相比较，如果两者相同，就接受这个包，做进一步处理，否则就丢弃这个包。这种发送方式就叫做”广播”（broadcasting）。 网络层 按照以太网协议的规则我们可以依靠MAC地址来向外发送数据。理论上依靠MAC地址，你电脑的网卡就可以找到身在世界另一个角落的某台电脑的网卡了，但是这种做法有一个重大缺陷就是以太网采用广播方式发送数据包，所有成员人手一”包”，不仅效率低，而且发送的数据只能局限在发送者所在的子网络。也就是说如果两台计算机不在同一个子网络，广播是传不过去的。这种设计是合理且必要的，因为如果互联网上每一台计算机都会收到互联网上收发的所有数据包，那是不现实的。 因此，必须找到一种方法区分哪些MAC地址属于同一个子网络，哪些不是。如果是同一个子网络，就采用广播方式发送，否则就采用”路由”方式发送。这就导致了”网络层”的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做”网络地址”，简称”网址”。 “网络层”出现以后，每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是网络管理员分配的。网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理MAC地址。 规定网络地址的协议，叫做IP协议。它所定义的地址，就被称为IP地址。目前，广泛采用的是IP协议第四版，简称IPv4。IPv4这个版本规定，网络地址由32个二进制位组成，我们通常习惯用分成四段的十进制数表示IP地址，从0.0.0.0一直到255.255.255.255。 根据IP协议发送的数据，就叫做IP数据包。IP数据包也分为”标头”和”数据”两个部分：”标头”部分主要包括版本、长度、IP地址等信息，”数据”部分则是IP数据包的具体内容。IP数据包的”标头”部分的长度为20到60字节，整个数据包的总长度最大为65535字节。 传输层 有了MAC地址和IP地址，我们已经可以在互联网上任意两台主机上建立通信。但问题是同一台主机上会有许多程序都需要用网络收发数据，比如QQ和浏览器这两个程序都需要连接互联网并收发数据，我们如何区分某个数据包到底是归哪个程序的呢？也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做”端口”（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。 “端口”是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。有了IP和端口我们就能实现唯一确定互联网上一个程序，进而实现网络间的程序通信。 我们必须在数据包中加入端口信息，这就需要新的协议。最简单的实现叫做UDP协议，它的格式几乎就是在数据前面，加上端口号。UDP数据包，也是由”标头”和”数据”两部分组成：”标头”部分主要定义了发出端口和接收端口，”数据”部分就是具体的内容。UDP数据包非常简单，”标头”部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。 UDP协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。为了解决这个问题，提高网络可靠性，TCP协议就诞生了。TCP协议能够确保数据不会遗失。它的缺点是过程复杂、实现困难、消耗较多的资源。TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。 应用层 应用程序收到”传输层”的数据，接下来就要对数据进行解包。由于互联网是开放架构，数据来源五花八门，必须事先规定好通信的数据格式，否则接收方根本无法获得真正发送的数据内容。”应用层”的作用就是规定应用程序使用的数据格式，例如我们TCP协议之上常见的Email、HTTP、FTP等协议，这些协议就组成了互联网协议的应用层。 如下图所示，发送方的HTTP数据经过互联网的传输过程中会依次添加各层协议的标头信息，接收方收到数据包之后再依次根据协议解包得到数据。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:1:1","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"socket编程 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"socket 图解 Socket是BSD UNIX的进程通信机制，通常也称作”套接字”，用于描述IP地址和端口，是一个通信链的句柄。Socket可以理解为TCP/IP网络的API，它定义了许多函数或例程，程序员可以用它们来开发TCP/IP网络上的应用程序。电脑上运行的应用程序通常通过”套接字”向网络发出请求或者应答网络请求。 socket图解 Socket是应用层与TCP/IP协议族通信的中间软件抽象层。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket后面，对用户来说只需要调用Socket规定的相关函数，让Socket去组织符合指定的协议数据然后进行通信。 Socket又称“套接字”，应用程序通常通过“套接字”向网络发出请求或者应答网络请求 常用的Socket类型有两种：流式Socket和数据报式Socket，流式是一种面向连接的Socket，针对于面向连接的TCP服务应用，数据报式Socket是一种无连接的Socket，针对于无连接的UDP服务应用 TCP：比较靠谱，面向连接，比较慢 UDP：不是太靠谱，比较快 举个例子：TCP就像货到付款的快递，送到家还必须见到你人才算一整套流程。UDP就像某快递快递柜一扔就走管你收到收不到，一般直播用UDP。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:1","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"TCP编程 Go语言实现TCP通信 TCP协议 TCP/IP(Transmission Control Protocol/Internet Protocol) 即传输控制协议/网际协议，是一种面向连接（连接导向）的、可靠的、基于字节流的传输层（Transport layer）通信协议，因为是面向连接的协议，数据像水流一样传输，会存在黏包问题。 TCP服务端 一个TCP服务端可以同时连接很多个客户端，例如世界各地的用户使用自己电脑上的浏览器访问淘宝网。因为Go语言中创建多个goroutine实现并发非常方便和高效，所以我们可以每建立一次链接就创建一个goroutine去处理。 TCP服务端程序的处理流程： 监听端口 接收客户端请求建立链接 创建goroutine处理链接。 我们使用Go语言的net包实现的TCP服务端代码如下： // tcp/server/main.go // TCP server端 // 处理函数 func process(conn net.Conn) { defer conn.Close() // 关闭连接 for { reader := bufio.NewReader(conn) var buf [128]byte n, err := reader.Read(buf[:]) // 读取数据 if err != nil { fmt.Println(\"read from client failed, err:\", err) break } recvStr := string(buf[:n]) fmt.Println(\"收到client端发来的数据：\", recvStr) conn.Write([]byte(recvStr)) // 发送数据 } } func main() { listen, err := net.Listen(\"tcp\", \"127.0.0.1:20000\") if err != nil { fmt.Println(\"listen failed, err:\", err) return } for { conn, err := listen.Accept() // 建立连接 if err != nil { fmt.Println(\"accept failed, err:\", err) continue } go process(conn) // 启动一个goroutine处理连接 } } 将上面的代码保存之后编译成server或server.exe可执行文件。 TCP客户端 一个TCP客户端进行TCP通信的流程如下： 建立与服务端的链接 进行数据收发 关闭链接 使用Go语言的net包实现的TCP客户端代码如下： // tcp/client/main.go // 客户端 func main() { conn, err := net.Dial(\"tcp\", \"127.0.0.1:20000\") if err != nil { fmt.Println(\"err :\", err) return } defer conn.Close() // 关闭连接 inputReader := bufio.NewReader(os.Stdin) for { input, _ := inputReader.ReadString('\\n') // 读取用户输入 inputInfo := strings.Trim(input, \"\\r\\n\") if strings.ToUpper(inputInfo) == \"Q\" { // 如果输入q就退出 return } _, err = conn.Write([]byte(inputInfo)) // 发送数据 if err != nil { return } buf := [512]byte{} n, err := conn.Read(buf[:]) if err != nil { fmt.Println(\"recv failed, err:\", err) return } fmt.Println(string(buf[:n])) } } 将上面的代码编译成client或client.exe可执行文件，先启动server端再启动client端，在client端输入任意内容回车之后就能够在server端看到client端发送的数据，从而实现TCP通信。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:2","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"UDP编程 Go语言实现UDP通信 UDP协议 UDP协议（User Datagram Protocol）中文名称是用户数据报协议，是OSI（Open System Interconnection，开放式系统互联）参考模型中一种无连接的传输层协议，不需要建立连接就能直接进行数据发送和接收，属于不可靠的、没有时序的通信，但是UDP协议的实时性比较好，通常用于视频直播相关领域。 UDP服务端 使用Go语言的net包实现的UDP服务端代码如下： // UDP/server/main.go // UDP server端 func main() { listen, err := net.ListenUDP(\"udp\", \u0026net.UDPAddr{ IP: net.IPv4(0, 0, 0, 0), Port: 30000, }) if err != nil { fmt.Println(\"listen failed, err:\", err) return } defer listen.Close() for { var data [1024]byte n, addr, err := listen.ReadFromUDP(data[:]) // 接收数据 if err != nil { fmt.Println(\"read udp failed, err:\", err) continue } fmt.Printf(\"data:%v addr:%v count:%v\\n\", string(data[:n]), addr, n) _, err = listen.WriteToUDP(data[:n], addr) // 发送数据 if err != nil { fmt.Println(\"write to udp failed, err:\", err) continue } } } UDP客户端 使用Go语言的net包实现的UDP客户端代码如下： // UDP 客户端 func main() { socket, err := net.DialUDP(\"udp\", nil, \u0026net.UDPAddr{ IP: net.IPv4(0, 0, 0, 0), Port: 30000, }) if err != nil { fmt.Println(\"连接服务端失败，err:\", err) return } defer socket.Close() sendData := []byte(\"Hello server\") _, err = socket.Write(sendData) // 发送数据 if err != nil { fmt.Println(\"发送数据失败，err:\", err) return } data := make([]byte, 4096) n, remoteAddr, err := socket.ReadFromUDP(data) // 接收数据 if err != nil { fmt.Println(\"接收数据失败，err:\", err) return } fmt.Printf(\"recv:%v addr:%v count:%v\\n\", string(data[:n]), remoteAddr, n) } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:3","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"TCP黏包 服务端代码如下： // socket_stick/server/main.go func process(conn net.Conn) { defer conn.Close() reader := bufio.NewReader(conn) var buf [1024]byte for { n, err := reader.Read(buf[:]) if err == io.EOF { break } if err != nil { fmt.Println(\"read from client failed, err:\", err) break } recvStr := string(buf[:n]) fmt.Println(\"收到client发来的数据：\", recvStr) } } func main() { listen, err := net.Listen(\"tcp\", \"127.0.0.1:30000\") if err != nil { fmt.Println(\"listen failed, err:\", err) return } defer listen.Close() for { conn, err := listen.Accept() if err != nil { fmt.Println(\"accept failed, err:\", err) continue } go process(conn) } } 客户端代码如下： // socket_stick/client/main.go func main() { conn, err := net.Dial(\"tcp\", \"127.0.0.1:30000\") if err != nil { fmt.Println(\"dial failed, err\", err) return } defer conn.Close() for i := 0; i \u003c 20; i++ { msg := `Hello, Hello. How are you?` conn.Write([]byte(msg)) } } 将上面的代码保存后，分别编译。先启动服务端再启动客户端，可以看到服务端输出结果如下： 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you? 客户端分10次发送的数据，在服务端并没有成功的输出10次，而是多条数据“粘”到了一起。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:4","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"为什么会出现粘包 主要原因就是tcp数据传递模式是流模式，在保持长连接的时候可以进行多次的收和发。 “粘包”可发生在发送端也可发生在接收端： 由Nagle算法造成的发送端的粘包：Nagle算法是一种改善网络传输效率的算法。简单来说就是当我们提交一段数据给TCP发送时，TCP并不立刻发送此段数据，而是等待一小段时间看看在等待期间是否还有要发送的数据，若有则会一次把这两段数据发送出去。 接收端接收不及时造成的接收端粘包：TCP会把接收到的数据存在自己的缓冲区中，然后通知应用层取数据。当应用层由于某些原因不能及时的把TCP的数据取出来，就会造成TCP缓冲区中存放了几段数据。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:5","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"解决办法 出现”粘包”的关键在于接收方不确定将要传输的数据包的大小，因此我们可以对数据包进行封包和拆包的操作。 封包：封包就是给一段数据加上包头，这样一来数据包就分为包头和包体两部分内容了(过滤非法包时封包会加入”包尾”内容)。包头部分的长度是固定的，并且它存储了包体的长度，根据包头长度固定以及包头中含有包体长度的变量就能正确的拆分出一个完整的数据包。 我们可以自己定义一个协议，比如数据包的前4个字节为包头，里面存储的是发送的数据的长度。 // socket_stick/proto/proto.go package proto import ( \"bufio\" \"bytes\" \"encoding/binary\" ) // Encode 将消息编码 func Encode(message string) ([]byte, error) { // 读取消息的长度，转换成int32类型（占4个字节） var length = int32(len(message)) var pkg = new(bytes.Buffer) // 写入消息头 err := binary.Write(pkg, binary.LittleEndian, length) if err != nil { return nil, err } // 写入消息实体 err = binary.Write(pkg, binary.LittleEndian, []byte(message)) if err != nil { return nil, err } return pkg.Bytes(), nil } // Decode 解码消息 func Decode(reader *bufio.Reader) (string, error) { // 读取消息的长度 lengthByte, _ := reader.Peek(4) // 读取前4个字节的数据 lengthBuff := bytes.NewBuffer(lengthByte) var length int32 err := binary.Read(lengthBuff, binary.LittleEndian, \u0026length) if err != nil { return \"\", err } // Buffered返回缓冲中现有的可读取的字节数。 if int32(reader.Buffered()) \u003c length+4 { return \"\", err } // 读取真正的消息数据 pack := make([]byte, int(4+length)) _, err = reader.Read(pack) if err != nil { return \"\", err } return string(pack[4:]), nil } 接下来在服务端和客户端分别使用上面定义的proto包的Decode和Encode函数处理数据。 服务端代码如下： // socket_stick/server2/main.go func process(conn net.Conn) { defer conn.Close() reader := bufio.NewReader(conn) for { msg, err := proto.Decode(reader) if err == io.EOF { return } if err != nil { fmt.Println(\"decode msg failed, err:\", err) return } fmt.Println(\"收到client发来的数据：\", msg) } } func main() { listen, err := net.Listen(\"tcp\", \"127.0.0.1:30000\") if err != nil { fmt.Println(\"listen failed, err:\", err) return } defer listen.Close() for { conn, err := listen.Accept() if err != nil { fmt.Println(\"accept failed, err:\", err) continue } go process(conn) } } 客户端代码如下： // socket_stick/client2/main.go func main() { conn, err := net.Dial(\"tcp\", \"127.0.0.1:30000\") if err != nil { fmt.Println(\"dial failed, err\", err) return } defer conn.Close() for i := 0; i \u003c 20; i++ { msg := `Hello, Hello. How are you?` data, err := proto.Encode(msg) if err != nil { fmt.Println(\"encode msg failed, err:\", err) return } conn.Write(data) } } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:6","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"http编程 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"web工作流程 Web服务器的工作原理可以简单地归纳为 客户机通过TCP/IP协议建立到服务器的TCP连接 客户端向服务器发送HTTP协议请求包，请求服务器里的资源文档 服务器向客户机发送HTTP协议应答包，如果请求的资源包含有动态语言的内容，那么服务器会调用动态语言的解释引擎负责处理“动态内容”，并将处理得到的数据返回给客户端 客户机与服务器断开。由客户端解释HTML文档，在客户端屏幕上渲染图形结果 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:1","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"HTTP协议 超文本传输协议(HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议，它详细规定了浏览器和万维网服务器之间互相通信的规则，通过因特网传送万维网文档的数据传送协议 HTTP协议通常承载于TCP协议之上 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:2","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"HTTP服务端 package main import ( \"fmt\" \"net/http\" ) func main() { //http://127.0.0.1:8000/go // 单独写回调函数 http.HandleFunc(\"/go\", myHandler) //http.HandleFunc(\"/ungo\",myHandler2 ) // addr：监听的地址 // handler：回调函数 http.ListenAndServe(\"127.0.0.1:8000\", nil) } // handler函数 func myHandler(w http.ResponseWriter, r *http.Request) { fmt.Println(r.RemoteAddr, \"连接成功\") // 请求方式：GET POST DELETE PUT UPDATE fmt.Println(\"method:\", r.Method) // /go fmt.Println(\"url:\", r.URL.Path) fmt.Println(\"header:\", r.Header) fmt.Println(\"body:\", r.Body) // 回复 w.Write([]byte(\"www.5lmh.com\")) } go的默认路由规则： URL 分为两种，末尾是 /：表示⼀个⼦树，后⾯可以跟其他⼦路径； 末尾不是 /，表示⼀个叶⼦，固定的路径 以/ 结尾的 URL 可以匹配它的任何⼦路径，⽐如 /images 会匹配 /images/cute-cat.jpg 它采⽤最⻓匹配原则，如果有多个匹配，⼀定采⽤匹配路径最⻓的那个进⾏处理 如果没有找到任何匹配项，会返回 404 错误 //Default Router func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler if handler == nil { handler = DefaultServeMux //使⽤缺省的Router } if req.RequestURI == \"*\" \u0026\u0026 req.Method == \"OPTIONS\" { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req) } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:3","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"HTTP客户端 package main import ( \"fmt\" \"io\" \"net/http\" ) func main() { //resp, _ := http.Get(\"http://www.baidu.com\") //fmt.Println(resp) resp, _ := http.Get(\"http://127.0.0.1:8000/go\") defer resp.Body.Close() // 200 OK fmt.Println(resp.Status) fmt.Println(resp.Header) buf := make([]byte, 1024) for { // 接收服务端信息 n, err := resp.Body.Read(buf) if err != nil \u0026\u0026 err != io.EOF { fmt.Println(err) return } else { fmt.Println(\"读取完毕\") res := string(buf[:n]) fmt.Println(res) break } } } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:4","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"构建RESTful服务 第三方的handler 更好的router //详情见https://github.com/julienschmidt/httprouter //handler多了一个参数 func Hello(w http.ResponseWriter, r *http.Request, ps httprouter.Params) { fmt.Fprintf(w, \"hello, %s!\\n\", ps.ByName(\"name\")) } func main() { router := httprouter.New() router.GET(\"/\", Index) router.GET(\"/hello/:name\", Hello) log.Fatal(http.ListenAndServe(\":8080\", router)) } RESTful程序设计很多时候会基于面向资源的架构（resource oriented architecture） package main import ( \"encoding/json\" \"fmt\" \"log\" \"net/http\" \"github.com/julienschmidt/httprouter\" ) type Employee struct { ID string `json:\"id\"` Name string `json:\"name\"` Age int `json:\"age\"` } var employeeDB map[string]*Employee func init() { employeeDB = map[string]*Employee{} employeeDB[\"Mike\"] = \u0026Employee{\"e-1\", \"Mike\", 35} employeeDB[\"Rose\"] = \u0026Employee{\"e-2\", \"Rose\", 45} } func Index(w http.ResponseWriter, r *http.Request, _ httprouter.Params) { fmt.Fprint(w, \"Welcome!\\n\") } func GetEmployeeByName(w http.ResponseWriter, r *http.Request, ps httprouter.Params) { qName := ps.ByName(\"name\") var ( ok bool info *Employee infoJson []byte err error ) if info, ok = employeeDB[qName]; !ok { w.Write([]byte(\"{\\\"error\\\":\\\"Not Found\\\"}\")) return } if infoJson, err = json.Marshal(info); err != nil { w.Write([]byte(fmt.Sprintf(\"{\\\"error\\\":,\\\"%s\\\"}\", err))) return } w.Write(infoJson) } func main() { router := httprouter.New() router.GET(\"/\", Index) router.GET(\"/employee/:name\", GetEmployeeByName) log.Fatal(http.ListenAndServe(\":8080\", router)) } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:5","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"WebSocket编程 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:4:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"webSocket是什么 WebSocket是一种在单个TCP连接上进行全双工通信的协议 WebSocket使得客户端和服务器之间的数据交换变得更加简单，允许服务端主动向客户端推送数据 在WebSocket API中，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输 需要安装第三方包： cmd中：go get -u -v github.com/gorilla/websocket ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:4:1","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"举个聊天室的小例子 在同一级目录下新建四个go文件connection.go|data.go|hub.go|server.go 运行 go run server.go hub.go data.go connection.go 运行之后执行local.html文件 server.go文件代码 package main import ( \"fmt\" \"net/http\" \"github.com/gorilla/mux\" ) func main() { router := mux.NewRouter() go h.run() router.HandleFunc(\"/ws\", myws) if err := http.ListenAndServe(\"127.0.0.1:8080\", router); err != nil { fmt.Println(\"err:\", err) } } hub.go文件代码 package main import \"encoding/json\" var h = hub{ c: make(map[*connection]bool), u: make(chan *connection), b: make(chan []byte), r: make(chan *connection), } type hub struct { c map[*connection]bool b chan []byte r chan *connection u chan *connection } func (h *hub) run() { for { select { case c := \u003c-h.r: h.c[c] = true c.data.Ip = c.ws.RemoteAddr().String() c.data.Type = \"handshake\" c.data.UserList = user_list data_b, _ := json.Marshal(c.data) c.sc \u003c- data_b case c := \u003c-h.u: if _, ok := h.c[c]; ok { delete(h.c, c) close(c.sc) } case data := \u003c-h.b: for c := range h.c { select { case c.sc \u003c- data: default: delete(h.c, c) close(c.sc) } } } } } data.go文件代码 package main type Data struct { Ip string `json:\"ip\"` User string `json:\"user\"` From string `json:\"from\"` Type string `json:\"type\"` Content string `json:\"content\"` UserList []string `json:\"user_list\"` } connection.go文件代码 package main import ( \"encoding/json\" \"fmt\" \"net/http\" \"github.com/gorilla/websocket\" ) type connection struct { ws *websocket.Conn sc chan []byte data *Data } var wu = \u0026websocket.Upgrader{ReadBufferSize: 512, WriteBufferSize: 512, CheckOrigin: func(r *http.Request) bool { return true } } func myws(w http.ResponseWriter, r *http.Request) { ws, err := wu.Upgrade(w, r, nil) if err != nil { return } c := \u0026connection{sc: make(chan []byte, 256), ws: ws, data: \u0026Data{} } h.r \u003c- c go c.writer() c.reader() defer func() { c.data.Type = \"logout\" user_list = del(user_list, c.data.User) c.data.UserList = user_list c.data.Content = c.data.User data_b, _ := json.Marshal(c.data) h.b \u003c- data_b h.r \u003c- c }() } func (c *connection) writer() { for message := range c.sc { c.ws.WriteMessage(websocket.TextMessage, message) } c.ws.Close() } var user_list = []string{} func (c *connection) reader() { for { _, message, err := c.ws.ReadMessage() if err != nil { h.r \u003c- c break } json.Unmarshal(message, \u0026c.data) switch c.data.Type { case \"login\": c.data.User = c.data.Content c.data.From = c.data.User user_list = append(user_list, c.data.User) c.data.UserList = user_list data_b, _ := json.Marshal(c.data) h.b \u003c- data_b case \"user\": c.data.Type = \"user\" data_b, _ := json.Marshal(c.data) h.b \u003c- data_b case \"logout\": c.data.Type = \"logout\" user_list = del(user_list, c.data.User) data_b, _ := json.Marshal(c.data) h.b \u003c- data_b h.r \u003c- c default: fmt.Print(\"========default================\") } } } func del(slice []string, user string) []string { count := len(slice) if count == 0 { return slice } if count == 1 \u0026\u0026 slice[0] == user { return []string{} } var n_slice = []string{} for i := range slice { if slice[i] == user \u0026\u0026 i == count { return slice[:count] } else if slice[i] == user { n_slice = append(slice[:i], slice[i+1:]...) break } } fmt.Println(n_slice) return n_slice } local.html文件代码 html \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003e\u003c/title\u003e \u003cmeta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\"\u003e \u003cstyle\u003e p { text-align: left; padding-left: 20px; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv style=\"width: 800px;height: 600px;margin: 30px auto;text-align: center\"\u003e \u003ch1\u003ewww.5lmh.comy演示聊天室\u003c/h1\u003e \u003cdiv style=\"width: 800px;border: 1px solid gray;height: 300px;\"\u003e \u003cdiv style=\"width: 200px;height: 300px;float: left;text-align: left;\"\u003e \u003cp\u003e\u003cspan\u003e当前在线:\u003c/span\u003e\u003cspan id=\"user_num\"\u003e0\u003c/span\u003e\u003c/p\u003e \u003cdiv id=\"user_list\" style=\"overflow: auto;\"\u003e \u003c/div\u003e \u003c/div\u003e \u003cdiv id=\"msg_list\" style=\"width: 598px;border: 1px solid gray; height: 300px;overflow: scroll;float: left;\"\u003e \u003c/div\u003e \u003c/div\u003e \u003cbr\u003e \u003ctextarea id=\"msg_box\" rows=\"6\" cols=\"50\" onkeydown=\"confirm(event)\"\u003e\u003c/textarea\u003e\u003cbr\u003e \u003cinput type=\"button\" value=\"发送\" onclick=\"send()\"\u003e \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:4:2","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 面向对象 Is Go an object-oriented language? Yes and no. Although Go has types and methods and allows an object-oriented style of programming, there is no type hierarchy. The concept of “interface” in Go provides a different approach that we believe is easy to use and in some ways more general. Also, the lack of a type hierarchy makes “objects” in Go feel much more lightweight than in languages such as C++ or Java. 个人认为官网的大致意思就是go算是面向对象语言，没有继承，但有更通用的“接口”，没有继承也使对象比某些语言更轻量。 ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:0:0","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"匿名字段 go支持只提供类型而不写字段名的方式，也就是匿名字段，也称为嵌入字段 package main import \"fmt\" //- go支持只提供类型而不写字段名的方式，也就是匿名字段，也称为嵌入字段 //人 type Person struct { name string sex string age int } type Student struct { Person id int addr string } func main() { // 初始化 s1 := Student{Person{\"5lmh\", \"man\", 20}, 1, \"bj\"} fmt.Println(s1) s2 := Student{Person: Person{\"5lmh\", \"man\", 20} } fmt.Println(s2) s3 := Student{Person: Person{name: \"5lmh\"} } fmt.Println(s3) } output： { {5lmh man 20} 1 bj} { {5lmh man 20} 0 } { {5lmh 0} 0 } 同名字段的情况 package main import \"fmt\" //人 type Person struct { name string sex string age int } type Student struct { Person id int addr string //同名字段 name string } func main() { var s Student // 给自己字段赋值了 s.name = \"5lmh\" fmt.Println(s) // 若给父类同名字段赋值，如下 s.Person.name = \"枯藤\" fmt.Println(s) } output： { { 0} 0 5lmh} { {枯藤 0} 0 5lmh} 所有的内置类型和自定义类型都是可以作为匿名字段去使用 package main import \"fmt\" //人 type Person struct { name string sex string age int } // 自定义类型 type mystr string // 学生 type Student struct { Person int mystr } func main() { s1 := Student{Person{\"5lmh\", \"man\", 18}, 1, \"bj\"} fmt.Println(s1) } output： { {5lmh man 18} 1 bj} 指针类型匿名字段 package main import \"fmt\" //人 type Person struct { name string sex string age int } // 学生 type Student struct { *Person id int addr string } func main() { s1 := Student{\u0026Person{\"5lmh\", \"man\", 18}, 1, \"bj\"} fmt.Println(s1) fmt.Println(s1.name) fmt.Println(s1.Person.name) } output： {0xc00006a360 1 bj} 5lmh 5lmh ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:1:0","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"接口 接口（interface）定义了一个对象的行为规范，只定义规范不实现，由具体的对象来实现规范的细节。（也可以说是定义对象之间交互的协议的） ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:0","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"接口 接口类型 在Go语言中接口（interface）是一种类型，一种抽象的类型。 interface是一组method的集合，是duck-type programming的一种体现。接口做的事情就像是定义一个协议（规则），只要一台机器有洗衣服和甩干的功能，我就称它为洗衣机。不关心属性（数据），只关心行为（方法）。 为什么要使用接口 type Cat struct{} func (c Cat) Say() string { return \"喵喵喵\" } type Dog struct{} func (d Dog) Say() string { return \"汪汪汪\" } func main() { c := Cat{} fmt.Println(\"猫:\", c.Say()) d := Dog{} fmt.Println(\"狗:\", d.Say()) } 上面的代码中定义了猫和狗，然后它们都会叫，你会发现main函数中明显有重复的代码，如果我们后续再加上猪、青蛙等动物的话，我们的代码还会一直重复下去。那我们能不能把它们当成“能叫的动物”来处理呢？ 像类似的例子在我们编程过程中会经常遇到： 比如一个网上商城可能使用支付宝、微信、银联等方式去在线支付，我们能不能把它们当成“支付方式”来处理呢？ 比如三角形，四边形，圆形都能计算周长和面积，我们能不能把它们当成“图形”来处理呢？ 比如销售、行政、程序员都能计算月薪，我们能不能把他们当成“员工”来处理呢？ Go语言中为了解决类似上面的问题，就设计了接口这个概念。接口区别于我们之前所有的具体类型，接口是一种抽象的类型。当你看到一个接口类型的值时，你不知道它是什么，唯一知道的是通过它的方法能做什么。 接口的定义： （Go语言提倡面向接口编程。） 接口是一个或多个方法签名的集合。 任何类型的方法集中只要拥有该接口'对应的全部方法'签名。 就表示它 \"实现\" 了该接口，无须在该类型上显式声明实现了哪个接口。 这称为Structural Typing。 所谓对应方法，是指有相同名称、参数列表 (不包括参数名) 以及返回值。 当然，该类型还可以有其他方法。 接口只有方法声明，没有实现，没有数据字段。 接口可以匿名嵌入其他接口，或嵌入到结构中。 对象赋值给接口时，会发生拷贝，而接口内部存储的是指向这个复制品的指针，既无法修改复制品的状态，也无法获取指针。 只有当接口存储的类型和对象都为nil时，接口才等于nil。 接口调用不会做receiver的自动转换。 接口同样支持匿名字段方法。 接口也可实现类似OOP中的多态。 空接口可以作为任何类型数据的容器。 一个类型可实现多个接口。 接口命名习惯以 er 结尾。 每个接口由数个方法组成，接口的定义格式如下： type 接口类型名 interface{ 方法名1( 参数列表1 ) 返回值列表1 方法名2( 参数列表2 ) 返回值列表2 … } 其中： 接口名：使用type将接口定义为自定义的类型名。Go语言的接口在命名时，一般会在单词后面添加er，如有写操作的接口叫Writer，有字符串功能的接口叫Stringer等。接口名最好要能突出该接口的类型含义。 方法名：当方法名首字母是大写且这个接口类型名首字母也是大写时，这个方法可以被接口所在的包（package）之外的代码访问。 参数列表、返回值列表：参数列表和返回值列表中的参数变量名可以省略。 举个例子： type writer interface{ Write([]byte) error } 当你看到这个接口类型的值时，你不知道它是什么，唯一知道的就是可以通过它的Write方法来做一些事情。 实现接口的条件 一个对象只要全部实现了接口中的方法，那么就实现了这个接口。换句话说，接口就是一个需要实现的方法列表。 我们来定义一个Sayer接口： // Sayer 接口 type Sayer interface { say() } 定义dog和cat两个结构体： type dog struct {} type cat struct {} 因为Sayer接口里只有一个say方法，所以我们只需要给dog和cat 分别实现say方法就可以实现Sayer接口了。 // dog实现了Sayer接口 func (d dog) say() { fmt.Println(\"汪汪汪\") } // cat实现了Sayer接口 func (c cat) say() { fmt.Println(\"喵喵喵\") } 接口的实现就是这么简单，只要实现了接口中的所有方法，就实现了这个接口。 接口类型变量 那实现了接口有什么用呢？ 接口类型变量能够存储所有实现了该接口的实例。 例如上面的示例中，Sayer类型的变量能够存储dog和cat类型的变量。 func main() { var x Sayer // 声明一个Sayer类型的变量x a := cat{} // 实例化一个cat b := dog{} // 实例化一个dog x = a // 可以把cat实例直接赋值给x x.say() // 喵喵喵 x = b // 可以把dog实例直接赋值给x x.say() // 汪汪汪 } 值接收者和指针接收者实现接口的区别 使用值接收者实现接口和使用指针接收者实现接口有什么区别呢？接下来我们通过一个例子看一下其中的区别。 我们有一个Mover接口和一个dog结构体。 type Mover interface { move() } type dog struct {} 值接收者实现接口 func (d dog) move() { fmt.Println(\"狗会动\") } 此时实现接口的是dog类型： func main() { var x Mover var wangcai = dog{} // 旺财是dog类型 x = wangcai // x可以接收dog类型 var fugui = \u0026dog{} // 富贵是*dog类型 x = fugui // x可以接收*dog类型 x.move() } 从上面的代码中我们可以发现，使用值接收者实现接口之后，不管是dog结构体还是结构体指针*dog类型的变量都可以赋值给该接口变量。因为Go语言中有对指针类型变量求值的语法糖，dog指针fugui内部会自动求值*fugui。 指针接收者实现接口 同样的代码我们再来测试一下使用指针接收者有什么区别： func (d *dog) move() { fmt.Println(\"狗会动\") } func main() { var x Mover var wangcai = dog{} // 旺财是dog类型 x = wangcai // x不可以接收dog类型 var fugui = \u0026dog{} // 富贵是*dog类型 x = fugui // x可以接收*dog类型 } 此时实现Mover接口的是*dog类型，所以不能给x传入dog类型的wangcai，此时x只能存储*dog类型的值。 用接口实现多态 接口的最佳实践 倾向于使⽤⼩的接⼝定义，很多接⼝只包含⼀个⽅法 较⼤的接⼝定义，可以由多个⼩接⼝定义组合⽽成 只依赖于必要功能的最小接口，这样方法的复用性更强。 下面的代码是一个比较好的面试题 请问下面的代码是否能通过编译？ type People interface { Speak(string) string } type Student struct{} func (stu *Student) Speak(think string) (talk string) { if think == \"sb\" { talk = \"你是个大帅比\" } else { talk = \"您好\" } return } func main() { var peo People = Student{} think := \"bitch\" fmt.Println(peo.Speak(think)) } ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:1","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"类型与接口的关系 一个类型实现多个接口 一个类型可以同时实现多个接口，而接口间彼此独立，不知道对方的实现。 例如，狗可以叫，也可以动。我们就分别定义Sayer接口和Mover接口，如下： Mover接口。 // Sayer 接口 type Sayer interface { say() } // Mover 接口 type Mover interface { move() } dog既可以实现Sayer接口，也可以实现Mover接口。 type dog struct { name string } // 实现Sayer接口 func (d dog) say() { fmt.Printf(\"%s会叫汪汪汪\\n\", d.name) } // 实现Mover接口 func (d dog) move() { fmt.Printf(\"%s会动\\n\", d.name) } func main() { var x Sayer var y Mover var a = dog{name: \"旺财\"} x = a y = a x.say() y.move() } 多个类型实现同一接口 Go语言中不同的类型还可以实现同一接口 首先我们定义一个Mover接口，它要求必须由一个move方法。 // Mover 接口 type Mover interface { move() } 例如狗可以动，汽车也可以动，可以使用如下代码实现这个关系： type dog struct { name string } type car struct { brand string } // dog类型实现Mover接口 func (d dog) move() { fmt.Printf(\"%s会跑\\n\", d.name) } // car类型实现Mover接口 func (c car) move() { fmt.Printf(\"%s速度70迈\\n\", c.brand) } 这个时候我们在代码中就可以把狗和汽车当成一个会动的物体来处理了，不再需要关注它们具体是什么，只需要调用它们的move方法就可以了。 func main() { var x Mover var a = dog{name: \"旺财\"} var b = car{brand: \"保时捷\"} x = a x.move() x = b x.move() } 上面的代码执行结果如下： 旺财会跑 保时捷速度70迈 并且一个接口的方法，不一定需要由一个类型完全实现，接口的方法可以通过在类型中嵌入其他类型或者结构体来实现。 // WashingMachine 洗衣机 type WashingMachine interface { wash() dry() } // 甩干器 type dryer struct{} // 实现WashingMachine接口的dry()方法 func (d dryer) dry() { fmt.Println(\"甩一甩\") } // 海尔洗衣机 type haier struct { dryer //嵌入甩干器 } // 实现WashingMachine接口的wash()方法 func (h haier) wash() { fmt.Println(\"洗刷刷\") } 接口嵌套 接口与接口间可以通过嵌套创造出新的接口。 // Sayer 接口 type Sayer interface { say() } // Mover 接口 type Mover interface { move() } // 接口嵌套 type animal interface { Sayer Mover } 嵌套得到的接口的使用与普通接口一样，这里我们让cat实现animal接口： type cat struct { name string } func (c cat) say() { fmt.Println(\"喵喵喵\") } func (c cat) move() { fmt.Println(\"猫会动\") } func main() { var x animal x = cat{name: \"花花\"} x.move() x.say() } ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:2","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"空接口 空接口的定义 空接口是指没有定义任何方法的接口。因此任何类型都实现了空接口。 空接口类型的变量可以存储任意类型的变量。 func main() { // 定义一个空接口x var x interface{} s := \"pprof.cn\" x = s fmt.Printf(\"type:%T value:%v\\n\", x, x) i := 100 x = i fmt.Printf(\"type:%T value:%v\\n\", x, x) b := true x = b fmt.Printf(\"type:%T value:%v\\n\", x, x) } ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:3","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"空接口的应用 空接口作为函数的参数 使用空接口实现可以接收任意类型的函数参数。 // 空接口作为函数参数 func show(a interface{}) { fmt.Printf(\"type:%T value:%v\\n\", a, a) } 空接口作为map的值 使用空接口实现可以保存任意值的字典。 // 空接口作为map值 var studentInfo = make(map[string]interface{}) studentInfo[\"name\"] = \"李白\" studentInfo[\"age\"] = 18 studentInfo[\"married\"] = false fmt.Println(studentInfo) ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:4","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"类型断言 空接口可以存储任意类型的值，那我们如何获取其存储的具体数据呢？ 接口值 一个接口的值（简称接口值）是由一个具体类型和具体类型的值两部分组成的。这两部分分别称为接口的动态类型和动态值。 我们来看一个具体的例子： var w io.Writer w = os.Stdout w = new(bytes.Buffer) w = nil 请看下图分解（来自go中文文档）：想要判断空接口中的值这个时候就可以使用类型断言，其语法格式： x.(T) 其中： x：表示类型为interface{}的变量 T：表示断言x可能是的类型。 该语法返回两个参数，第一个参数是x转化为T类型后的变量，第二个值是一个布尔值，若为true则表示断言成功，为false则表示断言失败。 举个例子： func main() { var x interface{} x = \"pprof.cn\" v, ok := x.(string) if ok { fmt.Println(v) } else { fmt.Println(\"类型断言失败\") } } 上面的示例中如果要断言多次就需要写多个if判断，这个时候我们可以使用switch语句来实现： func justifyType(x interface{}) { switch v := x.(type) { case string: fmt.Printf(\"x is a string，value is %v\\n\", v) case int: fmt.Printf(\"x is a int is %v\\n\", v) case bool: fmt.Printf(\"x is a bool is %v\\n\", v) default: fmt.Println(\"unsupport type！\") } } 因为空接口可以存储任意类型值的特点，所以空接口在Go语言中的使用十分广泛。 关于接口需要注意的是，只有当有两个或两个以上的具体类型必须以相同的方式进行处理时才需要定义接口。不要为了接口而写接口，那样只会增加不必要的抽象，导致不必要的运行时损耗。 ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:5","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 方法 ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:0:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"方法本质 一个以方法的 receiver 参数作为第一个参数的普通函数。 package main import ( \"fmt\" \"time\" ) type field struct { name string } func (p *field) print() { fmt.Println(p.name) } func main() { data1 := []*field{ {\"one\"}, {\"two\"}, {\"three\"} } for _, v := range data1 { go v.print() } data2 := []field{ {\"four\"}, {\"five\"}, {\"six\"} } for _, v := range data2 { go v.print() } time.Sleep(3 * time.Second) } one two three six six six 为什么会是这样的输出？ 根据 Go 方法的本质，也就是一个以方法的 receiver 参数作为第一个参数的普通函数，对这个程序做个等价变换 type field struct { name string } func (p *field) print() { fmt.Println(p.name) } func main() { data1 := []*field{ {\"one\"}, {\"two\"}, {\"three\"} } for _, v := range data1 { go (*field).print(v) } data2 := []field{ {\"four\"}, {\"five\"}, {\"six\"} } for _, v := range data2 { go (*field).print(\u0026v) } time.Sleep(3 * time.Second) } 我们把对 field 的方法 print 的调用，替换为 Method Expression 形式，替换前后的程序输出结果是一致的 使用 go 关键字启动一个新 Goroutine 时，method expression 形式的 print 函数是如何绑定参数的： 迭代 data1 时，由于 data1 中的元素类型是 field 指针 (*field)，因此赋值后 v 就是元素地址，与 print 的 receiver 参数类型相同，每次调用 (*field).print 函数时直接传入的 v 即可，实际上传入的也是各个 field 元素的地址； 迭代 data2 时，由于 data2 中的元素类型是 field（非指针），与 print 的 receiver 参数类型不同，因此需要将其取地址后再传入 (*field).print 函数。这样每次传入的 \u0026v 实际上是变量 v 的地址，而不是切片 data2 中各元素的地址。 ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:1:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"方法定义 Golang 方法总是绑定对象实例，并隐式将实例作为第一实参 (receiver)。 只能为当前包内命名类型定义方法。 参数 receiver 可任意命名。如方法中未曾使用 ，可省略参数名。 参数 receiver 类型可以是 T 或 *T。基类型 T 不能是接口或指针。 不支持方法重载，receiver 只是参数签名的组成部分。 可用实例 value 或 pointer 调用全部方法，编译器自动转换。 方法表达式（Method Expression）: 直接以类型名 T 调用方法的表达方式，被称为 Method Expression。通过 Method Expression 这种形式，类型 T 只能调用 T 的方法集合（Method Set）中的方法，同理类型 *T 也只能调用 *T 的方法集合中的方法 一个方法就是一个包含了接受者的函数，接受者可以是命名类型或者结构体类型的一个值或者是一个指针。 所有给定类型的方法属于该类型的方法集。 func (recevier type) methodName(参数列表)(返回值列表){} //参数和返回值可以省略 package main type Test struct{} // 无参数、无返回值 func (t Test) method0() { } // 单参数、无返回值 func (t Test) method1(i int) { } // 多参数、无返回值 func (t Test) method2(x, y int) { } // 无参数、单返回值 func (t Test) method3() (i int) { return } // 多参数、多返回值 func (t Test) method4(x, y int) (z int, err error) { return } // 无参数、无返回值 func (t *Test) method5() { } // 单参数、无返回值 func (t *Test) method6(i int) { } // 多参数、无返回值 func (t *Test) method7(x, y int) { } // 无参数、单返回值 func (t *Test) method8() (i int) { return } // 多参数、多返回值 func (t *Test) method9(x, y int) (z int, err error) { return } func main() {} 下面定义一个结构体类型和该类型的一个方法： package main import ( \"fmt\" ) //结构体 type User struct { Name string Email string } //方法 func (u User) Notify() { fmt.Printf(\"%v : %v \\n\", u.Name, u.Email) } func main() { // 值类型调用方法 u1 := User{\"golang\", \"golang@golang.com\"} u1.Notify() // 指针类型调用方法 u2 := User{\"go\", \"go@go.com\"} u3 := \u0026u2 u3.Notify() } output： golang : golang@golang.com go : go@go.com 解释： 首先我们定义了一个叫做 User 的结构体类型，然后定义了一个该类型的方法叫做 Notify，该方法的接受者是一个 User 类型的值。要调用 Notify 方法我们需要一个 User 类型的值或者指针。 在这个例子中当我们使用指针时，Go 调整和解引用指针使得调用可以被执行。注意，当接受者不是一个指针时，该方法操作对应接受者的值的副本(意思就是即使你使用了指针调用函数，但是函数的接受者是值类型，所以函数内部操作还是对副本的操作，而不是指针操作。 我们修改 Notify 方法，让它的接受者使用指针类型： package main import ( \"fmt\" ) //结构体 type User struct { Name string Email string } //方法 func (u *User) Notify() { fmt.Printf(\"%v : %v \\n\", u.Name, u.Email) } func main() { // 值类型调用方法 u1 := User{\"golang\", \"golang@golang.com\"} u1.Notify() // 指针类型调用方法 u2 := User{\"go\", \"go@go.com\"} u3 := \u0026u2 u3.Notify() } output： golang : golang@golang.com go : go@go.com 注意：当接受者是指针时，即使用值类型调用那么函数内部也是对指针的操作。 方法不过是一种特殊的函数，只需将其还原，就知道 receiver T 和 *T 的差别。 package main import \"fmt\" type Data struct { x int } func (self Data) ValueTest() { // func ValueTest(self Data); fmt.Printf(\"Value: %p\\n\", \u0026self) } func (self *Data) PointerTest() { // func PointerTest(self *Data); fmt.Printf(\"Pointer: %p\\n\", self) } func main() { d := Data{} p := \u0026d fmt.Printf(\"Data: %p\\n\", p) d.ValueTest() // ValueTest(d) d.PointerTest() // PointerTest(\u0026d) p.ValueTest() // ValueTest(*p) p.PointerTest() // PointerTest(p) } output: Data: 0xc42007c008 Value: 0xc42007c018 Pointer: 0xc42007c008 Value: 0xc42007c020 Pointer: 0xc42007c008 普通函数与方法的区别 1.对于普通函数，接收者为值类型时，不能将指针类型的数据直接传递，反之亦然。 2.对于方法（如struct的方法），接收者为值类型时，可以直接用指针类型的变量调用方法，反过来同样也可以。 package main //普通函数与方法的区别（在接收者分别为值类型和指针类型的时候） import ( \"fmt\" ) //1.普通函数 //接收值类型参数的函数 func valueIntTest(a int) int { return a + 10 } //接收指针类型参数的函数 func pointerIntTest(a *int) int { return *a + 10 } func structTestValue() { a := 2 fmt.Println(\"valueIntTest:\", valueIntTest(a)) //函数的参数为值类型，则不能直接将指针作为参数传递 //fmt.Println(\"valueIntTest:\", valueIntTest(\u0026a)) //compile error: cannot use \u0026a (type *int) as type int in function argument b := 5 fmt.Println(\"pointerIntTest:\", pointerIntTest(\u0026b)) //同样，当函数的参数为指针类型时，也不能直接将值类型作为参数传递 //fmt.Println(\"pointerIntTest:\", pointerIntTest(b)) //compile error:cannot use b (type int) as type *int in function argument } //2.方法 type PersonD struct { id int name string } //接收者为值类型 func (p PersonD) valueShowName() { fmt.Println(p.name) } //接收者为指针类型 func (p *PersonD) pointShowName() { fmt.Println(p.name) } func structTestFunc() { //值类型调用方法 personValue := PersonD{101, \"hello world\"} personValue.valueShowName() personValue.pointShowName() //指针类型调用方法 personPointer := \u0026PersonD{102, \"hello golang\"} personPointer.valueShowName() personPointer.pointShowName() //与普通函数不同，接收者为指针类型和值类型的方法，指针类型和值类型的变量均可相互调用 } func main() { structTestValue() structTestFunc() } output： valueIntTest: 12 ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:2:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"匿名字段 Golang匿名字段 ：可以像字段成员那样访问匿名字段方法，编译器负责查找。 package main import \"fmt\" type User struct { id int name string } type Manager struct { User } func (self *User) ToString() string { // receiver = \u0026(Manager.User) return fmt.Sprintf(\"User: %p, %v\", self, self) } func main() { m := Manager{User{1, \"Tom\"} } fmt.Printf(\"Manager: %p\\n\", \u0026m) fmt.Println(m.ToString()) } output: Manager: 0xc42000a060 User: 0xc42000a060, \u0026{1 Tom} 通过匿名字段，可获得和继承类似的复用能力。依据编译器查找次序，只需在外层定义同名方法，就可以实现 “override”。 package main import \"fmt\" type User struct { id int name string } type Manager struct { User title string } func (self *User) ToString() string { return fmt.Sprintf(\"User: %p, %v\", self, self) } func (self *Manager) ToString() string { return fmt.Sprintf(\"Manager: %p, %v\", self, self) } func main() { m := Manager{User{1, \"Tom\"}, \"Administrator\"} fmt.Println(m.ToString()) fmt.Println(m.User.ToString()) } output: Manager: 0xc420074180, \u0026\\{\\{1 Tom} Administrator} User: 0xc420074180, \u0026{1 Tom} ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:3:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"方法集以及如何选择 receiver 参数的类型。 Golang方法集 ：每个类型都有与之关联的方法集，这会影响到接口实现规则。 类型 T 方法集包含全部 receiver T 方法。 类型 *T 方法集包含全部 receiver T + *T 方法。 如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T + *T 方法。 不管嵌入 T 或 *T，*S 方法集总是包含 T + *T 方法。 用实例 value 和 pointer 调用方法 (含匿名字段) 不受方法集约束，编译器总是查找全部方法，并自动转换 receiver 实参。 Go 语言中内部类型方法集提升的规则： 类型 T 方法集包含全部 receiver T 方法。 package main import ( \"fmt\" ) type T struct { int } func (t T) test() { fmt.Println(\"类型 T 方法集包含全部 receiver T 方法。\") } func main() { t1 := T{1} fmt.Printf(\"t1 is : %v\\n\", t1) t1.test() } output： t1 is : {1} 类型 T 方法集包含全部 receiver T 方法。 类型 *T 方法集包含全部 receiver T + *T 方法。 package main import ( \"fmt\" ) type T struct { int } func (t T) testT() { fmt.Println(\"类型 *T 方法集包含全部 receiver T 方法。\") } func (t *T) testP() { fmt.Println(\"类型 *T 方法集包含全部 receiver *T 方法。\") } func main() { t1 := T{1} t2 := \u0026t1 fmt.Printf(\"t2 is : %v\\n\", t2) t2.testT() t2.testP() } output： t2 is : \u0026{1} 类型 *T 方法集包含全部 receiver T 方法。 类型 *T 方法集包含全部 receiver *T 方法。 给定一个结构体类型 S 和一个命名为 T 的类型，方法提升像下面规定的这样被包含在结构体方法集中： 如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。 这条规则说的是当我们嵌入一个类型，嵌入类型的接受者为值类型的方法将被提升，可以被外部类型的值和指针调用。 package main import ( \"fmt\" ) type S struct { T } type T struct { int } func (t T) testT() { fmt.Println(\"如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。\") } func main() { s1 := S{T{1} } s2 := \u0026s1 fmt.Printf(\"s1 is : %v\\n\", s1) s1.testT() fmt.Printf(\"s2 is : %v\\n\", s2) s2.testT() } output： s1 is : { {1} } 如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。 s2 is : \u0026{ {1} } 如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T + *T 方法。 这条规则说的是当我们嵌入一个类型的指针，嵌入类型的接受者为值类型或指针类型的方法将被提升，可以被外部类型的值或者指针调用。 package main import ( \"fmt\" ) type S struct { T } type T struct { int } func (t T) testT() { fmt.Println(\"如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T 方法\") } func (t *T) testP() { fmt.Println(\"如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 *T 方法\") } func main() { s1 := S{T{1} } s2 := \u0026s1 fmt.Printf(\"s1 is : %v\\n\", s1) s1.testT() s1.testP() fmt.Printf(\"s2 is : %v\\n\", s2) s2.testT() s2.testP() } output： s1 is : { {1} } 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T 方法 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 *T 方法 s2 is : \u0026{ {1} } 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T 方法 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 *T 方法 func (t T) M1() \u003c=\u003e F1(t T) func (t *T) M2() \u003c=\u003e F2(t *T) 当 receiver 参数的类型为 T 时： 当我们的方法 M1 采用类型为 T 的 receiver 参数时，代表 T 类型实例的 receiver 参数以值传递方式传递到 M1 方法体中的，实际上是 T 类型实例的副本，M1 方法体中对副本的任何修改操作，都不会影响到原 T 类型实例。 当 receiver 参数的类型为 *T 时： 方法 M2 采用类型为 *T 的 receiver 参数时，代表 *T 类型实例的 receiver 参数以值传递方式传递到 M2 方法体中的，实际上是 T 类型实例的地址，M2 方法体通过该地址可以对原 T 类型实例进行任何修改操作。 选择 receiver 参数类型的第一个原则：*如果 Go 方法要把对 receiver 参数代表的类型实例的修改，反映到原类型实例上，那么我们应该选择 T 作为 receiver 参数的类型。 无论是 T 类型实例，还是 *T 类型实例，都既可以调用 receiver 为 T 类型的方法，也可以调用 receiver 为 *T 类型的方法。 选择 receiver 参数类型的第二个原则： 一般情况下，我们通常会为 receiver 参数选择 T 类型，因为这样可以缩窄外部修改类型实例内部状态的“接触面”，也就是尽量少暴露可以修改类型内部状态的方法。 考虑到 Go 方法调用时，receiver 参数是以值拷贝的形式传入方法中的。那么，如果 receiver 参数类型的 size 较大，以值拷贝形式传入就会导致较大的性能开销，这时我们选择 *T 作为 receiver 类型可能更好些。 选择 receiver 参数类型的第三个原则：（其实是应该首先考虑的一点） 如果 T 类型需要实现某个接口，那我们就要使用 T 作为 receiver 参数的类型，来满足接口类型方法集合中的所有方法。 如果 T 不需要实现某一接口，但 *T 需要实现该接口，那么根据方法集合概念，*T 的方法集合是包含 T 的方法集合的，这样我们在确定 Go 方法的 receiver 的类型时，参考原则一和原则二就可以了。 ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:4:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"表达式 Golang 表达式 ：根据调用者不同，方法分为两种表现形式: instance.method(args...) ---\u003e \u003ctype\u003e.func(instance, args...) 前者称为 method value，后者 method expression。 两者都可像普通函数那样赋值和传参，区别在于 method value 绑定实例，而 method expression 则须显式传参。 package main import \"fmt\" type User struct { id int name string } func (self *User) Test() { fmt.Printf(\"%p, %v\\n\", self, self) } func main() { u := User{1, \"Tom\"} u.Test() mValue := u.Test mValue() // 隐式传递 receiver mExpression := (*User).Test mExpression(\u0026u) // 显式传递 receiver } output: 0xc42000a060, \u0026{1 Tom} 0xc42000a060, \u0026{1 Tom} 0xc42000a060, \u0026{1 Tom} 需要注意，method value 会复制 receiver。 package main import \"fmt\" type User struct { id int name string } func (self User) Test() { fmt.Println(self) } func main() { u := User{1, \"Tom\"} mValue := u.Test // 立即复制 receiver，因为不是指针类型，不受后续修改影响。 u.id, u.name = 2, \"Jack\" u.Test() mValue() } output: {2 Jack} {1 Tom} 在汇编层面，method value 和闭包的实现方式相同，实际返回 FuncVal 类型对象。 FuncVal { method_address, receiver_copy } 可依据方法集转换 method expression，注意 receiver 类型的差异。 package main import \"fmt\" type User struct { id int name string } func (self *User) TestPointer() { fmt.Printf(\"TestPointer: %p, %v\\n\", self, self) } func (self User) TestValue() { fmt.Printf(\"TestValue: %p, %v\\n\", \u0026self, self) } func main() { u := User{1, \"Tom\"} fmt.Printf(\"User: %p, %v\\n\", \u0026u, u) mv := User.TestValue mv(u) mp := (*User).TestPointer mp(\u0026u) mp2 := (*User).TestValue // *User 方法集包含 TestValue。签名变为 func TestValue(self *User)。实际依然是 receiver value copy。 mp2(\u0026u) } output: User: 0xc42000a060, {1 Tom} TestValue: 0xc42000a0a0, {1 Tom} TestPointer: 0xc42000a060, \u0026{1 Tom} TestValue: 0xc42000a100, {1 Tom} 将方法 “还原” 成函数，就容易理解下面的代码了。 package main type Data struct{} func (Data) TestValue() {} func (*Data) TestPointer() {} func main() { var p *Data = nil p.TestPointer() (*Data)(nil).TestPointer() // method value (*Data).TestPointer(nil) // method expression // p.TestValue() // invalid memory address or nil pointer dereference // (Data)(nil).TestValue() // cannot convert nil to type Data // Data.TestValue(nil) // cannot use nil as type Data in function argument } ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:5:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"自定义error ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:6:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"抛异常和处理异常 系统抛 package main import \"fmt\" // 系统抛 func test01() { a := [5]int{0, 1, 2, 3, 4} a[1] = 123 fmt.Println(a) //a[10] = 11 index := 10 a[index] = 10 fmt.Println(a) } func getCircleArea(radius float32) (area float32) { if radius \u003c 0 { // 自己抛 panic(\"半径不能为负\") } return 3.14 * radius * radius } func test02() { getCircleArea(-5) } // func test03() { // 延时执行匿名函数 // 延时到何时？（1）程序正常结束 （2）发生异常时 defer func() { // recover() 恢复 // 会返回程序为什么挂了 if err := recover(); err != nil { fmt.Println(err) } }() getCircleArea(-5) fmt.Println(\"这里有没有执行\") } func test04() { test03() fmt.Println(\"test04\") } func main() { test04() } 返回异常 package main import ( \"errors\" \"fmt\" ) func getCircleArea(radius float32) (area float32, err error) { if radius \u003c 0 { // 构建个异常对象 err = errors.New(\"半径不能为负\") return } area = 3.14 * radius * radius return } func main() { area, err := getCircleArea(-5) if err != nil { fmt.Println(err) } else { fmt.Println(area) } } 自定义error： package main import ( \"fmt\" \"os\" \"time\" ) type PathError struct { path string op string createTime string message string } func (p *PathError) Error() string { return fmt.Sprintf(\"path=%s \\nop=%s \\ncreateTime=%s \\nmessage=%s\", p.path, p.op, p.createTime, p.message) } func Open(filename string) error { file, err := os.Open(filename) if err != nil { return \u0026PathError{ path: filename, op: \"read\", message: err.Error(), createTime: fmt.Sprintf(\"%v\", time.Now()), } } defer file.Close() return nil } func main() { err := Open(\"/Users/5lmh/Desktop/go/src/test.txt\") switch v := err.(type) { case *PathError: fmt.Println(\"get path error,\", v) default: } } output： get path error, path=/Users/pprof/Desktop/go/src/test.txt op=read createTime=2018-04-05 11:25:17.331915 +0800 CST m=+0.000441790 message=open /Users/pprof/Desktop/go/src/test.txt: no such file or directory ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:6:1","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 函数 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:0:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"函数定义 go的函数特点： 无需声明原型。 支持不定参、变参。 支持多返回值。 支持命名返回参数。 支持匿名函数和闭包。（闭包详见后文） 函数也是一种类型，一个函数可以赋值给变量。 有返回值的函数，必须有明确的终止语句，否则会引发编译错误。 不支持 嵌套 (nested) 一个包不能有两个名字一样的函数。 不支持 重载 (overload) （区别于重写，重载(overloading) 是在一个类里面，方法名字相同，而参数不同。返回类型可以相同也可以不同。每个重载的方法（或者构造函数）都必须有一个独一无二的参数类型列表。最常用的地方就是构造器的重载。） 不支持 默认参数 (default parameter)。 没有函数体的函数声明，表示该函数不是以Go实现的。这样的声明定义了函数标识符。 所有参数都是值传递：slice，map，channel 会有传引⽤的错觉 string、切片、map 这些类型它们的内存表示对应的是它们数据内容的“描述符”。当这些类型作为实参类型时，值传递拷贝的也是它们数据内容的“描述符”，不包括数据内容本身，所以这些类型传递的开销是固定的，与数据内容大小无关。这种只拷贝“描述符”，不拷贝实际数据内容的拷贝过程，也被称为“浅拷贝”。 当函数的形参为接口类型，或者形参是变长参数时，简单的值传递就不能满足要求了，这时 Go 编译器会介入：对于类型为接口类型的形参，Go 编译器会把传递的实参赋值给对应的接口类型形参；对于为变长参数的形参，Go 编译器会将零个或多个实参按一定形式转换为对应的变长形参。 在 Go 中，变长参数实际上是通过切片来实现的。所以，我们在函数体中，就可以使用切片支持的所有操作来操作变长参数 关于函数的返回值： Go 标准库以及大多数项目代码中的函数，都选择了使用普通的非具名返回值形式。但在一些特定场景下，具名返回值也会得到应用。比如，当函数使用 defer，而且还在 defer 函数中修改外部函数返回值时，具名返回值可以让代码显得更优雅清晰。当函数的返回值个数较多时，每次显式使用 return 语句时都会接一长串返回值，这时，我们用具名返回值可以让函数实现的可读性更好一些 // $GOROOT/src/time/format.go func parseNanoseconds(value string, nbytes int) (ns int, rangeErrString string, err error) { if !commaOrPeriod(value[0]) { err = errBad return } if ns, err = atoi(value[1:nbytes]); err != nil { return } if ns \u003c 0 || 1e9 \u003c= ns { rangeErrString = \"fractional second\" return } scaleDigits := 10 - nbytes for i := 0; i \u003c scaleDigits; i++ { ns *= 10 } return } 函数是第一类对象，可作为参数传递。建议将复杂签名定义为函数类型，以便于阅读。 import \"fmt\" func test(fn func() int) int { return fn() } // 定义函数类型。 type FormatFunc func(s string, x, y int) string func format(fn FormatFunc, s string, x, y int) string { return fn(s, x, y) } func main() { s1 := test(func() int { return 100 }) // 直接将匿名函数当参数。 s2 := format(func(s string, x, y int) string { return fmt.Sprintf(s, x, y) }, \"%d, %d\", 10, 20) println(s1, s2) } output: 100 10,20 fmt里的一些格式化输入输出函数： func Printf(format string, a ...interface{}) (n int, err error) func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) func Sprintf(format string, a ...interface{}) string func Print(a ...interface{}) (n int, err error) func Fprint(w io.Writer, a ...interface{}) (n int, err error) func Sprint(a ...interface{}) string func Println(a ...interface{}) (n int, err error) func Fprintln(w io.Writer, a ...interface{}) (n int, err error) func Sprintln(a ...interface{}) string func Errorf(format string, a ...interface{}) error func Scanf(format string, a ...interface{}) (n int, err error) func Fscanf(r io.Reader, format string, a ...interface{}) (n int, err error) func Sscanf(str string, format string, a ...interface{}) (n int, err error) func Scan(a ...interface{}) (n int, err error) func Fscan(r io.Reader, a ...interface{}) (n int, err error) func Sscan(str string, a ...interface{}) (n int, err error) func Scanln(a ...interface{}) (n int, err error) func Fscanln(r io.Reader, a ...interface{}) (n int, err error) func Sscanln(str string, a ...interface{}) (n int, err error) ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:1:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"参数 map、slice、chan、指针、interface默认以引用的方式传递，其他的在默认情况下，使用的是值传递. 无论是值传递，还是引用传递，传递给函数的都是变量的副本，不过，值传递是值的拷贝。引用传递是地址的拷贝，一般来说，地址拷贝更为高效。而值拷贝取决于拷贝的对象大小，对象越大，则性能越低。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:2:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"不定参 就是函数的参数不是固定的，后面的类型是固定的。（可变参数） Golang 可变参数本质上就是 slice。只能有一个，且必须是最后一个。 在参数赋值时可以不用用一个一个的赋值，可以直接传递一个数组或者切片，特别注意的是在参数后加上“…”即可。 func myfunc(args ...int) { //0个或多个参数 } func add(a int, args…int) int { //1个或多个参数 } func add(a int, b int, args…int) int { //2个或多个参数 } 注意：其中args是一个slice，我们可以通过arg[index]依次访问所有参数,通过len(arg)来判断传递参数的个数. 任意类型的不定参数：就是函数的参数和每个参数的类型都不是固定的。 用interface{}传递任意类型数据是Go语言的惯例用法，而且interface{}是类型安全的。 func myfunc(args ...interface{}) { } func test(s string, n ...int) string { var x int for _, i := range n { x += i } return fmt.Sprintf(s, x) } func main() { s := []int{1, 2, 3} res := test(\"sum: %d\", s...) // slice... 展开slice,而不是只写变量名 println(res) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:2:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"返回值 Go 的返回值可以被命名，并且就像在函数体开头声明的变量那样使用。 返回值的名称应当具有一定的意义，可以作为文档使用。 没有参数的 return 语句返回各个返回变量的当前值。这种用法被称作 “裸”返回。 package main import ( \"fmt\" ) func add(a, b int) (c int) { c = a + b return } func calc(a, b int) (sum int, avg int) { sum = a + b avg = (a + b) / 2 return } func main() { var a, b int = 1, 2 c := add(a, b) sum, avg := calc(a, b) fmt.Println(a, b, c, sum, avg) } 命名返回参数可被同名局部变量遮蔽，此时需要显式返回。 Golang返回值不能用容器对象接收多返回值。只能用多个变量，或 “_” 忽略。或者多返回值可直接作为其他函数调用实参。 func test() (int, int) { return 1, 2 } func add(x, y int) int { return x + y } func sum(n ...int) int { var x int for _, i := range n { x += i } return x } func main() { println(add(test())) println(sum(test())) } 命名返回参数允许 defer 延迟调用通过闭包读取和修改。 package main func add(x, y int) (z int) { defer func() { z += 100 }() z = x + y return } func main() { println(add(1, 2)) } output:103 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:3:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"利用多返回值进行错误处理： ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:4:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"匿名函数 匿名函数的优越性在于可以直接使用函数内的变量，不必申明。Golang匿名函数可赋值给变量，做为结构字段，或者在 channel 里传送。 package main func main() { // --- function variable --- fn := func() { println(\"Hello, World!\") } fn() // --- function collection --- fns := [](func(x int) int){ func(x int) int { return x + 1 }, func(x int) int { return x + 2 }, } println(fns[0](100)) // --- function as field --- d := struct { fn func() string }{ fn: func() string { return \"Hello, World!\" }, } println(d.fn()) // --- channel of function --- fc := make(chan func() string, 2) fc \u003c- func() string { return \"Hello, World!\" } println((\u003c-fc)()) output: Hello, World! 101 Hello, World! Hello, World! } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:5:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"闭包、递归 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:6:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"闭包 闭包是由函数及其相关引用环境组合而成的实体。 “官方”的解释是：所谓“闭包”，指的是一个拥有许多变量和绑定了这些变量的环境的表达式（通常是一个函数），因而这些变量也是该表达式的一部分。 维基百科讲，闭包（Closure），是引用了自由变量的函数。这个被引用的自由变量将和这个函数一同存在，即使已经离开了创造它的环境也不例外。所以，有另一种说法认为闭包是由函数和与其相关的引用环境组合而成的实体。闭包在运行时可以有多个实例，不同的引用环境和相同的函数组合可以产生不同的实例。 目前在JavaScript、Go、PHP、Scala、Scheme、Common Lisp、Smalltalk、Groovy、Ruby、 Python、Lua、objective c、Swift 以及Java8以上等语言中都能找到对闭包不同程度的支持。 通过支持闭包的语法可以发现一个特点，他们都有垃圾回收(GC)机制。 go的闭包： package main import ( \"fmt\" ) func a() func() int { i := 0 b := func() int { i++ fmt.Println(i) return i } return b } func main() { c := a() c() c() c() a() //不会输出i } output: 1 2 3 当函数a()的内部函数b()被函数a()外的一个变量引用的时候，就创建了一个闭包。 闭包复制的是原对象指针，这就很容易解释延迟引用现象。（延迟引用，引用的只是某个变量的“最终值”，延迟闭包里引用的变量是原变量指针，这解释了后面为什么derfer碰上闭包的输出都是同一值） package main import \"fmt\" func test() func() { x := 100 fmt.Printf(\"x (%p) = %d\\n\", \u0026x, x) return func() { fmt.Printf(\"x (%p) = %d\\n\", \u0026x, x) } } func main() { f := test() f() } output: x (0xc42007c008) = 100 x (0xc42007c008) = 100 在汇编层 ，test 实际返回的是 FuncVal 对象，其中包含了匿名函数地址、闭包对象指针。当调用匿名函数时，只需以某个寄存器传递该对象即可。 Funcval对象： FuncVal { func_address, closure_var_pointer ... } 外部引用函数参数局部变量: package main import \"fmt\" // 外部引用函数参数局部变量 func add(base int) func(int) int { return func(i int) int { base += i return base } } func main() { tmp1 := add(10) fmt.Println(tmp1(1), tmp1(2)) // 此时tmp1和tmp2不是一个实体了 tmp2 := add(100) fmt.Println(tmp2(1), tmp2(2)) } 返回两个闭包： package main import \"fmt\" // 返回2个函数类型的返回值 func test01(base int) (func(int) int, func(int) int) { // 定义2个函数，并返回 // 相加 add := func(i int) int { base += i return base } // 相减 sub := func(i int) int { base -= i return base } // 返回 return add, sub } func main() { f1, f2 := test01(10) // base一直是没有消 fmt.Println(f1(1), f2(2)) // 此时base是9 fmt.Println(f1(3), f2(4)) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:6:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"go递归函数 构成递归的两个条件： 子问题须与原始问题为同样的事，且更为简单。 不能无限制地调用本身，须有个出口，化简为非递归状况处理。 go的递归和其他语言基本无差别。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:6:2","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"延迟调用（defer） 特性： 关键字 defer 用于注册延迟调用。 这些调用直到 return 前才被执行。因此，可以用来做资源清理。 多个defer语句，按先进后出的方式执行。（因为后面的defer可能会用到前面的资源） defer语句中的变量，在defer声明时就决定了。 发生panic依然会执行defer，但不是任何情况都会执行，比如：os.Exit()不会调用defer。os.Exit()退出时不输出当前调用栈信息 Go1.13前的版本defer的开销还是非常大的，在后续团队优化后现在的开销比较小可以放心使用 用途： 关闭文件句柄、锁资源释放、数据库连接释放 使用 defer 可以跟踪函数的执行过程 // trace.go package main func Trace(name string) func() { println(\"enter:\", name) return func() { println(\"exit:\", name) } } func foo() { defer Trace(\"foo\")() bar() } func bar() { defer Trace(\"bar\")() } func main() { defer Trace(\"main\")() foo() } 不足：调用 Trace 时需手动显式传入要跟踪的函数名；如果是并发应用，不同 Goroutine 中函数链跟踪混在一起无法分辨；输出的跟踪结果缺少层次感，调用关系不易识别；对要跟踪的函数，需手动调用 Trace 函数。 实现一个自动注入跟踪代码，并输出有层次感的函数调用链跟踪命令行工具： 自动获取所跟踪函数的函数名 充当“断言”，提示潜在bug defer功能强大，对于资源管理非常方便，但是如果没用好，也会有陷阱。 defer碰上闭包： package main import \"fmt\" func main() { var whatever [5]struct{} for i := range whatever { defer func() { fmt.Println(i) }() } } output: 4 4 4 4 4 延迟引用，闭包里的i是原变量指针。 defer.f.Close: package main import \"fmt\" type Test struct { name string } func (t *Test) Close() { fmt.Println(t.name, \" closed\") } func main() { ts := []Test{\"a\", \"b\", \"c\"} for _, t := range ts { defer t.Close() } } output: c closed c closed c closed package main import \"fmt\" type Test struct { name string } func (t *Test) Close() { fmt.Println(t.name, \" closed\") } func Close(t Test) { t.Close() } func main() { ts := []Test\"a\", \"b\", \"c\"} for _, t := range ts { defer Close(t) } //或者for _, t := range ts { // t2 := t // defer t2.Close() // } } output: c closed b closed a closed 结论： defer后面的语句在执行的时候，函数调用的参数会被保存起来，但是不执行。也就是复制了一份。但是并没有说struct这里的this指针如何处理，通过这个例子可以看出go语言并没有把这个明确写出来的this指针当作参数来看待。 多个 defer 注册，按 FILO 次序执行 ( 先进后出 )。哪怕函数或某个延迟调用发生错误，比如发生panic，这些defer调用依旧会被执行。 package main func test(x int) { defer println(\"a\") defer println(\"b\") defer func() { println(100 / x) // div0 异常未被捕获，逐步往外传递，最终终止进程。 }() defer println(\"c\") } func main() { test(0) } output: c b a panic: runtime error: integer divide by zero 延迟调用参数在注册时求值或复制，可用指针或闭包 “延迟” 读取。 package main func test() { x, y := 10, 20 defer func(i int) { println(\"defer:\", i, y) // y 闭包引用 }(x) // x 被复制 x += 10 y += 100 println(\"x =\", x, \"y =\", y) } func main() { test() } output: x = 20 y = 120 defer: 10 120 滥用 defer 可能会导致性能问题，尤其是在一个 “大循环” 里。 package main import ( \"fmt\" \"sync\" \"time\" ) var lock sync.Mutex func test() { lock.Lock() lock.Unlock() } func testdefer() { lock.Lock() defer lock.Unlock() } func main() { func() { t1 := time.Now() for i := 0; i \u003c 10000; i++ { test() } elapsed := time.Since(t1) fmt.Println(\"test elapsed: \", elapsed) }() func() { t1 := time.Now() for i := 0; i \u003c 10000; i++ { testdefer() } elapsed := time.Since(t1) fmt.Println(\"testdefer elapsed: \", elapsed) }() } output: test elapsed: 223.162µs testdefer elapsed: 781.304µs ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:7:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"defer陷阱 defer 与 closure： package main import ( \"errors\" \"fmt\" ) func foo(a, b int) (i int, err error) { defer fmt.Printf(\"first defer err %v\\n\", err) defer func(err error) { fmt.Printf(\"second defer err %v\\n\", err) }(err) defer func() { fmt.Printf(\"third defer err %v\\n\", err) }() if b == 0 { err = errors.New(\"divided by zero!\") return } i = a / b return } func main() { foo(2, 0) } output： third defer err divided by zero! second defer err \u003cnil\u003e first defer err \u003cnil\u003e 解释：如果 defer 后面跟的不是一个 closure 最后执行的时候我们得到的并不是最新的值。 defer 与 return: package main import \"fmt\" func foo() (i int) { i = 0 defer func() { fmt.Println(i) }() return 2 } func main() { foo() } output： 2 解释：在有具名返回值的函数中（这里具名返回值为 i），执行 return 2 的时候实际上已经将 i 的值重新赋值为 2。所以defer closure 输出结果为 2 而不是 1。 defer nil 函数: package main import ( \"fmt\" ) func test() { var run func() = nil defer run() fmt.Println(\"runs\") } func main() { defer func() { if err := recover(); err != nil { fmt.Println(err) } }() test() } output： runs runtime error: invalid memory address or nil pointer dereference 解释：名为 test 的函数一直运行至结束，然后 defer 函数会被执行且会因为值为 nil 而产生 panic 异常。然而值得注意的是，run() 的声明是没有问题，因为在test函数运行完成后它才会被调用。 在错误的位置使用 defer: 当 http.Get 失败时会抛出异常。 package main import \"net/http\" func do() error { res, err := http.Get(\"http://www.google.com\") defer res.Body.Close() if err != nil { return err } // ..code... return nil } func main() { do() } output： panic: runtime error: invalid memory address or nil pointer dereference 因为在这里我们并没有检查我们的请求是否成功执行，当它失败的时候，我们访问了 Body 中的空变量 res ，因此会抛出异常 解决方案： 总是在一次成功的资源分配下面使用 defer ，对于这种情况来说意味着：当且仅当 http.Get 成功执行时才使用 defer package main import \"net/http\" func do() error { res, err := http.Get(\"http://xxxxxxxxxx\") if res != nil { defer res.Body.Close() } if err != nil { return err } // ..code... return nil } func main() { do() } 在上述的代码中，当有错误的时候，err 会被返回，否则当整个函数返回的时候，会关闭 res.Body 。 解释：在这里，你同样需要检查 res 的值是否为 nil ，这是 http.Get 中的一个警告。通常情况下，出错的时候，返回的内容应为空并且错误会被返回，可当你获得的是一个重定向 error 时， res 的值并不会为 nil ，但其又会将错误返回。上面的代码保证了无论如何 Body 都会被关闭，如果你没有打算使用其中的数据，那么你还需要丢弃已经接收的数据。 不检查错误: 在这里，f.Close() 可能会返回一个错误，可这个错误会被我们忽略掉。 package main import \"os\" func do() error { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer f.Close() } // ..code... return nil } func main() { do() } 改进一下： package main import \"os\" func do() error { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer func() { if err := f.Close(); err != nil { // log etc } }() } // ..code... return nil } func main() { do() } 再改进一下：通过命名的返回变量来返回defer内的错误。 package main import \"os\" func do() (err error) { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer func() { if ferr := f.Close(); ferr != nil { err = ferr } }() } // ..code... return nil } func main() { do() } 释放相同的资源 如果你尝试使用相同的变量释放不同的资源，那么这个操作可能无法正常执行。 package main import ( \"fmt\" \"os\" ) func do() error { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer func() { if err := f.Close(); err != nil { fmt.Printf(\"defer close book.txt err %v\\n\", err) } }() } // ..code... f, err = os.Open(\"another-book.txt\") if err != nil { return err } if f != nil { defer func() { if err := f.Close(); err != nil { fmt.Printf(\"defer close another-book.txt err %v\\n\", err) } }() } return nil } func main() { do() } 输出结果： defer close book.txt err close ./another-book.txt: file already closed 当延迟函数执行时，只有最后一个变量会被用到，因此，f 变量 会成为最后那个资源 (another-book.txt)。而且两个 defer 都会将这个资源作为最后的资源来关闭 解决方案： package main import ( \"fmt\" \"io\" \"os\" ) func do() error { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer func(f io.Closer) { if err := f.Close(); err != nil { fmt.Printf(\"defer close book.txt err %v\\n\", err) } }(f) } // ..code... f, err = os.Open(\"another-book.txt\") if err != nil { return err } if f != nil { defer func(f io.Closer) { if err := f.Close(); err != nil { fmt.Printf(\"defer close another-book.txt err %v\\n\", err) } }(f) } return nil } func main() { do() } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:7:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"异常处理，错误处理 提倡及早失败，避免嵌套 Golang 没有结构化异常，使用 panic 抛出错误，recover 捕获错误。 异常的使用场景简单描述：Go中可以抛出一个panic的异常，然后在defer中通过recover捕获这个异常，然后正常处理。 panic: 内置函数 假如函数F中书写了panic语句，会终止其后要执行的代码，在panic所在函数F内如果存在要执行的defer函数列表，按照defer的逆序执行 返回函数F的调用者G，在G中，调用函数F语句之后的代码不会执行，假如函数G中存在要执行的defer函数列表，按照defer的逆序执行 直到goroutine整个退出，并报告错误 recover: 内置函数 用来控制一个goroutine的panicking行为，捕获panic，从而影响应用的行为 一般的调用建议 只能用在defer函数中，通过recever来终止一个goroutine的panicking过程，从而恢复正常代码的执行 可以获取通过panic传递的error 注意： 利用recover处理panic指令，defer 必须放在 panic 之前定义，另外 recover 只有在 defer 调用的函数中才有效。否则当panic时，recover无法捕获到panic，无法防止panic扩散。 recover 处理异常后，逻辑并不会恢复到 panic 那个点去，函数跑到 defer 之后的那个点。 多个 defer 会形成 defer 栈，后定义的 defer 语句会被最先调用。 当心recoer成为恶魔，因为recover并不会检测发生了什么错误。可能是系统的某些核心资源消耗完了，强制恢复之后系统依然无法正常工作的。还会导致一些健康检查程序无法检测出错误，health check无法检测出错误（很多health check程序只是检查这个进程在还是不在），形成僵尸服务进程（存在着但不能提供服务）。不如采用一种可恢复的设计模式，“Let it Crash”，干脆让进程crash掉，然后就会帮我们重新把服务进程提起来，如同重启。（重启是恢复不确定性最好的方法） package main func main() { test() } func test() { defer func() { if err := recover(); err != nil { println(err.(string)) // 将 interface{} 转型为具体类型。 } }() panic(\"panic error!\") } output: panic error! 由于 panic、recover 参数类型为 interface{}，因此可抛出任何类型对象。 func panic(v interface{}) func recover() interface{} 向已关闭的通道发送数据会引发panic package main import ( \"fmt\" ) func main() { defer func() { if err := recover(); err != nil { fmt.Println(err) } }() var ch chan int = make(chan int, 10) close(ch) ch \u003c- 1 } output: send on closed channel 延迟调用中引发的错误，可被后续延迟调用捕获，但仅最后一个错误可被捕获。 package main import \"fmt\" func test() { defer func() { fmt.Println(recover()) }() defer func() { panic(\"defer panic\") }() panic(\"test panic\") } func main() { test() } output: defer panic 捕获函数 recover 只有在延迟调用内直接调用才会终止错误，否则总是返回 nil。任何未捕获的错误都会沿调用堆栈向外传递。 package main import \"fmt\" func test() { defer func() { fmt.Println(recover()) //有效 }() defer recover() //无效！ defer fmt.Println(recover()) //无效！ defer func() { func() { println(\"defer inner\") recover() //无效！ }() }() panic(\"test panic\") } func main() { test() } output: defer inner \u003cnil\u003e test panic 使用延迟匿名函数或下面这样都是有效的。 package main import ( \"fmt\" ) func except() { fmt.Println(recover()) } func test() { defer except() panic(\"test panic\") } func main() { test() } output： test panic 如果需要保护代码段，可将代码块重构成匿名函数，如此可确保后续代码被执行。 package main import \"fmt\" func test(x, y int) { var z int func() { defer func() { if recover() != nil { z = 0 } }() panic(\"test panic\") z = x / y return }() fmt.Printf(\"x / y = %d\\n\", z) } func main() { test(2, 1) } output： x / y = 0 除用 panic 引发中断性错误外，还可返回 error 类型错误对象来表示函数调用状态。（error类型实现了error接口） type error interface { Error() string } 标准库 errors.New 和 fmt.Errorf 函数用于创建实现 error 接口的错误对象。通过判断错误对象实例来确定具体错误类型。 package main import ( \"errors\" \"fmt\" ) //定义预置错误 var ErrDivByZero = errors.New(\"division by zero\") func div(x, y int) (int, error) { if y == 0 { return 0, ErrDivByZero } return x / y, nil } func main() { defer func() { fmt.Println(recover()) }() switch z, err := div(10, 0); err { case nil: println(z) case ErrDivByZero: panic(err) } } output: division by zero go实现类似 try catch 的异常处理。``` go` go package main import “fmt” func Try(fun func(), handler func(interface{})) { defer func() { if err := recover(); err != nil { handler(err) } }() fun() } func main() { Try(func() { panic(“test panic”) }, func(err interface{}) { fmt.Println(err) }) } output： test panic 如何区别使用 panic 和 error 两种方式? 惯例是:导致关键流程出现不可修复性错误的使用 panic，其他使用 error。 几种错误处理策略： 1. 透明错误处理策略 ```go err := doSomething() if err != nil { // 不关心err变量底层错误值所携带的具体上下文信息 // 执行简单错误处理逻辑并返回 ... ... return err } “哨兵” 当错误处理方不能只根据“透明的错误值”就做出错误处理路径选取的情况下，错误处理方会尝试对返回的错误值进行检视，于是就有可能出现下面代码中的反模式： 反模式就是，错误处理方以透明错误值所能提供的唯一上下文信息（描述错误的字符串），作为错误处理路径选择的依据。但这种“反模式”会造成严重的隐式耦合。这也就意味着，错误值构造方不经意间的一次错误描述字符串的改动，都会造成错误处理方处理行为的变化，并且这种通过字符串比较的方式，对错误值进行检视的性能也很差。 data, err := b.Peek(1) if err != nil { switch err.Error() { case \"bufio: negative count\": // ... ... return case \"bufio: buffer full\": // ... ... return case \"bufio: invalid use of UnreadByte\": // ... ... return default: // ... ... return } } go 标准库采用了定义导出的（Export","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:8:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"为啥说函数是Go语言的一等公民： 引用一下 wiki 发明人、C2 站点作者沃德·坎宁安 (Ward Cunningham)对“一等公民”的解释： 如果一门编程语言对某种语言元素的创建和使用没有限制，我们可以像对待值（value）一样对待这种语法元素，那么我们就称这种语法元素是这门编程语言的“一等公民”。拥有“一等公民”待遇的语法元素可以存储在变量中，可以作为参数传递给函数，可以在函数内部创建并可以作为返回值从函数返回 Go 函数可以存储在变量中。 支持在函数内创建并通过返回值返回。 作为参数传入函数。 拥有自己的类型。 应用go函数的这些灵活性： 函数类型的妙用 函数也可以被显式转型，见下面web server的例子。 func greeting(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Welcome, Gopher!\\n\") } func main() { //greeting这个函数被显示转化为HandleFunc类型，ListenAndServe的第二个参数是个需要实现ServeHTTP方法（即需要实现Handle接口）的类型 http.ListenAndServe(\":8080\", http.HandlerFunc(greeting)) } …… // $GOROOT/src/net/http/server.go type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } 利用闭包简化函数调用。 func partialTimes(x int) func(int) int { return func(y int) int { return times(x, y) } } func main() { timesTwo := partialTimes(2) // 以高频乘数2为固定乘数的乘法函数 timesThree := partialTimes(3) // 以高频乘数3为固定乘数的乘法函数 fmt.Println(timesTwo(5)) // 10，等价于times(2, 5) fmt.Println(timesTwo(6)) // 12，等价于times(2, 6) fmt.Println(timesThree(5)) // 15，等价于times(3, 5) fmt.Println(timesThree(6)) // 18，等价于times(3, 6) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:9:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"单元测试 单元测试还是挺重要的。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"go test工具 Go语言中的测试依赖go test命令。编写测试代码和编写普通的Go代码过程是类似的，并不需要学习新的语法、规则或工具。 go test命令是一个按照一定约定和组织的测试代码的驱动程序。在包目录内，所有以_test.go为后缀名的源代码文件都是go test测试的一部分，不会被go build编译到最终的可执行文件中。 在*_test.go文件中有三种类型的函数，单元测试函数、基准测试函数和示例函数。 类型 格式 作用 测试函数 函数名前缀为Test 测试程序的一些逻辑行为是否正确 基准函数 函数名前缀为Benchmark 测试函数的性能 示例函数 函数名前缀为Example 为文档提供示例文档 go test命令会遍历所有的*_test.go文件中符合上述命名规则的函数，然后生成一个临时的main包用于调用相应的测试函数，然后构建并运行、报告测试结果，最后清理测试中生成的临时文件。 Golang单元测试对文件名和方法名，参数都有很严格的要求。 文件名必须以xx_test.go命名 方法必须是Test[^a-z]开头 方法参数必须 t *testing.T 使用go test执行单元测试 go test的参数解读： go test是go语言自带的测试工具，其中包含的是两类，单元测试和性能测试 通过go help test可以看到go test的使用说明： 格式形如： go test [-c] [-i] [build flags] [packages] [flags for test binary] 参数解读： -c : 编译go test成为可执行的二进制文件，但是不运行测试。 -i : 安装测试包依赖的package，但是不运行测试。 关于build flags，调用go help build，这些是编译运行过程中需要使用到的参数，一般设置为空 关于packages，调用go help packages，这些是关于包的管理，一般设置为空 关于flags for test binary，调用go help testflag，这些是go test过程中经常使用到的参数 -test.v : 是否输出全部的单元测试用例（不管成功或者失败），默认没有加上，所以只输出失败的单元测试用例。 -test.run pattern: 只跑哪些单元测试用例 -test.bench patten: 只跑那些性能测试用例 -test.benchmem : 是否在性能测试的时候输出内存情况 -test.benchtime t : 性能测试运行的时间，默认是1s -test.cpuprofile cpu.out : 是否输出cpu性能分析文件 -test.memprofile mem.out : 是否输出内存性能分析文件 -test.blockprofile block.out : 是否输出内部goroutine阻塞的性能分析文件 -test.memprofilerate n : 内存性能分析的时候有一个分配了多少的时候才打点记录的问题。这个参数就是设置打点的内存分配间隔，也就是profile中一个sample代表的内存大小。默认是设置为512 * 1024的。如果你将它设置为1，则每分配一个内存块就会在profile中有个打点，那么生成的profile的sample就会非常多。如果你设置为0，那就是不做打点了。 你可以通过设置memprofilerate=1和GOGC=off来关闭内存回收，并且对每个内存块的分配进行观察。 -test.blockprofilerate n: 基本同上，控制的是goroutine阻塞时候打点的纳秒数。默认不设置就相当于-test.blockprofilerate=1，每一纳秒都打点记录一下 -test.parallel n : 性能测试的程序并行cpu数，默认等于GOMAXPROCS。 -test.timeout t : 如果测试用例运行时间超过t，则抛出panic -test.cpu 1,2,4 : 程序运行在哪些CPU上面，使用二进制的1所在位代表，和nginx的nginx_worker_cpu_affinity是一个道理 -test.short : 将那些运行时间较长的测试用例运行时间缩短 目录结构： test | —— calc.go | —— calc_test.go ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"性能分析工具 可以安装go-torch 通过⽂件⽅式输出 Profile - 灵活性⾼，适⽤于特定代码段的分析 - 通过⼿动调⽤ runtime/pprof 的 API - API 相关⽂档 https://studygolang.com/static/pkgdoc/pkg/runtime_pprof.htm - go tool pprof [binary] [binary.prof] 通过 HTTP ⽅式输出 Profile - 简单，适合于持续性运⾏的应⽤ - 在应⽤程序中导⼊ import _ \"net/http/pprof\"，并启动 http server 即可 - http://\u003chost\u003e:\u003cport\u003e/debug/pprof/ - go tool pprof http://\u003chost\u003e:\u003cport\u003e/debug/pprof/profile?seconds=10 （默认值为30秒） - go-torch -seconds 10 http://\u003chost\u003e:\u003cport\u003e/debug/pprof/profile Go ⽀持的多种 Profile go help testflag https://golang.org/src/runtime/pprof/pprof.go 性能调优 性能调优过程： 常⻅分析指标 Wall Time CPU Time Block Time Memory allocation GC times/time spent ch47 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:2","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"测试函数 测试函数的格式 每个测试函数必须导入testing包，测试函数的基本格式（签名）如下： func TestName(t *testing.T){ // ... } 测试函数的名字必须以Test开头，可选的后缀名必须以大写字母开头，举几个例子：\\ func TestAdd(t *testing.T){ ... } func TestSum(t *testing.T){ ... } func TestLog(t *testing.T){ ... } 其中参数t用于报告测试失败和附加的日志信息。 testing.T的拥有的方法如下： func (c *T) Error(args ...interface{}) func (c *T) Errorf(format string, args ...interface{}) func (c *T) Fail() func (c *T) FailNow() func (c *T) Failed() bool func (c *T) Fatal(args ...interface{}) func (c *T) Fatalf(format string, args ...interface{}) func (c *T) Log(args ...interface{}) func (c *T) Logf(format string, args ...interface{}) func (c *T) Name() string func (t *T) Parallel() func (t *T) Run(name string, f func(t *T)) bool func (c *T) Skip(args ...interface{}) func (c *T) SkipNow() func (c *T) Skipf(format string, args ...interface{}) func (c *T) Skipped() bool 测试函数示例 就像细胞是构成我们身体的基本单位，一个软件程序也是由很多单元组件构成的。单元组件可以是函数、结构体、方法和最终用户可能依赖的任意东西。总之我们需要确保这些组件是能够正常运行的。单元测试是一些利用各种方法测试单元组件的程序，它会将结果与预期输出进行比较。 接下来，我们定义一个split的包，包中定义了一个Split函数，具体实现如下： // split/split.go package split import \"strings\" // split package with a single split function. // Split slices s into all substrings separated by sep and // returns a slice of the substrings between those separators. func Split(s, sep string) (result []string) { i := strings.Index(s, sep) for i \u003e -1 { result = append(result, s[:i]) s = s[i+1:] i = strings.Index(s, sep) } result = append(result, s) return } 在当前目录下，我们创建一个split_test.go的测试文件，并定义一个测试函数如下：（表格测试法） // split/split_test.go package split import ( \"reflect\" \"testing\" ) func TestSplit(t *testing.T) { // 测试函数名必须以Test开头，必须接收一个*testing.T类型参数 got := Split(\"a🅱️c\", \":\") // 程序输出的结果 want := []string{\"a\", \"b\", \"c\"} // 期望的结果 if !reflect.DeepEqual(want, got) { // 因为slice不能比较直接，借助反射包中的方法比较 t.Errorf(\"excepted:%v, got:%v\", want, got) // 测试失败输出错误提示 } } 此时split这个包中的文件如下： split $ ls -l total 16 -rw-r--r-- 1 pprof staff 408 4 29 15:50 split.go -rw-r--r-- 1 pprof staff 466 4 29 16:04 split_test.go 在split包路径下，执行go test命令，可以看到输出结果如下： split $ go test PASS ok github.com/pprof/studygo/code_demo/test_demo/split 0.005s 一个测试用例有点单薄，我们再编写一个测试使用多个字符切割字符串的例子，在split_test.go中添加如下测试函数： func TestMoreSplit(t *testing.T) { got := Split(\"abcd\", \"bc\") want := []string{\"a\", \"d\"} if !reflect.DeepEqual(want, got) { t.Errorf(\"excepted:%v, got:%v\", want, got) } } 再次运行go test命令，输出结果如下： split $ go test --- FAIL: TestMultiSplit (0.00s) split_test.go:20: excepted:[a d], got:[a cd] FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.006s 这一次，我们的测试失败了。我们可以为go test命令添加-v参数，查看测试函数名称和运行时间： split $ go test -v === RUN TestSplit --- PASS: TestSplit (0.00s) === RUN TestMoreSplit --- FAIL: TestMoreSplit (0.00s) split_test.go:21: excepted:[a d], got:[a cd] FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.005s 这一次我们能清楚的看到是TestMoreSplit这个测试没有成功。 还可以在go test命令后添加-run参数，它对应一个正则表达式，只有函数名匹配上的测试函数才会被go test命令执行。 split $ go test -v -run=\"More\" === RUN TestMoreSplit --- FAIL: TestMoreSplit (0.00s) split_test.go:21: excepted:[a d], got:[a cd] FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.006s 现在我们回过头来解决我们程序中的问题。很显然我们最初的split函数并没有考虑到sep为多个字符的情况，我们来修复下这个Bug： package split import \"strings\" // split package with a single split function. // Split slices s into all substrings separated by sep and // returns a slice of the substrings between those separators. func Split(s, sep string) (result []string) { i := strings.Index(s, sep) for i \u003e -1 { result = append(result, s[:i]) s = s[i+len(sep):] // 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return } 这一次我们再来测试一下，我们的程序。注意，当我们修改了我们的代码之后不要仅仅执行那些失败的测试函数，我们应该完整的运行所有的测试，保证不会因为修改代码而引入了新的问题。 split $ go test -v === RUN TestSplit --- PASS: TestSplit (0.00s) === RUN TestMoreSplit --- PASS: TestMoreSplit (0.00s) PASS ok github.com/pprof/studygo/code_demo/test_demo/split 0.006s 这一次我们的测试都通过了 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:3","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"测试组 我们现在还想要测试一下split函数对中文字符串的支持，这个时候我们可以再编写一个TestChineseSplit测试函数，但是我们也可以使用如下更友好的一种方式来添加更多的测试用例。 func TestSplit(t *testing.T) { // 定义一个测试用例类型 type test struct { input string sep string want []string } // 定义一个存储测试用例的切片 tests := []test{ {input: \"a🅱️c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, {input: \"a🅱️c\", sep: \",\", want: []string{\"a🅱️c\"} }, {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"枯藤\", \"树昏鸦\"} }, } // 遍历切片，逐一执行测试用例 for _, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"excepted:%v, got:%v\", tc.want, got) } } } 我们通过上面的代码把多个测试用例合到一起，再次执行go test命令。 split $ go test -v === RUN TestSplit --- FAIL: TestSplit (0.00s) split_test.go:42: excepted:[枯藤 树昏鸦], got:[ 枯藤 树昏鸦] FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.006s 我们的测试出现了问题，仔细看打印的测试失败提示信息：excepted:[枯藤 树昏鸦], got:[ 枯藤 树昏鸦]，你会发现[ 枯藤 树昏鸦]中有个不明显的空串，这种情况下十分推荐使用%#v的格式化方式。 我们修改下测试用例的格式化输出错误提示部分： func TestSplit(t *testing.T) { ... for _, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"excepted:%#v, got:%#v\", tc.want, got) } } } 此时运行go test命令后就能看到比较明显的提示信息了： split $ go test -v === RUN TestSplit --- FAIL: TestSplit (0.00s) split_test.go:42: excepted:[]string{\"枯藤\", \"树昏鸦\"}, got:[]string{\"\", \"枯藤\", \"树昏鸦\"} FAIL exit status 1 FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:4","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"子测试 看起来都挺不错的，但是如果测试用例比较多的时候，我们是没办法一眼看出来具体是哪个测试用例失败了。我们可能会想到下面的解决办法 func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \"simple\": {input: \"a🅱️c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, \"wrong sep\": {input: \"a🅱️c\", sep: \",\", want: []string{\"a🅱️c\"} }, \"more sep\": {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, \"leading sep\": {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"枯藤\", \"树昏鸦\"} }, } for name, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"name:%s excepted:%#v, got:%#v\", name, tc.want, got) // 将测试用例的name格式化输出 } } } 上面的做法是能够解决问题的。同时Go1.7+中新增了子测试，我们可以按照如下方式使用t.Run执行子测试： func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \"simple\": {input: \"a🅱️c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, \"wrong sep\": {input: \"a🅱️c\", sep: \",\", want: []string{\"a🅱️c\"} }, \"more sep\": {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, \"leading sep\": {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"枯藤\", \"树昏鸦\"} }, } for name, tc := range tests { t.Run(name, func(t *testing.T) { // 使用t.Run()执行子测试 got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"excepted:%#v, got:%#v\", tc.want, got) } }) } } 此时我们再执行go test命令就能够看到更清晰的输出内容了： split $ go test -v === RUN TestSplit === RUN TestSplit/leading_sep === RUN TestSplit/simple === RUN TestSplit/wrong_sep === RUN TestSplit/more_sep --- FAIL: TestSplit (0.00s) --- FAIL: TestSplit/leading_sep (0.00s) split_test.go:83: excepted:[]string{\"枯藤\", \"树昏鸦\"}, got:[]string{\"\", \"枯藤\", \"树昏鸦\"} --- PASS: TestSplit/simple (0.00s) --- PASS: TestSplit/wrong_sep (0.00s) --- PASS: TestSplit/more_sep (0.00s) FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.006s 这个时候我们要把测试用例中的错误修改回来： func TestSplit(t *testing.T) { ... tests := map[string]test{ // 测试用例使用map存储 \"simple\": {input: \"a🅱️c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, \"wrong sep\": {input: \"a🅱️c\", sep: \",\", want: []string{\"a🅱️c\"} }, \"more sep\": {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, \"leading sep\": {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"\", \"枯藤\", \"树昏鸦\"} }, } ... } 我们都知道可以通过-run=RegExp来指定运行的测试用例，还可以通过/来指定要运行的子测试用例，例如：go test -v -run=Split/simple只会运行simple对应的子测试用例。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:5","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"测试覆盖率 测试覆盖率是你的代码被测试套件覆盖的百分比。通常我们使用的都是语句的覆盖率，也就是在测试中至少被运行一次的代码占总代码的比例。 Go提供内置功能来检查你的代码覆盖率。我们可以使用go test -cover来查看测试覆盖率。例如： split $ go test -cover PASS coverage: 100.0% of statements ok github.com/pprof/studygo/code_demo/test_demo/split 0.005s 从上面的结果可以看到我们的测试用例覆盖了100%的代码。 Go还提供了一个额外的-coverprofile参数，用来将覆盖率相关的记录信息输出到一个文件。例如： split $ go test -cover -coverprofile=c.out PASS coverage: 100.0% of statements ok github.com/pprof/studygo/code_demo/test_demo/split 0.005s 上面的命令会将覆盖率相关的信息输出到当前文件夹下面的c.out文件中，然后我们执行go tool cover -html=c.out，使用cover工具来处理生成的记录信息，该命令会打开本地的浏览器窗口生成一个HTML报告。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:6","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"基准测试 基准测试函数格式 基准测试就是在一定的工作负载之下检测程序性能的一种方法。基准测试的基本格式如下： func BenchmarkName(b *testing.B){ // ... } 基准测试以Benchmark为前缀，需要一个*testing.B类型的参数b，基准测试必须要执行b.N次，这样的测试才有对照性，b.N的值是系统根据实际情况去调整的，从而保证测试的稳定性。 testing.B拥有的方法如下： func (c *B) Error(args ...interface{}) func (c *B) Errorf(format string, args ...interface{}) func (c *B) Fail() func (c *B) FailNow() func (c *B) Failed() bool func (c *B) Fatal(args ...interface{}) func (c *B) Fatalf(format string, args ...interface{}) func (c *B) Log(args ...interface{}) func (c *B) Logf(format string, args ...interface{}) func (c *B) Name() string func (b *B) ReportAllocs() func (b *B) ResetTimer() func (b *B) Run(name string, f func(b *B)) bool func (b *B) RunParallel(body func(*PB)) func (b *B) SetBytes(n int64) func (b *B) SetParallelism(p int) func (c *B) Skip(args ...interface{}) func (c *B) SkipNow() func (c *B) Skipf(format string, args ...interface{}) func (c *B) Skipped() bool func (b *B) StartTimer() func (b *B) StopTimer() 基准测试示例 我们为split包中的Split函数编写基准测试如下： func BenchmarkSplit(b *testing.B) { for i := 0; i \u003c b.N; i++ { Split(\"枯藤老树昏鸦\", \"老\") } } 基准测试并不会默认执行，需要增加-bench参数，所以我们通过执行go test -bench=Split命令执行基准测试，输出结果如下： split $ go test -bench=Split goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 203 ns/op PASS ok github.com/pprof/studygo/code_demo/test_demo/split 2.255s 其中BenchmarkSplit-8表示对Split函数进行基准测试，数字8表示GOMAXPROCS的值，这个对于并发基准测试很重要。10000000和203ns/op表示每次调用Split函数耗时203ns，这个结果是10000000次调用的平均值。 我们还可以为基准测试添加-benchmem参数，来获得内存分配的统计数据。 split $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 215 ns/op 112 B/op 3 allocs/op PASS ok github.com/pprof/studygo/code_demo/test_demo/split 2.394s 其中，112 B/op表示每次操作内存分配了112字节，3 allocs/op则表示每次操作进行了3次内存分配。 我们将我们的Split函数优化如下： func Split(s, sep string) (result []string) { result = make([]string, 0, strings.Count(s, sep)+1) i := strings.Index(s, sep) for i \u003e -1 { result = append(result, s[:i]) s = s[i+len(sep):] // 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return } 这一次我们提前使用make函数将result初始化为一个容量足够大的切片，而不再像之前一样通过调用append函数来追加。我们来看一下这个改进会带来多大的性能提升： split $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 127 ns/op 48 B/op 1 allocs/op PASS ok github.com/pprof/studygo/code_demo/test_demo/split 1.423s 这个使用make函数提前分配内存的改动，减少了2/3的内存分配次数，并且减少了一半的内存分配。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:7","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"x性能比较函数 上面的基准测试只能得到给定操作的绝对耗时，但是在很多性能问题是发生在两个不同操作之间的相对耗时，比如同一个函数处理1000个元素的耗时与处理1万甚至100万个元素的耗时的差别是多少？再或者对于同一个任务究竟使用哪种算法性能最佳？我们通常需要对两个不同算法的实现使用相同的输入来进行基准比较测试。 性能比较函数通常是一个带有参数的函数，被多个不同的Benchmark函数传入不同的值来调用。举个例子如下： func benchmark(b *testing.B, size int){/* ... */} func Benchmark10(b *testing.B){ benchmark(b, 10) } func Benchmark100(b *testing.B){ benchmark(b, 100) } func Benchmark1000(b *testing.B){ benchmark(b, 1000) } 例如我们编写了一个计算斐波那契数列的函数如下： // fib.go // Fib 是一个计算第n个斐波那契数的函数 func Fib(n int) int { if n \u003c 2 { return n } return Fib(n-1) + Fib(n-2) } 我们编写的性能比较函数如下： // fib_test.go func benchmarkFib(b *testing.B, n int) { for i := 0; i \u003c b.N; i++ { Fib(n) } } func BenchmarkFib1(b *testing.B) { benchmarkFib(b, 1) } func BenchmarkFib2(b *testing.B) { benchmarkFib(b, 2) } func BenchmarkFib3(b *testing.B) { benchmarkFib(b, 3) } func BenchmarkFib10(b *testing.B) { benchmarkFib(b, 10) } func BenchmarkFib20(b *testing.B) { benchmarkFib(b, 20) } func BenchmarkFib40(b *testing.B) { benchmarkFib(b, 40) } 运行基准测试： split $ go test -bench=. goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/fib BenchmarkFib1-8 1000000000 2.03 ns/op BenchmarkFib2-8 300000000 5.39 ns/op BenchmarkFib3-8 200000000 9.71 ns/op BenchmarkFib10-8 5000000 325 ns/op BenchmarkFib20-8 30000 42460 ns/op BenchmarkFib40-8 2 638524980 ns/op PASS ok github.com/pprof/studygo/code_demo/test_demo/fib 12.944s 这里需要注意的是，默认情况下，每个基准测试至少运行1秒。如果在Benchmark函数返回时没有到1秒，则b.N的值会按1,2,5,10,20,50，…增加，并且函数再次运行。 最终的BenchmarkFib40只运行了两次，每次运行的平均值只有不到一秒。像这种情况下我们应该可以使用-benchtime标志增加最小基准时间，以产生更准确的结果。例如： split $ go test -bench=Fib40 -benchtime=20s goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/fib BenchmarkFib40-8 50 663205114 ns/op PASS ok github.com/pprof/studygo/code_demo/test_demo/fib 33.849s 这一次BenchmarkFib40函数运行了50次，结果就会更准确一些了。 使用性能比较函数做测试的时候一个容易犯的错误就是把b.N作为输入的大小，例如以下两个例子都是错误的示范： // 错误示范1 func BenchmarkFibWrong(b *testing.B) { for n := 0; n \u003c b.N; n++ { Fib(n) } } // 错误示范2 func BenchmarkFibWrong2(b *testing.B) { Fib(b.N) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:8","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"重置时间 b.ResetTimer之前的处理不会放到执行时间里，也不会输出到报告中，所以可以在之前做一些不计划作为测试报告的操作。例如： func BenchmarkSplit(b *testing.B) { time.Sleep(5 * time.Second) // 假设需要做一些耗时的无关操作 b.ResetTimer() // 重置计时器 for i := 0; i \u003c b.N; i++ { Split(\"枯藤老树昏鸦\", \"老\") } } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:9","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"并行测试 func (b B) RunParallel(body func(PB))会以并行的方式执行给定的基准测试。 RunParallel会创建出多个goroutine，并将b.N分配给这些goroutine执行， 其中goroutine数量的默认值为GOMAXPROCS。用户如果想要增加非CPU受限（non-CPU-bound）基准测试的并行性， 那么可以在RunParallel之前调用SetParallelism 。RunParallel通常会与-cpu标志一同使用。 func BenchmarkSplitParallel(b *testing.B) { // b.SetParallelism(1) // 设置使用的CPU数 b.RunParallel(func(pb *testing.PB) { for pb.Next() { Split(\"枯藤老树昏鸦\", \"老\") } }) } 执行一下基准测试： split $ go test -bench=. goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 131 ns/op BenchmarkSplitParallel-8 50000000 36.1 ns/op PASS ok github.com/pprof/studygo/code_demo/test_demo/split 3.308s 还可以通过在测试命令后添加-cpu参数如go test -bench=. -cpu 1来指定使用的CPU数量。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:10","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"Setup与TearDown 测试程序有时需要在测试之前进行额外的设置（setup）或在测试之后进行拆卸（teardown）。 TestMain 通过在*_test.go文件中定义TestMain函数来可以在测试之前进行额外的设置（setup）或在测试之后进行拆卸（teardown）操作。 如果测试文件包含函数:func TestMain(m *testing.M)那么生成的测试会先调用 TestMain(m)，然后再运行具体测试。TestMain运行在主goroutine中, 可以在调用 m.Run前后做任何设置（setup）和拆卸（teardown）。退出测试的时候应该使用m.Run的返回值作为参数调用os.Exit。 一个使用TestMain来设置Setup和TearDown的示例如下： func TestMain(m *testing.M) { fmt.Println(\"write setup code here...\") // 测试之前的做一些设置 // 如果 TestMain 使用了 flags，这里应该加上flag.Parse() retCode := m.Run() // 执行测试 fmt.Println(\"write teardown code here...\") // 测试之后做一些拆卸工作 os.Exit(retCode) // 退出测试 } 需要注意的是：在调用TestMain时, flag.Parse并没有被调用。所以如果TestMain 依赖于command-line标志 (包括 testing 包的标记), 则应该显示的调用flag.Parse。 子测试的Setup与Teardown 有时候我们可能需要为每个测试集设置Setup与Teardown，也有可能需要为每个子测试设置Setup与Teardown。下面我们定义两个函数工具函数如下： // 测试集的Setup与Teardown func setupTestCase(t *testing.T) func(t *testing.T) { t.Log(\"如有需要在此执行:测试之前的setup\") return func(t *testing.T) { t.Log(\"如有需要在此执行:测试之后的teardown\") } } // 子测试的Setup与Teardown func setupSubTest(t *testing.T) func(t *testing.T) { t.Log(\"如有需要在此执行:子测试之前的setup\") return func(t *testing.T) { t.Log(\"如有需要在此执行:子测试之后的teardown\") } } 使用方式如下： func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \"simple\": {input: \"a🅱️c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, \"wrong sep\": {input: \"a🅱️c\", sep: \",\", want: []string{\"a🅱️c\"} }, \"more sep\": {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, \"leading sep\": {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"\", \"枯藤\", \"树昏鸦\"} }, } teardownTestCase := setupTestCase(t) // 测试之前执行setup操作 defer teardownTestCase(t) // 测试之后执行testdoen操作 for name, tc := range tests { t.Run(name, func(t *testing.T) { // 使用t.Run()执行子测试 teardownSubTest := setupSubTest(t) // 子测试之前执行setup操作 defer teardownSubTest(t) // 测试之后执行testdoen操作 got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"excepted:%#v, got:%#v\", tc.want, got) } }) } } 测试结果如下： split $ go test -v === RUN TestSplit === RUN TestSplit/simple === RUN TestSplit/wrong_sep === RUN TestSplit/more_sep === RUN TestSplit/leading_sep --- PASS: TestSplit (0.00s) split_test.go:71: 如有需要在此执行:测试之前的setup --- PASS: TestSplit/simple (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/wrong_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/more_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/leading_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown split_test.go:73: 如有需要在此执行:测试之后的teardown === RUN ExampleSplit --- PASS: ExampleSplit (0.00s) PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:11","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"示例函数 示例函数的格式 被go test特殊对待的第三种函数就是示例函数，它们的函数名以Example为前缀。它们既没有参数也没有返回值。标准格式如下： func ExampleName() { // ... } 示例函数示例 下面的代码是我们为Split函数编写的一个示例函数： func ExampleSplit() { fmt.Println(split.Split(\"a🅱️c\", \":\")) fmt.Println(split.Split(\"枯藤老树昏鸦\", \"老\")) // Output: // [a b c] // [ 枯藤 树昏鸦] } 为你的代码编写示例代码有如下三个用处： 示例函数能够作为文档直接使用，例如基于web的godoc中能把示例函数与对应的函数或包相关联。 示例函数只要包含了// Output:也是可以通过go test运行的可执行测试。 split $ go test -run Example PASS ok github.com/pprof/studygo/code_demo/test_demo/split 0.006s 示例函数提供了可以直接运行的示例代码，可以直接在golang.org的godoc文档服务器上使用Go Playground运行示例代码。下图为strings.ToUpper函数在Playground的示例函数效果 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:12","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"func ToUpper func ToUpper(s string) string ToUpper returms a copy of the sring s with all Unicode ltters mapped to their upper case. Example: go package main import ( \"fnt\" \"strings\" ) func main() { fmt. Println(strings . ToUpper(\"Gopher\")) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:13","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"压力测试 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:11:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"Go怎么写测试用例 开发程序其中很重要的一点是测试，我们如何保证代码的质量，如何保证每个函数是可运行，运行结果是正确的，又如何保证写出来的代码性能是好的，我们知道单元测试的重点在于发现程序设计或实现的逻辑错误，使问题及早暴露，便于问题的定位解决，而性能测试的重点在于发现程序设计上的一些问题，让线上的程序能够在高并发的情况下还能保持稳定。本小节将带着这一连串的问题来讲解Go语言中如何来实现单元测试和性能测试。 Go语言中自带有一个轻量级的测试框架testing和自带的go test命令来实现单元测试和性能测试，testing框架和其他语言中的测试框架类似，你可以基于这个框架写针对相应函数的测试用例，也可以基于该框架写相应的压力测试用例，那么接下来让我们一一来看一下怎么写。 另外建议安装gotests插件自动生成测试代码: go get -u -v github.com/cweill/gotests/... ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:11:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"如何编写测试用例 由于go test命令只能在一个相应的目录下执行所有文件，所以我们接下来新建一个项目目录gotest,这样我们所有的代码和测试代码都在这个目录下。 接下来我们在该目录下面创建两个文件：gotest.go和gotest_test.go gotest.go:这个文件里面我们是创建了一个包，里面有一个函数实现了除法运算: package gotest import ( \"errors\" ) func Division(a, b float64) (float64, error) { if b == 0 { return 0, errors.New(\"除数不能为0\") } return a / b, nil } gotest_test.go:这是我们的单元测试文件，但是记住下面的这些原则： 文件名必须是_test.go结尾的，这样在执行go test的时候才会执行到相应的代码 你必须import testing这个包 所有的测试用例函数必须是Test开头 测试用例会按照源代码中写的顺序依次执行 测试函数TestXxx()的参数是testing.T，我们可以使用该类型来记录错误或者是测试状态 测试格式：func TestXxx (t *testing.T),Xxx部分可以为任意的字母数字的组合，但是首字母不能是小写字母[a-z]，例如Testintdiv是错误的函数名。 函数中通过调用testing.T的Error, Errorf, FailNow, Fatal, FatalIf方法，说明测试不通过，调用Log方法用来记录测试的信息。 下面是我们的测试用例的代码： package gotest import ( \"testing\" ) func Test_Division_1(t *testing.T) { if i, e := Division(6, 2); i != 3 || e != nil { //try a unit test on function t.Error(\"除法函数测试没通过\") // 如果不是如预期的那么就报错 } else { t.Log(\"第一个测试通过了\") //记录一些你期望记录的信息 } } func Test_Division_2(t *testing.T) { t.Error(\"就是不通过\") } 我们在项目目录下面执行go test,就会显示如下信息： --- FAIL: Test_Division_2 (0.00 seconds) gotest_test.go:16: 就是不通过 FAIL exit status 1 FAIL gotest 0.013s 从这个结果显示测试没有通过，因为在第二个测试函数中我们写死了测试不通过的代码t.Error，那么我们的第一个函数执行的情况怎么样呢？默认情况下执行go test是不会显示测试通过的信息的，我们需要带上参数go test -v，这样就会显示如下信息： === RUN Test_Division_1 --- PASS: Test_Division_1 (0.00 seconds) gotest_test.go:11: 第一个测试通过了 === RUN Test_Division_2 --- FAIL: Test_Division_2 (0.00 seconds) gotest_test.go:16: 就是不通过 FAIL exit status 1 FAIL gotest 0.012s 上面的输出详细的展示了这个测试的过程，我们看到测试函数1Test_Division_1测试通过，而测试函数2Test_Division_2测试失败了，最后得出结论测试不通过。接下来我们把测试函数2修改成如下代码： func Test_Division_2(t *testing.T) { if _, e := Division(6, 0); e == nil { //try a unit test on function t.Error(\"Division did not work as expected.\") // 如果不是如预期的那么就报错 } else { t.Log(\"one test passed.\", e) //记录一些你期望记录的信息 } } 然后我们执行go test -v，就显示如下信息，测试通过了： === RUN Test_Division_1 --- PASS: Test_Division_1 (0.00 seconds) gotest_test.go:11: 第一个测试通过了 === RUN Test_Division_2 --- PASS: Test_Division_2 (0.00 seconds) gotest_test.go:20: one test passed. 除数不能为0 PASS ok gotest 0.013s //一般 go test -v -run funcName fileName.go go test -v -cover -run funcName fileName.go ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:11:2","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"如何编写压力测试 压力测试用来检测函数(方法）的性能，和编写单元功能测试的方法类似,此处不再赘述，但需要注意以下几点： 压力测试用例必须遵循如下格式，其中XXX可以是任意字母数字的组合，但是首字母不能是小写字母 func BenchmarkXXX(b *testing.B) { ... } go test不会默认执行压力测试的函数，如果要执行压力测试需要带上参数-test.bench，语法:-test.bench=”test_name_regex”,例如go test -test.bench=\".*“表示测试全部的压力测试函数 在压力测试用例中,请记得在循环体内使用testing.B.N,以使测试可以正常的运行 文件名也必须以_test.go结尾 下面我们新建一个压力测试文件webbench_test.go，代码如下所示： import ( \"testing\" ) func Benchmark_Division(b *testing.B) { for i := 0; i \u003c b.N; i++ { //use b.N for looping Division(4, 5) } } func Benchmark_TimeConsumingFunction(b *testing.B) { b.StopTimer() //调用该函数停止压力测试的时间计数 //做一些初始化的工作,例如读取文件数据,数据库连接之类的, //这样这些时间不影响我们测试函数本身的性能 b.StartTimer() //重新开始时间 for i := 0; i \u003c b.N; i++ { Division(4, 5) } } 我们执行命令go test webbench_test.go -test.bench=”.*\"，可以看到如下结果： Benchmark_Division-4 500000000 7.76 ns/op 456 B/op 14 allocs/op Benchmark_TimeConsumingFunction-4 500000000 7.80 ns/op 224 B/op 4 allocs/op PASS ok gotest 9.364s 上面的结果显示我们没有执行任何TestXXX的单元测试函数，显示的结果只执行了压力测试函数，第一条显示了Benchmark_Division执行了500000000次，每次的执行平均时间是7.76纳秒，第二条显示了Benchmark_TimeConsumingFunction执行了500000000，每次的平均执行时间是7.80纳秒。最后一条显示总共的执行时间。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:11:3","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"BDD Behavior Driven Development行为驱动开发 行为驱动开发（Behavior-Driven Development）（简写BDD），在软件工程中，BDD是一种敏捷软件开发的技术。行为驱动开发(BDD)是测试驱动开发的延伸，开发使用简单的，特定于领域的脚本语言。这些DSL将结构化自然语言语句转换为可执行测试。结果是与给定功能的验收标准以及用于验证该功能的测试之间的关系更密切。因此，它一般是测试驱动开发(TDD)测试的自然延伸。（摘自百度百科） BDD in Go 一个测试框架 项⽬⽹站 https://github.com/smartystreets/goconvey 安装 go get -u github.com/smartystreets/goconvey/convey 启动 WEB UI $GOPATH/bin/goconvey package testing import ( \"testing\" . \"github.com/smartystreets/goconvey/convey\" ) func TestSpec(t *testing.T) { // Only pass t into top-level Convey calls Convey(\"Given 2 even numbers\", t, func() { a := 3 b := 4 Convey(\"When add the two numbers\", func() { c := a + b Convey(\"Then the result is still even\", func() { So(c%2, ShouldEqual, 0) }) }) }) } convey这个pkg还提供了一个web界面，在gopath下的bin目录里面有二进制程序goconvey。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:12:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 流程控制 ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:0:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"if Go 编程语言中 if 语句的语法如下： 可省略条件表达式括号。 持初始化语句，可定义代码块局部变量，也叫做if语句的自用变量，而这自用变量作用域在其声明所在的代码块，但不能在该if语句块之外。 代码块左花括号必须在条件表达式尾部。 可嵌套 if 布尔表达式 { /* 在布尔表达式为 true 时执行 */ } if分支有三种：单分支、二分支、多分支，要减少多分支结构，甚至是二分支结构的使用。这样的代码更优雅、简洁、易读、易维护。单分支结构的使用被称为符合“快乐路径”原则。 Go不支持三元操作符(三目运算符) “a \u003e b ? a : b” ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:1:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"switch Golang switch 分支表达式可以是任意类型，不限于常量。可省略 break，默认自动终止。 可以同时测试多个可能符合条件的值，使用逗号分割它们，例如：case val1, val2, val3。 switch 语句支持声明临时变量。 如： switch { case grade == \"A\" : fmt.Printf(\"优秀!\\n\" ) case grade == \"B\", grade == \"C\" : fmt.Printf(\"良好\\n\" ) case grade == \"D\" : fmt.Printf(\"及格\\n\" ) case grade == \"F\": fmt.Printf(\"不及格\\n\" ) default: fmt.Printf(\"差\\n\" ) } ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:2:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"Type Switch switch 语句还可以被用于 type-switch 来判断某个 interface 变量中实际存储的变量类型。 switch x.(type){ case type1: statement(s) case type2: statement(s) /* 你可以定义任意个数的case */ default: /* 可选 */ statement(s) } ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:2:1","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"select select 语句类似于 switch 语句，但是select会随机执行一个可运行的case。如果没有case可运行，且没有default语句，它将阻塞，直到有case可运行。 每个case都必须是一个通信 所有channel表达式都会被求值 所有被发送的表达式都会被求值 如果任意某个通信可以进行，它就执行；其他被忽略。 如果有多个case都可以运行，Select会随机公平地选出一个执行。其他不会执行。 否则： 如果有default子句，则执行该语句。 如果没有default字句，select将阻塞，直到某个通信可以运行；Go不会重新对channel或值进行求值。 select可以监听channel的数据流动 select的用法与switch语法非常类似，由select开始的一个新的选择块，每个选择条件由case语句来描述 与switch语句可以选择任何使用相等比较的条件相比，select有比较多的限制，其中最大的一条限制就是每个case语句里必须是一个IO操作 select { //不停的在这里检测 case \u003c-chanl : //检测有没有数据可以读 //如果chanl成功读取到数据，则进行该case处理语句 case chan2 \u003c- 1 : //检测有没有可以写 //如果成功向chan2写入数据，则进行该case处理语句 //假如没有default，那么在以上两个条件都不成立的情况下，就会在此阻塞//一般default会不写在里面，select中的default子句总是可运行的，因为会很消耗CPU资源 default: //如果以上都没有符合条件，那么则进行default处理流程 } 在一个select语句中，Go会按顺序从头到尾评估每一个发送和接收的语句。 如果其中的任意一个语句可以继续执行（即没有被阻塞），那么就从那些可以执行的语句中任意选择一条来使用。 如果没有任意一条语句可以执行（即所有的通道都被阻塞），那么有两种可能的情况： ①如果给出了default语句，那么就会执行default的流程，同时程序的执行会从select语句后的语句中恢复。 ②如果没有default语句，那么select语句将被阻塞，直到至少有一个case可以进行下去。 ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:3:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"基本使用 select是Go中的一个控制结构，类似于switch语句，用于处理异步IO操作。select会监听case语句中channel的读写操作，当case中channel读写操作为非阻塞状态（即能读写）时，将会触发相应的动作。 select中的case语句必须是一个channel操作 select中的default子句总是可运行的。 如果有多个case都可以运行，select会随机公平地选出一个执行，其他不会执行。 如果没有可运行的case语句，且有default语句，那么就会执行default的动作。 如果没有可运行的case语句，且没有default语句，select将阻塞，直到某个case通信可以运行 例如： package main import \"fmt\" func main() { var c1, c2, c3 chan int var i1, i2 int select { case i1 = \u003c-c1: fmt.Printf(\"received \", i1, \" from c1\\n\") case c2 \u003c- i2: fmt.Printf(\"sent \", i2, \" to c2\\n\") case i3, ok := (\u003c-c3): // same as: i3, ok := \u003c-c3 if ok { fmt.Printf(\"received \", i3, \" from c3\\n\") } else { fmt.Printf(\"c3 is closed\\n\") } default: fmt.Printf(\"no communication\\n\") } } //输出：no communication ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:3:1","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"典型用法 超时判断： //比如在下面的场景中，使用全局resChan来接受response，如果时间超过3S,resChan中还没有数据返回，则第二条case将执行 var resChan = make(chan int) // do request func test() { select { case data := \u003c-resChan: doData(data) case \u003c-time.After(time.Second * 3): fmt.Println(\"request time out\") } } func doData(data int) { //... } 退出 //主线程（协程）中如下： var shouldQuit=make(chan struct{}) fun main(){ { //loop } //...out of the loop select { case \u003c-c.shouldQuit: cleanUp() return default: } //... } //再另外一个协程中，如果运行遇到非法操作或不可处理的错误，就向shouldQuit发送数据通知程序停止运行 close(shouldQuit) 判断channel是否阻塞 //在某些情况下是存在不希望channel缓存满了的需求的，可以用如下方法判断 ch := make (chan int, 5) //... data：=0 select { case ch \u003c- data: default: //做相应操作，比如丢弃data。视需求而定 } ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:3:2","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"for Go语言的For循环有3中形式，只有其中的一种使用分号。 for init; condition; post { } for condition { } for { } init： 一般为赋值表达式，给控制变量赋初值； condition： 关系表达式或逻辑表达式，循环控制条件； post： 一般为赋值表达式，给控制变量增量或减量。 for语句执行过程如下： ①先对表达式 init 赋初值； ②判别赋值表达式 init 是否满足给定 condition 条件，若其值为真，满足循环条件，则执行循环体内语句，然后执行 post，进入第二次循环，再判别 condition；否则判断 condition 的值为假，不满足条件，就终止for循环，执行循环体外语句。 可嵌套，可无限循环。 ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:4:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"range Golang range类似迭代器操作，返回 (索引, 值) 或 (键, 值)。 for 循环的 range 格式可以对 slice、map、数组、字符串等进行迭代循环。格式如下： for key, value := range oldMap { newMap[key] = value } ****注意range会复制对象： package main import \"fmt\" func main() { a := [3]int{0, 1, 2} for i, v := range a { // index、value 都是从复制品中取出。 if i == 0 { // 在修改前，我们先修改原数组。 a[1], a[2] = 999, 999 fmt.Println(a) // 确认修改有效，输出 [0, 999, 999]。 } a[i] = v + 100 // 使用复制品中取出的 value 修改原数组。 } fmt.Println(a) // 输出 [100, 101, 102]。 } output: [0 999 999] [100 101 102] 这说明开始执行range时就已经保存了当下的数组，所以在循环里修改数组也不会改动遍历时的使用的数组的值。 改用引用类型，其底层数据不会被复制： package main func main() { s := []int{1, 2, 3, 4, 5} for i, v := range s { // 复制 struct slice { pointer, len, cap }。 if i == 0 { s = s[:3] // 对 slice 的修改，不会影响 range。 s[2] = 100 // 对底层数据的修改。 } println(i, v) } } output: 0 1 1 2 2 100 3 4 4 5 另外两种引用类型 map、channel 也能应用于for range遍历上，是指针包装，而不像 slice 是 struct。 for 和 for range有什么区别? 主要是使用场景不同 for可以 遍历array和slice 遍历key为整型递增的map 遍历string for range可以完成所有for可以做的事情，却能做到for不能做的，包括 遍历key为string类型的map并同时获取key和value 遍历channel ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:5:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"Goto Break Continue 循环控制语句： Goto、Break、Continue 三个语句都可以配合标签(label)使用 标签名区分大小写，定以后若不使用会造成编译错误 continue、break配合标签(label)可用于多层循环跳出 goto是调整执行位置，与continue、break配合标签(label)的结果并不相同 ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:6:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:0:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"数组 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:1:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"数组基础 数组是存放在连续内存空间上的相同类型数据的集合。 因为数组的在内存空间的地址是连续的，所以我们在删除或者增添元素的时候，就难免要移动其他元素的地址。 数组元素无法删除，只能覆盖。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:1:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"二分查找 给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target ，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。 有序数组、无重复元素便可想到用二分法查找。注意区间左闭右闭还是左闭右开。 简单实现： package main func search(nums []int, target int) int { left := 0 right := len(nums) - 1 for left \u003c= right { middle := left + (right - left)/2 if nums[middle] \u003e target { right = middle - 1 }else if nums[middle] \u003c target { left = middle + 1 }else{ return middle; } } return -1 } func search(nums []int,t int)int{ left,middle:=0 right:=len(nums) for left\u003c=right{ num=(left+right)/2 if nums[num]==t{ return num } if nums[num]\u003et{ right=num-1 }else{ left=num+1 } } } func search(nums []int,t int)int{ if middle:=len(nums)/2;nums[middle]==t{ return middle } if len(nums)==1{ return -1 } if nums[middle]\u003et{ return search(nums[:middle],t) }else{ return search(nums[middle+1:]) } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:1:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"移除元素 给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。 不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并原地修改输入数组。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 示例 1: 给定 nums = [3,2,2,3], val = 3, 函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 给定 nums = [0,1,2,2,3,0,4,2], val = 2, 函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。 你不需要考虑数组中超出新长度后面的元素 双指针法（快慢指针法）： 通过一个快指针和慢指针在一个for循环下完成两个for循环的工作。 func removeElement(nums []int, val int) int { length:=len(nums) res:=0 for i:=0;i\u003clength;i++{ if nums[i]!=val { nums[res]=nums[i] res++ } } return res } func removeElement(nums []int, val int) int { num := 0 for i, v := range nums { if v == val { num++ } else { nums[i-num] = v } } return len(nums)-num } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:1:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"有序数组的平方 给你一个按 非递减顺序 排序的整数数组 nums，返回 每个数字的平方 组成的新数组，要求也按 非递减顺序 排序。 示例 1： 输入：nums = [-4,-1,0,3,10] 输出：[0,1,9,16,100] 解释：平方后，数组变为 [16,1,0,9,100]，排序后，数组变为 [0,1,9,16,100] 示例 2： 输入：nums = [-7,-3,2,3,11] 输出：[4,9,9,49,121] 双指针法： func sortedSquares(nums []int) []int { n := len(nums) i, j, k := 0, n-1, n-1 ans := make([]int, n) for i \u003c= j { lm, rm := nums[i]*nums[i], nums[j]*nums[j] if lm \u003e rm { ans[k] = lm i++ } else { ans[k] = rm j-- } k-- } return ans } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:1:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"长度最小的子数组 给定一个含有 n 个正整数的数组和一个正整数 s ，找出该数组中满足其和 ≥ s 的长度最小的 连续 子数组，并返回其长度。如果不存在符合条件的子数组，返回 0。 示例： 输入：s = 7, nums = [2,3,1,2,4,3] 输出：2 解释：子数组 [4,3] 是该条件下的长度最小的子数组 滑动窗口 func minSubArrayLen(target int, nums []int) int { i := 0 l := len(nums) // 数组长度 sum := 0 // 子数组之和 result := l + 1 // 初始化返回长度为l+1，目的是为了判断“不存在符合条件的子数组，返回0”的情况 for j := 0; j \u003c l; j++ { sum += nums[j] for sum \u003e= target { subLength := j - i + 1 if subLength \u003c result { result = subLength } sum -= nums[i] i++ } } if result == l+1 { return 0 } else { return result } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:1:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"螺旋矩阵II 给定一个正整数 n，生成一个包含 1 到 n^2 所有元素，且元素按顺时针顺序螺旋排列的正方形矩阵。 示例: 输入: 3 输出: [ [ 1, 2, 3 ], [ 8, 9, 4 ], [ 7, 6, 5 ] ] #思路 模拟行为： func generateMatrix(n int) [][]int { top, bottom := 0, n-1 left, right := 0, n-1 num := 1 tar := n * n matrix := make([][]int, n) for i := 0; i \u003c n; i++ { matrix[i] = make([]int, n) } for num \u003c= tar { for i := left; i \u003c= right; i++ { matrix[top][i] = num num++ } top++ for i := top; i \u003c= bottom; i++ { matrix[i][right] = num num++ } right-- for i := right; i \u003e= left; i-- { matrix[bottom][i] = num num++ } bottom-- for i := bottom; i \u003e= top; i-- { matrix[i][left] = num num++ } left++ } return matrix } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:1:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"总结 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:1:7","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"链表 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"链表理论基础 循环链表可以用来解决约瑟夫环问题。 链表和数组对比： 数组 插入删除时间复杂度：$O(n)$ 查询时间复杂度：$O(1)$ 适用场景：数据量固定，频繁查询，较少增删 链表 插入删除时间复杂度：$O(1)$ 查询时间复杂度：$O(n)$ 适用场景：数据量不固定，频繁增删，较少查询 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"移除链表元素 题意：删除链表中等于给定值 val 的所有节点。 示例 1： 输入：head = [1,2,6,3,4,5,6], val = 6 输出：[1,2,3,4,5] 示例 2： 输入：head = [], val = 1 输出：[] 示例 3： 输入：head = [7,7,7,7], val = 7 输出：[] /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeElements(head *ListNode, val int) *ListNode { dummyHead := \u0026ListNode{} dummyHead.Next = head cur := dummyHead for cur != nil \u0026\u0026 cur.Next != nil { if cur.Next.Val == val { cur.Next = cur.Next.Next } else { cur = cur.Next } } return dummyHead.Next } //如果是c或者c++要在删除后free或者delete掉节点释放内存，java,python,go都会自动回收 另外，移除头节点的话，如果没有虚拟头节点，将头节点往后移一个节点，有虚拟头节点就一般删除节点即可。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"设计链表 设计五个接口： 获取链表第index个节点的数值 在链表的最前面插入一个节点 在链表的最后面插入一个节点 在链表第index个节点前面插入一个节点 删除链表的第index个节点 链表操作的两种方式： 直接使用原来的链表来进行操作。 设置一个虚拟头结点在进行操作。（更方便一点） 题意： 在链表类中实现这些功能： get(index)：获取链表中第 index 个节点的值。如果索引无效，则返回-1。 addAtHead(val)：在链表的第一个元素之前添加一个值为 val 的节点。插入后，新节点将成为链表的第一个节点。 addAtTail(val)：将值为 val 的节点追加到链表的最后一个元素。 addAtIndex(index,val)：在链表中的第 index 个节点之前添加值为 val 的节点。如果 index 等于链表的长度，则该节点将附加到链表的末尾。如果 index 大于链表长度，则不会插入节点。如果index小于0，则在头部插入节点。 deleteAtIndex(index)：如果索引 index 有效，则删除链表中的第 index 个节点。 //循环双链表 type MyLinkedList struct { dummy *Node } type Node struct { Val int Next *Node Pre *Node } //仅保存哑节点，pre-\u003e rear, next-\u003e head /** Initialize your data structure here. */ func Constructor() MyLinkedList { rear := \u0026Node{ Val: -1, Next: nil, Pre: nil, } rear.Next = rear rear.Pre = rear return MyLinkedList{rear} } /** Get the value of the index-th node in the linked list. If the index is invalid, return -1. */ func (this *MyLinkedList) Get(index int) int { head := this.dummy.Next //head == this, 遍历完全 for head != this.dummy \u0026\u0026 index \u003e 0 { index-- head = head.Next } //否则, head == this, 索引无效 if 0 != index { return -1 } return head.Val } /** Add a node of value val before the first element of the linked list. After the insertion, the new node will be the first node of the linked list. */ func (this *MyLinkedList) AddAtHead(val int) { dummy := this.dummy node := \u0026Node{ Val: val, //head.Next指向原头节点 Next: dummy.Next, //head.Pre 指向哑节点 Pre: dummy, } //更新原头节点 dummy.Next.Pre = node //更新哑节点 dummy.Next = node //以上两步不能反 } /** Append a node of value val to the last element of the linked list. */ func (this *MyLinkedList) AddAtTail(val int) { dummy := this.dummy rear := \u0026Node{ Val: val, //rear.Next = dummy(哑节点) Next: dummy, //rear.Pre = ori_rear Pre: dummy.Pre, } //ori_rear.Next = rear dummy.Pre.Next = rear //update dummy dummy.Pre = rear //以上两步不能反 } /** Add a node of value val before the index-th node in the linked list. If index equals to the length of linked list, the node will be appended to the end of linked list. If index is greater than the length, the node will not be inserted. */ func (this *MyLinkedList) AddAtIndex(index int, val int) { head := this.dummy.Next //head = MyLinkedList[index] for head != this.dummy \u0026\u0026 index \u003e 0 { head = head.Next index-- } if index \u003e 0 { return } node := \u0026Node{ Val: val, //node.Next = MyLinkedList[index] Next: head, //node.Pre = MyLinkedList[index-1] Pre: head.Pre, } //MyLinkedList[index-1].Next = node head.Pre.Next = node //MyLinkedList[index].Pre = node head.Pre = node //以上两步不能反 } /** Delete the index-th node in the linked list, if the index is valid. */ func (this *MyLinkedList) DeleteAtIndex(index int) { //链表为空 if this.dummy.Next == this.dummy { return } head := this.dummy.Next //head = MyLinkedList[index] for head.Next != this.dummy \u0026\u0026 index \u003e 0 { head = head.Next index-- } //验证index有效 if index == 0 { //MyLinkedList[index].Pre = index[index-2] head.Next.Pre = head.Pre //MyLinedList[index-2].Next = index[index] head.Pre.Next = head.Next //以上两步顺序无所谓 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"翻转链表 //双指针 func reverseList(head *ListNode) *ListNode { var pre *ListNode cur := head for cur != nil { next := cur.Next cur.Next = pre pre = cur cur = next } return pre } //递归 func reverseList(head *ListNode) *ListNode { return help(nil, head) } func help(pre, head *ListNode)*ListNode{ if head == nil { return pre } next := head.Next head.Next = pre return help(head, next) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"两两交换链表中的节点 给定一个链表，两两交换其中相邻的节点，并返回交换后的链表。 你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。 正常模拟，使用虚拟头节点。 func swapPairs(head *ListNode) *ListNode { dummy := \u0026ListNode{ Next: head, } //head=list[i] //pre=list[i-1] pre := dummy for head != nil \u0026\u0026 head.Next != nil { pre.Next = head.Next next := head.Next.Next head.Next.Next = head head.Next = next //pre=list[(i+2)-1] pre = head //head=list[(i+2)] head = next } return dummy.Next } // 递归版本 func swapPairs(head *ListNode) *ListNode { if head == nil || head.Next == nil { return head } next := head.Next head.Next = swapPairs(next.Next) next.Next = head return next } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"删除链表的倒数第N个节点 给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点。 进阶：你能尝试使用一趟扫描实现吗？ 双指针很好做： /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { dummyHead := \u0026ListNode{} dummyHead.Next = head cur := head prev := dummyHead i := 1 for cur != nil { cur = cur.Next if i \u003e n { prev = prev.Next } i++ } prev.Next = prev.Next.Next return dummyHead.Next } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"链表相交 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 null 。 图示两个链表在节点 c1 开始相交： 题目数据 保证 整个链式结构中不存在环。 注意，函数返回结果后，链表必须 保持其原始结构 。 func getIntersectionNode(headA, headB *ListNode) *ListNode { curA := headA curB := headB lenA, lenB := 0, 0 // 求A，B的长度 for curA != nil { curA = curA.Next lenA++ } for curB != nil { curB = curB.Next lenB++ } var step int var fast, slow *ListNode // 请求长度差，并且让更长的链表先走相差的长度 if lenA \u003e lenB { step = lenA - lenB fast, slow = headA, headB } else { step = lenB - lenA fast, slow = headB, headA } for i:=0; i \u003c step; i++ { fast = fast.Next } // 遍历两个链表遇到相同则跳出遍历 for fast != slow { fast = fast.Next slow = slow.Next } return fast } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:7","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"环形链表II 题意： 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 为了表示给定链表中的环，使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。 说明：不允许修改给定的链表。 判断是否有环：快慢指针法 如何确定环的入口：一个简单的数学题 func detectCycle(head *ListNode) *ListNode { slow, fast := head, head for fast != nil \u0026\u0026 fast.Next != nil { slow = slow.Next fast = fast.Next.Next if slow == fast { for slow != head { slow = slow.Next head = head.Next } return head } } return nil } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:8","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"如何实现LRU缓存淘汰算法? 缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 CPU 缓存、数据库缓存、浏览器缓存等等。 缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用策略 LRU（Least Recently Used）。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:9","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"链表结构 我们先从底层的存储结构上来看一看。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。 链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。我们首先来看最简单、最常用的单链表。 其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。 循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。 接下来我们再来看一个稍微复杂的，在实际的软件开发中，也更加常用的链表结构：双向链表。 相比单链表，双向链表适合解决哪种问题呢？ 从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。 对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗 如何基于链表实现 LRU 缓存淘汰算法？我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。 如果此数据没有在缓存链表中，又可以分为两种情况： 如果此时缓存未满，则将此结点直接插入到链表的头部；如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。 现在我们来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为 O(n)。实际上，我们可以继续优化这个实现思路，比如引入散列表（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)。因为要涉及我们还没有讲到的数据结构，所以这个优化方案，我现在就不详细说了，等讲到散列表的时候，我会再拿出来讲。 除了基于链表的实现思路，实际上还可以用数组来实现 LRU 缓存淘汰策略。如何利用数组实现 LRU 缓存淘汰策略呢？ ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:10","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"哈希表 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"哈希表理论基础 哈希表是根据关键码的值而直接进行访问的数据结构，比如数组、map（映射）、set（集合）。 一般用来快速判断一个元素是否出现在集合里。 比如把字符串映射为索引的例子： 通过哈希函数/hashCode将字符串转化为数值 如果得到的数值大于哈希表的大小了，取模，保证所有字符串映射到哈希表上。 如果字符串数量都大于哈希表的大小了，会出现同一索引不同字符串的情况。也称，哈希碰撞。 解决哈希碰撞： 拉链法 发生冲突的元素用链表存储 要选择适当的哈希表的大小，这样既不会因为数组空值而浪费大量内存，也不会因为链表太长而在查找上浪费太多时间。 线性探测法 保证tableSize大于dataSize，避免碰撞。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"有效的字母异位词 给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。 示例 1: 输入: s = “anagram”, t = “nagaram” 输出: true 示例 2: 输入: s = “rat”, t = “car” 输出: false 说明: 你可以假设字符串只包含小写字母。 func isAnagram(s string, t string) bool { if len(s)!=len(t){ return false } exists := make(map[byte]int) for i:=0;i\u003clen(s);i++{ if v,ok:=exists[s[i]];v\u003e=0\u0026\u0026ok{ exists[s[i]]=v+1 }else{ exists[s[i]]=1 } } for i:=0;i\u003clen(t);i++{ if v,ok:=exists[t[i]];v\u003e=1\u0026\u0026ok{ exists[t[i]]=v-1 }else{ return false } } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"两个数组的交集 使用数组来做哈希的题目，是因为题目都限制了数值的大小。 而这道题目没有限制数值的大小，就无法使用数组来做哈希表了。 func intersection(nums1 []int, nums2 []int) []int { m := make(map[int]int) for _, v := range nums1 { m[v] = 1 } var res []int // 利用count\u003e0，实现重复值只拿一次放入返回结果中 for _, v := range nums2 { if count, ok := m[v]; ok \u0026\u0026 count \u003e 0 { res = append(res, v) m[v]-- } } return res } //优化版，利用set，减少count统计 func intersection(nums1 []int, nums2 []int) []int { set:=make(map[int]struct{},0) res:=make([]int,0) for _,v:=range nums1{ if _,ok:=set[v];!ok{ set[v]=struct{}{} } } for _,v:=range nums2{ //如果存在于上一个数组中，则加入结果集，并清空该set值 if _,ok:=set[v];ok{ res=append(res,v) delete(set, v) } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"快乐数 编写一个算法来判断一个数 n 是不是快乐数。 「快乐数」定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是 无限循环 但始终变不到 1。如果 可以变为 1，那么这个数就是快乐数。 如果 n 是快乐数就返回 True ；不是，则返回 False 。 示例： 输入：19 输出：true 解释： 1^2 + 9^2 = 82 8^2 + 2^2 = 68 6^2 + 8^2 = 100 1^2 + 0^2 + 0^2 = 1 func isHappy(n int) bool { m := make(map[int]bool) for n != 1 \u0026\u0026 !m[n] { n, m[n] = getSum(n), true } return n == 1 } func getSum(n int) int { sum := 0 for n \u003e 0 { sum += (n % 10) * (n % 10) n = n / 10 } return sum } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"两数之和 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素不能使用两遍。 示例: 给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9 所以返回 [0, 1] func twoSum(nums []int, target int) []int { for k1, _ := range nums { for k2 := k1 + 1; k2 \u003c len(nums); k2++ { if target == nums[k1] + nums[k2] { return []int{k1, k2} } } } return []int{} } // 使用map方式解题，降低时间复杂度 func twoSum(nums []int, target int) []int { m := make(map[int]int) for index, val := range nums { if preIndex, ok := m[target-val]; ok { return []int{preIndex, index} } else { m[val] = index } } return []int{} } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"四数相加II 给定四个包含整数的数组列表 A , B , C , D ,计算有多少个元组 (i, j, k, l) ，使得 A[i] + B[j] + C[k] + D[l] = 0。 为了使问题简单化，所有的 A, B, C, D 具有相同的长度 N，且 0 ≤ N ≤ 500 。所有整数的范围在 -2^28 到 2^28 - 1 之间，最终结果不会超过 2^31 - 1 。 例如: 输入: A = [ 1, 2] B = [-2,-1] C = [-1, 2] D = [ 0, 2] 输出: 2 解释: 两个元组如下: (0, 0, 0, 1) -\u003e A[0] + B[0] + C[0] + D[1] = 1 + (-2) + (-1) + 2 = 0 (1, 1, 0, 0) -\u003e A[1] + B[1] + C[0] + D[0] = 2 + (-1) + (-1) + 0 = 0 func fourSumCount(nums1 []int, nums2 []int, nums3 []int, nums4 []int) int { m := make(map[int]int) count := 0 for _, v1 := range nums1 { for _, v2 := range nums2 { m[v1+v2]++ } } for _, v3 := range nums3 { for _, v4 := range nums4 { count += m[-v3-v4] } } return count } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"赎金信 给定一个赎金信 (ransom) 字符串和一个杂志(magazine)字符串，判断第一个字符串 ransom 能不能由第二个字符串 magazines 里面的字符构成。如果可以构成，返回 true ；否则返回 false。 (题目说明：为了不暴露赎金信字迹，要从杂志上搜索各个需要的字母，组成单词来表达意思。杂志字符串中的每个字符只能在赎金信字符串中使用一次。) 注意： 你可以假设两个字符串均只含有小写字母。 canConstruct(“a”, “b”) -\u003e false canConstruct(“aa”, “ab”) -\u003e false canConstruct(“aa”, “aab”) -\u003e true func canConstruct(ransomNote string, magazine string) bool { record := make([]int, 26) for _, v := range magazine { record[v-'a']++ } for _, v := range ransomNote { record[v-'a']-- if record[v-'a'] \u003c 0 { return false } } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:7","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"三数之和 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有满足条件且不重复的三元组。 注意： 答案中不可以包含重复的三元组。 示例： 给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ] 解法：哈希、双指针（更高效） func threeSum(nums []int)[][]int{ sort.Ints(nums) res:=[][]int{} for i:=0;i\u003clen(nums)-2;i++{ n1:=nums[i] if n1\u003e0{ break } if i\u003e0\u0026\u0026n1==nums[i-1]{ continue } l,r:=i+1,len(nums)-1 for l\u003cr{ n2,n3:=nums[l],nums[r] if n1+n2+n3==0{ res=append(res,[]int{n1,n2,n3}) for l\u003cr\u0026\u0026nums[l]==n2{ l++ } for l\u003cr\u0026\u0026nums[r]==n3{ r-- } }else if n1+n2+n3\u003c0{ l++ }else { r-- } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:8","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"四数之和 题意：给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组。 注意： 答案中不可以包含重复的四元组。 示例： 给定数组 nums = [1, 0, -1, 0, -2, 2]，和 target = 0。 满足要求的四元组集合为： [ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2] ] func fourSum(nums []int, target int) [][]int { if len(nums) \u003c 4 { return nil } sort.Ints(nums) var res [][]int for i := 0; i \u003c len(nums)-3; i++ { n1 := nums[i] // if n1 \u003e target { // 不能这样写,因为可能是负数 // break // } if i \u003e 0 \u0026\u0026 n1 == nums[i-1] { continue } for j := i + 1; j \u003c len(nums)-2; j++ { n2 := nums[j] if j \u003e i+1 \u0026\u0026 n2 == nums[j-1] { continue } l := j + 1 r := len(nums) - 1 for l \u003c r { n3 := nums[l] n4 := nums[r] sum := n1 + n2 + n3 + n4 if sum \u003c target { l++ } else if sum \u003e target { r-- } else { res = append(res, []int{n1, n2, n3, n4}) for l \u003c r \u0026\u0026 n3 == nums[l+1] { // 去重 l++ } for l \u003c r \u0026\u0026 n4 == nums[r-1] { // 去重 r-- } // 找到答案时,双指针同时靠近 r-- l++ } } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:9","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"字符串 库函数是工具，基础更重要。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:4:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"反转字符串 I 和反转链表相同，都用双指针法。 func reverseString(s []byte) { left:=0 right:=len(s)-1 for left\u003cright{ s[left],s[right]=s[right],s[left] left++ right-- } } II 给定一个字符串 s 和一个整数 k，你需要对从字符串开头算起的每隔 2k 个字符的前 k 个字符进行反转。 如果剩余字符少于 k 个，则将剩余字符全部反转。 如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。 示例: 输入: s = \"abcdefg\", k = 2 输出: \"bacdfeg\" func reverseStr(s string, k int) string { ss := []byte(s) length := len(s) for i := 0; i \u003c length; i += 2 * k { if i + k \u003c= length { reverse(ss[i:i+k]) } else { reverse(ss[i:length]) } } return string(ss) } func reverse(b []byte) { left := 0 right := len(b) - 1 for left \u003c right { b[left], b[right] = b[right], b[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:4:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"替换空格 请实现一个函数，把字符串 s 中的每个空格替换成\"%20\"。 示例 1： 输入：s = “We are happy.” 输出：“We%20are%20happy.” // 遍历添加 func replaceSpace(s string) string { b := []byte(s) result := make([]byte, 0) for i := 0; i \u003c len(b); i++ { if b[i] == ' ' { result = append(result, []byte(\"%20\")...) } else { result = append(result, b[i]) } } return string(result) } // 原地修改 func replaceSpace(s string) string { b := []byte(s) length := len(b) spaceCount := 0 // 计算空格数量 for _, v := range b { if v == ' ' { spaceCount++ } } // 扩展原有切片 resizeCount := spaceCount * 2 tmp := make([]byte, resizeCount) b = append(b, tmp...) i := length - 1 j := len(b) - 1 for i \u003e= 0 { if b[i] != ' ' { b[j] = b[i] i-- j-- } else { b[j] = '0' b[j-1] = '2' b[j-2] = '%' i-- j = j - 3 } } return string(b) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:4:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"翻转字符串里的单词 给定一个字符串，逐个翻转字符串中的每个单词。 输入: \"the sky is blue\" 输出: \"blue is sky the\" import ( \"fmt\" ) func reverseWords(s string) string { //1.使用双指针删除冗余的空格 slowIndex, fastIndex := 0, 0 b := []byte(s) //删除头部冗余空格 for len(b) \u003e 0 \u0026\u0026 fastIndex \u003c len(b) \u0026\u0026 b[fastIndex] == ' ' { fastIndex++ } //删除单词间冗余空格 for ; fastIndex \u003c len(b); fastIndex++ { if fastIndex-1 \u003e 0 \u0026\u0026 b[fastIndex-1] == b[fastIndex] \u0026\u0026 b[fastIndex] == ' ' { continue } b[slowIndex] = b[fastIndex] slowIndex++ } //删除尾部冗余空格 if slowIndex-1 \u003e 0 \u0026\u0026 b[slowIndex-1] == ' ' { b = b[:slowIndex-1] } else { b = b[:slowIndex] } //2.反转整个字符串 reverse(\u0026b, 0, len(b)-1) //3.反转单个单词 i单词开始位置，j单词结束位置 i := 0 for i \u003c len(b) { j := i for ; j \u003c len(b) \u0026\u0026 b[j] != ' '; j++ { } reverse(\u0026b, i, j-1) i = j i++ } return string(b) } func reverse(b *[]byte, left, right int) { for left \u003c right { (*b)[left], (*b)[right] = (*b)[right], (*b)[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:4:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"左旋转字符串 输入: s = \"abcdefg\", k = 2 输出: \"cdefgab\" func reverseLeftWords(s string, n int) string { b := []byte(s) // 1. 反转前n个字符 // 2. 反转第n到end字符 // 3. 反转整个字符 reverse(b, 0, n-1) reverse(b, n, len(b)-1) reverse(b, 0, len(b)-1) return string(b) } // 切片是引用传递 func reverse(b []byte, left, right int){ for left \u003c right{ b[left], b[right] = b[right],b[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:4:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"实现strStr() 给定一个 haystack 字符串和一个 needle 字符串，在 haystack 字符串中找出 needle 字符串出现的第一个位置 (从0开始)。如果不存在，则返回 -1。 示例 1: 输入: haystack = “hello”, needle = “ll” 输出: 2 示例 2: 输入: haystack = “aaaaa”, needle = “bba” 输出: -1 说明: 当 needle 是空字符串时，我们应当返回什么值呢？这是一个在面试中很好的问题。 对于本题而言，当 needle 是空字符串时我们应当返回 0 。这与C语言的 strstr() 以及 Java的 indexOf() 定义相符。 什么是KMP KMP算法是一种改进的字符串匹配算法，由D.E.Knuth，J.H.Morris和V.R.Pratt提出的，因此人们称它为克努特—莫里斯—普拉特操作（简称KMP算法）。KMP算法的核心是利用匹配失败后的信息，尽量减少模式串与主串的匹配次数以达到快速匹配的目的。具体实现就是通过一个next()函数实现，函数本身包含了模式串的局部匹配信息。KMP算法的时间复杂度O(m+n) [1] 。（来自百度百科） KMP的经典思想就是:当出现字符串不匹配时，可以记录一部分之前已经匹配的文本内容，利用这些信息避免从头再去做匹配。 如何记录已经匹配的文本内容，是KMP的重点，也是next数组的任务。 什么是前缀表与next数组 next数组就是一个前缀表。 前缀表是用来回退的，它记录了模式串与主串(文本串)不匹配的时候，模式串应该从哪里开始重新匹配。 什么是前缀表：记录下标i之前（包括i）的字符串中，有多大长度的相同前缀后缀。 最长公共前后缀：字符串aa的最长相等前后缀为1。 字符串aaa的最长相等前后缀为2。 等等….. 前缀表要求的就是最长相同前后缀的长度。它能告诉我们上次匹配的位置。 下标5之前这部分的字符串（也就是字符串aabaa）的最长相等的前缀和后缀字符串是子字符串aa ，因为找到了最长相等的前缀和后缀，匹配失败的位置是后缀子串的后面，那么我们找到与其相同的前缀的后面从新匹配就可以了。 前缀表与next数组有什么关系： next数组即可以就是前缀表，也可以是前缀表统一减一（右移一位，初始位置为-1）。 前缀表统一减一之后的next数组： 时间复杂度分析： 其中n为文本串长度，m为模式串长度，因为在匹配的过程中，根据前缀表不断调整匹配的位置，可以看出匹配的过程是$O(n)$，之前还要单独生成next数组，时间复杂度是$O(m)$。所以整个KMP算法的时间复杂度是$O(n+m)$的。 暴力的解法显而易见是$O(n × m)$，所以KMP在字符串匹配中极大的提高的搜索的效率。 为了和力扣题目28.实现strStr保持一致，方便大家理解，以下文章统称haystack为文本串, needle为模式串。 都知道使用KMP算法，一定要构造next数组 构造next数组： 我们定义一个函数getNext来构建next数组，函数参数为指向next数组的指针，和一个字符串。 代码如下： void getNext(int* next, const string\u0026 s) 构造next数组其实就是计算模式串s，前缀表的过程。 主要有如下三步： 初始化 定义两个指针i和j，j指向前缀起始位置，i指向后缀起始位置。 然后还要对next数组进行初始化赋值，如下： int j = -1; next[0] = j; j 为什么要初始化为 -1呢，因为之前说过 前缀表要统一减一的操作仅仅是其中的一种实现，我们这里选择j初始化为-1，下文我还会给出j不初始化为-1的实现代码。 next[i] 表示 i（包括i）之前最长相等的前后缀长度（其实就是j） 所以初始化next[0] = j 。 处理前后缀不相同的情况 因为j初始化为-1，那么i就从1开始，进行s[i] 与 s[j+1]的比较。 所以遍历模式串s的循环下标i 要从 1开始，代码如下： for(int i = 1; i \u003c s.size(); i++) { 如果 s[i] 与 s[j+1]不相同，也就是遇到 前后缀末尾不相同的情况，就要向前回退。 怎么回退呢？ next[j]就是记录着j（包括j）之前的子串的相同前后缀的长度。 那么 s[i] 与 s[j+1] 不相同，就要找 j+1前一个元素在next数组里的值（就是next[j]）。 所以，处理前后缀不相同的情况代码如下： while (j \u003e= 0 \u0026\u0026 s[i] != s[j + 1]) { // 前后缀不相同了 j = next[j]; // 向前回退 } 处理前后缀相同的情况 如果s[i] 与 s[j + 1] 相同，那么就同时向后移动i 和j 说明找到了相同的前后缀，同时还要将j（前缀的长度）赋给next[i], 因为next[i]要记录相同前后缀的长度。 代码如下： if (s[i] == s[j + 1]) { // 找到相同的前后缀 j++; } next[i] = j; 最后整体构建next数组的函数代码如下： void getNext(int* next, const string\u0026 s){ int j = -1; next[0] = j; for(int i = 1; i \u003c s.size(); i++) { // **注意**i从1开始 while (j \u003e= 0 \u0026\u0026 s[i] != s[j + 1]) { // 前后缀不相同了 j = next[j]; // 向前回退 } if (s[i] == s[j + 1]) { // 找到相同的前后缀 j++; } next[i] = j; // 将j（前缀的长度）赋给next[i] } } 使用next数组进行匹配 在文本串s里 找是否出现过模式串t。 定义两个下标j 指向模式串起始位置，i指向文本串起始位置。 那么j初始值依然为-1，为什么呢？ 依然因为next数组里记录的起始位置为-1。 i就从0开始，遍历文本串，代码如下： for (int i = 0; i \u003c s.size(); i++) 接下来就是 s[i] 与 t[j + 1] （因为j从-1开始的） 进行比较。 如果 s[i] 与 t[j + 1] 不相同，j就要从next数组里寻找下一个匹配的位置。 代码如下： while(j \u003e= 0 \u0026\u0026 s[i] != t[j + 1]) { j = next[j]; } 如果 s[i] 与 t[j + 1] 相同，那么i 和 j 同时向后移动， 代码如下： if (s[i] == t[j + 1]) { j++; // i的增加在for循环里 } 如何判断在文本串s里出现了模式串t呢，如果j指向了模式串t的末尾，那么就说明模式串t完全匹配文本串s里的某个子串了。 本题要在文本串字符串中找出模式串出现的第一个位置 (从0开始)，所以返回当前在文本串匹配模式串的位置i 减去 模式串的长度，就是文本串字符串中出现模式串的第一个位置。 代码如下： if (j == (t.size() - 1) ) { return (i - t.size() + 1); } 那么使用next数组，用模式串匹配文本串的整体代码如下： int j = -1; // 因为next数组里记录的起始位置为-1 for (int i = 0; i \u003c s.size(); i++) { // **注意**i就从0开始 while(j \u003e= 0 \u0026\u0026 s[i] != t[j + 1]) { // 不匹配 j = next[j]; // j 寻找之前匹配的位置 } if (s[i] == t[j + 1]) { // 匹配，j和i同时向后移动 j++; // i的增加在for循环里 } if (j == (t.size() - 1) ) { // 文本串s里出现了模式串t return (i - t.size() + 1); } } 此时所有逻辑的代码都已经写出来了，力扣 28.实现strStr 题目的整体代码如下： class Solution { public: void getNext(int* next, const string\u0026 s) { int j = -1; next[0] = j; for(int i = 1; i \u003c s.size(); i++) { // **注意**i从1开始 while (j \u003e= 0 \u0026\u0026 s[i] != s[j + 1]) { // 前后缀不相同了 j = next[j]; // 向前回退 } if (s[i] == s[j + 1]) { // 找到相同的前后缀 j++; } next[i] = j; // 将j（前缀的长度）赋给next[i] } } int strStr(string haystack, string needle) { if (needle.size() == 0) { return 0; } int next[needle.size()]; getNext(next, needle); int j = -1; // // 因为next数组里记录的起始位置为-1 for (int i = 0; i \u003c haystack.size(); i++) { // **注意**i就从0开始 while(j \u003e= 0 \u0026\u0026 haystack[i] != needle[j + 1]) { // 不匹配 j = next[j]; // j 寻找之前匹配","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:4:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"重复的子字符串 给定一个非空的字符串，判断它是否可以由它的一个子串重复多次构成。给定的字符串只含有小写英文字母，并且长度不超过10000。 示例 1: 输入: \"abab\" 输出: True 解释: 可由子字符串 \"ab\" 重复两次构成。 示例 2: 输入: \"aba\" 输出: False 示例 3: 输入: \"abcabcabcabc\" 输出: True 解释: 可由子字符串 \"abc\" 重复四次构成。 (或者子字符串 \"abcabc\" 重复两次构成。) 标准的KMP题目~ 数组长度减去最长相同前后缀的长度相当于是第一个周期的长度，也就是一个周期的长度，如果这个周期可以被整除，就说明整个数组就是这个周期的循环。 强烈建议大家把next数组打印出来，看看next数组里的规律，有助于理解KMP算法 代码实现： //前缀表统一减一的实现 func repeatedSubstringPattern(s string) bool { n := len(s) if n == 0 { return false } next := make([]int, n) j := -1 next[0] = j for i := 1; i \u003c n; i++ { for j \u003e= 0 \u0026\u0026 s[i] != s[j+1] { j = next[j] } if s[i] == s[j+1] { j++ } next[i] = j } // next[n-1]+1 最长相同前后缀的长度 if next[n-1] != -1 \u0026\u0026 n%(n-(next[n-1]+1)) == 0 { return true } return false } //前缀表不减一 func repeatedSubstringPattern(s string) bool { n := len(s) if n == 0 { return false } j := 0 next := make([]int, n) next[0] = j for i := 1; i \u003c n; i++ { for j \u003e 0 \u0026\u0026 s[i] != s[j] { j = next[j-1] } if s[i] == s[j] { j++ } next[i] = j } // next[n-1] 最长相同前后缀的长度 if next[n-1] != 0 \u0026\u0026 n%(n-next[n-1]) == 0 { return true } return false } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:4:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"双指针法 双指针法（快慢指针法）在数组和链表的操作中是非常常见的，很多考察数组、链表、字符串等操作的面试题，都使用双指针法。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"移除元素 给你一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，并返回移除后数组的新长度。 不要使用额外的数组空间，你必须仅使用 $O(1)$ 额外空间并原地修改输入数组。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 示例 1: 给定 nums = [3,2,2,3], val = 3, 函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 给定 nums = [0,1,2,2,3,0,4,2], val = 2, 函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。 你不需要考虑数组中超出新长度后面的元素。 func removeElement(nums []int, val int) int { length:=len(nums) res:=0 for i:=0;i\u003clength;i++{ if nums[i]!=val { nums[res]=nums[i] res++ } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"反转字符串 func reverseString(s []byte) { left:=0 right:=len(s)-1 for left\u003cright{ s[left],s[right]=s[right],s[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"替换空格 // 遍历添加 func replaceSpace(s string) string { b := []byte(s) result := make([]byte, 0) for i := 0; i \u003c len(b); i++ { if b[i] == ' ' { result = append(result, []byte(\"%20\")...) } else { result = append(result, b[i]) } } return string(result) } // 原地修改 func replaceSpace(s string) string { b := []byte(s) length := len(b) spaceCount := 0 // 计算空格数量 for _, v := range b { if v == ' ' { spaceCount++ } } // 扩展原有切片 resizeCount := spaceCount * 2 tmp := make([]byte, resizeCount) b = append(b, tmp...) i := length - 1 j := len(b) - 1 for i \u003e= 0 { if b[i] != ' ' { b[j] = b[i] i-- j-- } else { b[j] = '0' b[j-1] = '2' b[j-2] = '%' i-- j = j - 3 } } return string(b) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"翻转字符串里的单词 import ( \"fmt\" ) func reverseWords(s string) string { //1.使用双指针删除冗余的空格 slowIndex, fastIndex := 0, 0 b := []byte(s) //删除头部冗余空格 for len(b) \u003e 0 \u0026\u0026 fastIndex \u003c len(b) \u0026\u0026 b[fastIndex] == ' ' { fastIndex++ } //删除单词间冗余空格 for ; fastIndex \u003c len(b); fastIndex++ { if fastIndex-1 \u003e 0 \u0026\u0026 b[fastIndex-1] == b[fastIndex] \u0026\u0026 b[fastIndex] == ' ' { continue } b[slowIndex] = b[fastIndex] slowIndex++ } //删除尾部冗余空格 if slowIndex-1 \u003e 0 \u0026\u0026 b[slowIndex-1] == ' ' { b = b[:slowIndex-1] } else { b = b[:slowIndex] } //2.反转整个字符串 reverse(\u0026b, 0, len(b)-1) //3.反转单个单词 i单词开始位置，j单词结束位置 i := 0 for i \u003c len(b) { j := i for ; j \u003c len(b) \u0026\u0026 b[j] != ' '; j++ { } reverse(\u0026b, i, j-1) i = j i++ } return string(b) } func reverse(b *[]byte, left, right int) { for left \u003c right { (*b)[left], (*b)[right] = (*b)[right], (*b)[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"翻转链表 //双指针 func reverseList(head *ListNode) *ListNode { var pre *ListNode cur := head for cur != nil { next := cur.Next cur.Next = pre pre = cur cur = next } return pre } //递归 func reverseList(head *ListNode) *ListNode { return help(nil, head) } func help(pre, head *ListNode)*ListNode{ if head == nil { return pre } next := head.Next head.Next = pre return help(head, next) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"删除链表的倒数第N个节点 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { dummyHead := \u0026ListNode{} dummyHead.Next = head cur := head prev := dummyHead i := 1 for cur != nil { cur = cur.Next if i \u003e n { prev = prev.Next } i++ } prev.Next = prev.Next.Next return dummyHead.Next } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"链表相交 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 null 。 func getIntersectionNode(headA, headB *ListNode) *ListNode { curA := headA curB := headB lenA, lenB := 0, 0 // 求A，B的长度 for curA != nil { curA = curA.Next lenA++ } for curB != nil { curB = curB.Next lenB++ } var step int var fast, slow *ListNode // 请求长度差，并且让更长的链表先走相差的长度 if lenA \u003e lenB { step = lenA - lenB fast, slow = headA, headB } else { step = lenB - lenA fast, slow = headB, headA } for i:=0; i \u003c step; i++ { fast = fast.Next } // 遍历两个链表遇到相同则跳出遍历 for fast != slow { fast = fast.Next slow = slow.Next } return fast } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:7","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"环形链表II 题意： 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 func detectCycle(head *ListNode) *ListNode { slow, fast := head, head for fast != nil \u0026\u0026 fast.Next != nil { slow = slow.Next fast = fast.Next.Next if slow == fast { for slow != head { slow = slow.Next head = head.Next } return head } } return nil } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:8","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"三数之和 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有满足条件且不重复的三元组。 注意： 答案中不可以包含重复的三元组。 示例： 给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ] func threeSum(nums []int)[][]int{ sort.Ints(nums) res:=[][]int{} for i:=0;i\u003clen(nums)-2;i++{ n1:=nums[i] if n1\u003e0{ break } if i\u003e0\u0026\u0026n1==nums[i-1]{ continue } l,r:=i+1,len(nums)-1 for l\u003cr{ n2,n3:=nums[l],nums[r] if n1+n2+n3==0{ res=append(res,[]int{n1,n2,n3}) for l\u003cr\u0026\u0026nums[l]==n2{ l++ } for l\u003cr\u0026\u0026nums[r]==n3{ r-- } }else if n1+n2+n3\u003c0{ l++ }else { r-- } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:9","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"四数之和 题意：给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组。 注意： 答案中不可以包含重复的四元组。 示例： 给定数组 nums = [1, 0, -1, 0, -2, 2]，和 target = 0。 满足要求的四元组集合为： [ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2] ] func fourSum(nums []int, target int) [][]int { if len(nums) \u003c 4 { return nil } sort.Ints(nums) var res [][]int for i := 0; i \u003c len(nums)-3; i++ { n1 := nums[i] // if n1 \u003e target { // 不能这样写,因为可能是负数 // break // } if i \u003e 0 \u0026\u0026 n1 == nums[i-1] { continue } for j := i + 1; j \u003c len(nums)-2; j++ { n2 := nums[j] if j \u003e i+1 \u0026\u0026 n2 == nums[j-1] { continue } l := j + 1 r := len(nums) - 1 for l \u003c r { n3 := nums[l] n4 := nums[r] sum := n1 + n2 + n3 + n4 if sum \u003c target { l++ } else if sum \u003e target { r-- } else { res = append(res, []int{n1, n2, n3, n4}) for l \u003c r \u0026\u0026 n3 == nums[l+1] { // 去重 l++ } for l \u003c r \u0026\u0026 n4 == nums[r-1] { // 去重 r-- } // 找到答案时,双指针同时靠近 r-- l++ } } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:10","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"双指针总结 总结 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:11","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"栈和队列 需要知道栈和队列的底层实现，不同编程语言不同STL的实现原理都是不尽相同的。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"理论基础(c++) 栈其实就是递归的一种实现结构。 栈（本身可以说是一个容器适配器）是以底层容器（数组或者链表）完成其所有的工作，对外提供统一的接口，底层容器是可插拔的（也就是说我们可以控制使用哪种容器来实现栈的功能）。 SGI -- Silicon Graphics [Computer System] Inc. 硅图[计算机系统] 公司. STL -- Standard Template Library 标准模板库。 SGI STL -- SGI的标准模板库。 SGI的全称 -- 硅图[计算机系统] 公司。 我们常用的SGI STL，如果没有指定底层实现的话，默认是以deque为缺省情况下栈的低层结构。 deque是一个双向队列，只要封住一段，只开通另一端就可以实现栈的逻辑了。 SGI STL中 队列底层实现缺省情况下一样使用deque实现的。 我们也可以指定vector为栈的底层实现，初始化语句如下： std::stack\u003cint, std::vector\u003cint\u003e \u003e third; // 使用vector为底层容器的栈 对应的队列的情况是一样的。 队列中先进先出的数据结构，同样不允许有遍历行为，不提供迭代器, SGI STL中队列一样是以deque为缺省情况下的底部结构。 也可以指定list 为起底层实现，初始化queue的语句如下： std::queue\u003cint, std::list\u003cint\u003e\u003e third; // 定义以list为底层容器的队列 所以STL 队列也不被归类为容器，而被归类为container adapter（ 容器适配器）。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"用栈实现队列 使用栈实现队列的下列操作： push(x) – 将一个元素放入队列的尾部。 pop() – 从队列首部移除元素。 peek() – 返回队列首部的元素。 empty() – 返回队列是否为空。 示例: MyQueue queue = new MyQueue(); queue.push(1); queue.push(2); queue.peek(); // 返回 1 queue.pop(); // 返回 1 queue.empty(); // 返回 false 说明: 你只能使用标准的栈操作 – 也就是只有 push to top, peek/pop from top, size, 和 is empty 操作是合法的。 你所使用的语言也许不支持栈。你可以使用 list 或者 deque（双端队列）来模拟一个栈，只要是标准的栈操作即可。 假设所有操作都是有效的 （例如，一个空的队列不会调用 pop 或者 peek 操作） type MyQueue struct { stack []int back []int } /** Initialize your data structure here. */ func Constructor() MyQueue { return MyQueue{ stack: make([]int, 0), back: make([]int, 0), } } /** Push element x to the back of queue. */ func (this *MyQueue) Push(x int) { for len(this.back) != 0 { val := this.back[len(this.back)-1] this.back = this.back[:len(this.back)-1] this.stack = append(this.stack, val) } this.stack = append(this.stack, x) } /** Removes the element from in front of queue and returns that element. */ func (this *MyQueue) Pop() int { for len(this.stack) != 0 { val := this.stack[len(this.stack)-1] this.stack = this.stack[:len(this.stack)-1] this.back = append(this.back, val) } if len(this.back) == 0 { return 0 } val := this.back[len(this.back)-1] this.back = this.back[:len(this.back)-1] return val } /** Get the front element. */ func (this *MyQueue) Peek() int { for len(this.stack) != 0 { val := this.stack[len(this.stack)-1] this.stack = this.stack[:len(this.stack)-1] this.back = append(this.back, val) } if len(this.back) == 0 { return 0 } val := this.back[len(this.back)-1] return val } /** Returns whether the queue is empty. */ func (this *MyQueue) Empty() bool { return len(this.stack) == 0 \u0026\u0026 len(this.back) == 0 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"用队列实现栈 使用队列实现栈的下列操作： push(x) -- 元素 x 入栈 pop() -- 移除栈顶元素 top() -- 获取栈顶元素 empty() -- 返回栈是否为空 注意: 你只能使用队列的基本操作– 也就是 push to back, peek/pop from front, size, 和 is empty 这些操作是合法的。 你所使用的语言也许不支持队列。 你可以使用 list 或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。 你可以假设所有操作都是有效的（例如, 对一个空的栈不会调用 pop 或者 top 操作） 用两个队列实现： type MyStack struct { //创建两个队列 queue1 []int queue2 []int } func Constructor() MyStack { return MyStack{ //初始化 queue1:make([]int,0), queue2:make([]int,0), } } func (this *MyStack) Push(x int) { //先将数据存在queue2中 this.queue2 = append(this.queue2,x) //将queue1中所有元素移到queue2中，再将两个队列进行交换 this.Move() } func (this *MyStack) Move(){ if len(this.queue1) == 0{ //交换，queue1置为queue2,queue2置为空 this.queue1,this.queue2 = this.queue2,this.queue1 }else{ //queue1元素从头开始一个一个追加到queue2中 this.queue2 = append(this.queue2,this.queue1[0]) this.queue1 = this.queue1[1:] //去除第一个元素 this.Move() //重复 } } func (this *MyStack) Pop() int { val := this.queue1[0] this.queue1 = this.queue1[1:] //去除第一个元素 return val } func (this *MyStack) Top() int { return this.queue1[0] //直接返回 } func (this *MyStack) Empty() bool { return len(this.queue1) == 0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ 用一个队列实现： type MyStack struct { queue []int//创建一个队列 } /** Initialize your data structure here. */ func Constructor() MyStack { return MyStack{ //初始化 queue:make([]int,0), } } /** Push element x onto stack. */ func (this *MyStack) Push(x int) { //添加元素 this.queue=append(this.queue,x) } /** Removes the element on top of the stack and returns that element. */ func (this *MyStack) Pop() int { n:=len(this.queue)-1//判断长度 for n!=0{ //除了最后一个，其余的都重新添加到队列里 val:=this.queue[0] this.queue=this.queue[1:] this.queue=append(this.queue,val) n-- } //弹出元素 val:=this.queue[0] this.queue=this.queue[1:] return val } /** Get the top element. */ func (this *MyStack) Top() int { //利用Pop函数，弹出来的元素重新添加 val:=this.Pop() this.queue=append(this.queue,val) return val } /** Returns whether the stack is empty. */ func (this *MyStack) Empty() bool { return len(this.queue)==0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"有效的括号 给定一个只包括 ‘('，')'，'{'，'}'，'['，']’ 的字符串，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 注意空字符串可被认为是有效字符串。 栈的经典问题。 func isValid(s string) bool { hash := map[byte]byte{')':'(', ']':'[', '}':'{'} stack := make([]byte, 0) if s == \"\" { return true } for i := 0; i \u003c len(s); i++ { if s[i] == '(' || s[i] == '[' || s[i] == '{' { stack = append(stack, s[i]) } else if len(stack) \u003e 0 \u0026\u0026 stack[len(stack)-1] == hash[s[i]] { stack = stack[:len(stack)-1] } else { return false } } return len(stack) == 0 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"删除字符串中的所有相邻重复项 给出由小写字母组成的字符串 S，重复项删除操作会选择两个相邻且相同的字母，并删除它们。 在 S 上反复执行重复项删除操作，直到无法继续删除。 在完成所有重复项删除操作后返回最终的字符串。答案保证唯一。 示例： 输入：\"abbaca\" 输出：\"ca\" 解释：例如，在 \"abbaca\" 中，我们可以删除 \"bb\" 由于两字母相邻且相同，这是此时唯一可以执行删除操作的重复项。之后我们得到字符串 \"aaca\"，其中又只有 \"aa\" 可以执行重复项删除操作，所以最后的字符串为 \"ca\"。 提示： 1 \u003c= S.length \u003c= 20000 S 仅由小写英文字母组成 func removeDuplicates(s string) string { var stack []byte for i := 0; i \u003c len(s);i++ { // 栈不空 且 与栈顶元素不等 if len(stack) \u003e 0 \u0026\u0026 stack[len(stack)-1] == s[i] { // 弹出栈顶元素 并 忽略当前元素(s[i]) stack = stack[:len(stack)-1] }else{ // 入栈 stack = append(stack, s[i]) } } return string(stack) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"逆波兰表达式求值 根据 逆波兰表示法，求表达式的值。 有效的运算符包括 + , - , * , / 。每个运算对象可以是整数，也可以是另一个逆波兰表达式。 说明： 整数除法只保留整数部分。 给定逆波兰表达式总是有效的。换句话说，表达式总会得出有效数值且不存在除数为 0 的情况。 示例 1： 输入: [\"2\", \"1\", \"+\", \"3\", \" * \"] 输出: 9 解释: 该算式转化为常见的中缀算术表达式为：((2 + 1) * 3) = 9 逆波兰表达式：是一种后缀表达式，所谓后缀就是指算符写在后面。 平常使用的算式则是一种中缀表达式，如 ( 1 + 2 ) * ( 3 + 4 ) 。 该算式的逆波兰表达式写法为 ( ( 1 2 + ) ( 3 4 + ) * ) 。 逆波兰表达式主要有以下两个优点： 去掉括号后表达式无歧义，上式即便写成 1 2 + 3 4 + * 也可以依据次序计算出正确结果。 适合用栈操作运算：遇到数字则入栈；遇到算符则取出栈顶两个数字进行计算，并将结果压入栈中 func evalRPN(tokens []string) int { stack := []int{} for _, token := range tokens { val, err := strconv.Atoi(token) if err == nil { stack = append(stack, val) } else { num1, num2 := stack[len(stack)-2], stack[(len(stack))-1] stack = stack[:len(stack)-2] switch token { case \"+\": stack = append(stack, num1+num2) case \"-\": stack = append(stack, num1-num2) case \"*\": stack = append(stack, num1*num2) case \"/\": stack = append(stack, num1/num2) } } } return stack[0] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"滑动窗口最大值 给定一个数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。 每个窗口都有一个最大值，返回滑动窗口中的最大值数组。 进阶：你能在线性时间复杂度内解决此题吗？ // 封装单调队列的方式解题 type MyQueue struct { queue []int } func NewMyQueue() *MyQueue { return \u0026MyQueue{ queue: make([]int, 0), } } func (m *MyQueue) Front() int { return m.queue[0] } func (m *MyQueue) Back() int { return m.queue[len(m.queue)-1] } func (m *MyQueue) Empty() bool { return len(m.queue) == 0 } func (m *MyQueue) Push(val int) { for !m.Empty() \u0026\u0026 val \u003e m.Back() { m.queue = m.queue[:len(m.queue)-1] } m.queue = append(m.queue, val) } func (m *MyQueue) Pop(val int) { if !m.Empty() \u0026\u0026 val == m.Front() { m.queue = m.queue[1:] } } func maxSlidingWindow(nums []int, k int) []int { queue := NewMyQueue() length := len(nums) res := make([]int, 0) // 先将前k个元素放入队列 for i := 0; i \u003c k; i++ { queue.Push(nums[i]) } // 记录前k个元素的最大值 res = append(res, queue.Front()) for i := k; i \u003c length; i++ { // 滑动窗口移除最前面的元素 queue.Pop(nums[i-k]) // 滑动窗口添加最后面的元素 queue.Push(nums[i]) // 记录最大值 res = append(res, queue.Front()) } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:7","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"前K个高频元素 给定一个非空的整数数组，返回其中出现频率前 k 高的元素。 示例 1: 输入: nums = [1,1,1,2,2,3], k = 2 输出: [1,2] //方法一：小顶堆 func topKFrequent(nums []int, k int) []int { map_num:=map[int]int{} //记录每个元素出现的次数 for _,item:=range nums{ map_num[item]++ } h:=\u0026IHeap{} heap.Init(h) //所有元素入堆，堆的长度为k for key,value:=range map_num{ heap.Push(h,[2]int{key,value}) if h.Len()\u003ek{ heap.Pop(h) } } res:=make([]int,k) //按顺序返回堆中的元素 for i:=0;i\u003ck;i++{ res[k-i-1]=heap.Pop(h).([2]int)[0] } return res } //构建小顶堆 type IHeap [][2]int func (h IHeap) Len()int { return len(h) } func (h IHeap) Less (i,j int) bool { return h[i][1]\u003ch[j][1] } func (h IHeap) Swap(i,j int) { h[i],h[j]=h[j],h[i] } func (h *IHeap) Push(x interface{}){ *h=append(*h,x.([2]int)) } func (h *IHeap) Pop() interface{}{ old:=*h n:=len(old) x:=old[n-1] *h=old[0:n-1] return x } //方法二:利用O(logn)排序 func topKFrequent(nums []int, k int) []int { ans:=[]int{} map_num:=map[int]int{} for _,item:=range nums { map_num[item]++ } for key,_:=range map_num{ ans=append(ans,key) } //核心思想：排序 //可以不用包函数，自己实现快排 sort.Slice(ans,func (a,b int)bool{ return map_num[ans[a]]\u003emap_num[ans[b]] }) return ans[:k] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:8","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"二叉树 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"理论基础 一般主要会碰到满二叉树以及完全二叉树。 完全二叉树的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2^h -1 个节点。 优先级队列其实是一个堆，堆就是一棵完全二叉树，同时保证父子节点的顺序关系。 二叉搜索树： 与前面两个树不同，该树有节点权值。 有序树，左节点 \u003c 中节点 \u003c 右节点 平衡二叉搜索树：又被称为AVL（Adelson-Velsky and Landis）树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 二叉树可以链式存储，也可以顺序存储。 深度优先遍历 前序遍历（递归法，迭代法） 中序遍历（递归法，迭代法） 后序遍历（递归法，迭代法） 广度优先遍历 层次遍历（迭代法） type TreeNode struct { Val int Left *TreeNode Right *TreeNode } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"递归遍历 递归的实现就是：每一次递归调用都会把函数的局部变量、参数值和返回地址等压入调用栈中，然后递归返回的时候，从栈顶弹出上一次递归的各项参数，所以这就是递归为什么可以返回上一层位置的原因。 写递归还是得有方法论。 确定递归函数的参数和返回值： 确定哪些参数是递归的过程中需要处理的，那么就在递归函数里加上这个参数， 并且还要明确每次递归的返回值是什么进而确定递归函数的返回类型。 确定终止条件： 写完了递归算法, 运行的时候，经常会遇到栈溢出的错误，就是没写终止条件或者终止条件写的不对，操作系统也是用一个栈的结构来保存每一层递归的信息，如果递归没有终止，操作系统的内存栈必然就会溢出。 确定单层递归的逻辑： 确定每一层递归需要处理的信息。在这里也就会重复调用自己来实现递归的过程。 前序遍历： func preorderTraversal(root *TreeNode) (res []int) { var traversal func(node *TreeNode) traversal = func(node *TreeNode) { if node == nil { return } res = append(res,node.Val) traversal(node.Left) traversal(node.Right) } traversal(root) return res } func pre(r *TreeNode)(res []int){ } 中序遍历： func inorderTraversal(root *TreeNode) (res []int) { var traversal func(node *TreeNode) traversal = func(node *TreeNode) { if node == nil { return } traversal(node.Left) res = append(res,node.Val) traversal(node.Right) } traversal(root) return res } 后序遍历: func postorderTraversal(root *TreeNode) (res []int) { var traversal func(node *TreeNode) traversal = func(node *TreeNode) { if node == nil { return } traversal(node.Left) traversal(node.Right) res = append(res,node.Val) } traversal(root) return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"迭代遍历 迭代法前序遍历 func preorderTraversal(root *TreeNode) []int { ans := []int{} if root == nil { return ans } st := list.New() st.PushBack(root) for st.Len() \u003e 0 { node := st.Remove(st.Back()).(*TreeNode) ans = append(ans, node.Val) if node.Right != nil { st.PushBack(node.Right) } if node.Left != nil { st.PushBack(node.Left) } } return ans } func preorderTraver(root *TreeNode) []int { } 迭代法后序遍历 func postorderTraversal(root *TreeNode) []int { ans := []int{} if root == nil { return ans } st := list.New() st.PushBack(root) for st.Len() \u003e 0 { node := st.Remove(st.Back()).(*TreeNode) ans = append(ans, node.Val) if node.Left != nil { st.PushBack(node.Left) } if node.Right != nil { st.PushBack(node.Right) } } reverse(ans) return ans } func reverse(a []int) { l, r := 0, len(a) - 1 for l \u003c r { a[l], a[r] = a[r], a[l] l, r = l+1, r-1 } } 迭代法中序遍历 func inorderTraversal(root *TreeNode) []int { ans := []int{} if root == nil { return ans } st := list.New() cur := root for cur != nil || st.Len() \u003e 0 { if cur != nil { st.PushBack(cur) cur = cur.Left } else { cur = st.Remove(st.Back()).(*TreeNode) ans = append(ans, cur.Val) cur = cur.Right } } return ans } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"统一迭代 迭代法实现的先中后序，其实风格也不是那么统一，除了先序和后序，有关联，中序完全就是另一个风格了，一会用栈遍历，一会又用指针来遍历。 使用迭代法实现先中后序遍历，很难写出统一的代码，不像是递归法，实现了其中的一种遍历方式，其他两种只要稍稍改一下节点顺序就可以了。 要处理的节点放入栈之后，紧接着放入一个空指针作为标记。 这种方法也可以叫做标记法 前序遍历统一迭代法 /** type Element struct { // 元素保管的值 Value interface{} // 内含隐藏或非导出字段 } func (l *List) Back() *Element 前序遍历：中左右 压栈顺序：右左中 **/ func preorderTraversal(root *TreeNode) []int { if root == nil { return nil } var stack = list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u003e0{ e := stack.Back() stack.Remove(e)//弹出元素 if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：右左中 if node.Right!=nil{ stack.PushBack(node.Right) } if node.Left!=nil{ stack.PushBack(node.Left) } stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) } return res } 中序遍历统一迭代法 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ //中序遍历：左中右 //压栈顺序：右中左 func inorderTraversal(root *TreeNode) []int { if root==nil{ return nil } stack:=list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u003e0{ e := stack.Back() stack.Remove(e) if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：右中左 if node.Right!=nil{ stack.PushBack(node.Right) } stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) if node.Left!=nil{ stack.PushBack(node.Left) } } return res } 后序遍历统一迭代法 //后续遍历：左右中 //压栈顺序：中右左 func postorderTraversal(root *TreeNode) []int { if root == nil { return nil } var stack = list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u003e0{ e := stack.Back() stack.Remove(e) if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：中右左 stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) if node.Right!=nil{ stack.PushBack(node.Right) } if node.Left!=nil{ stack.PushBack(node.Left) } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"层序遍历 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"102.二叉树的层序遍历 https://leetcode-cn.com/problems/binary-tree-level-order-traversal/ /** 102. 二叉树的层序遍历 */ func levelOrder(root *TreeNode) [][]int { res:=[][]int{} if root==nil{//防止为空 return res } queue:=list.New() queue.PushBack(root) var tmpArr []int for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node.Val)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]int{}//清空层的数据 } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"107.二叉树的层次遍历II https://leetcode-cn.com/problems/binary-tree-level-order-traversal-ii/ /** 107. 二叉树的层序遍历 II */ func levelOrderBottom(root *TreeNode) [][]int { queue:=list.New() res:=[][]int{} if root==nil{ return res } queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() tmp:=[]int{} for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmp=append(tmp,node.Val) } res=append(res,tmp) } //反转结果集 for i:=0;i\u003clen(res)/2;i++{ res[i],res[len(res)-i-1]=res[len(res)-i-1],res[i] } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:7","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"199.二叉树的右视图 https://leetcode-cn.com/problems/binary-tree-right-side-view/ func rightSideView(root *TreeNode) []int { queue:=list.New() res:=[][]int{} var finaRes []int if root==nil{ return finaRes } queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() tmp:=[]int{} for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmp=append(tmp,node.Val) } res=append(res,tmp) } //取每一层的最后一个元素 for i:=0;i\u003clen(res);i++{ finaRes=append(finaRes,res[i][len(res[i])-1]) } return finaRes } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:8","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"637.二叉树的层平均值 https://leetcode-cn.com/problems/average-of-levels-in-binary-tree/ /** 637. 二叉树的层平均值 */ func averageOfLevels(root *TreeNode) []float64 { res:=[][]int{} var finRes []float64 if root==nil{//防止为空 return finRes } queue:=list.New() queue.PushBack(root) var tmpArr []int for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node.Val)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]int{}//清空层的数据 } //计算每层的平均值 length:=len(res) for i:=0;i\u003clength;i++{ var sum int for j:=0;j\u003clen(res[i]);j++{ sum+=res[i][j] } tmp:=float64(sum)/float64(len(res[i])) finRes=append(finRes,tmp)//将平均值放入结果集合 } return finRes } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:9","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"429.N叉树的层序遍历 https://leetcode-cn.com/problems/n-ary-tree-level-order-traversal/ func levelOrder(root *Node) [][]int { queue:=list.New() res:=[][]int{}//结果集 if root==nil{ return res } queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len()//记录当前层的数量 var tmp []int for T:=0;T\u003clength;T++{//该层的每个元素：一添加到该层的结果集中；二找到该元素的下层元素加入到队列中，方便下次使用 myNode:=queue.Remove(queue.Front()).(*Node) tmp=append(tmp,myNode.Val) for i:=0;i\u003clen(myNode.Children);i++{ queue.PushBack(myNode.Children[i]) } } res=append(res,tmp) } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:10","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"515.在每个树行中找最大值 https://leetcode-cn.com/problems/find-largest-value-in-each-tree-row/ /** 515. 在每个树行中找最大值 */ func largestValues(root *TreeNode) []int { res:=[][]int{} var finRes []int if root==nil{//防止为空 return finRes } queue:=list.New() queue.PushBack(root) var tmpArr []int //层次遍历 for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node.Val)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]int{}//清空层的数据 } //找到每层的最大值 for i:=0;i\u003clen(res);i++{ finRes=append(finRes,max(res[i]...)) } return finRes } func max(vals...int) int { max:=int(math.Inf(-1))//负无穷 for _, val := range vals { if val \u003e max { max = val } } return max } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:11","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"116.填充每个节点的下一个右侧节点指针 https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node/ /** 116. 填充每个节点的下一个右侧节点指针 117. 填充每个节点的下一个右侧节点指针 II */ func connect(root *Node) *Node { res:=[][]*Node{} if root==nil{//防止为空 return root } queue:=list.New() queue.PushBack(root) var tmpArr []*Node for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*Node)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]*Node{}//清空层的数据 } //遍历每层元素,指定next for i:=0;i\u003clen(res);i++{ for j:=0;j\u003clen(res[i])-1;j++{ res[i][j].Next=res[i][j+1] } } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:12","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"117.填充每个节点的下一个右侧节点指针II https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node-ii/ /** 116. 填充每个节点的下一个右侧节点指针 117. 填充每个节点的下一个右侧节点指针 II */ func connect(root *Node) *Node { res:=[][]*Node{} if root==nil{//防止为空 return root } queue:=list.New() queue.PushBack(root) var tmpArr []*Node for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*Node)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]*Node{}//清空层的数据 } //遍历每层元素,指定next for i:=0;i\u003clen(res);i++{ for j:=0;j\u003clen(res[i])-1;j++{ res[i][j].Next=res[i][j+1] } } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:13","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"104.二叉树的最大深度 https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/ /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func maxDepth(root *TreeNode) int { ans:=0 if root==nil{ return 0 } queue:=list.New() queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } ans++//记录深度，其他的是层序遍历的板子 } return ans } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:14","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"111.二叉树的最小深度 https://leetcode-cn.com/problems/minimum-depth-of-binary-tree/ go： /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func minDepth(root *TreeNode) int { ans:=0 if root==nil{ return 0 } queue:=list.New() queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left==nil\u0026\u0026node.Right==nil{//当前节点没有左右节点，则代表此层是最小层 return ans+1//返回当前层 ans代表是上一层 } if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } ans++//记录层数 } return ans+1 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:15","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"翻转二叉树 二叉树，当然是左右翻转。 递归版本的前序遍历 func invertTree(root *TreeNode) *TreeNode { if root ==nil{ return nil } temp:=root.Left root.Left=root.Right root.Right=temp invertTree(root.Left) invertTree(root.Right) return root } 递归版本的后序遍历 func invertTree(root *TreeNode) *TreeNode { if root==nil{ return root } invertTree(root.Left)//遍历左节点 invertTree(root.Right)//遍历右节点 root.Left,root.Right=root.Right,root.Left//交换 return root } 迭代版本的前序遍历 func invertTree(root *TreeNode) *TreeNode { stack:=[]*TreeNode{} node:=root for node!=nil||len(stack)\u003e0{ for node!=nil{ node.Left,node.Right=node.Right,node.Left//交换 stack=append(stack,node) node=node.Left } node=stack[len(stack)-1] stack=stack[:len(stack)-1] node=node.Right } return root } 迭代版本的后序遍历 func invertTree(root *TreeNode) *TreeNode { stack:=[]*TreeNode{} node:=root var prev *TreeNode for node!=nil||len(stack)\u003e0{ for node!=nil{ stack=append(stack,node) node=node.Left } node=stack[len(stack)-1] stack=stack[:len(stack)-1] if node.Right==nil||node.Right==prev{ node.Left,node.Right=node.Right,node.Left//交换 prev=node node=nil }else { stack=append(stack,node) node=node.Right } } return root } 层序遍历 func invertTree(root *TreeNode) *TreeNode { if root==nil{ return root } queue:=list.New() node:=root queue.PushBack(node) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ e:=queue.Remove(queue.Front()).(*TreeNode) e.Left,e.Right=e.Right,e.Left//交换 if e.Left!=nil{ queue.PushBack(e.Left) } if e.Right!=nil{ queue.PushBack(e.Right) } } } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:16","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"对称二叉树 检查二叉树是否镜像对称。 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ // 递归 func defs(left *TreeNode, right *TreeNode) bool { if left == nil \u0026\u0026 right == nil { return true; }; if left == nil || right == nil { return false; }; if left.Val != right.Val { return false; } return defs(left.Left, right.Right) \u0026\u0026 defs(right.Left, left.Right); } func isSymmetric(root *TreeNode) bool { return defs(root.Left, root.Right); } // 迭代 func isSymmetric(root *TreeNode) bool { var queue []*TreeNode; if root != nil { queue = append(queue, root.Left, root.Right); } for len(queue) \u003e 0 { left := queue[0]; right := queue[1]; queue = queue[2:]; if left == nil \u0026\u0026 right == nil { continue; } if left == nil || right == nil || left.Val != right.Val { return false; }; queue = append(queue, left.Left, right.Right, right.Left, left.Right); } return true; } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:17","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最大深度 /** * definition for a binary tree node. * type treenode struct { * val int * left *treenode * right *treenode * } */ func max (a, b int) int { if a \u003e b { return a; } return b; } // 递归 func maxdepth(root *treenode) int { if root == nil { return 0; } return max(maxdepth(root.left), maxdepth(root.right)) + 1; } // 遍历 func maxdepth(root *treenode) int { levl := 0; queue := make([]*treenode, 0); if root != nil { queue = append(queue, root); } for l := len(queue); l \u003e 0; { for ;l \u003e 0;l-- { node := queue[0]; if node.left != nil { queue = append(queue, node.left); } if node.right != nil { queue = append(queue, node.right); } queue = queue[1:]; } levl++; l = len(queue); } return levl; } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:18","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最小深度 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func min(a, b int) int { if a \u003c b { return a; } return b; } // 递归 func minDepth(root *TreeNode) int { if root == nil { return 0; } if root.Left == nil \u0026\u0026 root.Right != nil { return 1 + minDepth(root.Right); } if root.Right == nil \u0026\u0026 root.Left != nil { return 1 + minDepth(root.Left); } return min(minDepth(root.Left), minDepth(root.Right)) + 1; } // 迭代 func minDepth(root *TreeNode) int { dep := 0; queue := make([]*TreeNode, 0); if root != nil { queue = append(queue, root); } for l := len(queue); l \u003e 0; { dep++; for ; l \u003e 0; l-- { node := queue[0]; if node.Left == nil \u0026\u0026 node.Right == nil { return dep; } if node.Left != nil { queue = append(queue, node.Left); } if node.Right != nil { queue = append(queue, node.Right); } queue = queue[1:]; } l = len(queue); } return dep; } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:19","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"完全二叉树的节点个数 递归版本 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ //本题直接就是求有多少个节点，无脑存进数组算长度就行了。 func countNodes(root *TreeNode) int { if root == nil { return 0 } res := 1 if root.Right != nil { res += countNodes(root.Right) } if root.Left != nil { res += countNodes(root.Left) } return res } 利用完全二叉树特性的递归解法 func countNodes(root *TreeNode) int { if root == nil { return 0 } leftH, rightH := 0, 0 leftNode := root.Left rightNode := root.Right for leftNode != nil { leftNode = leftNode.Left leftH++ } for rightNode != nil { rightNode = rightNode.Right rightH++ } if leftH == rightH { return (2 \u003c\u003c leftH) - 1 } return countNodes(root.Left) + countNodes(root.Right) + 1 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:20","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"平衡二叉树 给定一个二叉树，判断它是否是高度平衡的二叉树。 本题中，一棵高度平衡二叉树定义为：一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过1。 func isBalanced(root *TreeNode) bool { if root==nil{ return true } if !isBalanced(root.Left) || !isBalanced(root.Right){ return false } LeftH:=maxdepth(root.Left)+1 RightH:=maxdepth(root.Right)+1 if abs(LeftH-RightH)\u003e1{ return false } return true } func maxdepth(root *TreeNode)int{ if root==nil{ return 0 } return max(maxdepth(root.Left),maxdepth(root.Right))+1 } func max(a,b int)int{ if a\u003eb{ return a } return b } func abs(a int)int{ if a\u003c0{ return -a } return a } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:21","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"二叉树的所有路径 递归法： func binaryTreePaths(root *TreeNode) []string { res := make([]string, 0) var travel func(node *TreeNode, s string) travel = func(node *TreeNode, s string) { if node.Left == nil \u0026\u0026 node.Right == nil { v := s + strconv.Itoa(node.Val) res = append(res, v) return } s = s + strconv.Itoa(node.Val) + \"-\u003e\" if node.Left != nil { travel(node.Left, s) } if node.Right != nil { travel(node.Right, s) } } travel(root, \"\") return res } 迭代法： func binaryTreePaths(root *TreeNode) []string { stack := []*TreeNode{} paths := make([]string, 0) res := make([]string, 0) if root != nil { stack = append(stack, root) paths = append(paths, \"\") } for len(stack) \u003e 0 { l := len(stack) node := stack[l-1] path := paths[l-1] stack = stack[:l-1] paths = paths[:l-1] if node.Left == nil \u0026\u0026 node.Right == nil { res = append(res, path+strconv.Itoa(node.Val)) continue } if node.Right != nil { stack = append(stack, node.Right) paths = append(paths, path+strconv.Itoa(node.Val)+\"-\u003e\") } if node.Left != nil { stack = append(stack, node.Left) paths = append(paths, path+strconv.Itoa(node.Val)+\"-\u003e\") } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:22","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"二叉树的递归+回溯 100.相同的树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func isSameTree(p *TreeNode, q *TreeNode) bool { switch { case p == nil \u0026\u0026 q == nil: return true case p == nil || q == nil: fallthrough case p.Val != q.Val: return false } return isSameTree(p.Left, q.Left) \u0026\u0026 isSameTree(p.Right, q.Right) } 257.二叉的所有路径 递归法 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func binaryTreePaths(root *TreeNode) []string { var result []string traversal(root,\u0026result,\"\") return result } func traversal(root *TreeNode,result *[]string,pathStr string){ //判断是否为第一个元素 if len(pathStr)!=0{ pathStr=pathStr+\"-\u003e\"+strconv.Itoa(root.Val) }else{ pathStr=strconv.Itoa(root.Val) } //判断是否为叶子节点 if root.Left==nil\u0026\u0026root.Right==nil{ *result=append(*result,pathStr) return } //左右 if root.Left!=nil{ traversal(root.Left,result,pathStr) } if root.Right!=nil{ traversal(root.Right,result,pathStr) } } 回溯法 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func binaryTreePaths(root *TreeNode) []string { var result []string var path []int traversal(root,\u0026result,\u0026path) return result } func traversal(root *TreeNode,result *[]string,path *[]int){ *path=append(*path,root.Val) //判断是否为叶子节点 if root.Left==nil\u0026\u0026root.Right==nil{ pathStr:=strconv.Itoa((*path)[0]) for i:=1;i\u003clen(*path);i++{ pathStr=pathStr+\"-\u003e\"+strconv.Itoa((*path)[i]) } *result=append(*result,pathStr) return } //左右 if root.Left!=nil{ traversal(root.Left,result,path) *path=(*path)[:len(*path)-1]//回溯到上一个节点（因为traversal会加下一个节点值到path中） } if root.Right!=nil{ traversal(root.Right,result,path) *path=(*path)[:len(*path)-1]//回溯 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:23","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"左叶子之和 递归法 func sumOfLeftLeaves(root *TreeNode) int { var res int findLeft(root,\u0026res) return res } func findLeft(root *TreeNode,res *int){ //左节点 if root.Left!=nil\u0026\u0026root.Left.Left==nil\u0026\u0026root.Left.Right==nil{ *res=*res+root.Left.Val } if root.Left!=nil{ findLeft(root.Left,res) } if root.Right!=nil{ findLeft(root.Right,res) } } 迭代法 func sumOfLeftLeaves(root *TreeNode) int { var res int queue:=list.New() queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil\u0026\u0026node.Left.Left==nil\u0026\u0026node.Left.Right==nil{ res=res+node.Left.Val } if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:24","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"找树左下角的值 给定一个二叉树，在树的最后一行找到最左边的值。 递归法： var maxDeep int // 全局变量 深度 var value int //全局变量 最终值 func findBottomLeftValue(root *TreeNode) int { if root.Left==nil\u0026\u0026root.Right==nil{//需要提前判断一下（不要这个if的话提交结果会出错，但执行代码不会。防止这种情况出现，故先判断是否只有一个节点） return root.Val } findLeftValue (root,maxDeep) return value } func findLeftValue (root *TreeNode,deep int){ //最左边的值在左边 if root.Left==nil\u0026\u0026root.Right==nil{ if deep\u003emaxDeep{ value=root.Val maxDeep=deep } } //递归 if root.Left!=nil{ deep++ findLeftValue(root.Left,deep) deep--//回溯 } if root.Right!=nil{ deep++ findLeftValue(root.Right,deep) deep--//回溯 } } 迭代法： func findBottomLeftValue(root *TreeNode) int { queue:=list.New() var gradation int queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if i==0{gradation=node.Val} if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } } return gradation } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:25","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"路径总和 给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和 路径总和 //递归法 /** * definition for a binary tree node. * type treenode struct { * val int * left *treenode * right *treenode * } */ func haspathsum(root *treenode, targetsum int) bool { var flage bool //找没找到的标志 if root==nil{ return flage } pathsum(root,0,targetsum,\u0026flage) return flage } func pathsum(root *treenode, sum int,targetsum int,flage *bool){ sum+=root.val if root.left==nil\u0026\u0026root.right==nil\u0026\u0026sum==targetsum{ *flage=true return } if root.left!=nil\u0026\u0026!(*flage){//左节点不为空且还没找到 pathsum(root.left,sum,targetsum,flage) } if root.right!=nil\u0026\u0026!(*flage){//右节点不为空且没找到 pathsum(root.right,sum,targetsum,flage) } } 113 递归法 /** * definition for a binary tree node. * type treenode struct { * val int * left *treenode * right *treenode * } */ func pathsum(root *treenode, targetsum int) [][]int { var result [][]int//最终结果 if root==nil{ return result } var sumnodes []int//经过路径的节点集合 haspathsum(root,\u0026sumnodes,targetsum,\u0026result) return result } func haspathsum(root *treenode,sumnodes *[]int,targetsum int,result *[][]int){ *sumnodes=append(*sumnodes,root.val) if root.left==nil\u0026\u0026root.right==nil{//叶子节点 fmt.println(*sumnodes) var sum int var number int for k,v:=range *sumnodes{//求该路径节点的和 sum+=v number=k } tempnodes:=make([]int,number+1)//新的nodes接受指针里的值，防止最终指针里的值发生变动，导致最后的结果都是最后一个sumnodes的值 for k,v:=range *sumnodes{ tempnodes[k]=v } if sum==targetsum{ *result=append(*result,tempnodes) } } if root.left!=nil{ haspathsum(root.left,sumnodes,targetsum,result) *sumnodes=(*sumnodes)[:len(*sumnodes)-1]//回溯 } if root.right!=nil{ haspathsum(root.right,sumnodes,targetsum,result) *sumnodes=(*sumnodes)[:len(*sumnodes)-1]//回溯 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:26","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"从中序、后序遍历序列构造二叉树 106 从中序与后序遍历序列构造二叉树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func buildTree(inorder []int, postorder []int) *TreeNode { if len(inorder)\u003c1||len(postorder)\u003c1{return nil} //先找到根节点（后续遍历的最后一个就是根节点） nodeValue:=postorder[len(postorder)-1] //从中序遍历中找到一分为二的点，左边为左子树，右边为右子树 left:=findRootIndex(inorder,nodeValue) //构造root root:=\u0026TreeNode{Val: nodeValue, Left: buildTree(inorder[:left],postorder[:left]),//将后续遍历一分为二，左边为左子树，右边为右子树 Right: buildTree(inorder[left+1:],postorder[left:len(postorder)-1])} return root } func findRootIndex(inorder []int,target int) (index int){ for i:=0;i\u003clen(inorder);i++{ if target==inorder[i]{ return i } } return -1 } 105 从前序与中序遍历序列构造二叉树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func buildTree(preorder []int, inorder []int) *TreeNode { if len(preorder)\u003c1||len(inorder)\u003c1{return nil} left:=findRootIndex(preorder[0],inorder) root:=\u0026TreeNode{ Val: preorder[0], Left: buildTree(preorder[1:left+1],inorder[:left]), Right: buildTree(preorder[left+1:],inorder[left+1:])} return root } func findRootIndex(target int,inorder []int) int{ for i:=0;i\u003clen(inorder);i++{ if target==inorder[i]{ return i } } return -1 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:27","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最大二叉树 给定一个不含重复元素的整数数组。一个以此数组构建的最大二叉树定义如下： 二叉树的根是数组中的最大元素。 左子树是通过数组中最大值左边部分构造出的最大二叉树。 右子树是通过数组中最大值右边部分构造出的最大二叉树。 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func constructMaximumBinaryTree(nums []int) *TreeNode { if len(nums)\u003c1{return nil} //首选找到最大值 index:=findMax(nums) //其次构造二叉树 root:=\u0026TreeNode{ Val: nums[index], Left:constructMaximumBinaryTree(nums[:index]),//左半边 Right:constructMaximumBinaryTree(nums[index+1:]),//右半边 } return root } func findMax(nums []int) (index int){ for i:=0;i\u003clen(nums);i++{ if nums[i]\u003enums[index]{ index=i } } return } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:28","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"合并二叉树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ //前序遍历（递归遍历，跟105 106差不多的思路） func mergeTrees(t1 *TreeNode, t2 *TreeNode) *TreeNode { var value int var nullNode *TreeNode//空node，便于遍历 nullNode=\u0026TreeNode{ Val:0, Left:nil, Right:nil} switch { case t1==nil\u0026\u0026t2==nil: return nil//终止条件 default : //如果其中一个节点为空，则将该节点置为nullNode，方便下次遍历 if t1==nil{ value=t2.Val t1=nullNode }else if t2==nil{ value=t1.Val t2=nullNode }else { value=t1.Val+t2.Val } } root:=\u0026TreeNode{//构造新的二叉树 Val: value, Left: mergeTrees(t1.Left,t2.Left), Right: mergeTrees(t1.Right,t2.Right)} return root } // 前序遍历简洁版 func mergeTrees(root1 *TreeNode, root2 *TreeNode) *TreeNode { if root1 == nil { return root2 } if root2 == nil { return root1 } root1.Val += root2.Val root1.Left = mergeTrees(root1.Left, root2.Left) root1.Right = mergeTrees(root1.Right, root2.Right) return root1 } // 迭代版本 func mergeTrees(root1 *TreeNode, root2 *TreeNode) *TreeNode { queue := make([]*TreeNode,0) if root1 == nil{ return root2 } if root2 == nil{ return root1 } queue = append(queue,root1) queue = append(queue,root2) for size:=len(queue);size\u003e0;size=len(queue){ node1 := queue[0] queue = queue[1:] node2 := queue[0] queue = queue[1:] node1.Val += node2.Val // 左子树都不为空 if node1.Left != nil \u0026\u0026 node2.Left != nil{ queue = append(queue,node1.Left) queue = append(queue,node2.Left) } // 右子树都不为空 if node1.Right !=nil \u0026\u0026 node2.Right !=nil{ queue = append(queue,node1.Right) queue = append(queue,node2.Right) } // 树 1 的左子树为 nil，直接接上树 2 的左子树 if node1.Left == nil{ node1.Left = node2.Left } // 树 1 的右子树为 nil，直接接上树 2 的右子树 if node1.Right == nil{ node1.Right = node2.Right } } return root1 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:29","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"二叉搜索树 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:30","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"搜索 给定二叉搜索树（BST）的根节点和一个值。 你需要在BST中找到节点值等于给定值的节点。 返回以该节点为根的子树。 如果节点不存在，则返回 NULL。 递归法： //递归法 func searchBST(root *TreeNode, val int) *TreeNode { if root==nil||root.Val==val{ return root } if root.Val\u003eval{ return searchBST(root.Left,val) } return searchBST(root.Right,val) } 迭代法： //迭代法 func searchBST(root *TreeNode, val int) *TreeNode { for root!=nil{ if root.Val\u003eval{ root=root.Left }else if root.Val\u003cval{ root=root.Right }else{ break } } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:31","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"验证 给定一个二叉树，判断其是否是一个有效的二叉搜索树。 假设一个二叉搜索树具有如下特征： 节点的左子树只包含小于当前节点的数。 节点的右子树只包含大于当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 import \"math\" func isValidBST(root *TreeNode) bool { // 二叉搜索树也可以是空树 if root == nil { return true } // 由题目中的数据限制可以得出min和max return check(root,math.MinInt64,math.MaxInt64) } func check(node *TreeNode,min,max int64) bool { if node == nil { return true } if min \u003e= int64(node.Val) || max \u003c= int64(node.Val) { return false } // 分别对左子树和右子树递归判断，如果左子树和右子树都符合则返回true return check(node.Right,int64(node.Val),max) \u0026\u0026 check(node.Left,min,int64(node.Val)) } // 中序遍历解法 func isValidBST(root *TreeNode) bool { // 保存上一个指针 var prev *TreeNode var travel func(node *TreeNode) bool travel = func(node *TreeNode) bool { if node == nil { return true } leftRes := travel(node.Left) // 当前值小于等于前一个节点的值，返回false if prev != nil \u0026\u0026 node.Val \u003c= prev.Val { return false } prev = node rightRes := travel(node.Right) return leftRes \u0026\u0026 rightRes } return travel(root) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:32","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最小绝对差 给你一棵所有节点为非负值的二叉搜索树，请你计算树中任意两节点的差的绝对值的最小值。 中序遍历，然后计算最小差值 func getMinimumDifference(root *TreeNode) int { var res []int findMIn(root,\u0026res) min:=1000000//一个比较大的值 for i:=1;i\u003clen(res);i++{ tempValue:=res[i]-res[i-1] if tempValue\u003cmin{ min=tempValue } } return min } //中序遍历 func findMIn(root *TreeNode,res *[]int){ if root==nil{return} findMIn(root.Left,res) *res=append(*res,root.Val) findMIn(root.Right,res) } // 中序遍历的同时计算最小值 func getMinimumDifference(root *TreeNode) int { // 保留前一个节点的指针 var prev *TreeNode // 定义一个比较大的值 min := math.MaxInt64 var travel func(node *TreeNode) travel = func(node *TreeNode) { if node == nil { return } travel(node.Left) if prev != nil \u0026\u0026 node.Val - prev.Val \u003c min { min = node.Val - prev.Val } prev = node travel(node.Right) } travel(root) return min } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:33","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"众数 给定一个有相同值的二叉搜索树（BST），找出 BST 中的所有众数（出现频率最高的元素）。 暴力法 func findMode(root *TreeNode) []int { var history map[int]int var maxValue int var maxIndex int var result []int history=make(map[int]int) traversal(root,history) for k,value:=range history{ if value\u003emaxValue{ maxValue=value maxIndex=k } } for k,value:=range history{ if value==history[maxIndex]{ result=append(result,k) } } return result } func traversal(root *TreeNode,history map[int]int){ if root.Left!=nil{ traversal(root.Left,history) } if value,ok:=history[root.Val];ok{ history[root.Val]=value+1 }else{ history[root.Val]=1 } if root.Right!=nil{ traversal(root.Right,history) } } 计数法，不使用额外空间，利用二叉树性质，中序遍历 func findMode(root *TreeNode) []int { res := make([]int, 0) count := 1 max := 1 var prev *TreeNode var travel func(node *TreeNode) travel = func(node *TreeNode) { if node == nil { return } travel(node.Left) if prev != nil \u0026\u0026 prev.Val == node.Val { count++ } else { count = 1 } if count \u003e= max { if count \u003e max \u0026\u0026 len(res) \u003e 0 { res = []int{node.Val} } else { res = append(res, node.Val) } max = count } prev = node travel(node.Right) } travel(root) return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:34","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"二叉树最近公共祖先 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先（可以是自己） func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { // check if root == nil { return root } // 相等 直接返回root节点即可 if root == p || root == q { return root } // Divide left := lowestCommonAncestor(root.Left, p, q) right := lowestCommonAncestor(root.Right, p, q) // Conquer // 左右两边都不为空，则根节点为祖先 if left != nil \u0026\u0026 right != nil { return root } if left != nil { return left } if right != nil { return right } return nil } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:35","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"二叉搜索树的最近公共祖先 递归法： //利用BSL的性质（前序遍历有序） func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { if root==nil{return nil} if root.Val\u003ep.Val\u0026\u0026root.Val\u003eq.Val{//当前节点的值大于给定的值，则说明满足条件的在左边 return lowestCommonAncestor(root.Left,p,q) }else if root.Val\u003cp.Val\u0026\u0026root.Val\u003cq.Val{//当前节点的值小于各点的值，则说明满足条件的在右边 return lowestCommonAncestor(root.Right,p,q) }else {return root}//当前节点的值在给定值的中间（或者等于），即为最深的祖先 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:36","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"插入操作 给定二叉搜索树（BST）的根节点和要插入树中的值，将值插入二叉搜索树。 返回插入后二叉搜索树的根节点。 输入数据保证，新值和原始二叉搜索树中的任意节点值都不同。 递归法 func insertIntoBST(root *TreeNode, val int) *TreeNode { if root == nil { root = \u0026TreeNode{Val: val} return root } if root.Val \u003e val { root.Left = insertIntoBST(root.Left, val) } else { root.Right = insertIntoBST(root.Right, val) } return root } 迭代法 func insertIntoBST(root *TreeNode, val int) *TreeNode { if root == nil { return \u0026TreeNode{Val:val} } node := root var pnode *TreeNode for node != nil { if val \u003e node.Val { pnode = node node = node.Right } else { pnode = node node = node.Left } } if val \u003e pnode.Val { pnode.Right = \u0026TreeNode{Val: val} } else { pnode.Left = \u0026TreeNode{Val: val} } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:37","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"删除节点 搜索树的节点删除要比节点增加复杂的多。 // 递归版本 func deleteNode(root *TreeNode, key int) *TreeNode { if root==nil{ return nil } if key\u003croot.Val{ root.Left=deleteNode(root.Left,key) return root } if key\u003eroot.Val{ root.Right=deleteNode(root.Right,key) return root } if root.Right==nil{ return root.Left } if root.Left==nil{ return root.Right } minnode:=root.Right for minnode.Left!=nil{ minnode=minnode.Left } root.Val=minnode.Val root.Right=deleteNode1(root.Right) return root } func deleteNode1(root *TreeNode)*TreeNode{ if root.Left==nil{ pRight:=root.Right root.Right=nil return pRight } root.Left=deleteNode1(root.Left) return root } // 迭代版本 func deleteOneNode(target *TreeNode) *TreeNode { if target == nil { return target } if target.Right == nil { return target.Left } cur := target.Right for cur.Left != nil { cur = cur.Left } cur.Left = target.Left return target.Right } func deleteNode(root *TreeNode, key int) *TreeNode { // 特殊情况处理 if root == nil { return root } cur := root var pre *TreeNode for cur != nil { if cur.Val == key { break } pre = cur if cur.Val \u003e key { cur = cur.Left } else { cur = cur.Right } } if pre == nil { return deleteOneNode(cur) } // pre 要知道是删除左孩子还有右孩子 if pre.Left != nil \u0026\u0026 pre.Left.Val == key { pre.Left = deleteOneNode(cur) } if pre.Right != nil \u0026\u0026 pre.Right.Val == key { pre.Right = deleteOneNode(cur) } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:38","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"修剪 给定一个二叉搜索树，同时给定最小边界L 和最大边界 R。通过修剪二叉搜索树，使得所有节点的值在[L, R]中 (R\u003e=L) 。你可能需要改变树的根节点，所以结果应当返回修剪好的二叉搜索树的新的根节点。 // 递归 func trimBST(root *TreeNode, low int, high int) *TreeNode { if root==nil{ return nil } if root.Val\u003clow{//如果该节点值小于最小值，则该节点更换为该节点的右节点值，继续遍历 right:=trimBST(root.Right,low,high) return right } if root.Val\u003ehigh{//如果该节点的值大于最大值，则该节点更换为该节点的左节点值，继续遍历 left:=trimBST(root.Left,low,high) return left } root.Left=trimBST(root.Left,low,high) root.Right=trimBST(root.Right,low,high) return root } // 迭代 func trimBST(root *TreeNode, low int, high int) *TreeNode { if root == nil { return nil } // 处理 root，让 root 移动到[low, high] 范围内，**注意**是左闭右闭 for root != nil \u0026\u0026 (root.Val\u003clow||root.Val\u003ehigh){ if root.Val \u003c low{ root = root.Right }else{ root = root.Left } } cur := root // 此时 root 已经在[low, high] 范围内，处理左孩子元素小于 low 的情况（左节点是一定小于 root.Val，因此天然小于 high） for cur != nil{ for cur.Left!=nil \u0026\u0026 cur.Left.Val \u003c low{ cur.Left = cur.Left.Right } cur = cur.Left } cur = root // 此时 root 已经在[low, high] 范围内，处理右孩子大于 high 的情况 for cur != nil{ for cur.Right!=nil \u0026\u0026 cur.Right.Val \u003e high{ cur.Right = cur.Right.Left } cur = cur.Right } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:39","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"将有序数组转化为二叉搜索树 将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。 本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。 递归（隐含回溯） func sortedArrayToBST(nums []int) *TreeNode { if len(nums)==0{return nil}//终止条件，最后数组为空则可以返回 root:=\u0026TreeNode{nums[len(nums)/2],nil,nil}//按照BSL的特点，从中间构造节点 root.Left=sortedArrayToBST(nums[:len(nums)/2])//数组的左边为左子树 root.Right=sortedArrayToBST(nums[len(nums)/2+1:])//数字的右边为右子树 return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:40","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"把二叉搜索树转化为累加树 给出二叉 搜索 树的根节点，该树的节点值各不相同，请你将其转换为累加树（Greater Sum Tree），使每个节点 node 的新值等于原树中大于或等于 node.val 的值之和。 弄一个sum暂存其和值 //右中左 func bstToGst(root *TreeNode) *TreeNode { var sum int RightMLeft(root,\u0026sum) return root } func RightMLeft(root *TreeNode,sum *int) *TreeNode { if root==nil{return nil}//终止条件，遇到空节点就返回 RightMLeft(root.Right,sum)//先遍历右边 temp:=*sum//暂存总和值 *sum+=root.Val//将总和值变更 root.Val+=temp//更新节点值 RightMLeft(root.Left,sum)//遍历左节点 return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:41","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"回溯算法 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"理论基础 也叫回溯搜索算法。 回溯是递归的副产品，只要有递归就会有回溯 回溯的本质是穷举，穷举所有可能，然后选出我们想要的答案，并不算高效。加一些剪枝操作或许会高效一点。 一般用来解决除了暴力搜索无可奈何的情况。 回溯法，一般可以解决如下几种问题： 组合问题：N个数里面按一定规则找出k个数的集合 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 排列问题：N个数按一定规则全排列，有几种排列方式 棋盘问题：N皇后，解数独等等 回溯法解决的问题都可以抽象为树形结构 因为回溯法解决的都是在集合中递归查找子集，集合的大小就构成了树的宽度，递归的深度，都构成的树的深度。 回溯模板： void backtracking(参数) { if (终止条件) { 存放结果; return; } for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) { 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"*组合问题及其优化 给定两个整数 n 和 k，返回 1 … n 中所有可能的 k 个数的组合。 回溯法三部曲：函数参数、终止条件和单层搜索 剪枝优化： 可以剪枝的地方就在递归中每一层的for循环所选择的起始位置。 如果for循环选择的起始位置之后的元素个数已经不足我们需要的元素个数了，那么就没有必要搜索了。 var res [][]int func combine(n int, k int) [][]int { res=[][]int{} if n \u003c= 0 || k \u003c= 0 || k \u003e n { return res } backtrack(n, k, 1, []int{}) return res } func backtrack(n,k,start int,track []int){ if len(track)==k{ temp:=make([]int,k) copy(temp,track) res=append(res,temp) } if len(track)+n-start+1 \u003c k { return } for i:=start;i\u003c=n;i++{ track=append(track,i) backtrack(n,k,i+1,track) track=track[:len(track)-1] } } 剪枝：go语言的剪枝优化会爆内存溢出，不知道是为啥…… var res [][]int func combine(n int, k int) [][]int { res=[][]int{} if n \u003c= 0 || k \u003c= 0 || k \u003e n { return res } backtrack(n, k, 1, []int{}) return res } func backtrack(n,k,start int,track []int){ if len(track)==k{ temp:=make([]int,k) copy(temp,track) res=append(res,temp) } if len(track)+n-start+1 \u003c k { return } for i:=start;i\u003c=(n-k+len(track)+1);i++{ track=append(track,i) backtrack(n,k,i+1,track) track=track[:len(track)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"组合总和III 找出所有相加之和为 n 的 k 个数的组合。组合中只允许含有 1 - 9 的正整数，并且每种组合中不存在重复的数字。 说明： 所有数字都是正整数。 解集不能包含重复的组合。 回溯+减枝 func combinationSum3(k int, n int) [][]int { var track []int// 遍历路径 var result [][]int// 存放结果集 backTree(n,k,1,\u0026track,\u0026result) return result } func backTree(n,k,startIndex int,track *[]int,result *[][]int){ if len(*track)==k{ var sum int tmp:=make([]int,k) for k,v:=range *track{ sum+=v tmp[k]=v } if sum==n{ *result=append(*result,tmp) } return } for i:=startIndex;i\u003c=9-(k-len(*track))+1;i++{//减枝（k-len(*track)表示还剩多少个可填充的元素） *track=append(*track,i)//记录路径 backTree(n,k,i+1,track,result)//递归 *track=(*track)[:len(*track)-1]//回溯 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"电话号码的字母组合 给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。 给出数字到字母的映射如电话按键。注意 1 不对应任何字母。 主要在于递归中传递下一个数字 func letterCombinations(digits string) []string { lenth:=len(digits) if lenth==0 ||lenth\u003e4{ return nil } digitsMap:= [10]string{ \"\", // 0 \"\", // 1 \"abc\", // 2 \"def\", // 3 \"ghi\", // 4 \"jkl\", // 5 \"mno\", // 6 \"pqrs\", // 7 \"tuv\", // 8 \"wxyz\", // 9 } res:=make([]string,0) recursion(\"\",digits,0,digitsMap,\u0026res) return res } func recursion(tempString ,digits string, Index int,digitsMap [10]string, res *[]string) {//index表示第几个数字 if len(tempString)==len(digits){//终止条件，字符串长度等于digits的长度 *res=append(*res,tempString) return } tmpK:=digits[Index]-'0' // 将index指向的数字转为int（确定下一个数字） letter:=digitsMap[tmpK]// 取数字对应的字符集 for i:=0;i\u003clen(letter);i++{ tempString=tempString+string(letter[i])//拼接结果 recursion(tempString,digits,Index+1,digitsMap,res) tempString=tempString[:len(tempString)-1]//回溯 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"组合总和 给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。candidates 中的数字可以无限制重复被选取。 主要在于递归中传递下一个数字 func combinationSum(candidates []int, target int) [][]int { var trcak []int var res [][]int backtracking(0,0,target,candidates,trcak,\u0026res) return res } func backtracking(startIndex,sum,target int,candidates,trcak []int,res *[][]int){ //终止条件 if sum==target{ tmp:=make([]int,len(trcak)) copy(tmp,trcak)//拷贝 *res=append(*res,tmp)//放入结果集 return } if sum\u003etarget{return} //回溯 for i:=startIndex;i\u003clen(candidates);i++{ //更新路径集合和sum trcak=append(trcak,candidates[i]) sum+=candidates[i] //递归 backtracking(i,sum,target,candidates,trcak,res) //回溯 trcak=trcak[:len(trcak)-1] sum-=candidates[i] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"组合总和II 给定一个数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。 candidates 中的每个数字在每个组合中只能使用一次。 主要在于如何在回溯中去重 使用used数组 func combinationSum2(candidates []int, target int) [][]int { var trcak []int var res [][]int var history map[int]bool history=make(map[int]bool) sort.Ints(candidates) backtracking(0,0,target,candidates,trcak,\u0026res,history) return res } func backtracking(startIndex,sum,target int,candidates,trcak []int,res *[][]int,history map[int]bool){ //终止条件 if sum==target{ tmp:=make([]int,len(trcak)) copy(tmp,trcak)//拷贝 *res=append(*res,tmp)//放入结果集 return } if sum\u003etarget{return} //回溯 // used[i - 1] == true，说明同一树枝candidates[i - 1]使用过 // used[i - 1] == false，说明同一树层candidates[i - 1]使用过 for i:=startIndex;i\u003clen(candidates);i++{ if i\u003e0\u0026\u0026candidates[i]==candidates[i-1]\u0026\u0026history[i-1]==false{ continue } //更新路径集合和sum trcak=append(trcak,candidates[i]) sum+=candidates[i] history[i]=true //递归 backtracking(i+1,sum,target,candidates,trcak,res,history) //回溯 trcak=trcak[:len(trcak)-1] sum-=candidates[i] history[i]=false } } 不使用used数组 func combinationSum2(candidates []int, target int) [][]int { var trcak []int var res [][]int sort.Ints(candidates) backtracking(0,0,target,candidates,trcak,\u0026res) return res } func backtracking(startIndex,sum,target int,candidates,trcak []int,res *[][]int){ //终止条件 if sum==target{ tmp:=make([]int,len(trcak)) //拷贝 copy(tmp,trcak) //放入结果集 *res=append(*res,tmp) return } //回溯 for i:=startIndex;i\u003clen(candidates) \u0026\u0026 sum+candidates[i]\u003c=target;i++{ // 若当前树层有使用过相同的元素，则跳过 if i\u003estartIndex\u0026\u0026candidates[i]==candidates[i-1]{ continue } //更新路径集合和sum trcak=append(trcak,candidates[i]) sum+=candidates[i] backtracking(i+1,sum,target,candidates,trcak,res) //回溯 trcak=trcak[:len(trcak)-1] sum-=candidates[i] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"分割回文串 给定一个字符串 s，将 s 分割成一些子串，使每个子串都是回文串。 返回 s 所有可能的分割方案。 示例: 输入: \"aab\" 输出: [ [\"aa\",\"b\"], [\"a\",\"a\",\"b\"] ] 注意切片（go切片是披着值类型外衣的引用类型） func partition(s string) [][]string { var tmpString []string//切割字符串集合 var res [][]string//结果集合 backTracking(s,tmpString,0,\u0026res) return res } func backTracking(s string,tmpString []string,startIndex int,res *[][]string){ if startIndex==len(s){//到达字符串末尾了 //进行一次切片拷贝，怕之后的操作影响tmpString切片内的值 t := make([]string, len(tmpString)) copy(t, tmpString) *res=append(*res,t) } for i:=startIndex;i\u003clen(s);i++{ //处理（首先通过startIndex和i判断切割的区间，进而判断该区间的字符串是否为回文，若为回文，则加入到tmpString，否则继续后移，找到回文区间）（这里为一层处理） if isPartition(s,startIndex,i){ tmpString=append(tmpString,s[startIndex:i+1]) }else{ continue } //递归 backTracking(s,tmpString,i+1,res) //回溯 tmpString=tmpString[:len(tmpString)-1] } } //判断是否为回文 func isPartition(s string,startIndex,end int)bool{ left:=startIndex right:=end for ;left\u003cright;{ if s[left]!=s[right]{ return false } //移动左右指针 left++ right-- } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:7","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"复原IP地址 给定一个只包含数字的字符串，复原它并返回所有可能的 IP 地址格式。 有效的 IP 地址 正好由四个整数（每个整数位于 0 到 255 之间组成，且不能含有前导 0），整数之间用 ‘.’ 分隔。 例如：“0.1.2.201” 和 “192.168.1.1” 是 有效的 IP 地址，但是 “0.011.255.245”、“192.168.1.312” 和 “192.168@1.1” 是 无效的 IP 地址。 示例 1： 输入：s = \"25525511135\" 输出：[\"255.255.11.135\",\"255.255.111.35\"] 示例 2： 输入：s = \"0000\" 输出：[\"0.0.0.0\"] 回溯（对于前导 0的IP（特别注意s[startIndex]==‘0’的判断，不应该写成s[startIndex]==0，因为s截取出来不是数字）） func restoreIpAddresses(s string) []string { var res,path []string backTracking(s,path,0,\u0026res) return res } func backTracking(s string,path []string,startIndex int,res *[]string){ //终止条件 if startIndex==len(s)\u0026\u0026len(path)==4{ tmpIpString:=path[0]+\".\"+path[1]+\".\"+path[2]+\".\"+path[3] *res=append(*res,tmpIpString) } for i:=startIndex;i\u003clen(s);i++{ //处理 path:=append(path,s[startIndex:i+1]) if i-startIndex+1\u003c=3\u0026\u0026len(path)\u003c=4\u0026\u0026isNormalIp(s,startIndex,i){ //递归 backTracking(s,path,i+1,res) }else {//如果首尾超过了3个，或路径多余4个，或前导为0，或大于255，直接回退 return } //回溯 path=path[:len(path)-1] } } func isNormalIp(s string,startIndex,end int)bool{ checkInt,_:=strconv.Atoi(s[startIndex:end+1]) if end-startIndex+1\u003e1\u0026\u0026s[startIndex]=='0'{//对于前导 0的IP（特别**注意**s[startIndex]=='0'的判断，不应该写成s[startIndex]==0，因为s截取出来不是数字） return false } if checkInt\u003e255{ return false } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:8","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"子集问题 给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 var res [][]int func subset(nums []int) [][]int { res = make([][]int, 0) sort.Ints(nums) Dfs([]int{}, nums, 0) return res } func Dfs(temp, nums []int, start int){ tmp := make([]int, len(temp)) copy(tmp, temp) res = append(res, tmp) for i := start; i \u003c len(nums); i++{ //if i\u003estart\u0026\u0026nums[i]==nums[i-1]{ // continue //} temp = append(temp, nums[i]) Dfs(temp, nums, i+1) temp = temp[:len(temp)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:9","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"子集II 给定一个可能包含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 var res[][]int func subsetsWithDup(nums []int)[][]int { res=make([][]int,0) sort.Ints(nums) dfs([]int{},nums,0) return res } func dfs(temp, num []int, start int) { tmp:=make([]int,len(temp)) copy(tmp,temp) res=append(res,tmp) for i:=start;i\u003clen(num);i++{ if i\u003estart\u0026\u0026num[i]==num[i-1]{ continue } temp=append(temp,num[i]) dfs(temp,num,i+1) temp=temp[:len(temp)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:10","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"递增子序列 给定一个整型数组, 你的任务是找到所有该数组的递增子序列，递增子序列的长度至少是2。给定数组中可能包含重复数字，相等的数字应该被视为递增的一种情况 func findSubsequences(nums []int) [][]int { var subRes []int var res [][]int backTring(0,nums,subRes,\u0026res) return res } func backTring(startIndex int,nums,subRes []int,res *[][]int){ if len(subRes)\u003e1{ tmp:=make([]int,len(subRes)) copy(tmp,subRes) *res=append(*res,tmp) } history:=[201]int{}//记录本层元素使用记录 for i:=startIndex;i\u003clen(nums);i++{ //分两种情况判断：一，当前取的元素小于子集的最后一个元素，则继续寻找下一个适合的元素 // 或者二，当前取的元素在本层已经出现过了，所以跳过该元素，继续寻找 if len(subRes)\u003e0\u0026\u0026nums[i]\u003csubRes[len(subRes)-1]||history[nums[i] + 100]==1{ continue } history[nums[i] + 100]=1//表示本层该元素使用过了 subRes=append(subRes,nums[i]) backTring(i+1,nums,subRes,res) subRes=subRes[:len(subRes)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:11","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"全排列 给定一个 没有重复 数字的序列，返回其所有可能的全排列。 var res [][]int func permute(nums []int) [][]int { res = [][]int{} backTrack(nums,len(nums),[]int{}) return res } func backTrack(nums []int,numsLen int,path []int) { if len(nums)==0{ p:=make([]int,len(path)) copy(p,path) res = append(res,p) } for i:=0;i\u003cnumsLen;i++{ cur:=nums[i] path = append(path,cur) nums = append(nums[:i],nums[i+1:]...)//直接使用切片 backTrack(nums,len(nums),path) nums = append(nums[:i],append([]int{cur},nums[i:]...)...)//回溯的时候切片也要复原，元素位置不能变 path = path[:len(path)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:12","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"全排列II 给定一个可包含重复数字的序列 nums ，按任意顺序 返回所有不重复的全排列。 var res [][]int func permute(nums []int) [][]int { res = [][]int{} backTrack(nums,len(nums),[]int{}) return res } func backTrack(nums []int,numsLen int,path []int) { if len(nums)==0{ p:=make([]int,len(path)) copy(p,path) res = append(res,p) } used := [21]int{}//跟前一题唯一的区别，同一层不使用重复的数。关于used的思想carl在递增子序列那一题中提到过 for i:=0;i\u003cnumsLen;i++{ if used[nums[i]+10]==1{ continue } cur:=nums[i] path = append(path,cur) used[nums[i]+10]=1 nums = append(nums[:i],nums[i+1:]...) backTrack(nums,len(nums),path) nums = append(nums[:i],append([]int{cur},nums[i:]...)...) path = path[:len(path)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:13","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"回溯算法去重问题的另一种写法 https://programmercarl.com/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E5%8E%BB%E9%87%8D%E9%97%AE%E9%A2%98%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%86%99%E6%B3%95.html#_90-%E5%AD%90%E9%9B%86ii 看c++版的 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:14","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"重新安排行程 深搜和回溯也是相辅相成的，都用了递归。 给定一个机票的字符串二维数组 [from, to]，子数组中的两个成员分别表示飞机出发和降落的机场地点，对该行程进行重新规划排序。所有这些机票都属于一个从 JFK（肯尼迪国际机场）出发的先生，所以该行程必须从 JFK 开始。 提示： 如果存在多种有效的行程，请你按字符自然排序返回最小的行程组合。例如，行程 [“JFK”, “LGA”] 与 [“JFK”, “LGB”] 相比就更小，排序更靠前 所有的机场都用三个大写字母表示（机场代码）。 假定所有机票至少存在一种合理的行程。 所有的机票必须都用一次 且 只能用一次。 示例 1： 输入：[[\"MUC\", \"LHR\"], [\"JFK\", \"MUC\"], [\"SFO\", \"SJC\"], [\"LHR\", \"SFO\"]] 输出：[\"JFK\", \"MUC\", \"LHR\", \"SFO\", \"SJC\"] 示例 2： 输入：[[\"JFK\",\"SFO\"],[\"JFK\",\"ATL\"],[\"SFO\",\"ATL\"],[\"ATL\",\"JFK\"],[\"ATL\",\"SFO\"]] 输出：[\"JFK\",\"ATL\",\"JFK\",\"SFO\",\"ATL\",\"SFO\"] 解释：另一种有效的行程是 [\"JFK\",\"SFO\",\"ATL\",\"JFK\",\"ATL\",\"SFO\"]。但是它自然排序更大更靠后 class Solution { private: // unordered_map\u003c出发机场, map\u003c到达机场, 航班次数\u003e\u003e targets unordered_map\u003cstring, map\u003cstring, int\u003e\u003e targets; bool backtracking(int ticketNum, vector\u003cstring\u003e\u0026 result) { if (result.size() == ticketNum + 1) { return true; } for (pair\u003cconst string, int\u003e\u0026 target : targets[result[result.size() - 1]]) { if (target.second \u003e 0 ) { // 记录到达机场是否飞过了 result.push_back(target.first); target.second--; if (backtracking(ticketNum, result)) return true; result.pop_back(); target.second++; } } return false; } public: vector\u003cstring\u003e findItinerary(vector\u003cvector\u003cstring\u003e\u003e\u0026 tickets) { targets.clear(); vector\u003cstring\u003e result; for (const vector\u003cstring\u003e\u0026 vec : tickets) { targets[vec[0]][vec[1]]++; // 记录映射关系 } result.push_back(\"JFK\"); // 起始机场 backtracking(tickets.size(), result); return result; } }; ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:15","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"N皇后 n 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。 给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。 每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 ‘Q’ 和 ‘.’ 分别代表了皇后和空位。 import \"strings\" var res [][]string func isValid(board [][]string, row, col int) (res bool){ n := len(board) for i:=0; i \u003c row; i++ { if board[i][col] == \"Q\" { return false } } for i := 0; i \u003c n; i++{ if board[row][i] == \"Q\" { return false } } for i ,j := row, col; i \u003e= 0 \u0026\u0026 j \u003e=0 ; i, j = i - 1, j- 1{ if board[i][j] == \"Q\"{ return false } } for i, j := row, col; i \u003e=0 \u0026\u0026 j \u003c n; i,j = i-1, j+1 { if board[i][j] == \"Q\" { return false } } return true } func backtrack(board [][]string, row int) { size := len(board) if row == size{ temp := make([]string, size) for i := 0; i\u003csize;i++{ temp[i] = strings.Join(board[i],\"\") } res =append(res,temp) return } for col := 0; col \u003c size; col++ { if !isValid(board, row, col){ continue } board[row][col] = \"Q\" backtrack(board, row+1) board[row][col] = \".\" } } func solveNQueens(n int) [][]string { res = [][]string{} board := make([][]string, n) for i := 0; i \u003c n; i++{ board[i] = make([]string, n) } for i := 0; i \u003c n; i++{ for j := 0; j\u003cn;j++{ board[i][j] = \".\" } } backtrack(board, 0) return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:16","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"解数独 编写一个程序，通过填充空格来解决数独问题。 func solveSudoku(board [][]byte) { var backtracking func(board [][]byte) bool backtracking=func(board [][]byte) bool{ for i:=0;i\u003c9;i++{ for j:=0;j\u003c9;j++{ //判断此位置是否适合填数字 if board[i][j]!='.'{ continue } //尝试填1-9 for k:='1';k\u003c='9';k++{ if isvalid(i,j,byte(k),board)==true{//如果满足要求就填 board[i][j]=byte(k) if backtracking(board)==true{ return true } board[i][j]='.' } } return false } } return true } backtracking(board) } //判断填入数字是否满足要求 func isvalid(row,col int,k byte,board [][]byte)bool{ for i:=0;i\u003c9;i++{//行 if board[row][i]==k{ return false } } for i:=0;i\u003c9;i++{//列 if board[i][col]==k{ return false } } //方格 startrow:=(row/3)*3 startcol:=(col/3)*3 for i:=startrow;i\u003cstartrow+3;i++{ for j:=startcol;j\u003cstartcol+3;j++{ if board[i][j]==k{ return false } } } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:8:17","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"贪心算法 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"理论基础 贪心算法一般分为如下四步： 将问题分解为若干个子问题 找出适合的贪心策略 求解每一个子问题的最优解 将局部最优解堆叠成全局最优解 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"分发饼干 假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。 对每个孩子 i，都有一个胃口值 g[i]，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j，都有一个尺寸 s[j] 。如果 s[j] \u003e= g[i]，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。 示例 1: 输入: g = [1,2,3], s = [1,1] 输出: 1 解释:你有三个孩子和两块小饼干，3个孩子的胃口值分别是：1,2,3。虽然你有两块小饼干，由于他们的尺寸都是1，你只能让胃口值是1的孩子满足。所以你应该输出1。 //排序后，局部最优 func findContentChildren(g []int, s []int) int { sort.Ints(g) sort.Ints(s) // 从小到大 child := 0 for sIdx := 0; child \u003c len(g) \u0026\u0026 sIdx \u003c len(s); sIdx++ { if s[sIdx] \u003e= g[child] {//如果饼干的大小大于或等于孩子的为空则给与，否则不给予，继续寻找选一个饼干是否符合 child++ } } return child } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"摆动序列 如果连续数字之间的差严格地在正数和负数之间交替，则数字序列称为摆动序列。第一个差（如果存在的话）可能是正数或负数。少于两个元素的序列也是摆动序列。 例如， [1,7,4,9,2,5] 是一个摆动序列，因为差值 (6,-3,5,-7,3) 是正负交替出现的。相反, [1,4,7,2,5] 和 [1,7,4,5,5] 不是摆动序列，第一个序列是因为它的前两个差值都是正数，第二个序列是因为它的最后一个差值为零。 给定一个整数序列，返回作为摆动序列的最长子序列的长度。 通过从原始序列中删除一些（也可以不删除）元素来获得子序列，剩下的元素保持其原始顺序。 示例 1: 输入: [1,7,4,9,2,5] 输出: 6 解释: 整个序列均为摆动序列。 示例 2: 输入: [1,17,5,10,13,15,10,5,16,8] 输出: 7 解释: 这个序列包含几个长度为 7 摆动序列，其中一个可为[1,17,10,13,10,16,8]。 贪心或者dp func wiggleMaxLength(nums []int) int { var count,preDiff,curDiff int count=1 if len(nums)\u003c2{ return count } for i:=0;i\u003clen(nums)-1;i++{ curDiff=nums[i+1]-nums[i] //如果有正有负则更新下标值||或者只有前一个元素为0（针对两个不等元素的序列也视作摆动序列，且摆动长度为2） if (curDiff \u003e 0 \u0026\u0026 preDiff \u003c= 0) || (preDiff \u003e= 0 \u0026\u0026 curDiff \u003c 0){ preDiff=curDiff count++ } } return count } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最大子序和 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例: 输入: [-2,1,-3,4,-1,2,1,-5,4] 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6 贪心或者dp func maxSubArray(nums []int) int { maxSum := nums[0] for i := 1; i \u003c len(nums); i++ { if nums[i] + nums[i-1] \u003e nums[i] { nums[i] += nums[i-1] } if nums[i] \u003e maxSum { maxSum = nums[i] } } return maxSum } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"买卖股票的最佳时机II 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: [7,1,5,3,6,4] 输出: 7 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4。随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 示例 2: 输入: [1,2,3,4,5] 输出: 4 解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。**注意**你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3: 输入: [7,6,4,3,1] 输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0 贪心或者dp //贪心算法 func maxProfit(prices []int) int { var sum int for i := 1; i \u003c len(prices); i++ { // 累加每次大于0的交易 if prices[i]-prices[i-1] \u003e 0 { sum += prices[i]-prices[i-1] } } return sum } //确定售卖点 func maxProfit(prices []int) int { var result,buy int prices=append(prices,0)//在price末尾加个0，防止price一直递增 /** 思路：检查后一个元素是否大于当前元素，如果小于，则表明这是一个售卖点，当前元素的值减去购买时候的值 如果不小于，说明后面有更好的售卖点， **/ for i:=0;i\u003clen(prices)-1;i++{ if prices[i]\u003eprices[i+1]{ result+=prices[i]-prices[buy] buy=i+1 }else if prices[buy]\u003eprices[i]{//更改最低购买点 buy=i } } return result } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"跳跃游戏 I 给定一个非负整数数组，你最初位于数组的第一个位置。 数组中的每个元素代表你在该位置可以跳跃的最大长度。 判断你是否能够到达最后一个位置。 func canJUmp(nums []int) bool { if len(nums)\u003c=1{ return true } dp:=make([]bool,len(nums)) dp[0]=true for i:=1;i\u003clen(nums);i++{ for j:=i-1;j\u003e=0;j--{ if dp[j]\u0026\u0026nums[j]+j\u003e=i{ dp[i]=true break } } } return dp[len(nums)-1] } II 给定一个非负整数数组，你最初位于数组的第一个位置。 数组中的每个元素代表你在该位置可以跳跃的最大长度。 你的目标是使用最少的跳跃次数到达数组的最后一个位置。 func jump(nums []int) int { dp:=make([]int ,len(nums)) dp[0]=0 for i:=1;i\u003clen(nums);i++{ dp[i]=i for j:=0;j\u003ci;j++{ if nums[j]+j\u003ei{ dp[i]=min(dp[j]+1,dp[i]) } } } return dp[len(nums)-1] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"K次取反后最大化的数组和 给定一个整数数组 A，我们只能用以下方法修改该数组：我们选择某个索引 i 并将 A[i] 替换为 -A[i]，然后总共重复这个过程 K 次。（我们可以多次选择同一个索引 i。） 以这种方式修改数组后，返回数组可能的最大和。 func largestSumAfterKNegations(nums []int, K int) int { sort.Slice(nums, func(i, j int) bool { return math.Abs(float64(nums[i])) \u003e math.Abs(float64(nums[j])) }) for i := 0; i \u003c len(nums); i++ { if K \u003e 0 \u0026\u0026 nums[i] \u003c 0 { nums[i] = -nums[i] K-- } } if K%2 == 1 { nums[len(nums)-1] = -nums[len(nums)-1] } result := 0 for i := 0; i \u003c len(nums); i++ { result += nums[i] } return result } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:7","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"加油站 在一条环路上有 N 个加油站，其中第 i 个加油站有汽油 gas[i] 升。 你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。 如果你可以绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1。 如果题目有解，该答案即为唯一答案。 输入数组均为非空数组，且长度相同。 输入数组中的元素均为非负数。 func canCompleteCircuit(gas []int, cost []int) int { curSum := 0 totalSum := 0 start := 0 for i := 0; i \u003c len(gas); i++ { curSum += gas[i] - cost[i] totalSum += gas[i] - cost[i] if curSum \u003c 0 { start = i+1 curSum = 0 } } if totalSum \u003c 0 { return -1 } return start } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:8","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"分发糖果 老师想给孩子们分发糖果，有 N 个孩子站成了一条直线，老师会根据每个孩子的表现，预先给他们评分。 你需要按照以下要求，帮助老师给这些孩子分发糖果： 每个孩子至少分配到 1 个糖果。 相邻的孩子中，评分高的孩子必须获得更多的糖果。 那么这样下来，老师至少需要准备多少颗糖果呢？ 示例 1: 输入: [1,0,2] 输出: 5 解释: 你可以分别给这三个孩子分发 2、1、2 颗糖果。 示例 2: 输入: [1,2,2] 输出: 4 解释: 你可以分别给这三个孩子分发 1、2、1 颗糖果。第三个孩子只得到 1 颗糖果，这已满足上述两个条件。 func candy(ratings []int) int { /**先确定一边，再确定另外一边 1.先从左到右，当右边的大于左边的就加1 2.再从右到左，当左边的大于右边的就再加1 **/ need:=make([]int,len(ratings)) sum:=0 //初始化(每个人至少一个糖果) for i:=0;i\u003clen(ratings);i++{ need[i]=1 } //1.先从左到右，当右边的大于左边的就加1 for i:=0;i\u003clen(ratings)-1;i++{ if ratings[i]\u003cratings[i+1]{ need[i+1]=need[i]+1 } } //2.再从右到左，当左边的大于右边的就右边加1，但要花费糖果最少，所以需要做下判断 for i:=len(ratings)-1;i\u003e0;i--{ if ratings[i-1]\u003eratings[i]{ need[i-1]=findMax(need[i-1],need[i]+1) } } //计算总共糖果 for i:=0;i\u003clen(ratings);i++{ sum+=need[i] } return sum } func findMax(num1 int ,num2 int) int{ if num1\u003enum2{ return num1 } return num2 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:9","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"柠檬水找零 在柠檬水摊上，每一杯柠檬水的售价为 5 美元。 顾客排队购买你的产品，（按账单 bills 支付的顺序）一次购买一杯。 每位顾客只买一杯柠檬水，然后向你付 5 美元、10 美元或 20 美元。你必须给每个顾客正确找零，也就是说净交易是每位顾客向你支付 5 美元。 注意，一开始你手头没有任何零钱。 如果你能给每位顾客正确找零，返回 true ，否则返回 false 。 示例 1： 输入：[5,5,5,10,20] 输出：true 解释： 前 3 位顾客那里，我们按顺序收取 3 张 5 美元的钞票。 第 4 位顾客那里，我们收取一张 10 美元的钞票，并返还 5 美元。 第 5 位顾客那里，我们找还一张 10 美元的钞票和一张 5 美元的钞票。 由于所有客户都得到了正确的找零，所以我们输出 true。 示例 2： 输入：[5,5,10] 输出：true func lemonadeChange(bills []int) bool { //left表示还剩多少 下标0位5元的个数 ，下标1为10元的个数 left:=[2]int{0,0} //第一个元素不为5，直接退出 if bills[0]!=5{ return false } for i:=0;i\u003clen(bills);i++{ //先统计5元和10元的个数 if bills[i]==5{ left[0]+=1 } if bills[i]==10{ left[1]+=1 } //接着处理找零的 tmp:=bills[i]-5 if tmp==5{ if left[0]\u003e0{ left[0]-=1 }else { return false } } if tmp==15{ if left[1]\u003e0\u0026\u0026left[0]\u003e0{ left[0]-=1 left[1]-=1 }else if left[1]==0\u0026\u0026left[0]\u003e2{ left[0]-=3 }else{ return false } } } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:10","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"根据身高重建队列 假设有打乱顺序的一群人站成一个队列，数组 people 表示队列中一些人的属性（不一定按顺序）。每个 people[i] = [hi, ki] 表示第 i 个人的身高为 hi ，前面 正好 有 ki 个身高大于或等于 hi 的人。 请你重新构造并返回输入数组 people 所表示的队列。返回的队列应该格式化为数组 queue ，其中 queue[j] = [hj, kj] 是队列中第 j 个人的属性（queue[0] 是排在队列前面的人）。 示例 1： 输入：people = [[7,0],[4,4],[7,1],[5,0],[6,1],[5,2]] 输出：[[5,0],[7,0],[5,2],[6,1],[4,4],[7,1]] 解释： 编号为 0 的人身高为 5 ，没有身高更高或者相同的人排在他前面。 编号为 1 的人身高为 7 ，没有身高更高或者相同的人排在他前面。 编号为 2 的人身高为 5 ，有 2 个身高更高或者相同的人排在他前面，即编号为 0 和 1 的人。 编号为 3 的人身高为 6 ，有 1 个身高更高或者相同的人排在他前面，即编号为 1 的人。 编号为 4 的人身高为 4 ，有 4 个身高更高或者相同的人排在他前面，即编号为 0、1、2、3 的人。 编号为 5 的人身高为 7 ，有 1 个身高更高或者相同的人排在他前面，即编号为 1 的人。 因此 [[5,0],[7,0],[5,2],[6,1],[4,4],[7,1]] 是重新构造后的队列。 func reconstructQueue(people [][]int) [][]int { //先将身高从大到小排序，确定最大个子的相对位置 sort.Slice(people,func(i,j int)bool{ if people[i][0]==people[j][0]{ return people[i][1]\u003cpeople[j][1]//这个才是当身高相同时，将K按照从小到大排序 } return people[i][0]\u003epeople[j][0]//这个只是确保身高按照由大到小的顺序来排，并不确定K是按照从小到大排序的 }) //再按照K进行插入排序，优先插入K小的 result := make([][]int, 0) for _, info := range people { result = append(result, info) copy(result[info[1] +1:], result[info[1]:])//将插入位置之后的元素后移动一位（意思是腾出空间） result[info[1]] = info//将插入元素位置插入元素 } return result } //链表法 func reconstructQueue(people [][]int) [][]int { sort.Slice(people,func (i,j int) bool { if people[i][0]==people[j][0]{ return people[i][1]\u003cpeople[j][1]//当身高相同时，将K按照从小到大排序 } //先将身高从大到小排序，确定最大个子的相对位置 return people[i][0]\u003epeople[j][0] }) l:=list.New()//创建链表 for i:=0;i\u003clen(people);i++{ position:=people[i][1] mark:=l.PushBack(people[i])//插入元素 e:=l.Front() for position!=0{//获取相对位置 position-- e=e.Next() } l.MoveBefore(mark,e)//移动位置 } res:=[][]int{} for e:=l.Front();e!=nil;e=e.Next(){ res=append(res,e.Value.([]int)) } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:11","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"用最少数量的箭引爆气球 在二维空间中有许多球形的气球。对于每个气球，提供的输入是水平方向上，气球直径的开始和结束坐标。由于它是水平的，所以纵坐标并不重要，因此只要知道开始和结束的横坐标就足够了。开始坐标总是小于结束坐标。 一支弓箭可以沿着 x 轴从不同点完全垂直地射出。在坐标 x 处射出一支箭，若有一个气球的直径的开始和结束坐标为 xstart，xend， 且满足 xstart ≤ x ≤ xend，则该气球会被引爆。可以射出的弓箭的数量没有限制。 弓箭一旦被射出之后，可以无限地前进。我们想找到使得所有气球全部被引爆，所需的弓箭的最小数量。 给你一个数组 points ，其中 points [i] = [xstart,xend] ，返回引爆所有气球所必须射出的最小弓箭数。 示例 1： 输入：points = [[10,16],[2,8],[1,6],[7,12]] 输出：2 解释：对于该样例，x = 6 可以射爆 [2,8],[1,6] 两个气球，以及 x = 11 射爆另外两个气球 示例 2： 输入：points = [[1,2],[3,4],[5,6],[7,8]] 输出：4 示例 3： 输入：points = [[1,2],[2,3],[3,4],[4,5]] 输出：2 示例 4： 输入：points = [[1,2]] 输出：1 示例 5： 输入：points = [[2,3],[2,3]] 输出：1 提示： 0 \u003c= points.length \u003c= 10^4 points[i].length == 2 -2^31 \u003c= xstart \u003c xend \u003c= 2^31 - 1 func findMinArrowShots(points [][]int) int { var res int =1//弓箭数 //先按照第一位排序 sort.Slice(points,func (i,j int) bool{ return points[i][0]\u003cpoints[j][0] }) for i:=1;i\u003clen(points);i++{ if points[i-1][1]\u003cpoints[i][0]{//如果前一位的右边界小于后一位的左边界，则一定不重合 res++ }else{ points[i][1] = min(points[i - 1][1], points[i][1]); // 更新重叠气球最小右边界,覆盖该位置的值，留到下一步使用 } } return res } func min(a,b int) int{ if a\u003eb{ return b } return a } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:12","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"无重叠区间 给定一个区间的集合，找到需要移除区间的最小数量，使剩余区间互不重叠。 注意: 可以认为区间的终点总是大于它的起点。 区间 [1,2] 和 [2,3] 的边界相互“接触”，但没有相互重叠。 示例 1: 输入: [ [1,2], [2,3], [3,4], [1,3] ] 输出: 1 解释: 移除 [1,3] 后，剩下的区间没有重叠。 func eraseOverlapIntervals(intervals [][]int) int { var flag int //先排序 sort.Slice(intervals,func(i,j int)bool{ return intervals[i][0]\u003cintervals[j][0] }) fmt.Println(intervals) for i:=1;i\u003clen(intervals);i++{ if intervals[i-1][1]\u003eintervals[i][0]{ flag++ intervals[i][1]=min(intervals[i-1][1],intervals[i][1])//由于是先排序的，所以，第一位是递增顺序，故只需要将临近两个元素的第二个值最小值更新到该元素的第二个值即可作之后的判断 } } return flag } func min(a,b int)int{ if a\u003eb{ return b } return a } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:13","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"划分字母区间 字符串 S 由小写字母组成。我们要把这个字符串划分为尽可能多的片段，同一字母最多出现在一个片段中。返回一个表示每个字符串片段的长度的列表。 示例： 输入：S = “ababcbacadefegdehijhklij” 输出：[9,7,8] 解释： 划分结果为 “ababcbaca”, “defegde”, “hijhklij”。 每个字母最多出现在一个片段中。 像 “ababcbacadefegde”, “hijhklij” 的划分是错误的，因为划分的片段数较少。 func partitionLabels(s string) []int { var res []int; var marks [26]int; size, left, right := len(s), 0, 0; for i := 0; i \u003c size; i++ { marks[s[i] - 'a'] = i; } for i := 0; i \u003c size; i++ { right = max(right, marks[s[i] - 'a']); if i == right { res = append(res, right - left + 1); left = i + 1; } } return res; } func max(a, b int) int { if a \u003c b { a = b; } return a; } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:14","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"合并区间 给出一个区间的集合，请合并所有重叠的区间。 func merge(intervals [][]int) [][]int { //先从小到大排序 sort.Slice(intervals,func(i,j int)bool{ return intervals[i][0]\u003cintervals[j][0] }) //再弄重复的 for i:=0;i\u003clen(intervals)-1;i++{ if intervals[i][1]\u003e=intervals[i+1][0]{ intervals[i][1]=max(intervals[i][1],intervals[i+1][1])//赋值最大值 intervals=append(intervals[:i+1],intervals[i+2:]...) i-- } } return intervals } func max(a,b int)int{ if a\u003eb{ return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:15","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"单调递增的数字 给定一个非负整数 N，找出小于或等于 N 的最大的整数，同时这个整数需要满足其各个位数上的数字是单调递增 示例： 输入: N = 332 输出: 299 func monotoneIncreasingDigits(N int) int { s := strconv.Itoa(N)//将数字转为字符串，方便使用下标 ss := []byte(s)//将字符串转为byte数组，方便更改。 n := len(ss) if n \u003c= 1 { return N } for i:=n-1 ; i\u003e0; i-- { if ss[i-1] \u003e ss[i] {//前一个大于后一位,前一位减1，后面的全部置为9 ss[i-1] -= 1 for j := i ; j \u003c n; j++ {//后面的全部置为9 ss[j] = '9' } } } res, _ := strconv.Atoi(string(ss)) return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:16","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"买卖股票的最佳时机含手续费 给定一个整数数组 prices，其中第 i 个元素代表了第 i 天的股票价格 ；非负整数 fee 代表了交易股票的手续费用。 你可以无限次地完成交易，但是你每笔交易都需要付手续费。如果你已经购买了一个股票，在卖出它之前你就不能再继续购买股票了。 返回获得利润的最大值。 注意：这里的一笔交易指买入持有并卖出股票的整个过程，每笔交易你只需要为支付一次手续费。 示例 1: 输入: prices = [1, 3, 2, 8, 4, 9], fee = 2 输出: 8 解释: 能够达到的最大利润: 在此处买入 prices[0] = 1 在此处卖出 prices[3] = 8 在此处买入 prices[4] = 4 在此处卖出 prices[5] = 9 总利润: ((8 - 1) - 2) + ((9 - 4) - 2) = 8. 贪心或者dp func maxProfit(prices []int, fee int) int { var minBuy int = prices[0] //第一天买入 var res int for i:=0;i\u003clen(prices);i++{ //如果当前价格小于最低价，则在此处买入 if prices[i]\u003cminBuy{ minBuy=prices[i] } //如果以当前价格卖出亏本，则不卖，继续找下一个可卖点 if prices[i]\u003e=minBuy\u0026\u0026prices[i]-fee-minBuy\u003c=0{ continue } //可以售卖了 if prices[i]\u003eminBuy+fee{ //累加每天的收益 res+=prices[i]-minBuy-fee //更新最小值（如果还在收获利润的区间里，表示并不是真正的卖出，而计算利润每次都要减去手续费，所以要让minBuy = prices[i] - fee;，这样在明天收获利润的时候，才不会多减一次手续费！） minBuy=prices[i]-fee } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:17","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"监控二叉树 给定一个二叉树，我们在树的节点上安装摄像头。 节点上的每个摄影头都可以监视其父对象、自身及其直接子对象。 计算监控树的所有节点所需的最小摄像头数量。 const inf = math.MaxInt64 / 2 func minCameraCover(root *TreeNode) int { var dfs func(*TreeNode) (a, b, c int) dfs = func(node *TreeNode) (a, b, c int) { if node == nil { return inf, 0, 0 } lefta, leftb, leftc := dfs(node.Left) righta, rightb, rightc := dfs(node.Right) a = leftc + rightc + 1 b = min(a, min(lefta+rightb, righta+leftb)) c = min(a, leftb+rightb) return } _, ans, _ := dfs(root) return ans } func min(a, b int) int { if a \u003c= b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:9:18","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"动态规划 有很多重叠子问题，优先考虑使用动态规划。 与贪心的区别：贪心不会考虑之前的状态而只考虑局部最优。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"理论基础 dp步骤： 确定dp数组（dp table）以及下标的含义 确定递推公式 dp数组如何初始化 确定遍历顺序 举例推导dp数组 debug:把dp数组打印出来 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:1","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"斐波那契数 func fib(n int) int { if n \u003c 2 { return n } a, b, c := 0, 1, 0 for i := 1; i \u003c n; i++ { c = a + b a, b = b, c } return c } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:2","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"爬楼梯 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： 输入： 2 输出： 2 解释： 有两种方法可以爬到楼顶。 1 阶 + 1 阶 2 阶 func climbStairs(n int) int { if n==1{ return 1 } dp:=make([]int,n+1) dp[1]=1 dp[2]=2 for i:=3;i\u003c=n;i++{ dp[i]=dp[i-1]+dp[i-2] } return dp[n] } 使用最小花费爬楼梯 数组的每个下标作为一个阶梯，第 i 个阶梯对应着一个非负数的体力花费值 cost[i]（下标从 0 开始）。 每当你爬上一个阶梯你都要花费对应的体力值，一旦支付了相应的体力值，你就可以选择向上爬一个阶梯或者爬两个阶梯。 请你找出达到楼层顶部的最低花费。在开始时，你可以选择从下标为 0 或 1 的元素作为初始阶梯。 示例 1： 输入：cost = [10, 15, 20] 输出：15 解释：最低花费是从 cost[1] 开始，然后走两步即可到阶梯顶，一共花费 15 func minCostClimbingStairs(cost []int) int { dp := make([]int, len(cost)) dp[0], dp[1] = cost[0], cost[1] for i := 2; i \u003c len(cost); i++ { dp[i] = min(dp[i-1], dp[i-2]) + cost[i] } return min(dp[len(cost)-1], dp[len(cost)-2]) } func min(a, b int) int { if a \u003c b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:3","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"不同路径 I 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。 问总共有多少条不同的路径？ func uniquePaths(m int, n int) int { dp := make([][]int, m) for i := range dp { dp[i] = make([]int, n) dp[i][0] = 1 } for j := 0; j \u003c n; j++ { dp[0][j] = 1 } for i := 1; i \u003c m; i++ { for j := 1; j \u003c n; j++ { dp[i][j] = dp[i-1][j] + dp[i][j-1] } } return dp[m-1][n-1] } II 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。 现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？ func uniquePathsWithObstacles(obstacleGrid [][]int) int { m,n:= len(obstacleGrid),len(obstacleGrid[0]) // 定义一个dp数组 dp := make([][]int,m) for i,_ := range dp { dp[i] = make([]int,n) } // 初始化 for i:=0;i\u003cm;i++ { // 如果是障碍物, 后面的就都是0, 不用循环了 if obstacleGrid[i][0] == 1 { break } dp[i][0]=1 } for i:=0;i\u003cn;i++ { if obstacleGrid[0][i] == 1 { break } dp[0][i]=1 } // dp数组推导过程 for i:=1;i\u003cm;i++ { for j:=1;j\u003cn;j++ { // 如果obstacleGrid[i][j]这个点是障碍物, 那么我们的dp[i][j]保持为0 if obstacleGrid[i][j] != 1 { // 否则我们需要计算当前点可以到达的路径数 dp[i][j] = dp[i-1][j]+dp[i][j-1] } } } // debug遍历dp //for i,_ := range dp { // for j,_ := range dp[i] { // fmt.Printf(\"%.2v,\",dp[i][j]) // } // fmt.Println() //} return dp[m-1][n-1] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:4","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"整数拆分 给定一个正整数 n，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回你可以获得的最大乘积。 func integerBreak(n int) int { /** 动态五部曲 1.确定dp下标及其含义 2.确定递推公式 3.确定dp初始化 4.确定遍历顺序 5.打印dp **/ dp:=make([]int,n+1) dp[1]=1 dp[2]=1 for i:=3;i\u003cn+1;i++{ for j:=1;j\u003ci-1;j++{ // i可以差分为i-j和j。由于需要最大值，故需要通过j遍历所有存在的值，取其中最大的值作为当前i的最大值，在求最大值的时候，一个是j与i-j相乘，一个是j与dp[i-j]. dp[i]=max(dp[i],max(j*(i-j),j*dp[i-j])) } } return dp[n] } func max(a,b int) int{ if a\u003eb{ return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:5","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"不同的二叉搜索树 给定一个整数 n，求以 1 … n 为节点组成的二叉搜索树有多少种？ func numTrees(n int)int{ dp:=make([]int,n+1) dp[0]=1 for i:=1;i\u003c=n;i++{ for j:=1;j\u003c=i;j++{ dp[i]+=dp[j-1]*dp[i-j] } } return dp[n] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:6","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"0-1背包理论基础 I 暴力的解法是指数级别的时间复杂度。进而才需要动态规划的解法来进行优化 代码随想录详解 func test_2_wei_bag_problem1(weight, value []int, bagweight int) int { // 定义dp数组 dp := make([][]int, len(weight)) for i, _ := range dp { dp[i] = make([]int, bagweight+1) } // 初始化 for j := bagweight; j \u003e= weight[0]; j-- { dp[0][j] = dp[0][j-weight[0]] + value[0] } // 递推公式 for i := 1; i \u003c len(weight); i++ { //正序,也可以倒序 for j := weight[i];j\u003c= bagweight ; j++ { dp[i][j] = max(dp[i-1][j], dp[i-1][j-weight[i]]+value[i]) } } return dp[len(weight)-1][bagweight] } func max(a,b int) int { if a \u003e b { return a } return b } func main() { weight := []int{1,3,4} value := []int{15,20,30} test_2_wei_bag_problem1(weight,value,4) } II 代码随想录详解 func test_1_wei_bag_problem(weight, value []int, bagWeight int) int { // 定义 and 初始化 dp := make([]int,bagWeight+1) // 递推顺序 for i := 0 ;i \u003c len(weight) ; i++ { // 这里必须倒序,区别二维,因为二维dp保存了i的状态 for j:= bagWeight; j \u003e= weight[i] ; j-- { // 递推公式 dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) } } //fmt.Println(dp) return dp[bagWeight] } func max(a,b int) int { if a \u003e b { return a } return b } func main() { weight := []int{1,3,4} value := []int{15,20,30} test_1_wei_bag_problem(weight,value,4) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:7","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"分割等和子集 给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 注意: 每个数组中的元素不会超过 100 数组的大小不会超过 200 示例 1: 输入: [1, 5, 11, 5] 输出: true 解释: 数组可以分割成 [1, 5, 5] 和 [11]. 示例 2: 输入: [1, 2, 3, 5] 输出: false 解释: 数组不能分割成两个元素和相等的子集. // 分割等和子集 动态规划 // 时间复杂度O(n^2) 空间复杂度O(n) func canPartition(nums []int) bool { sum := 0 for _, num := range nums { sum += num } // 如果 nums 的总和为奇数则不可能平分成两个子集 if sum % 2 == 1 { return false } target := sum / 2 dp := make([]int, target + 1) for _, num := range nums { for j := target; j \u003e= num; j-- { if dp[j] \u003c dp[j - num] + num { dp[j] = dp[j - num] + num } } } return dp[target] == target } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:8","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最后一块石头的重量II 有一堆石头，每块石头的重量都是正整数。 每一回合，从中选出任意两块石头，然后将它们一起粉碎。假设石头的重量分别为 x 和 y，且 x \u003c= y。那么粉碎的可能结果如下： 如果 x == y，那么两块石头都会被完全粉碎； 如果 x != y，那么重量为 x 的石头将会完全粉碎，而重量为 y 的石头新重量为 y-x。 最后，最多只会剩下一块石头。返回此石头最小的可能重量。如果没有石头剩下，就返回 0。 示例： 输入：[2,7,4,1,8,1] 输出：1 解释： 组合 2 和 4，得到 2，所以数组转化为 [2,7,1,8,1]， 组合 7 和 8，得到 1，所以数组转化为 [2,1,1,1]， 组合 2 和 1，得到 1，所以数组转化为 [1,1,1]， 组合 1 和 1，得到 0，所以数组转化为 [1]，这就是最优值。 func lastStoneWeightII(stones []int) int { // 15001 = 30 * 1000 /2 +1 dp := make([]int, 15001) // 求target sum := 0 for _, v := range stones { sum += v } target := sum / 2 // 遍历顺序 for i := 0; i \u003c len(stones); i++ { for j := target; j \u003e= stones[i]; j-- { // 推导公式 dp[j] = max(dp[j], dp[j-stones[i]]+stones[i]) } } return sum - 2 * dp[target] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:9","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"目标和 给定一个非负整数数组，a1, a2, …, an, 和一个目标数，S。现在你有两个符号 + 和 -。对于数组中的任意一个整数，你都可以从 + 或 -中选择一个符号添加在前面。 返回可以使最终数组和为目标数 S 的所有添加符号的方法数。 示例： 输入：nums: [1, 1, 1, 1, 1], S: 3 输出：5 解释： -1+1+1+1+1 = 3 +1-1+1+1+1 = 3 +1+1-1+1+1 = 3 +1+1+1-1+1 = 3 +1+1+1+1-1 = 3 一共有5种方法让最终目标和为3。 可回溯可dp func findTargetSumWays(nums []int, target int) int { sum := 0 for _, v := range nums { sum += v } if target \u003e sum { return 0 } if (sum+target)%2 == 1 { return 0 } // 计算背包大小 bag := (sum + target) / 2 // 定义dp数组 dp := make([]int, bag+1) // 初始化 dp[0] = 1 // 遍历顺序 for i := 0; i \u003c len(nums); i++ { for j := bag; j \u003e= nums[i]; j-- { //推导公式 dp[j] += dp[j-nums[i]] //fmt.Println(dp) } } return dp[bag] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:10","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"一和零 给你一个二进制字符串数组 strs 和两个整数 m 和 n 。 请你找出并返回 strs 的最大子集的大小，该子集中 最多 有 m 个 0 和 n 个 1 。 如果 x 的所有元素也是 y 的元素，集合 x 是集合 y 的 子集 。 示例 1： 输入：strs = [“10”, “0001”, “111001”, “1”, “0”], m = 5, n = 3 输出：4 解释：最多有 5 个 0 和 3 个 1 的最大子集是 {“10”,“0001”,“1”,“0”} ，因此答案是 4 。 其他满足题意但较小的子集包括 {“0001”,“1”} 和 {“10”,“1”,“0”} 。{“111001”} 不满足题意，因为它含 4 个 1 ，大于 n 的值 3 。 func findMaxForm(strs []string, m int, n int) int { // 定义数组 dp := make([][]int, m+1) for i,_ := range dp { dp[i] = make([]int, n+1 ) } // 遍历 for i:=0;i\u003clen(strs);i++ { zeroNum,oneNum := 0 , 0 //计算0,1 个数 //或者直接strings.Count(strs[i],\"0\") for _,v := range strs[i] { if v == '0' { zeroNum++ } } oneNum = len(strs[i])-zeroNum // 从后往前 遍历背包容量 for j:= m ; j \u003e= zeroNum;j-- { for k:=n ; k \u003e= oneNum;k-- { // 推导公式 dp[j][k] = max(dp[j][k],dp[j-zeroNum][k-oneNum]+1) } } //fmt.Println(dp) } return dp[m][n] } func max(a,b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:11","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"完全背包理论基础 每件物品都有无限个（也就是可以放入背包多次） // test_CompletePack1 先遍历物品, 在遍历背包 func test_CompletePack1(weight, value []int, bagWeight int) int { // 定义dp数组 和初始化 dp := make([]int, bagWeight+1) // 遍历顺序 for i := 0; i \u003c len(weight); i++ { // 正序会多次添加 value[i] for j := weight[i]; j \u003c= bagWeight; j++ { // 推导公式 dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) // debug //fmt.Println(dp) } } return dp[bagWeight] } // test_CompletePack2 先遍历背包, 在遍历物品 func test_CompletePack2(weight, value []int, bagWeight int) int { // 定义dp数组 和初始化 dp := make([]int, bagWeight+1) // 遍历顺序 // j从0 开始 for j := 0; j \u003c= bagWeight; j++ { for i := 0; i \u003c len(weight); i++ { if j \u003e= weight[i] { // 推导公式 dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) } // debug //fmt.Println(dp) } } return dp[bagWeight] } func max(a, b int) int { if a \u003e b { return a } return b } func main() { weight := []int{1, 3, 4} price := []int{15, 20, 30} fmt.Println(test_CompletePack1(weight, price, 4)) fmt.Println(test_CompletePack2(weight, price, 4)) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:12","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"零钱兑换II 给定不同面额的硬币和一个总金额。写出函数来计算可以凑成总金额的硬币组合数。假设每一种面额的硬币有无限个。 示例 1: 输入: amount = 5, coins = [1, 2, 5] 输出: 4 解释: 有四种方式可以凑成总金额: 5=5 5=2+2+1 5=2+1+1+1 5=1+1+1+1+1 func change(amount int, coins []int) int { // 定义dp数组 dp := make([]int, amount+1) // 初始化,0大小的背包, 当然是不装任何东西了, 就是1种方法 dp[0] = 1 // 遍历顺序 // 遍历物品 for i := 0 ;i \u003c len(coins);i++ { // 遍历背包 for j:= coins[i] ; j \u003c= amount ;j++ { // 推导公式 dp[j] += dp[j-coins[i]] } } return dp[amount] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:13","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"组合总和（IV） 给定一个由正整数组成且不存在重复数字的数组，找出和为给定目标正整数的组合的个数。 示例: nums = [1, 2, 3] target = 4 所有可能的组合为： (1, 1, 1, 1) (1, 1, 2) (1, 2, 1) (1, 3) (2, 1, 1) (2, 2) (3, 1) 请注意，顺序不同的序列被视作不同的组合。 因此输出为 7 func combinationSum4(nums []int, target int) int { //定义dp数组 dp := make([]int, target+1) // 初始化 dp[0] = 1 // 遍历顺序, 先遍历背包,再循环遍历物品 for j:=0;j\u003c=target;j++ { for i:=0 ;i \u003c len(nums);i++ { if j \u003e= nums[i] { dp[j] += dp[j-nums[i]] } } } return dp[target] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:14","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"爬楼梯（进阶） 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： 输入： 2 输出： 2 解释： 有两种方法可以爬到楼顶。 1 阶 + 1 阶 2 阶 func climbStairs(n int) int { //定义 dp := make([]int, n+1) //初始化 dp[0] = 1 // 本题物品只有两个1,2 m := 2 // 遍历顺序 for j := 1; j \u003c= n; j++ { //先遍历背包 for i := 1; i \u003c= m; i++ { //再遍历物品 if j \u003e= i { dp[j] += dp[j-i] } //fmt.Println(dp) } } return dp[n] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:15","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"零钱兑换 给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。 你可以认为每种硬币的数量是无限的。 示例 1： 输入：coins = [1, 2, 5], amount = 11 输出：3 解释：11 = 5 + 5 + 1 // 版本一, 先遍历物品,再遍历背包 func coinChange1(coins []int, amount int) int { dp := make([]int, amount+1) // 初始化dp[0] dp[0] = 0 // 初始化为math.MaxInt32 for j := 1; j \u003c= amount; j++ { dp[j] = math.MaxInt32 } // 遍历物品 for i := 0; i \u003c len(coins); i++ { // 遍历背包 for j := coins[i]; j \u003c= amount; j++ { if dp[j-coins[i]] != math.MaxInt32 { // 推导公式 dp[j] = min(dp[j], dp[j-coins[i]]+1) //fmt.Println(dp,j,i) } } } // 没找到能装满背包的, 就返回-1 if dp[amount] == math.MaxInt32 { return -1 } return dp[amount] } // 版本二,先遍历背包,再遍历物品 func coinChange2(coins []int, amount int) int { dp := make([]int, amount+1) // 初始化dp[0] dp[0] = 0 // 遍历背包,从1开始 for j := 1; j \u003c= amount; j++ { // 初始化为math.MaxInt32 dp[j] = math.MaxInt32 // 遍历物品 for i := 0; i \u003c len(coins); i++ { if j \u003e= coins[i] \u0026\u0026 dp[j-coins[i]] != math.MaxInt32 { // 推导公式 dp[j] = min(dp[j], dp[j-coins[i]]+1) //fmt.Println(dp) } } } // 没找到能装满背包的, 就返回-1 if dp[amount] == math.MaxInt32 { return -1 } return dp[amount] } func min(a, b int) int { if a \u003c b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:16","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"完全平方数 给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, …）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。 给你一个整数 n ，返回和为 n 的完全平方数的 最少数量 。 完全平方数 是一个整数，其值等于另一个整数的平方；换句话说，其值等于一个整数自乘的积。例如，1、4、9 和 16 都是完全平方数，而 3 和 11 不是。 示例 1： 输入：n = 12 输出：3 解释：12 = 4 + 4 + 4 // 版本一,先遍历物品, 再遍历背包 func numSquares1(n int) int { //定义 dp := make([]int, n+1) // 初始化 dp[0] = 0 for i := 1; i \u003c= n; i++ { dp[i] = math.MaxInt32 } // 遍历物品 for i := 1; i \u003c= n; i++ { // 遍历背包 for j := i*i; j \u003c= n; j++ { dp[j] = min(dp[j], dp[j-i*i]+1) } } return dp[n] } // 版本二,先遍历背包, 再遍历物品 func numSquares2(n int) int { //定义 dp := make([]int, n+1) // 初始化 dp[0] = 0 // 遍历背包 for j := 1; j \u003c= n; j++ { //初始化 dp[j] = math.MaxInt32 // 遍历物品 for i := 1; i \u003c= n; i++ { if j \u003e= i*i { dp[j] = min(dp[j], dp[j-i*i]+1) } } } return dp[n] } func min(a, b int) int { if a \u003c b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:17","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"单词拆分 给定一个非空字符串 s 和一个包含非空单词的列表 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。 说明： 拆分时可以重复使用字典中的单词。 你可以假设字典中没有重复的单词。 示例 1： 输入: s = “leetcode”, wordDict = [“leet”, “code”] 输出: true 解释: 返回 true 因为 “leetcode” 可以被拆分成 “leet code”。 func wordBreak(s string,wordDict []string) bool { wordDictSet:=make(map[string]bool) for _,w:=range wordDict{ wordDictSet[w]=true } dp:=make([]bool,len(s)+1) dp[0]=true for i:=1;i\u003c=len(s);i++{ for j:=0;j\u003ci;j++{ if dp[j]\u0026\u0026 wordDictSet[s[j:i]]{ dp[i]=true break } } } return dp[len(s)] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:18","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"打家劫舍 I 你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。 示例 1： 输入：[1,2,3,1] 输出：4 解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 func rob(nums []int) int { if len(nums)\u003c1{ return 0 } if len(nums)==1{ return nums[0] } if len(nums)==2{ return max(nums[0],nums[1]) } dp :=make([]int,len(nums)) dp[0]=nums[0] dp[1]=max(nums[0],nums[1]) for i:=2;i\u003clen(nums);i++{ dp[i]=max(dp[i-2]+nums[i],dp[i-1]) } return dp[len(dp)-1] } func max(a, b int) int { if a\u003eb{ return a } return b } II 你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都 围成一圈 ，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警 。 给定一个代表每个房屋存放金额的非负整数数组，计算你 在不触动警报装置的情况下 ，能够偷窃到的最高金额。 示例 1： 输入：nums = [2,3,2] 输出：3 解释：你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。 // 打家劫舍Ⅱ 动态规划 // 时间复杂度O(n) 空间复杂度O(n) func rob(nums []int) int { if len(nums) == 1 { return nums[0] } if len(nums) == 2 { return max(nums[0], nums[1]) } result1 := robRange(nums, 0) result2 := robRange(nums, 1) return max(result1, result2) } // 偷盗指定的范围 func robRange(nums []int, start int) int { dp := make([]int, len(nums)) dp[1] = nums[start] for i := 2; i \u003c len(nums); i++ { dp[i] = max(dp[i - 2] + nums[i - 1 + start], dp[i - 1]) } return dp[len(nums) - 1] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:19","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"III 在上次打劫完一条街道之后和一圈房屋后，小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为“根”。 除了“根”之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。 计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。 动态规划 func rob(root *TreeNode) int { res := robTree(root) return max(res[0], res[1]) } func max(a, b int) int { if a \u003e b { return a } return b } func robTree(cur *TreeNode) []int { if cur == nil { return []int{0, 0} } // 后序遍历 left := robTree(cur.Left) right := robTree(cur.Right) // 考虑去偷当前的屋子 robCur := cur.Val + left[0] + right[0] // 考虑不去偷当前的屋子 notRobCur := max(left[0], left[1]) + max(right[0], right[1]) // **注意**顺序：0:不偷，1:去偷 return []int{notRobCur, robCur} } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:20","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"买卖股票的最佳时机 I 给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。 你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。 返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。 示例 1： 输入：[7,1,5,3,6,4] 输出：5 解释：在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。**注意**利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格；同时，你不能在买入前卖出股票。 func maxProfit(prices []int) int { length:=len(prices) if length==0{return 0} dp:=make([][]int,length) for i:=0;i\u003clength;i++{ dp[i]=make([]int,2) } dp[0][0]=-prices[0] dp[0][1]=0 for i:=1;i\u003clength;i++{ dp[i][0]=max(dp[i-1][0],-prices[i]) dp[i][1]=max(dp[i-1][1],dp[i-1][0]+prices[i]) } return dp[length-1][1] } func max(a,b int)int { if a\u003eb{ return a } return b } II 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: [7,1,5,3,6,4] 输出: 7 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4。随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 // 买卖股票的最佳时机Ⅱ 动态规划 // 时间复杂度：O(n) 空间复杂度：O(n) func maxProfit(prices []int) int { dp := make([][]int, len(prices)) status := make([]int, len(prices) * 2) for i := range dp { dp[i] = status[:2] status = status[2:] } dp[0][0] = -prices[0] for i := 1; i \u003c len(prices); i++ { dp[i][0] = max(dp[i - 1][0], dp[i - 1][1] - prices[i]) dp[i][1] = max(dp[i - 1][1], dp[i - 1][0] + prices[i]) } return dp[len(prices) - 1][1] } func max(a, b int) int { if a \u003e b { return a } return b } func maxProfit(prices []int) int { //创建数组 dp:=make([][]int,len(prices)) for i:=0;i\u003clen(prices);i++{ dp[i]=make([]int,2) } dp[0][0]=-prices[0] dp[0][1]=0 for i:=1;i\u003clen(prices);i++{ dp[i][0]=max(dp[i-1][0],dp[i-1][1]-prices[i]) dp[i][1]=max(dp[i-1][1],dp[i-1][0]+prices[i]) } return dp[len(prices)-1][1] } func max(a,b int)int{ if a\u003cb{ return b } return a } III 给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入：prices = [3,3,5,0,0,3,1,4] 输出：6 解释：在第 4 天（股票价格 = 0）的时候买入，在第 6 天（股票价格 = 3）的时候卖出，这笔交易所能获得利润 = 3-0 = 3 。随后，在第 7 天（股票价格 = 1）的时候买入，在第 8 天 （股票价格 = 4）的时候卖出，这笔交易所能获得利润 = 4-1 = 3。 func maxProfit(prices []int) int { dp:=make([][]int,len(prices)) for i:=0;i\u003clen(prices);i++{ dp[i]=make([]int,5) } dp[0][0]=0 dp[0][1]=-prices[0] dp[0][2]=0 dp[0][3]=-prices[0] dp[0][4]=0 for i:=1;i\u003clen(prices);i++{ dp[i][0]=dp[i-1][0] dp[i][1]=max(dp[i-1][1],dp[i-1][0]-prices[i]) dp[i][2]=max(dp[i-1][2],dp[i-1][1]+prices[i]) dp[i][3]=max(dp[i-1][3],dp[i-1][2]-prices[i]) dp[i][4]=max(dp[i-1][4],dp[i-1][3]+prices[i]) } return dp[len(prices)-1][4] } func max(a,b int)int{ if a\u003eb{ return a } return b } IV 给定一个整数数组 prices ，它的第 i 个元素 prices[i] 是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 k 笔交易。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1： 输入：k = 2, prices = [2,4,1] 输出：2 解释：在第 1 天 (股票价格 = 2) 的时候买入，在第 2 天 (股票价格 = 4) 的时候卖出，这笔交易所能获得利润 = 4-2 = 2。 版本一： // 买卖股票的最佳时机IV 动态规划 // 时间复杂度O(kn) 空间复杂度O(kn) func maxProfit(k int, prices []int) int { if k == 0 || len(prices) == 0 { return 0 } dp := make([][]int, len(prices)) status := make([]int, (2 * k + 1) * len(prices)) for i := range dp { dp[i] = status[:2 * k + 1] status = status[2 * k + 1:] } for j := 1; j \u003c 2 * k; j += 2 { dp[0][j] = -prices[0] } for i := 1; i \u003c len(prices); i++ { for j := 0; j \u003c 2 * k; j += 2 { dp[i][j + 1] = max(dp[i - 1][j + 1], dp[i - 1][j] - prices[i]) dp[i][j + 2] = max(dp[i - 1][j + 2], dp[i - 1][j + 1] + prices[i]) } } return dp[len(prices) - 1][2 * k] } func max(a, b int) int { if a \u003e b { return a } return b } func maxProfit(k int, prices []int) int { if len(prices)==0{ return 0 } dp:=make([][]int,len(prices)) for i:=0;i\u003clen(prices);i++{ dp[i]=make([]int,2*k+1) } for i:=1;i\u003clen(dp[0]);i++{ if i%2!=0{ dp[0][i]=-prices[0] } } for i:=1;i\u003clen(prices);i++{ dp[i][0]=dp[i-1][0] for j:=1;j\u003clen(dp[0]);j++{ if j%2!=0{ dp[i][j]=max(dp[i-1][j],dp[i-1][j-1]-prices[i]) }else { dp[i][j]=max(dp[i-1][j],dp[i-1][j-1]+prices[i]) } } } return dp[len(prices)-1][2*k] } func max(a,b int)int{ if a\u003eb{ return a } return b } 最佳买卖股票时机含冷冻期 给定一个整数数组，其中第 i 个元素代表了第 i","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:21","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"股票问题总结 代码随想录 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:22","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最长上升子序列 给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 子序列是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 示例 1： 输入：nums = [10,9,2,5,3,7,101,18] 输出：4 解释：最长递增子序列是 [2,3,7,101]，因此长度为 4 。 func lengthOfLIS(nums []int ) int { dp := []int{} for _, num := range nums { if len(dp) ==0 || dp[len(dp) - 1] \u003c num { dp = append(dp, num) } else { l, r := 0, len(dp) - 1 pos := r for l \u003c= r { mid := (l + r) \u003e\u003e 1 if dp[mid] \u003e= num { pos = mid; r = mid - 1 } else { l = mid + 1 } } dp[pos] = num }//二分查找 } return len(dp) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:23","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最长连续递增序列 给定一个未经排序的整数数组，找到最长且 连续递增的子序列，并返回该序列的长度。 连续递增的子序列 可以由两个下标 l 和 r（l \u003c r）确定，如果对于每个 l \u003c= i \u003c r，都有 nums[i] \u003c nums[i + 1] ，那么子序列 [nums[l], nums[l + 1], …, nums[r - 1], nums[r]] 就是连续递增子序列。 示例 1： 输入：nums = [1,3,5,4,7] 输出：3 解释：最长连续递增序列是 [1,3,5], 长度为3。 尽管 [1,3,5,7] 也是升序的子序列, 但它不是连续的，因为 5 和 7 在原数组里被 4 隔开。 动态规划： py class Solution: def findLengthOfLCIS(self, nums: List[int]) -\u003e int: if len(nums) == 0: return 0 result = 1 dp = [1] * len(nums) for i in range(len(nums)-1): if nums[i+1] \u003e nums[i]: #连续记录 dp[i+1] = dp[i] + 1 result = max(result, dp[i+1]) return result 贪心法： py class Solution: def findLengthOfLCIS(self, nums: List[int]) -\u003e int: if len(nums) == 0: return 0 result = 1 #连续子序列最少也是1 count = 1 for i in range(len(nums)-1): if nums[i+1] \u003e nums[i]: #连续记录 count += 1 else: #不连续，count从头开始 count = 1 result = max(result, count) return result ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:24","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最长重复子数组 给两个整数数组 A 和 B ，返回两个数组中公共的、长度最长的子数组的长度。 示例： 输入： A: [1,2,3,2,1] B: [3,2,1,4,7] 输出：3 解释： 长度最长的公共子数组是 [3, 2, 1] 。 func findLength(A []int, B []int) int { m, n := len(A), len(B) res := 0 dp := make([][]int, m+1) for i := 0; i \u003c= m; i++ { dp[i] = make([]int, n+1) } for i := 1; i \u003c= m; i++ { for j := 1; j \u003c= n; j++ { if A[i-1] == B[j-1] { dp[i][j] = dp[i-1][j-1] + 1 } if dp[i][j] \u003e res { res = dp[i][j] } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:25","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最长公共子序列 给定两个字符串 text1 和 text2，返回这两个字符串的最长公共子序列的长度。 一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。 例如，“ace” 是 “abcde” 的子序列，但 “aec” 不是 “abcde” 的子序列。两个字符串的「公共子序列」是这两个字符串所共同拥有的子序列。 若这两个字符串没有公共子序列，则返回 0。 示例 1: 输入：text1 = \"abcde\", text2 = \"ace\" 输出：3 解释：最长公共子序列是 \"ace\"，它的长度为 3。 func longestCommonSubsequence(text1 string, text2 string) int { t1 := len(text1) t2 := len(text2) dp:=make([][]int,t1+1) for i:=range dp{ dp[i]=make([]int,t2+1) } for i := 1; i \u003c= t1; i++ { for j := 1; j \u003c=t2; j++ { if text1[i-1]==text2[j-1]{ dp[i][j]=dp[i-1][j-1]+1 }else{ dp[i][j]=max(dp[i-1][j],dp[i][j-1]) } } } return dp[t1][t2] } func max(a,b int)int { if a\u003eb{ return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:26","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"不相交的线 我们在两条独立的水平线上按给定的顺序写下 A 和 B 中的整数。 现在，我们可以绘制一些连接两个数字 A[i] 和 B[j] 的直线，只要 A[i] == B[j]，且我们绘制的直线不与任何其他连线（非水平线）相交。 以这种方法绘制线条，并返回我们可以绘制的最大连线数。 func maxUncrossedLines(A []int, B []int) int { m, n := len(A), len(B) dp := make([][]int, m+1) for i := range dp { dp[i] = make([]int, n+1) } for i := 1; i \u003c= len(A); i++ { for j := 1; j \u003c= len(B); j++ { if (A[i - 1] == B[j - 1]) { dp[i][j] = dp[i - 1][j - 1] + 1 } else { dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]) } } } return dp[m][n] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:27","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最大子序和 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例: 输入: [-2,1,-3,4,-1,2,1,-5,4] 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6 // solution // 1, dp // 2, 贪心 func maxSubArray(nums []int) int { n := len(nums) // 这里的dp[i] 表示，最大的连续子数组和，包含num[i] 元素 dp := make([]int,n) // 初始化，由于dp 状态转移方程依赖dp[0] dp[0] = nums[0] // 初始化最大的和 mx := nums[0] for i:=1;i\u003cn;i++ { // 这里的状态转移方程就是：求最大和 // 会面临2种情况，一个是带前面的和，一个是不带前面的和 dp[i] = max(dp[i-1]+nums[i],nums[i]) mx = max(mx,dp[i]) } return mx } func max(a,b int) int{ if a\u003eb { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:28","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"判断子序列 给定字符串 s 和 t ，判断 s 是否为 t 的子序列。 字符串的一个子序列是原始字符串删除一些（也可以不删除）字符而不改变剩余字符相对位置形成的新字符串。（例如，“ace\"是\"abcde\"的一个子序列，而\"aec\"不是）。 示例 1： 输入：s = “abc”, t = “ahbgdc” 输出：true 示例 2： 输入：s = “axc”, t = “ahbgdc” 输出：false func isSubsequence(s string, t string) bool { dp := make([][]int,len(s)+1) for i:=0;i\u003clen(dp);i++{ dp[i] = make([]int,len(t)+1) } for i:=1;i\u003clen(dp);i++{ for j:=1;j\u003clen(dp[i]);j++{ if s[i-1] == t[j-1]{ dp[i][j] = dp[i-1][j-1] +1 }else{ dp[i][j] = dp[i][j-1] } } } return dp[len(s)][len(t)]==len(s) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:29","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"不同的子序列 给定一个字符串 s 和一个字符串 t ，计算在 s 的子序列中 t 出现的个数。 字符串的一个 子序列 是指，通过删除一些（也可以不删除）字符且不干扰剩余字符相对位置所组成的新字符串。（例如，“ACE” 是 “ABCDE” 的一个子序列，而 “AEC” 不是） 题目数据保证答案符合 32 位带符号整数范围。 func numDistinct(s string, t string) int { dp:= make([][]int,len(s)+1) for i:=0;i\u003clen(dp);i++{ dp[i] = make([]int,len(t)+1) } // 初始化 for i:=0;i\u003clen(dp);i++{ dp[i][0] = 1 } // dp[0][j] 为 0，默认值，因此不需要初始化 for i:=1;i\u003clen(dp);i++{ for j:=1;j\u003clen(dp[i]);j++{ if s[i-1] == t[j-1]{ dp[i][j] = dp[i-1][j-1] + dp[i-1][j] }else{ dp[i][j] = dp[i-1][j] } } } return dp[len(dp)-1][len(dp[0])-1] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:30","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"两个字符串的删除操作 给定两个单词 word1 和 word2，找到使得 word1 和 word2 相同所需的最小步数，每步可以删除任意一个字符串中的一个字符。 示例： 输入: \"sea\", \"eat\" 输出: 2 解释: 第一步将\"sea\"变为\"ea\"，第二步将\"eat\"变为\"ea\" ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:31","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"编辑距离 给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符 删除一个字符 替换一个字符 示例 1： 输入：word1 = “horse”, word2 = “ros” 输出：3 解释： horse -\u003e rorse (将 ‘h’ 替换为 ‘r’) rorse -\u003e rose (删除 ‘r’) rose -\u003e ros (删除 ‘e’) func minDistance(word1 string, word2 string) int { m, n := len(word1), len(word2) dp := make([][]int, m+1) for i := range dp { dp[i] = make([]int, n+1) } for i := 0; i \u003c m+1; i++ { dp[i][0] = i // word1[i] 变成 word2[0], 删掉 word1[i], 需要 i 部操作 } for j := 0; j \u003c n+1; j++ { dp[0][j] = j // word1[0] 变成 word2[j], 插入 word1[j]，需要 j 部操作 } for i := 1; i \u003c m+1; i++ { for j := 1; j \u003c n+1; j++ { if word1[i-1] == word2[j-1] { dp[i][j] = dp[i-1][j-1] } else { // Min(插入，删除，替换) dp[i][j] = Min(dp[i][j-1], dp[i-1][j], dp[i-1][j-1]) + 1 } } } return dp[m][n] } func Min(args ...int) int { min := args[0] for _, item := range args { if item \u003c min { min = item } } return min } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:32","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"编辑距离总结 代码随想录 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:33","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"回文子串 给定一个字符串，你的任务是计算这个字符串中有多少个回文子串。 具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被视作不同的子串。 示例 1： 输入：“abc” 输出：3 解释：三个回文子串: “a”, “b”, “c” func countSubstrings(s string) int { res:=0 dp:=make([][]bool,len(s)) for i:=0;i\u003clen(s);i++{ dp[i]=make([]bool,len(s)) } for i:=len(s)-1;i\u003e=0;i--{ for j:=i;j\u003clen(s);j++{ if s[i]==s[j]{ if j-i\u003c=1{ res++ dp[i][j]=true }else if dp[i+1][j-1]{ res++ dp[i][j]=true } } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:34","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"最长回文子序列 给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度。可以假设 s 的最大长度为 1000 。 示例 1: 输入: “bbbab” 输出: 4 一个可能的最长回文子序列为 “bbbb”。 示例 2: 输入:“cbbd” 输出: 2 一个可能的最长回文子序列为 “bb”。 func longestPalindromeSubseq(s string) int { lenth:=len(s) dp:=make([][]int,lenth) for i:=0;i\u003clenth;i++{ for j:=0;j\u003clenth;j++{ if dp[i]==nil{ dp[i]=make([]int,lenth) } if i==j{ dp[i][j]=1 } } } for i:=lenth-1;i\u003e=0;i--{ for j:=i+1;j\u003clenth;j++{ if s[i]==s[j]{ dp[i][j]=dp[i+1][j-1]+2 }else { dp[i][j]=max(dp[i+1][j],dp[i][j-1]) } } } return dp[0][lenth-1] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:35","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"DP总结 代码随想录 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:36","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"开篇词｜为什么大厂都爱考动态规划？ 你好，我是卢誉声，很高兴能在这个专栏与你见面，和你一起搞定动态规划。开门见山，我先做一个自我介绍。最开始，我在思科系统（Cisco Systems）工作，曾参与设计和开发了下一代视频会议系统的核心数据交换服务。我的工作涵盖了协议栈开发、微服务设计、分布式系统编配以及弹性算法设计。这段经历让我形成了一个认知：算法对设计关键服务来说十分重要，它决定了系统的稳定性、弹性以及可扩展性。后来，我加入了 Autodesk，成为了一款三维设计旗舰软件的框架和平台软件工程师。负责开发了基于大规模结构化数据的高性能搜索引擎，首次将灵活的多线程和异步框架带入产品框架层面，在原有的底层内存模型上采用了改进后的检索引擎，相较于原有的搜索功能，实现了超过 300 倍的性能提升。除此之外，我还改进并维护了用于改进用户体验的数据处理系统，在平台框架层面的工作，让我积累了大量的工程实践经验。现在，我在 Autodesk 数据平台就职，负责设计和开发大规模数据的分析、丰富化以及流化分布式服务。我发现自己的职业发展一直围绕着数据在不断前进。基于此，我常说的一句话是：“数据即是正义”。那直到今天，我的态度依然没有变。数据为媒，算法为介，而在极其重要的算法中，动态规划其实占了很大的比重。 事实上，如果你平常关注大厂面试的话，你会发现，但凡是研发岗位，无论是招聘初级还是高级工程师，大厂都倾向于安排一轮或多轮专门的算法面试环节，而且在面试环节提出动态规划相关问题的这种趋势已经愈发明显。这是为什么呢？我来谈谈我的看法。 先说算法这件事吧。我想请你回想一下，当处理数据结构相关的问题时，你有没有这样的经历？你本能地到工具函数或者库函数中寻找有没有现成的工具。如果问题得到快速解决，它是不是迅速就成了过眼云烟？如果这个问题看起来比较棘手，它不是一个典型的算法问题，那么就寻求搜索引擎的帮助，或者干脆访问 Stack Overflow 这样的“智库”寻找前人留下的解决方案？虽然平时工作中表现优异，但当你想换工作参加大厂面试时，又发现自己难以解决面试官提出的算法问题，无从下手，面对白板“望洋兴叹”？ 相信我，你不是一个人！这种现象很普遍。其实，对于开发人员来说，算法和数据结构就是我们的基本功。我们常常自嘲软件研发人员的工作就是复制粘贴，搬砖就是日常工作的全部。但当公司或部门要求你去研究一个全新的技术，或者快速阅读一份开发多年且成熟的开源项目代码，并对其改造来服务于自己的产品功能时，你的压力会让你明白基本功到底有多重要！关于基本功这事儿，我要插个故事进来，再多说几句。我曾有幸与 C++ 之父 Bjarne Stroustrup 先生进行过面对面的交流。我问了他一个问题：“如今新生代技术人员倾向于学习 Java、Go 或 Python 这些更容易上手的编程语言，您是如何看待这个现象的？”Stroustrup 先生的回答大概是这样的：“如果一个人只了解一种编程语言，那么他不能称自己是专业人士，而从我的角度上看，将 C++ 作为基础，能让你深入洞察各种各样编程语言背后的思想和设计思路。” 我作为面试官曾接触过许多优秀的候选人，他们有着各种各样的背景，既有潜力又非常努力，但在面对算法问题和解决问题时没有太多思路，始终无法更上一层楼，十分遗憾。而动态规划恰恰是解决问题的重要方法论，面对很多数据处理的应用场景，它在降低时间复杂度上极具优势，因此成为了考察重点。不仅如此，动态规划问题还能很好地考察面试者的数学模型抽象能力和逻辑思维能力，可以反应个人在算法上的综合能力。所以我觉得，大厂之所以如此看中一个面试者的算法基础，特别是动态规划问题的解决能力，是因为他们更加看中一位面试者解决问题的思路与逻辑思维能力，而不只是工具与技能的熟练程度。 不同于普通算法，如排序或递归，动态规划从名字上看就显得很特别，有些“高端、大气、上档次”的味道在里面。但其实它离我们很近。我举个例子你就明白了，在云计算平台上一个解决方案的计算能力（容量）肯定是有限的，那么为了高效服务那些重要程度或优先级最高的客户，同时又不想浪费计算资源（说白了为了省钱），我们该怎么办？这个问题其实可以通过队列这样的分发方式来进行一个简单的编配。但是这不够好，如果我们能够事先知道一个计算任务的重要程度和所需的计算时长，就可以通过动态规划算法来进行预演算，从数学角度推导出一个严谨的编排结果，实现有限资源的最大化利用。 你看，似乎遥不可及的动态规划问题，其实就是求最优解问题，它无时无刻都在我们身边，总是戏剧般地提高了最优化问题的性能！这再一次凸显出大厂为何青睐于动态规划问题，而且成为了区别面试者的一个隐形门槛。甚至可以说，掌握动态规划思想，在工作面试、技术等级晋升上都扮演了核心角色。总之一句话，动归必学。 模块一：初识动态规划我会为你讲解复杂面试题的思考和解决方式。从贪心算法开始，一步步阐述动态规划的由来，并通过一个贯穿全篇的例子来展现动态规划的强大之处。学习和掌握这些经典的处理方法，能够为你后续掌握动态规划打下一个坚实基础。通过这部分内容，你会系统了解到动态规划问题的特点和解题经验。模块二：动态规划的套路我会为你讲解动态规划问题的解题框架和套路，你可以把这个套路理解成是解决动归问题的模板。在此模板的基础上，我会向你讲解面试真题，有针对性地套用解题框架。而应对面试题的纷繁复杂，我会为你进行有效的分类，并针对每一种动态规划问题进行深入而全面的讲解。通过这部分内容，你会快速掌握常见面试题的解题套路。模块三：举一反三，突破套路我会针对几种特别易考的动态规划面试题进行总结，帮助你攻破套路。并在这些高级话题的基础上，提出设计动态规划算法的关键问题。另外，还有刷题指南，所谓孰能生巧，必要的练习我们还是要的。通过这部分内容，你会快速掌握动态规划面试题的进阶法门。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:37","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"什么样的问题应该使用动态规划？ 动态规划问题的典型特点：求“最”优解问题（最大值和最小值）、求可行性（True 或 False）、求方案总数、数据结构不可排序（Unsortable）、算法不可使用交换（Non-swappable）。 数据不可排序（Unsortable） 假设我们有一个无序数列，希望求出这个数列中最大的两个数字之和。很多初学者刚刚学完动态规划会走火入魔到看到最优化问题就想用动态规划来求解，嗯，那么这样应该也是可以的吧……不，等等，这个问题不是简单做一个排序或者做一个遍历就可以求解出来了吗？ 数据不可交换（Non-swapable） 还有一类问题，可以归类到我们总结的几类问题里去，但是不存在动态规划要求的重叠子问题（比如经典的八皇后问题），那么这类问题就无法通过动态规划求解。这种情况需要避免被套进去。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:38","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"常见的动态规划面试题串烧 简单的路径规划、带障碍的路径规划、跳跃游戏（题目：给出一个非负整数数组 A，你最初定位在数组的第一个位置。数组中的每个元素代表你在那个位置可以跳跃的最大长度。判断你是否能到达数组的最后一个位置。） ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:10:39","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"算法性能分析 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:0:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？ 复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:1:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"为什么需要复杂度分析？ 你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？首先，我可以肯定地说，你这种评估算法执行效率的方法是正确的。很多数据结构和算法书籍还给这种方法起了一个名字，叫事后统计法。但是，这种统计方法有非常大的局限性。 测试结果非常依赖测试环境 测试结果受数据规模的影响很大 所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是我们今天要讲的时间、空间复杂度分析方法。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:1:1","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"时间复杂度分析 只关注循环执行次数最多的一段代码 加法法则：总复杂度等于量级最大的那段代码的复杂度 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:1:2","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"空间复杂度分析 时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:1:3","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"时间复杂度 时间复杂度是定性描述算法运行时间的函数。 实际情况中会因为数据用例、数据规模不同而变化，一般讨论一般情况。 时间复杂度$O(log(n))$并不以某一个确定的数为底数，因为可以通过乘以某个对数常数达到换底数的效果。 递归算法的时间复杂度：递归次数*每次递归的时间复杂度（操作的次数） ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:2:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"空间复杂度分析 空间复杂度：程序运行时占用内存的大小，受很多因素的影响，比如编译器的内存对齐，编程语言容器的底层实现等 递归算法的空间复杂度：递归深度*每次递归的空间复杂度 递归所需的空间都被压到调用栈里，一次递归结束，这个栈就是就是把本次递归的数据弹出去。所以这个栈最大的长度就是递归的深度。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:3:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"算法复杂度主方法 主方法亦可称为主定理。适用于求那些用分治法以及有递推关系式的算法的复杂度。 假设有递推关系式：$T(n)=aT(n/b)+f(n)$ n是问题规模 a为递推的子问题数量 n/b是每个子问题的规模，假设每个子问题规模一致 f(n)为递推以外进行的计算工作 T(n)为非负整数 分类讨论： 若$f(n)=O(n^{log_b(a-e)}),e\u003e0$ 则$T(n)=Θ(n^{log_b(a)})$ 若$f(n)=Θ(n^{log_b(a)})$ 则$T(n)=Θ(n^{log_b(a)}log(n))$ 若$f(n)=Ω(n^{log_b(a+e)}),e\u003e0$，且对于某个常数c \u003c 1和所有充分大的n有$af(n/b)\u003c=cf(n)$， 则$T(n)=Θ(f(n))$ 不是很容易记忆。下面有一种简化的版本： 若算法运行时间$T(n)\u003c=aT(n/b)+O(n^d)$ a\u003e=1是子问题的个数，b\u003e=1是输入规模减小的倍数，d\u003e=0是递归过程之外的步骤的时间复杂度指数，则： ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:4:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"代码的内存消耗 每种语言都有着自己的内存管理方式。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:5:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"内存对齐 对于各种基本数据类型来说，它的变量的内存地址值必须是其类型本身大小的整数倍。在go里面，对于结构体而言，它的变量的内存地址，只要是它最长字段长度与系统对齐系数两者之间较小的那个的整数倍就可以了。但对于结构体类型来说，我们还要让它每个字段的内存地址都严格满足内存对齐要求。 举个例子： type T struct { b byte i int64 u uint16 } 64bit平台系统对齐系数是8 计算过程：sum=1+7+8+2+6 第一个阶段是对齐结构体的各个字段: 首先，我们看第一个字段 b 是长度 1 个字节的 byte 类型变量，这样字段 b 放在任意地址上都可以被 1 整除，所以我们说它是天生对齐的。我们用一个 sum 来表示当前已经对齐的内存空间的大小，这个时候 sum=1； 接下来，我们看第二个字段 i，它是一个长度为 8 个字节的 int64 类型变量。按照内存对齐要求，它应该被放在可以被 8 整除的地址上。但是，如果把 i 紧邻 b 进行分配，当 i 的地址可以被 8 整除时，b 的地址就无法被 8 整除。这个时候，我们需要在 b 与 i 之间做一些填充，使得 i 的地址可以被 8 整除时，b 的地址也始终可以被 8 整除，于是我们在 i 与 b 之间填充了 7 个字节，此时此刻 sum=1+7+8； 再下来，我们看第三个字段 u，它是一个长度为 2 个字节的 uint16 类型变量，按照内存对其要求，它应该被放在可以被 2 整除的地址上。有了对其的 i 作为基础，我们现在知道将 u 与 i 相邻而放，是可以满足其地址的对齐要求的。i 之后的那个字节的地址肯定可以被 8 整除，也一定可以被 2 整除。于是我们把 u 直接放在 i 的后面，中间不需要填充，此时此刻，sum=1+7+8+2。 结构体 T 的所有字段都已经对齐了，开始第二个阶段，也就是对齐整个结构体: 结构体的内存地址为 min（结构体最长字段的长度，系统内存对齐系数）的整数倍，那么这里结构体 T 最长字段为 i，它的长度为 8，而 64bit 系统上的系统内存对齐系数一般为 8，两者相同，我们取 8 就可以了。那么整个结构体的对齐系数就是 8。 在尾部填充6字节原因： 结构体 T 的对齐系数是 8，那么我们就要保证每个结构体 T 的变量的内存地址，都能被 8 整除。如果我们只分配一个 T 类型变量，不再继续填充，也可能保证其内存地址为 8 的倍数。但如果考虑我们分配的是一个元素为 T 类型的数组，数组是元素连续存储的一种类型，元素 T[1]的地址为 T[0]地址 +T 的大小 (18)，显然无法被 8 整除，这将导致 T[1]及后续元素的地址都无法对齐，这显然不能满足内存对齐的要求。 所以，定义结构体时，一定要注意结构体中字段顺序，尽量合理排序，降低结构体对内存空间的占用。 前面例子中的内存填充部分，是由编译器自动完成的。不过，有些时候，为了保证某个字段的内存地址有更为严格的约束，我们也会做主动填充。比如 runtime 包中的 mstats 结构体定义就采用了主动填充： // $GOROOT/src/runtime/mstats.go type mstats struct { ... ... // Add an uint32 for even number of size classes to align below fields // to 64 bits for atomic operations on 32 bit platforms. _ [1 - _NumSizeClasses%2]uint32 // 这里做了主动填充,通常我们会通过空标识符来进行主动填充 last_gc_nanotime uint64 // last gc (monotonic time) last_heap_inuse uint64 // heap_inuse at mark termination of the previous GC ... ... } 为什么会有内存对齐？ 平台原因：不是所有的硬件平台都能访问任意内存地址上的任意数据，某些硬件平台只能在某些地址处取某些特定类型的数据，否则抛出硬件异常。为了同一个程序可以在多平台运行，需要内存对齐。 硬件原因：经过内存对齐后，CPU访问内存的速度大大提升 CPU读取内存不是一次读取单个字节，而是一块一块的来读取内存，块的大小可以是2，4，8，16个字节，具体取多少个字节取决于硬件。 只要可以跨平台的编程语言都需要做内存对齐，不做内存对齐会使运行速度下降，因为寻址访存次数多了。现在的编译器一般都会做内存对齐的优化操作，也就是说当考虑程序真正占用的内存大小的时候，也需要认识到内存对齐的影响。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:5:1","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"go语言的内存管理 显然，go的内存管理内部机制建立于操作系统以及机器硬件如何管理内存之上的。尽可能扬长避短。 介绍一下和开发者关系较大的操作系统内存管理机制。 暂停 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:5:2","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"编程相关 ","date":"2022-01-02 18:57:57","objectID":"/programming_literacy/:0:0","tags":["data structure"],"title":"Programming_literacy","uri":"/programming_literacy/"},{"categories":["Coding"],"content":"代码风格与规范 go其实没有什么要说的。。 ","date":"2022-01-02 18:57:57","objectID":"/programming_literacy/:1:0","tags":["data structure"],"title":"Programming_literacy","uri":"/programming_literacy/"},{"categories":["Coding"],"content":"变量命名 主要以团队风格为主； 主流有如下三种变量规则： 小驼峰、大驼峰(帕斯卡命名法)命名法（java,go） 下划线命名法(python,linux下的c/c++编程) 匈牙利命名法 该命名规范，要求前缀字母用变量类型的缩写，其余部分用变量的英文或英文的缩写，单词第一个字母大写。（很少用，在windows下的c/c++编程有时会用,没有IDE的时代挺好） c int iMyAge; // “i”: int char cMyName[10]; // “c”: char float fManHeight; // “f”: float ``` ","date":"2022-01-02 18:57:57","objectID":"/programming_literacy/:1:1","tags":["data structure"],"title":"Programming_literacy","uri":"/programming_literacy/"},{"categories":["Coding"],"content":"核心代码模式和ACM模式 核心代码模式： 把要处理的数据都已经放入容器里，可以直接写逻辑 ACM输入模式呢： 自己构造输入数据格式，把要需要处理的容器填充好，不会给你任何代码，包括include哪些函数都要自己写，最后也要自己控制返回数据的格式。 ","date":"2022-01-02 18:57:57","objectID":"/programming_literacy/:1:2","tags":["data structure"],"title":"Programming_literacy","uri":"/programming_literacy/"},{"categories":["Advanced learning"],"content":" 极客时间学习笔记 MySQL基础 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:0:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"一条sql查询语句 sql mysql\u003e select * from T where ID=10； 一条简单的查询语句，MYSQL内部发生了什么？ 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。可以在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。 不同的存储引擎共用一个 Server 层，也就是从连接器到执行器的部分。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:1:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接。 sql mysql -h$ip -P$port -u$user -p 如果把密码直接写在-p后面容易导致密码泄露 连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码： 如果用户名或密码不对，你就会收到一个\"Access denied for user\"的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限（注意修改权限后下次登录才生效）。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到该连接的command是sleep的。show processlist 是显示用户正在运行的线程，需要注意的是，除了 root 用户能看到所有正在运行的线程外，其他用户都只能看到自己正在运行的线程，看不到其它用户正在运行的线程。除非单独个这个用户赋予了PROCESS 权限。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。 sql mysql\u003e show variables like 'wait_timeout'; 长连接和短连接： 长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。（推荐，因为建立连接的过程通常是比较复杂的） 短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 但是全部使用长连接后，有些时候 MySQL 占用内存涨得特别快，这是因为 **MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。**所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 解决方法： 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 mysql_reset_connection：重置连接以清除会话状态。（将句柄都给置空，并且清理掉了预处理语句、结果集） mysql_reset_connection()的作用类似于mysql_change_user()或自动重新连接，除了未关闭并重新打开连接且未完成重新身份验证外。 与连接有关的状态受到以下影响： 回滚所有活动的事务，并重置自动提交模式。 所有 table 锁均已释放。 所有TEMPORARYtable 均已关闭(并删除)。 会话系统变量将重新初始化为相应的全局系统变量的值，包括由诸如SET NAMES之类的语句隐式设置的系统变量。 用户变量设置丢失。 准备好的语句被释放。 HANDLER个变量已关闭。 LAST_INSERT_ID()的值重置为 0. 用GET_LOCK()获取的锁被释放。 Return Values 零成功。如果发生错误，则为非零值。 查询缓存 MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。 查询缓存的弊端： 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定： sql mysql\u003e select SQL_CACHE * from T where ID=10； MySQL 8.0 版本直接将查询缓存的整块功能删掉了。 分析器 分析器对 SQL 语句做解析让mysql知道你要做什么： 词法分析 你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。 MySQL 从你输入的\"select\"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID” 语法分析 根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如你的查询语句 select 少打了开头的字母“s”。 优化器 执行前，需经过优化器的处理： 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join： sql mysql\u003e select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 执行器 执行语句： 先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。 sql mysql\u003e select * from T where ID=10; ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T' 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的： 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。 在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:1:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"一条sql更新语句 日志系统：一条SQL更新语句是如何执行的? sql mysql\u003e create table T(ID int primary key, c int); mysql\u003e update T set c=c+1 where ID=2; 查询语句的那一套流程，更新语句也是同样会走一遍。 执行语句前要先连接数据库，这是连接器的工作。 在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。 分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块，redo log（重做日志）和 binlog（归档日志）。redo log 和 binlog 在设计上有很多有意思的地方。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"重要的日志模块：redo log 如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。 WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。 InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB。从头开始写，写到末尾就又回到开头循环写。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示日志满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。 redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"重要的日志模块 binlog MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面的 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。 最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 两种日志的不同： redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程： 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 注：redo log 的写入拆成了两个步骤：prepare 和 commit，这就是\"两阶段提交\"。 这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"两阶段提交 两阶段提交是为了让两份日志之间的逻辑一致。 binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做： 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库； 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。 为什么日志需要“两阶段提交”？ 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。这两种方式的问题： 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 即，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。 不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"一天一备份和一周一备份的区别 一天一备份“最长恢复时间”更短。 频繁全量备份需要消耗更多存储空间，所以这个 RTO 是成本换来的，就需要你根据业务重要性来评估了。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:4","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"事务隔离 转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这 100 块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。 简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。 以下会以 InnoDB 为例，剖析 MySQL 在事务支持方面的特定实现。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:3:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"隔离性与隔离级别 事务：ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性） 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 隔离做得越好，往往效率越低，所以需要互相平衡。 SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。(事务在执行期间看到的数据前后必须是一致的。) 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，一定要记得将 MySQL 的隔离级别设置为“读提交”。 配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。你可以用 show variables 来查看当前的值: sql mysql\u003e show variables like 'transaction_isolation'; +-----------------------+----------------+ | Variable_name | Value | +-----------------------+----------------+ | transaction_isolation | READ-COMMITTED | +-----------------------+----------------+ 应用案例： 假设你在管理一个个人银行账户表。一个表存了账户余额，一个表存了账单明细。到了月底你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。 使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:3:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"事务隔离的实现 可重复读： 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会保存相应记录。 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view(eg:将2改成1)。 同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC） 系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 为什么建议你尽量不要使用长事务？ 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。数据只有 20GB，但回滚段可能有 200GB 的库。最终只好为了清理回滚段，重建整个库。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。 很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:3:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"事务的启动方式 MySQL 的事务启动方式有以下几种： 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。 因此，建议总是使用 set autocommit=1, 通过显式语句的方式来启动事务。 若考虑到交互次数的增加，建议使用 commit work and chain 语法。 在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务： sql select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))\u003e60 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:3:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"索引 索引的出现就是为了提高数据查询的效率，就像书的目录一样。 索引常见模型（数据结构）： 哈希表 一般用拉链法解决哈希等值数据的存储，适用于只有等值查询的场景。范围查询则需要全部扫描一遍。 比如 Memcached 及其他一些 NoSQL 引擎。 有序数组 有序数组在等值查询和范围查询场景中的性能就都非常优秀。 查询效率最好的数据结构，更新数据效率低下。 只适用于静态存储引擎 搜索树 查询的时候当然要控制尽量少地读磁盘，查询的时候尽量少地访问数据块（树根的数据块一般都在内存中，而其余节点的数据块大概率都需要访问磁盘），二叉搜索树理论上搜索效率虽高，但实际应用时都是用 N 叉树，N 取决于数据块的大小 N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。 在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:4:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"InnoDB的索引模型 在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。 InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。 sql mysql\u003e create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB; 表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。 如果语句是 select * from T where ID=500（ID是主键），即主键查询方式，则只需要搜索 ID 这棵 B+ 树；如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。 索引维护 B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。否则需要挪动插入索引位置后的数据甚至导致页分裂，性能下降，空间利用率也下降。 哪些场景下应该使用自增主键，而哪些场景下不应该？ 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。 有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小 所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 用业务字段直接做主键：比如，有些业务的场景需求是这样的：（典型的KV场景） 只有一个索引； 该索引必须是唯一索引。 由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。 重建索引：索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:4:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"索引2 在下面这个表 T 中，如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？下面是这个表的初始化语句。 sql mysql\u003e create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg'); 在 k 索引树上找到 k=3 的记录，取得 ID = 300；再到 ID 索引树查到 ID=300 对应的 R3；在 k 索引树取下一个值 k=5，取得 ID=500；再回到 ID 索引树查到 ID=500 对应的 R4；在 k 索引树取下一个值 k=6，不满足条件，循环结束。 回到主键索引树搜索的过程，我们称为回表。有没有可能经过索引优化，避免回表过程呢？ ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:5:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"覆盖索引 如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。 需要注意的是，在引擎内部使用覆盖索引在索引 k 上其实读了三个记录，R3~R5（对应的索引 k 上的记录项），但是对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2。 在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？假设这个市民表的定义是这样的： sqlCREATETABLE`tuser`(`id`int(11)NOTNULL,`id_card`varchar(32)DEFAULTNULL,`name`varchar(32)DEFAULTNULL,`age`int(11)DEFAULTNULL,`ismale`tinyint(1)DEFAULTNULL,PRIMARYKEY(`id`),KEY`id_card`(`id_card`),KEY`name_age`(`name`,`age`))ENGINE=InnoDB 如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:5:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"最左前缀原则 B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录，加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 在建立联合索引的时候，如何安排索引内的字段顺序。这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 如果既有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护 (a,b)、(b) 这两个索引。这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:5:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"索引下推 如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的： sql mysql\u003e select * from tuser where name like '张%' and age=10 and ismale=1; 这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3 判断其他条件是否满足: 在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 图三： 图四：索引下推 图 4 跟图 3 的区别是，InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:5:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"全局锁和表锁 数据库资源需要被并发访问，锁是为了合理地控制资源的访问规则。 根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:6:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"全局锁 顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。 在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视图的，就是在可重复读隔离级别下开启一个事务。 官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。 有了这个功能，为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。 所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。 不用set global readonly=true而用 FTWRL 方式设置全库只读的原因： 一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。 二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:6:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。 在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。 另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。 因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 虽然 MDL 锁是系统默认会加的，但却是你不能忽略的一个机制。比如下面这个例子，我经常看到有人掉到这个坑里：给一个小表加个字段，导致整个库挂了。 你肯定知道，给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。我们来看一下下面的操作序列，假设表 t 是一个小表。 我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。 如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。 如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。 你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。 基于上面的分析，我们来讨论一个问题，如何安全地给小表加字段？ 首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。 但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？ 这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。 MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。 sql ALTER TABLE tbl_name NOWAIT add column ... ALTER TABLE tbl_name WAIT N add column ... ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:6:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"小结 全局锁主要用在逻辑备份过程中。对于全部是 InnoDB 引擎的库，我建议你选择使用–single-transaction 参数，对应用会更友好。表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有 lock tables 这样的语句，你需要追查一下，比较可能的情况是：要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎；要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把 lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:6:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"行锁 MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。 了解行锁，以及如何通过减少锁冲突来提升业务并发度。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:7:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"从两阶段说起 我先给你举个例子。在下面的操作序列中，事务 B 的 update 语句执行时会是什么现象呢？假设字段 id 是表 t 的主键。 这个问题的结论取决于事务 A 在执行完两条 update 语句后，持有哪些锁，以及在什么时候释放。你可以验证一下：实际上事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行。知道了这个答案，你一定知道了事务 A 持有的两个记录的行锁，都是在 commit 的时候才释放的。也就是说，在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。我给你举个例子。 假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作：从顾客 A 账户余额中扣除电影票价；给影院 B 的账户余额增加这张电影票价；记录一条交易日志。 也就是说，要完成这个交易，我们需要 update 两条记录，并 insert 一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的顺序呢？试想如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。好了，现在由于你的正确设计，影院余额这一行的行锁在一个事务中不会停留很长时间。但是，这并没有完全解决你的困扰。如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动时间开始的时候，你的 MySQL 就挂了。你登上服务器一看，CPU 消耗接近 100%，但整个数据库每秒就执行不到 100 个事务。这是什么原因呢？这里，我就要说到死锁和死锁检测了。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:7:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"死锁和死锁检测 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。 这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。那如果是我们上面说到的所有事务都要更新同一行的场景呢？ 每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。根据上面的分析，我们来讨论一下，**怎么解决由这种热点行更新导致的性能问题呢？**问题的症结在于，死锁检测要耗费大量的 CPU 资源。 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。 另一个思路是控制并发度。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。 因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。 可能你会问，如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:7:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"小结 今天，我和你介绍了 MySQL 的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。其中，我以两阶段协议为起点，和你一起讨论了在开发的时候如何安排正确的事务语句。这里的原则 / 我给你的建议是：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。但是，调整语句顺序并不能完全避免死锁。所以我们引入了死锁和死锁检测的概念，以及提供了三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是控制访问相同资源的并发事务量。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:7:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"事务要不要隔离？ 我在第 3 篇文章和你讲事务隔离级别的时候提到过，如果是可重复读隔离级别，事务 T 启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。也就是说，一个在可重复读隔离级别下执行的事务，好像与世无争，不受外界影响。但是，我在上一篇文章中，和你分享行锁的时候又提到，一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？我给你举一个例子吧。下面是一个只有两行的表的初始化语句。 mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); 图 1 事务 A、B、C 的执行流程 这里，我们需要注意的是事务的启动时机。begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。第一种启动方式，一致性视图是在执行第一个快照读语句时创建的；第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。 还需要注意的是，在整个专栏里面，我们的例子中如果没有特别说明，都是默认 autocommit=1。在这个例子中，事务 C 没有显式地使用 begin/commit，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交。事务 B 在更新了行之后查询 ; 事务 A 在一个只读事务中查询，并且时间顺序上是在事务 B 的查询之后。这时，如果我告诉你事务 B 查到的 k 的值是 3，而事务 A 查到的 k 的值是 1，你是不是感觉有点晕呢？所以，今天这篇文章，我其实就是想和你说明白这个问题，希望借由把这个疑惑解开的过程，能够帮助你对 InnoDB 的事务和锁有更进一步的理解。 在 MySQL 里，有两个“视图”的概念：一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。 它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。在第 3 篇文章《事务隔离：为什么你改了我还看不见？》中，我跟你解释过一遍 MVCC 的实现逻辑。今天为了说明查询和更新的区别，我换一个方式来说明，把 read view 拆开。你可以结合这两篇文章的说明来更深一步地理解 MVCC。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:8:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"“快照”在 MVCC 里是怎么工作的？ 在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。这时，你会说这看上去不太现实啊。如果一个库有 100G，那么我启动一个事务，MySQL 就要拷贝 100G 的数据出来，这个过程得多慢啊。可是，我平时的事务执行起来很快啊。实际上，我们并不需要拷贝出这 100G 的数据。我们先来看看这个快照是怎么实现的。InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。如图 2 所示，就是一个记录被多个事务连续更新后的状态。 图 2 行状态变更图 图中虚线框里是同一行数据的 4 个版本，当前最新版本是 V4，k 的值是 22，它是被 transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。你可能会问，前面的文章不是说，语句更新会生成 undo log（回滚日志）吗？那么，undo log 在哪呢？ 实际上，图 2 中的三个虚线箭头，就是 undo log；而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。明白了多版本和 row trx_id 的概念后，我们再来想一下，InnoDB 是怎么定义那个“100G”的快照的。按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。这个视图数组把所有的 row trx_id 分成了几种不同的情况。 图 3 数据版本可见性规则 这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；如果落在黄色部分，那就包括两种情况a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 比如，对于图 2 中的数据来说，如果有一个事务，它的低水位是 18，那么当它访问这一行数据时，就会从 V4 通过 U3 计算出 V3，所以在它看来，这一行的值是 11。你看，有了这个声明后，系统里面随后发生的更新，是不是就跟这个事务看到的内容无关了呢？因为之后的更新，生成的版本一定属于上面的 2 或者 3(a) 的情况，而对它来说，这些新的数据版本是不存在的，所以这个事务的快照，就是“静态”的了。所以你现在知道了，InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。 接下来，我们继续看一下图 1 中的三个事务，分析下事务 A 的语句返回的结果，为什么是 k=1。这里，我们不妨做如下假设： 事务 A 开始前，系统里面只有一个活跃事务 ID 是 99；事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务；三个事务开始前，(1,1）这一行数据的 row trx_id 是 90。 这样，事务 A 的视图数组就是[99,100], 事务 B 的视图数组是[99,100,101], 事务 C 的视图数组是[99,100,101,102]。为了简化分析，我先把其他干扰语句去掉，只画出跟事务 A 查询逻辑有关的操作： 图 4 事务 A 查询数据逻辑图 从图中可以看到，第一个有效更新是事务 C，把数据从 (1,1) 改成了 (1,2)。这时候，这个数据的最新版本的 row trx_id 是 102，而 90 这个版本已经成为了历史版本。第二个有效更新是事务 B，把数据从 (1,2) 改成了 (1,3)。这时候，这个数据的最新版本（即 row trx_id）是 101，而 102 又成为了历史版本。你可能注意到了，在事务 A 查询的时候，其实事务 B 还没有提交，但是它生成的 (1,3) 这个版本已经变成当前版本了。但这个版本对事务 A 必须是不可见的，否则就变成脏读了。好，现在事务 A 要来读数据了，它的视图数组是[99,100]。当然了，读数据都是从当前版本读起的。所以，事务 A 查询语句的读数据流程是这样的： 找到 (1,3) 的时候，判断出 row trx_id=101，比高水位大，处于红色区域，不可见；接着，找到上一个历史版本，一看 row trx_id=102，比高水位大，处于红色区域，不可见；再往前找，终于找到了（1,1)，它的 row trx_id=90，比低水位小，处于绿色区域，可见。 这样执行下来，虽然期间这一行数据被修改过，但是事务 A 不论在什么时候查询，看到这行数据的结果都是一致的，所以我们称之为一致性读。这个判断规则是从代码逻辑直接转译过来的，但是正如你所见，用于人肉分析可见性很麻烦。所以，我来给你翻译一下。一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见；版本已提交，但是是在视图创建后提交的，不可见；版本已提交，而且是在视图创建前提交的，可见。 现在，我们用这个规则来判断图 4 中的查询结果，事务 A 的查询语句的视图数组是在事务 A 启动的时候生成的，这时候：(1,3) 还没提交，属于情况 1，不可见；(1,2) 虽然提交了，但是是在视图数组创建之后提交的，属于情况 2，不可见；(1,1) 是在视图数组创建之前提交的，可见。 你看，去掉数字对比后，只用时间先后顺序来判断，分析起来是不是轻松多了。所以，后面我们就都用这个规则来分析。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:8:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"更新逻辑 细心的同学可能有疑问了：事务 B 的 update 语句，如果按照一致性读，好像结果不对哦？你看图 5 中，事务 B 的视图数组是先生成的，之后事务 C 才提交，不是应该看不见 (1,2) 吗，怎么能算出 (1,3) 来？ 图 5 事务 B 更新逻辑图 是的，如果事务 B 在更新之前查询一次数据，这个查询返回的 k 的值确实是 1。但是，当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务 C 的更新就丢失了。因此，事务 B 此时的 set k=k+1 是在（1,2）的基础上进行的操作。所以，这里就用到了这样一条规则：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。 因此，在更新的时候，当前读拿到的数据是 (1,2)，更新后生成了新版本的数据 (1,3)，这个新版本的 row trx_id 是 101。所以，在执行事务 B 查询语句的时候，一看自己的版本号是 101，最新数据的版本号也是 101，是自己的更新，可以直接使用，所以查询得到的 k 的值是 3。这里我们提到了一个概念，叫作当前读。其实，除了 update 语句外，select 语句如果加锁，也是当前读。所以，如果把事务 A 的查询语句 select * from t where id=1 修改一下，加上 lock in share mode 或 for update，也都可以读到版本号是 101 的数据，返回的 k 的值是 3。下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。 mysql\u003e select k from t where id=1 lock in share mode; mysql\u003e select k from t where id=1 for update; 再往前一步，假设事务 C 不是马上提交的，而是变成了下面的事务 C’，会怎么样呢？ 图 6 事务 A、B、C’的执行流程 事务 C’的不同是，更新后并没有马上提交，在它提交前，事务 B 的更新语句先发起了。前面说过了，虽然事务 C’还没提交，但是 (1,2) 这个版本也已经生成了，并且是当前的最新版本。那么，事务 B 的更新语句会怎么处理呢？这时候，我们在上一篇文章中提到的“两阶段锁协议”就要上场了。事务 C’没提交，也就是说 (1,2) 这个版本上的写锁还没释放。而事务 B 是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务 C’释放这个锁，才能继续它的当前读。 图 7 事务 B 更新逻辑图（配合事务 C'） 到这里，我们把一致性读、当前读和行锁就串起来了。现在，我们再回到文章开头的问题：事务的可重复读的能力是怎么实现的？ 可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 那么，我们再看一下，在读提交隔离级别下，事务 A 和事务 B 的查询语句查到的 k，分别应该是多少呢？这里需要说明一下，“start transaction with consistent snapshot; ”的意思是从这个语句开始，创建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于普通的 start transaction。下面是读提交时的状态图，可以看到这两个查询语句的创建视图数组的时机发生了变化，就是图中的 read view 框。（注意：这里，我们用的还是事务 C 的逻辑直接提交，而不是事务 C’） 图 8 读提交隔离级别下的事务状态图 这时，事务 A 的查询语句的视图数组是在执行这个语句的时候创建的，时序上 (1,2)、(1,3) 的生成时间都在创建这个视图数组的时刻之前。但是，在这个时刻：(1,3) 还没提交，属于情况 1，不可见；(1,2) 提交了，属于情况 3，可见。 所以，这时候事务 A 查询语句返回的是 k=2。显然地，事务 B 查询结果 k=3。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:8:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"小结 InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。对于可重复读，查询只承认在事务启动前就已经提交完成的数据；对于读提交，查询只承认在语句启动前就已经提交完成的数据； 而当前读，总是读取已经提交完成的最新版本。你也可以想一下，为什么表结构不支持“可重复读”？这是因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。当然，MySQL 8.0 已经可以把表结构放在 InnoDB 字典里了，也许以后会支持表结构的可重复读。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:8:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["School courses"],"content":" 2022春JOANNA老师的软件体系结构 topics 1.Introduction to Software Architecture Why is Software Architecture Important? The Many Contexts of Software Architecture 2.Architecture modelling and representation: Architectural structures and views 3.Quality attributes :Understanding quality attributes and availability Availability Quality attributes: interoperability and modifiability Interoperability Quality attributes: Modifiability Quality attributes: Performance, Security Quality attributes: Security Patterns and Tactics Quality Attribute Modelling and Analysis Designing for architecturally significant requirements Designing and evaluating an architecture Exercise 1 Capturing ASR in practice . exercise2 exercise3 10%+30%+60% ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:0:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"topics Introduction to the subject: objectives, plan, assessment.Introduction to software architecture: concept and importance of software architecture Architecture modelling and representation: architectural structures and views, UML Quality attributes: Understanding quality attributes and availability Quality attributes: interoperability and modifiability Quality attributes: performance and security Quality attributes: testability, usability and other quality attributes Achieving quality attribute through tactics and patterns: architectural tactics and patterns Achieving quality attribute through tactics and patterns: architectural tactics and patterns Achieving quality attribute through tactics and patterns: quality attribute modelling and analysis Achieving quality attribute through tactics and patterns: designing for architecturally significant requirements Designing and evaluating an architecture: TOGAF, ADD and ATAM Architecture reuse: software product lines, frameworks and middleware Capturing ASR in practice (practical exercises) Designing an architecture in practice (practical exercises) Documenting software architecture in practice (practical exercises) Architecture evaluation in practice (practical exercises) ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:1:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"1.Introduction to Software Architecture The software architecture of a system is the set of structures needed to reason about the system, which comprise software elements, relations among them, and properties of both. We can say that architecture is an abstraction. What does it mean that architecture is an abstraction? If the information about element is no useful for reasoning about the system, architecture will omit it. Architecture shows the system in an abstract way - in terms of its elements and their relationships. The architects design structures – make decisions on what architectural elements should be used in a system to solve the problems that the system will face. Some compositions of architectural elements that solve particular problems may be used again in different systems facing similar problems. If a certain composition of architectural elements that solves a particular problem was found useful over time and over many domains, has been documented and disseminated, it may become a/an…. Architectural pattern Architecture Is a Set of Software Structures : A structure is a set of elements held together by a relation. Software systems are composed of many structures, and no single structure holds claim to being the architecture. There are three important categories of architectural structures. Module Component and Connector Allocation Architecture vs Design The architecture - the selection of architectural elements, their interaction and the restrictions on those elements and their interactions. The design - modularization and the detailed interfaces of the elements of the system, their algorithms and procedures, and the kinds of data needed to support the architecture and satisfy the requirements. Architecture is an Abstraction An architecture specifically omits certain information about elements that is not useful for reasoning about the system. The architectural abstraction lets us look at the system in terms of its elements, how they are arranged, how they interact, how they are composed, and so forth. This abstraction is essential to taming the complexity of an architecture. Every System has a Software Architecture But the architecture may not be known to anyone. Perhaps all of the people who designed the system are long gone Perhaps the documentation has vanished (or was never produced) Perhaps the source code has been lost (or was never delivered) An architecture can exist independently of its description or specification Structures and Views A view is a representation of a coherent set of architectural elements, as written by and read by system stakeholders. A structure is the set of elements itself, as they exist in software or hardware. In short, a view is a representation of a structure. For example, a module structure is the set of the system’s modules and their organization. A module view is the representation of that structure, documented according to a template in a chosen notation, and used by some system stakeholders. Architects design structures. They document views of those structures. Architects design structures. Architects document views of those structures. What is a view? a representation of a coherent set of architectural elements is written by the architects may be read by the testers or the project clients Architectural Patterns Architectural elements can be composed in ways that solve particular problems. The compositions have been found useful over time, and over many different domains They have been documented and disseminated. These compositions of architectural elements, called architectural patterns. Patterns provide packaged strategies for solving some of the problems facing a system. An architectural pattern delineates the element types and their forms of interaction used in solving the problem. What Makes a “Good” Architecture? There is no such thing as an inherently good or bad architecture. Architectures can be evaluated but only in the context of specific stated goals. Architec","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:2:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Why is Software Architecture Important? Inhibiting or Enabling System’s Quality Attributes Whether a system will be able to exhibit its desired (or required) quality attributes is substantially determined by its architecture. Performance Modifiability Security Scalability Reusability Reasoning About and Managing Change About 80 percent of a typical software system’s total cost occurs after initial deployment accommodate new features adapt to new environments, fix bugs, and so forth. Every architecture partitions possible changes into three categories A local change can be accomplished by modifying a single element. A nonlocal change requires multiple element modifications but leaves the underlying architectural approach intact. An architectural change affects the fundamental ways in which the elements interact with each other and will require changes all over the system. Obviously, local changes are the most desirable A good architecture is one in which the most common changes are local, and hence easy to make. Which kind of a change in the system is most desirable?(1 answer) A local change that can be accomplished by modifying a single element. A nonlocal change that requires multiple element modifications but leaves the underlying architectural approach intact. An architectural change that affects the fundamental ways in which the elements interact with each other and will require changes all over the system. Predicting System Qualities When we examine an architecture we can confidently predict that the architecture will exhibit the associated qualities. The earlier you can find a problem in your design, the cheaper, easier, and less disruptive it will be to fix. Enhancing Communication Among Stakeholders The architecture—or at least parts of it—is sufficiently abstract that most nontechnical people can understand it. Most of the system’s stakeholders can use as a basis for creating mutual understanding, negotiating, forming consensus, and communicating with each other. Each stakeholder of a software system is concerned with different characteristics of the system Users, client, manager, architect Earliest Design Decisions Software architecture is a manifestation of the earliest design decisions about a system. These early decisions affect the system’s remaining development, its deployment, and its maintenance life. Earliest Design Decisions What are these early design decisions? Will the system run on one processor or be distributed across multiple processors? Will the software be layered? If so, how many layers will there be? What will each one do? Will components communicate synchronously or asynchronously? What communication protocol will we choose? Will the system depend on specific features of the operating system or hardware? Will the information that flows through the system be encrypted or not? Defining Constraints on an Implementation An implementation exhibits an architecture if it conforms to the design decisions prescribed by the architecture. The implementation must be implemented as the set of prescribed elements These elements must interact with each other in the prescribed fashion Each of these prescriptions is a constraint on the implementer. Influencing the Organizational Structure Architecture prescribes the structure of the system being developed. The architecture is typically used as the basis for the work-breakdown structure. The work-breakdown structure in turn dictates units of planning, scheduling, and budget interteam communication channels configuration and file-system organization integration and test plans and procedures; the maintenance activity Enabling Evolutionary Prototyping Once an architecture has been defined, it can be prototyped as a skeletal system. A skeletal system is one in which at least some of the infrastructure is built before much of the system’s functionality has been created. The fidelity of the system increases as prototype parts are replaced with complete versions of these","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:2:1","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"The Many Contexts of Software Architecture Contexts of Software Architecture We put software architecture in its place relative to four contexts: Technical. What technical role does the software architecture play in the system? Project life cycle. How does a software architecture relate to the other phases of a software development life cycle? Business. How does the presence of a software architecture affect an organization’s business environment? Professional. What is the role of a software architect in an organization or a development project? Technical Context The most important technical context factor is the set of quality attributes that the architecture can help to achieve. The architecture’s current technical environment is also an important factor. Standard industry practices Software engineering techniques prevalent in the architect’s professional community. Project Life-cycle Context Software development processes are standard approaches for developing software systems. They tell the members of the team what to do next. There are four dominant software development processes: Waterfall Iterative Agile Model-driven development Architecture Activities Architecture is a special kind of design, so architecture finds a home in each process of software development. There are activities involved in creating a software architecture, using it to realize a complete design, and then implementing Understanding the architecturally significant requirements Creating or selecting the architecture Documenting and communicating the architecture Analyzing or evaluating the architecture Implementing and testing the system based on the architecture Ensuring that the implementation conforms to the architecture Business Context Systems are created to satisfy the business goals of one or more organizations. Development organizations: e.g., make a profit, or capture market, or help their customers do their jobs better, or keep their staff employed, or make their stockholders happy Customers have their own goals: e.g. ,make their lives easier or more productive. Other organizations, such as subcontractors or government regulatory agencies, have their own goals Architects need to understand the goals. Many of these goals will influence the architecture. Architecture and business goals: Professional Context Architects need more than just technical skills. Architects need diplomatic, negotiation, and communication skills. Architects need the ability to communicate ideas clearly Architects need up-to-date knowledge. Know about (for example) patterns, or database platforms, or web services standards. Know about business considerations. Stakeholders A stakeholder is anyone who has a stake in the success of the system Stakeholders typically have different specific concerns on the system Early engagement of stakeholders to understand the constraints of the task, manage expectations, negotiate priorities, and make tradeoffs. How is Architecture Influenced? Technical requirements Architects Business, social, and technical environment What Do Architectures Influence? Technical context The architecture can affect stakeholder’s requirements for the next system A customer may relax some of their requirements to gain these economies. Shrinkwrapped software has affected people’s requirements, as it is inexpensive and of high quality. Project context The architecture affects the structure of the developing organization. An architecture prescribes the units of software to be implemented and integrated to the system. These units are the basis for the development project’s structure. the development, test, and integration activities all revolve around the units. Business context The architecture can affect the business goals of the developing organization. The architecture can provide opportunities for the efficient production and deployment of similar systems, and the organization may adjust its goals to take advantage of its newfound expertise. Professional co","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:2:2","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"2.Architecture modelling and representation: Architectural structures and views Intended Learning Outcomes By the end of this lesson you will be able to: differentiate between structures and views list useful structures and views and understand how they relate to each other choose the relevant structure and views combine views use UML to express the views Architecture Is a Set of Software Structures A structure is a set of elements held together by a relation. Software systems are composed of many structures, and no single structure holds claim to being the architecture. There are three important categories of architectural structures. Module Component and Connector Allocation Module Structures Some structures partition systems into implementation units, which we call modules. Modules are assigned specific computational responsibilities, and are the basis of work assignments for programming teams. In large projects, these elements (modules) are subdivided for assignment to sub-teams. Component-and-connector Structures Other structures focus on the way the elements interact with each other at runtime to carry out the system’s functions. We call runtime structures component-and-connector (C\u0026C) structures. In our use, a component is always a runtime entity. In SOA, the system is to be built as a set of services. These services are made up of (compiled from) the programs in the various implementation units – modules. Allocation Structures Allocation structures describe the mapping from software structures to the system’s environments For example Modules are assigned to teams to develop, and assigned to places in a file structure for implementation, integration, and testing. Components are deployed onto hardware in order to execute. Which Structures are Architectural? A structure is architectural if it supports reasoning about the system and the system’s properties. The reasoning should be about an attribute of the system that is important to some stakeholder. These include functionality achieved by the system the availability of the system in the face of faults the difficulty of making specific changes to the system the responsiveness of the system to user requests, many others. Programming team developing our cafeteria system wants to know what units of implementation will be assigned with functional responsibilities. Which structures can help reason about that? Module structures Structures and Views A structure is the set of elements itself, as they exist in software or hardware. In short, a view is a representation of a structure. Architects design structures. They document views of those structures. Physiological Structures The neurologist, the orthopedist, the hematologist, and the dermatologist all have different views of the structure of a human body. Ophthalmologists, cardiologists, and podiatrists concentrate on specific subsystems. The kinesiologist and psychiatrist are concerned with different aspects of the entire arrangement’s behavior. Although these views are pictured differently and have different properties, all are inherently related, interconnected. Together they describe the architecture of the human body. So it is with software! Module Structures Module structures embody decisions as to how the system is to be structured as a set of code or data units In any module structure, the elements are modules of some kind (perhaps classes, or layers, or merely divisions of functionality, all of which are units of implementation). Modules are assigned areas of functional responsibility. Component-and-connector Structures Component-and-connector structures embody decisions as to how the system is to be structured as a set of elements that have runtime behavior (components) and interactions (connectors). Elements are runtime components such as services, peers, clients, servers, or many other types of runtime element) Connectors are the communication vehicles among components, such as call-return, process synchronization o","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:3:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"3.Quality attributes :Understanding quality attributes and availability Intended Learning Outcomes By the end of this lesson you will be able to: specify a quality attribute requirement understand quality design decisions apply the design decision categories to different quality attributes Architecture and Requirements System requirements can be categorized as: Functional requirements state what the system must do, how it must behave or react to run-time stimuli. Quality attribute requirements qualify functional requirements, e.g., how fast the function must be performed, how resilient it must be to erroneous input, etc. Constraints. A constraint is a design decision with zero degrees of freedom. Which of the above system requirements will have an impact on the architecture of the system?（1 answer） Functional requirements Quality attributes requirements Constraints Functionality Functionality is the ability of the system to do the work for which it was intended. Functionality has a strange relationship to architecture: functionality does not determine architecture; Quality Attribute Considerations If a functional requirement is “when the user presses the green button the Options dialog appears”: a performance qualification might describe how quickly the dialog will appear; an availability qualification might describe how often this function will fail, and how quickly it will be repaired; a usability qualification might describe how easy it is to learn this function. Two Categories of Quality Attributes The ones that describe some properties of the system at runtime Availability, performance, usability, security The ones that describe some properties of the development of system Modifiability Testability Quality Attribute Considerations There are problems with previous discussions of quality attributes: Untestable definitions. The definitions provided for an attribute are not testable. It is meaningless to say that a system will be “modifiable” Overlapping concerns. Is a system failure due to a denial of service attack an aspect of availability, performance, security, or usability? A solution to the problems (untestable definitions and overlapping concerns) is to use quality attribute scenarios as a means of characterizing quality attributes. Specifying Quality Attribute Requirements We use a common form to specify all quality attribute requirements as scenarios. Our representation of quality attribute scenarios has these parts: Stimulus Stimulus source Response Response measure Environment Artifact Specifying Quality Attribute Requirements Stimulus. he stimulus is a condition that requires a response when it arrives at a system. Source of stimulus. his is some entity (a human, a computer system, or any other actuator) that generated the stimulus. Response. he response is the activity undertaken as the result of the arrival of the stimulus. Response measure. hen the response occurs, it should be measurable in some fashion so that the requirement can be tested. Environment. he stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation, or some other relevant state. Artifact. his may be a collection of systems, the whole system, or some piece or pieces of it. Some artifact is stimulated. Specifying Quality Attribute Requirements General quality attribute scenarios are system independent and can, potentially, pertain to any system Concrete quality attribute scenarios are specific to the particular system under consideration. Specifying Quality Attribute Requirements Example general scenario for availability: Achieving Quality Attributes Through Tactics There are a collection of primitive design techniques that an architect can use to achieve a quality attribute response. We call these architectural design primitives tactics. Tactics, like design patterns, are techniques that architects have been using for years. We do not invent tactics, we simply capture what architects do in pract","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:4:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Availability What is Availability? Availability refers to a property of software that it is there and ready to carry out its task when you need it to be. Availability refers to the ability of a system to mask or repair faults such that the cumulative service outage period does not exceed a required value over a specified time interval. Availability is about minimizing service outage time by mitigating faults Availability Availability v.s. reliability or dependability Availability encompasses what is normally called reliability. Availability encompasses other consideration such as service outage due to period maintenance Availability is closely related to security, e..g, denial-of-service performance … Availability General Scenario Stimulus … … … … … … ? Stimulus source … … … ? Response … … … … … …? Response measure … …? Environment … … … … .? Artifact … … … … … … ..? Specifying Quality Attribute Requirements Stimulus. he stimulus is a condition that requires a response when it arrives at a system. Source of stimulus. his is some entity (a human, a computer system, or any other actuator) that generated the stimulus. Response. he response is the activity undertaken as the result of the arrival of the stimulus. Response measure. hen the response occurs, it should be measurable in some fashion so that the requirement can be tested. Environment. he stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation, or some other relevant state. Artifact. his may be a collection of systems, the whole system, or some piece or pieces of it. Some artifact is stimulated. Availability General Scenario: Sample Concrete Availability Scenario The heartbeat monitor determines that the server is nonresponsive during normal operations. The system informs the operator and continues to operate with no downtime. The heartbeat monitor determines that the server is nonresponsive during normal operations. The system informs the operator and continues to operate with no downtime. Let us analyze if this scenario is complete Specifying Quality Attribute Requirements Stimulus. he stimulus is a condition that requires a response when it arrives at a system. Source of stimulus. his is some entity (a human, a computer system, or any other actuator) that generated the stimulus. Response. he response is the activity undertaken as the result of the arrival of the stimulus. Response measure. hen the response occurs, it should be measurable in some fashion so that the requirement can be tested. Environment. he stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation, or some other relevant state. Artifact. his may be a collection of systems, the whole system, or some piece or pieces of it. Some artifact is stimulated. Sample Concrete Availability Scenario The heartbeat monitor determines that the server is nonresponsive during normal operations. The system informs the operator and continues to operate with no downtime. Stimulus: ??? The stimulus is a condition that requires a response when it arrives at a system. Sample Concrete Availability Scenario The heartbeat monitor determines that the server is nonresponsive during normal operations. The system informs the operator and continues to operate with no downtime. Stimulus: non-responsiveness Response: inform the operator Response measure: no downtime, or 100 availability percentages Environment: normal operation Artifact: heartbeat monitor Stimulus source: server We will continue this topic on Monday Which statement is NOT true?（The last one） A stakeholder is anyone who has a stake in the success of the system Stakeholders typically have different specific concerns on the system Stakeholders participate in some parts of the design process. Architect is the only stakeholder of the system Recall our earlier example University Town, has directed its software development subsidiary, Campus Software, to develop a cafeteria s","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:4:1","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality attributes: interoperability and modifiability ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:5:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Interoperability Intended Learning Outcomes By the end of this lesson you will be able to: apply the design decision categories to interoperability What is Interoperability? Interoperability is about the degree to which two or more systems can usefully exchange meaningful information via interfaces in a particular context. Any discussion of a system’s interoperability needs to identify with whom, and under what circumstance. Syntactic interoperability is the ability to exchange data. Semantic interoperability is the ability to interpret the data being exchanged. Two perspectives for achieving interoperability With the knowledge about the interfaces of external systems, design that knowledge into the system Without the knowledge about other systems, design the system to interoperate in a more general fashion Motivation The system provides a service to be used by a collection of unknown systems, eg., GoogleMaps The system is constructed from existing systems, for example Producing a representation of what was sensed Interpreting the data Processing the raw data Sensing the environment Two Important Aspects of Interoperability Discovery. The consumer of a service must discover the location, identity, and interface of service Handling the response. Three possibilities: The service reports back to the requester The service sends its response on to another system The service broadcasts its response to any interested parties Example Will this system need to provide services to other system(s)? What do we know about these systems? Will this system be constructed by combining capabilities from other systems? Example - a traffic sensing system where the input comes from individual vehicles, the raw data is processed into common units of measurement, is interpreted and fused, and traffic congestion information is broadcast one of the existing systems is responsible for sensing its environment another one is responsible for processing the raw data a third is responsible for interpreting the data a final one is responsible for producing and distributing a representation of what was sensed Intelligent transportation system Interoperability General Scenario Sample Concrete Interoperability Scenario Our vehicle information system sends our current location to the traffic monitoring system. The traffic monitoring system combines our location with other information, overlays this information on a Google Map, and broadcasts it. Our location information is correctly included with a probability of 99.9%. Sample Concrete Interoperability Scenario Goal of Interoperability Tactics For two or more systems to usefully exchange information they must Know about each other. That is the purpose behind the locate tactics. Exchange information in a semantically meaningful fashion. That is the purpose behind the manage interfaces tactics. Two aspects of the exchange are Provide services in the correct sequence Modify information produced by one actor to a form acceptable to the second actor. Goal of Interoperability Tactics Interoperability Tactics Locate Service Discovery : Locate a service through searching There are many service discovery mechanisms: UDDI for Webservices Jini for Jave objects Simple Service Discovery Protocol (SSDP) as used in Universal plug-and-play (UPnP) DNS Service Discovery (DNS-SD) Bluetooth Service Discovery Protocol (SDP) Service Discovery – Necessary conditions The searcher wants to find the searched entity and the searched entity wants to be found The searched entity must have identifiers The searcher must acquire sufficient identifiers to identify the searched entity Searching Method – Searcher’s initiative Flood/Broadcast request Ask every entity and wait for answer Examples Paging in the location area to find the mobile terminal DHCP discover: the client broadcasts on the local subnet to find available servers to ask for IP address Efficient and less resource consuming for the searcher Low resource consuming for the searched B","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:5:1","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality attributes: Modifiability Intended Learning Outcomes By the end of this lesson you will be able to: apply the design decision categories to modifiability What is Modifiability? Modifiability is about change and our interest in it is in the cost and risk of making changes. To plan for modifiability, an architect has to consider four questions: What can change? What is the likelihood of the change? When is the change made and who makes it? What is the cost of the change? Recall our earlier example University Town, has directed its software development subsidiary, Campus Software, to develop a cafeteria system that supports a network of cafeteria Kiosks and POSs (points of sale). Kiosks are distributed in diverse locations of University Town near cafeterias and POSs are located near meal serving stations inside of cafeterias. The users are students, faculty and other employees of University Town. They use Kiosks to make queries, and funds transfers from their bank accounts to their cafeteria accounts. They use POSs to pay for their meals. What can change? The functions of the system computers The platforms, i.e., the hardware, operating system, middleware The environment in which the system operates The systems with which it must interoperate The protocols it uses to communicate The capacity Number of users supported Number of simultaneous operations When is the change made and who makes it? Changes can be made during implementation by modifying the source code build by choice of libraries execution by parameter setting, plugins, etc Changes can also be made by a developer an end user a system administrator What is the cost of the change? Involving two types of cost The cost of introducing the mechanisms to make the system more modifiable The cost of making the modification using the mechanisms Example User interface builder Example You are an architect on the Campus Software team. There is a possibility that in the future, the client will request changes to be made to the user interface. Your team is considering two ways to handle this problem Do nothing for now. Wait for a change request to come in, then change the source code to accommodate request. Add additional component to the system now – user interface builder. When there is a change request, the designer will use a drag-and-drop editor to design a new interface, and the user interface builder will produce the new source code directly Let us consider what is the cost of each option Considering the differences in cost between these options, which one will be a better choice? Which option will be a better choice if the chance of the need to make a change is not high?（1 answer） Do nothing for now. Add user interface builder to the system. Which option will be a better choice if there is a chance that the changes in the interface will be requested many times??(1 answer) Do nothing for now. Add user interface builder to the system. Modifiability General Scenario Sample Concrete Modifiability Scenario The developer wishes to change the user interface by modifying the code at design time. The modifications are made with no side effects within three hours. Stimulus – Wishes to change UI Artifact – Code Environment: Design time Response – Change made Response measure – No side effects in three hours Source - Developer Goal of Modifiability Tactics Goal of modifiability controlling the complexity of making changes, controlling the time and cost to make changes. Modifiability Tactics Design 1 – the module includes a great deal of capability Design B – the module is refined into several smaller modules (each capability is represented in a separate module) If there is a chance needed in this module, which design will have smaller modification cost?(Second one is right) Reduce Size of a Module Split Module: If the module being modified includes a great deal of capability, the modification costs will likely be high. Refining the module into several smaller modules should reduce","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:5:2","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality attributes: Performance, Security Intended Learning Outcomes By the end of this lesson you will be able to: apply the design decision categories to diverse quality attributes What is Performance? It is about time Performance is about time and the software system’s ability to meet timing requirements When events occur, the system must respond to them in time Events include interrupts, messages, requests from users or other systems, or clock events marking the passage of time Performance General Scenario Performance concrete scenario - example You are an architect on the Campus Software team and you are designing the system for performance In the requirements document you have found this requirement: The system should process the transactions with an average latency of two seconds Is this description complete, is it clear enough for the architect to make good design decisions? Which parts are missing? Which parts are missing in the performance concrete scenario example we discussed? Source Stimulus Artifact Environment Response Response measure Sample Concrete Performance Scenario – not complete The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: ??? Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: ??? This scenario is not complete, source and environment parts are missing How to add the remaining parts? We can use the general scenario to help us look for ideas Then, clarify with the stakeholders about the details of the requirement Performance General Scenario Sample Concrete Performance Scenario - complete Users initiate transactions under normal operations. The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: users Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: under normal operations Sample Concrete Performance Scenario Users initiate transactions under normal operations. The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: users Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: under normal operations Performance Modeling Two basic contributors to the response time Processing time is the time that the system is working to respond Blocked time is the time that the system is unable to respond Blocked time is caused by Contention for resources Availability of resources Dependency on other computations Goal of Performance Tactics To generate a response to an event arriving the system within some time-based constraint The event can be single or a stream, and is the trigger to perform computation First, lets consider an example not related to the computing You are an owner of the noodle shop. You hired one cook who is able to cook 100 bowls during lunch time Your restaurant has 20 seats On the first day, there were over 200 people lining up for your noodles You want to improve the performance of your noodle shop to make sure you serve as many customers as possible during the day. Give examples of the tactics that you could use Two Tactic Categories Control resource demand To produce smaller demand on the resources Operate on the demand side Manage resources To make the resources at hand work more effectively in handling the demands Operate on the response side Resources Hardware resources, e.g., CPU, data stores, network bandwidth, and memory Software resources, e.g., buffers, or critical sections Performance Tactics Control Resource Demand Manage Sampling Rate: to reduce the sampling frequency at which a stream of data is captured Prioritize Events: to impose a priority scheme that ranks events according to the importance Ignore low-priority events when resources are not enough Reduce Overhead: The use of intermediaries increas","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:6:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality attributes: Security What is Security? Security is a measure of the system’s ability to protect data and information from unauthorized access while still providing access to people and systems that are authorized An action taken against a computer system with the intention of doing harm is called an attack Attack can be in different forms unauthorized attempt to access/modify data or service intended to deny services to legitimate users What is Security? Security has three main characteristics, called CIA: Confidentiality is the property that data or services are protected from unauthorized access. For example, a hacker cannot access your income tax returns on a government computer. Integrity is the property that data or services are not subject to unauthorized manipulation. For example, your grade has not been changed since your instructor assigned it. Availability is the property that the system will be available for legitimate use. For example, a denial-of-service attack prevent you from ordering a book from an online bookstore. What is Security? Other characteristics that support CIA are Authentication verifies the identities of the parties to a transaction and checks if they are truly who they claim to be. Authorization grants a user the privileges to perform a task. Nonrepudiation guarantees that the sender/recipient of a message cannot later deny having sent/received the message Sample Concrete Security Scenario A disgruntled employee from a remote location attempts to modify the pay rate table during normal operations. The system maintains an audit trail and the correct data is restored within a day. Stimulus: unauthorized attempts to modify the pay rate table Stimulus source: a disgruntled employee Artifact: the system with pay rate table Environment: during normal operation Response: maintains an audit trail Response measure: correct data is restored within a day Security Tactics Detect Attacks Detect Intrusion: compare network traffic or service request patterns within a system to a set of signatures or known patterns of malicious behavior stored in a database. Detect Service Denial: comparison of the pattern or signature of network traffic coming into a system to historic profiles of known Denial of Service (DoS) attacks. Denial of Service Attack Ping of Death UDP Flood TCP SYN Detect Attacks Verify Message Integrity: use techniques such as checksums or hash values to verify the integrity of messages, resource files, deployment files, and configuration files. Detect Attacks Detect Message Delay: checking the time that it takes to deliver a message, it is possible to detect suspicious timing behavior, i.e., man-in-the-middle attack Resist Attacks Identify Actors: identify the source of any external input to the system. Authenticate Actors: ensure that a user or remote computer is actually who or what it purports to be. Authorize Actors: ensuring that an authenticated actor has the rights to access and modify either data or services. Limit Access: limiting access to resources such as memory, network connections, or access points. Resist Attacks Limit Exposure: minimize the attack surface of a system by having the fewest possible number of access points. E.g., firewall is a single point of access to the intranet E.g., closing a port Passive defense – no human analysis or interaction - limiting security gaps and exposure to threats through firewalls, antimalware systems, intrusion prevention systems, antivirus protection, intrusion detection systems… Resist Attacks Encrypt Data: apply some form of encryption to data and to communication. Symmetric encryption： asymmetric encryption： Resist Attacks Separate Entities: can be done through physical separation on different servers, the use of virtual machines Change Default Settings: Force the user to change settings assigned by default. React to Attacks Revoke Access: limit access to sensitive resources, even for normally legitimate users and uses, if an attack is s","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:6:1","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Patterns and Tactics Intended Learning Outcomes By the end of this lesson you will be able to: Understand how patterns achieve quality attributes Understand relation between tactics and patterns Know how to use tactics together What is a Pattern? An architecture pattern is a package of design decisions that is found repeatedly in practice has known properties that permits reuse, and describes a class of architectures Architectural Patterns Module patterns Layered pattern Component-and-Connector patterns Broker pattern Model-View-Controller pattern Pipe-and-Filter pattern Client-Server pattern Peer-to-Peer pattern Service-Oriented Architecture (SOA) pattern Publish-Subscribe pattern Shared-Data pattern Allocation patterns Map-Reduce pattern Multi-tier Pattern Layer Pattern Context: Modules of the system may be independently developed and maintained. Problem: To minimize the interaction among the different development organizations, and support portability, modifiability, and reuse. Layer Pattern Solution: the layered pattern divides the software into units called layers. Each layer is a grouping of modules that offers a cohesive set of services. Each layer is exposed through a public interface. The usage must be unidirectional. Layer Pattern Example Layer Pattern Solution The layered pattern defines layers and a unidirectional allowed-to-use relation among the layers. Elements: Layer, a kind of module. Relations: Allowed to use. Constraints: Every piece of software is allocated to exactly one layer. There are at least two layers The allowed-to-use relations should not be circular Weaknesses: The addition of layers adds up cost and complexity to a system. Layers contribute a performance penalty. Three layered applications Architectural Patterns Module patterns Layered pattern Component-and-Connector patterns Broker pattern Model-View-Controller pattern Pipe-and-Filter pattern Client-Server pattern Peer-to-Peer pattern Service-Oriented Architecture (SOA) pattern Publish-Subscribe pattern Shared-Data pattern Allocation patterns Map-Reduce pattern Multi-tier Pattern Architectural Patterns Component-and-Connector patterns Broker pattern Model-View-Controller pattern Pipe-and-Filter pattern Client-Server pattern Peer-to-Peer pattern Service-Oriented Architecture (SOA) pattern Publish-Subscribe pattern Shared-Data pattern Broker Pattern Context: Many systems are constructed from a collection of services distributed across multiple servers Problem: How do we structure distributed software so that service users do not need to know the nature and location of service providers? Solution: The broker pattern separates clients from providers servers by inserting an intermediary, called a broker. When a client needs a service, it queries a broker via a service interface. The broker then forwards the client’s service request to a server, which processes the request. Broker Solution Overview: The broker pattern defines a runtime component, called a broker, that mediates the communication between a number of clients and servers. Elements: Client, a requester of services Server, a provider of services Broker, an intermediary that locates an appropriate server to fulfill a client’s request, forwards the request to the server, and returns the results to the client Broker Solution Constraints: The client can only attach to a broker. The server can only attach to a broker. Weaknesses: Brokers add latency between clients and servers, and it may be a communication bottleneck. …. Consider there is one broker that mediates the communication between a large number of clients and servers. What happens when the broker fails? Nothing happens, the clients can start communicating directly with the servers All the clients and server are not able to communicate in this situation Broker Solution Constraints: The client can only attach to a broker. The server can only attach to a broker. Weaknesses: Brokers add latency between clients and servers, and it may be a ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:7:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality Attribute Modelling and Analysis Intended Learning Outcomes By the end of this lesson you will be able to: Model architectures to enable quality attribute analysis Use quality attribute checklists Generate and carry out a thought experiment to perform early analysis Analyse architectures using alternative ways: experiments, simulations and prototypes Analyse at different stages of the life cycle How do we know the quality attributes of a software? Analytic model Experiment and measurement Simulations Modeling Architectures to Enable Quality Attribute Analysis Some quality attributes, most notably performance and availability, have well-understood, time-tested analytic models that can be used to assist in an analysis. By analytic model, we mean one that supports quantitative analysis. Let us first consider performance. Performance Models Parameters: arrival rate of events, chosen queuing discipline, chosen scheduling algorithm, service time for events, network topology, network bandwidth, routing algorithm chosen Allocation Model for MVC Queuing Model for MVC Arrivals View sends requests to Controller Actions returned to View Actions returned to model Model sends actions to View Parameters To solve a queuing model for MVC performance, the following parameters must be known or estimated: The frequency of arrivals from outside the system The queuing discipline used at the view queue The time to process a message within the view The number and size of messages that the view sends to the controller The bandwidth of the network that connects the view and the controller The queuing discipline used by the controller The time to process a message within the controller The number and size of messages that the controller sends back to the view The bandwidth of the network from the controller to the view The number and size of messages that the controller sends to the model The queuing discipline used by the model The time to process a message within the model The number and size of messages the model sends to the view The bandwidth of the network connecting the model and the view Parameters To solve a queuing model for MVC performance, the following parameters must be known or estimated: The frequency of arrivals from outside the system The queuing discipline used at the view queue The time to process a message within the view The number and size of messages that the view sends to the controller The bandwidth of the network that connects the view and the controller The queuing discipline used by the controller The time to process a message within the controller The number and size of messages that the controller sends back to the view The bandwidth of the network from the controller to the view The number and size of messages that the controller sends to the model The queuing discipline used by the model The time to process a message within the model The number and size of messages the model sends to the view The bandwidth of the network connecting the model and the view Cost/benefit of Performance Modeling Cost: determining the parameters previously mentioned Benefit: estimate of the latency The more accurately the parameters can be estimated, the better the predication of latency. This is worthwhile when latency is important and questionable. This is not worthwhile when it is obvious there is sufficient capacity to satisfy the demand. Availability Modeling Another quality attribute with a well-understood analytic framework is availability. Modeling an architecture for availability—or to put it more carefully, modeling an architecture to determine the availability of a system based on that architecture—is a matter of determining the failure rates and the recovery times of the components. Example Just as for performance, to model an architecture for availability, we need an architecture to analyze. Suppose we want to increase the availability of a system that uses the Broker pattern, by applying redundancy tactics. Availability M","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:8:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Designing for architecturally significant requirements Intended Learning Outcomes By the end of this lesson you will be able to: understand the concept of ASR identify and capture ASR design for ASR Architecturally Significant Requirement Architectures exist to build systems that satisfy requirements. But, to an architect, not all requirements are created equal. An Architecturally Significant Requirement (ASR) is a requirement that will have a profound effect on the architecture. How do we find those? Approaches to Capture ASRs From Requirements Document By Interviewing Stakeholders By Understanding the Business Goals In Utility Tree ASRs and Requirements Documents An obvious location to look for candidate ASRs is in the requirements documents Requirements should be in requirements documents! Unfortunately, this is not usually the case. Don’t Get Your Hopes Up Many projects don’t create or maintain the detailed, high-quality requirements documents. Standard requirements pay more attention to functionality than quality attributes. The architecture is driven by quality attribute requirements rather than functionalities Most requirements specification does not affect the architecture. Don’t Get Your Hopes Up Quality attributes are often captured poorly, e.g. “The system shall be modular” “The system shall exhibit high usability” “The system shall meet users’ performance expectations” Much of what is useful to an archit ect is not in even the best requirements document ASRs often derive from business goals in the development organization itself Gathering ASRs from Stakeholders Stakeholders often have no idea what QAs they want in a system if you insist on quantitative QA requirements, you’re likely to get numbers that are arbitrary. at least some of those requirements will be very difficult to satisfy. Architects often have very good ideas about what QAs are reasonable to provide. Interviewing the stakeholders is the surest way to learn what they know and need. “The system shall meet users’ performance expectations”- this is an example of a poorly captured quality attribute requirement. Do you know a tool to help us express quality requirements in a better way? No, there is no better way. There should be a better way, but I do not know how Yes – use a quality attribute scenario and make sure all 6 parts are included Gathering ASRs from Stakeholders The results of stakeholder interviews should include a list of architectural drivers a set of QA scenarios that the stakeholders (as a group) prioritized. This information can be used to: refine system and software requirements understand and clarify the system’s architectural drivers provide rationale for why the architect subsequently made certain design decisions guide the development of prototypes and simulations influence the order in which the architecture is developed. Quality Attribute Scenario: Example Our vehicle information system sends our current location to the traffic monitoring system. The traffic monitoring system combines our location with other information, overlays this information on a Google Map, and broadcasts it. Our location information is correctly included with a probability of 99.9%. Quality Attribute Scenario: Example The developer wishes to change the user interface by modifying the code at design time. The modifications are made with no side effects within three hours. Stimulus – Wishes to change UI Artifact – Code Environment: Design time Response – Change made Response measure – No side effects in three hours Source - Developer Quality Attribute Scenario: Example Users initiate transactions under normal operations. The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: users Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: under normal operation Quality Attribute Scenario: Example A disgruntled employee from a remote location","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:9:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Designing and evaluating an architecture Intended Learning Outcomes By the end of this lesson you will be able to: design a software architecture using ADD approach evaluate an architecture using ATAM method Design Strategy Decomposition Designing to Architecturally Significant Requirements Generate and Test The Attribute-Driven Design Method An iterative method. At each iteration you combine the 3 design strategies we studied earlier Choose a part of the system to design. Marshal all the architecturally significant requirements for that part. Generate and test a design for that part. ADD does not result in a complete design Set of containers with responsibilities Interactions and information flow among containers Does not produce an API for containers. ADD Inputs Requirements Functional, quality, constraints A context description What are the boundary of the system being designed? What are the external systems, devices, users and environment conditions with which the system being designed must interact? ADD Outputs Architectural elements and their relationship Responsibility of elements Interactions Information flow among the elements The Steps of ADD Choose an element of the system to design. Identify the ASRs for the chosen element. Generate a design solution for the chosen element. Inventory remaining requirements and select the input for the next iteration. Repeat steps 1–4 until all the ASRs have been satisfied. Step 1: Choose an Element of the System to Design For green field designs, the element chosen is usually the whole system. For legacy designs, the element is the portion to be added. After the first iteration: Step 1 example We want to design a system for online travel agency Step 1 example In iteration #1, we decide to apply SOA pattern (Service Oriented Architecture, where functions of the system will become services) We decompose our system into necessary components based on SOA pattern Step 1 example Then, we need to choose which part of the system to design next, In iteration #2, we choose to further design SOA infrastructure components element We decide to decompose it into 3 components Step 1 example Then, we need to choose which part of the system to design next, In iteration #3, which element should we choose to design? Refine one of the remaining SOA elements? Refine one of the SOA infrastructure components? Which Element Comes Next? Two basic refinement strategies: Breadth first Depth first Which one to choose? If using new technology =\u003e depth first: explore the implications of using that technology. If a team needs work =\u003e depth first: generate requirements for that team. Otherwise =\u003e breadth first. Step 2: Identify the ASRs for the Chosen Element If the chosen element is the whole system, then use a utility tree (as described earlier). If the chosen element is further down the decomposition tree, then generate a utility tree from the requirements for that element. Step 3: Generate a Design Solution for the Chosen Element Apply generate and test to the chosen element with its ASRs Step 4 Inventory remaining requirements and select the input for the next iteration We need to consider 3 kinds of requirements Functional requirements Quality attribute requirements Constraints Step 4: Select the Input for the Next Iteration For each functional requirement Ensure that requirement has been satisfied. If not, then add responsibilities to satisfy the requirement. Add them to container with similar requirements If no such container, may need to create new one or add to container with dissimilar responsibilities (coherence) If container has too many requirements for a team, split it into two portions. Try to achieve loose coupling when splitting. Quality Attribute Requirements If the quality attribute requirement has been satisfied, it does not need to be further considered. If the quality attribute requirement has not been satisfied then either Delegate it to one of the child elements Split it among the child e","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:10:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Exercise 1 Capturing ASR in practice . Intended Learning Outcomes By the end of this lesson you will be able to: Specify concrete scenarios for a real system Create a utility tree for the real system Our system – ATM network Please check our WeChat group – I have sent a document The document in front of you describes the requirements for the system, including functional requirements and known quality attribute requirements Activity: Scenario Brainstorming (1) Based on the received document, express scenarios representing your concerns with respect to the system. Describe your scenarios in as many details as you can (do NOT use the concrete scenario form yet) Pick one of the scenarios and submit as an answer to the question. Activity: Scenario Brainstorming (2) Read scenarios of other participants, while reading, you can write more scenarios Your new scenarios can be inspired by what the other participant shared, but not the same Submit a new scenario you have wrote, inspired by scenarios of other participants Next steps Scenarios consolidation Participants discuss and consolidate the scenarios where reasonable. Scenario Prioritization - voting Each participants receives a number of votes equal to 30 percent of the total number of scenarios Each participant allocates their votes to scenarios Refine and elaborate the top scenarios Put the scenarios in the six-part scenario form (concrete scenario) Specifying Quality Attribute Requirements Stimulus. he stimulus is a condition that requires a response when it arrives at a system. Source of stimulus. his is some entity (a human, a computer system, or any other actuator) that generated the stimulus. Response. he response is the activity undertaken as the result of the arrival of the stimulus. Response measure. hen the response occurs, it should be measurable in some fashion so that the requirement can be tested. Environment. he stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation, or some other relevant state. Artifact. his may be a collection of systems, the whole system, or some piece or pieces of it. Some artifact is stimulated. Quality Attribute Scenario: Example Our vehicle information system sends our current location to the traffic monitoring system. The traffic monitoring system combines our location with other information, overlays this information on a Google Map, and broadcasts it. Our location information is correctly included with a probability of 99.9%. Quality Attribute Scenario: Example The developer wishes to change the user interface by modifying the code at design time. The modifications are made with no side effects within three hours. Stimulus – Wishes to change UI Artifact – Code Environment: Design time Response – Change made Response measure – No side effects in three hours Source - Developer Quality Attribute Scenario: Example Users initiate transactions under normal operations. The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: users Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: under normal operation Quality Attribute Scenario: Example A disgruntled employee from a remote location attempts to modify the pay rate table during normal operations. The system maintains an audit trail and the correct data is restored within a day. Stimulus: unauthorized attempts to modify the pay rate table Stimulus source: a disgruntled employee Artifact: the system with pay rate table Environment: during normal operation Response: maintains an audit trail Response measure: correct data is restored within a day Quality Attribute Scenario: Example The user downloads a new application and is using it productively after two minutes of experimentation. Source: user Stimulus: download a new application Artifact: system Environment: runtime Response: user uses application productively Response ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:11:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"exercise2 Please comply with epidemic prevention and control policies Choose the seat that you will use during the whole semester and register your information in the system. Make sure to take the same seat during every lecture. Intended Learning Outcomes By the end of this lesson you will be able to: Create a design for a real system using the ADD method employing and instantiating a pattern ASRs We will add/expand the following ASRs to our utility tree Modifiability Performance Modifiability The system must be easily modified to take advantage of new platform capabilities (for example, it must not be tied to a single database or to a single kind of client hardware or software) and that it must be extensible to allow the addition of new functions and new business rules. Modifiability scenarios A developer wishes to add a new auditing business rule at design time and makes the modification, without affecting other functionality, in 10 person-days. A developer wishes to change the relational schema to add a new view to the database, without affecting other functionality, in 30 person-days. A system administrator wishes to employ a new database and makes the modification, without affecting other functionality, in 18 person-months. A developer wishes to add a new function to a client menu, without side effects, in 15 person-days. A developer needs to add a Web-based client to the system, without affecting the functionality of the existing ATM client, in 90 person-days. Performance Latency of an operation (such as an ATM withdrawal) Performance scenario “The user can withdraw a limit of $300 from an account that has sufficient funds in less than 10 seconds.” There are two functional requirements and one performance requirement in this scenario. One function is a withdrawal Another function is a limit (a constraint of $300 if it is in the account) Performance constraint/quality attribute - “less than 10 seconds” ADD method The ADD method defines a software architecture by basing the design process on the quality attribute requirements of the system. The ADD approach follows a recursive decomposition process where, at each stage in the decomposition, architectural tactics and patterns are selected to satisfy a chosen set of high-priority quality scenarios. ADD: The Steps of ADD Choose an element of the system to design. Identify the ASRs for the chosen element. Generate a design solution for the chosen element. Inventory remaining requirements and select the input for the next iteration. Repeat steps 1–4 until all the ASRs have been satisfied. Step 1: Choose an Element of the System to Design For green field designs, the element chosen is usually the whole system. For legacy designs, the element is the portion to be added. After the first iteration: Step 2: Identify the ASRs for the Chosen Element If the chosen element is the whole system, then use a utility tree (as described earlier). Let us first look at our modifiability scenarios Modifiability scenarios A developer wishes to add a new auditing business rule at design time and makes the modification, without affecting other functionality, in 10 person-days. A developer wishes to change the relational schema to add a new view to the database, without affecting other functionality, in 30 person-days. A system administrator wishes to employ a new database and makes the modification, without affecting other functionality, in 18 person-months. A developer wishes to add a new function to a client menu, without side effects, in 15 person-days. A developer needs to add a Web-based client to the system, without affecting the functionality of the existing ATM client, in 90 person-days. Step 3: Generate a Design Solution for the Chosen Element Apply generate and test to the chosen element with its ASRs Activity: Use patterns and tactics Based on the modifiability scenarios, generate a candidate design employing and instantiating a pattern. Which tactics are you going to use? Tips (1) In or","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:12:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"exercise3 Please comply with epidemic prevention and control policies Choose the seat that you will use during the whole semester and register your information in the system. Make sure to take the same seat during every lecture. Intended Learning Outcomes By the end of this lesson you will be able to: Create a documentation for a real system Structures and Views Architecture Is a Set of Software Structures A structure is the set of elements itself, as they exist in software or hardware. In short, a view is a representation of a structure. Architects design structures. They document views of those structures. Architecture Documentation Even the best architecture will be useless if the people who need it do not know what it is; cannot understand it well enough to use, build, or modify it; misunderstand it and apply it incorrectly. All of the effort, analysis, hard work, and insightful design on the part of the architecture team will have been wasted. Architecture Documentation and Stakeholders Education Introducing people to the system New members of the team External analysts or evaluators New architect Primary vehicle for communication among stakeholders Especially architect to developers Especially architect to future architect! Basis for system analysis and construction Architecture tells implementers what to implement. Each module has interfaces that must be provided and uses interfaces from other modules. Documentation can serve as a receptacle for registering and communicating unresolved issues. Architecture documentation serves as the basis for architecture evaluation. Views Principle of architecture documentation: Documenting an architecture is a matter of documenting the relevant views and then adding documentation that applies to more than one view. Which Views? The Ones You Need! Different views support different goals and uses. The views you should document depend on the uses you expect to make of the documentation. Each view has a cost and a benefit; you should ensure that the benefits of maintaining a view outweigh its costs. Building the Documentation Package Documentation package consists of Views Documentation beyond views View Template Implementation View Describes the static organization of the software in its development environment. Viewer Programmers and Software Managers Considers Software module organization - Hierarchy of layers, software management, reuse, constraints of tools Style layered style Notation Activity For our simple ATM network, with additional database servers deployed, create an implementation view Implementation and deployment views Implementation view Deployment View Describes the mapping(s) of the software onto the hardware and reflects its distributed aspect. Viewer System Engineers Considers Non-functional requirement (reliability, availability and performance) regarding to underlying hardware. Deployment view example Activity For our simple ATM network, with additional database servers deployed, create a deployment view Implementation and deployment views Implementation view Deployment view Use Case View Captures system functionality as seen by users Built in early stages of development Developed by analysts and domain experts System behavior, that is what functionality it must provide, is documented in a use case model. Use Case Model Illustrates the system’s intended functions (use cases), its surroundings (actors), and relationships between the use cases and actors (use case diagrams). provides a vehicle used by the customers or end users and the developers to discuss the system’s functionality and behavior. starts in the Inception phase with the identification of actors and principal use cases for the system, and is then matured in the elaboration phase, by adding more details and additional use cases. Graphical Constructs Actors represent anyone or anything that must interact with the system: they are not part of the system. may: only input information to the system only recei","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:13:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"系统模型 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:0:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"简介 物理模型是描述系统的-个最显式的方法，它从计算机(和其他设备，例如移动电话)及其互联的网络方面考虑系统的硬件组成。 体系结构模型从系统的计算元素执行的计算和通信任务方面来描述系统。 基础模型采用抽象的观点描述分布式系统的某个方面。本章介绍考察分布式系统三个重要方面的基础模型: 交互模型，它考虑在系统元素之间通信的结构和顺序; 故障模型，它考虑一个系统可能不能正确操作的方式; 安全模型，它考虑如何保护系统使其不受到正确操作的干扰或不被窃取数据。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:1:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"物理模型 物理模型是从计算机和所用网络技术的特定细节中抽象出来的分布式系统底层硬件元素的表示。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:2:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"体系结构模型 本节采取一种三阶段方法: 首先，描述支撑现代分布式系统的核心基本体系结构元素，重点展示现在已有方法的不同; 考察能在开发复杂分布式系统解决方案中单独使用或组合使用的复合体系结构模式; 最后，对于以上体系结构风格中出现的不同编程风格，考虑可用于支持它们的中间件平台。 注意，有许多与本章中介绍的体系结构模型相关的权衡，其中涉及采用的系统体系结构元素、所采用的模式和(在合适的地方)使用的中间件，它们会影响结果系统的性能和有效性。理解这样的权衡可以说是分布式系统设计中的关键技能。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:3:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"体系结构元素 通信实体：从系统的观点，回答通常是非常清楚的,这是因为在一个分布式系统中通信的实体通常是进程，这导致普遍地把分布式系统看成是带有恰当进程间通信范型的多个进程(如在第4章中讨论的)，有两个注意事项: 在一些原始环境中，例如传感器网络,基本的操作系统可能不支持进程抽象(或甚至任何形式的隔离)，因此在这些系统中通信的实体是结点。 在大多数分布式系统环境中，用线程补充进程，所以，严格说来，通信的末端是线程。 对象：对象已被引人以便在分布式系统中使用面向对象的方法(包括面向对象的设计和面向对象的编程语言)。在分布式面向对象的方法中，-个计算由若千交互的对象组成，这些对象代表分解给定问题领域的自然单元。对象通过接口被访问，用一个相关的接口定义语言(IDL) 提供定义在一个对象上的方法的规约。分布式对象已经成为分布式系统研究的一个主要领域，第5章和第8章将进一步讨论这个话题。 组件:因为对象的引入，许多重要的问题已被认为与分布式对象有关，组件技术的出现及使用是对这些弱点的一个直接响应。组件类似于对象，因为它们为构造分布式系统提供面向问题的抽象，也是通过接口被访问。关键的区别在于组件不仅指定其(提供的)接口而且给出关于其他组件/接口的假设，其他组件/接口是组件完成它的功能必须有的。换句话说,组件使得所有依赖显式化，为系统的构造提供一个更完整的合约。这个合约化的方法鼓励和促进第三方开发组件，也通过去除隐含的依赖 提升了一个更纯粹的组合化方法来构造分布式系统。基于组件的中间件经常对关键领域如部署和服务器方编程支持提供额外的支持[ Heineman and Councill 2001]。关于基于组件方法的进一步细节请参见第8章。 Web服务: Web 服务代表开发分布式系统的第三种重要的范型[Alonso et al. 2004]。Web 服务与对象和组件紧密相关，也是采取基于行为封装和通过接口访问的方法。但是，相比而言，通过利用Web标准表示和发现服务，Web 服务本质上是被集成到万维网(即W3C)的。W3C ( World WideWeb)联盟把Web服务定义成： 一个软件应用，通过URI被辨识，它的接口和绑定能作为XML制品被定义描述和发现。一个Web服务通过在基于互联网的协议上利用基于XML的消息交换支持与其他软件代理的直接交互。 换句话说，Web服务采用的基于Web的技术在-定程度上定义了Web服务。另一个重要的区别来源于技术使用的风格。对象和组件经常在一个组织内部使用，用于开发紧耦合的应用，但Web服务本身通常被看成完整的服务，它们可以组合起来获得增值服务，它们经常跨组织边界，因此可以实现业 务到业务的集成。Web服务可以由不同的提供商用不同的底层技术实现。Web服务将在第9章做进一步的探讨。 通信范型我们现在转向在分 布式系统中实体如何通信,考虑三种通信范型: 进程间通信; 远程调用; 间接通信。 进程间通信指的是用于分布式系统进程之间通信的相对底层的支持，包括消息传递原语、直接访问由互联网协议提供的API (套接字编程)和对多播通信的支持。第4章将详细讨论这样的服务。 远程调用代表分布式系统中最常见的通信范型，覆盖一系列分布式系统中通信实体之间基于双向交换的技术，包括调用远程操作、过程或方法。进一步的定义参见下面内容(详细讨论见第5章): 请求-应答协议是一个有效的模式，它加在一个底层消息传递服务之上，用于支持客户-服务器计算。特别的，这样的协议通常涉及一对消息的交换，消息从客户到服务器，接着从服务器返回客户，第一个消息包含在服务器端执行的操作的编码，然后是保存相关参数的字节数组，第二个消息包含操作的结果，它也被编码成字节数组。这种范型相对原始，实际上仅被用于嵌人式系统，对嵌人式系统来说性能是至关重要的。这个方法也被用在5.2节描述的HTTP协议中。正如下面讨论的，大多数分布式系统将选择使用远程过程调用或者远程方法调用，但注意底层的请求-应答交换支持两种方法。 远程过程调用( Remote Procedure Call, RPC)的概念，最初由Birell 和Nelson [1984] 提出，代表了分布式计算中的一个主要突破。在RPC中,远程计算机上进程中的过程能被调用，好像它们是在本地地址空间中的过程一样。底层RPC系统隐藏了分布的重要方面，包括参数和结果的编码和解码、消息的传递和保持过程调用所要求的语义。这个方法直接而且得体地支持了客户-服务器计算，其中,服务器通过一个服务接口提供一套操作, 当这些操作本地可用时客户直接调用这些操作。因此，RPC系统(在最低程度上)提供访问和位置透明性。 远程方法调用( Remote Method Invocation, RMI) 非常类似于远程过程调用，但它应用于分布式对象的环境。用这种方法，-个发起调用的对象能调用一个远程对象中的方法。与RPC一样，底层的细节都对用户隐藏。不过，通过支持对象标识和在远程调用中传递对象标识符作为参数，RMI 实现做得更多。它们也从与面向对象语言(见第5章相关讨论)的紧密集成中获得更多的好处。 上述技术具有一个共同点:通信代表发送者和接收者之间的双向关系，其中，发送者显式地把消息/调用送往相关的接收者。接收者通常了解发送者的标识，在大多数情况下，双方必须在同时存在。相比而言，已经出现若千技术，这些技术支持间接通信,通过第三个实体，允许在发送者和接收者之间的深度解耦合。尤其是: 发送者不需要知道他们正在发送给谁(空间解耦合)。 发送者和接收者不需要同时存在(时间解耦合)。 第6章将详细讨论间接通信。 间接通信的关键技术包括: 组通信:组通信涉及消息传递给若干接收者，因此是支持一对多通信的多方通信范型。组通信依赖组抽象，-一个组在系统中用-一个组标识符表示。接收方通过加人组，就能选择性接收发送到组的消息。发送者通过组标识符发送消息给组，因此，不需要知道消息的接收者。组通常也要维护组成员,具有处理组成员故障的机制。 发布-订阅系统:许多系统，例如第1章中金融贸易的例子，被归类于信息分发系统，其中，大量生产者(或发布者)为大量的消费者(或订阅者)发布他们感兴趣的信息项(事件)。采用前述的任-核心通信范型来实现这个需求是复杂且低效的，因此，出现了发布-订阅系统(有时也叫分布式基于事件的系统)用于满足此项重要需求[ Muhl et al. 2006]。发布-订阅系统共享同一个关键的特征，即提供-一个中间服务,有效确保由生产者生成的信息被路由到需要这个信息的消费者。 消息队列:虽然发布-订阅系统提供一种一对多风格的通信,但消息队列提供了点对点服务,其中生产者进程能发送消息到一个指定的队列，消费者进程能从队列中接收消息，或被通知队列里有新消息到达。因此，队列是生产者和消费者进程的中介。 元组空间:元组空间提供了进-步的间接通信服务，并支持这样的模型一进程 能把任意的结构化数据项( 称为元组)放到一个持久元组空间，其他进程可以指定感兴趣的模式，从而可以在元组空间读或者删除元组。因为元组空间是持久的，读操作者和写操作者不需要同时存在。这种风格的编程，也被称为生成通信，由Gelemter [ 1985]作为一种并行编程范型引人。已经开发了不少分布式实现，采用了客户-服务器-风格的实现或采用了更分散的对等方法。 分布式共享内存:分布式共享内存(Distributed Shared Memory, DSM)系统提供一种抽象，用于支持在不共享物理(内存的进程之间共享数据。提供给程序员的是-套熟悉的读或写(共享)数据结构的抽象，就好像这些数据在程序员自已本地的地址空间一样,从而提供了高层的分布透明性。基本的基础设施必须确保以及时的方式提供副本，也必须处理与数据同步和一致性相关的问题。分布式共享内存的概述在第6章中介绍。 到目前为止讨论的体系结构 角色和责任，在一个分布式系统中, 进程，或者说，对象、组件、服务，包括Web服务(为简单起见，我们在本节中使用术语“进程”)相互交互完成一个有用的活动，例如支持一次聊天会话。在这样做的时候，进程扮演给定的角色，在建立所采用的整体体系结构时，这些角色是基本的。本节我们考察两种起源于单个进程角色的体系结构风格:客户-服务器风格和对等风格。 客户-服务器:这是讨论分布式系统时最常引用的体系结构。它是历史上最重要的体系结构，现在仍被广泛地使用。图2-3 给出了一个简单的结构，其中，进程扮演服务器和客户的角色。特别是,为了访问服务器管理的共享资源，客户进程可以与不同主机上的服务器进程交互。如图2-3所示，一台服务器也可以是其他服务器的客户。例如，Web服务器通常是管理存储Web页面文件的本地文件服务器的客户。Web 服务器和大多数其他互联网服务是DNS服务的客户，DNS服务用于将互联网域名翻译成网络地址。另一个与Web相关的例子是搜索引擎,搜索引擎能让用户通过互联网查看Web页面上可用的信息汇总。这些信息汇总通过称为“Web 抓取”的程序形成，该程序在搜索引擎站点以后台方式运行，利用HTTP请求访问互联网上的Web服务器。因此，搜索引擎既是服务器又是客户:它回答来自浏览器客户的查询，并且运行作为其他Web服务器客户的Web抓取程序。在这个例子中，服务器任务(对用户查询的回答)和Web抓取的任务( 向其他Web服务器发送请求)是完全独立的，很少需要同步它们，它们可以并行运行。事实上,一个典型的搜索引擎正常情况下包含许多并发执行的线程，一些线程为它的客户服务，另一些线程运行Web抓取程序。 对等体系结构:在这种体系结构中,涉及一项任务或活动的所有进程扮演相同的角色，作为对","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:3:1","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"体系结构模式 我们给出分布式系统中几个关键的体系结构模型，包括分层体系结构( layering architecture)、层次化体系结构( tiered ar-chitecture)和瘦客户相关的概念(包括虚拟网络计算的特定机制)。我们也把Web服务当做一个体系结构模式进行了考察，给出了其他可以应用在分布式系统中的模式。 分层，分层的概念是-一个熟悉的概念，与抽象紧密相关。在分层方法中，一个复杂的系统被分成若干层，每层利用下层提供的服务。因此，一个给定的层提供-一个软件抽象，更高的层不清楚实现细节，或不清楚在它下面的其他层。就分布式系统而言，这等同于把服务垂直组织成服务层。一个分布式服务可由一个或多个服务器进程提供，这些进程相互交互，并与客户进程交互，维护服务中的资源在系统范围内的-致视图。 层次化体系结构层次化体系结构 与分层体系结构是互补的。分层将服务垂直组织成抽象层，而层次化是一项组织给定层功能的技术，它把这个功能放在合适的服务器上，或者作为第二选择放在物理结点上。 AJAX的作用:在1.6节中，我们介绍了AJAX ( Asynchronous Javascript And XML)是Web所使用的标准客户-服务器交互方式的扩展。AJAX满足了Javascript 前端程序( 运行在Web浏览器中)和基于服务器的后端程序(拥有描述应用状态的数据)之间的细粒度通信的需要。概括而言，在标准的Web交互方式中，浏览器发送HTTP请求给服务器，请求给定URL的页面、图像或其他资源。服务器发送整个页面作为应答，这个页面或者从服务器上的-一个文件中读取，或者由一个程序生成，取决于 URL中可识别的资源类型。当客户收到内容时，浏览器根据其MIME类型( text/html、image/jpg 等)相关的显示方式呈现它。虽然Web页面由不同类型的内容项组成，但是整个页面以它在html页面定义中指定的方式由浏览器组合并呈现。 瘦客户，分布式计算的趋势是将复杂性从最终用户设备移向互联网服务。这点在向云计算(见第1章)发展的趋势中最明显，在上面讨论的层次化体系结构中也能看到。这个趋势导致了对瘦客户概念的兴趣，它使得能以很少的对客户设备的假设或需求，获得对复杂网络化服务的访问，这些服务可以通过云解决方案提供。更具体来说，术语瘦客户指的是一个软件层，在执行一个应用程序或访问远程计算机上的服务时，由该软件层提供一个基于窗口的本地用户界面。 其他经常出现的模式： 代理 代理(proxy) 模式是分布式系统中经常出现的模式，其主要用于支持远程过程调用或远程方法调用的位置透明性。用这种方法，一个代理在本地地址空间中被创建，用于代表远程对象。这个代理提供与远程对象-样的接口，程序员调用这个代理对象，因此无须了解交互的分布式特性。在RPC和RMI中，代理支持位置透明性的作用将在第5章做进一步的讨论。 注意代理也被用于封装其他的功能( 诸如复制或缓存的放置策略等)。 web服务中的业务代理 Web服务中的业务代理(brokerage)的使用能被看成是一个在可能很复杂的分布式基础设施中支持互操作性的体系结构模式。特别地，这个模式是由服务提供商、服务请求者和服务代理(提供与请求的服务-致的服务)三部分组成，如图2-11所示。这个业务代理模式在分布式系统的多个领域被多次应用，例如Java RMI中的注册服务、CORBA中的名字服务( 分别参见第5章和第8章的讨论)。 反射 反射( reflection)模式在分布式系统中作为支持内省(系统的动态发现的特性)和从中调停(动态修改结构或行为的能力)的手段而被持续地使用。.例如，Java的内省能力被用于RMI的实现中，提供通用的分发(参见5.4.2节的讨论)。在一个反射系统中，标准的服务接口在基础层可供使.用，但元层接口也可以提供对涉及服务实现的组件及组件参数的访问。许多技术在元层可用，包括截获到达的消息或调用、动态发现由给定对象提供的接口、发现和适应系统底层体系结构的能力。反射被应用于分布式系统中的多个领域，特别是反射中间件领域，例如，可以用于支持更多的可配置及重配置中间件体系结构[Kon et al. 2001]。与分布式系统相关的体系结构模式更多的例子可以在Bushmann等人[2007] 的著作中找到。. ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:3:2","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"相关的中间件解决方案 第1章引入了中间件，在2.3.2节讨论分层体系结构时又重温了中间件。中间件的任务是为分布式系统的开发提供-一个高层的编程抽象，并且通过分层，对底层基础设施中的异构性提供抽象，从而提升互操作性和可移植性。中间件解决方案是基于2. 3. 1节引人的体系结构模型，也支持更复杂的体系结构模式。本节我们简要回顾一下现在存在的中间件类别，为在本书的其他部分进-步研究这 些解决方案做好准备。 中间件的类别 远程过程调用包, (如Sun RPC,第5章)和组通信(如ISIS,第6章和第18章)属于最早的中间件实例。从那以后，出现了大量不同风格的中间件，大部分都基于上面介绍的体系结构模型。我们在图2-12中给出了中间件平台的分类，其中交叉引用了其他章,那些章更详细地讨论了不同种类的中间件。需要强调的是分类并不精确，现代中间件平台试图提供混合的解决方案。例如，许多分布式对象平台提供分布式事件服务，来补充传统的对远程方法调用的支持。类似地，出于互操作性的原因,许多基于组件的平台(和平台的其他分类)也支持Web服务和标准。从中间件标准和今天可用的技术的角度来看,还应该强调这个分类并不完整，其目的在于给出中间件的主要类别。其他(未给出的)解决方案是比较特定的，例如，特定于提供某-通信范型，如消息传递、远程过程调用、分布式共享内存、元组空间或组通信。 图2-12中的中间件的顶层分类是根据通信实体和相关通信范型而确定的，遵循五个主要的体系结构模型:分布式对象、分布式组件、发布-订阅系统、消息队列和Web服务。对等系统是这些类别的补充，基于2.3. 1节讨论的协作方法，对等系统是中间件-一个相当独立的分支。应用服务器，显示为分布式组件的子类，也提供对三层体系结构的直接支持。特别地，应用服务器提供了结构以支持应用逻辑和数据存储的分离，以及对其他特性( 如安全性和可靠性)的支持。详细细节将延后到第8章讨论。 除了编程抽象之外，中间件也能提供分布式系统的基础设施服务,供应用程序或其他服务使用。这些基础设施服务与中间件提供的分布式编程模式是紧密绑定的。例如，CORBA (第8章)提供给应用一系列的CORBA服务，包括对程序安全和可靠的支持。如上所述和在第8章中的进-一步讨论，应用服务器也提供对这些服务的内在支持。 中间件的限制 许多分布式应用完全依赖中间件提供的服务来支持应用的通信和数据共享需求。例如，一个适合客户-服务器模型的应用，如一个名字和地址的数据库，可以依赖只提供远程方法调用的中间件。 通过依靠中间件支持的开发，能大大简化分布式系统的编程，但系统可依赖性的一些方面要求应用层面的支持。 考虑从发送者的邮件主机传递大量的电子邮件消息到接收者的邮件主机。乍一看，这是一个TCP数据传输协议的简单应用( 见第3章的相关讨论)。但考虑这样的问题:用户试图在一个可能不可靠的网络上传递非常大的文件。TCP提供一些错误检测和更正， 但它不能从严重的网络中断中恢复。因此，邮件传递服务增加了另一层次的容错，维护一个进展记录，如果原来的TCP连接断开了，用一个新的TCP连接继续传递。 Saltzer、Reed 和Clarke的一篇经典论文[Saltzer et al. 1984]对分布式系统的设计给出了类似的、有价值的观点，他们称之为“端到端争论”。可将他们的陈述表述为: 一些与通信相关的功能，可以只依靠通信系统终点(end point)的应用的知识和帮助，即可完整、可靠地实现。因此，将这些功能作为通信系统的特征不总是明智的(虽然由通信系统提供一个不完全版本的功能有时对性能提高是有用的)。可以看出他们的论点与通过引人适当的中间件层将所有通信活动从应用编程中抽象出来的观点是相反的。 争论的关键是分布式程序正确的行为在很多层面上依赖检查、错误校正机制和安全手段，其中有些要求访问应用的地址空间的数据。任何企图在通信系统中单独完成的检查将只能保证部分正确性。因此，可能在应用程序中重复同样的任务,降低了编程效率,更重要的是增加了不必要的复杂性并要执行冗余的计算。 这里不进一步介绍他们的争论细节,强烈推荐读者阅读前面提到的那篇论文一那里有许多说明的实例。原文作者之一最近指出:争论给互联网设计带来的实质性好处最近面临着为满足当前应用需求而转向网络服务专门化的危险[ www. reed. com]。 这个争论给中间件设计者带来一个实际的两难困境，而且给定当代分布式系统中种类繁多的应用(和相关的环境条件) (见第1章)，这些困难与日俱增。本质上，底层中间件行为与一个给定应用或应用集的需求和相关环境上下文(如底层网络的状态和风格)有关。这个看法推动了对上下文感知和中间件自适应解决方案的兴趣，见Kon等人的讨论[2002] 。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:3:3","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"基础模型 上面的各种系统模型完全不同，但具有一些基本特性。特别是，所有的模型都由若干进程组成,这些进程通过在计算机网络上发送消息而相互通信，所有的模型都共享下列设计需求:实现进程及网络的性能和可靠性特征，确保系统中资源的安全性。本节给出基于基本特性的模型，利用这些模型，我们能更详细地描述系统可能展示的特征、故障和安全风险。 通常，为了理解和推理系统行为的某些方面，一个基础模型应该仅包含我们要考虑的实质性成分。这样一个模型的目的是: 显式地表示有关我们正在建模的系统的假设。 给定这些假设，就什么是可能的、什么是不可能的给出结论。结论以通用算法或要确保的特性 的形式给出。特性成立的保证依赖于逻辑分析和(适当时候的)数学证明。 了解设计依赖什么、不依赖什么，我们就能从中获益。如果在一个特定系统中实现-个设计，这个设计能否运作，我们只需询向在那个系统中假设是否成立。通过清晰、显式地给出我们的假设，就能利用数学技巧证明系统的特征，这些特征对任何满足假设的系统都成立。最后，通过从细节(如硬件)中抽象系统的基本实体和特性，我们就能阐明对系统的理解。 我们希望在我们的基本模型中提取的分布式系统情况能解决下列问题: 交互:计算在进程中发生，进程通过传递消息交互，并引发进程之间的通信(信息流)和协调(活动的同步和排序)。在分布式系统的分析和设计中,我们特别关注这些交互。交互模型必须反映通信带来的延迟，这些延迟的持续时间会比较长，交互模型必须反映独立进程相互配合的准确性受限于这些延迟，受限于在分布式系统中很难跨所有计算机维护同一时间概念。 故障:只要分布式系统运行的任-计算机上出现故障(包括软件故障)或连接它们的网络出现故障，分布式系统的正确操作就会受到威胁。我们的模型将对这些故障进行定义和分类。这为分析它们潜在效果以及设计能容忍每种类型故障的系统奠定了基础。 安全:分布式系统的模块特性和开放性将其暴露在外部代理和内部代理的攻击下。我们的安全模型对发生这种攻击的形式给出了定义并进行了分类，为分析对系统的威胁以及设计能抵御这些威胁的系统奠定了基础。为了帮助讨论和推理，我们对本章介绍的模型进行了必要的简化，省略了许多真实系统中的细节。 它们与真实系统的关系，以及在模型帮助下揭示的问题环境中的解决方案是本书讨论的主题。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:4:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"交互模型 体系结构模型对系统体系结构的讨论表明分布式系统由多个以复杂方式进行交互的进程组成。例如: 多个服务器进程能相互协作提供服务，前面提到的例子有域名服务(它将数据分区并复制到互联网中的服务器上)和Sun的网络信息服务(它在局域网的几个服务器上保存口令文件的复制版本)。 对等进程能相互协作获得一个共同的目标。例如，一个语音会议系统，它以类似的方式分布音频数据流，但它有严格的实时限制。 大多数程序员非常熟悉算法的概念一采取一系列步骤以执行期望的计算。简单的程序由算法控制，算法中的每一步都有严格的顺序。由算法决定程序的行为和程序变量的状态。这样的程序作为一个进程执行。由多个上面所说的进程组成的分布式系统是很复杂的。它们的行为和状态能用分布式算法描述一分布式算法定 义了组成系统的每个进程所采取的步骤，包括它们之间消息的传递。消息在进程之间传递以便在它们之间传递信息并协调它们的活动。每个进程执行的速率和进程之间消息传递的时限通常是不能预测的。要描述分布式算法的所有状态也非常困难，因为它必须处理所涉及的一个或多个进程的故障或消息传递的故障。 进程交互完成了分布式系统中所有的活动。每个进程有它自已的状态，该状态由进程能访问和更新的数据集组成，包括程序中的变量。属于每个进程的状态完全是私有的一也就是说， 它不能被其他进程访向或更新。 本节讨论分布式系统中影响进程交互的两个重要因素: 通信性能经常是一个限制特性。 不可能维护-个全局时间概念。 通信通道的性能。在我们的模型中， 通信通道在分布式系统中可用许多方法实现，例如，通过计算机网络上的流或简单消息传递来实现。计算机网络上的通信有下列与延迟(lateney)、 带宽( band-widh)和抖动(itter) 有关的性能特征: 从一个进程开始发送消息到另–个进程开始接收消息之间的间隔时间称为延迟。延迟包括: 第一串比特通过网络传递到目的地所花费的时间。例如，通过卫星链接传递消息的延迟是无线电信号到达卫星并返回的时间。 访问网络的延迟，当网络负载很重时，延迟增长很快。例如，对以太网传送而言，发送站点要等待网络空闲。 操作系统通信服务在发送进程和接收进程上所花费的时间，这个时间会随操作系统当前的负载的变化而变化。 计算机网络的带寬是指在给定时间内网络能传递的信息总量。当大量通信通道使用同-一个网络时，它们就不得不共享可用的带宽。 抖动是传递一系列消息所花费的时间的变化值。抖动与多媒体数据有关。例如，如果音频数据 的连续采样在不同的时间间隔内播放，那么声音将严重失真。 计算机时钟和时序事件。分布式系统中的每台计算机有自己的内部时钟,本地进程用这个时钟获得当前时间值。因此，在不同计算机上运行的两个进程能将时间戳与它们的事件关联起来。但是，即使两个进程在同时读它们的时钟,它们各自的本地时钟也会提供不同的时间值。这是因为计算机时钟和绝对时间之间有偏移，更重要的是，它们的漂移率互不相同。术语时钟漂移率( clock drit rate)指的是计算机时钟偏离绝对参考时钟的比率。即使分布式系统中所有计算机的时钟在初始情况下都设置成相同的时间，它们的时钟最后也会相差巨大，除非进行校正。 有几种校正计算机时钟的时间的方法。例如，计算机可使用无线电接收器从全球定位系统( GPS)以大约1μs的精度接收时间读数。但GPS接收器不能在建筑物内工作，同时，为每一台计算机增加GPS在费用上也不合理。相反，具有精确时间源(如GPS)的计算机可发送时序消息给网络中的其他计算机。在两个本地时钟时间之间进行协商当然会受消息延迟的影响。有关时钟漂移和时钟同步的更详细的讨论见第14章。 交互模型的两个变体。在分布式系统中，很难对进程执行、消息传递或时钟漂移所花的时间设置时间限制。两种截然相反的观点提供了一对简单模型:第一个模型对时间有严格的假设，第二个模型对时间没有假设。 同步分布式系统: Hadzilacos 和Toueg [1994] 定义了一个同步分布式系统，它满足下列约束: 进程执行每一步的时间有一个上限和下限。 通过通道传递的每个消息在一个已知的时间范围内接收到。 每个进程有一个本地时钟，它与实际时间的偏移率在一个已知的范围内。 对于分布式系统，建议给出合适的关于进程执行时间、消息延迟和时钟漂移率的上界和下界是可能的。但是达到实际值并对所选值提供保证是比较困难的。除非能保证上界和下界的值，否则任何基于所选值的设计都不可靠。但是，按同步系统构造算法，可以对算法在实际分布式系统的行为提供一些想法。例如，在同步系统中，可以使用超时来检测进程的故障，参见下面的2.4.2节。 同步分布式系統是能够被构造出来的。所要求的是进程用已知的资源需求完成任务，这些资源需求保证有足够的处理器周期和网络能力;还有要为进程提供漂移率在一定范围内的时钟。 异步分布式系统:许多分布式系统，例如互联网，是非常有用的，但它们不具备同步系统的资格。 因此我们需要另一个模型。异步分布式系统是对下列因素没有限制的系统: 进程执行速度 例如， 进程的一步可能只花费亿万分之一秒，而进程的另一步要花费一个世纪的时间，也就是说，每一步能花费任意长的时间。 消息传递延迟 例如， 从进程A到进程B传递一个消息的时间可能快得可以忽略，也可能要花费几年时间。换句话说，消息可在任意长时间后接收到。 时钟漂移率 时钟漂移率可以是任意的。 异步模型对执行的时间间隔没有任何假设。这正好与互联网一致，在互联网中,服务器或网络负载没有内在的约束，对像用FTP传输文件要花费多长时间也没有限制。有时电子邮件消息要花几天时间才能到达。下面的“Pepperland协定”部分说明在异步分布式系统中达成协定的困难性。即使有这些假设，有些设计问题也能得到解决。例如，虽然Web并不总能在一个合理的时间限制内提供特定的响应，但浏览器的设计可以做到让用户在等待时做其他事情。对异步分布式系统有效的任何解决方案对同步系统同样有效。 实际的分布式系统经常是异步的，因为进程需要共享处理器，而通信通道需要共享网络。例如，如果有太多特性未知的进程共享一个处理器，那么任何一个进程的性能都不能保证。但是，有许多不能在异步系统中解决的设计问题,在使用时间的某些特征后就能解决。在最终期限之前传递多媒体数据流的每个元素就是这样-个问题。对这样的问题，可使用同步模型。 事件排序。在许多情况下， 我们有兴趣知道一个进程中的一个事件(发送或接收一个消息)是发生在另一个进程中的另一个事件之前、之后或同时。尽管缺乏精确的时钟，但系统的执行仍能用事件和它们的顺序来描述。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:4:1","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"故障模型 在分布式系统中，进程和通信通道都有可能出故障，即它们可能偏离被认为是正确或所期望的行为。故障模型定义了故障可能发生的方式，以便理解故障所产生的影响。Hadzilacos 和Toueg [ 1994]提供了一种分类法，用于区分进程故障和通信通道故障。这些故障将分别在下面的“ 遗漏故障”、“随机故障”和“时序故障”部分介绍。 本书将贯穿使用故障模型。例如: 第4章给出数据报和流通信的Java接口，它们分别提供不同程度的可靠性。 第5章给出支持RMI的请求-应答协议。它的故障特征取决于进程和通信通道两者的故障特征。该协议能用数据报或流通信实现。可根据实现的简单性、性能和可靠性作出决定。 第17章给出事务的两阶段的提交协议。它用于在面对进程和通信通道的确定性故障时完成事务。 遗漏故障。遗漏故障类错误指的是进程或通信通道不能完成它应该做的动作。 进程遗漏故障:进程主要的遗漏故障是崩溃。当我们说进程崩溃了，意为进程停止了，将不再执行程序的任何步骤。能在故障面前存活的服务,如果假设该服务所依赖的服务能干净利落地崩溃，即进程仍能正确运行或者停止运行,那么它的设计能被简化。其他进程通过下列事实能检测到这种进程崩溃:这个进程一再地不能对调用消息进行应答。然而，这种崩溃检测的方法依赖超时的使用，即进程用一段固定时间等待某个事件的发生。在异步系统中，超时只能表明进程没有响应一它 可能是崩溃了，也可能是执行速度慢，或者是消息还没有到达。如果其他进程能确切检测到进程已经崩溃，那么这个进程崩溃称为故障-停止。在同步系统中, 如果确保消息已被传递，而其他进程又没有响应时，进程使用超时来检测，那么就会产生故障–停止 行为。 通信遗漏故障:考虑通信原语send和re-ceive。进程p通过将消息m插人到它的外发消息缓冲区来执行send。通信通道将m传输到q的接收消息缓冲区。进程q通过将m从它的接收消息缓冲区取走并完成传递来执行receive ( 见图2-14)。通常由操作系统提供外发消息缓冲区和接收消息缓冲区。 随机故障术语随机故障或拜占庭故障用于描述可能出现的最坏的故障，此时可能发生任何类型的错误。 时序故障时序故障适用于同步分布式系统。在这样的系统中，对进程执行时间、消息传递时间和时钟漂移率均有限制。时序故障见图2-16的列表。这些故障中的任何一个均可导致在指定时间间隔内对客户没有响应。 故障屏蔽分布式系统中的每个组件通常是基于其他一组组件构造的。利用存在故障的组件构造可靠的服务是可能的。例如，保存有数据副本的多个服务器在其中一个服务器崩溃时能继续提供服务。 一对一通信的可靠性虽然基本的通信通道可能出现前面描述的遗漏故障，但用它来构造一个能屏蔽某些故障的通信服务是可能的。 术语可靠通信可从下列有效性和完整性的角度来定义: 有效性:外发消息缓冲区中的任何消息最终能传递到接收消息缓冲区。 完整性:接收到的消息与发送的消息一致，没有消息被传递两次。 对完整性的威胁来自两个方面: 任何重发消息但不拒绝到达两次的消息的协议。要检测消息是否到达了两次，可以在协议中给消息附加序号。 心怀恶意的用户，他们可能插人伪造的消息、重放旧的消息或篡改消息。在面对这种攻击时为维护完整性要采取相应的安全措施。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:4:2","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"安全模型 在第1章中，我们识别出资源共享是分布式系统的一个激发因素。 在2.3节中，我们用进程来描述分布式系统的体系结构，其中可能封装了如对象、组件或服务等的高层抽象，而且，我们通过与其他进程的交互来访问系统。那个体系结构模型为我们的安全模型提供了基础: 通过保证进程和用于进程交互的通道的安全以及保护所封装的对象免遭未授权访问可实现分布式系统的安全。 保护对象。用户运行客户程序，由客户程序向服务器发送调用以完成在对象上的操作。服务器完成每个调用指定的操作并将结果发给客户。 保护进程和它们的交互进程通过发送消息进行交互。消息易于受到攻击，因为它们所使用的网络和通信服务是开放的，以使得任一对进程可以进行交互。服务器和对等进程暴露它们的接口，使得任何其他进程能给它们发送调用。 敌人。为了给安全威胁建模，我们假定敌人(有时也称为对手)能给任何进程发送任何消息，并读取或复制一对进程之间的任何消息，如图2-18所示。这种攻击能很简单地实现，它利用连接在网上的计算机运行一个程序读取那些发送给网络上其他计算机的网络消息，或是运行一个程序生成假的服务请求消息并声称来自授权的用户。攻击可能来自合法连接到网络的计算机或以非授权方式连接到网络的计算机。来自一个潜在敌人的威胁包括对进程的威胁和对通信通道的威胁。 对进程的威胁:在分布式系统中,一个用于处理到达的请求的进程可以接收来自其他进程的消息，但它未必能确定发送方的身份。通信协议(如IP)确实在每个消息中包括了源计算机的地址，但对一个敌人而言，用一个假的源地址生成一个消息并不困难。缺乏消息源的可靠的知识对服务器和客户的正确工作而言是一个威胁，具体解释如下: 服务器:因为服务器能接收来自许多不同客户的调用，所以它未必能确定进行调用的主体的身份。即使服务器要求在每个调用中加入主体的身份，敌人也可能用假的身份生成一个调用。在没有关于发送方身份的可靠知识时，服务器不能断定应执行操作还是拒绝执行操作。例如，邮件服务器不知道从指定邮箱中请求一个邮件的用户是否有权限这样做，或者它是否为来自一个敌人的请求。 客户:当客户接收到服务器的调用结果时，它未必能区分结果消息来自预期的服务器还是来自一个“哄骗”邮件服务器的敌人。因此，客户可能接收到一个与原始调用无关的结果，如一个假的邮件(不在用户邮箱中的邮件)。 对通信通道的威胁:一个敌人在网络和网关上行进时能复制、改变或插人消息。当信息在网络上传递时，这种攻击会对信息的私密性和完整性构成威胁，对系统的完整性也会构成威胁。例如，包含用户邮件的结果消息可能泄露给另-一个用户或者可能被改变成完全不同的东西。另一种形式的攻击是试图保存消息的拷贝并在以后重放这个消息，这使得反复重用同一消息成为可能。例如，有些人通过重发请求从一个银行账户转账到另一个银行账户的调用消息而受益。利用安全通道可解除这些威胁，安全通道是基于密码学和认证的，详细内容见下面的描述。 解除安全威胁。下面将介绍安全系统所基于的主要技术。 第11章将详细讨论安全的分布式系统的设计和实现。 密码学和共享秘密:假设一对进程(例如某个客户和某个服务器)共享一个秘密，即它们两个知道秘密但分布式系统中的其他进程不知道这个秘密。如果由一对进程交换的消息包括证明发送方共享秘密的信息，那么接收方就能确认发送方是一对进程中的另一个进程。当然，必须小心以确保共享的秘密不泄露给敌人。 密码学是保证消息安全的科学,加密是将消息编码以隐藏其内容的过程。现代密码学基于使用密钥(很难猜测的大数)的加密算法来传输数据，这些数据只能用相应的解密密钥恢复。 认证:共享秘密和加密的使用为消息的认证(证明由发送方提供的身份)奠定了基础。基本的认证技术是在消息中包含加密部分，该部分中包含足够的消息内容以保证它的真实性。对文件服务器的一个读取部分文件的请求，其认证部分可能包括请求的主体身份的表示、文件的标识、请求的日期和时间，所有内容都用一个在文件服务器和请求的进程之间共享的密钥加密。服务器能解密这个请求并检查它是否与请求中指定的未加密细节相对应。 安全通道:加密和认证用于构造安全通道，安全通道作为已有的通信服务层之上的服务层。安全通道是连接一对进程的通信通道，每个进程代表一个主体行事，如图2- 19所示。- 一个安全通道有下列特性: 每个进程确切知道其他正在执行的进程所代表的主体身份。因此，如果客户和服务器通过安全通道通信，那么服务器要知道发起调用的主体身份，并能在执行操作之前检查它们的访向权限。这使得服务器能正确地保护它的对象，以便客户相信它是从真实的服务器上接收到的结果。 安全通道确保在其上传送的数据的私密性和完整性(防止篡改)。 每个消息包括一个物理的或逻辑的时间戳以防消息被重放或重排序。 其他可能的来自敌人的威胁。1.5.3 节简要介绍了两个安全威胁一拒绝服 务攻击和移动代码的部署。作为敌人破坏进程活动的可能的机会，我们要再介绍一下这两个安全威胁。 拒绝服务:在这种攻击形式下，敌人通过超量地、无意义地调用服务或在网络上进行消息传送，干扰授权用户的活动，导致物理资源( 网络带宽、服务器处理能力)的过载。这种攻击通常意在延迟或阻碍其他用户的动作。例如，建筑物中的电子门锁可能由于受到对计算机控制的电子锁的过多非法请求而失效。 移动代码:如果进程接收和执行来自其他地方的程序代码(如1. 5.3节提到的邮件附件)，那么这些移动代码就会带来新的、有趣的安全问题。这样的代码很容易扮演特洛伊木马的角色，声称完成的是无害的事情但事实上包括了访问或修改资源的代码，这些资源对宿主进程是合法可用的但对代码的编写者是不合法的。实现这种攻击有多种不同的方法，因此必须非常小心地构造宿主环境以避免攻击。其中的大多数问题已在Java和其他移动代码系统中解决了，但从最近的一段历史看，移动代码问题暴露了一些让人窘迫的弱点。这-点也很好地说明了所有安全系统的设计都需要严格的分析。 安全模型的使用。有人认为, 在分布式系统中获得安全是件简单的事，即根据预定义的访问权限控制对象的访问以及通信的安全通道的使用，但是通常却不是这样。安全技术(如加密)和访问控制的使用会产生实质性的处理和管理开销。前面概述的安全模型提供了分析和设计安全系统的基础，其中这些开销保持最少,但对分布式系统的威胁会在许多地方出现，需要对系统网络环境、物理环境和人际环境中所有可能引发的威胁进行仔细的分析。这种分析涉及构造威胁模型，由它列出系统会遭遇 的各种形式的攻击、风险评估和每个威胁所造成的后果。要在抵御威胁所需的安全技术的有效性和开销之间做出权衡。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:4:3","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"小结 如2.2节所展示的，从底层物理特性角度，例如，系统的规模、系统内在的异构性、从特性角度(如安全)提供端到端解决方案的实际需求等，分布式系统的复杂性正在增加。这使得从模型角度理解和探讨分布式系统显得更加重要。本章考虑了底层物理模型，并深度考察了支撑分布式系统的体系结构模型和基础模型。 本章从所包含的体系结构模型角度给出了描述分布式系统的方法，明晰了这个设计空间的内涵，包括查看什么在通信以及这些实体如何通信等核心问题，以及基于给定物理基础设施，考虑每个元素可以扮演的角色与合适的放置策略，并把它们补充到设计中去。 本章还介绍了体系结构模式在由底层核心元素(例如上述的客户-服务器模型)构造复杂设计中发挥的关键作用,给出了支持分布式系统的中间件解决方案的主要类型，包括基于分布式对象、组件、Web服务和分布式事件的解决方案。从体系结构模型角度看，客户-服务器方法是一种常见的体系结构模型一 Web 和其他互联网服务(如FIP、新闻和邮件以及Web服务和DNS)均基于这个模型，文件归档和其他本地服务也是如此。像DNS这种有大量的用户并管理大量信息的服务是基于多个服务器的，并使用数据分区和复制来提高可用性和容错能力。客户和代理服务器上的缓存得到广泛使用以提高服务的性能。不过，现在有许多方法对分布式系统进行建模，包括各种可替代的观点，如对等计算和更多的面向问题的抽象(如对象、组件或服务)。 基础棋型补充了体系结构模型，它们帮助从诸如性能、可靠性和安全角度对分布式系统的特性进行推理。特别地，我们给出了交互模型、故障模型和安全模型。它们识别出构造分布式系统的基本组件的共同特征。交互模型关注进程和通信通道的性能以及全局时钟的缺乏。它将同步系统看成在进程执行时间、消息传递时间和时钟漂移上有已知范围的系统，将异步系统看成在进程执行时间、消息传递时间和时钟漂移上没有限制的系统一这是对互联网行为的描述。 故障模型将分布式系统中的进程故障和基本的通信通道故障进行了分类。屏蔽是一项技术，依靠它，可将不太可靠的服务中的故障加以屏蔽，并基于此构造出较可靠的服务。特别是，通过屏蔽基本的通信通道的故障，可从基本的通信通道构造出可靠的通信服务。例如，遗漏故障可通过重传丢失的消息加以屏蔽。完整性是可靠通信的一个性质一它要求接收到的消息 与发送的消息一致，并且没有消息被发送两次。有效性是可靠通信的另一个性质一它要求发送消息缓冲区中的任何消息最终都能传递到接收消息缓冲区。 安全模型可识别出在一个开放的分布式系统中对进程和通信通道可能的威胁。有些威胁与完整性有关:恶意用户可能篡改消息或重放消息。其他的威胁则会损害私密性。另一个安全问题是发送消息所代表的主体(用户或服务器)的认证。安全通道使用密码技术来确保消息的完整性和私密性，并使得相互通信的主体可以进行验证。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:5:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"分布式系统的特征 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:0:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"简介 我们把分布式系统定义成一个其硬件或软件组件分布在联网的计算机上，组件之间通过传递消息进行通信和动作协调的系统。 我们定义的分布式系统具有如下的具体特征： 并发：在一个计算机网络中，执行并发程序是常见的行为。用户可以在各自的计算机上工作，在必要时共享诸如Web页面或文件之类的资源。系统处理共享资源的能力会随着网络资源（例如，计算机）的增加而提高。 缺乏全局时钟：在程序需要协作时，它们通过交换消息来协调它们的动作。密切的协作通常取决于对程序动作发生的时间的共识。但是，事实证明，网络上的计算机与时钟同步所达到的准确性是有限的，即没有一个正确时间的全局概念。原因也很简单，因为基于网络的通信天然存在时延，而这个时延可能会由于网络状态等条件而不断发生变化，这么一来，就会导致各项动作没办法按照原定的时间来工作，反而会出现一些不好的影响。 故障独立性：所有的计算机系统都可能会出现故障，一般由系统设计者负责为可能的故障设计结果。分布式系统可能会以新的方式出现故障。网络故障导致网上互联的计算机的隔离，但这并不意味着它们停止运行，事实上，计算机上的程序不能检测到网络是出现故障还是网络运行得比通常慢。类似的，计算机的故障或系统中程序得异常终止（崩溃），并不能让与它通信的其他组件了解。系统的每个组件会单独地出现故障，而其他组件还在运行。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:1:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"分布式系统的例子 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:2:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"web搜索 Web搜索引擎的任务是为万维网的所有内容建立索引，其中包含各种信息类型，例如Web页面、多媒体资源和扫描后的书。考虑到大多数搜索引擎是分析整个Web内容，并在这个巨大的数据库上完成复杂的处理，那么这个任务自身就是对分布式系统涉及的一个巨大挑战。 Google，Web搜索技术上的市场领导者，在支持用于搜索（与其他Google应用和服务，如Google Earth）的复杂的分布式系统基础设施上做出了巨大努力。该设施最突出的亮点包括： 一个底层物理设施：它由超大数目的位于全世界多个数据中心的连网计算机组成。 一个分布式文件系统：支持超大文件，并根据搜索和其他Google应用的使用方式（特别是在文件中以快速而持久的速度读取）进行了深度优化 一个相关的结构化分布式存储系统：它提供了对超大数据集的快速访问 一个锁服务：它提供了诸如分布式加锁和协定等分布式系统功能。 一个编程模式：它支持对底层物理基础设施上的超大并行和分布式计算的管理。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:2:1","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"大型多人在线游戏 大型多人在线游戏（Massively Multiplayer Online Game,MMOG）提供了一种身临其境的体验，超大数目用户通过互联网在一个持久的虚拟世界中交互。这类游戏的主要例子是Sony的EverQuest Ⅱ和芬兰公司CCP Games公司的EVE Online。 MMOG工程体现了分布式系统技术面临的巨大挑战，尤其是它对快速响应时间的需求。其他挑战包括事件实时传播给多个玩家和维护对共享世界的一个一致的视图。 针对大型多人在线游戏，提出了许多解决方案： 可能有点出乎意料，最大的在线游戏EVE Online，采用了CS体系结构，在一个集中式服务器上维护了游戏世界状态的单个拷贝，供运行在玩家终端或其他设备上的客户程序访问。为了支持大量客户，服务器自身是一个复杂的实体，拥有上百个计算机节点组成的集群结构。从虚拟世界的管理看，集中式体系结构有极大的益处，单个拷贝也简化了一致性问题。接着，目标是用过优化网络协议和快速响应到达事件来确保快速的响应。为了支持这点，对负载进行分区，把单个“星系”分配给集群中指定的计算机，这样，高负载星系会拥有自己的专用计算机，而其他星系则共享一台计算机。通过跟踪王家在i星系之间的移动，到达事件会被导向集群中正确的计算机上。 其他MMOG采用更多的分布式体系结构，宇宙被划分到大量（可能是超多）服务器上，这些服务器可能地理上分散部署。接着，用户基于当前的使用模式和到服务器的网络延迟（基于地理最近）被动态地分配到一个特定的服务器。这种体系结构风格被EverQuest采用，它通过增加新的服务器，可自然地扩展。 大多数商业系统采用上述两个模型中的一个，但研究者现在也在寻找更极端的体系结构，即不基于客户-服务器原理而是基于对等技术采用完全分散的方法。采用对等技术，意味着每个参与者贡献（存储和处理）资源来容纳游戏。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:2:2","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"金融交易 金融行业以其需求一直处在分布式系统技术的最前沿，特别是在实时访问大范围的信息源方面（例如，当前股票价格和趋势，经济和政治发展）。金融行业采用自动监控和交易应用。 此类系统的重点是对感兴趣数据项的通信和处理。感兴趣数据项在分布式系统中称为事件，在金融行业中的需求是可靠和及时地传递事件给可能是大量对此信息有兴趣地客户。这要求底层地体系结构具有与前述风格（例如CS）完全不同的风格，这样的系统通常采用分布式基于事件的系统。 下图说明了一个典型的金融交易的例子。它显示了一系列事件进入一个指定的金融机构。这样的事件输入具有下列特征。（图片来自分布式系统：概念与设计（原书第五版） by George Coulouris Jean Dollimore Tim Kindberg Gordon Blair (z-lib.org)） 首先，事件源通常具有多种格式，例如路透社的市场数据事件和FIX事件（符合金融信息交换协议特定格式的事件），事件源还来自不同的事件技术，这说明了在大多数分布式系统中回到异构性问题。图中使用了适配器，它把异构性格式转换成一个公共的内部格式。 其次交易系统必须处理各种各样的事件流，这些事件流高速到达，经常需要实时处理来检测表示交易机会的模式。这在过去曾今是手工处理的，但在竞争压力下变成自动处理，这就是所谓的复杂事件处理（Complex Event Processing，CEP），它提供了一种方法来将一起发生的事件组成逻辑的、时序的或空间的模式。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:2:3","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"分布式系统的趋势 分布式系统正在经历巨大的变化，这可追溯到一系列有影响力的趋势： 出现了泛在联网技术 出现了无处不在计算，它伴随着分布式系统中支持用户移动性的意愿 对多媒体设备的需求 把分布式系统作为一个设施 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"泛在联网和现代互联网 互联网上的计算机程序通过传递消息进行交互，采用了一种公共的通信手段。互联网通信机制（互联网协议）的设计和构造是一项重大的技术成果，它使得一个在某处运行的程序能给另一个地方的程序发送消息。 互联网是一个超大的分布式系统。互联网和其支持得服务的实现，使得必须开发实用解决方案来解决分布式系统中的许多问题。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:1","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"移动和无处不在计算 设备小型化和无线网络方面的技术进步已经逐渐使得小型和便携式计算设备集成到分布式系统中。这些设备包括： 笔记本电脑 手持设备（包括移动电话、智能电话、GPS设备、摄像机等） 可穿戴设备 嵌入式家电 这些设备大多具有可便携性，再加上它们可以在不同的地方方便地连接到网络的能力，使得移动计算成为可能。移动计算是指用户在移动或访问某个非常规环境时执行计算任务的性能。 无处不在计算是指对用户的的物理环境（包括家庭、办公室和其他自然环境）中存在的多个小型、便宜的计算设备的利用。 移动和无处不在计算是一个热门的研究领域。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:2","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"分布式多媒体系统 另一个重要趋势是在分布式系统中支持多媒体服务的需求。多媒体支持可以定义为以集成的方式支持多种媒体类型的能力。人们可以期望分布式多媒体系统支持离散型媒体（如图片或正文消息）的存储、传输和展示。分布式多媒体系统应该能对连续类型媒体（如音频和视频）完成相同的功能，即它应该能存储和定位音频或视频文件，并通过网络传输它们（可能需要以实时的方式，因为流来自摄像机），从而能给用户展示多种媒体类型，以及在一组用户中共享多种类型的媒体。 连续媒体的重要特点时它们包括一个时间维度，媒体类型的完整性从根本上依赖于在媒体类型的元素之间保持实时关系。 分布式多媒体计算的好处时相当大的，因为能在桌面环境提供大量的新（多媒体）服务和应用，包括访问实况或预先录下的电视广播、访问提供视屏点播服务的电影资料库、访问音乐资料库、提供音频和视频会议设施、提供集成的电话功能。 网络播放（webcasting） 是分布式多媒体技术的应用。网络播放是在互联网上广播连续媒体（典型是音频和视频）的能力，现在常见以这种方式广播主要的体育或音乐事件。 分布式多媒体应用（例如网络播放）对底层的分布式基础设施提出了大量的要求，包括： 提供对一系列（可扩展的）编码和加密格式的支持，例如MPEG系列标准（包括如流行的MP3标准，也称MPEG-1音频第三层）和HDTV 提供一系列机制来保障所需的服务质量能够得到满足 提供相关的资源管理策略，包括合适的调度策略，来支持所需的服务质量 提供适配策略类处理在开放系统中不可避免的场景，即服务质量不能得到满足或维持 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:3","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"把分布式计算作为一个公共设施 随着分布式下基础设施的不断成熟，不少公司在推广这样的观点：把分布式资源看作一个商品或公共设施，把分布式资源和其他公共设施进行类比。采用这种模型，资源通过合适的服务提供者提供，能被最终用户有效地租赁而不是拥有。这种模型可以应用到物理资源和更多的逻辑服务上。 联网的计算机可用诸如存储和处理这样的物理资源，从而无需自己拥有这样的资源。从一个维度来看，用户可以为其文件存储需求和文件备份需求选择一个远程存储设施。类似的，利用这个方法，用户能租到一个或多个计算结点，从而满足他们的基本计算需求或者完成分布式计算。从另一个维度来看，用户现在能用像Amazon和Google之类的公司提供的服务访问复杂的数据中心或计算基础设施。操作系统虚拟化时该方法关键的使能技术，它意味着实际上可以通过一个虚拟的而不是物理的结点为用户提供服务。这从资源管理角度给服务提供者提供了更大的灵活性。 用这种方法，软件服务也能跨全球互联网使用。 关于计算作为公共设施，术语云计算（cloud computi）被用来刻画其前景。云被定义成一组基于互联网的应用，并且足以满足大多数用户需求的存储和计算服务的集合，这使得用户能大部分或全部免于本地数据存储和应用软件的使用。该术语也推广“把每个事物看成一个服务”的观点。 通常，云实现在集群计算机上，从而提供每个服务所要求的必须的伸缩性和性能。**集群计算机（cluster computer）**是互联的计算机集合，它们密切协作提供单一的、集成的高性能计算能力。 集群服务器的总目的时提供一系列的云服务，包括高性能计算能力、大容量存储能力（例如通过数据中心）、丰富的应用服务（如Web搜索——Google依赖大容量集群计算机体系结构来实现其搜索引擎和其他服务） 网格计算也能被看作时一种云计算。但网格计算通常被看作时云计算这种更通用模式的先驱，它只是偏重于支持科学计算。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:4","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"关注资源共享 从硬件资源来看，大家共享设备可以减少花费，但对用户具用更大意义的是共享与用户应用、日常工作和社会活动有关的更高层的资源。例如用户惯性以共享数据库或Web页面集方式出现的共享数据，而不是实现上述服务的硬盘和处理器。类似的，用户关心诸如搜索引擎或货币换算器之类的共享资源，而不关心提供这些服务的服务器。 实际上，资源共享的模式随着其工作范围和与用户工作的密切程度的不同而不同。一种极端是，Web上的搜索引擎是给全世界的用户提供工具，而用户之间并不需要直接接触；另一种极端是，在计算机支持协调工作（Computer Supported Working。CSCW） 中，若干直接进行合作的用户在一个小型封闭的小组中共享诸如文档之类的资源。用户在地理上的分布以及用户之间进行共享的模式决定了系统必须提供协调用户动作的机制。 我们使用属于服务表示计算机系统中管理相关资源并提供功能给用户和应用的一个单独的部分。 服务将资源访问限制为一组定义良好的操作，这在某种程度上属于标准的软件工程实践。同时它也反映出分布式系统的物理组织。分布式相同的资源是物理地封装在计算机内，其他计算机只能通过通信访问。为了实现有效的共享，每个资源必须由一个程序管理，这个程序提供通信接口使得对资源进行可靠和一致的访问和更新。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:4:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"挑战 随着分布式系统的应用范围和规模扩大，可能会遇到相同的和其他的挑战。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"异构性 互联网使得用户能在大量异构计算机和网络上访问服务和运行应用程序。 下面这些均存在异构性（即存在多样性和差别）： 网络 计算机硬件 操作系统 编程语言 由不同开发者完成的软件实现 中间件：指一个软件层，它提供了一个编程抽象，同时屏蔽了底层网络、硬件、操作系统和编程语言的异构性。有些中间件。如Java远程方法调用（Remote Method Invocation,RMI）,仅支持一种编程语言。大多数中间件在互联网协议上实现，由这些协议屏蔽了底层网路的差异，但所有的中间件要解决操作系统和硬件的不同。 处理解决异构性问题之外，中间件为服务器和分布式应用的程序员提供了一致的计算模型。这些模型包括远程方法调用、远程时间通知、远程SQL访问和分布式事务处理。 异构性和移动代码中移动代码是指能从一台计算机发送到另一台计算机发送到另一台计算机，并在目的计算机上执行的代码，Java applet是一个例子。适合在一种计算机上运行的代码未必适合在另一种计算机上运行，因为可执行程序通常依赖于计算机的指令集和操作系统。 虚拟机方法提供了一种使代码可在任何计算机上运行的方法：某种语言的编译器生成一台虚拟机的代码而不是某种硬件代码，例如，Java编译器生成Java虚拟机的代码，虚拟机通过解释的方法来执行它。为了使Java程序嫩个运行，要在每种计算机上实现一次Java虚拟机。 今天，最常使用的移动代码是将一些Web页面的JavaScript程序装载到客户端浏览器中。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:1","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"开放性 计算机系统的开放性是决定系统能否以不同的方式被扩展和重新实现的特征。分布式系统的开放性主要取决于新的资源共享服务能被增加和供多种客户程序使用的程度。 除非软件开发者能获得系统组件的关键软件接口的规范和文档，否则无法实现开放性。一句话发布关键接口。这个过程类似接口的标准化，但它进程避开官方的标准化过程，官方的标准化过程非常繁琐且进度缓慢。 然而发布接口仅是分布式系统增加和扩展服务的起点。设计者所面临的挑战是解决由不同人构造的由许多组件组成的分布式系统的复杂性。 互联网协议的设计者引入了一系列称为“征求意见文档”（Requests For Comments，RFC）的文档，每个文档有一个编号。 按这种方式支持资源共享的系统之所以被称为开放的分布式系统，主要是强调它们是可扩展的。它们通过在网络中增加计算机实现在硬件层次上的扩展，通过引入新的服务、重新实现旧的服务实现在软件层次上的扩展，最终使得应用程序能够共享资源。开放系统常被提到的好处是它们与销售商无关。 开放的分布式系统的特征总结如下： 发布系统的关键接口是开放系统的特征 开放的分布式系统是基于一致的通信机制和发布接口访问共享资源的。 开放的分布式系统能用不同销售商提供的异构硬件和软件构造，但如果想让系统正确工作，就要仔细测试和验证每个组件与发布的标准之间的一致性。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:2","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"安全性 分布式系统中维护和使用的众多信息资源对用户具有很高的内在价值，因此它们的安全相当重要。信息资源的安全性包括三个部分：机密性（防止泄露给未授权的个人）、完整性（防止被改变或被破坏）、可用性（防止对访问资源的手段的干扰）。 安全性不止涉及对消息的内容保密，还涉及确切知道用户或代表用户发送消息的其他代理的身份。利用机密技术可满足这两个挑战。 然而，下面两个安全方面所面临的挑战目前还没有完美解决： 拒绝服务攻击：另一个安全问题是处于某些用户可能希望中断服务。可用下面的方法实现这个目的：用大量无意义的请求攻击服务器，使得重要的用户不能使用它。这称为拒绝服务攻击。现在通过在世间发生后抓获和惩罚犯罪者来解决这种攻击，但这不是解决这种问题的通用方法。 移动代码的安全性：移动代码需要小心处理。因为有些程序表面上可能一副有意思的画，但实际上却在访问本地资源，或者可能是拒绝服务攻击的一部分。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:3","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"可伸缩性 分布式系统可在不同的规模（从小型企业内部网到互联网）下有效且高效地运转。如果资源数量和用户数量激增，系统仍能保持其有效性，那么该系统就被称为可伸缩性。 可伸缩性分布式系统的设计面临下列挑战： 控制物理资源的开销 控制性能损耗 防止软件资源用尽：例如IP地址 避免性能瓶颈：通常，算法应该是分散型，以避免性能瓶颈。例如域名系统的前身，那时名字表被保存在一个主文件中，可被任何需要它的计算机下载。当互联网中只有几百个计算机时，这是可以的，但这不久就变成了一个严重的性能和管理瓶颈。现在，域名系统将名字表分区，分散到互联网中的服务器上，并采用本地管理的方式，从而解决了这个瓶颈。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:4","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"故障处理 计算机系统有时会出现故障。当硬件或软件发生故障时，程序可能会产生不正确的结果或者在它们完成应该进行的计算之前就停止了。 分布式系统的故障时部分的，也就是说，有些组件出了故障而有些组件运行正常。因此故障的处理相当困难。接下来我们讨论一下处理故障的技术： 检测故障：有些故障能被检测到。例如，校验和可用于检测消息或文件中出现的错误。而有些故障时很难甚至不能被检测到的。面临的挑战是如何在有故障出现的情况下进行管理，这些故障不能被检测到但可以被猜到。 掩盖故障：有些被检测到的故障能被隐藏起来或降低她的严重程度。下面是隐藏故障的两个例子 ： 1）消息在不能到达时重传。 2）将文件数据写入两个磁盘，如果一个磁盘损坏，那么另一个磁盘的数据仍是正确的。 降低故障严重程度的例子是丢掉被损坏的消息。这样，该消息可以被重传。读者可能意识到，隐藏故障的技术不能保证在最坏情况下有效。例如，第二个磁盘上的数据可能也坏了，或消息无论怎样重传都不能在合理的时间到达。 容错：互联网上的大多服务确实可能发生故障，试图检测并隐藏在这样大的网络、这么多的组件中发生的所有故障是不太实际的。服务的客户能被设计成容错的，这通常也涉及用户要容忍错误。例如，当Web浏览器不能与Web服务器连接时，它不会让用户一直等待它与服务器建立连接，而是通知用户这个问题，让用户自由选择是否尝试稍后再连接。 故障恢复：恢复涉及软件的设计，以便在服务器崩溃后，永久数据的状态能被恢复或“回滚”。通常再出现错误时，程序完成的计算是不完整的，被更新的永久数据（文件和其他保存在永久存储介质中的资料）可能处在不一致的状态。 冗余：利用冗余组件，服务可以实现容错。考虑下面的例子： 1）在互联网的任意两个路由器之间，至少存在两个不同的路由。 2）在域名系统中，每个名字表至少被复制到两个不同的服务器上。 3）数据库可以被复制到几个服务器上，以保证在任何一个服务器上有错误时，客户就被重定向到剩下的服务器上。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:5","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"并发性 在分布式系统中，服务和应用均提供可被客户共享的资源。因此，可能有几个客户同时试图访问一个共享资源的情况。 管理共享资源的进程可以一次接受一个客户请求，但这种方法限制了吞吐量。因此，服务和应用通常被允许并发地处理多个客户请求。 在分布式系统中，代表共享资源的任何一个对象必须负责确保它在并发环境中操作正确，这不仅适用于服务器，也适用于服务器，也适用于应用中的对象。因此，持有未打算用于分布式系统的对象实现的程序员必须做一些事情，使得对象在并发环境中能安全使用。 为了使对象在并发环境中能安全使用，它的操作必须在数据一致的基础上同步。者可通过标准的技术（如大多数操作系统所采用的信号量）来实现。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:6","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"透明性 透明性被定义成对用户和应用程序员屏蔽分布式系统的组件的分离性，使系统被认为是一个整体，而不是独立组件的集合。 ANSA参考手册和国际化标准化组织的开放分布式处理的参考模型（RM-ODP）识别出八种透明性（并用范围更广的移动透明性替换迁移透明性）： 访问透明性：用相同的操作访问本地资源和远程资源。 位置透明性：不需要知道资源的物理或网络位置就能访问它们 并发透明性：几个进程能并发地使用共享资源进行操作且互不干扰 复制透明性：使用资源的多个实例提升可靠性和性能，而用户和应用程序员无需知道副本的相关信息 故障透明性：屏蔽错误，不论是硬件组件故障还是软件组件故障，用户和应用程序员能够完成他们的任务 移动透明性：资源和客户能够在系统内移动而不会影响用户或程序的操作 性能透明性：当负载发生变化时，系统能被重新配置以提高性能 伸缩透明性：系统和应用能够进行扩展而不改变系统结构或应用算法 最重要的两个透明性是访问透明性和位置透明性，它们的有无对分布式资源的利用有很大影响，又是它们被统一称为网络透明性。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:7","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"服务质量 一旦提供给用户他们要求的功能，例如在一个分布式系统中的文件服务，我们就能继续探寻所提供的服务质量。系统的主要的非功能特性，即影响客户和用户体验的服务质量是可靠性、安全性和性能。满足变化的系统配置和资源可用性的适用性已被公认为服务质量的一个重要方面。 可靠性和安全性问题再设计大多是计算机系统时时关键的。服务质量的性能源于及时性和计算吞吐量，但它已被重新定义成满足及时性保证的能力。 一些应用，包括多媒体应用，处理时间关键性数据，这些数据是要求以固定速度处理或从一个进程传送到另一个进程的数据流。例如，一个电影服务可能由一个客户程序组成，该程序从一个视频服务器中检索电影并把它呈现到用户的屏幕上。该视频的连续帧在指定时间限制内显示给用户，才算是一个满意的结果。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:8","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"实例研究：万维网 在1989年3月，互联网还只属于少数人。在这一互联网的黎明期，HTTP诞生了。 蒂姆·伯纳斯-李博士提出了一种让远隔两地的研究者们共享知识的设想，这一最初设想的基本理念是：借助多文档之间相互关联形成的超文本（HyperText），连成可相互参阅的WWW（World Wide Web,万维网）。 超文本（Hypertext）：在互联网早期的时候，我们输入的信息只能保存在本地，无法和其他电脑交互。我们保存的信息通常都以文本即简单字符的形式存在，文本是一种能够被计算机解析的有意义的二进制数据包。而随着互联网的高速发展，两台电脑之间能够进行数据的传输后，人们不再满足只能在两台电脑之间传输文字，还想要传输图片、音频、视频，甚至是超链接，那么文本的语义就被扩大了，这种语义扩大后的文本就被称为超文本（HyperText）。 而为了实现这一理念，现在已经提出了3项WWW构建技术，分别是： 把SGML（Standard Generalized Markup Langrage，标准通用标记语言）作为页面的文本标记语言的HTML（HyperText Markup Language，超文本标记语言） 作为文档传输协议的HTTP（HyperText Transfer Protocol,超文本传输协议） 指定文档所在地址的URL（Uniform Resource Locator，统一资源定位符） WWW这一名称，是Web浏览器当年用来浏览超文本的客户端应用程序是的名称。现在则用来表示这一系列的集合，也可简称为Web。 除了这三样技术外，还有Javascript，提供比HTML标准化窗口部件质量更好的用户交互，用于更新Web页面的部分内容而不必取得该页面的全新版本并重新显示。AJAX处理异步情况等等技术。 Web之所以取得巨大的成功，是因为许多个人或机构能比较容易地发布资源，它的超文本结构适合组织多种类型的信息，而且Web体系结构具有开放性。Web体系结构所基于的标准很简单，且早被广泛发布。它使得许多新的资源类型和服务可以集成到一起。 Web成功的背后也存在一些设计问题，首先，它的超文本模型再某些方面有所欠缺。如果删除或移动了一个资源，那么就会存在对资源所谓“悬空”链接，会使用户请求落空。此外，还存在用户“在超空间迷失”这个常见的问题。用户经常发现自己处于混乱状态下，跟随许多无关的链接打开完全不同的页面，使得有些情况下可靠性值得怀疑。 在Web上查找信息的另一种方法是使用搜索引擎，但这种方法在满足用户真正需求方面是相当不完美的。要解决这个问题，资源描述框架[www.w3.org V]中介绍过,一种方法是生成标准的表达事务元数据的词汇、语法和语义，并将元数据封装在相应的Web资源中供程序访问。除了查找Web页面中出现的词组外，从原理上讲，程序可以完成对针对元数据的搜索，然后，根据语义匹配编译相关的链接列表。总而言之，由互连的元数据资源组成的Web就是语义Web。 作为一个系统体系结构，Web面临规模的问题。常见的Web服务器会在一秒中有很多点击量，结果导致对用户的应答变慢。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:6:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"小结 分布式系统无处不在。 资源共享是构造分布式系统的主要因素。 分布式系统的构造面临许多挑战： 异构性 开放性 安全性 可伸缩性 故障处理 透明性 服务质量 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:7:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":null,"content":"关于我 某 c9 19级 本科学生 ","date":"2021-11-06 00:00:00","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"菜鸡的编程历程 大一接触编程，学校开设了C语言。 大一至大二为了获得创新学分，我同小组5人用JAVA开发了一个未完全成形的拍照记账APP，当时的话JAVA水平也只能算是入门级别的，没有深入去学习，我当时负责的功能需求是实现记账数据的可视化，我用 LitePal 操作数据库，并利用一个开源的图表库 MPAndroidChart，实现了期望的数据图表化。忙活了几个月，做出了下面的效果（实际截图） 无论是配安卓开发环境还是解决写程序过程中AS产生的“莫名奇妙”的bug，都十分令人头大，不过我还是比较能接受吧，毕竟当你解决掉一个“不简单”的问题的时候，成就感还是挺足的。 大二，学校有软件设计的课程，我同一位同学组队进行游戏开发，当时的设想是用cocos2dx游戏引擎做一款类似Hex War的回合制策略游戏，原游戏截图： 基本功能完成（包括：开拓视野，行军,征兵,调粮，各类资源消耗，交战等）后部分截图： 我只写了几百行代码，更多的代码是队友写的，我c++水平也只能算是入门，没有深入学习细节。游戏素材是我用ps扣出来的，说来也辛酸，费了不少时间最后效果也就“差强人意”。 大三上，这个学期我接触到了 go 语言，经过JAVA安卓开发和c++游戏开发的双重折磨后，我立马就被 go 的简单高效的特性所吸引，从而开始学习这门语言，11月份的时候学校的数据库大作业需要自己设计数据库并实现前后端，当时有几个选题，我选了实验室软件管理系统，用PowerDesigner设计数据库结构，用Workbench进行数据库的操作与管理，用HTML写前端，用go写后端。贴一下当时的图： ER图： 登陆界面： 其他界面示例： 坦白说，当时我是把实现功能作为第一位的，基本没有对安全性和性能的考虑……而且代码结构也不够清晰。业务逻辑就是一堆千把行代码的屎山，贴一下目录结构（一直尘封着未重构）： D:. │ go.mod │ go.sum │ main.exe │ main.go │ ReadMe.md │ ├─controller │ controller.go │ ├─models │ todo.go │ ├─mysql_ │ mysql_.go │ ├─routers │ routersGin.go │ ├─static │ ├─css │ │ bootstrap.min.css │ │ materialdesignicons.min.css │ │ style.min.css │ │ │ ├─fonts │ │ fontawesome-webfont.eot │ │ fontawesome-webfont.ttf │ │ fontawesome-webfont.woff │ │ fontawesome-webfont.woff2 │ │ glyphicons-halflings-regular.eot │ │ glyphicons-halflings-regular.ttf │ │ glyphicons-halflings-regular.woff │ │ glyphicons-halflings-regular.woff2 │ │ materialdesignicons.eot │ │ materialdesignicons.svg │ │ materialdesignicons.ttf │ │ materialdesignicons.woff │ │ materialdesignicons.woff2 │ │ │ ├─images │ │ favicon.ico │ │ hit.jpg │ │ logo-ico.png │ │ logo2.png │ │ │ ├─js │ │ bootstrap.min.js │ │ jquery.min.js │ │ perfect-scrollbar.min.js │ │ │ └─pic │ title.png │ │ └─templates login_2.html teacher_admin.html teacher_college.html teacher_computer.html teacher_course.html teacher_labor.html teacher_software.html teacher_teacher.html user_admin.html user_college.html user_computer.html user_course.html user_labor.html user_software.html user_teacher.html 大三下5月份，参加了ByteDance的后端青训营，课程比较硬核，但那个月课程繁忙，而且要进行毕设导师的双选，所以落下不少青训营课程没听，青训营项目截止日期是6月中旬左右，刚好是期末考试周，因为考虑到时间紧凑，所以一开始强行组队的时候，我就不打算当组长来着，找个组协作开发。虽然最后的结果不太理想，但还是有结果。 六个人开发一个抖音后端项目，不同于以往和认识的同学协作开发，这次和陌生人组队的体验不太行（具体就不细说了），虽然说大家可能技术水平参差不齐，但对于这种入门级企业活动项目来说水平是次要的，只要肯学还是问题不大的。不过这次团队协作让我意识到：对于一个团队来说，态度、责任心是最基本的，团队交流是最重要的，团队负责人是团队最核心的，三者都做不到是万万不能的。青训营官方强迫不能个人组队的良苦用心有部分人是没有意识到的。 7月份泡在极客时间里，一两个礼拜的免费会员，我看了不少课程，然后自己也买了一些课程。包括计算机基础的，golang语言相关的，微服务相关的……不得不说，真香！唯一的不足就是极客时间的网页客户端体验感不好，网页的延迟高而且网站有时还会挂…… 回顾自己的这些项目好像都没什么亮点，然后我就有了用微服务框架重构一下抖音后端项目的想法。我先调研了一下，有很多优秀的 RPC 协议，例如腾讯的 Tars、阿里的 Dubbo、微博的 Motan、字节的 Kitex、Facebook 的 Thrift、RPCX 等等，然而我觉得对于我这个初学者来说，还是使用更普及的 gRPC 比较合适。 可能是因为自己急于求成了，没有好好消化知识，现在都忘了一大半了，不过好在做了笔记。 ","date":"2021-11-06 00:00:00","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"github 统计 ","date":"2021-11-06 00:00:00","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"TODO 继续学习计算机基础、go、微服务等 ","date":"2021-11-06 00:00:00","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":" https://zhuanlan.zhihu.com/p/32052223 Raft Raft 算法比Zookeeper的ZAB协议要简单一些，Raft算法是分布式一致性算法，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等，用于管理多副本状态机的日志复制。 Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选人（Candidate）： Leader：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。 Follower：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。 Candidate：Leader选举过程中的临时角色。 ","date":"0001-01-01 00:00:00","objectID":"/raft_algo/:0:0","tags":null,"title":"","uri":"/raft_algo/"},{"categories":null,"content":"title: “Go_chenhao” date: 2022-09-30T07:39:09+08:00 lastmod: tags: [] categories: [] slug: draft: true ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:1:0","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"08 | Go语言，Docker和新技术 陈皓 2017-10-26 我想写这篇文章，并从两个方面来论述一下我的观点和看法。一个方面，为什么 Go 语言和 Docker 会是新一代的云计算技术。另一个方面，作为技术人员，我们如何识别什么样的新技术会是未来的趋势。 我对 Go 语言有如下几点体会。 第一，语言简单，上手快。Go 语言的语法特性简直是太简单了，简单到你几乎玩不出什么花招，直来直去的，学习难度很低，容易上手。 第二，并行和异步编程几乎无痛点。Go 语言的 Goroutine 和 Channel 这两个神器简直就是并发和异步编程的巨大福音。像 C、C++、Java、Python 和 JavaScript 这些语言的并发和异步的编程方式控制起来就比较复杂了，并且容易出错，但 Go 语言却用非常优雅和流畅的方式解决了这个问题。这对于编程多年受尽并发和异步折磨的我来说，完全就是眼前一亮的感觉。 第三，Go 语言的 lib 库“麻雀虽小，五脏俱全”。Go 语言的 lib 库中基本上有绝大多数常用的库，虽然有些库还不是很好，但我觉得这都不是主要问题，因为随着技术的发展和成熟，这些问题肯定也都会随之解决。 第四，C 语言的理念和 Python 的姿态。C 语言的理念是信任程序员，保持语言的小巧，不屏蔽底层且对底层友好，关注语言的执行效率和性能。而 Python 的姿态是用尽量少的代码完成尽量多的事。于是我能够感觉到，Go 语言是想要把 C 和 Python 统一起来，这是多棒的一件事。 即便 Go 语言存在诸多的问题，比如垃圾回收、异常处理、泛型编程等，但相较于上面这几个优势，我认为这些问题都是些小问题。（那都是几年前的事了） 一个技术能不能发展起来，关键还要看三点。 有没有一个比较好的社区。像 C、C++、Java、Python 和 JavaScript 的生态圈都是非常丰富和火爆的。尤其是有很多商业机构参与的社区那就更是人气爆棚了，比如 Linux 社区。 有没有一个工业化的标准。像 C、C++、Java 这些编程语言都是有标准化组织的。尤其是 Java，它在架构上还搞出了像 J2EE 这样的企业级标准。 有没有一个或多个杀手级应用。C、C++ 和 Java 的杀手级应用不用多说了，就算是对于 PHP 这样还不能算是一个优秀的编程语言来说，因为是 Linux 时代的第一个杀手级解决方案 LAMP 中的关键技术，所以，也发展起来了。 Go 语言容易上手；Go 语言解决了并发编程和底层应用开发效率的痛点；Go 语言有 Google 这个世界一流的技术公司在后面；Go 语言的杀手级应用是 Docker 容器，而容器的生态圈这几年可谓是发展繁荣，也是热点领域。 Go 语言所吞食的项目应该主要是中间层的项目，既不是非常底层也不会是业务层。 Go 语言不会吞食底层到 C 和 C++ 那个级别的，也不会吞食到上层如 Java 业务层的项目。Go 语言能吞食的一定是 PaaS 上的项目，比如一些消息缓存中间件、服务发现、服务代理、控制系统、Agent、日志收集等等，他们没有复杂的业务场景，也到不了特别底层（如操作系统）的软件项目或工具。而 C 和 C++ 会被打到更底层，Java 会被打到更上层的业务层。这是我的一个判断。 Docker 容易上手。Docker 解决了运维中的环境问题以及服务调度的痛点。Docker 的生态圈中有大公司在后面助力，比如 Google。Docker 产出了工业界标准 OCI。Docker 的社区和生态圈已经出现像 Java 和 Linux 那样的态势。…… Kubernetes 作为服务和容器调度的关键技术一定会是最后的赢家。 关于 Docker 我还想多说几句，这是云计算中 PaaS 的关键技术。虽然，这世上在出现 Docker 之前，几乎所有的要玩公有 PaaS 的公司和产品都玩不起来，比如：Google 的 GAE，国内的各种 XAE，如淘宝的 TAE，新浪的 SAE 等。但我还是想说，PaaS 是一个被世界或是被产业界严重低估的平台。PaaS 层是承上启下的关键技术，任何一个不重视 PaaS 的公司，其技术架构都不可能让这家公司成长为一个大型的公司。因为 PaaS 层的技术主要能解决下面这些问题。 软件生产线的问题。持续集成和持续发布，以及 DevOps 中的技术必须通过 PaaS。 分布式服务化的问题。分布式服务化的服务高可用、服务编排、服务调度、服务发现、服务路由，以及分布式服务化的支撑技术完全是 PaaS 的菜。 提高服务的可用性 SLA。提高服务可用性 SLA 所需要的分布式、高可用的技术架构和运维工具，也是 PaaS 层提供的。 软件能力的复用。软件工程中的核心就是软件能力的复用，这一点也完美地体现在 PaaS 平台的技术上。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:2:0","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"27 | 洞悉PaaS平台的本质 陈皓，杨爽 2018-01-02 请先允许我来谈谈软件工程的本质。我认为，一家商业公司的软件工程能力主要体现在三个地方。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:3:0","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"第一，提高服务的 SLA。 所谓服务的 SLA，也就是我们能提供多少个 9 的系统可用性，而每提高一个 9 的可用性都是对整个系统架构的重新洗礼。在我看来，提高系统的 SLA 主要表现在两个方面：高可用的系统；自动化的运维。 你可以看一下我在 CoolShell 上写的《关于高可用系统》这篇文章，它主要讲了构建高可用的系统需要使用的分布式系统设计思路。然而这还不够，我们还需要一个高度自动化的运维和管理系统，因为故障是常态，如果没有自动化的故障恢复，就很难提高服务的 SLA。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:3:1","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"第二，能力和资源重用或复用。 软件工程还有一个重要的能力就是让能力和资源可以重用。其主要表现在如下两个方面：软件模块的重用；软件运行环境和资源的重用。 为此，需要我们有两个重要的能力：一个是“软件抽象的能力”，另一个是“软件标准化的能力”。你可以认为软件抽象就是找出通用的软件模块或服务，软件标准化就是使用统一的软件通讯协议、统一的开发和运维管理方法……这样能让整体软件开发运维的能力和资源得到最大程度的复用，从而增加效率。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:3:2","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"第三，过程的自动化。 编程本来就是把一个重复工作自动化的过程，所以，软件工程的第三个本质就是把软件生产和运维的过程自动化起来。也就是下面这两个方面： 软件生产流水线；软件运维自动化。 为此，我们除了需要 CI/CD 的 DevOps 式的自动化之外，也需要能够对正在运行的生产环境中的软件进行自动化运维。通过了解软件工程的这三个本质，你会发现，我们上面所说的那些分布式的技术点是高度一致的，也就是下面这三个方面的能力。（是的，世界就是这样的。当参透了本质之后，你会发现世界是大同的。） 分布式多层的系统架构。服务化的能力供应。自动化的运维能力。 只有做到了这些，我们才能够真正拥有云计算的威力。这就是所谓的 Cloud Native。而这些目标都完美地体现在 PaaS 平台上。前面讲述的分布式系统关键技术和软件工程的本质，都可以在 PaaS 平台上得到完全体现。所以，需要一个 PaaS 平台把那么多的东西给串联起来。这里，我结合自己的认知给你讲一下 PaaS 相关的东西，并把前面讲过的所有东西做一个总结。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:3:3","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"PaaS 平台的本质 一个好的 PaaS 平台应该具有分布式、服务化、自动化部署、高可用、敏捷以及分层开放的特征，并可与 IaaS 实现良好的联动。 下面这三件事是 PaaS 跟传统中间件最大的差别。 服务化是 PaaS 的本质。软件模块重用，服务治理，对外提供能力是 PaaS 的本质。 分布式是 PaaS 的根本特性。多租户隔离、高可用、服务编排是 PaaS 的基本特性。 自动化是 PaaS 的灵魂。自动化部署安装运维，自动化伸缩调度是 PaaS 的关键。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:3:4","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"PaaS 平台的总体架构 从下面的图中可以看到，我用了 Docker+Kubernetes 层来做了一个“技术缓冲层”。也就是说，如果没有 Docker 和 Kubernetes，构建 PaaS 将会复杂很多。当然，如果你正在开发一个类似 PaaS 的平台，那么你会发现自己开发出来的东西会跟 Docker 和 Kubernetes 非常像。相信我，最终你还是会放弃自己的轮子而采用 Docker+Kubernetes 的。 在 Docker+Kubernetes 层之上，我们看到了两个相关的 PaaS 层。一个是 PaaS 调度层，很多人将其称为 iPaaS；另一个是 PaaS 能力层，通常被称为 aPaaS。没有 PaaS 调度层，PaaS 能力层很难被管理和运维，而没有 PaaS 能力层，PaaS 就失去了提供实际能力的业务价值。而本文更多的是在讲 PaaS 调度层上的东西。 在两个相关的 PaaS 层之上，有一个流量调度的接入模块，这也是 PaaS 中非常关键的东西。流控、路由、降级、灰度、聚合、串联等等都在这里，包括最新的 AWS Lambda Service 的小函数等也可以放在这里。这个模块应该是像 CDN 那样来部署的。 然后，在这个图的两边分别是与运营和运维相关的。运营这边主要是管理一些软件资源方面的东西（类似 Docker Hub 和 CMDB），以及外部接入和开放平台上的东西，这主要是对外提供能力的相关组件；而运维这边主要是对内的相关东西，主要就是 DevOps。 总结一下，一个完整的 PaaS 平台会包括以下几部分。PaaS 调度层 – 主要是 PaaS 的自动化和分布式对于高可用高性能的管理。PaaS 能力服务层 – 主要是 PaaS 真正提供给用户的服务和能力。PaaS 的流量调度 – 主要是与流量调度相关的东西，包括对高并发的管理。PaaS 的运营管理 – 软件资源库、软件接入、认证和开放平台门户。PaaS 的运维管理 – 主要是 DevOps 相关的东西。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:3:5","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"PaaS 平台的生产和运维 下面的图我给出了一个大概的软件生产、运维和服务接入的流程，它把之前的东西都串起来了。 从左上开始软件构建，进入软件资产库（Docker Registry+ 一些软件的定义），然后走 DevOps 的流程，通过整体架构控制器进入生产环境，生产环境通过控制器操作 Docker+Kubernetes 集群进行软件部署和生产变更。其中，同步服务的运行状态，并通过生命周期管理来拟合状态，如图右侧部分所示。服务运行时的数据会进入到相关应用监控，应用监控中的一些监控事件会同步到生命周期管理中，再由生命周期管理器来做出决定，通过控制器来调度服务运行。当应用监控中心发现流量变化，要进行强制性伸缩时，它通过生命周期管理来通知控制系统进行伸缩。左下是服务接入的相关组件，主要是网关服务，以及 API 聚合编排和流程处理。这对应于之前说过的流量调度和 API Gateway 的相关功能。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:3:6","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"107 | Go编程模式：切片、接口、时间和性能 陈皓 2021-01-14 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:4:0","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"深度比较 （如果是不同的类型，即使是底层类型相同，相应的值也相同，那么两者也不是“深度”相等。） 当我们复制一个对象时，这个对象可以是内建数据类型、数组、结构体、Map……在复制结构体的时候，如果我们需要比较两个结构体中的数据是否相同，就要使用深度比较，而不只是简单地做浅度比较。这里需要使用到反射 reflect.DeepEqual() ，下面是几个示例： import ( \"fmt\" \"reflect\" ) func main() { v1 := data{} v2 := data{} fmt.Println(\"v1 == v2:\",reflect.DeepEqual(v1,v2)) //prints: v1 == v2: true m1 := map[string]string{\"one\": \"a\",\"two\": \"b\"} m2 := map[string]string{\"two\": \"b\", \"one\": \"a\"} fmt.Println(\"m1 == m2:\",reflect.DeepEqual(m1, m2)) //prints: m1 == m2: true s1 := []int{1, 2, 3} s2 := []int{1, 2, 3} fmt.Println(\"s1 == s2:\",reflect.DeepEqual(s1, s2)) //prints: s1 == s2: true } 类型 深度相等情形 Array 相同索引处的元素“深度”相等 Struct 相应字段，包含导出和不导出，“深度”相等 Func 只有两者都是 nil 时 Interface 两者存储的具体值“深度”相等 Map 1、都为 nil；2、非空、长度相等，指向同一个 map 实体对象，或者相应的 key 指向的 value “深度”相等 Pointer 1、使用 == 比较的结果相等；2、指向的实体“深度”相等 Slice 1、都为 nil；2、非空、长度相等，首元素指向同一个底层数组的相同元素，即 \u0026x[0] == \u0026y[0] 或者 相同索引处的元素“深度”相等 numbers, bools, strings, and channels 使用 == 比较的结果为真 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:4:1","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"接口编程 面向对象编程方法的黄金法则——“Program to an interface not an implementation” ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:4:2","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"接口完整性检查 package main import \"fmt\" type A interface { AA() BB() } type B struct { l int } func (b *B) AA() { fmt.Print(\"1\") } func (b *B) BB() { fmt.Print(\"2\") } var _ A = (*B)(nil) func main() { fmt.Println(\"Hello world!\") } ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:4:3","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"时间 在 Go 语言中，你一定要使用 time.Time 和 time.Duration 这两个类型。 在命令行上，flag 通过 time.ParseDuration 支持了 time.Duration。 JSON 中的 encoding/json 中也可以把time.Time 编码成 RFC 3339 的格式。 数据库使用的 database/sql 也支持把 DATATIME 或 TIMESTAMP 类型转成 time.Time。 YAML 也可以使用 gopkg.in/yaml.v2 支持 time.Time 、time.Duration 和 RFC 3339 格式。 如果你要和第三方交互，实在没有办法，也请使用 RFC 3339 的格式。最后，如果你要做全球化跨时区的应用，一定要把所有服务器和时间全部使用 UTC 时间。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:4:4","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"性能小提示 如果需要把数字转换成字符串，使用 strconv.Itoa() 比 fmt.Sprintf() 要快一倍左右。 尽可能避免把String转成[]Byte ，这个转换会导致性能下降。 如果在 for-loop 里对某个 Slice 使用 append()，请先把 Slice 的容量扩充到位，这样可以避免内存重新分配以及系统自动按 2 的 N 次方幂进行扩展但又用不到的情况，从而避免浪费内存。 使用StringBuffer 或是StringBuild 来拼接字符串，性能会比使用 + 或 +=高三到四个数量级。 尽可能使用并发的 goroutine，然后使用 sync.WaitGroup 来同步分片操作。 避免在热代码中进行内存分配，这样会导致 gc 很忙。尽可能使用 sync.Pool 来重用对象。 使用 lock-free 的操作，避免使用 mutex，尽可能使用 sync/Atomic包（关于无锁编程的相关话题，可参看《无锁队列实现》或《无锁 Hashmap 实现》）。 使用 I/O 缓冲，I/O 是个非常非常慢的操作，使用 bufio.NewWrite() 和 bufio.NewReader() 可以带来更高的性能。 对于在 for-loop 里的固定的正则表达式，一定要使用 regexp.Compile() 编译正则表达式。性能会提升两个数量级。 如果你需要更高性能的协议，就要考虑使用 protobuf 或 msgp 而不是 JSON，因为 JSON 的序列化和反序列化里使用了反射。 你在使用 Map 的时候，使用整型的 key 会比字符串的要快，因为整型比较比字符串比较要快。 ","date":"0001-01-01 00:00:00","objectID":"/go_chenhao/:4:5","tags":null,"title":"","uri":"/go_chenhao/"},{"categories":null,"content":"title: “dytt” date: 2022-01-12T22:17:18+08:00 lastmod: 2022-01-06 tags: [dytt] categories: [fytt] slug: Go_Data_Operation draft: true dytt ","date":"0001-01-01 00:00:00","objectID":"/dytt/:1:0","tags":null,"title":"","uri":"/dytt/"},{"categories":null,"content":"介绍一下项目的架构 DYTT 是我做的一个基于 Gin 和 gRPC 开发的抖音后端项目，包括 API 网关以及一些业务服务，比如 用户 服务实现了用户登陆注册获取用户主页信息的接口。一个简单的流程就是，API 服务接受 RESTful 请求，路由交给相应的 handler 处理，handler 经过参数验证后又传给 RPC 客户端，然后 RPC 客户端通过 rpc 框架发送请求数据给某个服务，并接受响应数据，然后便继续将数据向上传，最后响应 json 序列化的数据。 采用 (HTTP API 层 + RPC Service 层+Dal 层) 项目结构； 使用 x.509 证书对服务间通信进行加密和认证； 首先安装证书签发工具，创建 CA 配置文件，用来配置根证书的使用场景和具体参数。然后创建证书签名请求文件 CSR 配置文件。然后通过 CFSSL工具创建 CA 证书和私钥，创建的同时会生成 csr 文件，也就是证书签名请求。用于交叉签名或者重新签名。创建完根证书以及私钥之后，创建业务服务的 csr 配置文件，最后利用根证书和私钥创建各服务的 CA 证书和私钥。 使用 go-grpc-middleware中的日志记录、认证、和恢复； 拦截器的话客户端和服务端都可以设置，分为流式拦截器和一元拦截器，根据请求调用的类型决定，请求如果是流式的用到的就是流式拦截器。grpc本身的话一种类型的拦截器只能设置一个，不用这个grpc中间件的话只能将所有逻辑耦合在一起，代码结构就不清晰了。具体的话，自己实现一下go-grpc-middleware每个拦截器相应函数类型然后调用就行了。 日志记录其实就是自己实现一个可以返回zap.Logger实例的 grpc-middleware ZapInterceptor 的拦截器函数， 认证的话用的一个bearer token验证，就是客户端调用时添加token然后在服务端对以authorization为头部，形式为bearer token的Token进行验证，使用bearer token是规范了与HTTP请求的对接，毕竟gRPC也可以同时支持HTTP请求。然后恢复的话把gRPC中的panic转成error，从而恢复程序。 使用 JWT 进行用户token的校验； 简单地实现了两个方法，创建 token 解析token 使用 ETCD 进行服务发现和服务注册； etcd采用的是raft算法， 使用 Minio 实现视频文件和图片的对象存储； 使用 Gorm 对 MySQL 进行 ORM 操作； 使用 Zipkin 实现链路跟踪； 数据库表建立了索引和外键约束，对于具有关联性的操作一旦出错立刻回滚，保证数据一致性和安全性； HTTP 服务和 RPC 服务的优雅停止。 ","date":"0001-01-01 00:00:00","objectID":"/dytt/:2:0","tags":null,"title":"","uri":"/dytt/"},{"categories":null,"content":"遇到了哪些困难，怎么解决的？ 采用 (HTTP API 层 + RPC Service 层+Dal 层) 项目结构； 使用了 x509 证书对服务间通信进行加密和认证； 使用了 go-grpc-middleware 拦截器做日志记录、认证、和恢复； 使用了 JWT 进行用户token的校验； 使用 ETCD 进行服务发现和服务注册； 使用 Gorm 对 MySQL 进行 ORM 操作，使用 Minio 实现视频文件和图片的对象存储； 使用 Zipkin 实现链路跟踪； 数据库表建立了索引和外键约束，对于具有关联性的操作一旦出错立刻回滚，保证数据一致性和安全性； ","date":"0001-01-01 00:00:00","objectID":"/dytt/:3:0","tags":null,"title":"","uri":"/dytt/"},{"categories":null,"content":"项目开发 ","date":"0001-01-01 00:00:00","objectID":"/dytt/:4:0","tags":null,"title":"","uri":"/dytt/"},{"categories":null,"content":"项目测试 ","date":"0001-01-01 00:00:00","objectID":"/dytt/:5:0","tags":null,"title":"","uri":"/dytt/"},{"categories":null,"content":"项目部署 ","date":"0001-01-01 00:00:00","objectID":"/dytt/:6:0","tags":null,"title":"","uri":"/dytt/"},{"categories":null,"content":"【Golang开发面经】字节跳动（三轮技术面） 写在前面 整体面试下来，感觉其实字节对于语言本身并没有很多的涉及，更加注重基础，比如数据结构与算法，计算机网络，组成原理，操作系统，数据库等等，语言本身并没有涉及太多。 这里就省去了一些我简历上的问题，也就是深挖项目。 笔试 略 一面 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:0:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"epoll、select、poll 区别 select 机制刚开始的时候，需要把 fd_set 从用户空间拷贝到内核空间，并且检测的 fd 数是有限制的，由 FD_SETSIZE 设置，一般是1024。数组实现。 poll 的实现和 select 非常相似，只是描述 fd集合 的方式不同，poll使用 pollfd结构 而不是 select的 fd_set 结构，其他的都差不多。链表实现。 epol l引入了 epoll_ctl系统调用，将高频调用的 epoll_wait 和低频的 epoll_ctl 隔离开。epoll_ctl 通过(EPOLL_CTL_ADD、EPOLL_CTL_MOD、EPOLL_CTL_DEL)三个操作来分散对需要监控的fds集合的修改，做到了有变化才变更，将select或poll高频、大块内存拷贝(集中处理)变成epoll_ctl的低频、小块内存的拷贝(分散处理)，避免了大量的内存拷贝。 epoll使用 红黑树 来组织监控的fds集合 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:1:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"epoll 的水平触发和边缘触发的区别 Edge Triggered (ET) 边沿触发： socket 的接收缓冲区状态变化时触发读事件，即空的接收缓冲区刚接收到数据时触发读事件。 socket 的发送缓冲区状态变化时触发写事件，即满的缓冲区刚空出空间时触发读事件。 仅在缓冲区状态变化时触发事件。 Level Triggered (LT) 水平触发： socket 接收缓冲区不为空，有数据可读，则读事件一直触发。 socket 发送缓冲区不满可以继续写入数据，则写事件一直触发。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:2:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"TCP 的流量控制 因为我们总希望数据传输的更快一些。但如果发送方把数据发得过快，接收方就可能来不及接收，这就会造成数据的丢失。流量控制（flow control）就是让发送方的发送速率不要太快，要让接收方来得及接收。 利用滑动窗口机制可以很方便地在 TCP连接上实现发送方流量控制。通过接收方的确认报文中的窗口字段，发送方能够准确地控制发送字节数。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:3:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"为什么有了流量控制还要有拥塞控制? 流量控制是避免发送方的数据填满接收方的缓存，但并不知道网络中发生了什么。 计算机网络是处在一个共享的环境中。因此也有可能会发生网络的拥堵。在网络出现拥堵时，如果继续发送大量的数据包，可能会导致数据包时延、丢失，这时 TCP 就会重传数据，但是⼀重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包。 控制的目的就是避免发送方的数据填满整个网络。为了在发送方调节所要发送数据的数据量，定义了⼀个叫做「拥塞窗口」的概念。拥塞窗口 cwnd 是发送方维护的⼀个的状态变量，它会根据网络的拥塞程度动态变化的。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:4:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"TCP 不是可靠传输吗？为什么会丢包呢？ TCP的可靠传输是会在丢包的时候进行重传，来形成可靠的传输，丢包这是网络的问题，而不是TCP机制的问题。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:5:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"那你介绍一下拥塞控制的算法？ 拥塞控制一共有四个算法： 慢启动：TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是⼀点⼀点的提高发送数据包的数量。慢启动的算法规则：当发送方每收到⼀个 ACK，拥塞窗⼝ cwnd 的大小就会加 1。 拥塞避免：当拥塞窗口 cwnd「超过」慢启动门限 ssthresh 就会进⼊拥塞避免算法。那么进⼊拥塞避免算法后，它的规则是：每当收到⼀个 ACK 时，cwnd 增加 1/cwnd。 快重传：当接收方发现丢了⼀个中间包的时候，发送三次前⼀个包的ACK，于是发送端就会快速地重传，不必等待超时再重传。 快恢复：快重传和快恢复⼀般同时s使用，快速恢复算法是认为，你还能收到 3 个重复的 ACK 说明网络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。进⼊快速恢复之前， cwnd 和 ssthresh 已被更新了：cwnd = cwnd/2 ，也就是设置为原来的⼀半; ssthresh = cwnd 。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:6:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"进程、线程的区别 进程 线程 系统中正在运行的一个应用程序 系统分配处理器时间资源的基本单元 程序一旦运行就是进程 进程之内独立执行的一个单元执行流 资源分配的最小单位 程序执行的最小单位 进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。 线程有自己的堆栈和局部变量，但线程没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些 要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:7:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"Go里面GMP模型是怎么样的？ G：表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P：表示逻辑processor，P 的数量决定了系统内最大可并行的 G 的数量（前提：系统的物理cpu核数 \u003e= P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M：M 代表着真正的执行计算资源，物理 Processor。 G 如果想运行起来必须依赖 P，因为 P 是它的逻辑处理单元，但是 P 要想真正的运行，他也需要与 M 绑定，这样才能真正的运行起来，P 和 M 的这种关系就相当于 Linux 系统中的用户层面的线程和内核的线程是一样的。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:8:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"算法：旋转矩阵，牛客上写过，easy，秒 二面 上来就一个算法，结束又一个算法，难受。。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:9:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"如何用栈实现队列 我们可以通过切片来模拟出队列。 只需要完成简单的 push，pop，再判断是否为空即可！ package test_queue import ( \"fmt\" \"testing\" ) //结构体，包含两个一维切片 type GoQueue struct { stack []int back []int } //初始化， func NewQueue() GoQueue { return GoQueue{ stack: make([]int, 0), back: make([]int, 0), } } func (q *GoQueue) Push(x int) { q.stack = append(q.stack, x) } func (q *GoQueue) Pop() int { if len(q.back) == 0 { for len(q.stack) != 0 { val := q.stack[len(q.stack)-1] q.stack = q.stack[:len(q.stack)-1] //切片，更新栈 q.back = append(q.back, val) } } val := q.back[len(q.back)-1] q.back = q.back[:len(q.back)-1] return val } func (q *GoQueue) Empty() bool { return len(q.back) == 0 \u0026\u0026 len(q.stack) == 0 } func TestQueue(t *testing.T) { q := NewQueue() q.Push(1) q.Push(2) q.Push(3) for !q.Empty() { fmt.Println(q.Pop()) } } ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:10:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"如何判断一个链表有没有环？ 用hash来判断是否存在相同的值，也就是环。 快慢指针：快指针走两步，慢指针走一步，最终会相遇。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:11:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"那为什么快慢指针一定能够相遇？ 因为当快指针出现在慢指针后面之后，每一次快指针往前走两步、慢指针往前走一步，相当于快指针和慢指针之间的相对距离减少1步。当快指针刚刚绕到慢指针后面时，快指针离慢指针有 n 步。那么，对于接下来的每一次快指针往前走两步、慢指针往前走一步，快指针和慢指针之间的距离由 n 步变成 n-1 步、由 n-1 步变成 n-2 步、……、由 3 步变成 2 步、由 2 步变成 1 步、由 1 步变成 0 步。最终相遇。 快慢指针第一次相遇的时候，慢指针少走了多少？ 我们设从起点到 X 点的距离是 x，因此当在第一次相遇的时候，在n点，慢指针的路程是 x+n ，而因为快指针的速度是慢指针的一倍，它们从头开始，运动时间就是完全一致的，因此当二者发生相遇的时候，有如下几点值得注意： 当二者发生相遇时，慢指针一定已经进入了环中； 快指针一定先于慢指针进入环中； 快指针移动过的路程累计是慢指针的1倍，也就是2 * ( x + n )，其中早在路程为x时，快指针已经进入了环，它在环中运动了 x+2n 的路程。 在 n 点时，慢指针的路程是 x+n，快指针的路程是2*(x+n)，说明此时快指针比慢指针多走了 x+n，这同时也说明，从 n 处开始，继续往后推移 x+n 个距离，可以再次回到Z点，因此 x+n 的距离至少是环巡距离的一倍。慢指针和快指针在环中第一次相遇时的路程差值，是环路程的倍数。 所以慢指针少走了环巡距离的一倍。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:12:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"你用的是 mysql 是吧，那 B树 和 B+树 的区别是？ B 树 B+树 B+ 树内节点不存储数据，所有 data 存储在叶节点导致查询时间复杂度固定为log (n)。而 B-树 查询时间复杂度不固定，与 key 在树中的位置有关，最好为 O(1)。 B+ 树叶节点两两相连可大大增加区间访问性，可使用在范围查询等，而 B- 树 每个节点 key 和 data 在一起，则无法区间查找。同时我们访问某数据后，可以将相近的数据预读进内存，这是利用了空间局部性。 B+ 树更适合外部存储。由于内节点无 data 域，每个节点能索引的范围更大更精确。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:13:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"介绍一下死锁产生的必要条件 互斥条件，请求和保持条件，不剥夺条件，环路等待条件。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:14:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"如何实现互斥锁？ 一个互斥锁需要有阻塞和唤醒功能，所以需要一下几个情况。 需要有一个标记锁状态的 state 变量。 需要记录哪个线程持有了锁。 需要有一个队列维护所有的线程。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:15:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"如何实现自旋锁？ 当时真不会，只磕磕绊绊说了一些自旋锁的东西。。 大家可以看这篇博客 自旋锁C++实现方式 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:16:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"算法：三数之和。秒 三面 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:17:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"kafka 和 其他消息队列，比如 rocketmq，rabbitmq ，有什么优势？ kafka 具备非常高可靠的分布式架构，功能简单，只支持一些主要的MQ功能，像一些消息查询，消息回溯等功能没有提供。时效是在ms级别以内的。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:18:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"kafka如何保证消息不丢失？ 那我们可以从两个方面也就是生产者，消费者以及 broker ，来说说 生产者：生产者(Producer) 调用 send 方法发送消息之后，消息可能因为网络问题并没有发送过去。Kafka提供了同步发送消息方法，会返回一个Future对象，调用get()方法进行阻塞等待，就可以知道消息是否发送成功。如果消息发送失败的话，可以通过 Producer 的 retries（重试次数）参数进行设置，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议还要设置重试间隔，因为间隔太小的话重试的效果就不明显。 消费者：消息在被追加到 Partition(分区) 的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。 Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。 可以通过enable.auto.commit设置为false，关闭自动提交 offset ，每次在真正消费完消息之后之后再自己手动提交 offset 。 broker：当消息发送到了分区的leader副本，leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。解决办法就是 将producer设置 acks = all 。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:19:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"https为什么是安全的？ 因为 https 是基于 ssl 和 tls 加密而成的，https 的 s 就代表 ssl 和 tls。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:20:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"ssl/tls 是怎么保证安全的？经过几次握手？ 经过四次握手，客户端向服务器端索要并验证公钥，双方协商生成\"对话密钥\"，双方采用\"对话密钥\"进行加密通信。 四次握手主要是交换以下信息： 数字证书：该证书包含了公钥等信息，一般是由服务器发给客户端，接收方通过验证这个证书是不是由信赖的CA签发，或者与本地的证书相对比，来判断证书是否可信；假如需要双向验证，则服务器和客户端都需要发送数字证书给对方验证； 三个随机数：这三个随机数构成了后续通信过程中用来对数据进行对称加密解密的“对话密钥”。 首先客户端先发第一个随机数N1，然后服务器回了第二个随机数N2（这个过程同时把之前提到的证书发给客户端），这两个随机数都是明文的；而第三个随机数N3，客户端用数字证书的公钥进行非对称加密，发给服务器； 而服务器用只有自己知道的私钥来解密，获取第三个随机数。 这样，服务端和客户端都有了三个随机数N1+N2+N3，然后两端就使用这三个随机数来生成“对话密钥”，在此之后的通信都是使用这个“对话密钥”来进行对称加密解密。 因为这个过程中，服务端的私钥只用来解密第三个随机数，从来没有在网络中传输过，这样的话，只要私钥没有被泄露，那么数据就是安全的。 加密通信协议：就是双方商量使用哪一种加密方式，假如两者支持的加密方式不匹配，则无法进行通信； ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:21:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"事务的四大特性？ 原子性、隔离性、一致性、持久性 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:22:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"用过哪些排序？ 快排，堆排，归并比较常用。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:23:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"快排一定最快吗？ 不一定，当待排序的序列已经有序，不管是升序还是降序。此时快速排序最慢，一般当数据量很大的时候，用快速排序比较好，为了避免原来的序列有序。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:24:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"场景题：如果我有100G文件，但是只有 500 M 的内存，这些文件存着一行行的数字，如何获取最小的10个？ 这是经典的 Top K 问题，分治+堆排，我们可以先进行一个将文件进行分治，将100G的文件的分成一个个的 500 M，以此放到内存中每个500M的文件取出进行最小堆的排序，取出前10个写到新文件中。再对着一个个的新文件进行合并成 500 M 再进行最小堆的排序，最终获取最小的10个数字。 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:25:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"算法：最长有序括号，常见题，秒 ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:26:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"参考链接 [1] https://blog.csdn.net/weixin_35973945/article/details/124245495 [2] https://www.cnblogs.com/yaochunhui/p/14128777.html ","date":"0001-01-01 00:00:00","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:27:0","tags":null,"title":"","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":null,"content":"【Golang开发面经】B站（两轮技术面） 写在前面 面试下来我感觉我都讲出来了，算法题也写出来了，但是二面完一查结果就直接淘汰了。这个B是不是不招人啊。。 笔试 略 一面 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:0:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"Go的GMP模型 G：表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P：表示逻辑processor，P 的数量决定了系统内最大可并行的 G 的数量（前提：系统的物理cpu核数 \u003e= P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M：M 代表着真正的执行计算资源，物理 Processor。 G 如果想运行起来必须依赖 P，因为 P 是它的逻辑处理单元，但是 P 要想真正的运行，他也需要与 M 绑定，这样才能真正的运行起来，P 和 M 的这种关系就相当于 Linux 系统中的用户层面的线程和内核的线程是一样的。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:1:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"GO的GC Go是采用三色标记法来进行垃圾回收的，是传统 Mark-Sweep 的一个改进，它是一个并发的 GC 算法。on-the-fly 原理如下 整个进程空间里申请每个对象占据的内存可以视为一个图， 初始状态下每个内存对象都是白色标记。 先stop the world，将扫描任务作为多个并发的goroutine立即入队给调度器，进而被CPU处理，第一轮先扫描所有可达的内存对象，标记为灰色放入队列 第二轮可以恢复start the world，将第一步队列中的对象引用的对象置为灰色加入队列，一个对象引用的所有对象都置灰并加入队列后，这个对象才能置为黑色并从队列之中取出。循环往复，最后队列为空时，整个图剩下的白色内存空间即不可到达的对象，即没有被引用的对象； 第三轮再次stop the world，将第二轮过程中新增对象申请的内存进行标记（灰色），这里使用了writebarrier（写屏障）去记录这些内存的身份； 这个算法可以实现 on-the-fly，也就是在程序执行的同时进行收集，并不需要暂停整个程序。 简化步骤如下： 1、首先创建三个集合：白、灰、黑。 2、将所有对象放入白色集合中。 3、然后从根节点开始遍历所有对象（注意这里并不递归遍历），把遍历到的对象从白色集合放入灰色集合。 因为root set 指向了A、F，所以从根结点开始遍历的是A、F，所以是把A、F放到灰色集合中。 4、之后遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合 我们可以发现这个A指向了B，C，D所以也就是把BCD放到灰色中，把A放到黑色中，而F没有指任何的对象，所以直接放到黑色中。 5、重复 4 直到灰色中无任何对象 因为D指向了A所以D也放到了黑色中，而B和C能放到黑色集合中的道理和F一样，已经没有了可指向的对象了。 6、通过write-barrier检测对象有无变化，重复以上操作 由于这个EGH并没有和RootSet有直接或是间接的关系，所以就会被清除。 7、收集所有白色对象（垃圾） 所以我们可以看出这里的情况，只要是和root set根集合直接相关的对象或是间接相关的对象都不会被清楚。只有不相关的才会被回收。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:2:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"Go的map底层是怎么实现的？ map 的底层是一个结构体 // Go map 的底层结构体表示 type hmap struct { count int // map中键值对的个数，使用len()可以获取 flags uint8 B uint8 // 哈希桶的数量的log2，比如有8个桶，那么B=3 noverflow uint16 // 溢出桶的数量 hash0 uint32 // 哈希种子 buckets unsafe.Pointer // 指向哈希桶数组的指针，数量为 2^B oldbuckets unsafe.Pointer // 扩容时指向旧桶的指针，当扩容时不为nil nevacuate uintptr extra *mapextra // 可选字段 } const ( bucketCntBits = 3 bucketCnt = 1 \u003c\u003c bucketCntBits // 桶数量 1 \u003c\u003c 3 = 8 ) // Go map 的一个哈希桶，一个桶最多存放8个键值对 type bmap struct { // tophash存放了哈希值的最高字节 tophash [bucketCnt]uint8 // 在这里有几个其它的字段没有显示出来，因为k-v的数量类型是不确定的，编译的时候才会确定 // keys: 是一个数组，大小为bucketCnt=8，存放Key // elems: 是一个数组，大小为bucketCnt=8，存放Value // 你可能会想到为什么不用空接口，空接口可以保存任意类型。但是空接口底层也是个结构体，中间隔了一层。因此在这里没有使用空接口。 // 注意：之所以将所有key存放在一个数组，将value存放在一个数组，而不是键值对的形式，是为了消除例如map[int64]所需的填充整数8（内存对齐） // overflow: 是一个指针，指向溢出桶，当该桶不够用时，就会使用溢出桶 } 当向 map 中存储一个 kv 时，通过 k 的 hash 值与 buckets 长度取余，定位到 key 在哪一个bucket中，hash 值的高8位存储在 bucket 的 tophash[i] 中，用来快速判断 key是否存在。当一个 bucket 满时，通过 overflow 指针链接到下一个 bucket。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:3:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"遍历map是有序的吗？为什么？ 不是有序的，使用 range 多次遍历 map 时输出的 key 和 value 的顺序可能不同，map在遍历时，并不是从固定的0号bucket开始遍历的，每次遍历，都会从一个随机值序号的bucket，再从其中随机的 cell 开始遍历。map 遍历时，是按序遍历 bucket，同时按需遍历 bucket 和其 overflow bucket 中 的 cell。 但是 map 在扩容后，会发生 key 的搬迁，这造成原来落在一个 bucket 中的 key，搬迁后，有可能会落到其他 bucket 中了，从这个角度看，遍历 map 的结果就不可能是按照原来的顺序了。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:4:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"map作为函数是什么传递? map 传的是地址值 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:5:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"在函数里面修改map会影响原来的吗？ 会的，因为传递的map的地址，会对原来的map进行修改。 func TestMap(t *testing.T) { a := make(map[int]int) a[0] = 1 fmt.Println(a) changeMap(a) fmt.Println(a) } func changeMap(b map[int]int) { b[0] = 2 } 结果如下 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:6:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"那数组呢？ 数组不会，数组不是引用类型，传的值传递，并不是应用传递。 func TestArray(t *testing.T) { a := [3]int{1, 2, 3} fmt.Println(a) changeArray(a) fmt.Println(a) } func changeArray(a [3]int) { a[0] = 1 } ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:7:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"切片呢？ 会，和 map 一样，都是引用类型。传递的是地址。 func TestSlice(t *testing.T) { a := make([]int, 3, 3) a[0] = 1 a[1] = 2 a[2] = 3 fmt.Println(a) changeSlice(a) fmt.Println(a) } func changeSlice(a []int) { a[0] = 4 } ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:8:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"linux有用过是吧？如何查看一个服务是否在运行？ 如果我们知道服务名，我们可以使用 ps 命令： ps -ef | grep 服务名 或 ps aux |grep 服务名 如果知道端口号，我们可以使用 lsof 命令：lsof -i:端口号 或者也可以用这个：systemctl status 服务名 或 service 服务名 status 如果是基于 tcp 的连接，还可以使用 netstat 查看端口和进程等相关内容。netstat -tnlp\r ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:9:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"我只知道这个文件名，能找到这个文件在哪里吗？ 可以使用 find 命令 find / -name \"main.go\" ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:10:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"用过mysql是吧？mysql 索引说一下？ mysql 的索引包括** 基于InnoDB的聚集索引、基于MyISAM的非聚集索引、primary key 主键索引、secondary key 次要索引** 基于InnoDB的聚集索引：主键索引(聚集索引)的叶子结点会存储数据行，也就是说数据和索引在一起，辅助索引只会存储主键值 基于MyISAM的非聚集索引：B+树叶子结点只会存储数据行（数据文件）的指针，简单来说就是数据和索引不在一起，非聚集索引包含 主键索引 和 辅助索引 到会存储指针的值。 primary key 主键索引：InnoDB要求表必须有主键(MyISAM可以没有)，如果没有，MySQL系统会自动选择一个唯一标识数据记录的列作为主键。 secondary key 次要索引：结构和主键搜索引没有任何区别，同样用 B+Tree，data域存储相应记录主键的值而不是地址。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:11:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"死锁是怎么产生的 产生死锁就有四个必要条件：互斥条件、请求和保持条件、不剥夺条件、环路等待条件。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:12:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"了解分布式锁吗？讲讲红锁？ 只磕磕绊绊讲了一些分布式锁，红锁就不太记得了（ 可以看看这篇博客 基于Go语言的分布式红锁 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:13:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"算法：反转链表。这写不出来的话，就说不过去了。 二面 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:14:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"TCP和UDP区别？ TCP 是可靠传输，面向连接，基于流，占用资源多，效率低。 UDP是尽最大努力交付，基于无连接，基于报文，UDP 占用系统资源较少，效率高。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:15:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"TCP是可靠传输，为什么还有丢包的情况？ 丢包是网络问题，TCP的可靠是可靠在如果发生丢包，那么会立即重传报文段。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:16:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"UDP能实现可靠传输吗？怎么实现？ 可以的，我们只需要仿照TCP的可靠传输机制就可以了，比如说设置ACK确认机制，一旦没有收到，或是收到三次上一个报文的ACK，我们就立即重传丢失的报文。再比如说设置滑动窗口来保证数据传输的安全性等等… ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:17:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"TCP和IP的区别是什么？ TCP 是传输控制协议（Transmission Control Protocal），是基于IP的传输层协议，是传输层的，IP 是因特网协议（Internet Protocol）在网络层的。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:18:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"四次挥手的细节？ 数据传输完毕之后，通信的双方都可释放连接。现在A和B都处于ESTABLISHED状态。 A的应用进程先向TCP发出连接释放报文段，并停止再发送数据，主动关闭TCP连接。A把链接释放报文段首部的终止控制位FIN置为1，其序号为seq=u，它等于前面以传送过的数据的最后一个字节的序号加1.这时候A进入了FIN-WAIT-1(终止等待1)状态，等待B的确认。 注意：TCP规定，FIN报文段即使不携带数据，他也消耗掉一个序号！！ B 收到链接释放报文段后即发出确认，确认号是ack = u + 1，而这个报文段自己的序号是v，等于B前面已传送过的数据的最后一个字节的序号加1.然后B就进入CLOSE-WAIT(关闭等待)状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的链接就释放了，这时的TCP链接处于半关闭状态，即A已经没有数据要发送了，但B若发送数据，A仍要接收，也就是说，从B到A这个方向的连接并未关闭。这个状态可能要维持一段时间。 A收到来自B的确认后，就进入了FIN-WAIT-2(终止等待2)状态满等待B发出的连接释放报文段。若B已经没有要向A发送的数据，其应用进程就通知TCP释放连接，这时B发出的连接释放报文段必须使FIN = 1，现假定B的序号为w(在半关闭状态B可能又发送了一些数据)。B还必须重复上次已发送过的确认号ack = u + 1.这时B就进入LAST-ACK(最后确认)状态，等待A的确认。 A在收到了B的链接释放报文段后，必须对此发出确认。在确认报文段中把ACK置1，确认号ack=w+1，而自己的序号是seq=u+1(根据TCP标准，前面发送过的FIN报文段要消耗一个序号)。然后进入到TIME-WAIT(时间等待)状态。注意： 现在TCP连接还没有还没有释放掉。必须经过时间等待计时器设置的时间2MSL后，A才能进入CLOSED状态。 时间MSL叫做最长报文段寿命，RFC793建议设在两分钟。但是在现在工程来看两分钟太长了，所以TCP允许不同的实现可以根据具体情况使用更小的MSL值。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:19:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"为什么 time_wait 是2MLS? 为了保证A发送的最后一个ACK报文段能够到达B。 这个ACK报文段有可能丢失，因而使处于在 LAST-ASK 状态的B收不到对己发送的 FIN-ACK 报文段的确认。B会超时重传这个FIN+ACK报文段，而A就能在 2MSL 时间内收到这个重传的FIN+ACK报文段。而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。接着A重传一次确认，重新启动2MSL计时器。最后的A和B都正常进入CLOSED状态。如果A在TIME-WAIT状态不等待一段时间，而实发送完ACK报文段后立即释放连接，那么就无法收到B重传的FIN+ACK报文段，因而也不会再发送一次确认报文段。 防止了“已失效的连接请求报文段”。 A在发送完最后一个ACK报文段后，在经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，这样就可以使下一个连接中不会出现这种旧的连接请求报文段。B只要收到了A发出的确认，就进入CLOSED状态。同样，B在撤销相应的传输控制块TCB后，就结束了这次的TCP连接。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:20:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"大量处于 close wait 的是什么场景？ 如何解决？ 通常出现大量的CLOSE_WAIT，说明Server端没有发起close()操作，这基本上是用户server 端程序的问题了； 通常情况下，Server都是等待Client访问，如果Client退出请求关闭连接，server端自觉close()对应的连接。 一般是程序 Bug，或者关闭 socket 不及时。服务端接口耗时较长，客户端主动断开了连接，此时，服务端就会出现 close_wait。 这个我们就只能检查自己代码了，用netstat或是其他工具，检测代码为啥耗时长。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:21:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"cookie和session有什么区别？ cookie和session的共同之处在于：cookie和session都是用来跟踪浏览器用户身份的会话方式。cookie数据保存在客户端，session数据保存在服务器端。 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:22:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"你是用go的是吧？chan用过吧？那说说对一个关闭的chan做读写会发生什么操作？为什么？ 读已经关闭的 chan 能一直读到东西，但是读到的内容根据通道内关闭前是否有元素而不同。如果有元素，就继续读剩下的元素，如果没有就是这个chan类型的零值，比如整型是 int，字符串是 \"\"。 写已经关闭的 chan 会 panic。因为源码上面就是这样写的，可以看src/runtime/chan.go ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:23:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"map 的底层说一下？ 看上面 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:24:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"如果你这个项目，我突然有一个时间段，多了很多流量，要怎么处理? 我们要延长一些 token，cookie的设置时间，或是设置这些过期时间不一样，防止缓存雪崩的情况。 设置布隆过滤器，防止缓冲击穿情况。 使用 nginx 进行 http/https 的流量分发。使用轮询，随机，哈希，一致性哈希等等进行负载均衡等等… 提高服务自身性能，比如sql的索引，语法层面的调参等等… 引入CDN进行加速。 提高服务器配置。 … ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:25:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"算法：连续子序列的最大和 参考资料 [1] https://blog.csdn.net/Peerless__/article/details/125458742 ","date":"0001-01-01 00:00:00","objectID":"/b.-b%E7%AB%99/:26:0","tags":null,"title":"","uri":"/b.-b%E7%AB%99/"},{"categories":null,"content":"【Golang开发面经】百度（三轮技术面） 写在前面 百度一顿面试下来感觉挺不错的，面试官水平很高，不愧是互联网的黄埔军校，技术都很硬。可能是我项目讲的不好吧，最终挂了。 笔试 略 一面 一直深挖项目，挖了快半小时。然后再写两道题，最后再问一些简单的问题。 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:0:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"算法：判断是否为镜面二叉树 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:1:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"算法：二叉树的俯视图 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:2:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"一个协程被网络io卡住了，对应的线程会不会卡住？ 不会。因为都用epoll那是非阻塞调用，网络io和系统调用不一样的处理方式。网络io 是利用非阻塞，系统调用会创建新的线程来接管其他 goroutine。 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:3:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"go 里面 make 和 new 有什么区别？ make 一般用来创建引用类型 slice、map 以及 channel 等等，并且是非零值的。而new 用于类型的内存分配，并且内存置为零。make 返回的是引用类型本身；而 new 返回的是指向类型的指针。 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:4:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"map 是怎么实现的？ map 的底层是一个结构体 // Go map 的底层结构体表示 type hmap struct { count int // map中键值对的个数，使用len()可以获取 flags uint8 B uint8 // 哈希桶的数量的log2，比如有8个桶，那么B=3 noverflow uint16 // 溢出桶的数量 hash0 uint32 // 哈希种子 buckets unsafe.Pointer // 指向哈希桶数组的指针，数量为 2^B oldbuckets unsafe.Pointer // 扩容时指向旧桶的指针，当扩容时不为nil nevacuate uintptr extra *mapextra // 可选字段 } const ( bucketCntBits = 3 bucketCnt = 1 \u003c\u003c bucketCntBits // 桶数量 1 \u003c\u003c 3 = 8 ) // Go map 的一个哈希桶，一个桶最多存放8个键值对 type bmap struct { // tophash存放了哈希值的最高字节 tophash [bucketCnt]uint8 // 在这里有几个其它的字段没有显示出来，因为k-v的数量类型是不确定的，编译的时候才会确定 // keys: 是一个数组，大小为bucketCnt=8，存放Key // elems: 是一个数组，大小为bucketCnt=8，存放Value // 你可能会想到为什么不用空接口，空接口可以保存任意类型。但是空接口底层也是个结构体，中间隔了一层。因此在这里没有使用空接口。 // 注意：之所以将所有key存放在一个数组，将value存放在一个数组，而不是键值对的形式，是为了消除例如map[int64]所需的填充整数8（内存对齐） // overflow: 是一个指针，指向溢出桶，当该桶不够用时，就会使用溢出桶 } 当向 map 中存储一个 kv 时，通过 k 的 hash 值与 buckets 长度取余，定位到 key 在哪一个bucket中，hash 值的高8位存储在 bucket 的 tophash[i] 中，用来快速判断 key是否存在。当一个 bucket 满时，通过 overflow 指针链接到下一个 bucket。 二面 又深挖项目，这次挖了快40分钟了。。 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:5:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"go里面 slice 和 array 有区别吗？ slice， 是切片，是引用类型，长度可变。 array，是数组，是值类型，长度不可变。 slice的底层其实是基于array 实现的。 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:6:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"归并排序是稳定的吗？时间复杂度是多少？ 是的，归并排序中相等元素的顺序不会改变。 总时间 = 分解时间+解决问题时间+合并时间。 分解时间就是把一个待排序序列分解成两序列，时间为一常数，时间复杂度o(1)。 解决问题时间是两个递归式，把一个规模为n 的问题分成两个规模分别为 n/2 的子问题，时间为2T(n/2)。 合并时间复杂度为o(n)。 总时间T(n)=2T(n/2)+o(n)，这个递归式可以用递归树来解，其解是o(nlogn)。 所以是O(nlogn) ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:7:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"写一个归并排序吧 func mergeSort(arr *[]int, l int, r int) { if l \u003e= r { //不可再分隔,只有一个元素 return } mid := (l+r)/2 mergeSort(arr,l,mid) mergeSort(arr,mid+1,r) if (*arr)[mid] \u003e (*arr)[mid+1] { merge(arr, l, mid, r) } } //将arr[l...mid]和arr[mid+1...r]两部分进行归并 func merge(arr *[]int, l int, mid int, r int) { //先把arr中[l..r]区间的值copy一份到arr2 //注意:这里可优化，copyarr 可改为长度为r-l+1的数组,下面的赋值等操作按偏移量l来修改即可， copyarr := make([]int, r+1) for index := l; index \u003c= r; index++ { copyarr[index] = (*arr)[index] } //定义要合并的两个子数组各自目前数组内还没被合并的首位数字下标为i,j //初始化i，j i := l j := mid+1 //遍历并逐个确定数组[l,r]区间内数字的顺序 for k := l; k \u003c= r; k++ { //防止i/j\"越界\"，应该先判断i和j的下变是否符合条件（因为两个子数组应该符合i\u003c=mid j\u003c=r） if i \u003e mid { (*arr)[k] = copyarr[j] j++ }else if j \u003e r { (*arr)[k] = copyarr[i] i++ }else if copyarr[i] \u003c copyarr[j] { (*arr)[k] = copyarr[i] i++ }else{ (*arr)[k] = copyarr[j] j++ } } } func TestMerge(t *testing.T) { arr := []int{8,6,2,3,1,5,7,4} mergeSort(\u0026arr, 0, len(arr)-1) fmt.Println(arr) } ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:8:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"空chan和关闭的chan进行读写会怎么样？ 空chan 读会读取到该chan类型的零值。 写会直接写到chan中。 关闭的chan 读已经关闭的 chan 能一直读到东西，但是读到的内容根据通道内关闭前是否有元素而不同。如果有元素，就继续读剩下的元素，如果没有就是这个chan类型的零值，比如整型是 int，字符串是\"\" 。 写已经关闭的 chan 会 panic。因为源码上面就是这样写的，可以看src/runtime/chan.go 三面 感觉就走个过场。。面完简历就共享了。。 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:9:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"讲讲redis分布式锁的设计与实现 这篇博客很详细了，可以看这篇博客 redis分布式锁实现 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:10:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"讲讲redis的哨兵模式 一主两从三哨兵集群，当 master 节点宕机时，通过哨兵(sentinel)重新推选出新的master节点，保证集群的可用性。 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:11:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"限流算法有哪些？令牌桶算法怎么实现？ 限流算法常见有计数器算法，滑动窗口，令牌桶算法。 令牌桶算法是比较常见的限流算法之一，如下： 所有的请求在处理之前都需要拿到一个可用的令牌才会被处理； 根据限流大小，设置按照一定的速率往桶里添加令牌； 桶设置最大的放置令牌限制，当桶满时、新添加的令牌就被丢弃或者拒绝； 请求达到后首先要获取令牌桶中的令牌，拿着令牌才可以进行其他的业务逻辑，处理完业务逻辑之后，将令牌直接删除； 令牌桶有最低限额，当桶中的令牌达到最低限额的时候，请求处理完之后将不会删除令牌，以此保证足够的限流； ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:12:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"算法：交换二叉树左右节点 ","date":"0001-01-01 00:00:00","objectID":"/c.-%E7%99%BE%E5%BA%A6/:13:0","tags":null,"title":"","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":null,"content":"【Golang开发面经】滴滴（三轮技术面） 写在前面 滴滴面试赶紧感觉还行吧，挺注重基础的，很多时间都花在了挖项目上面，所以大家一定要很熟悉自己的项目！面试官水平也很高。不经感叹这个曾经的大厂现在变成这个样子，唉。。 笔试 略 一面 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:0:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"进程间通信方式 管道、消息队列、信号量、共享内存 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:1:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"栈上分配内存快还是堆上，为什么? 显然从栈上分配内存更快，因为从栈上分配内存仅仅就是栈指针的移动而已 操作系统会在底层对栈提供支持，会分配专门的寄存器存放栈的地址。 栈的入栈出栈操作也十分简单，并且有专门的指令执行，所以栈的效率比较高也比较快。 堆的生长空间向上，地址越来越大，栈的生长空间向下，地址越来越小。 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:2:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"channel 底层 type hchan struct { qcount uint // channel 里的元素计数 dataqsiz uint // 可以缓冲的数量，如 ch := make(chan int, 10)。 此处的 10 即 dataqsiz elemsize uint16 // 要发送或接收的数据类型大小 buf unsafe.Pointer // 当 channel 设置了缓冲数量时，该 buf 指向一个存储缓冲数据的区域，该区域是一个循环队列的数据结构 closed uint32 // 关闭状态 sendx uint // 当 channel 设置了缓冲数量时，数据区域即循环队列此时已发送数据的索引位置 recvx uint // 当 channel 设置了缓冲数量时，数据区域即循环队列此时已接收数据的索引位置 recvq waitq // 想读取数据但又被阻塞住的 goroutine 队列 sendq waitq // 想发送数据但又被阻塞住的 goroutine 队列 lock mutex ... } ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:3:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"七层模型 从上而下分别是 应用层，表示层，会话层，传输层，网络层，数据链路层，物理层等等… ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:4:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"tcp、udp TCP 是可靠传输，面向连接，基于流，占用资源多，效率低。 UDP是尽最大努力交付，基于无连接，基于报文，UDP 占用系统资源较少，效率高。 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:5:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"redis持久化的方式以及使用场景 Redis提供 RDB 和 AOF 两种持久化机制 ， 有了持久化机制我们基本上就可以避免进程异常退出时所造成的数据丢失的问题了，Redis能在下一次重启的时候利用之间产生的持久化文件实现数据恢复。 RDB持久化就是指的讲当前进程的数据生成快照存入到磁盘中，触发RDB机制又分为手动触发与自动触发。 AOF 持久化是以独立的日志记录每次写命令，重启 Redis 的时候再重新执行AOF文件中命令以达到恢复数据，所以AOF主要就是解决持久化的实时性。 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:6:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"如何实现线程池？ 线程池有两部分组成：同步队列和线程池。 同步队列：以链表的形式创建一个同步队列，同步队列的主要工作就是通过Put()函数向队列里面添加事务，通过Get()函数从队列里面取出事务。 线程池：主要由线程组（注：线程组里面存放的时 thread 类型 的共享只能指针，指向工作的线程）构成，通过Start()函数向线程组中添加numThreads个线程，并使得每一个线程调用RunThread()函数来获取同步队列的事务并执行事务；通过Stop() 函数停止线程池的工作。 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:7:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"算法：二叉树俯视图 二面 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:8:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"怎么判断给定ip是否在给定ip区间内？ IP地址可以转换成整数，可以将IP返回化整为整数范围进行排查。 package main import ( \"fmt\" \"strconv\" \"strings\" ) func main() { ipVerifyList := \"192.168.1.0-192.172.3.255\" ip := \"192.170.223.1\" ipSlice := strings.Split(ipVerifyList, `-`) if len(ipSlice) \u003c 0 { return } if ip2Int(ip) \u003e= ip2Int(ipSlice[0]) \u0026\u0026 ip2Int(ip) \u003c= ip2Int(ipSlice[1]) { fmt.Println(\"ip in iplist\") return } fmt.Println(\"ip not in iplist\") } func ip2Int(ip string) int64 { if len(ip) == 0 { return 0 } bits := strings.Split(ip, \".\") if len(bits) \u003c 4 { return 0 } b0 := string2Int(bits[0]) b1 := string2Int(bits[1]) b2 := string2Int(bits[2]) b3 := string2Int(bits[3]) var sum int64 sum += int64(b0) \u003c\u003c 24 sum += int64(b1) \u003c\u003c 16 sum += int64(b2) \u003c\u003c 8 sum += int64(b3) return sum } func string2Int(in string) (out int) { out, _ = strconv.Atoi(in) return } ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:9:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"如何优化慢SQL？ 查看是否使用到了索引。 查看 SQL语句 是否符合最左匹配原则。 对查询进行优化，尽可能避免全表扫描 字段冗余，减少跨库查询或多表连接操作 将一些常用的数据结构放在缓冲中（部门名字，组织架构之类的），就不需要查数据库了。 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:10:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"什么情况不建议使用索引 在where条件中（包括group by以及order by）里用不到的字段不需要创建索引，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的。 数据量小的表最好不要使用索引，少于1000个 字段中如果有大量重复数据（性别），也不用创建索引 避免对经常更新的表创建过多的索引。 不建议使用无序的值作为索引。 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:11:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"线程池的几种拒绝策略及其应用场景 当时都不知道这是个啥… 具体看这篇博客吧 线程池拒绝策略应用场景 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:12:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"http与tcp区别 http 就是基于传输层的 tcp 实现的一个应用层协议。 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:13:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"长连接与短连接 长连接意味着进行一次数据传输后，不关闭连接，长期保持连通状态。 短连接意味着每一次的数据传输都需要建立一个新的连接，用完再马上关闭它。下次再用的时候重新建立一个新的连接，如此反复。 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:14:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"讲讲tcp的挥手？ 数据传输完毕之后，通信的双方都可释放连接。现在A和B都处于ESTABLISHED状态。 A的应用进程先向TCP发出连接释放报文段，并停止再发送数据，主动关闭TCP连接。A把链接释放报文段首部的终止控制位FIN置为1，其序号为seq=u，它等于前面以传送过的数据的最后一个字节的序号加1.这时候A进入了FIN-WAIT-1(终止等待1)状态，等待B的确认。 注意：TCP规定，FIN报文段即使不携带数据，他也消耗掉一个序号！！ B 收到链接释放报文段后即发出确认，确认号是ack = u + 1，而这个报文段自己的序号是v，等于B前面已传送过的数据的最后一个字节的序号加1.然后B就进入CLOSE-WAIT(关闭等待)状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的链接就释放了，这时的TCP链接处于半关闭状态，即A已经没有数据要发送了，但B若发送数据，A仍要接收，也就是说，从B到A这个方向的连接并未关闭。这个状态可能要维持一段时间。 A收到来自B的确认后，就进入了FIN-WAIT-2(终止等待2)状态满等待B发出的连接释放报文段。若B已经没有要向A发送的数据，其应用进程就通知TCP释放连接，这时B发出的连接释放报文段必须使FIN = 1，现假定B的序号为w(在半关闭状态B可能又发送了一些数据)。B还必须重复上次已发送过的确认号ack = u + 1.这时B就进入LAST-ACK(最后确认)状态，等待A的确认。 A在收到了B的链接释放报文段后，必须对此发出确认。在确认报文段中把ACK置1，确认号ack=w+1，而自己的序号是seq=u+1(根据TCP标准，前面发送过的FIN报文段要消耗一个序号)。然后进入到TIME-WAIT(时间等待)状态。注意： 现在TCP连接还没有还没有释放掉。必须经过时间等待计时器设置的时间2MSL后，A才能进入CLOSED状态。 时间MSL叫做最长报文段寿命，RFC793建议设在两分钟。但是在现在工程来看两分钟太长了，所以TCP允许不同的实现可以根据具体情况使用更小的MSL值。 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:15:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"time wait，过多怎么办 修改TIME_WAIT连接状态的上限值 启动快速回收机制 开启复用机制 修改短连接为长连接方式 由客户端来主动断开连接 ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:16:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"算法：忘了… 三面 聊人生…. ","date":"0001-01-01 00:00:00","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:17:0","tags":null,"title":"","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":null,"content":"【Golang开发面经】米哈游（一轮游。。） 写在前面 米哈游 面试下来感觉还行吧，挺注重基础的，面试官水平也很高。但是感觉不是在招人的样子，我有好多同学都是简历挂（ 笔试 无笔试，很奇怪（ 一面 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:0:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"线程和协程有什么区别？各自有什么优缺点？ 进程 线程 系统中正在运行的一个应用程序 系统分配处理器时间资源的基本单元 程序一旦运行就是进程 进程之内独立执行的一个单元执行流 资源分配的最小单位 程序执行的最小单位 进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。 线程有自己的堆栈和局部变量，但线程没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些 要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:1:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"进程之间如何进行通信？ 管道、消息队列、共享内存、信号量、信号、socket ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:2:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"什么是信号，信号量是如何实现的？ 信号： 在Linux中，为了响应各种事件，提供了几十种信号，可以通过kill -l命令查看。 如果是运行在 shell终端 的进程，可以通过键盘组合键来给进程发送信号，例如使用Ctrl+C 产生 SIGINT 信号，表示终止进程。 如果是运行在后台的进程，可以通过命令来给进程发送信号，例如使用kill -9 PID 产生SIGKILL信号，表示立即结束进程。 信号量： 当使用共享内存的通信方式，如果有多个进程同时往共享内存写入数据，有可能先写的进程的内容被其他进程覆盖了。 因此需要一种保护机制，信号量本质上是一个整型的计数器，用于实现进程间的互斥和同步。 信号量代表着资源的数量，操作信号量的方式有两种： P操作：这个操作会将信号量减一，相减后信号量如果小于0，则表示资源已经被占用了，进程需要阻塞等待；如果大于等于0，则说明还有资源可用，进程可以正常执行。 V操作：这个操作会将信号量加一，相加后信号量如果小于等于0，则表明当前有进程阻塞，于是会将该进程唤醒；如果大于0，则表示当前没有阻塞的进程。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:3:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"讲讲Go里面的GMP模型？ ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:4:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"Go的GMP模型 G：表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P：表示逻辑processor，P 的数量决定了系统内最大可并行的 G 的数量（前提：系统的物理cpu核数 \u003e= P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M：M 代表着真正的执行计算资源，物理 Processor。 G 如果想运行起来必须依赖 P，因为 P 是它的逻辑处理单元，但是 P 要想真正的运行，他也需要与 M 绑定，这样才能真正的运行起来，P 和 M 的这种关系就相当于 Linux 系统中的用户层面的线程和内核的线程是一样的。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:5:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"map用过吧？怎么对map进行排序？ go的map不保证有序性，所以按key排序需要取出key，对key排序，再遍历输出value ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:6:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"讲讲map底层？为什么是无序的？ map 的底层是一个结构体 // Go map 的底层结构体表示 type hmap struct { count int // map中键值对的个数，使用len()可以获取 flags uint8 B uint8 // 哈希桶的数量的log2，比如有8个桶，那么B=3 noverflow uint16 // 溢出桶的数量 hash0 uint32 // 哈希种子 buckets unsafe.Pointer // 指向哈希桶数组的指针，数量为 2^B oldbuckets unsafe.Pointer // 扩容时指向旧桶的指针，当扩容时不为nil nevacuate uintptr extra *mapextra // 可选字段 } const ( bucketCntBits = 3 bucketCnt = 1 \u003c\u003c bucketCntBits // 桶数量 1 \u003c\u003c 3 = 8 ) // Go map 的一个哈希桶，一个桶最多存放8个键值对 type bmap struct { // tophash存放了哈希值的最高字节 tophash [bucketCnt]uint8 // 在这里有几个其它的字段没有显示出来，因为k-v的数量类型是不确定的，编译的时候才会确定 // keys: 是一个数组，大小为bucketCnt=8，存放Key // elems: 是一个数组，大小为bucketCnt=8，存放Value // 你可能会想到为什么不用空接口，空接口可以保存任意类型。但是空接口底层也是个结构体，中间隔了一层。因此在这里没有使用空接口。 // 注意：之所以将所有key存放在一个数组，将value存放在一个数组，而不是键值对的形式，是为了消除例如map[int64]所需的填充整数8（内存对齐） // overflow: 是一个指针，指向溢出桶，当该桶不够用时，就会使用溢出桶 } 当向 map 中存储一个 kv 时，通过 k 的 hash 值与 buckets 长度取余，定位到 key 在哪一个bucket中，hash 值的高8位存储在 bucket 的 tophash[i] 中，用来快速判断 key是否存在。当一个 bucket 满时，通过 overflow 指针链接到下一个 bucket。 在源码runtime.mapiterinit中 func mapiterinit(t *maptype, h *hmap, it *hiter) { ... it.t = t it.h = h it.B = h.B it.buckets = h.buckets if t.bucket.kind\u0026kindNoPointers != 0 { h.createOverflow() it.overflow = h.extra.overflow it.oldoverflow = h.extra.oldoverflow } r := uintptr(fastrand()) if h.B \u003e 31-bucketCntBits { r += uintptr(fastrand()) \u003c\u003c 31 } it.startBucket = r \u0026 bucketMask(h.B) it.offset = uint8(r \u003e\u003e h.B \u0026 (bucketCnt - 1)) it.bucket = it.startBucket ... mapiternext(it) } 通过对 mapiterinit 方法阅读，可得知其主要用途是在 map 进行遍历迭代时进行初始化动作。共有三个形参，用于读取当前哈希表的类型信息、当前哈希表的存储信息和当前遍历迭代的数据 源码中 fastrand ... // decide where to start r := uintptr(fastrand()) if h.B \u003e 31-bucketCntBits { r += uintptr(fastrand()) \u003c\u003c 31 } it.startBucket = r \u0026 bucketMask(h.B) it.offset = uint8(r \u003e\u003e h.B \u0026 (bucketCnt - 1)) // iterator state it.bucket = it.startBucket 在这段代码中，它生成了随机数。用于决定从哪里开始循环迭代。更具体的话就是根据随机数，选择一个桶位置作为起始点进行遍历迭代。 因此每次重新 for range map，你见到的结果都是不一样的。那是因为它的起始位置根本就不固定！ ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:7:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"知道TCP连接吧？三次握手的目的是什么？ 因为通信的前提是确保双方都是接收和发送信息是正常的。 三次握手是为了建立可靠的数据传输通道， 第一次握手就是让 接收方 知道 发送方 有发送信息的能力 第二次握手就是让 发送方 知道 接收方 有发送和接受信息的能力 第三次握手就是让 接收方 知道 发送方 有接受信息的能力 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:8:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"那四次挥手有了解过吗？为什么是四次？ 四次挥手则是为了保证等数据完成的被接收完再关闭连接。 既然提到需要保证数据完整的传输完，那就需要保证双方 都达到关闭连接的条件才能断开。 第一次挥手：客户端发起关闭连接的请求给服务端； 第二次挥手：服务端收到关闭请求的时候可能这个时候数据还没发送完，所以服务端会先回复一个确认报文，表示自己知道客户端想要关闭连接了，但是因为数据还没传输完，所以还需要等待； 第三次挥手：当数据传输完了，服务端会主动发送一个 FIN 报文，告诉客户端，表示数据已经发送完了，服务端这边准备关闭连接了。 第四次挥手：当客户端收到服务端的 FIN 报文过后，会回复一个 ACK 报文，告诉服务端自己知道了，再等待一会就关闭连接。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:9:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"什么是粘包和拆包？为什么会出现？ 粘包就是两个或者多个以上的包粘在一起，拆包就是为什么解决粘包问题。 出现粘包原因有两个，一个是发送方发送不及时，导致多个包在发送端粘黏。另一个是接收方接受不及时，导致包在接收端堆叠导致。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:10:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"怎么解决？ 设计一个带包头的应用层报文结构就能解决。包头定长，以特定标志开头，里带着负载长度，这样接收侧只要以定长尝试读取包头，再按照包头里的负载长度读取负载就行了，多出来的数据都留在缓冲区里即可。其实ip报文和tcp报文都是这么干的。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:11:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"什么是事务？ 事务具有原子性、一致性、隔离性、持久性特性，并且 指将一系列数据操作捆绑成为一个整体进行统一管理，如果某一事务执行成功，则在该事物中进行的所有数据更改均会提交，成为数据库中的永久组成部分。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:12:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"那 redis 支持事务吗？ redis 是不支持事务的，因为 他不支持原子性，但是支持一致性，是最终一致性。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:13:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"算法：手撕链表，要求递归实现。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:14:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"算法：忘记了。。好像是中等题，反正不难，也不简单。 二面 我以为过了简历，免了笔试，一面算法写出来，八股也能吹了，应该有二面，但还是挂了（ 真的是 海量 hc 吗。。。 后面问了一下，原来基本不招本科生… ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:15:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"【Golang开发面经】深信服（两轮技术面） 写在前面 深信服面试起来感觉有点偏向应用，没有涉及高并发等等内容，想想也确实，深信服更多偏向B端。业务能力扎实也是应该的。深信服挺好的，但我想找toc的，就拒掉了。。 一面 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:0:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"了解过切片和数组吗？有什么区别？ 切片的底层其实是数组，数组是不可变长度的，而切片是可变长度的。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:1:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"那这样初始化可以吗？有什么问题？ var array []int 这种其实不好，因为这样是不能赋予地址的。所以容易报错，我们要么用 make去创建，要么用语法糖 array:=[]int{}去创建。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:2:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"用过map吧？怎么遍历map？ for k,v:=range map{ ... } ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:3:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"那遍历 map 是有序的吗？ 无序的 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:4:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"为什么是无序的？ 不是有序的，使用 range 多次遍历 map 时输出的 key 和 value 的顺序可能不同，map在遍历时，并不是从固定的0号bucket开始遍历的，每次遍历，都会从一个随机值序号的bucket，再从其中随机的 cell 开始遍历。map 遍历时，是按序遍历 bucket，同时按需遍历 bucket 和其 overflow bucket 中 的 cell。 但是 map 在扩容后，会发生 key 的搬迁，这造成原来落在一个 bucket 中的 key，搬迁后，有可能会落到其他 bucket 中了，从这个角度看，遍历 map 的结果就不可能是按照原来的顺序了。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:5:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"用过chan吧？怎么声明一个chan呢？ chan是引用类型，一般我们应该用make去创建一个chan ch:=make(chan int) ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:6:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"怎么发消息给chan呢？ ch \u003c- 1 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:7:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"给一个关闭的chan发消息会怎么样？ 会引起panic ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:8:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"讲讲 GMP 吧 G：表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P：表示逻辑processor，P 的数量决定了系统内最大可并行的 G 的数量（前提：系统的物理cpu核数 \u003e= P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M：M 代表着真正的执行计算资源，物理 Processor。 G 如果想运行起来必须依赖 P，因为 P 是它的逻辑处理单元，但是 P 要想真正的运行，他也需要与 M 绑定，这样才能真正的运行起来，P 和 M 的这种关系就相当于 Linux 系统中的用户层面的线程和内核的线程是一样的。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:9:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"那GC有了解过吗？ Go是采用三色标记法来进行垃圾回收的，是传统 Mark-Sweep 的一个改进，它是一个并发的 GC 算法。on-the-fly 原理如下 整个进程空间里申请每个对象占据的内存可以视为一个图， 初始状态下每个内存对象都是白色标记。 先stop the world，将扫描任务作为多个并发的goroutine立即入队给调度器，进而被CPU处理，第一轮先扫描所有可达的内存对象，标记为灰色放入队列 第二轮可以恢复start the world，将第一步队列中的对象引用的对象置为灰色加入队列，一个对象引用的所有对象都置灰并加入队列后，这个对象才能置为黑色并从队列之中取出。循环往复，最后队列为空时，整个图剩下的白色内存空间即不可到达的对象，即没有被引用的对象； 第三轮再次stop the world，将第二轮过程中新增对象申请的内存进行标记（灰色），这里使用了writebarrier（写屏障）去记录这些内存的身份； 这个算法可以实现 on-the-fly，也就是在程序执行的同时进行收集，并不需要暂停整个程序。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:10:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"mysql 有用过吧？MVCC 是怎么实现的？ MVCC 的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，Read View 来实现的。所以我们先来看看这个三个 point 的概念 隐式字段 每行记录除了我们自定义的字段外，还有数据库隐式定义的 DB_TRX_ID, DB_ROLL_PTR, DB_ROW_ID 等字段 DB_TRX_ID：6 byte，最近修改(修改/插入)事务 ID：记录创建这条记录/最后一次修改该记录的事务 ID DB_ROLL_PTR：7 byte，回滚指针，指向这条记录的上一个版本（存储于 rollback segment 里） DB_ROW_ID：6 byte，隐含的自增 ID（隐藏主键），如果数据表没有主键，InnoDB 会自动以DB_ROW_ID ，产生一个聚簇索引。 undo日志 insert undo log：代表事务在 insert 新记录时产生的 undo log，只在事务回滚时需要，并且在事务提交后可以被立即丢弃。 update undo log：事务在进行 update 或 delete 时产生的 undo log ，不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被 purge线程统一清除。 Read View Read View 就是事务进行 快照读 操作的时候生产的 读视图 (Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的 ID (当每个事务开启时，都会被分配一个 ID , 这个 ID 是递增的，所以最新的事务，ID 值越大)。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:11:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"mysql 的锁是怎么实现的？ 当时确实没了解如何实现的，只知道如何for update这些，可以看这篇博客，很详细了。 Mysql锁机制及原理简析 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:12:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"用过gin是吧？gin是怎么处理请求的？ Gin其实是通过一个context来进行上下文的传递，将这个传递参数，参数返回。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:13:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"如果有一个业务给你，你怎么写这个请求？ 首先肯定是传统的MVC模型来进行操作。 controller层来接受请求，service层来进行请求的处理，dao层来写sql语句。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:14:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"算法：将重复的元素移到最后 二面 二面基本都是跟着项目走。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:15:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"如果你的系统突然多了10w的访问量，你要怎么处理？ nginx来进行负载均衡。 用户验证的信息的过期时间设置长一点，以免发生缓冲雪崩的问题。 设置布尔过滤器。 检查sql语句，是否符合要求，是否是慢sql，如果是则解决。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:16:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"redis用过是吧？说说你在项目里面的排行榜？你说说redis的底层是怎么处理的？ 主要是用redis的zset实现的。 zset主要是跳跃表实现的。 Redis 的跳跃表由 redis.h/zskiplistNode 和 redis.h/zskiplist 两个结构定义，其中 zskiplistNode 结构用于表示跳跃表节点，而 zskiplist 结构则用于保存跳跃表节点的相关信息，比如节点的数量，以及指向表头节点和表尾节点的指针等等。 上图中展示了一个跳跃表示例，最左边的就是 zskiplist 结构。 header：指向跳跃表的表头节点。 tail：指向跳跃表的表尾节点。 level：记录目前跳跃表内，层数最大的那个节点的层数。 记录跳跃表的长度，也就是，跳跃表目前包含节点的数量。 层（level）：节点中用L1、L2、L3等字样标记节点的各个层，L1表示第一层，L2代表第二层，以此类推。每层都带有两个属性：前进指针和跨度。前进指针用于方位位于表尾方向的其他节点。而跨度则记录了前进指针所指向节点和当前节点的距离。 后退（backward）指针： 节点中用BW字样标记的后退指针，他指向当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。 分值(score)：各个节点中的 1.0、2.0、3.0是节点所保存的分值。在跳跃表中，节点按各个所保存的分值从小到大排序。 成员对象(obj)：各个节点中的o1，o2 和 o3 是节点所保存的成员对象。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:17:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"关于RabbitMQ的一些面试题 0. 什么是RabbitMQ RabbitMQ采用AMQP高级新消息队列协议的一种消息队列技术，最大的特点是消费并不需要确保提供方实现，实现了服务之间的高度解耦 1. 延时队列底层实现 延迟队列存储的对象肯定是对应的延迟消息，所谓”延迟消息”是指当消息被发送以后，并不想让消费者立即拿到消息，而是等待指定时间后，消费者才拿到这个消息进行消费 订单时间 定时任务 TTL（Time To Live） RabbitMQ可以针对Queue和Message设置 x-message-tt，来控制消息的生存时间，如果超时，则消息变为dead letter RabbitMQ针对队列中的消息过期时间有两种方法可以设置。 A: 通过队列属性设置，队列中所有消息都有相同的过期时间。 B: 对消息进行单独设置，每条消息TTL可以不同。 给对列设置过期时间，将消息加入对列，过期时间之后消息自动进入死信队列，监听死信队列，进行消费操作可以实现延迟队列 2. 使用RabbitMQ需要注意什么 消息丢失，消息重复消费等等 3. RabbitMQ效率 4. 插入延时队列的过期时间是单调的麻？ 5. 如何确保消息正确地发送至RabbitMQ？ 如何确保消息接收方消费了消息？ ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:0:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"5.1 发送方确认模式 将信道设置成confirm模式（发送方确认模式），则所有在信道上发布的消息都会被指派一个唯一ID。一旦消息被投递到目的队列后，或者消息被写入磁盘后，信道会发送一个确认给生产者（包括消息的ID）。 如果RabbitMQ发生内部错误从而导致消息丢失，会发送一条nack消息。 发送方确认模式是异步的，生产者应用程序在等待确认的同时，可以继续发送消息。 当确认消息到达生产者应用程序，生产者应用的回调方法就会被触发来处理确认消息。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:1:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"5.2 接收方确认机制 消费者接受每一条消息后都必须进行确认，只要有消费者确认了消息，MQ才能安全的把消息从队列中删除。 这里并没有用到超时机制，MQ仅通过Consumer的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMQ给了Consumer足够长的时间来处理消息。保证了数据的最终一致性。 还有几种情况： 如果消费者接受到消息，在确认之前断开了连接或取消订阅，RabbitMQ会认为消息没有被分发，然后重新分发给下一个订阅的消费者。（可能存在消费重复的隐患，需要去重） 如果消费者接受到消息却没有确认消息，连接也未断开，则RabbitMQ认为该消费者繁忙。则不会给该消费者分发更多的消息。 6. 如何避免消息重复投递或重复消费？ 在消息生产时，MQ内部针对每条生产者发送的消息生成一个inner-msg-id，作为去重的依据（消息投递失败并重传），避免重复的消息进入队列，在消息消费时，要求消息体中 必须要有一个bizID（对于同一个业务全局唯一） 作为去重的依据，避免同一条消息被重复消费。 7. 消息基于什么传输？ 由于 TCP 连接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈。RabbitMQ 使用信道的方式来传输数据。信道是建立在真实的 TCP 连接内的虚拟连接，且每条 TCP 连接上的信道数量没有限制。 8、消息如何分发？ 一个生产者，多个消费者 多个消费者时，是轮询机制，依次分发给消费者(每个消费者按顺序依次消费) no_act设置是否确认消息处理完？ no_act = True , 消费者不发送确认信息，RabbitMQ从发送消息队列后，不管消费者是否处理完，删除queue 设置no_act=False，RabbitMQ等待消费者的callback处理完，发送确认信息，如果此时消费者down了，则RabbitMQ把消息轮询发送给下一个消费者，等待确认才会删除queue 去掉no_act=True，需要在回调函数中新增代码，手动向RabbitMQ发送确认信息 9、消息怎么路由？ Direct：直连模式 Topic： 转发模式 Topic 模式下可以使用统配符表示bingKey：'*‘表示匹配一个单词， ‘#‘则表示匹配没有或者多个单词。由此可以实现一个queue接收多个路由的消息。 Fanout ：广播模式 广播模式下，不用理会routing key。Fanout Exchange 会将消息传递到 exchange 绑定好的 queue list 上去。 10、如何确保消息不丢失？ ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:2:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"10.1 生产者丢失消息 可以选择使用 RabbitMQ 提供事务功能，就是生产者在发送数据之前开启事务，然后发送消息，如果消息没有成功被RabbitMQ接收到，那么生产者会受到异常报错，这时就可以回滚事物，然后尝试重新发送；如果收到了消息，那么就可以提交事物。 缺点： RabbitMQ 事务已开启，就会变为同步阻塞操作，生产者会阻塞等待是否发送成功，太耗性能会造成吞吐量的下降。 还有就是上面的第五点 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:3:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"10.2 RabbitMQ自己丢了数据 持久化 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:4:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"10.3 消费者弄丢了数据 使用 RabbitMQ 提供的 ACK 机制，首先关闭 RabbitMQ 的自动ACK，然后每次在确保处理完这个消息之后，在代码里手动调用 ACK。这样就可以避免消息还没有处理完就ACK。 11、RabbitMQ的集群 12、使用RabbitMQ有什么好处？ ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:5:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"12.1 削峰 把消息压入RabbitMQ中可以缓冲系统压力。比如现在系统只能接受2000请求，但是一下子有10000个请求过来，那么这个请求就会压在RabbitMQ中，那么就可以慢慢进行消费了。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:6:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"12.2 异步 以前是先去发短信，再去发邮件。引入RabbitMQ之后，我们就可以进行在发短信的同时再去发邮箱。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:7:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"12.3 解耦 当多个系统耦合在一起的时候，系统的消息会发送给连在一起的系统，但是这个消息有些系统可能是不需要的。所以引入了之后，很方便将这个系统进行解耦，每个系统需要的就在消息队列解耦。 13、MQ 的缺点 虽然能提供削峰，异步，解耦，但是这个还是有很多要考虑的问题，消息丢失，重复消费。 14、介绍Rabbitmq的手动ACK和自动ACK 当消息一旦被消费者接收，队列中的消息就会被删除。那么问题来了：RabbitMQ怎么知道消息被接收了呢？ 这就要通过消息确认机制（Acknowlege）来实现了。当消费者获取消息后，会向RabbitMQ发送回执ACK，告知消息已经被接收。不过这种回执ACK分两种情况： 自动ACK：消息一旦被接收，消费者自动发送ACK 手动ACK：消息接收后，不会发送ACK，需要手动调用 这两ACK要怎么选择呢？这需要看消息的重要性： 如果消息不太重要，丢失也没有影响，那么自动ACK会比较方便 如果消息非常重要，不容丢失。那么最好在消费完成后手动ACK，否则接收消息后就自动ACK，RabbitMQ就会把消息从队列中删除。如果此时消费者宕机，那么消息就丢失了。 参考连接 分布式消息中间件-RabbitMQ面试题（必问） RabbitMQ-解耦、异步、削峰 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:8:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"分布式题目集锦 1. 负载均衡算法 随机访问策略。 系统随机访问，缺点：可能造成服务器负载压力不均衡，俗话讲就是撑的撑死，饿的饿死。 轮询策略。 请求均匀分配，如果服务器有性能差异，则无法实现性能好的服务器能够多承担一部分。 权重轮询策略。 权值需要静态配置，无法自动调节，不适合对长连接和命中率有要求的场景。 Hash取模策略。 不稳定，如果列表中某台服务器宕机，则会导致路由算法产生变化，由此导致命中率的急剧下降。 简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 0 ~ 2^32-1（即哈希值是一个32位无符号整形），整个哈希环如下： 多个服务器都通过这种方式进行计算，最后都会各自映射到圆环上的某个点，这样每台机器就能确定其在哈希环上的位置，如下图所示。 那么用户访问，如何分配访问的服务器呢？我们根据用户的 IP 使用上面相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针行走，遇到的第一台服务器就是其应该定位到的服务器。 2. 熔断和降级 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:0:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"2.1 熔断 一般是某个服务故障或者是异常引起的，当某个异常条件被触发，直接熔断整个服务，而不是一直等到此服务超时，为了防止防止整个系统的故障。 而采用了一些保护措施。过载保护。比如A服务的X功能依赖B服务的某个接口，当B服务接口响应很慢时，A服务X功能的响应也会被拖慢，进一步导致了A服务的线程都卡在了X功能上，A服务的其它功能也会卡主或拖慢。此时就需要熔断机制，即A服务不在请求B这个接口，而可以直接进行降级处理。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:1:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"2.2 降级 服务器当压力剧增的时候，根据当前业务情况及流量，对一些服务和页面进行有策略的降级。以此缓解服务器资源的的压力，以保证核心业务的正常运行，同时也保持了客户和大部分客户的得到正确的响应。 自动降级：超时、失败次数、故障、限流 （1）配置好超时时间(异步机制探测回复情况)； （2）不稳的api调用次数达到一定数量进行降级(异步机制探测回复情况)； （3）调用的远程服务出现故障(dns、http服务错误状态码、网络故障、Rpc服务异常)，直接进行降级。 人工降级：秒杀、双十一大促降级非重要的服务。 3. 幂等 幂等性的核心思想，其实就是保证这个接口的执行结果只影响一次，后续即便再次调用，也不能对数据产生影响，之所以要考虑到幂等性问题，是因为在网络通信中，存在两种行为可能会导致接口被重复执行。 用户的重复提交或者用户的恶意攻击，导致这个请求会被多次重复执行。 在分布式架构中，为了避免网络通信导致的数据丢失，在服务之间进行通信的时候都会设计超时重试的机制，而这种机制有可能导致服务端接口被重复调用。所以在程序设计中，对于数据变更类操作的接口，需要保证接口的幂等性。 使用 redis 里面提供的 setNX 指令，比如对于MQ消费的场景，为了避免MQ重复消费导致数据多次被修改的问题，可以在接受到MQ的消息时，把这个消息通过setNx写入到redis里面，一旦这个消息被消费过，就不会再次消费。 建去重表，将业务中由唯一标识的字段保存到去重表，如果表中存在，则表示已经处理过了。 版本控制，增加版本号，当版本号符合时候，才更新数据。 状态控制，例如订单有状态已支付，未支付，支付中，支付失败，当处于未支付的时候才允许修改成支付中。 4. 分布式事务 在分布式系统中，一次业务处理可能需要多个应用来实现，比如用户发送一次下单请求，就涉及到订单系统创建订单，库存系统减库存，而对于一次下单，订单创建与减库存应该是要同时成功或者同时失效，但在分布式系统中，如果不做处理，就很有可能订单创建成功，但是减库存失败，那么解决这类问题，就需要用到分布式事务，常用的解决方案如下： 本地消息表：创建订单时，将减库存消息加入在本地事务中，一起提交到数据库存入本地消息表，然后调用库存系统，如果调用成功则修改本地。 消息状态为成功，如果调用库存系统失败，则由后台定时任务从本地消息表中取出未成功的消息，重试调用库存系统。 消息队列：目前 RocketMQ 中支持事务消息，它的工作原理是： 产订单系统先发送一条 half 消息到 Broker， half 消息对消费者而言是不可见的。 再创建订单，根据创建订单成功与否，向 Broker 发送 commit 或 rollback。 并且生产者订单系统还可以提供 Broker 回调接口，当 Broker 发现一段时间 half 消息没有收到任何操作命令，则会主动调此接口来查询订单是否创建成功。 如果消费失败，则根据重试策略进行重试，最后还失败则进入死信队列，等待进一步处理。 5. 分布式锁 在单体架构中，多个线程都是属于同一个进程的，所以在线程并发执行时，遇到资源竞争时，可以利用ReentrantLock、 synchronized等技术来作为锁来共享资源的使用。 而在分布式架构中，多个线程是可能处于不同进程中的，而这些线程并发执行遇到资源竞争时，利用 ReentrantLock synchronized 等技术是没办法，来控制多个进程中的线程的，所以需要分布式锁，意思就是，需要一个分布式锁生成器，分布式系统中的应用程序都可以来使用这个生成器所提供的锁，从而达到多个进程中的线程使用同一把锁。 6. 分布式主键id 在开发中，我们通常会需要一个唯一ID来标识数据，如果是单体架构，我们可以通过数据库的主键，或直接在内存中维护一个自增数字来作为ID都是可以的，但对于-个分布式系统，就会有可能会出现ID冲突,此时有以下解决方案: uuid，这种方案复杂度最低，但是会影响存储空间和性能。 利用单机数据库的自增主键，作为分布式ID的生成器，复杂度适中，ID长度较之uuid更短，但是受到单机数据库性能的限制，并发量大的时候，此方案也不是最优方案。 利用redis、zookeeper的特性来生成id，比如redis的自增命令、zookeeper的顺序节点， 这种方案和单机数据库(mysq|)相比，性能有所提高，可以适当选用。 4.雪花算法，一切问题如果能直接用算法解决，那就是最合适的，利用雪花算法也可以生成分布式ID，底层原理就是通过某台机器在某一毫秒内对某一个数字自增，这种方案也能保证分布式架构中的系统 id 唯一，但是只能保证趋势递增。业界存在tinyid、 leaf 等开源中间件实现了雪花算法。 7. 池化技术 对象池技术基本原理的核心有两点：缓存和共享，即对于那些被频繁使用的对象，在使用完后，不立即将它们释放，而是将它们缓存起来，以供后续的应用程序重复使用，从而减少创建对象和释放对象的次数，进而改善应用程序的性能。 事实上，由于对象池技术将对象限制在一定的数量，也有效地减少了应用程序内存上的开销。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:2:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"【Golang开发面经】得物（两轮技术面） 写在前面 得物一顿面试下来感觉还行吧，挺注重基础的，面试官水平也很高。就是聊的挺开心的。 笔试 略 一面 聊项目。大概20分钟吧，如何优化之类的。。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:0:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"你用过gorm？那我直接用sql不也可以吗？为什么用gorm？ 用 sql 是可以，但是会有sql注入的风险，而gorm是通过占位符的形式来一定程度减少了sql的注入。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:1:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"你用过哪些锁？ 用过读写锁 sync 包下的排斥锁或是读写锁，这两种锁。 一般在出现数据竞争的情况下使用的。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:2:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"那map是线程安全的吗？为什么？ 不是的，map 会发生数据竞争，因为底层结构中 map 是没有锁的支持的。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:3:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"chan呢？ chan的底层是有锁结构，所以是线程安全的。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:4:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"对一个关闭的chan读写会怎么样？ 对一个关闭的chan进行读： 如果chan里面还有值，那么就读出chan里面的东西 如果chan里面没有值了，就会读出这个chan类型的零值。 对一个关闭的chan进行写： 直接panic ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:5:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"算法：一道回溯的题目，不太记得了 二面 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:6:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"redis 用过吧？你知道redis有多少种高可靠集群的模型？ 我目前了解到的是redis的哨兵模式。 哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它独立运行。 其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:7:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"redis 支持事务吗？ 不支持，redis不支持原子性，只是支持最终一致性。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:8:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"mysql 的隔离等级？ 未提交读（READ UNCOMMITTED）：READ UNCOMMITTED 提供了事务之间最小限度的隔离。除了容易产生虚幻的读操作和不能重复的读操作外，处于这个隔离级的事务可以读到其他事务还没有提交的数据，如果这个事务使用其他事务不提交的变化作为计算的基础，然后那些未提交的变化被它们的父事务撤销，这就导致了大量的数据变化。 提交读（READ COMMITTED）：READ COMMITTED 隔离级别的安全性比 REPEATABLE READ 隔离级别的安全性要差。处于 READ COMMITTED 级别的事务可以看到其他事务对数据的修改。也就是说，在事务处理期间，如果其他事务修改了相应的表，那么同一个事务的多个 SELECT 语句可能返回不同的结果。 可重复读（REPEATABLE READ）：在可重复读在这一隔离级别上，事务不会被看成是一个序列。不过，当前正在执行事务的变化仍然不能被外部看到，也就是说，如果用户在另外一个事务中执行同条 SELECT 语句数次，结果总是相同的。（因为正在执行的事务所产生的数据变化不能被外部看到）。 序列化（SERIALIZABLE）：如果隔离级别为序列化，则用户之间通过一个接一个顺序地执行当前的事务，这种隔离级别提供了事务之间最大限度的隔离。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:9:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"最左匹配原则是什么？ 索引的底层是一颗B+树，构建一颗B+树只能根据一个值来构建。此处的索引当然也包括联合索引，当索引类型为联合索引时，数据库会依据联合索引最左的字段来构建B+树，也叫最左匹配原则。 最左匹配原则：最左优先，以最左边的为起点任何连续的索引都能匹配上。 同时遇到范围查询(\u003e、\u003c、between、like)就会停止匹配。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:10:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"我能在性别这个字段上面建索引吗？为什么？ 关于区分度不高的字段，比如性别，比如状态字段，不应该建索引。索引并不是建了就快，唯一性太差的字段不需要创建索引，只有2种取值的字段，建了索引数据库也不一定会用，只会白白增加索引维护的额外开销。因为索引也是需要存储的，所以插入和更新的写入操作，同时需要插入和更新你这个字段的索引的。 我们也可以从索引的选择性这方面去说：索引的选择性是指索引列中不同值的数目和表的记录数的比值。 假如表里面有 1000 条数据，表索引列有 980 个不同的值，这时候索引的选择性就是 980/1000=0.98 。索引的选择性越接近 1，这个索引的效率很高。 性别可以认为是3种，男，女，其他。如果创建索引，查询语句 性别=‘男’的数据，索引的选择性就是3/1000=0.003。索引的选择性值很低，对查询提升不大，所以性别建索引意义不大。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:11:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"go 里面 context 是怎么用的？ context 一般我们用来传递上下文信息，在go中，理解为goroutine的运行状态、现场，存在上下层goroutine context的传递，上层goroutine会把context传递给下层goroutine。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:12:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"rabbitmq 是如何保证消息不丢失的？ ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:13:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"5.1 发送方确认模式 将信道设置成confirm模式（发送方确认模式），则所有在信道上发布的消息都会被指派一个唯一ID。一旦消息被投递到目的队列后，或者消息被写入磁盘后，信道会发送一个确认给生产者（包括消息的ID）。 如果RabbitMQ发生内部错误从而导致消息丢失，会发送一条nack消息。 发送方确认模式是异步的，生产者应用程序在等待确认的同时，可以继续发送消息。 当确认消息到达生产者应用程序，生产者应用的回调方法就会被触发来处理确认消息。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:14:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"5.2 接收方确认机制 消费者接受每一条消息后都必须进行确认，只要有消费者确认了消息，MQ才能安全的把消息从队列中删除。 这里并没有用到超时机制，MQ仅通过Consumer的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMQ给了Consumer足够长的时间来处理消息。保证了数据的最终一致性。 还有几种情况： 如果消费者接受到消息，在确认之前断开了连接或取消订阅，RabbitMQ会认为消息没有被分发，然后重新分发给下一个订阅的消费者。（可能存在消费重复的隐患，需要去重） 如果消费者接受到消息却没有确认消息，连接也未断开，则RabbitMQ认为该消费者繁忙。则不会给该消费者分发更多的消息。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:15:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"9个球，外观一样，但有一个球是重一点，怎么快速找到这个特殊的球？ 最快两次就好了，分三组，第一组和第二组对比，如果相等，那么第三组抽两个出来就选出来了，如果不相等，就在重的那组抽两个出来对比就好了。如果抽出来的这两个一样重，那么就是第三个了。如果倾向另一边，那么就是一边的重了。 没有算法，很奇怪，因为每次面试都要手撕代码，这次没有反而觉得缺了点什么。。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:16:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"关于MySQL的一些面试题 1. 内，左，右，全连接 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:0:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1.1 内连接 **内连接：inner join ** 返回的是两个表之间的交集 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:1:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1.2 左连接 **内连接：left join ** 返回的是左表的内容，右边中不符合条件的内容不会显示。 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:2:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1.3 右连接 **内连接：right join ** 返回的是右表的内容，左边中不符合条件的内容不会显示。 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:3:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1.4 全连接 关键词：union / union all 通过union连接的SQL，它们分别取出的列数必须相同； 不要求合并的表列名称相同时，以第一个sql 表列名为准； 使用union 时，完全相等的行，将会被合并，由于合并比较耗时，一般不直接使用 union 进行合并，而是通常采用union all 进行合并； 被union 连接的sql 子句，单个子句中不用写order by ，因为不会有排序的效果。但可以对最终的结果集进行排序； 2. redo，undo，bin 三大日志的用法 redo log：重做日志，用于记录数据修改后的记录，顺序记录 undo log：回滚日志，undo日志用于存放数据被修改前的值。用于回滚，保证了事务的一致性。 当 buffer pool 中的 dirty page 还没有刷新到磁盘的时候，发生crash，启动服务后，可通过redo log 找到需要重新刷新到磁盘文件的记录； buffer pool 中的数据直接flush到disk file，是一个随机IO，效率较差，而把buffer pool中的数据记录到redo log，是一个顺序IO，可以提高事务提交的速度 bin log：用于主从复制的交互的日志文件 3. MVCC的理解 维持一个数据的多版本，使读写操作没有冲突。 首先是获得事务版本号 获取一个readView 查询到数据与readView的事务版本号进行对比 不匹配的话就从undo log里获取历史版本数据 返回符合规则的数据。 4. MySQL 用索引和不用索引的时间复杂度 索引：每个结点 m 分支，log(m)n，另外结点内分 m 区间，二分查找，所以 m*log(m)n。m 是常数，所以 log（m）n。 不用索引：o(n) 底层是双向链表 5. MySQL的锁的实现 6. MySQL 的隔离级别 隔离级别 读数据一致性 脏读 不可重复读 幻读 未提交读（Read uncommitted） 最低级别，只能保证不读取物理上损坏的数据，事务可以看到其他事务没有被提交的数据（脏数据） 是 是 是 已提交度（Read committed） 语句级，事务可以看到其他事务已经提交的数据 否 是 是 可重复读（Repeatable read） 事务级，事务中两次查询的结果相同 否 否 是 可序列化（Serializable） 串行 否 否 否 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:4:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"【Golang开发面经】奇安信（两轮技术面） 写在前面 奇安信一顿面试下来感觉很一般。被面试官嘲讽了一波（ 不是搞安全的就不要去奇安信了。。 笔试 略 一面 项目问的比较多 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:0:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"说一下TCP的三次握手和四次挥手 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:1:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"TCP和UDP的区别 TCP 是可靠传输，面向连接，基于流，占用资源多，效率低。 UDP是尽最大努力交付，基于无连接，基于报文，UDP 占用系统资源较少，效率高。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:2:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"怎么让UDP变得可靠 可以的，我们只需要仿照TCP的可靠传输机制就可以了，比如说设置ACK确认机制，一旦没有收到，或是收到三次上一个报文的ACK，我们就立即重传丢失的报文。再比如说设置滑动窗口来保证数据传输的安全性等等… ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:3:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"写一下sql的查询语句，比如说我要查询id=3的user。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:4:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"说一下 InnoDB 的特性 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:5:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"讲讲mysql的索引 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:6:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"如何创建索引？ ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:7:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"画过流程图吗？我们工作要经常画这些图。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:8:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"算法：给你一个字符串 s 和一个整数 m ，请你找出 s 中的最长子串， 要求该子串中的每一字符出现次数都不少于 m 。返回这一子串的长度。 二面 有一道概率论的问题卡住了，被面试官嘲讽。 “就你这还xx学校，xx专业啊？” 不想写了，别去什么奇安信了，睿智公司。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:9:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"【Golang开发面经】360（一轮游） 写在前面 这个公司估计是走个形式…. 笔试 略 一面 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:0:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"TCP 和 UDP 区别 TCP 是可靠传输，面向连接，基于流，占用资源多，效率低。 UDP是尽最大努力交付，基于无连接，基于报文，UDP 占用系统资源较少，效率高。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:1:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"UDP能可靠传输吗？ 可以的，我们只需要仿照TCP的可靠传输机制就可以了，比如说设置ACK确认机制，一旦没有收到，或是收到三次上一个报文的ACK，我们就立即重传丢失的报文。再比如说设置滑动窗口来保证数据传输的安全性等等… ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:2:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"MYSQL的隔离等级 未提交读（READ UNCOMMITTED）：READ UNCOMMITTED 提供了事务之间最小限度的隔离。除了容易产生虚幻的读操作和不能重复的读操作外，处于这个隔离级的事务可以读到其他事务还没有提交的数据，如果这个事务使用其他事务不提交的变化作为计算的基础，然后那些未提交的变化被它们的父事务撤销，这就导致了大量的数据变化。 提交读（READ COMMITTED）：READ COMMITTED 隔离级别的安全性比 REPEATABLE READ 隔离级别的安全性要差。处于 READ COMMITTED 级别的事务可以看到其他事务对数据的修改。也就是说，在事务处理期间，如果其他事务修改了相应的表，那么同一个事务的多个 SELECT 语句可能返回不同的结果。 可重复读（REPEATABLE READ）：在可重复读在这一隔离级别上，事务不会被看成是一个序列。不过，当前正在执行事务的变化仍然不能被外部看到，也就是说，如果用户在另外一个事务中执行同条 SELECT 语句数次，结果总是相同的。（因为正在执行的事务所产生的数据变化不能被外部看到）。 序列化（SERIALIZABLE）：如果隔离级别为序列化，则用户之间通过一个接一个顺序地执行当前的事务，这种隔离级别提供了事务之间最大限度的隔离。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:3:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"左连接，右连接有什么区别？ 左连接：只要左边表中有记录，数据就能检索出来，而右边有的记录必要在左边表中有的记录才能被检索出来。 右连接：右连接是只要右边表中有记录，数据就能检索出来。 右连接与左连接相反，左连接A LEFT JOIN B，连接查询的数据，在A中必须有，在B中可以有可以没有。右连接A INNER JOIN B，在A中也有，在B中也有的数据才能查询出来。 左连接是已左边表中的数据为基准，若左表有数据右表没有数据，则显示左表中的数据右表中的数据显示为空。右联接是左向外联接的反向联接，将返回右表的所有行。如果右表的某行在左表中没有匹配行，则将为左表返回空值。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:4:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"JOIN的性能一定好吗？ 小表在前可以提高sql执行效率。 尽量不要使用where语句。 从上面例子可以看出，尽可能满足ON的条件，而少用Where的条件。从执行性能来看第二个显然更加省时。 Inner join 是不分主从表的，结果是取两个表针对 On 条件相匹配的最小集。如果是很大的表，首先针对两张表做Where条件筛选，然后再做 Join。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:5:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"线程的通信方式？ 消息队列、内存共享、信号 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:6:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"快排原理？堆排呢？ 快速排序实现的重点在于数组的拆分，通常我们将数组的第一个元素定义为比较元素，然后将数组中小于比较元素的数放到左边，将大于比较元素的放到右边，这样我们就将数组拆分成了左右两部分：小于比较元素的数组；大于比较元素的数组。我们再对这两个数组进行同样的拆分，直到拆分到不能再拆分，数组就自然而然地以升序排列了。 堆排序的基本思想是：将待排序序列构造成一个大顶堆，此时，整个序列的最大值就是堆顶的根节点。将其与末尾元素进行交换，此时末尾就为最大值。然后将剩余 n-1 个元素重新构造成一个堆，这样会得到n个元素的次小值。如此反复执行，便能得到一个有序序列了。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:7:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"算法：top k 问题 估计是不招人了，一面后一点消息也没有。。。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:8:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"Golang-Interview 记录实习、秋招中大小公司的面经，包括字节跳动，腾讯，滴滴，百度等等… 公司涉及 字节跳动 B站 百度 滴滴 腾讯 360 深信服 奇安信 得物 蔚来 小米 ","date":"0001-01-01 00:00:00","objectID":"/readme/:0:0","tags":null,"title":"","uri":"/readme/"}]