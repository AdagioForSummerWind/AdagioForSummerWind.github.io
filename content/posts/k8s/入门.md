## 生产环境

生产质量的 Kubernetes 集群需要规划和准备。 如果你的 Kubernetes 集群是用来运行关键负载的，该集群必须被配置为弹性的（Resilient）。 本页面阐述你在安装生产就绪的集群或将现有集群升级为生产用途时可以遵循的步骤。 如果你已经熟悉生产环境安装，因此只关注一些链接，则可以跳到[接下来](https://kubernetes.io/zh-cn/docs/setup/production-environment/#what-s-next)节。

### 生产环境考量[](https://kubernetes.io/zh-cn/docs/setup/production-environment/#production-considerations)

通常，一个生产用 Kubernetes 集群环境与个人学习、开发或测试环境所使用的 Kubernetes 相比有更多的需求。 生产环境可能需要被很多用户安全地访问，需要提供一致的可用性，以及能够与需求变化相适配的资源。

在你决定在何处运行你的生产用 Kubernetes 环境（在本地或者在云端）， 以及你希望承担或交由他人承担的管理工作量时， 需要考察以下因素如何影响你对 Kubernetes 集群的需求：

- **可用性**：一个单机的 Kubernetes [学习环境](https://kubernetes.io/zh-cn/docs/setup/#%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83) 具有单点失效特点。创建高可用的集群则意味着需要考虑：
    - 将控制面与工作节点分开
    - 在多个节点上提供控制面组件的副本
    - 为针对集群的 [API 服务器](https://kubernetes.io/zh-cn/docs/concepts/overview/components/#kube-apiserver) 的流量提供负载均衡
    - 随着负载的合理需要，提供足够的可用的（或者能够迅速变为可用的）工作节点

- **规模**：如果你预期你的生产用 Kubernetes 环境要承受固定量的请求， 你可能可以针对所需要的容量来一次性完成安装。 不过，如果你预期服务请求会随着时间增长，或者因为类似季节或者特殊事件的原因而发生剧烈变化， 你就需要规划如何处理请求上升时对控制面和工作节点的压力，或者如何缩减集群规模以减少未使用资源的消耗。

- **安全性与访问管理**：在你自己的学习环境 Kubernetes 集群上，你拥有完全的管理员特权。 但是针对运行着重要工作负载的共享集群，用户账户不止一两个时， 就需要更细粒度的方案来确定谁或者哪些主体可以访问集群资源。 你可以使用基于角色的访问控制（[RBAC](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/)） 和其他安全机制来确保用户和负载能够访问到所需要的资源， 同时确保工作负载及集群自身仍然是安全的。 你可以通过管理[策略](https://kubernetes.io/zh-cn/docs/concepts/policy/)和 [容器资源](https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers) 来针对用户和工作负载所可访问的资源设置约束。

在自行构建 Kubernetes 生产环境之前， 请考虑将这一任务的部分或者全部交给[云方案承包服务](https://kubernetes.io/zh-cn/docs/setup/production-environment/turnkey-solutions)提供商或者其他 [Kubernetes 合作伙伴](https://kubernetes.io/zh-cn/partners/)。选项有：

- **无服务**：仅是在第三方设备上运行负载，完全不必管理集群本身。 你需要为 CPU 用量、内存和磁盘请求等付费。
- **托管控制面**：让供应商决定集群控制面的规模和可用性，并负责打补丁和升级等操作。
- **托管工作节点**：配置一个节点池来满足你的需要，由供应商来确保节点始终可用，并在需要的时候完成升级。
- **集成**：有一些供应商能够将 Kubernetes 与一些你可能需要的其他服务集成， 这类服务包括存储、容器镜像仓库、身份认证方法以及开发工具等。

无论你是自行构造一个生产用 Kubernetes 集群还是与合作伙伴一起协作， 请审阅下面章节以评估你的需求，因为这关系到你的集群的**控制面**、**工作节点**、**用户访问**以及**负载资源**。

### 生产用集群安装[](https://kubernetes.io/zh-cn/docs/setup/production-environment/#production-cluster-setup)

在生产质量的 Kubernetes 集群中，控制面用不同的方式来管理集群和可以分布到多个计算机上的服务。 每个工作节点则代表的是一个可配置来运行 Kubernetes Pod 的实体。

#### 生产用控制面[](https://kubernetes.io/zh-cn/docs/setup/production-environment/#production-control-plane)

最简单的 Kubernetes 集群中，整个控制面和工作节点服务都运行在同一台机器上。 你可以通过添加工作节点来提升环境运算能力，正如 [Kubernetes 组件](https://kubernetes.io/zh-cn/docs/concepts/overview/components/)示意图所示。 如果只需要集群在很短的一段时间内可用，或者可以在某些事物出现严重问题时直接丢弃， 这种配置可能符合你的需要。

如果你需要一个更为持久的、高可用的集群，那么就需要考虑扩展控制面的方式。 根据设计，运行在一台机器上的单机控制面服务不是高可用的。 如果你认为保持集群的正常运行并需要确保它在出错时可以被修复是很重要的， 可以考虑以下步骤：

- **选择部署工具**：你可以使用类似 kubeadm、kops 和 kubespray 这类工具来部署控制面。 参阅[使用部署工具安装 Kubernetes](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/) 以了解使用这类部署方法来完成生产就绪部署的技巧。 存在不同的[容器运行时](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/) 可供你的部署采用。

- **管理证书**：控制面服务之间的安全通信是通过证书来完成的。 证书是在部署期间自动生成的，或者你也可以使用自己的证书机构来生成它们。 参阅 [PKI 证书和需求](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/)了解细节。

- **为 API 服务器配置负载均衡**：配置负载均衡器来将外部的 API 请求散布给运行在不同节点上的 API 服务实例。 参阅[创建外部负载均衡器](https://kubernetes.io/zh-cn/docs/tasks/access-application-cluster/create-external-load-balancer/)了解细节。

- **分离并备份 etcd 服务**：etcd 服务可以运行于其他控制面服务所在的机器上， 也可以运行在不同的机器上以获得更好的安全性和可用性。 因为 etcd 存储着集群的配置数据，应该经常性地对 etcd 数据库进行备份， 以确保在需要的时候你可以修复该数据库。与配置和使用 etcd 相关的细节可参阅 [etcd FAQ](https://kubernetes.io/https://etcd.io/docs/v3.5/faq/)。 更多的细节可参阅[为 Kubernetes 运维 etcd 集群](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/) 和[使用 kubeadm 配置高可用的 etcd 集群](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)。

- **创建多控制面系统**：为了实现高可用性，控制面不应被限制在一台机器上。 如果控制面服务是使用某 init 服务（例如 systemd）来运行的，每个服务应该至少运行在三台机器上。 不过，将控制面作为服务运行在 Kubernetes Pod 中可以确保你所请求的个数的服务始终保持可用。 调度器应该是可容错的，但不是高可用的。 某些部署工具会安装 [Raft](https://raft.github.io/) 票选算法来对 Kubernetes 服务执行领导者选举。 如果主节点消失，另一个服务会被选中并接手相应服务。

- **跨多个可用区**：如果保持你的集群一直可用这点非常重要，可以考虑创建一个跨多个数据中心的集群； 在云环境中，这些数据中心被视为可用区。若干个可用区在一起可构成地理区域。 通过将集群分散到同一区域中的多个可用区内，即使某个可用区不可用，整个集群能够继续工作的机会也大大增加。 更多的细节可参阅[跨多个可用区运行](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/)。

- **管理演进中的特性**：如果你计划长时间保留你的集群，就需要执行一些维护其健康和安全的任务。 例如，如果你采用 kubeadm 安装的集群， 则有一些可以帮助你完成 [证书管理](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/) 和[升级 kubeadm 集群](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade) 的指令。 参见[管理集群](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster)了解一个 Kubernetes 管理任务的较长列表。

如要了解运行控制面服务时可使用的选项，可参阅 [kube-apiserver](https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/)、 [kube-controller-manager](https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-controller-manager/) 和 [kube-scheduler](https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/) 组件参考页面。 如要了解高可用控制面的例子，可参阅[高可用拓扑结构选项](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/)、 [使用 kubeadm 创建高可用集群](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/) 以及[为 Kubernetes 运维 etcd 集群](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/)。 关于制定 etcd 备份计划，可参阅[对 etcd 集群执行备份](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster)。

#### 生产用工作节点[](https://kubernetes.io/zh-cn/docs/setup/production-environment/#production-worker-nodes)

生产质量的工作负载需要是弹性的；它们所依赖的其他组件（例如 CoreDNS）也需要是弹性的。 无论你是自行管理控制面还是让云供应商来管理，你都需要考虑如何管理工作节点 （有时也简称为**节点**）。

- **配置节点**：节点可以是物理机或者虚拟机。如果你希望自行创建和管理节点， 你可以安装一个受支持的操作系统，之后添加并运行合适的[节点服务](https://kubernetes.io/zh-cn/docs/concepts/overview/components/#node-components)。考虑：
    - 在安装节点时要通过配置适当的内存、CPU 和磁盘读写速率、存储容量来满足你的负载的需求。
    - 是否通用的计算机系统即足够，还是你有负载需要使用 GPU 处理器、Windows 节点或者 VM 隔离。

- **验证节点**：参阅[验证节点配置](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/)以了解如何确保节点满足加入到 Kubernetes 集群的需求。

- **添加节点到集群中**：如果你自行管理你的集群，你可以通过安装配置你的机器， 之后或者手动加入集群，或者让它们自动注册到集群的 API 服务器。 参阅[节点](https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/)节，了解如何配置 Kubernetes 以便以这些方式来添加节点。

- **扩缩节点**：制定一个扩充集群容量的规划，你的集群最终会需要这一能力。 参阅[大规模集群考察事项](https://kubernetes.io/zh-cn/docs/setup/best-practices/cluster-large/) 以确定你所需要的节点数； 这一规模是基于你要运行的 Pod 和容器个数来确定的。 如果你自行管理集群节点，这可能意味着要购买和安装你自己的物理设备。

- **节点自动扩缩容**：大多数云供应商支持 [集群自动扩缩器（Cluster Autoscaler）](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme) 以便替换不健康的节点、根据需求来增加或缩减节点个数。 参阅[常见问题](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md) 了解自动扩缩器的工作方式，并参阅 [Deployment](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment) 了解不同云供应商是如何实现集群自动扩缩器的。 对于本地集群，有一些虚拟化平台可以通过脚本来控制按需启动新节点。

- **安装节点健康检查**：对于重要的工作负载，你会希望确保节点以及在节点上运行的 Pod 处于健康状态。 通过使用 [Node Problem Detector](https://kubernetes.io/zh-cn/docs/tasks/debug/debug-cluster/monitor-node-health/)， 你可以确保你的节点是健康的。

#### 生产级用户环境[](https://kubernetes.io/zh-cn/docs/setup/production-environment/#production-user-management)

在生产环境中，情况可能不再是你或者一小组人在访问集群，而是几十上百人需要访问集群。 在学习环境或者平台原型环境中，你可能具有一个可以执行任何操作的管理账号。 在生产环境中，你会需要对不同名字空间具有不同访问权限级别的很多账号。

建立一个生产级别的集群意味着你需要决定如何有选择地允许其他用户访问集群。 具体而言，你需要选择验证尝试访问集群的人的身份标识（身份认证）， 并确定他们是否被许可执行他们所请求的操作（鉴权）：

- **认证（Authentication）**：API 服务器可以使用客户端证书、持有者令牌、 身份认证代理或者 HTTP 基本认证机制来完成身份认证操作。 你可以选择你要使用的认证方法。通过使用插件， API 服务器可以充分利用你所在组织的现有身份认证方法， 例如 LDAP 或者 Kerberos。 关于认证 Kubernetes 用户身份的不同方法的描述， 可参阅[身份认证](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/authentication/)。

- **鉴权（Authorization）**：当你准备为一般用户执行权限判定时， 你可能会需要在 RBAC 和 ABAC 鉴权机制之间做出选择。 参阅[鉴权概述](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/authorization/)， 了解对用户账户（以及访问你的集群的服务账户）执行鉴权的不同模式。
    
    - **基于角色的访问控制**（[RBAC](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/)）： 让你通过为通过身份认证的用户授权特定的许可集合来控制集群访问。 访问许可可以针对某特定名字空间（Role）或者针对整个集群（ClusterRole）。 通过使用 RoleBinding 和 ClusterRoleBinding 对象，这些访问许可可以被关联到特定的用户身上。
    
    - **基于属性的访问控制**（[ABAC](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/abac/)）： 让你能够基于集群中资源的属性来创建访问控制策略，基于对应的属性来决定允许还是拒绝访问。 策略文件的每一行都给出版本属性（apiVersion 和 kind）以及一个规约属性的映射， 用来匹配主体（用户或组）、资源属性、非资源属性（/version 或 /apis）和只读属性。 参阅[示例](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/abac/#examples)以了解细节。

作为在你的生产用 Kubernetes 集群中安装身份认证和鉴权机制的负责人，要考虑的事情如下：

- **设置鉴权模式**：当 Kubernetes API 服务器（[kube-apiserver](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/)）启动时， 所支持的鉴权模式必须使用 `--authorization-mode` 标志配置。 例如，`kube-apiserver.yaml`（位于 `/etc/kubernetes/manifests` 下）中对应的标志可以设置为 `Node,RBAC`。 这样就会针对已完成身份认证的请求执行 Node 和 RBAC 鉴权。

- **创建用户证书和角色绑定（RBAC）**：如果你在使用 RBAC 鉴权，用户可以创建由集群 CA 签名的 CertificateSigningRequest（CSR）。接下来你就可以将 Role 和 ClusterRole 绑定到每个用户身上。 参阅[证书签名请求](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/)了解细节。

- **创建组合属性的策略（ABAC）**：如果你在使用 ABAC 鉴权， 你可以设置属性组合以构造策略对所选用户或用户组执行鉴权， 判定他们是否可访问特定的资源（例如 Pod）、名字空间或者 apiGroup。 进一步的详细信息可参阅[示例](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/abac/#examples)。

- **考虑准入控制器**：针对指向 API 服务器的请求的其他鉴权形式还包括 [Webhook 令牌认证](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/authentication/#webhook-token-authentication)。 Webhook 和其他特殊的鉴权类型需要通过向 API 服务器添加[准入控制器](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/)来启用。

### 为负载资源设置约束[](https://kubernetes.io/zh-cn/docs/setup/production-environment/#set-limits-on-workload-resources)

生产环境负载的需求可能对 Kubernetes 的控制面内外造成压力。 在针对你的集群的负载执行配置时，要考虑以下条目：

- **设置名字空间限制**：为每个名字空间的内存和 CPU 设置配额。 参阅[管理内存、CPU 和 API 资源](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/manage-resources/)以了解细节。 你也可以设置[层次化名字空间](https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/)来继承这类约束。

- **为 DNS 请求做准备**：如果你希望工作负载能够完成大规模扩展，你的 DNS 服务也必须能够扩大规模。 参阅[自动扩缩集群中 DNS 服务](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)。

- **创建额外的服务账户**：用户账户决定用户可以在集群上执行的操作，服务账号则定义的是在特定名字空间中 Pod 的访问权限。默认情况下，Pod 使用所在名字空间中的 default 服务账号。 参阅[管理服务账号](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/)以了解如何创建新的服务账号。 例如，你可能需要：
    - 为 Pod 添加 Secret，以便 Pod 能够从某特定的容器镜像仓库拉取镜像。 参阅[为 Pod 配置服务账号](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-service-account/)以获得示例。
    - 为服务账号设置 RBAC 访问许可。参阅[服务账号访问许可](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/#service-account-permissions)了解细节。

### 接下来[](https://kubernetes.io/zh-cn/docs/setup/production-environment/#%E6%8E%A5%E4%B8%8B%E6%9D%A5)

- 决定你是想自行构造自己的生产用 Kubernetes， 还是从某可用的[云服务外包厂商](https://kubernetes.io/zh-cn/docs/setup/production-environment/turnkey-solutions/)或 [Kubernetes 合作伙伴](https://kubernetes.io/zh-cn/partners/)获得集群。
- 如果你决定自行构造集群，则需要规划如何处理[证书](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/)并为类似 [etcd](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/) 和 [API 服务器](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/)这些功能组件配置高可用能力。

- 选择使用 [kubeadm](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/)、 [kops](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/) 或 [Kubespray](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/) 作为部署方法。

- 通过决定[身份认证](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/authentication/)和[鉴权](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/authorization/)方法来配置用户管理。

- 通过配置[资源限制](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/manage-resources/)、 [DNS 自动扩缩](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)和[服务账号](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/)来为应用负载作准备。


### 容器运行时

**说明：** 自 1.24 版起，Dockershim 已从 Kubernetes 项目中移除。阅读 [Dockershim 移除的常见问题](https://kubernetes.io/zh-cn/dockershim)了解更多详情。

你需要在集群内每个节点上安装一个 [容器运行时](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes) 以使 Pod 可以运行在上面。本文概述了所涉及的内容并描述了与节点设置相关的任务。

Kubernetes 1.26 要求你使用符合[容器运行时接口](https://kubernetes.io/zh-cn/docs/concepts/overview/components/#container-runtime)（CRI）的运行时。

有关详细信息，请参阅 [CRI 版本支持](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cri-versions)。 本页简要介绍在 Kubernetes 中几个常见的容器运行时的用法。

- [containerd](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#containerd)
- [CRI-O](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cri-o)
- [Docker Engine](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#docker)
- [Mirantis Container Runtime](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#mcr)

**说明：**

v1.24 之前的 Kubernetes 版本直接集成了 Docker Engine 的一个组件，名为 **dockershim**。 这种特殊的直接整合不再是 Kubernetes 的一部分 （这次删除被作为 v1.20 发行版本的一部分[宣布](https://kubernetes.io/zh-cn/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation)）。

你可以阅读[检查 Dockershim 移除是否会影响你](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/)以了解此删除可能会如何影响你。 要了解如何使用 dockershim 进行迁移， 请参阅[从 dockershim 迁移](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/)。

如果你正在运行 v1.26 以外的 Kubernetes 版本，查看对应版本的文档。

#### 安装和配置先决条件[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#install-and-configure-prerequisites)

以下步骤将通用设置应用于 Linux 上的 Kubernetes 节点。

如果你确定不需要某个特定设置，则可以跳过它。

有关更多信息，请参阅[网络插件要求](https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements)或特定容器运行时的文档。

##### 转发 IPv4 并让 iptables 看到桥接流量[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#%E8%BD%AC%E5%8F%91-ipv4-%E5%B9%B6%E8%AE%A9-iptables-%E7%9C%8B%E5%88%B0%E6%A1%A5%E6%8E%A5%E6%B5%81%E9%87%8F)

执行下述指令：

```bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# 设置所需的 sysctl 参数，参数在重新启动后保持不变
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# 应用 sysctl 参数而不重新启动
sudo sysctl --system
```

通过运行以下指令确认 `br_netfilter` 和 `overlay` 模块被加载：

```bash
lsmod | grep br_netfilter
lsmod | grep overlay
```

通过运行以下指令确认 `net.bridge.bridge-nf-call-iptables`、`net.bridge.bridge-nf-call-ip6tables` 和 `net.ipv4.ip_forward` 系统变量在你的 `sysctl` 配置中被设置为 1：

```bash
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
```

#### cgroup 驱动[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cgroup-drivers)

在 Linux 上，[控制组（CGroup）](https://kubernetes.io/zh-cn/docs/reference/glossary/?all=true#term-cgroup)用于限制分配给进程的资源。

[kubelet](https://kubernetes.io/docs/reference/generated/kubelet) 和底层容器运行时都需要对接控制组来强制执行 [为 Pod 和容器管理资源](https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/) 并为诸如 CPU、内存这类资源设置请求和限制。若要对接控制组，kubelet 和容器运行时需要使用一个 **cgroup 驱动**。 关键的一点是 kubelet 和容器运行时需使用相同的 cgroup 驱动并且采用相同的配置。

可用的 cgroup 驱动有两个：

- [`cgroupfs`](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cgroupfs-cgroup-driver)
- [`systemd`](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#systemd-cgroup-driver)

##### cgroupfs 驱动[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cgroupfs-cgroup-driver)

`cgroupfs` 驱动是 kubelet 中默认的 cgroup 驱动。当使用 `cgroupfs` 驱动时， kubelet 和容器运行时将直接对接 cgroup 文件系统来配置 cgroup。

当 [systemd](https://www.freedesktop.org/wiki/Software/systemd/) 是初始化系统时， **不** 推荐使用 `cgroupfs` 驱动，因为 systemd 期望系统上只有一个 cgroup 管理器。 此外，如果你使用 [cgroup v2](https://kubernetes.io/zh-cn/docs/concepts/architecture/cgroups)， 则应用 `systemd` cgroup 驱动取代 `cgroupfs`。

##### systemd cgroup 驱动[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#systemd-cgroup-driver)

当某个 Linux 系统发行版使用 [systemd](https://www.freedesktop.org/wiki/Software/systemd/) 作为其初始化系统时，初始化进程会生成并使用一个 root 控制组（`cgroup`），并充当 cgroup 管理器。

systemd 与 cgroup 集成紧密，并将为每个 systemd 单元分配一个 cgroup。 因此，如果你 `systemd` 用作初始化系统，同时使用 `cgroupfs` 驱动，则系统中会存在两个不同的 cgroup 管理器。

同时存在两个 cgroup 管理器将造成系统中针对可用的资源和使用中的资源出现两个视图。某些情况下， 将 kubelet 和容器运行时配置为使用 `cgroupfs`、但为剩余的进程使用 `systemd` 的那些节点将在资源压力增大时变得不稳定。

当 systemd 是选定的初始化系统时，缓解这个不稳定问题的方法是针对 kubelet 和容器运行时将 `systemd` 用作 cgroup 驱动。

要将 `systemd` 设置为 cgroup 驱动，需编辑 [`KubeletConfiguration`](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/) 的 `cgroupDriver` 选项，并将其设置为 `systemd`。例如：

```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
...
cgroupDriver: systemd
```

如果你将 `systemd` 配置为 kubelet 的 cgroup 驱动，你也必须将 `systemd` 配置为容器运行时的 cgroup 驱动。 参阅容器运行时文档，了解指示说明。例如：

- [containerd](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#containerd-systemd)
- [CRI-O](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cri-o)

**注意：**

注意：更改已加入集群的节点的 cgroup 驱动是一项敏感的操作。 如果 kubelet 已经使用某 cgroup 驱动的语义创建了 Pod，更改运行时以使用别的 cgroup 驱动，当为现有 Pod 重新创建 PodSandbox 时会产生错误。 重启 kubelet 也可能无法解决此类问题。

如果你有切实可行的自动化方案，使用其他已更新配置的节点来替换该节点， 或者使用自动化方案来重新安装。

##### 将 kubeadm 管理的集群迁移到 `systemd` 驱动[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#%E5%B0%86-kubeadm-%E7%AE%A1%E7%90%86%E7%9A%84%E9%9B%86%E7%BE%A4%E8%BF%81%E7%A7%BB%E5%88%B0-systemd-%E9%A9%B1%E5%8A%A8)

如果你希望将现有的由 kubeadm 管理的集群迁移到 `systemd` cgroup 驱动， 请按照[配置 cgroup 驱动](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/)操作。

#### CRI 版本支持[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cri-versions)

你的容器运行时必须至少支持 v1alpha2 版本的容器运行时接口。

Kubernetes 1.26 默认使用 v1 版本的 CRI API。如果容器运行时不支持 v1 版本的 API， 则 kubelet 会回退到使用（已弃用的）v1alpha2 版本的 API。

#### 容器运行时[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6)

**说明：** 本部分链接到提供 Kubernetes 所需功能的第三方项目。Kubernetes 项目作者不负责这些项目。此页面遵循[CNCF 网站指南](https://github.com/cncf/foundation/blob/master/website-guidelines.md)，按字母顺序列出项目。要将项目添加到此列表中，请在提交更改之前阅读[内容指南](https://kubernetes.io/docs/contribute/style/content-guide/#third-party-content)。

##### containerd[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#containerd)

本节概述了使用 containerd 作为 CRI 运行时的必要步骤。

使用以下命令在系统上安装 Containerd：

按照[开始使用 containerd](https://github.com/containerd/containerd/blob/main/docs/getting-started.md) 的说明进行操作。 创建有效的配置文件 `config.toml` 后返回此步骤。

- [Linux](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#%e6%89%be%e5%88%b0-config-toml-%e6%96%87%e4%bb%b6-0)
- [Windows](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#%e6%89%be%e5%88%b0-config-toml-%e6%96%87%e4%bb%b6-1)

你可以在路径 `/etc/containerd/config.toml` 下找到此文件。

你可以在路径 `C:\Program Files\containerd\config.toml` 下找到此文件。

在 Linux 上，containerd 的默认 CRI 套接字是 `/run/containerd/containerd.sock`。 在 Windows 上，默认 CRI 端点是 `npipe://./pipe/containerd-containerd`。

###### 配置 `systemd` cgroup 驱动[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#containerd-systemd)

结合 `runc` 使用 `systemd` cgroup 驱动，在 `/etc/containerd/config.toml` 中设置：

```
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  ...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true
```

如果你使用 [cgroup v2](https://kubernetes.io/zh-cn/docs/concepts/architecture/cgroups)，则推荐 `systemd` cgroup 驱动。

**说明：**

如果你从软件包（例如，RPM 或者 `.deb`）中安装 containerd，你可能会发现其中默认禁止了 CRI 集成插件。

你需要启用 CRI 支持才能在 Kubernetes 集群中使用 containerd。 要确保 `cri` 没有出现在 `/etc/containerd/config.toml` 文件中 `disabled_plugins` 列表内。如果你更改了这个文件，也请记得要重启 `containerd`。

如果你在初次安装集群后或安装 CNI 后遇到容器崩溃循环，则随软件包提供的 containerd 配置可能包含不兼容的配置参数。考虑按照 [getting-started.md](https://github.com/containerd/containerd/blob/main/docs/getting-started.md#advanced-topics) 中指定的 `containerd config default > /etc/containerd/config.toml` 重置 containerd 配置，然后相应地设置上述配置参数。

如果你应用此更改，请确保重新启动 containerd：

```shell
sudo systemctl restart containerd
```

当使用 kubeadm 时，请手动配置 [kubelet 的 cgroup 驱动](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/#configuring-the-kubelet-cgroup-driver)。

###### 重载沙箱（pause）镜像[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#override-pause-image-containerd)

在你的 [containerd 配置](https://github.com/containerd/containerd/blob/main/docs/cri/config.md)中， 你可以通过设置以下选项重载沙箱镜像：

```toml
[plugins."io.containerd.grpc.v1.cri"]
  sandbox_image = "registry.k8s.io/pause:3.2"
```

一旦你更新了这个配置文件，可能就同样需要重启 `containerd`：`systemctl restart containerd`。

##### CRI-O[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cri-o)

本节包含安装 CRI-O 作为容器运行时的必要步骤。

要安装 CRI-O，请按照 [CRI-O 安装说明](https://github.com/cri-o/cri-o/blob/main/install.md#readme)执行操作。

###### cgroup 驱动[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cgroup-driver)

CRI-O 默认使用 systemd cgroup 驱动，这对你来说可能工作得很好。 要切换到 `cgroupfs` cgroup 驱动，请编辑 `/etc/crio/crio.conf` 或在 `/etc/crio/crio.conf.d/02-cgroup-manager.conf` 中放置一个插入式配置，例如：

```toml
[crio.runtime]
conmon_cgroup = "pod"
cgroup_manager = "cgroupfs"
```

你还应该注意当使用 CRI-O 时，并且 CRI-O 的 cgroup 设置为 `cgroupfs` 时，必须将 `conmon_cgroup` 设置为值 `pod`。 通常需要保持 kubelet 的 cgroup 驱动配置（通常通过 kubeadm 完成）和 CRI-O 同步。

对于 CRI-O，CRI 套接字默认为 `/var/run/crio/crio.sock`。

###### 重载沙箱（pause）镜像[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#override-pause-image-cri-o)

在你的 [CRI-O 配置](https://github.com/cri-o/cri-o/blob/main/docs/crio.conf.5.md)中， 你可以设置以下配置值：

```toml
[crio.image]
pause_image="registry.k8s.io/pause:3.6"
```

这一设置选项支持动态配置重加载来应用所做变更：`systemctl reload crio`。 也可以通过向 `crio` 进程发送 `SIGHUP` 信号来实现。

##### Docker Engine[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#docker)

**说明：**

以下操作假设你使用 [`cri-dockerd`](https://github.com/Mirantis/cri-dockerd) 适配器来将 Docker Engine 与 Kubernetes 集成。

1. 在你的每个节点上，遵循[安装 Docker Engine](https://docs.docker.com/engine/install/#server) 指南为你的 Linux 发行版安装 Docker。

2. 按照源代码仓库中的说明安装 [`cri-dockerd`](https://github.com/Mirantis/cri-dockerd)。

对于 `cri-dockerd`，默认情况下，CRI 套接字是 `/run/cri-dockerd.sock`。

##### Mirantis 容器运行时[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#mcr)

[Mirantis Container Runtime](https://docs.mirantis.com/mcr/20.10/overview.html) (MCR) 是一种商用容器运行时，以前称为 Docker 企业版。 你可以使用 MCR 中包含的开源 [`cri-dockerd`](https://github.com/Mirantis/cri-dockerd) 组件将 Mirantis Container Runtime 与 Kubernetes 一起使用。

要了解有关如何安装 Mirantis Container Runtime 的更多信息， 请访问 [MCR 部署指南](https://docs.mirantis.com/mcr/20.10/install.html)。

检查名为 `cri-docker.socket` 的 systemd 单元以找出 CRI 套接字的路径。

###### 重载沙箱（pause）镜像[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#override-pause-image-cri-dockerd-mcr)

`cri-dockerd` 适配器能够接受指定用作 Pod 的基础容器的容器镜像（“pause 镜像”）作为命令行参数。 要使用的命令行参数是 `--pod-infra-container-image`。

#### 接下来[](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#%E6%8E%A5%E4%B8%8B%E6%9D%A5)

除了容器运行时，你的集群还需要有效的[网络插件](https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model)。


### 使用部署工具安装 Kubernetes

---

#### [使用 kubeadm 引导集群](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/)
##### [安装 kubeadm](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)

##### [对 kubeadm 进行故障排查](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/)

##### [使用 kubeadm 创建集群](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)

##### [使用 kubeadm API 定制组件](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/control-plane-flags/)

##### [高可用拓扑选项](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/)

##### [利用 kubeadm 创建高可用集群](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/)

##### [使用 kubeadm 创建一个高可用 etcd 集群](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)

##### [使用 kubeadm 配置集群中的每个 kubelet](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/kubelet-integration/)

##### [使用 kubeadm 支持双协议栈](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/dual-stack-support/)
#### [使用 kOps 安装 Kubernetes](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/)
本篇快速入门介绍了如何在 AWS 上轻松安装 Kubernetes 集群。 本篇使用了一个名为 [`kOps`](https://github.com/kubernetes/kops) 的工具。

`kOps` 是一个自动化的制备系统：

- 全自动安装流程
- 使用 DNS 识别集群
- 自我修复：一切都在自动扩缩组中运行
- 支持多种操作系统（Amazon Linux、Debian、Flatcar、RHEL、Rocky 和 Ubuntu）， 参考 [images.md](https://github.com/kubernetes/kops/blob/master/docs/operations/images.md)。
- 支持高可用，参考 [high\_availability.md](https://github.com/kubernetes/kops/blob/master/docs/operations/high_availability.md)。
- 可以直接提供或者生成 terraform 清单，参考 [terraform.md](https://github.com/kubernetes/kops/blob/master/docs/terraform.md)。

##### 准备开始[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B)

- 你必须安装 [kubectl](https://kubernetes.io/zh-cn/docs/tasks/tools/)。
- 你必须安装[安装](https://github.com/kubernetes/kops#installing) `kops` 到 64 位的（AMD64 和 Intel 64）设备架构上。
- 你必须拥有一个 [AWS 账户](https://docs.aws.amazon.com/zh_cn/polly/latest/dg/setting-up.html)， 生成 [IAM 秘钥](https://docs.aws.amazon.com/zh_cn/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys) 并[配置](https://docs.aws.amazon.com/zh_cn/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration) 该秘钥。IAM 用户需要[足够的权限许可](https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user)。

##### 创建集群[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#creating-a-cluster)

###### (1/5) 安装 kops[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#1-5-%E5%AE%89%E8%A3%85-kops)

####### 安装[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#%E5%AE%89%E8%A3%85)

从[下载页面](https://github.com/kubernetes/kops/releases)下载 kops （从源代码构建也很方便）：

- [macOS](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#kops-installation-0)
- [Linux](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#kops-installation-1)

使用下面的命令下载最新发布版本：

```shell
curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-darwin-amd64
```

要下载特定版本，使用特定的 kops 版本替换下面命令中的部分：

```shell
$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)
```

例如，要下载 kops v1.20.0，输入：

```shell
curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-darwin-amd64
```

令 kops 二进制文件可执行：

```shell
chmod +x kops-darwin-amd64
```

将 kops 二进制文件移到你的 PATH 下：

```shell
sudo mv kops-darwin-amd64 /usr/local/bin/kops
```

你也可以使用 [Homebrew](https://brew.sh/) 安装 kops：

```shell
brew update && brew install kops
```

使用命令下载最新发布版本：

```shell
curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
```

要下载 kops 的特定版本，用特定的 kops 版本替换下面命令中的部分：

```shell
$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)
```

例如，要下载 kops v1.20 版本，输入：

```shell
curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64
```

令 kops 二进制文件可执行：

```shell
chmod +x kops-linux-amd64
```

将 kops 二进制文件移到 PATH 下：

```shell
sudo mv kops-linux-amd64 /usr/local/bin/kops
```

你也可以使用 [Homebrew](https://docs.brew.sh/Homebrew-on-Linux) 来安装 kops。

```shell
brew update && brew install kops
```

###### (2/5) 为你的集群创建一个 route53 域名[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#2-5-%E4%B8%BA%E4%BD%A0%E7%9A%84%E9%9B%86%E7%BE%A4%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA-route53-%E5%9F%9F%E5%90%8D)

kops 在集群内部和外部都使用 DNS 进行发现操作，这样你可以从客户端访问 kubernetes API 服务器。

kops 对集群名称有明显的要求：它应该是有效的 DNS 名称。这样一来，你就不会再使集群混乱， 可以与同事明确共享集群，并且无需依赖记住 IP 地址即可访问集群。

你可以，或许应该使用子域名来划分集群。作为示例，我们将使用域名 `useast1.dev.example.com`。 这样，API 服务器端点域名将为 `api.useast1.dev.example.com`。

Route53 托管区域可以服务子域名。你的托管区域可能是 `useast1.dev.example.com`，还有 `dev.example.com` 甚至 `example.com`。 kops 可以与以上任何一种配合使用，因此通常你出于组织原因选择不同的托管区域。 例如，允许你在 `dev.example.com` 下创建记录，但不能在 `example.com` 下创建记录。

假设你使用 `dev.example.com` 作为托管区域。你可以使用 [正常流程](https://docs.aws.amazon.com/zh_cn/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html) 或者使用诸如 `aws route53 create-hosted-zone --name dev.example.com --caller-reference 1` 之类的命令来创建该托管区域。

然后，你必须在父域名中设置你的 DNS 记录，以便该域名中的记录可以被解析。 在这里，你将在 `example.com` 中为 `dev` 创建 DNS 记录。 如果它是根域名，则可以在域名注册机构配置 DNS 记录。 例如，你需要在购买 `example.com` 的地方配置 `example.com`。

检查你的 route53 域已经被正确设置（这是导致问题的最常见原因！）。 如果你安装了 dig 工具，则可以通过运行以下步骤再次检查集群是否配置正确：

`dig NS dev.example.com`

你应该看到 Route53 分配了你的托管区域的 4 条 DNS 记录。

###### (3/5) 创建一个 S3 存储桶来存储集群状态[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#3-5-%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA-s3-%E5%AD%98%E5%82%A8%E6%A1%B6%E6%9D%A5%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81)

kops 使你即使在安装后也可以管理集群。为此，它必须跟踪已创建的集群及其配置、所使用的密钥等。 此信息存储在 S3 存储桶中。S3 权限用于控制对存储桶的访问。

多个集群可以使用同一 S3 存储桶，并且你可以在管理同一集群的同事之间共享一个 S3 存储桶 - 这比传递 kubecfg 文件容易得多。 但是有权访问 S3 存储桶的任何人都将拥有对所有集群的管理访问权限， 因此你不想在运营团队之外共享它。

因此，通常每个运维团队都有一个 S3 存储桶（而且名称通常对应于上面托管区域的名称！）

在我们的示例中，我们选择 `dev.example.com` 作为托管区域，因此我们选择 `clusters.dev.example.com` 作为 S3 存储桶名称。

- 导出 `AWS_PROFILE` 文件（如果你需要选择一个配置文件用来使 AWS CLI 正常工作）
- 使用 `aws s3 mb s3://clusters.dev.example.com` 创建 S3 存储桶
- 你可以进行 `export KOPS_STATE_STORE=s3://clusters.dev.example.com` 操作， 然后 kops 将默认使用此位置。 我们建议将其放入你的 bash profile 文件或类似文件中。

###### (4/5) 建立你的集群配置[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#4-5-%E5%BB%BA%E7%AB%8B%E4%BD%A0%E7%9A%84%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE)

运行 `kops create cluster` 以创建你的集群配置：

`kops create cluster --zones=us-east-1c useast1.dev.example.com`

kops 将为你的集群创建配置。请注意，它**仅**创建配置，实际上并没有创建云资源。 你将在下一步中使用 `kops update cluster` 进行创建。 这使你有机会查看配置或进行更改。

它打印出可用于进一步探索的命令：

- 使用以下命令列出集群：`kops get cluster`
- 使用以下命令编辑该集群：`kops edit cluster useast1.dev.example.com`
- 使用以下命令编辑你的节点实例组：`kops edit ig --name = useast1.dev.example.com nodes`
- 使用以下命令编辑你的主实例组：`kops edit ig --name = useast1.dev.example.com master-us-east-1c`

如果这是你第一次使用 kops，请花几分钟尝试一下！实例组是一组实例，将被注册为 Kubernetes 节点。 在 AWS 上，这是通过 auto-scaling-groups 实现的。你可以有多个实例组。 例如，你可能想要混合了 Spot 实例和按需实例的节点，或者混合了 GPU 实例和非 GPU 实例的节点。

###### (5/5) 在 AWS 中创建集群[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#5-5-%E5%9C%A8-aws-%E4%B8%AD%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4)

运行 `kops update cluster` 以在 AWS 中创建集群：

`kops update cluster useast1.dev.example.com --yes`

这需要几秒钟的时间才能运行，但实际上集群可能需要几分钟才能准备就绪。 每当更改集群配置时，都会使用 `kops update cluster` 工具。 它将在集群中应用你对配置进行的更改，根据需要重新配置 AWS 或者 Kubernetes。

例如，在你运行 `kops edit ig nodes` 之后，然后运行 `kops update cluster --yes` 应用你的配置，有时你还必须运行 `kops rolling-update cluster` 立即回滚更新配置。

如果没有 `--yes` 参数，`kops update cluster` 操作将向你显示其操作的预览效果。这对于生产集群很方便！

###### 探索其他附加组件[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#%E6%8E%A2%E7%B4%A2%E5%85%B6%E4%BB%96%E9%99%84%E5%8A%A0%E7%BB%84%E4%BB%B6)

请参阅[附加组件列表](https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/addons/)探索其他附加组件， 包括用于 Kubernetes 集群的日志记录、监视、网络策略、可视化和控制的工具。

##### 清理[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#cleanup)

- 删除集群：`kops delete cluster useast1.dev.example.com --yes`

##### 接下来[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/#%E6%8E%A5%E4%B8%8B%E6%9D%A5)

- 了解有关 Kubernetes 的[概念](https://kubernetes.io/zh-cn/docs/concepts/)和 [`kubectl`](https://kubernetes.io/zh-cn/docs/reference/kubectl/) 的更多信息。
- 参阅 `kOps` [进阶用法](https://kops.sigs.k8s.io/) 获取教程、最佳实践和进阶配置选项。
- 通过 Slack：[社区讨论](https://github.com/kubernetes/kops#other-ways-to-communicate-with-the-contributors) 参与 `kOps` 社区讨论。
- 通过解决或提出一个 [GitHub Issue](https://github.com/kubernetes/kops/issues) 来为 `kOps` 做贡献。
#### [使用 Kubespray 安装 Kubernetes](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/)
此快速入门有助于使用 [Kubespray](https://github.com/kubernetes-sigs/kubespray) 安装在 GCE、Azure、OpenStack、AWS、vSphere、Equinix Metal（曾用名 Packet）、Oracle Cloud Infrastructure（实验性）或 Baremetal 上托管的 Kubernetes 集群。

Kubespray 是由若干 [Ansible](https://docs.ansible.com/) Playbook、 [清单（inventory）](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md#inventory)、 制备工具和通用 OS/Kubernetes 集群配置管理任务的领域知识组成的。

Kubespray 提供：

- 高可用性集群
- 可组合属性（例如可选择网络插件）
- 支持大多数流行的 Linux 发行版
    - Flatcar Container Linux
    - Debian Bullseye、Buster、Jessie、Stretch
    - Ubuntu 16.04、18.04、20.04、22.04
    - CentOS/RHEL 7、8、9
    - Fedora 35、36
    - Fedora CoreOS
    - openSUSE Leap 15.x/Tumbleweed
    - Oracle Linux 7、8、9
    - Alma Linux 8、9
    - Rocky Linux 8、9
    - Kylin Linux Advanced Server V10
    - Amazon Linux 2
- 持续集成测试

要选择最适合你的用例的工具，请阅读 [kubeadm](https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/) 和 [kops](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kops/) 之间的[这份比较](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md)。

##### 创建集群[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#creating-a-cluster)

###### （1/5）满足下层设施要求[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#1-5-%E6%BB%A1%E8%B6%B3%E4%B8%8B%E5%B1%82%E8%AE%BE%E6%96%BD%E8%A6%81%E6%B1%82)

按以下[要求](https://github.com/kubernetes-sigs/kubespray#requirements)来配置服务器：

- **Kubernetes** 的最低版本要求为 V1.22
- **在将运行 Ansible 命令的计算机上安装 Ansible v2.11（或更高版本）、Jinja 2.11（或更高版本）和 python-netaddr**
- 目标服务器必须**能够访问 Internet** 才能拉取 Docker 镜像。否则， 需要其他配置（[请参见离线环境](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md)）
- 目标服务器配置为允许 **IPv4 转发**
- 如果针对 Pod 和 Service 使用 IPv6，则目标服务器配置为允许 **IPv6 转发**
- **防火墙不是由 kubespray 管理的**。你需要根据需求设置适当的规则策略。为了避免部署过程中出现问题，可以禁用防火墙。
- 如果从非 root 用户帐户运行 kubespray，则应在目标服务器中配置正确的特权升级方法并指定 `ansible_become` 标志或命令参数 `--become` 或 `-b`

Kubespray 提供以下实用程序来帮助你设置环境：

- 为以下云驱动提供的 [Terraform](https://www.terraform.io/) 脚本：
    - [AWS](https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws)
    - [OpenStack](https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/openstack)
    - [Equinix Metal](https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/metal)

###### （2/5）编写清单文件[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#2-5-%E7%BC%96%E5%86%99%E6%B8%85%E5%8D%95%E6%96%87%E4%BB%B6)

设置服务器后，请创建一个 [Ansible 的清单文件](https://docs.ansible.com/ansible/latest/network/getting_started/first_inventory.html)。 你可以手动执行此操作，也可以通过动态清单脚本执行此操作。有关更多信息，请参阅 “[建立你自己的清单](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#building-your-own-inventory)”。

###### （3/5）规划集群部署[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#3-5-%E8%A7%84%E5%88%92%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2)

Kubespray 能够自定义部署的许多方面：

- 选择部署模式： kubeadm 或非 kubeadm
- CNI（网络）插件
- DNS 配置
- 控制平面的选择：本机/可执行文件或容器化
- 组件版本
- Calico 路由反射器
- 组件运行时选项
    - [Docker](https://docs.docker.com/engine/)
    - [containerd](https://containerd.io/docs/)
    - [CRI-O](https://cri-o.io/#what-is-cri-o)
- 证书生成方式

可以修改[变量文件](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html)以进行 Kubespray 定制。 如果你刚刚开始使用 Kubespray，请考虑使用 Kubespray 默认设置来部署你的集群并探索 Kubernetes。

###### （4/5）部署集群[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#4-5-%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4)

接下来，部署你的集群：

使用 [ansible-playbook](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#starting-custom-deployment) 进行集群部署。

```shell
ansible-playbook -i your/inventory/inventory.ini cluster.yml -b -v \
  --private-key=~/.ssh/private_key
```

大型部署（超过 100 个节点） 可能需要[特定的调整](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md)， 以获得最佳效果。

###### （5/5）验证部署[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#5-5-%E9%AA%8C%E8%AF%81%E9%83%A8%E7%BD%B2)

Kubespray 提供了一种使用 [Netchecker](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md) 验证 Pod 间连接和 DNS 解析的方法。 Netchecker 确保 netchecker-agents Pod 可以解析 DNS 请求， 并在默认命名空间内对每个请求执行 ping 操作。 这些 Pod 模仿其他工作负载类似的行为，并用作集群运行状况指示器。

##### 集群操作[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#cluster-operations)

Kubespray 提供了其他 Playbook 来管理集群： **scale** 和 **upgrade**。

###### 扩展集群[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#scale-your-cluster)

你可以通过运行 scale playbook 向集群中添加工作节点。有关更多信息， 请参见 “[添加节点](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#adding-nodes)”。 你可以通过运行 remove-node playbook 来从集群中删除工作节点。有关更多信息， 请参见 “[删除节点](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#remove-nodes)”。

###### 升级集群[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#upgrade-your-cluster)

你可以通过运行 upgrade-cluster Playbook 来升级集群。有关更多信息，请参见 “[升级](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md)”。

##### 清理[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#cleanup)

你可以通过 [reset playbook](https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml) 重置节点并清除所有与 Kubespray 一起安装的组件。

**注意：**

运行 reset playbook 时，请确保不要意外地将生产集群作为目标！

##### 反馈[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#feedback)

- Slack 频道：[#kubespray](https://kubernetes.slack.com/messages/kubespray/) （你可以在[此处](https://slack.k8s.io/)获得邀请）。
- [GitHub 问题](https://github.com/kubernetes-sigs/kubespray/issues)。

##### 接下来[](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/#%E6%8E%A5%E4%B8%8B%E6%9D%A5)

- 查看有关 Kubespray 的 [路线图](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/roadmap.md)的计划工作。
- 查阅有关 [Kubespray](https://github.com/kubernetes-sigs/kubespray) 的更多信息。


## 最佳实践

---

### [大规模集群的注意事项](https://kubernetes.io/zh-cn/docs/setup/best-practices/cluster-large/)
集群是运行 Kubernetes 代理的、 由[控制平面](https://kubernetes.io/zh-cn/docs/reference/glossary/?all=true#term-control-plane)管理的一组 [节点](https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/)（物理机或虚拟机）。 Kubernetes v1.26 单个集群支持的最大节点数为 5,000。 更具体地说，Kubernetes 旨在适应满足以下**所有**标准的配置：

- 每个节点的 Pod 数量不超过 110
- 节点数不超过 5,000
- Pod 总数不超过 150,000
- 容器总数不超过 300,000

你可以通过添加或删除节点来扩展集群。集群扩缩的方式取决于集群的部署方式。

#### 云供应商资源配额[](https://kubernetes.io/zh-cn/docs/setup/best-practices/cluster-large/#quota-issues)

为避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑以下事项：

- 请求增加云资源的配额，例如：
    - 计算实例
    - CPU
    - 存储卷
    - 使用中的 IP 地址
    - 数据包过滤规则集
    - 负载均衡数量
    - 网络子网
    - 日志流
- 由于某些云供应商限制了创建新实例的速度，因此通过分批启动新节点来控制集群扩展操作，并在各批之间有一个暂停。

#### 控制面组件[](https://kubernetes.io/zh-cn/docs/setup/best-practices/cluster-large/#control-plane-components)

对于大型集群，你需要一个具有足够计算能力和其他资源的控制平面。

通常，你将在每个故障区域运行一个或两个控制平面实例， 先垂直缩放这些实例，然后在到达下降点（垂直）后再水平缩放。

你应该在每个故障区域至少应运行一个实例，以提供容错能力。 Kubernetes 节点不会自动将流量引向相同故障区域中的控制平面端点。 但是，你的云供应商可能有自己的机制来执行此操作。

例如，使用托管的负载均衡器时，你可以配置负载均衡器发送源自故障区域 **A** 中的 kubelet 和 Pod 的流量， 并将该流量仅定向到也位于区域 **A** 中的控制平面主机。 如果单个控制平面主机或端点故障区域 **A** 脱机，则意味着区域 **A** 中的节点的所有控制平面流量现在都在区域之间发送。 在每个区域中运行多个控制平面主机能降低出现这种结果的可能性。

##### etcd 存储[](https://kubernetes.io/zh-cn/docs/setup/best-practices/cluster-large/#etcd-storage)

为了提高大规模集群的性能，你可以将事件对象存储在单独的专用 etcd 实例中。

在创建集群时，你可以（使用自定义工具）：

- 启动并配置额外的 etcd 实例
- 配置 [API 服务器](https://kubernetes.io/zh-cn/docs/concepts/overview/components/#kube-apiserver)，将它用于存储事件

有关为大型集群配置和管理 etcd 的详细信息， 请参阅[为 Kubernetes 运行 etcd 集群](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/) 和使用 [kubeadm 创建一个高可用 etcd 集群](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)。

##### 插件资源[](https://kubernetes.io/zh-cn/docs/setup/best-practices/cluster-large/#addon-resources)

Kubernetes [资源限制](https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/) 有助于最大程度地减少内存泄漏的影响以及 Pod 和容器可能对其他组件的其他方式的影响。 这些资源限制适用于[插件](https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/addons/)资源， 就像它们适用于应用程序工作负载一样。

例如，你可以对日志组件设置 CPU 和内存限制：

```yaml
  ...
  containers:
  - name: fluentd-cloud-logging
    image: fluent/fluentd-kubernetes-daemonset:v1
    resources:
      limits:
        cpu: 100m
        memory: 200Mi
```

插件的默认限制通常基于从中小规模 Kubernetes 集群上运行每个插件的经验收集的数据。 插件在大规模集群上运行时，某些资源消耗常常比其默认限制更多。 如果在不调整这些值的情况下部署了大规模集群，则插件可能会不断被杀死，因为它们不断达到内存限制。 或者，插件可能会运行，但由于 CPU 时间片的限制而导致性能不佳。

为避免遇到集群插件资源问题，在创建大规模集群时，请考虑以下事项：

- 部分垂直扩展插件 —— 总有一个插件副本服务于整个集群或服务于整个故障区域。 对于这些附加组件，请在扩大集群时加大资源请求和资源限制。
- 许多水平扩展插件 —— 你可以通过运行更多的 Pod 来增加容量——但是在大规模集群下， 可能还需要稍微提高 CPU 或内存限制。 VerticalPodAutoscaler 可以在 **recommender** 模式下运行， 以提供有关请求和限制的建议数字。
- 一些插件在每个节点上运行一个副本，并由 DaemonSet 控制： 例如，节点级日志聚合器。与水平扩展插件的情况类似， 你可能还需要稍微提高 CPU 或内存限制。

#### 接下来[](https://kubernetes.io/zh-cn/docs/setup/best-practices/cluster-large/#%E6%8E%A5%E4%B8%8B%E6%9D%A5)

- `VerticalPodAutoscaler` 是一种自定义资源，你可以将其部署到集群中，帮助你管理 Pod 的资源请求和资源限制。 了解有关 [Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme) 的更多信息，了解如何用它扩展集群组件（包括对集群至关重要的插件）的信息。
    
- [集群自动扩缩器](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme) 与许多云供应商集成在一起，帮助你在你的集群中，按照资源需求级别运行正确数量的节点。
    

- [addon resizer](https://github.com/kubernetes/autoscaler/tree/master/addon-resizer#readme) 可帮助你在集群规模变化时自动调整插件的大小。
### [运行于多可用区环境](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/)
本页描述如何跨多个区（Zone）中运行集群。

#### 背景[](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#%E8%83%8C%E6%99%AF)

Kubernetes 从设计上允许同一个 Kubernetes 集群跨多个失效区来运行， 通常这些区位于某个称作 _区域（region）_ 逻辑分组中。 主要的云提供商都将区域定义为一组失效区的集合（也称作 _可用区（Availability Zones）_）， 能够提供一组一致的功能特性：每个区域内，各个可用区提供相同的 API 和服务。

典型的云体系结构都会尝试降低某个区中的失效影响到其他区中服务的概率。

#### 控制面行为[](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#control-plane-behavior)

所有的[控制面组件](https://kubernetes.io/zh-cn/docs/concepts/overview/components/#control-plane-components) 都支持以一组可相互替换的资源池的形式来运行，每个组件都有多个副本。

当你部署集群控制面时，应将控制面组件的副本跨多个失效区来部署。 如果可用性是一个很重要的指标，应该选择至少三个失效区，并将每个 控制面组件（API 服务器、调度器、etcd、控制器管理器）复制多个副本， 跨至少三个失效区来部署。如果你在运行云控制器管理器，则也应该将 该组件跨所选的三个失效区来部署。

**说明：**

Kubernetes 并不会为 API 服务器端点提供跨失效区的弹性。 你可以为集群 API 服务器使用多种技术来提升其可用性，包括使用 DNS 轮转、SRV 记录或者带健康检查的第三方负载均衡解决方案等等。

#### 节点行为[](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#node-behavior)

Kubernetes 自动为负载资源（如[Deployment](https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/) 或 [StatefulSet](https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/))） 跨集群中不同节点来部署其 Pods。 这种分布逻辑有助于降低失效带来的影响。

节点启动时，每个节点上的 kubelet 会向 Kubernetes API 中代表该 kubelet 的 Node 对象 添加 [标签](https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/labels/)。 这些标签可能包含[区信息](https://kubernetes.io/zh-cn/docs/reference/labels-annotations-taints/#topologykubernetesiozone)。

如果你的集群跨了多个可用区或者地理区域，你可以使用节点标签，结合 [Pod 拓扑分布约束](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/) 来控制如何在你的集群中多个失效域之间分布 Pods。这里的失效域可以是 地理区域、可用区甚至是特定节点。 这些提示信息使得[调度器](https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/) 能够更好地分布 Pods，以实现更好的可用性，降低因为某种失效给整个工作负载 带来的风险。

例如，你可以设置一种约束，确保某个 StatefulSet 中的三个副本都运行在 不同的可用区中，只要其他条件允许。你可以通过声明的方式来定义这种约束， 而不需要显式指定每个工作负载使用哪些可用区。

##### 跨多个区分布节点[](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#distributing-nodes-across-zones)

Kubernetes 的核心逻辑并不会帮你创建节点，你需要自行完成此操作，或者使用 类似 [Cluster API](https://cluster-api.sigs.k8s.io/) 这类工具来替你管理节点。

使用类似 Cluster API 这类工具，你可以跨多个失效域来定义一组用做你的集群 工作节点的机器，以及当整个区的服务出现中断时如何自动治愈集群的策略。

#### 为 Pods 手动指定区[](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#%E4%B8%BA-pods-%E6%89%8B%E5%8A%A8%E6%8C%87%E5%AE%9A%E5%8C%BA)

你可以应用[节点选择算符约束](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) 到你所创建的 Pods 上，或者为 Deployment、StatefulSet 或 Job 这类工作负载资源 中的 Pod 模板设置此类约束。

#### 跨区的存储访问[](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#%E8%B7%A8%E5%8C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E8%AE%BF%E9%97%AE)

当创建持久卷时，`PersistentVolumeLabel` [准入控制器](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/) 会自动向那些链接到特定区的 PersistentVolume 添加区标签。 [调度器](https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/)通过其 `NoVolumeZoneConflict` 断言确保申领给定 PersistentVolume 的 Pods 只会 被调度到该卷所在的可用区。

你可以为 PersistentVolumeClaim 指定[StorageClass](https://kubernetes.io/zh-cn/docs/concepts/storage/storage-classes/) 以设置该类中的存储可以使用的失效域（区）。 要了解如何配置能够感知失效域或区的 StorageClass，请参阅 [可用的拓扑逻辑](https://kubernetes.io/zh-cn/docs/concepts/storage/storage-classes/#allowed-topologies)。

#### 网络[](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#networking)

Kubernetes 自身不提供与可用区相关的联网配置。 你可以使用[网络插件](https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) 来配置集群的联网，该网络解决方案可能拥有一些与可用区相关的元素。 例如，如果你的云提供商支持 `type=LoadBalancer` 的 Service，则负载均衡器 可能仅会将请求流量发送到运行在负责处理给定连接的负载均衡器组件所在的区。 请查阅云提供商的文档了解详细信息。

对于自定义的或本地集群部署，也可以考虑这些因素 [Service](https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/) [Ingress](https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/) 的行为， 包括处理不同失效区的方法，在很大程度上取决于你的集群是如何搭建的。

#### 失效恢复[](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#fault-recovery)

在搭建集群时，你可能需要考虑当某区域中的所有失效区都同时掉线时，是否以及如何 恢复服务。例如，你是否要求在某个区中至少有一个节点能够运行 Pod？ 请确保任何对集群很关键的修复工作都不要指望集群中至少有一个健康节点。 例如：当所有节点都不健康时，你可能需要运行某个修复性的 Job， 该 Job 要设置特定的[容忍度](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/) 以便修复操作能够至少将一个节点恢复为可用状态。

Kubernetes 对这类问题没有现成的解决方案；不过这也是要考虑的因素之一。

#### 接下来[](https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#%E6%8E%A5%E4%B8%8B%E6%9D%A5)

要了解调度器如何在集群中放置 Pods 并遵从所配置的约束，可参阅 [调度与驱逐](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/)。
### [校验节点设置](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/)
- [节点一致性测试](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#node-conformance-test)
- [节点的前提条件](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#node-prerequisite)
- [运行节点一致性测试](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#running-node-conformance-test)
- [针对其他硬件体系结构运行节点一致性测试](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#running-node-conformance-test-for-other-architectures)
- [运行特定的测试](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#running-selected-test)
- [注意事项](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#caveats)

#### 节点一致性测试[](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#node-conformance-test)

**节点一致性测试** 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。 测试验证节点是否满足 Kubernetes 的最低要求；通过测试的节点有资格加入 Kubernetes 集群。

该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。

#### 节点的前提条件[](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#node-prerequisite)

要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：

- 容器运行时 (Docker)
- Kubelet

#### 运行节点一致性测试[](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#running-node-conformance-test)

要运行节点一致性测试，请执行以下步骤：

1. 得出 kubelet 的 `--kubeconfig` 的值；例如：`--kubeconfig=/var/lib/kubelet/config.yaml`。 由于测试框架启动了本地控制平面来测试 kubelet，因此使用 `http://localhost:8080` 作为API 服务器的 URL。 一些其他的 kubelet 命令行参数可能会被用到：
    - `--cloud-provider`：如果使用 `--cloud-provider=gce`，需要移除这个参数来运行测试。

2. 使用以下命令运行节点一致性测试：
    
    ```shell
    # $CONFIG_DIR 是你 Kubelet 的 pod manifest 路径。
    # $LOG_DIR 是测试的输出路径。
    sudo docker run -it --rm --privileged --net=host \
      -v /:/rootfs -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \
      registry.k8s.io/node-test:0.2
    ```
    

#### 针对其他硬件体系结构运行节点一致性测试[](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#running-node-conformance-test-for-other-architectures)

Kubernetes 也为其他硬件体系结构的系统提供了节点一致性测试的 Docker 镜像：

| 架构 | 镜像 |
| --- | --- |
| amd64 | node-test-amd64 |
| arm | node-test-arm |
| arm64 | node-test-arm64 |

#### 运行特定的测试[](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#running-selected-test)

要运行特定测试，请使用你希望运行的测试的特定表达式覆盖环境变量 `FOCUS`。

```shell
sudo docker run -it --rm --privileged --net=host \
  -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \
  -e FOCUS=MirrorPod \ # Only run MirrorPod test
  registry.k8s.io/node-test:0.2
```

要跳过特定的测试，请使用你希望跳过的测试的常规表达式覆盖环境变量 `SKIP`。

```shell
sudo docker run -it --rm --privileged --net=host \
  -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \
  -e SKIP=MirrorPod \ # 运行除 MirrorPod 测试外的所有一致性测试内容
  registry.k8s.io/node-test:0.2
```

节点一致性测试是[节点端到端测试](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md)的容器化版本。

默认情况下，它会运行所有一致性测试。

理论上，只要合理地配置容器和挂载所需的卷，就可以运行任何的节点端到端测试用例。但是这里**强烈建议只运行一致性测试**，因为运行非一致性测试需要很多复杂的配置。

#### 注意事项[](https://kubernetes.io/zh-cn/docs/setup/best-practices/node-conformance/#caveats)

- 测试会在节点上遗留一些 Docker 镜像，包括节点一致性测试本身的镜像和功能测试相关的镜像。
- 测试会在节点上遗留一些死的容器。这些容器是在功能测试的过程中创建的。
### [强制实施 Pod 安全性标准](https://kubernetes.io/zh-cn/docs/setup/best-practices/enforcing-pod-security-standards/)
本页提供实施 [Pod 安全标准（Pod Security Standards）](https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-standards) 时的一些最佳实践。

#### 使用内置的 Pod 安全性准入控制器[](https://kubernetes.io/zh-cn/docs/setup/best-practices/enforcing-pod-security-standards/#%E4%BD%BF%E7%94%A8%E5%86%85%E7%BD%AE%E7%9A%84-pod-%E5%AE%89%E5%85%A8%E6%80%A7%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6%E5%99%A8)

**特性状态：** `Kubernetes v1.25 [stable]`

[Pod 安全性准入控制器](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/#podsecurity) 尝试替换已被废弃的 PodSecurityPolicies。

##### 配置所有集群名字空间[](https://kubernetes.io/zh-cn/docs/setup/best-practices/enforcing-pod-security-standards/#configure-all-cluster-namespaces)

完全未经配置的名字空间应该被视为集群安全模型中的重大缺陷。 我们建议花一些时间来分析在每个名字空间中执行的负载的类型， 并通过引用 Pod 安全性标准来确定每个负载的合适级别。 未设置标签的名字空间应该视为尚未被评估。

针对所有名字空间中的所有负载都具有相同的安全性需求的场景， 我们提供了一个[示例](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/#applying-to-all-namespaces) 用来展示如何批量应用 Pod 安全性标签。

##### 拥抱最小特权原则[](https://kubernetes.io/zh-cn/docs/setup/best-practices/enforcing-pod-security-standards/#%E6%8B%A5%E6%8A%B1%E6%9C%80%E5%B0%8F%E7%89%B9%E6%9D%83%E5%8E%9F%E5%88%99)

在一个理想环境中，每个名字空间中的每个 Pod 都会满足 `restricted` 策略的需求。 不过，这既不可能也不现实，某些负载会因为合理的原因而需要特权上的提升。

- 允许 `privileged` 负载的名字空间需要建立并实施适当的访问控制机制。
- 对于运行在特权宽松的名字空间中的负载，需要维护其独特安全性需求的文档。 如果可能的话，要考虑如何进一步约束这些需求。

##### 采用多种模式的策略[](https://kubernetes.io/zh-cn/docs/setup/best-practices/enforcing-pod-security-standards/#%E9%87%87%E7%94%A8%E5%A4%9A%E7%A7%8D%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%AD%96%E7%95%A5)

Pod 安全性标准准入控制器的 `audit` 和 `warn` 模式（mode） 能够在不影响现有负载的前提下，让该控制器更方便地收集关于 Pod 的重要的安全信息。

针对所有名字空间启用这些模式是一种好的实践，将它们设置为你最终打算 `enforce` 的 _期望的_ 级别和版本。这一阶段中所生成的警告和审计注解信息可以帮助你到达这一状态。 如果你期望负载的作者能够作出变更以便适应期望的级别，可以启用 `warn` 模式。 如果你希望使用审计日志了监控和驱动变更，以便负载能够适应期望的级别，可以启用 `audit` 模式。

当你将 `enforce` 模式设置为期望的取值时，这些模式在不同的场合下仍然是有用的：

- 通过将 `warn` 设置为 `enforce` 相同的级别，客户可以在尝试创建无法通过合法检查的 Pod （或者包含 Pod 模板的资源）时收到警告信息。这些信息会帮助于更新资源使其合规。
- 在将 `enforce` 锁定到特定的非最新版本的名字空间中，将 `audit` 和 `warn` 模式设置为 `enforce` 一样的级别而非 `latest` 版本， 这样可以方便看到之前版本所允许但当前最佳实践中被禁止的设置。

#### 第三方替代方案[](https://kubernetes.io/zh-cn/docs/setup/best-practices/enforcing-pod-security-standards/#third-party-alternatives)

**说明：** 本部分链接到提供 Kubernetes 所需功能的第三方项目。Kubernetes 项目作者不负责这些项目。此页面遵循[CNCF 网站指南](https://github.com/cncf/foundation/blob/master/website-guidelines.md)，按字母顺序列出项目。要将项目添加到此列表中，请在提交更改之前阅读[内容指南](https://kubernetes.io/docs/contribute/style/content-guide/#third-party-content)。

Kubernetes 生态系统中也有一些其他强制实施安全设置的替代方案处于开发状态中：

- [Kubewarden](https://github.com/kubewarden).
- [Kyverno](https://kyverno.io/policies/).
- [OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper).

采用 _内置的_ 方案（例如 PodSecurity 准入控制器）还是第三方工具， 这一决策完全取决于你自己的情况。在评估任何解决方案时，对供应链的信任都是至关重要的。 最终，使用前述方案中的 _任何_ 一种都好过放任自流。
### [PKI 证书和要求](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/)

Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果你是使用 [kubeadm](https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/) 安装的 Kubernetes， 则会自动生成集群所需的证书。你还可以生成自己的证书。 例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。

#### 集群是如何使用证书的[](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/#how-certificates-are-used-by-your-cluster)

Kubernetes 需要 PKI 才能执行以下操作：

- Kubelet 的客户端证书，用于 API 服务器身份验证
- Kubelet [服务端证书](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/#client-and-serving-certificates)， 用于 API 服务器与 Kubelet 的会话
- API 服务器端点的证书
- 集群管理员的客户端证书，用于 API 服务器身份认证
- API 服务器的客户端证书，用于和 Kubelet 的会话
- API 服务器的客户端证书，用于和 etcd 的会话
- 控制器管理器的客户端证书或 kubeconfig，用于和 API 服务器的会话
- 调度器的客户端证书或 kubeconfig，用于和 API 服务器的会话
- [前端代理](https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/)的客户端及服务端证书

**说明：**

只有当你运行 kube-proxy 并要支持[扩展 API 服务器](https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/setup-extension-api-server/)时， 才需要 `front-proxy` 证书。

etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。

#### 证书存放的位置[](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/#where-certificates-are-stored)

假如通过 kubeadm 安装 Kubernetes，大多数证书都存储在 `/etc/kubernetes/pki`。 本文档中的所有路径都是相对于该目录的，但用户账户证书除外，kubeadm 将其放在 `/etc/kubernetes` 中。

#### 手动配置证书[](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/#configure-certificates-manually)

如果你不想通过 kubeadm 生成这些必需的证书，你可以使用一个单一的根 CA 来创建这些证书或者直接提供所有证书。 参见[证书](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/certificates/)以进一步了解创建自己的证书机构。 关于管理证书的更多信息，请参见[使用 kubeadm 进行证书管理](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/)。

##### 单根 CA[](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/#single-root-ca)

你可以创建由管理员控制的单根 CA。该根 CA 可以创建多个中间 CA，并将所有进一步的创建委托给 Kubernetes。

需要这些 CA：

| 路径 | 默认 CN | 描述 |
| --- | --- | --- |
| ca.crt,key | kubernetes-ca | Kubernetes 通用 CA |
| etcd/ca.crt,key | etcd-ca | 与 etcd 相关的所有功能 |
| front-proxy-ca.crt,key | kubernetes-front-proxy-ca | 用于[前端代理](https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/) |

上面的 CA 之外，还需要获取用于服务账户管理的密钥对，也就是 `sa.key` 和 `sa.pub`。

下面的例子说明了上表中所示的 CA 密钥和证书文件。

```console
/etc/kubernetes/pki/ca.crt
/etc/kubernetes/pki/ca.key
/etc/kubernetes/pki/etcd/ca.crt
/etc/kubernetes/pki/etcd/ca.key
/etc/kubernetes/pki/front-proxy-ca.crt
/etc/kubernetes/pki/front-proxy-ca.key
```

##### 所有的证书[](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/#all-certificates)

如果你不想将 CA 的私钥拷贝至你的集群中，你也可以自己生成全部的证书。

需要这些证书：

| 默认 CN | 父级 CA | O（位于 Subject 中） | kind | 主机 (SAN) |
| --- | --- | --- | --- | --- |
| kube-etcd | etcd-ca |  | server, client | `<hostname>`, `<Host_IP>`, `localhost`, `127.0.0.1` |
| kube-etcd-peer | etcd-ca |  | server, client | `<hostname>`, `<Host_IP>`, `localhost`, `127.0.0.1` |
| kube-etcd-healthcheck-client | etcd-ca |  | client |  |
| kube-apiserver-etcd-client | etcd-ca | system:masters | client |  |
| kube-apiserver | kubernetes-ca |  | server | `<hostname>`, `<Host_IP>`, `<advertise_IP>`, `[1]` |
| kube-apiserver-kubelet-client | kubernetes-ca | system:masters | client |  |
| front-proxy-client | kubernetes-front-proxy-ca |  | client |  |

\[1\]: 用来连接到集群的不同 IP 或 DNS 名 （就像 [kubeadm](https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/) 为负载均衡所使用的固定 IP 或 DNS 名：`kubernetes`、`kubernetes.default`、`kubernetes.default.svc`、 `kubernetes.default.svc.cluster`、`kubernetes.default.svc.cluster.local`）。

其中 `kind` 对应一种或多种类型的 x509 密钥用途，也可记录在 [CertificateSigningRequest](https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/authentication-resources/certificate-signing-request-v1#CertificateSigningRequest) 类型的 `.spec.usages` 中：

| kind | 密钥用途 |
| --- | --- |
| server | 数字签名、密钥加密、服务端认证 |
| client | 数字签名、密钥加密、客户端认证 |

**说明：**

上面列出的 Hosts/SAN 是推荐的配置方式；如果需要特殊安装，则可以在所有服务器证书上添加其他 SAN。

**说明：**

对于 kubeadm 用户：

- 不使用私钥，将证书复制到集群 CA 的方案，在 kubeadm 文档中将这种方案称为外部 CA。
- 如果将以上列表与 kubeadm 生成的 PKI 进行比较，你会注意到，如果使用外部 etcd，则不会生成 `kube-etcd`、`kube-etcd-peer` 和 `kube-etcd-healthcheck-client` 证书。

##### 证书路径[](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/#certificate-paths)

证书应放置在建议的路径中（以便 [kubeadm](https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/) 使用）。无论使用什么位置，都应使用给定的参数指定路径。

| 默认 CN | 建议的密钥路径 | 建议的证书路径 | 命令 | 密钥参数 | 证书参数 |
| --- | --- | --- | --- | --- | --- |
| etcd-ca | etcd/ca.key | etcd/ca.crt | kube-apiserver |  | \--etcd-cafile |
| kube-apiserver-etcd-client | apiserver-etcd-client.key | apiserver-etcd-client.crt | kube-apiserver | \--etcd-keyfile | \--etcd-certfile |
| kubernetes-ca | ca.key | ca.crt | kube-apiserver |  | \--client-ca-file |
| kubernetes-ca | ca.key | ca.crt | kube-controller-manager | \--cluster-signing-key-file | \--client-ca-file, --root-ca-file, --cluster-signing-cert-file |
| kube-apiserver | apiserver.key | apiserver.crt | kube-apiserver | \--tls-private-key-file | \--tls-cert-file |
| kube-apiserver-kubelet-client | apiserver-kubelet-client.key | apiserver-kubelet-client.crt | kube-apiserver | \--kubelet-client-key | \--kubelet-client-certificate |
| front-proxy-ca | front-proxy-ca.key | front-proxy-ca.crt | kube-apiserver |  | \--requestheader-client-ca-file |
| front-proxy-ca | front-proxy-ca.key | front-proxy-ca.crt | kube-controller-manager |  | \--requestheader-client-ca-file |
| front-proxy-client | front-proxy-client.key | front-proxy-client.crt | kube-apiserver | \--proxy-client-key-file | \--proxy-client-cert-file |
| etcd-ca | etcd/ca.key | etcd/ca.crt | etcd |  | \--trusted-ca-file, --peer-trusted-ca-file |
| kube-etcd | etcd/server.key | etcd/server.crt | etcd | \--key-file | \--cert-file |
| kube-etcd-peer | etcd/peer.key | etcd/peer.crt | etcd | \--peer-key-file | \--peer-cert-file |
| etcd-ca |  | etcd/ca.crt | etcdctl |  | \--cacert |
| kube-etcd-healthcheck-client | etcd/healthcheck-client.key | etcd/healthcheck-client.crt | etcdctl | \--key | \--cert |

注意事项同样适用于服务帐户密钥对：

| 私钥路径 | 公钥路径 | 命令 | 参数 |
| --- | --- | --- | --- |
| sa.key |  | kube-controller-manager | \--service-account-private-key-file |
|  | sa.pub | kube-apiserver | \--service-account-key-file |

下面的例子展示了自行生成所有密钥和证书时所需要提供的文件路径。 这些路径基于[前面的表格](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/#certificate-paths)。

```console
/etc/kubernetes/pki/etcd/ca.key
/etc/kubernetes/pki/etcd/ca.crt
/etc/kubernetes/pki/apiserver-etcd-client.key
/etc/kubernetes/pki/apiserver-etcd-client.crt
/etc/kubernetes/pki/ca.key
/etc/kubernetes/pki/ca.crt
/etc/kubernetes/pki/apiserver.key
/etc/kubernetes/pki/apiserver.crt
/etc/kubernetes/pki/apiserver-kubelet-client.key
/etc/kubernetes/pki/apiserver-kubelet-client.crt
/etc/kubernetes/pki/front-proxy-ca.key
/etc/kubernetes/pki/front-proxy-ca.crt
/etc/kubernetes/pki/front-proxy-client.key
/etc/kubernetes/pki/front-proxy-client.crt
/etc/kubernetes/pki/etcd/server.key
/etc/kubernetes/pki/etcd/server.crt
/etc/kubernetes/pki/etcd/peer.key
/etc/kubernetes/pki/etcd/peer.crt
/etc/kubernetes/pki/etcd/healthcheck-client.key
/etc/kubernetes/pki/etcd/healthcheck-client.crt
/etc/kubernetes/pki/sa.key
/etc/kubernetes/pki/sa.pub
```

#### 为用户帐户配置证书[](https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/#configure-certificates-for-user-accounts)

你必须手动配置以下管理员帐户和服务帐户：

| 文件名 | 凭据名称 | 默认 CN | O (位于 Subject 中) |
| --- | --- | --- | --- |
| admin.conf | default-admin | kubernetes-admin | system:masters |
| kubelet.conf | default-auth | system:node:`<nodeName>` （参阅注释） | system:nodes |
| controller-manager.conf | default-controller-manager | system:kube-controller-manager |  |
| scheduler.conf | default-scheduler | system:kube-scheduler |  |

**说明：**

`kubelet.conf` 中 `<nodeName>` 的值 **必须** 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。 有关更多详细信息，请阅读[节点授权](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/node/)。

1. 对于每个配置，请都使用给定的 CN 和 O 生成 x509 证书/密钥偶对。
    
2. 为每个配置运行下面的 `kubectl` 命令：
    

```
KUBECONFIG=<filename> kubectl config set-cluster default-cluster --server=https://<host ip>:6443 --certificate-authority <path-to-kubernetes-ca> --embed-certs
KUBECONFIG=<filename> kubectl config set-credentials <credential-name> --client-key <path-to-key>.pem --client-certificate <path-to-cert>.pem --embed-certs
KUBECONFIG=<filename> kubectl config set-context default-system --cluster default-cluster --user <credential-name>
KUBECONFIG=<filename> kubectl config use-context default-system
```

这些文件用途如下：

| 文件名 | 命令 | 说明 |
| --- | --- | --- |
| admin.conf | kubectl | 配置集群的管理员 |
| kubelet.conf | kubelet | 集群中的每个节点都需要一份 |
| controller-manager.conf | kube-controller-manager | 必需添加到 `manifests/kube-controller-manager.yaml` 清单中 |
| scheduler.conf | kube-scheduler | 必需添加到 `manifests/kube-scheduler.yaml` 清单中 |

下面是前表中所列文件的完整路径。

```console
/etc/kubernetes/admin.conf
/etc/kubernetes/kubelet.conf
/etc/kubernetes/controller-manager.conf
/etc/kubernetes/scheduler.conf
```