---
title: "note"
date: 2022-05-21T14:42:03+08:00
lastmod: 2022-05-21
tags: [papers]
categories: [Graduation project]
slug: 
draft: true
---

## 一文看懂25个神经网络模型
https://zhuanlan.zhihu.com/p/152371207

### CNN
卷积神经网络（CNN：Convolutional neural networks）或深度卷积神经网络（DCNN：deep convolutional neural networks）跟其它类型的神经网络大有不同。它们主要用于处理图像数据，但可用于其它形式数据的处理，如语音数据。对于卷积神经网络来说，一个典型的应用就是给它输入一个图像，而后它会给出一个分类结果。也就是说，如果你给它一张猫的图像，它就输出“猫”；如果你给一张狗的图像，它就输出“狗”。

卷积神经网络是从一个数据扫描层开始，这种形式的处理并没有尝试在一开始就解析整个训练数据。比如：对于一个大小为200X200像素的图像，你不会想构建一个40000个节点的神经元层。而是，构建一个20X20像素的输入扫描层，然后，把原始图像第一部分的20X20像素图像（通常是从图像的左上方开始）输入到这个扫描层。当这部分图像（可能是用于进行卷积神经网络的训练）处理完，你会接着处理下一部分的20X20像素图像：逐渐（通常情况下是移动一个像素，但是，移动的步长是可以设置的）移动扫描层，来处理原始数据。

注意，你不是一次性移动扫描层20个像素（或其它任何扫描层大小的尺度），也不是把原始图像切分成20X20像素的图像块，而是用扫描层在原始图像上滑过。这个输入数据（20X20像素的图像块）紧接着被输入到卷积层，而非常规的神经细胞层——卷积层的节点不是全连接。每一个输入节点只会和最近的那个神经元节点连接（至于多近要取决于具体的实现，但通常不会超过几个）。

这些卷积层会随着深度的增加而逐渐变小：大多数情况下，会按照输入层数量的某个因子缩小（比如：20个神经元的卷积层，后面是10个神经元的卷积层，再后面就是5个神经元的卷积层）。2的n次方（32, 16, 8, 4, 2, 1）也是一个非常常用的因子，因为它们在定义上可以简洁且完整地除尽。除了卷积层，池化层（pooling layers）也非常重要。

池化是一种过滤掉细节的方式：一种常用的池化方式是最大池化，比如用2X2的像素，然后取四个像素中值最大的那个传递。为了让卷积神经网络处理语音数据，需要把语音数据切分，一段一段输入。在实际应用中，通常会在卷积神经网络后面加一个前馈神经网络，以进一步处理数据，从而对数据进行更高水平的非线性抽象。

### RNN
循环神经网络（RNN：Recurrent neural networks）是具有时间联结的前馈神经网络：它们有了状态，通道与通道之间有了时间上的联系。神经元的输入信息，不仅包括前一神经细胞层的输出，还包括它自身在先前通道的状态。

这就意味着：你的输入顺序将会影响神经网络的训练结果：相比先输入“曲奇饼”再输入“牛奶”，先输入“牛奶”再输入“曲奇饼”后，或许会产生不同的结果。RNN存在一大问题：梯度消失（或梯度爆炸，这取决于所用的激活函数），信息会随时间迅速消失，正如FFNN会随着深度的增加而失去信息一样。

直觉上，这不算什么大问题，因为这些都只是权重，而非神经元的状态，但随时间变化的权重正是来自过去信息的存储；如果权重是0或1000000，那之前的状态就不再有信息价值。

原则上，RNN可以在很多领域使用，因为大部分数据在形式上不存在时间线的变化，（不像语音或视频），它们能以某种序列的形式呈现出来。一张图片或一段文字可以一个像素或者一个文字地进行输入，因此，与时间相关的权重描述了该序列前一步发生了什么，而不是多少秒之前发生了什么。一般来说，循环神经网络是推测或补全信息很好的选择，比如自动补全。

### 长短期记忆（LSTM）
长短期记忆（LSTM：Long / short term memory）网络试图通过引入门结构与明确定义的记忆单元来解决梯度消失/爆炸的问题。

这更多的是受电路图设计的启发，而非生物学上某种和记忆相关机制。每个神经元都有一个记忆单元和三个门：输入门、输出门、遗忘门。这三个门的功能就是通过禁止或允许信息流动来保护信息。

输入门决定了有多少前一神经细胞层的信息可留在当前记忆单元，输出层在另一端决定下一神经细胞层能从当前神经元获取多少信息。遗忘门乍看很奇怪，但有时候遗忘部分信息是很有用的：比如说它在学习一本书，并开始学一个新的章节，那遗忘前面章节的部分角色就很有必要了。

实践证明，LSTM可用来学习复杂的序列，比如像莎士比亚一样写作，或创作全新的音乐。值得注意的是，每一个门都对前一神经元的记忆单元赋有一个权重，因此会需要更多的计算资源。

## LSTM
https://zhuanlan.zhihu.com/p/32085405

## GRU
https://zhuanlan.zhihu.com/p/32481747

## DCRNN
https://zhuanlan.zhihu.com/p/65329766?from_voters_page=true


## 梯度消失、梯度爆炸
https://zhuanlan.zhihu.com/p/454044439




## 打字

半夜睡不着觉把心情哼成歌，只好到屋顶找另一个梦境。
睡梦中被敲醒我还是不确定，怎会有动人旋律在对面的屋顶。
我悄悄关上门dlv
