---
title: "Gb_2022_上"
date: 2022-10-29T20:28:49+08:00
lastmod: 2022-10-29
tags: [go official blogs]
categories: [2022]
slug:
draft: true
---


## Share your feedback about developing with Go

Alice Merrick, for the Go team  
18 January 2023

Happy 2023! For seven years, the Go Team has conducted a regular survey of Go developers. The results of this survey have helped the team plan and prioritize large changes to the Go ecosystem, such as dependency management, vulnerability scanning, and generics. We want Go to be increasingly usable by and useful to developers, and your feedback has been a critical aspect of making that happen.

So here is [the January 2023 edition of the Go Developer Survey](https://google.qualtrics.com/jfe/form/SV_bNnbAtFZ0vfRTH8?s=b)! This survey will run through Feb 8th, and we plan to publish anonymous, aggregated results in early March. It should take about 10 minutes to complete, and all questions are optional. Please help to shape Go’s future by sharing your thoughts and experiences.

[Take the Go Developer Survey](https://google.qualtrics.com/jfe/form/SV_bNnbAtFZ0vfRTH8?s=b) (open January 18th – February 8th)


## Go 1.20 is released!

Robert Griesemer, on behalf of the Go team  
1 February 2023

Today the Go team is thrilled to release Go 1.20, which you can get by visiting the [download page](https://go.dev/dl/).

Go 1.20 benefited from an extended development phase, made possible by earlier broad testing and improved overall stability of the code base.

We’re particularly excited to launch a preview of [profile-guided optimization](https://go.dev/doc/pgo) (PGO), which enables the compiler to perform application- and workload-specific optimizations based on run-time profile information. Providing a profile to `go build` enables the compiler to speed up typical applications by around 3–4%, and we expect future releases to benefit even more from PGO. Since this is a preview release of PGO support, we encourage folks to try it out, but there are still rough edges which may preclude production use.

Go 1.20 also includes a handful of language changes, many improvements to tooling and the library, and better overall performance.

### Language changes

- The predeclared [`comparable`](https://go.dev/ref/spec#Type_constraints) constraint is now also [satisfied](https://go.dev/ref/spec#Satisfying_a_type_constraint) by ordinary [comparable types](https://go.dev/ref/spec#Comparison_operators), such as interfaces, which will simplify generic code.
- The functions `SliceData`, `String`, and `StringData` have been added to package [`unsafe`](https://go.dev/ref/spec#Package_unsafe). They complete the set of functions for implementation-independent slice and string manipulation.
- Go’s type conversion rules have been extended to permit direct conversion [from a slice to an array](https://go.dev/ref/spec#Conversions_from_slice_to_array_or_array_pointer).
- The language specification now defines the exact order in which array elements and struct fields are [compared](https://go.dev/ref/spec#Comparison_operators). This clarifies what happens in case of panics during comparisons.

### Tool improvements

- The [`cover` tool](https://go.dev/testing/coverage) now can collect coverage profiles of whole programs, not just of unit tests.
- The [`go` tool](https://go.dev/cmd/go) no longer relies on pre-compiled standard library package archives in the `$GOROOT/pkg` directory, and they are no longer shipped with the distribution, resulting in smaller downloads. Instead, packages in the standard library are built as needed and cached in the build cache, like other packages.
- The implementation of `go test -json` has been improved to make it more robust in the presence of stray writes to `stdout`.
- The `go build`, `go install`, and other build-related commands now accept a `-pgo` flag enabling profile-guided optimizations as well as a `-cover` flag for whole-program coverage analysis.
- The `go` command now disables `cgo` by default on systems without a C toolchain. Consequently, when Go is installed on a system without a C compiler, it will now use pure Go builds for packages in the standard library that optionally use cgo, instead of using pre-distributed package archives (which have been removed, as noted above).
- The [`vet` tool](https://go.dev/cmd/vet) reports more loop variable reference mistakes that may occur in tests running in parallel.

### Standard library additions

- The new [`crypto/ecdh`](https://go.dev/pkg/crypto/ecdh) package provides explicit support for Elliptic Curve Diffie-Hellman key exchanges over NIST curves and Curve25519.
- The new function [`errors.Join`](https://go.dev/pkg/errors#Join) returns an error wrapping a list of errors which may be obtained again if the error type implements the `Unwrap() []error` method.
- The new [`http.ResponseController`](https://go.dev/pkg/net/http#ResponseController) type provides access to extended per-request functionality not handled by the [`http.ResponseWriter`](https://go.dev/pkg/net/http#ResponseWriter) interface.
- The [`httputil.ReverseProxy`](https://go.dev/pkg/net/http/httputil#ReverseProxy) forwarding proxy includes a new `Rewrite` hook function, superseding the previous `Director` hook.
- The new [`context.WithCancelCause`](https://go.dev/pkg/context#WithCancelCause) function provides a way to cancel a context with a given error. That error can be retrieved by calling the new [`context.Cause`](https://go.dev/pkg/context#Cause) function.
- The new [`os/exec.Cmd`](https://go.dev/pkg/os/exec#Cmd) fields [`Cancel`](https://go.dev/pkg/os/exec#Cmd.Cancel) and [`WaitDelay`](https://go.dev/pkg/os/exec#Cmd.WaitDelay) specify the behavior of the `Cmd` when its associated `Context` is canceled or its process exits.

### Improved performance

- Compiler and garbage collector improvements have reduced memory overhead and improved overall CPU performance by up to 2%.
- Work specifically targeting compilation times led to build improvements by up to 10%. This brings build speeds back in line with Go 1.17.

When [building a Go release from source](https://go.dev/doc/install/source), Go 1.20 requires a Go 1.17.13 or newer release. In the future, we plan to move the bootstrap toolchain forward approximately once a year. Also, starting with Go 1.21, some older operating systems will no longer be supported: this includes Windows 7, 8, Server 2008 and Server 2012, macOS 10.13 High Sierra, and 10.14 Mojave. On the other hand, Go 1.20 adds experimental support for FreeBSD on RISC-V.

For a complete and more detailed list of all changes see the [full release notes](https://go.dev/doc/go1.20).

Thanks to everyone who contributed to this release by writing code, filing bugs, sharing feedback, and testing the release candidates. Your efforts helped to ensure that Go 1.20 is as stable as possible. As always, if you notice any problems, please [file an issue](https://go.dev/issue/new).

Enjoy Go 1.20!


## Profile-guided optimization preview

Michael Pratt  
8 February 2023

When you build a Go binary, the Go compiler performs optimizations to try to generate the best performing binary it can. For example, constant propagation can evaluate constant expressions at compile time, avoiding runtime evaluation cost. Escape analysis avoids heap allocations for locally-scoped objects, avoiding GC overheads. Inlining copies the body of simple functions into callers, often enabling further optimization in the caller (such as additional constant propagation or better escape analysis).

Go improves optimizations from release to release, but this is not always an easy task. Some optimizations are tunable, but the compiler can’t just “turn it up to 11” on every function because overly aggressive optimizations can actually hurt performance or cause excessive build times. Other optimizations require the compiler to make a judgment call about what the “common” and “uncommon” paths in a function are. The compiler must make a best guess based on static heuristics because it can’t know which cases will be common at run time.

Or can it?

With no definitive information about how the code is used in a production environment, the compiler can operate only on the source code of packages. But we do have a tool to evaluate production behavior: [profiling](https://go.dev/doc/diagnostics#profiling). If we provide a profile to the compiler, it can make more informed decisions: more aggressively optimizing the most frequently used functions, or more accurately selecting common cases.

Using profiles of application behavior for compiler optimization is known as _Profile-Guided Optimization (PGO)_ (also known as Feedback-Directed Optimization (FDO)).

Go 1.20 includes initial support for PGO as a preview. See the [profile-guided optimization user guide](https://go.dev/doc/pgo) for complete documentation. There are still some rough edges that may prevent production use, but we would love for you to try it out and [send us any feedback or issues you encounter](https://go.dev/issue/new).

### Example

Let’s build a service that converts Markdown to HTML: users upload Markdown source to `/render`, which returns the HTML conversion. We can use [`gitlab.com/golang-commonmark/markdown`](https://pkg.go.dev/gitlab.com/golang-commonmark/markdown) to implement this easily.

#### Set up

```
$ go mod init example.com/markdown
$ go get gitlab.com/golang-commonmark/markdown@bf3e522c626a
```

In `main.go`:

```go
package main

import (
    "bytes"
    "io"
    "log"
    "net/http"
    _ "net/http/pprof"

    "gitlab.com/golang-commonmark/markdown"
)

func render(w http.ResponseWriter, r *http.Request) {
    if r.Method != "POST" {
        http.Error(w, "Only POST allowed", http.StatusMethodNotAllowed)
        return
    }

    src, err := io.ReadAll(r.Body)
    if err != nil {
        log.Printf("error reading body: %v", err)
        http.Error(w, "Internal Server Error", http.StatusInternalServerError)
        return
    }

    md := markdown.New(
        markdown.XHTMLOutput(true),
        markdown.Typographer(true),
        markdown.Linkify(true),
        markdown.Tables(true),
    )

    var buf bytes.Buffer
    if err := md.Render(&buf, src); err != nil {
        log.Printf("error converting markdown: %v", err)
        http.Error(w, "Malformed markdown", http.StatusBadRequest)
        return
    }

    if _, err := io.Copy(w, &buf); err != nil {
        log.Printf("error writing response: %v", err)
        http.Error(w, "Internal Server Error", http.StatusInternalServerError)
        return
    }
}

func main() {
    http.HandleFunc("/render", render)
    log.Printf("Serving on port 8080...")
    log.Fatal(http.ListenAndServe(":8080", nil))
}

```

Build and run the server:

```
$ go build -o markdown.nopgo.exe
$ ./markdown.nopgo.exe
2023/01/19 14:26:24 Serving on port 8080...
```

Let’s try sending some Markdown from another terminal. We can use the README from the Go project as a sample document:

```
$ curl -o README.md -L "https://raw.githubusercontent.com/golang/go/c16c2c49e2fa98ae551fc6335215fadd62d33542/README.md"
$ curl --data-binary @README.md http://localhost:8080/render
<h1>The Go Programming Language</h1>
<p>Go is an open source programming language that makes it easy to build simple,
reliable, and efficient software.</p>
...
```

#### Profiling

Now that we have a working service, let’s collect a profile and rebuild with PGO to see if we get better performance.

In `main.go`, we imported [net/http/pprof](https://pkg.go.dev/net/http/pprof) which automatically adds a `/debug/pprof/profile` endpoint to the server for fetching a CPU profile.

Normally you want to collect a profile from your production environment so that the compiler gets a representative view of behavior in production. Since this example doesn’t have a “production” environment, we will create a simple program to generate load while we collect a profile. Copy the source of [this program](https://go.dev/play/p/yYH0kfsZcpL) to `load/main.go` and start the load generator (make sure the server is still running!).

```
$ go run example.com/markdown/load
```

While that is running, download a profile from the server:

```
$ curl -o cpu.pprof "http://localhost:8080/debug/pprof/profile?seconds=30"
```

Once this completes, kill the load generator and the server.

#### Using the profile

We can ask the Go toolchain to build with PGO using the `-pgo` flag to `go build`. `-pgo` takes either the path to the profile to use, or `auto`, which will use the `default.pgo` file in the main package directory.

We recommending commiting `default.pgo` profiles to your repository. Storing profiles alongside your source code ensures that users automatically have access to the profile simply by fetching the repository (either via the version control system, or via `go get`) and that builds remain reproducible. In Go 1.20, `-pgo=off` is the default, so users still need to add `-pgo=auto`, but a future version of Go is expected to change the default to `-pgo=auto`, automatically giving anyone that builds the binary the benefit of PGO.

Let’s build:

```
$ mv cpu.pprof default.pgo
$ go build -pgo=auto -o markdown.withpgo.exe
```

#### Evaluation

We will use a Go benchmark version of the load generator to evaluate the effect of PGO on performance. Copy [this benchmark](https://go.dev/play/p/6FnQmHfRjbh) to `load/bench_test.go`.

First, we will benchmark the server without PGO. Start that server:

```
$ ./markdown.nopgo.exe
```

While that is running, run several benchmark iterations:

```
$ go test example.com/markdown/load -bench=. -count=20 -source ../README.md > nopgo.txt
```

Once that completes, kill the original server and start the version with PGO:

```
$ ./markdown.withpgo.exe
```

While that is running, run several benchmark iterations:

```
$ go test example.com/markdown/load -bench=. -count=20 -source ../README.md > withpgo.txt
```

Once that completes, let’s compare the results:

```
$ go install golang.org/x/perf/cmd/benchstat@latest
$ benchstat nopgo.txt withpgo.txt
goos: linux
goarch: amd64
pkg: example.com/markdown/load
cpu: Intel(R) Xeon(R) W-2135 CPU @ 3.70GHz
        │  nopgo.txt  │            withpgo.txt             │
        │   sec/op    │   sec/op     vs base               │
Load-12   393.8µ ± 1%   383.6µ ± 1%  -2.59% (p=0.000 n=20)
```

The new version is around 2.6% faster! In Go 1.20, workloads typically get between 2% and 4% CPU usage improvements from enabling PGO. Profiles contain a wealth of information about application behavior and Go 1.20 just begins to crack the surface by using this information for inlining. Future releases will continue improving performance as more parts of the compiler take advantage of PGO.

### Next steps

In this example, after collecting a profile, we rebuilt our server using the exact same source code used in the original build. In a real-world scenario, there is always ongoing development. So we may collect a profile from production, which is running last week’s code, and use it to build with today’s source code. That is perfectly fine! PGO in Go can handle minor changes to source code without issue.

For much more information on using PGO, best practices and caveats to be aware of, please see the [profile-guided optimization user guide](https://go.dev/doc/pgo).

Please send us your feedback! PGO is still in preview and we’d love to hear about anything that is difficult to use, doesn’t work correctly, etc. Please file issues at [https://go.dev/issue/new](https://go.dev/issue/new).


## All your comparable types

Robert Griesemer  
17 February 2023

On February 1 we released our latest Go version, 1.20, which included a few language changes. Here we’ll discuss one of those changes: the predeclared `comparable` type constraint is now satisfied by all [comparable types](https://go.dev/ref/spec#Comparison_operators). Surprisingly, before Go 1.20, some comparable types did not satisfy `comparable`!

If you’re confused, you’ve come to the right place. Consider the valid map declaration

```Go
var lookupTable map[any]string
```

where the map’s key type is `any` (which is a [comparable type](https://go.dev/ref/spec#Comparison_operators)). This works perfectly fine in Go. On the other hand, before Go 1.20, the seemingly equivalent generic map type

```Go
type genericLookupTable[K comparable, V any] map[K]V
```

could be used just like a regular map type, but produced a compile-time error when `any` was used as the key type:

```Go
var lookupTable genericLookupTable[any, string] // ERROR: any does not implement comparable (Go 1.18 and Go 1.19)
```

Starting with Go 1.20 this code will compile just fine.

The pre-Go 1.20 behavior of `comparable` was particularly annoying because it prevented us from writing the kind of generic libraries we were hoping to write with generics in the first place. The proposed [`maps.Clone`](https://go.dev/issue/57436) function

```Go
func Clone[M ~map[K]V, K comparable, V any](m M) M { … }
```

can be written but could not be used for a map such as `lookupTable` for the same reason our `genericLookupTable` could not be used with `any` as key type.

In this blog post, we hope to shine some light on the language mechanics behind all this. In order to do so, we start with a bit of background information.

### Type parameters and constraints

Go 1.18 introduced generics and, with that, [_type parameters_](https://go.dev/ref/spec#Type_parameter_declarations) as a new language construct.

In an ordinary function, a parameter ranges over a set of values that is restricted by its type. Analogously, in a generic function (or type), a type parameter ranges over a set of types that is restricted by its [_type constraint_](https://go.dev/ref/spec#Type_constraints). Thus, a type constraint defines the _set of types_ that are permissible as type arguments.

Go 1.18 also changed how we view interfaces: while in the past an interface defined a set of methods, now an interface defines a set of types. This new view is completely backward compatible: for any given set of methods defined by an interface, we can imagine the (infinite) set of all types that implement those methods. For instance, given an [`io.Writer`](https://go.dev/pkg/io#Writer) interface, we can imagine the infinite set of all types that have a `Write` method with the appropriate signature. All of these types _implement_ the interface because they all have the required `Write` method.

But the new type set view is more powerful than the old method set one: we can describe a set of types explicitly, not only indirectly through methods. This gives us new ways to control a type set. Starting with Go 1.18, an interface may embed not just other interfaces, but any type, a union of types, or an infinite set of types that share the same [underlying type](https://go.dev/ref/spec#Underlying_types). These types are then included in the [type set computation](https://go.dev/ref/spec#General_interfaces): the union notation `A|B` means “type `A` or type `B`”, and the `~T` notation stands for “all types that have the underlying type `T`”. For instance, the interface

```Go
interface {
    ~int | ~string
    io.Writer
}
```

defines the set of all types whose underlying types are either `int` or `string` and that also implement `io.Writer`’s `Write` method.

Such generalized interfaces can’t be used as variable types. But because they describe type sets they are used as type constraints, which are sets of types. For instance, we can write a generic `min` function

```Go
func min[P interface{ ~int64 | ~float64 }](x, y P) P
```

which accepts any `int64` or `float64` argument. (Of course, a more realistic implementation would use a constraint that enumerates all basic types with an `<` operator.)

As an aside, because enumerating explicit types without methods is common, a little bit of [syntactic sugar](https://en.wikipedia.org/wiki/Syntactic_sugar) allows us to [omit the enclosing `interface{}`](https://go.dev/ref/spec#General_interfaces), leading to the compact and more idiomatic

```Go
func min[P ~int64 | ~float64](x, y P) P { … }
```

With the new type set view we also need a new way to explain what it means to [_implement_](https://go.dev/ref/spec#Implementing_an_interface) an interface. We say that a (non-interface) type `T` implements an interface `I` if `T` is an element of the interface’s type set. If `T` is an interface itself, it describes a type set. Every single type in that set must also be in the type set of `I`, otherwise `T` would contain types that do not implement `I`. Thus, if `T` is an interface, it implements interface `I` if the type set of `T` is a subset of the type set of `I`.

Now we have all the ingredients in place to understand constraint satisfaction. As we have seen earlier, a type constraint describes the set of acceptable argument types for a type parameter. A type argument satisfies the corresponding type parameter constraint if the type argument is in the set described by the constraint interface. This is another way of saying that the type argument implements the constraint. In Go 1.18 and Go 1.19, constraint satisfaction meant constraint implementation. As we’ll see in a bit, in Go 1.20 constraint satisfaction is not quite constraint implementation anymore.

### Operations on type parameter values

A type constraint does not just specify what type arguments are acceptable for a type parameter, it also determines the operations that are possible on values of a type parameter. As we would expect, if a constraint defines a method such as `Write`, the `Write` method can be called on a value of the respective type parameter. More generally, an operation such as `+` or `*` that is supported by all types in the type set defined by a constraint is permitted with values of the corresponding type parameter.

For instance, given the `min` example, in the function body any operation that is supported by `int64` and `float64` types is permitted on values of the type parameter `P`. That includes all the basic arithmetic operations, but also comparisons such as `<`. But it does not include bitwise operations such as `&` or `|` because those operations are not defined on `float64` values.

### Comparable types

In contrast to other unary and binary operations, `==` is defined on not just a limited set of [predeclared types](https://go.dev/ref/spec#Types), but on an infinite variety of types, including arrays, structs, and interfaces. It is impossible to enumerate all these types in a constraint. We need a different mechanism to express that a type parameter must support `==` (and `!=`, of course) if we care about more than predeclared types.

We solve this problem through the predeclared type [`comparable`](https://go.dev/ref/spec#Predeclared_identifiers), introduced with Go 1.18. `comparable` is an interface type whose type set is the infinite set of comparable types, and that may be used as a constraint whenever we require a type argument to support `==`.

Yet, the set of types comprised by `comparable` is not the same as the set of all [comparable types](https://go.dev/ref/spec#Comparison_operators) defined by the Go spec. [By construction](https://go.dev/ref/spec#Interface_types), a type set specified by an interface (including `comparable`) does not contain the interface itself (or any other interface). Thus, an interface such as `any` is not included in `comparable`, even though all interfaces support `==`. What gives?

Comparison of interfaces (and of composite types containing them) may panic at run time: this happens when the dynamic type, the type of the actual value stored in the interface variable, is not comparable. Consider our original `lookupTable` example: it accepts arbitrary values as keys. But if we try to enter a value with a key that does not support `==`, say a slice value, we get a run-time panic:

```Go
lookupTable[[]int{}] = "slice"  // PANIC: runtime error: hash of unhashable type []int
```

By contrast, `comparable` contains only types that the compiler guarantees will not panic with `==`. We call these types _strictly comparable_.

Most of the time this is exactly what we want: it’s comforting to know that `==` in a generic function won’t panic if the operands are constrained by `comparable`, and it is what we would intuitively expect.

Unfortunately, this definition of `comparable` together with the rules for constraint satisfaction prevented us from writing useful generic code, such as the `genericLookupTable` type shown earlier: for `any` to be an acceptable argument type, `any` must satisfy (and therefore implement) `comparable`. But the type set of `any` is larger than (not a subset of) the type set of `comparable` and therefore does not implement `comparable`.

```Go
var lookupTable GenericLookupTable[any, string] // ERROR: any does not implement comparable (Go 1.18 and Go 1.19)
```

Users recognized the problem early on and filed a multitude of issues and proposals in short order ([#51338](https://go.dev/issue/51338), [#52474](https://go.dev/issue/52474), [#52531](https://go.dev/issue/52531), [#52614](https://go.dev/issue/52614), [#52624](https://go.dev/issue/52624), [#53734](https://go.dev/issue/53734), etc). Clearly this was a problem we needed to address.

The “obvious” solution was simply to include even non-strictly comparable types in the `comparable` type set. But this leads to inconsistencies with the type set model. Consider the following example:

```Go
func f[Q comparable]() { … }

func g[P any]() {
        _ = f[int] // (1) ok: int implements comparable
        _ = f[P]   // (2) error: type parameter P does not implement comparable
        _ = f[any] // (3) error: any does not implement comparable (Go 1.18, Go.19)
}
```

Function `f` requires a type argument that is strictly comparable. Obviously it is ok to instantiate `f` with `int`: `int` values never panic on `==` and thus `int` implements `comparable` (case 1). On the other hand, instantiating `f` with `P` is not permitted: `P`’s type set is defined by its constraint `any`, and `any` stands for the set of all possible types. This set includes types that are not comparable at all. Hence, `P` doesn’t implement `comparable` and thus cannot be used to instantiate `f` (case 2). And finally, using the type `any` (rather than a type parameter constrained by `any`) doesn’t work either, because of exactly the same problem (case 3).

Yet, we do want to be able to use the type `any` as type argument in this case. The only way out of this dilemma was to change the language somehow. But how?

### Interface implementation vs constraint satisfaction

As mentioned earlier, constraint satisfaction is interface implementation: a type argument `T` satisfies a constraint `C` if `T` implements `C`. This makes sense: `T` must be in the type set expected by `C` which is exactly the definition of interface implementation.

But this is also the problem because it prevents us from using non-strictly comparable types as type arguments for `comparable`.

So for Go 1.20, after almost a year of publicly discussing numerous alternatives (see the issues mentioned above), we decided to introduce an exception for just this case. To avoid the inconsistency, rather than changing what `comparable` means, we differentiated between _interface implementation_, which is relevant for passing values to variables, and _constraint satisfaction_, which is relevant for passing type arguments to type parameters. Once separated, we could give each of those concepts (slightly) different rules, and that is exactly what we did with proposal [#56548](https://go.dev/issue/56548).

The good news is that the exception is quite localized in the [spec](https://go.dev/ref/spec#Satisfying_a_type_constraint). Constraint satisfaction remains almost the same as interface implementation, with a caveat:

> A type `T` satisfies a constraint `C` if
> 
> - `T` implements `C`; or
> - `C` can be written in the form `interface{ comparable; E }`, where `E` is a basic interface and `T` is [comparable](https://go.dev/ref/spec#Comparison_operators) and implements `E`.

The second bullet point is the exception. Without going too much into the formalism of the spec, what the exception says is the following: a constraint `C` that expects strictly comparable types (and which may also have other requirements such as methods `E`) is satisfied by any type argument `T` that supports `==` (and which also implements the methods in `E`, if any). Or even shorter: a type that supports `==` also satisfies `comparable` (even though it may not implement it).

We can immediately see that this change is backward-compatible: before Go 1.20, constraint satisfaction was the same as interface implementation, and we still have that rule (1st bullet point). All code that relied on that rule continues to work as before. Only if that rule fails do we need to consider the exception.

Let’s revisit our previous example:

```Go
func f[Q comparable]() { … }

func g[P any]() {
        _ = f[int] // (1) ok: int satisfies comparable
        _ = f[P]   // (2) error: type parameter P does not satisfy comparable
        _ = f[any] // (3) ok: satisfies comparable (Go 1.20)
}
```

Now, `any` does satisfy (but not implement!) `comparable`. Why? Because Go permits `==` to be used with values of type `any` (which corresponds to the type `T` in the spec rule), and because the constraint `comparable` (which corresponds to the constraint `C` in the rule) can be written as `interface{ comparable; E }` where `E` is simply the empty interface in this example (case 3).

Interestingly, `P` still does not satisfy `comparable` (case 2). The reason is that `P` is a type parameter constrained by `any` (it _is not_ `any`). The operation `==` is _not_ available with all types in the type set of `P` and thus not available on `P`; it is not a [comparable type](https://go.dev/ref/spec#Comparison_operators). Therefore the exception doesn’t apply. But this is ok: we do like to know that `comparable`, the strict comparability requirement, is enforced most of the time. We just need an exception for Go types that support `==`, essentially for historical reasons: we always had the ability to compare non-strictly comparable types.

### Consequences and remedies

We gophers take pride in the fact that language-specific behavior can be explained and reduced to a fairly compact set of rules, spelled out in the language spec. Over the years we have refined these rules, and when possible made them simpler and often more general. We also have been careful to keep the rules orthogonal, always on the lookout for unintended and unfortunate consequences. Disputes are resolved by consulting the spec, not by decree. That is what we have aspired to since the inception of Go.

_One does not simply add an exception to a carefully crafted type system without consequences!_

So where’s the catch? There’s an obvious (if mild) drawback, and a less obvious (and more severe) one. Obviously, we now have a more complex rule for constraint satisfaction which is arguably less elegant than what we had before. This is unlikely to affect our day-to-day work in any significant way.

But we do pay a price for the exception: in Go 1.20, generic functions that rely on `comparable` are not statically type-safe anymore. The `==` and `!=` operations may panic if applied to operands of `comparable` type parameters, even though the declaration says that they are strictly comparable. A single non-comparable value may sneak its way through multiple generic functions or types by way of a single non-strictly comparable type argument and cause a panic. In Go 1.20 we can now declare

```Go
var lookupTable genericLookupTable[any, string]
```

without compile-time error, but we will get a run-time panic if we ever use a non-strictly comparable key type in this case, exactly like we would with the built-in `map` type. We have given up static type safety for a run-time check.

There may be situations where this is not good enough, and where we want to enforce strict comparability. The following observation allows us to do exactly that, at least in limited form: type parameters do not benefit from the exception that we added to the constraint satisfaction rule. For instance, in our earlier example, the type parameter `P` in the function `g` is constrained by `any` (which by itself is comparable but not strictly comparable) and so `P` does not satisfy `comparable`. We can use this knowledge to craft a compile-time assertion of sorts for a given type `T`:

```Go
type T struct { … }
```

We want to assert that `T` is strictly comparable. It’s tempting to write something like:

```Go
// isComparable may be instantiated with any type that supports ==
// including types that are not strictly comparable because of the
// exception for constraint satisfaction.
func isComparable[_ comparable]() {}

// Tempting but not quite what we want: this declaration is also
// valid for types T that are not strictly comparable.
var _ = isComparable[T] // compile-time error if T does not support ==
```

The dummy (blank) variable declaration serves as our “assertion”. But because of the exception in the constraint satisfaction rule, `isComparable[T]` only fails if `T` is not comparable at all; it will succeed if `T` supports `==`. We can work around this problem by using `T` not as a type argument, but as a type constraint:

```Go
func _[P T]() {
    _ = isComparable[P] // P supports == only if T is strictly comparable
}
```

Here is a [passing](https://go.dev/play/p/9i9iEto3TgE) and [failing](https://go.dev/play/p/5d4BeKLevPB) playground example illustrating this mechanism.

### Final observations

Interestingly, until two months before the Go 1.18 release, the compiler implemented constraint satisfaction exactly as we do now in Go 1.20. But because at that time constraint satisfaction meant interface implementation, we did have an implementation that was inconsistent with the language specification. We were alerted to this fact with [issue #50646](https://go.dev/issue/50646). We were extremely close to the release and had to make a decision quickly. In the absence of a convincing solution, it seemed safest to make the implementation consistent with the spec. A year later, and with plenty of time to consider different approaches, it seems that the implementation we had was the implementation we want in the first place. We have come full circle.

As always, please let us know if anything doesn’t work as expected by filing issues at [https://go.dev/issue/new](https://go.dev/issue/new).

Thank you!


## Code coverage for Go integration tests

Than McIntosh  
8 March 2023

Code coverage tools help developers determine what fraction of a source code base is executed (covered) when a given test suite is executed.

Go has for some time provided support ([introduced](https://go.dev/blog/cover) in the Go 1.2 release) to measure code coverage at the package level, using the **"-cover"** flag of the “go test” command.

This tooling works well in most cases, but has some weaknesses for larger Go applications. For such applications, developers often write “integration” tests that verify the behavior of an entire program (in addition to package-level unit tests).

This type of test typically involves building a complete application binary, then running the binary on a set of representative inputs (or under production load, if it is a server) to ensure that all of the component packages are working correctly together, as opposed to testing individual packages in isolation.

Because the integration test binaries are built with “go build” and not “go test”, Go’s tooling didn’t provide any easy way to collect a coverage profile for these tests, up until now.

With Go 1.20, you can now build coverage-instrumented programs using “go build -cover”, then feed these instrumented binaries into an integration test to extend the scope of coverage testing.

In this blog post we’ll give an example of how these new features work, and outline some of the use cases and workflow for collecting coverage profiles from integration tests.

### Example

Let’s take a very small example program, write a simple integration test for it, and then collect a coverage profile from the integration test.

For this exercise we’ll use the “mdtool” markdown processing tool from [`gitlab.com/golang-commonmark/mdtool`](https://pkg.go.dev/gitlab.com/golang-commonmark/mdtool). This is a demo program designed to show how clients can use the package [`gitlab.com/golang-commonmark/markdown`](https://pkg.go.dev/gitlab.com/golang-commonmark/markdown), a markdown-to-HTML conversion library.

### Set up for mdtool

First let’s download a copy of “mdtool” itself (we’re picking a specific version just to make these steps reproducible):

```
$ git clone https://gitlab.com/golang-commonmark/mdtool.git
...
$ cd mdtool
$ git tag example e210a4502a825ef7205691395804eefce536a02f
$ git checkout example
...
$
```

### A simple integration test

Now we’ll write a simple integration test for “mdtool”; our test will build the “mdtool” binary, then run it on a set of input markdown files. This very simple script runs the “mdtool” binary on each file from a test data directory, checking to make sure that it produces some output and doesn’t crash.

```
$ cat integration_test.sh
#!/bin/sh
BUILDARGS="$*"
#
# Terminate the test if any command below does not complete successfully.
#
set -e
#
# Download some test inputs (the 'website' repo contains various *.md files).
#
if [ ! -d testdata ]; then
  git clone https://go.googlesource.com/website testdata
  git -C testdata tag example 8bb4a56901ae3b427039d490207a99b48245de2c
  git -C testdata checkout example
fi
#
# Build mdtool binary for testing purposes.
#
rm -f mdtool.exe
go build $BUILDARGS -o mdtool.exe .
#
# Run the tool on a set of input files from 'testdata'.
#
FILES=$(find testdata -name "*.md" -print)
N=$(echo $FILES | wc -w)
for F in $FILES
do
  ./mdtool.exe +x +a $F > /dev/null
done
echo "finished processing $N files, no crashes"
$
```

Here is an example run of our test:

```
$ /bin/sh integration_test.sh
...
finished processing 380 files, no crashes
$
```

Success: we’ve verified that the “mdtool” binary successfully digested a set of input files… but how much of the tool’s source code have we actually exercised? In the next section we’ll collect a coverage profile to find out.

### Using the integration test to collect coverage data

Let’s write another wrapper script that invokes the previous script, but builds the tool for coverage and then post-processes the resulting profiles:

```
$ cat wrap_test_for_coverage.sh
#!/bin/sh
set -e
PKGARGS="$*"
#
# Setup
#
rm -rf covdatafiles
mkdir covdatafiles
#
# Pass in "-cover" to the script to build for coverage, then
# run with GOCOVERDIR set.
#
GOCOVERDIR=covdatafiles \
  /bin/sh integration_test.sh -cover $PKGARGS
#
# Post-process the resulting profiles.
#
go tool covdata percent -i=covdatafiles
$
```

Some key things to note about the wrapper above:

- it passes in the “-cover” flag when running `integration_test.sh`, which gives us a coverage-instrumented “mdtool.exe” binary
- it sets the GOCOVERDIR environment variable to a directory into which coverage data files will be written
- when the test is complete, it runs “go tool covdata percent” to produce a report on percentage of statements covered

Here’s the output when we run this new wrapper script:

```
$ /bin/sh wrap_test_for_coverage.sh
...
    gitlab.com/golang-commonmark/mdtool coverage: 48.1% of statements
$
# Note: covdatafiles now contains 381 files.
```

Voila! We now have some idea of how well our integration tests work in exercising the “mdtool” application’s source code.

If we make changes to enhance the test harness, then do a second coverage collection run, we’ll see the changes reflected in the coverage report. For example, suppose we improve our test by adding these two additional lines to `integration_test.sh`:

```
./mdtool.exe +ty testdata/README.md  > /dev/null
./mdtool.exe +ta < testdata/README.md  > /dev/null
```

Running the coverage testing wrapper again:

```
$ /bin/sh wrap_test_for_coverage.sh
finished processing 380 files, no crashes
    gitlab.com/golang-commonmark/mdtool coverage: 54.6% of statements
$
```

We can see the effects of our change: statement coverage has increased from 48% to 54%.

### Selecting packages to cover

By default, “go build -cover” will instrument just the packages that are part of the Go module being built, which in this case is the `gitlab.com/golang-commonmark/mdtool` package. In some cases however it is useful to extend coverage instrumentation to other packages; this can be accomplished by passing “-coverpkg” to “go build -cover”.

For our example program, “mdtool” is in fact largely just a wrapper around the package `gitlab.com/golang-commonmark/markdown`, so it is interesting to include `markdown` in the set of packages that are instrumented.

Here’s the `go.mod` file for “mdtool”:

```
$ head go.mod
module gitlab.com/golang-commonmark/mdtool

go 1.17

require (
    github.com/pkg/browser v0.0.0-20210911075715-681adbf594b8
    gitlab.com/golang-commonmark/markdown v0.0.0-20211110145824-bf3e522c626a
)
```

We can use the “-coverpkg” flag to control which packages are selected for inclusion in the coverage analysis to include one of the deps above. Here’s an example:

```
$ /bin/sh wrap_test_for_coverage.sh -coverpkg=gitlab.com/golang-commonmark/markdown,gitlab.com/golang-commonmark/mdtool
...
    gitlab.com/golang-commonmark/markdown   coverage: 70.6% of statements
    gitlab.com/golang-commonmark/mdtool coverage: 54.6% of statements
$
```

### Working with coverage data files

When a coverage integration test has completed and written out a set of raw data files (in our case, the contents of the `covdatafiles` directory), we can post-process these files in various ways.

#### Converting profiles to ‘-coverprofile’ text format

When working with unit tests, you can run `go test -coverprofile=abc.txt` to write a text-format coverage profile for a given coverage test run.

With binaries built with `go build -cover`, you can generate a text-format profile after the fact by running `go tool covdata textfmt` on the files emitted into the GOCOVERDIR directory.

Once this step is complete, you can use `go tool cover -func=<file>` or `go tool cover -html=<file>` to interpret/visualize the data just as you would with `go test -coverprofile`.

Example:

```
$ /bin/sh wrap_test_for_coverage.sh
...
$ go tool covdata textfmt -i=covdatafiles -o=cov.txt
$ go tool cover -func=cov.txt
gitlab.com/golang-commonmark/mdtool/main.go:40:     readFromStdin   100.0%
gitlab.com/golang-commonmark/mdtool/main.go:44:     readFromFile    80.0%
gitlab.com/golang-commonmark/mdtool/main.go:54:     readFromWeb 0.0%
gitlab.com/golang-commonmark/mdtool/main.go:64:     readInput   80.0%
gitlab.com/golang-commonmark/mdtool/main.go:74:     extractText 100.0%
gitlab.com/golang-commonmark/mdtool/main.go:88:     writePreamble   100.0%
gitlab.com/golang-commonmark/mdtool/main.go:111:    writePostamble  100.0%
gitlab.com/golang-commonmark/mdtool/main.go:118:    handler     0.0%
gitlab.com/golang-commonmark/mdtool/main.go:139:    main        51.6%
total:                          (statements)    54.6%
$
```

#### Merging raw profiles with ‘go tool covdata merge’

Each execution of a “-cover” built application will write out one or more data files to the directory specified in the GOCOVERDIR environment variable. If an integration test performs N program executions, you’ll wind up with O(N) files in your output directory. There is typically a lot of duplicated content in the data files, so to compact the data and/or combine data sets from different integration test runs, you can use the `go tool covdata merge` command to merge profiles together. Example:

```
$ /bin/sh wrap_test_for_coverage.sh
finished processing 380 files, no crashes
    gitlab.com/golang-commonmark/mdtool coverage: 54.6% of statements
$ ls covdatafiles
covcounters.13326b42c2a107249da22f6e0d35b638.772307.1677775306041466651
covcounters.13326b42c2a107249da22f6e0d35b638.772314.1677775306053066987
...
covcounters.13326b42c2a107249da22f6e0d35b638.774973.1677775310032569308
covmeta.13326b42c2a107249da22f6e0d35b638
$ ls covdatafiles | wc
    381     381   27401
$ rm -rf merged ; mkdir merged ; go tool covdata merge -i=covdatafiles -o=merged
$ ls merged
covcounters.13326b42c2a107249da22f6e0d35b638.0.1677775331350024014
covmeta.13326b42c2a107249da22f6e0d35b638
$
```

The `go tool covdata merge` operation also accepts a `-pkg` flag that can be used to select out a specific package or set of packages, if that is desired.

This merge capability is also useful to combine results from different types of test runs, including runs generated by other test harnesses.

### Wrap-up

That covers it: with the 1.20 release, Go’s coverage tooling is no longer limited to package tests, but supports collecting profiles from larger integration tests. We hope you will make good use of the new features to help understand how well your larger and more complicated tests are working, and which parts of your source code they are exercising.

Please try out these new features, and as always if you encounter problems, file issues on our [GitHub issue tracker](https://github.com/golang/go/issues). Thanks.






