---
title: "Pytorch"
date: 2022-07-15T08:55:59+08:00
lastmod:
tags: []
categories: []
slug:
draft: true
---


- [PyTorch深度学习实战](#pytorch深度学习实战)
  - [基础](#基础)
  - [01 | PyTorch：网红中的顶流明星](#01--pytorch网红中的顶流明星)
    - [PyTorch 登场](#pytorch-登场)
    - [安装 PyTorch 及其使用环境](#安装-pytorch-及其使用环境)
      - [使用 pip 安装 PyTorch](#使用-pip-安装-pytorch)
      - [其它方法安装 PyTorch](#其它方法安装-pytorch)
      - [验证是否安装成功](#验证是否安装成功)
      - [使用 Docker](#使用-docker)
    - [常用编程工具](#常用编程工具)
      - [Sublime Text](#sublime-text)
      - [PyCharm](#pycharm)
      - [Vim](#vim)
      - [Jupyter Notebook\&Lab](#jupyter-notebooklab)
      - [使用 pip 安装 Jupyter](#使用-pip-安装-jupyter)
      - [启动 Jupyter](#启动-jupyter)
      - [运行 Jupyter Notebook](#运行-jupyter-notebook)
    - [小结](#小结)
  - [02 | NumPy（上）：核心数据结构详解](#02--numpy上核心数据结构详解)
    - [什么是 NumPy](#什么是-numpy)
    - [NumPy 数组](#numpy-数组)
      - [创建数组](#创建数组)
      - [数组的属性](#数组的属性)
      - [其他创建数组的方式](#其他创建数组的方式)
    - [数组的轴](#数组的轴)
    - [小结](#小结-1)
  - [03 | NumPy（下）：深度学习中的常用操作](#03--numpy下深度学习中的常用操作)
    - [数据加载阶段](#数据加载阶段)
      - [Pillow 方式](#pillow-方式)
      - [OpenCV 方式：](#opencv-方式)
      - [索引与切片](#索引与切片)
      - [数组的拼接](#数组的拼接)
      - [方法一：使用 np.newaxis](#方法一使用-npnewaxis)
      - [方法二：直接赋值](#方法二直接赋值)
      - [深拷贝（副本）与浅拷贝（视图）](#深拷贝副本与浅拷贝视图)
    - [模型评估](#模型评估)
      - [Argmax Vs Argmin：](#argmax-vs-argmin)
      - [Argsort：数组排序后返回原数组的索引](#argsort数组排序后返回原数组的索引)
    - [小结](#小结-2)
  - [04 | Tensor：PyTorch中最基础的计算单元](#04--tensorpytorch中最基础的计算单元)
    - [什么是 Tensor](#什么是-tensor)
    - [Tensor 的类型、创建及转换](#tensor-的类型创建及转换)
      - [Tensor 的类型](#tensor-的类型)
      - [Tensor 的创建](#tensor-的创建)
      - [创建特殊形式的 Tensor](#创建特殊形式的-tensor)
      - [Tensor 的转换](#tensor-的转换)
      - [Tensor 的常用操作](#tensor-的常用操作)
      - [矩阵转秩 (维度转换）](#矩阵转秩-维度转换)
      - [形状变换](#形状变换)
      - [增减维度](#增减维度)
    - [小结](#小结-3)
  - [05 | Tensor变形记：快速掌握Tensor切分、变形等方法](#05--tensor变形记快速掌握tensor切分变形等方法)
    - [Tensor 的连接操作](#tensor-的连接操作)
    - [Tensor 的切分操作](#tensor-的切分操作)
    - [Tensor 的索引操作](#tensor-的索引操作)
    - [小结](#小结-4)
  - [模型训练](#模型训练)
  - [06 | Torchvision（上）：数据读取，训练开始的第一步](#06--torchvision上数据读取训练开始的第一步)
    - [PyTorch 中的数据读取](#pytorch-中的数据读取)
      - [Dataset 类](#dataset-类)
      - [DataLoader 类](#dataloader-类)
    - [什么是 Torchvision](#什么是-torchvision)
    - [利用 Torchvision 读取数据](#利用-torchvision-读取数据)
      - [MNIST 数据集简介](#mnist-数据集简介)
      - [数据读取](#数据读取)
      - [数据预览](#数据预览)
    - [小结](#小结-5)
  - [07 | Torchvision（中）：数据增强，让数据更加多样性](#07--torchvision中数据增强让数据更加多样性)
    - [图像处理工具之 torchvision.transforms](#图像处理工具之-torchvisiontransforms)
      - [数据类型转换](#数据类型转换)
      - [对 PIL.Image 和 Tensor 进行变换](#对-pilimage-和-tensor-进行变换)
    - [只对 Tensor 进行变换](#只对-tensor-进行变换)
      - [标准化](#标准化)
      - [变换的组合](#变换的组合)
    - [结合 datasets 使用](#结合-datasets-使用)
    - [小结](#小结-6)
  - [08 | Torchvision（下）：其他有趣的功能](#08--torchvision下其他有趣的功能)
    - [常见网络模型](#常见网络模型)
      - [torchvision.models 模块](#torchvisionmodels-模块)
      - [实例化一个 GoogLeNet 网络](#实例化一个-googlenet-网络)
      - [模型微调](#模型微调)
    - [其他常用函数](#其他常用函数)
    - [小结](#小结-7)
  - [09 | 卷积（上）：如何用卷积为计算机“开天眼”？](#09--卷积上如何用卷积为计算机开天眼)
    - [卷积](#卷积)
      - [最简单的情况](#最简单的情况)
      - [标准的卷积](#标准的卷积)
      - [Padding](#padding)
    - [PyTorch 中的卷积](#pytorch-中的卷积)
      - [验证 same 方式](#验证-same-方式)
    - [总结](#总结)
  - [10 | 卷积（下）：如何用卷积为计算机“开天眼”？](#10--卷积下如何用卷积为计算机开天眼)
    - [深度可分离卷积（Depthwise Separable Convolution）](#深度可分离卷积depthwise-separable-convolution)
    - [Depthwise（DW）卷积](#depthwisedw卷积)
    - [Pointwise（PW）卷积](#pointwisepw卷积)
      - [计算量](#计算量)
      - [PyTorch 中的实现](#pytorch-中的实现)
    - [空洞卷积](#空洞卷积)
      - [感受野](#感受野)
      - [计算方式](#计算方式)
    - [总结](#总结-1)
  - [11 | 损失函数：如何帮助模型学会“自省”？](#11--损失函数如何帮助模型学会自省)
    - [一个简单的例子](#一个简单的例子)
    - [过拟合与欠拟合](#过拟合与欠拟合)
    - [损失函数与代价函数](#损失函数与代价函数)
    - [常见损失函数](#常见损失函数)
      - [0-1 损失函数](#0-1-损失函数)
      - [平方损失函数](#平方损失函数)
      - [均方差损失函数和平均绝对误差损失函数](#均方差损失函数和平均绝对误差损失函数)
      - [交叉熵损失函数](#交叉熵损失函数)
      - [softmax 损失函数](#softmax-损失函数)
    - [小结](#小结-8)
  - [12 | 计算梯度：网络的前向与反向传播](#12--计算梯度网络的前向与反向传播)
    - [前馈网络](#前馈网络)
    - [导数、梯度与链式法](#导数梯度与链式法)
      - [导数](#导数)
      - [偏导数](#偏导数)
      - [梯度](#梯度)
      - [链式法则](#链式法则)
      - [反向传播](#反向传播)
    - [小结](#小结-9)
  - [13 | 优化方法：更新模型参数的方法](#13--优化方法更新模型参数的方法)
    - [用下山路线规划理解优化方法](#用下山路线规划理解优化方法)
    - [常见的梯度下降方法](#常见的梯度下降方法)
      - [批量梯度下降法（Batch Gradient Descent，BGD）](#批量梯度下降法batch-gradient-descentbgd)
    - [随机梯度下降（Stochastic Gradient Descent，SGD）](#随机梯度下降stochastic-gradient-descentsgd)
    - [小批量梯度下降（Mini-Batch Gradient Descent, MBGD）](#小批量梯度下降mini-batch-gradient-descent-mbgd)
    - [一个简单的抽象例子](#一个简单的抽象例子)
    - [总结](#总结-2)
  - [加餐 | 机器学习其实就那么几件事](#加餐--机器学习其实就那么几件事)
    - [人工智能、机器学习与深度学习](#人工智能机器学习与深度学习)
    - [机器学习（深度学习）](#机器学习深度学习)
    - [有监督学习 Vs 无监督学习](#有监督学习-vs-无监督学习)
    - [常见名词讲解](#常见名词讲解)
      - [训练集与验证集](#训练集与验证集)
      - [Epoch 与 Step](#epoch-与-step)
    - [模型训练本质](#模型训练本质)
  - [14 | 构建网络：一站式实现模型搭建与训练](#14--构建网络一站式实现模型搭建与训练)
    - [构建自己的模型](#构建自己的模型)
    - [nn.Module 模块](#nnmodule-模块)
    - [模型的训练](#模型的训练)
    - [模型保存与加载](#模型保存与加载)
    - [使用 Torchvison 中的模型进行训练](#使用-torchvison-中的模型进行训练)
    - [总结](#总结-3)
  - [15 | 可视化工具：如何实现训练的可视化监控？](#15--可视化工具如何实现训练的可视化监控)
    - [TensorboardX](#tensorboardx)
    - [训练过程可视化](#训练过程可视化)
    - [Visdom](#visdom)
    - [小结](#小结-10)
  - [16｜分布式训练：如何加速你的模型训练？](#16分布式训练如何加速你的模型训练)
    - [分布式训练原理](#分布式训练原理)
      - [单机单卡](#单机单卡)
      - [单机多卡](#单机多卡)
      - [多机多卡](#多机多卡)
    - [小试牛刀](#小试牛刀)
    - [小结](#小结-11)
  - [实战](#实战)
  - [17 | 图像分类（上）：图像分类原理与图像分类模型](#17--图像分类上图像分类原理与图像分类模型)
    - [图像分类原理](#图像分类原理)
      - [感知机](#感知机)
      - [全连接层](#全连接层)
    - [卷积神经网络](#卷积神经网络)
      - [ImageNet](#imagenet)
      - [VGG](#vgg)
      - [GoogLeNet](#googlenet)
      - [ResNet](#resnet)
      - [网络退化问题](#网络退化问题)
      - [残差学习](#残差学习)
    - [小结](#小结-12)
  - [18 | 图像分类（下）：如何构建一个图像分类模型?](#18--图像分类下如何构建一个图像分类模型)
    - [问题回顾](#问题回顾)
    - [数据准备](#数据准备)
    - [模型训练](#模型训练-1)
      - [EfficientNet](#efficientnet)
    - [加载数据](#加载数据)
    - [创建模型](#创建模型)
    - [模型微调](#模型微调-1)
    - [设定损失函数与优化方法](#设定损失函数与优化方法)
    - [模型评估](#模型评估-1)
      - [混淆矩阵](#混淆矩阵)
    - [小结](#小结-13)
  - [19 | 图像分割（上）：详解图像分割原理与图像分割模型](#19--图像分割上详解图像分割原理与图像分割模型)
    - [图像分割](#图像分割)
    - [语义分割原理](#语义分割原理)
      - [分类端](#分类端)
    - [网络结构](#网络结构)
      - [转置卷积](#转置卷积)
    - [损失函数](#损失函数)
    - [公开数据集](#公开数据集)
    - [小结](#小结-14)
  - [20 | 图像分割（下）：如何构建一个图像分割模型？](#20--图像分割下如何构建一个图像分割模型)
    - [数据部分](#数据部分)
      - [分割图像的标记](#分割图像的标记)
      - [数据读取](#数据读取-1)
    - [模型训练](#模型训练-2)
      - [网络结构：UNet](#网络结构unet)
    - [损失函数：Dice Loss](#损失函数dice-loss)
    - [训练流程](#训练流程)
    - [模型预测](#模型预测)
    - [模型评估](#模型评估-2)
    - [小结](#小结-15)
  - [21 | NLP基础（上）：详解自然语言处理原理与常用算法](#21--nlp基础上详解自然语言处理原理与常用算法)
    - [NLP 的应用无处不在](#nlp-的应用无处不在)
    - [NLP 的几个重要内容](#nlp-的几个重要内容)
      - [分词](#分词)
      - [文本表示的方法](#文本表示的方法)
    - [关键词的提取](#关键词的提取)
      - [基于统计特征的方法](#基于统计特征的方法)
      - [基于词图模型的关键词提取](#基于词图模型的关键词提取)
      - [基于主题模型的关键词提取](#基于主题模型的关键词提取)
    - [小结](#小结-16)
  - [22 | NLP基础（下）：详解语言模型与注意力机制](#22--nlp基础下详解语言模型与注意力机制)
    - [语言模型](#语言模型)
      - [统计语言模型](#统计语言模型)
      - [神经网络语言模型](#神经网络语言模型)
    - [注意力机制](#注意力机制)
    - [小结](#小结-17)
  - [23 | 情感分析：如何使用LSTM进行情感分析？](#23--情感分析如何使用lstm进行情感分析)
    - [数据准备](#数据准备-1)
    - [如何用 Torchtext 读取数据集](#如何用-torchtext-读取数据集)
    - [数据处理 pipelines](#数据处理-pipelines)
    - [生成训练数据](#生成训练数据)
    - [模型构建](#模型构建)
    - [模型训练与评估](#模型训练与评估)
    - [小结](#小结-18)
  - [24 | 文本分类：如何使用BERT构建文本分类模型？](#24--文本分类如何使用bert构建文本分类模型)
    - [问题背景与分析](#问题背景与分析)
    - [BERT 原理与特点分析](#bert-原理与特点分析)
    - [安装与准备](#安装与准备)
    - [模型构建](#模型构建-1)
    - [模型训练](#模型训练-3)
    - [小结](#小结-19)
  - [25 | 摘要：如何快速实现自动文摘生成？](#25--摘要如何快速实现自动文摘生成)
    - [问题背景](#问题背景)
    - [抽取与生成](#抽取与生成)
    - [评价指标](#评价指标)
    - [BART 原理与特点分析](#bart-原理与特点分析)
    - [快速文摘生成](#快速文摘生成)
    - [Fine-tuning BART](#fine-tuning-bart)
      - [模型加载](#模型加载)
      - [数据准备](#数据准备-2)
      - [模型训练](#模型训练-4)
    - [小结](#小结-20)

# PyTorch深度学习实战

先做个自我介绍吧！我曾先后供职于百度和腾讯两家公司，任职高级算法研究员，目前在一家国际知名互联网公司 Line China 担任数据科学家，从事计算机视觉与自然语言处理相关的研发工作，每天为千万级别的流量提供深度学习服务。想一想，我进入机器学习与深度学习的研究和应用领域已经有 10 年的时间了，这是个很有意思的过程。在人工智能快速发展的背景下，各种各样的深度学习框架层出不穷，有当下的主流，也有如今的新秀。为什么我会这么说呢？这其实可以追溯到我的研究生时期。最早，我只是把 PyTorch，打上了一个新秀的标签，记得那时候，深度学习的浪潮才刚刚兴起，传统的机器学习开始转到深度学习，但是我们能选的框架却十分有限。当时在学术界流行的一个深度学习框架是 Theano，可能有的同学都没有听说过它。这个框架就像是祖师爷般的存在，从 2008 年诞生之后的很长一段时间中，它都是深度学习开发和研究的行业标准。

为了复现论文中的算法，我开始学习 Theano。接触之后，我发现它的声明式编程，无论是风格还是逻辑都十分奇特。而且那时候的学习资料很匮乏，只能啃官方的说明文档。如此一来，我觉得 Theano 十分晦涩难学，入门门槛非常高。后来，我去到了互联网大厂的核心部门工作。那时学术界已经涌现出了很多深度学习方面的研究，而工业界才刚刚开始将深度学习技术落地。Google 的 TensorFlow 框架于 2015 年正式开源，而我们的团队也开始着手把深度学习技术应用于文本处理等方向。2017 年，Google 发布了 TensorFlow 1.0 版本，到了 2019 年，又发布了 2.0 新版本。TensorFlow 1.x 版本时期，TensorFlow 框架拥有大量的用户。不过，问题也非常明显，主要的弊端就是框架环境配置不兼容，新老版本函数差异也很大，且编程困难。

但凡涉及版本更新，总会出现 API 变化，前后版本不兼容的问题。并且当我阅读别人代码的时候，TensorFlow 1.x 的可读性也不是很高。这些问题都增加了我的学习成本。直到 TensorFlow 2.x 版本，TensorFlow 逐渐借鉴了 PyTorch 的优点，进行了自我完善。而与 TensorFlow 同一时期横空出世，也拥有众多用户的一个深度学习框架还有 Keras。Keras 的 API 对用户十分友好，使用起来很容易上手。如果有什么想法需要快速实验，看一看效果，那 Keras 绝对是不二的选择。但是，高度模块化的封装也同样会带来弊端，看起来学习 Keras 似乎十分容易，但我很快就遇到了瓶颈。高度封装就意味着不够灵活，比如说如果需要修改一些网络底层的结构，Keras 所提供的接口就没有支持。在使用 Keras 的大多数时间里，我们主要都停留在调用接口这个阶段，很难真正学习到深度学习的内容。


直到 PyTorch 出现，随着使用它的人越来越多，其技术迭代速度跟生态发展速度都很迅猛。如果你在 GitHub 找到了一个 PyTorch 项目相关的开源代码，我们可以很容易移植到自己的项目中来，直接站在巨人的肩膀上看世界。而且相比前面那些主流框架，PyTorch 有着对用户友好的命令式编程风格。PyTorch 设计得更科学，无需像 TensorFlow 那样，要在各种 API 之间切换，操作更加便捷。PyTorch 的环境配置也很方便，各种开发版本都能向下兼容，不存在老版本的代码在新版本上无法使用的困扰，而且 PyTorch 跟 NumPy 的风格比较像，能轻易和 Python 生态集成起来，我们只需掌握 NumPy 和基本的深度学习概念即可上手，在网络搭建方面也是快捷又灵活。


另外，PyTorch 在 debug 代码的过程也十分方便，可以随时输出中间向量结果。用 PyTorch 就像在 Python 中使用 print 一样简单，只要把一个 pdb 断点扔进 PyTorch 模型里，直接就能用。因为它的优雅灵活和高效可用，吸引了越来越多的人学习。如果还有人只把 PyTorch 当成一个新秀，觉得 PyTroch 不过是个“挑战者”，试图在 TensorFlow 主导的世界里划出一片自己的地盘。那么数据可以证明，这种想法已经时过境迁。事实上，PyTorch 无论在学术界还是在工业界，都已经霸占了半壁江山。从学术界来看，2019 年之前，TensorFlow 还是各大顶会论文选择的主流框架，而 2019 年之后，顶会几乎成了 PyTorch 的天下，此消彼长，PyTorch 只用了一年的时间。

![img](https://static001.geekbang.org/resource/image/96/57/96afc1a6981ea2f8a2d847584ccb7c57.jpg?wh=900x559)


要知道，机器学习这个领域始终是依靠研究驱动的，工业界自然也不能忽视科学研究的成果。就拿我所在的团队来说，现在也已经逐步向 PyTorch 框架迁移，新开展的项目都会首选用 PyTorch 框架进行实现。不得不说，PyTorch 的应用范围已经逐渐扩大，同时也促进了其生态建设的发展。由于现在越来越多的开发者都在使用 PyTroch，一旦我们的程序遇到了 error 或 bug，很容易就可以在开发论坛上寻找到解决方案。总之，一旦你掌握了 PyTorch，就相当于走上了深度学习、机器学习的快车道。以后学习其他深度学习框架也可以快速入门，融会贯通。

如果你即将或者已经进入了深度学习和机器学习相关领域，PyTorch 能够帮你快速实现模型与算法的验证，快速完成深度学习模型部署，提供高并发服务，还可以轻松实现图像生成、文本分析、情感分析等有趣的实验。另外有很多算法相关的岗位，也同样会要求你熟练使用 PyTorch 等工具。可以探索的方向还有很多，这里就不一一列举了。那么问题来了，既然 PyTorch 有这么多优点，我们要怎样快速上手呢？只看原理好比空中楼阁，而直接实战对初学者来说又相对困难。因此我推荐的方法是，先理一个整体框架，有了整体认知之后，再通过实战练习巩固认知。具体来说，我们要先把框架的基本语法大致了解一下，然后尽快融入到一个实际项目当中，看一看在实际任务中，我们是怎么基于框架去解决一个问题的。这个专栏，也正是沿着这样一个思路设计的。我在专栏里给你提供了丰富的代码和实战案例，可以帮助你快速上手 PyTorch。

![img](https://static001.geekbang.org/resource/image/08/7a/08b96da4677066769fe3e6246f70237a.jpg?wh=1920x1418)

通过这个专栏，你将会熟练使用 PyTorch 工具，解决自己的问题，这是这个专栏要实现的最基础的目标。除了掌握工具用法之上，我希望交付的终点是让你获得分析问题的能力和解决问题的方法，让你懂得如何优化你自己的算法与模型。在学习经验方面，我希望这个专栏为你打开一扇窗，让你知道走深度学习这条路，需要有怎样的知识储备。为了让你由入门到精通，我把专栏分成了三个递进的部分。

基础篇简要介绍 PyTorch 的发展趋势与框架安装方法，以及 PyTorch 的前菜——**NumPy 的常用操作**。我们约定使用 PyTorch 1.9.0 版本，以及默认你已经掌握了 Python 编程与简单的机器学习基础，不过你也不用太过担心，遇到新知识的我基本都会从 0 开始讲起的。


模型训练篇想要快速掌握一个框架，就要从核心模块入手。在这个部分，会结合深度学习模型训练的一系列流程，为你详解自动求导机制、搭建网络、更新模型参数、保存与加载模型、训练过程可视化、分布式训练等等模块，带你具体看看 PyTorch 能给我们提供怎样的帮助。通过这个部分的学习，你就能基于 PyTorch 搭建网络模型了。

实战篇我们整个专栏都是围绕 PyTorch 框架在具体项目实践中的应用来讲的，所以最后我还会结合当下流行的图像与自然语言处理任务，串联前面两个模块的内容，为你深入讲解 PyTorch 如何解决实际问题。


总之，除了交付给你一个系统的 PyTorch 技术学习框架，我还希望给你传递我在深度学习这条路上的经验思考。最后，给你一点建议，对于学习 PyTorch 来说，边学边查、边练边查是个很好的方法。因为在我们实际做项目的时候，肯定会遇到一些之前没有使用过的函数，自己去查的话可以很好地加强记忆。当然，我也会尽心做好一个引路人，带你一步步实现课程目标，也期待你能以更加积极的状态投入到本次的学习之旅。现在就让我们一起探索 PyTorch，打开深度学习的大门吧！
## 基础
## 01 | PyTorch：网红中的顶流明星

在基础篇中，我们带你了解 PyTorch 的发展趋势与框架安装方法，然后重点为你讲解 NumPy 和 Tensor 的常用知识点。掌握这些基础知识与技巧，能够让你使用 PyTorch 框架的时候更高效，也是从头开始学习机器学习与深度学习迈出的第一步。磨刀不误砍柴工，所以通过这个模块，我们的目标是做好学习的准备工作。今天这节课，我们先从 PyTorch 的安装和常用编程工具说起，先让你对 PyTorch 用到的语言、工具、技术做到心里有数，以便更好地开启后面的学习之旅。


### PyTorch 登场
为什么选择 PyTorch 框架，我在开篇词就已经说过了。从 19 年起，无论是学术界还是工程界 PyTorch 已经霸占了半壁江山，可以说 PyTorch 已经是现阶段的主流框架了。这里的 Py 我们不陌生，它就是 Python，那 Torch 是什么？从字面翻译过来是一个“火炬”。

![img](https://static001.geekbang.org/resource/image/8b/8d/8b83b03c5e25886e1c6fe5aed8572e8d.png?wh=480x141)

什么是火炬呢？其实这跟 TensorFlow 中的 Tensor 是一个意思，我们可以把它看成是能在 GPU 中计算的矩阵。那 PyTorch 框架具体是怎么用的呢？说白了就是一个计算的工具。借助它，我们就能用计算机完成复杂的计算流程。但是我们都知道，机器跟人类的“语言”并不相通，想要让机器替我们完成对数据的复杂计算，就得先把数据翻译成机器能够理解的内容。无论是图像数据、文本数据还是数值数据，都要转换成矩阵才能进行后续的变化和运算。搞定了读入数据这一步，我们就要靠 PyTorch 搞定后面各种复杂的计算功能。这些所有的计算功能，包括了从前向传播到反向传播，甚至还会涉及其它非常复杂的计算，而这些计算统统要交给 PyTorch 框架实现。PyTorch 会把我们需要计算的矩阵传入到 GPU（或 CPU）当中，在 GPU（或 CPU）中实现各种我们所需的计算功能。**因为 GPU 做矩阵运算比较快，所以在神经网络中的计算一般都首选使用 GPU，但对于学习来说，我们用 CPU 就可以了。**而我们要做的就是，设计好整个任务的流程、整个网络架构，这样 PyTorch 才能顺畅地完成后面的计算流程，从而帮我们正确地计算。

### 安装 PyTorch 及其使用环境
在 PyTorch 安装之前，还有安装 Python3 以及 pip 这些最基础的操作，这些你在网上随便搜一下就能找到，相信你可以独立完成。这里我直接从安装 PyTorch 开始说，PyTorch 安装起来非常非常简单，方法也有很多，这里我们先看看最简单的方法：使用 pip 安装。

#### 使用 pip 安装 PyTorch
CPU 版本安装：

```
# Linux
pip install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
# Mac & Windows
pip install torch torchvision torchaudio
```
GPU 版本安装：（默认 CUDA 11.1 版本）

```
# Linux & Windows
pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
```
我们只需要将上面的命令复制到计算机的命令行当中，即可实现快速安装。这里有两个版本，一个 GPU 版本，一个 CPU 版本。建议你最好选择安装 GPU 版本，也就是说我们的硬件设备中最好有英伟达独立显卡。用 GPU 训练深度学习模型是非常快速的，所以在实际项目中都是使用 GPU 来训练模型。但是如果说大家手里没有供开发使用的英伟达 GPU 显卡的话，那么安装 CPU 版本也是可以的，在学习过程中，CPU 也足够让我们的小实验运行起来。另外，安装 GPU 版本前，需要安装对应版本的 CUDA 工具包。我们可以到[英伟达官网](https://developer.nvidia.com/cuda-downloads)，选择相应操作系统的 CUDA 工具包，进行下载与安装。硬件设备中无英伟达显卡的，可以略过这部分。目前 PyTorch 的稳定版本是 1.9.0，后续如果 PyTorch 的版本升级更新了，我们再将命令中的版本号稍作修改就可以了。

#### 其它方法安装 PyTorch
[这里](https://pytorch.org/get-started/locally/)是 PyTorch 的官网，在页面如下图所示的位置，我们可以看到有一些配置选项和安装命令。

![img](https://static001.geekbang.org/resource/image/31/7d/31c693e2433525636f302a6e066c137d.png?wh=1624x624)

我们可以根据页面上的指引，依次选择 PyTorch 的版本、你的操作系统、安装方式、编程语言以及计算平台，然后根据最下方的执行命令进行安装即可。值得注意的是，Mac 的操作系统只能安装 CPU 版本。我尝试下来最简单的方式，还是使用 pip 来安装。


#### 验证是否安装成功
你在终端中输入“python”，就可以进入到 Python 交互模式。首先输入如下代码，如果没有报错，就意味着 PyTorch 已经顺利安装了。

```
import torch
```
接下来，输入下面的代码，如果输出结果是“True”，意味着你可以使用 GPU。 这行代码的意思是检测 GPU 是否可用。

```
torch.cuda.is_available()
```
这里你也许会有疑问，为什么我安装的明明是 GPU 版本，但是代码却返回了“False”，显示 GPU 不可用呢？对于这个问题，我们依次按照下面的步骤进行检查。

1. 检查计算机上是否有支持 CUDA 的 GPU。首先查看电脑的显卡型号以及是否有独立显卡，如果没有以“NVIDIAN”名称开头的独立显卡，则不能支持 CUDA，因此 GPU 不可用。然后，你可以在[这个页面](https://developer.nvidia.com/zh-cn/cuda-gpus#compute)查询 GPU 是否支持 CUDA。如果你的 GPU 型号在页面的列表中，则表示你的计算机搭载了能够利用 CUDA 加速应用的现代 GPU，否则 GPU 也不可用。若 GPU 支持 CUDA，你还需要确保已经完成了上面介绍过的 CUDA 工具包的安装。


2. 检查显卡驱动版本。在终端中输入“nvidia-smi”命令，会显示出显卡驱动的版本和 CUDA 的版本，如下图所示。

![img](https://static001.geekbang.org/resource/image/58/4b/587018eab4418fb70fc2ee76a72fcd4b.jpg?wh=587x401)

如果显卡驱动的版本过低，与 CUDA 版本不匹配，那么 GPU 也不可用，需要根据显卡的型号更新显卡驱动。我用表格的方式，帮你梳理了 CUDA 版本与 GPU 驱动版本的对应关系，你可以根据自己计算机驱动的情况对照查看。例如，CUDA 11.1 支持的 Linux 驱动程序是 450.80.02 以上。

![img](https://static001.geekbang.org/resource/image/34/ff/34007081a277979659f81019a5d559ff.jpg?wh=3168x1648)

我们可以在[这里](https://www.nvidia.com/Download/index.aspx?lang=cn)下载并安装显卡驱动程序。3. 检查 PyTorch 版本和 CUDA 版本是否匹配？PyTorch 版本同样与 CUDA 版本有对应关系，我们可以在这个页面查看它们之间的对应关系。如果两者版本不匹配，可以重新安装对应版本的 PyTorch，或者升级 CUDA 工具包。

#### 使用 Docker
通过 Docker 使用 PyTorch 也非常简单，连安装都不需要，但是前提是你需要熟悉有关 Docker 的知识。如果你会熟练地使用 Docker，我推荐后面这个网页链接供你参考，[这里](https://hub.docker.com/r/pytorch/pytorch/tags)有很多的 PyTorch 的 Docker 镜像，你可以找到自己需要的镜像，然后拉取一个镜像到你的服务器或者本地，直接启动就可以了，无需额外的环境配置。


### 常用编程工具
在使用 PyTorch 进行编程之前，我们先来看看几个常用的编程工具，但是并不要求你必须使用它们，你可以根据自己的喜好自由选择。

#### Sublime Text
Sublime Text 是一个非常轻量且强大的文本编辑工具，内置了很多快捷的功能，对于我们开发来说非常便捷。

![img](https://static001.geekbang.org/resource/image/5e/9f/5e9506c5188f8b0ac687f7b14f271e9f.jpg?wh=1920x1260)

例如，它可以自动为项目中的类、方法和函数生成索引，让我们可以跟踪代码。具体就是通过它的 goto anything 功能，根据一些关键字，查找到项目中的对应的代码行。另外，它能支持的插件功能也很丰富。


#### PyCharm
PyCharm  作为一款针对  Python  的编辑器，配置简单、功能强大，使用起来省时省心，对初学者十分友好。它拥有一般 IDE 所具备的功能，比如：语法高亮、项目管理、代码跳转、代码补全、调试、单元测试、版本控制等等。


#### Vim
Vim 是 Linux 系统中的文本编辑工具，非常方便快捷，并且很强大。我们在项目中经常用到它。在我们的项目中，经常是需要登录到服务器上进行开发的，服务器一般都是 Linux 系统，不会有 Sublime Text 与 PyCharm，所以，我们用 Vim 打开代码，直接去进行编辑就可以了。对于没有接触过 Linux，或者一直习惯使用 IDE 来编程开发的同学，初步接触的时候，可能觉得 Vim 不是很方便，但实际上，Vim 包含了丰富的快捷键，对于 Shell 与 Python 的开发来说非常高效。但是 Vim 的缺点正如刚才所说，你需要去学习它的使用方法，有一点点门槛，但是只要你学会了，我保证你将对它爱不释手（这里也推荐有需要的同学去看看隔壁的《Vim 实用技巧必知必会》专栏）。

#### Jupyter Notebook&Lab
Jupyter Notebook 是一个开源的 Web 应用，这也是我最想推荐给你的一个工具。它能够让你创建和分享包含可执行代码、可视化结构和文字说明的文档。

在后面的课程，如果涉及图片生成或结果展示，我们也会使用到 Jupyter Notebook，这里推荐你先安装好。简而言之，Jupyter Notebook 是以网页的形式打开，可以在网页页面中直接编写代码和运行代码，代码的运行结果也会直接在代码块下显示。比如在编程过程中需要编写说明文档，可以在同一个页面中直接编写，便于及时说明、解释。而 Jupyter Lab 可以看做是 Jupyter Notebook 的终极进化版，它不但包含了 Jupyter Notebook 所有功能，并且集成了操作终端、打开交互模式、查看 csv 文件及图片等功能。

Jupyter Notebook 在我们的深度学习领域非常活跃。在实验测试阶段，相比用 py 文件来直接编程，还是 Jupyter Notebook 方便一些。在项目结束之后如果要书写项目报告，我觉得用 Jupyter 也比较合适。

#### 使用 pip 安装 Jupyter
通过 pip 安装 Jupyter Notebook 的命令如下。

```
pip install jupyter
```
通过 pip 安装 Jupyter Lab 的命令如下。

```
pip install jupyterlab
```
#### 启动 Jupyter
完成安装，就可以启动了。我们直接在终端中，执行下面的命令，就可以启动  Jupyter Notebook。
```
jupyter notebook
```
启动 Jupyter Lab 需要在终端执行如下命令。

```
jupyter lab
```
不管在 macOS 系统里，还是在 Windows 系统，通过以上任意一种方式启动成功之后，浏览器都会自动打开 Jupyter Notebook 或者 Jupyter Lab 的开发环境（你可以回顾下“Jupyter Notebook & Lab”那个例子里的界面）。

#### 运行 Jupyter Notebook
进入到 Jupyter Notebook 的界面，我们尝试新建一个 Python 的 Notebook。具体操作方法如下图所示。点击“New”下拉菜单，然后点击“Python 3”选项，来创建一个新的 Python Notebook。

上面我们已经讲过了 PyTorch 的安装方法，我们可以执行下面这段代码，来看看 PyTorch 是否已经安装成功。

```
import torch
torch.__version__
```
点击运行按钮，我们可以看到代码执行的结果，输出了当前安装的 PyTorch 的版本，即 PyTorch 1.9.0 的 GPU 版本。这说明 PyTorch 框架已经安装成功。


### 小结
恭喜你完成了这节课的学习。今天，我们一起了解了 PyTorch 框架的用途，简单来说就是能利用 GPU 帮我们搞定深度学习中一系列复杂运算的框架。想要用好这个工具，我们就得设计好整个任务的流程、整个网络架构，这样 PyTorch 才能实现各种各样复杂的计算功能。之后我们学习了 PyTorch 框架的安装方法，我还给你推荐了一些深度学习编程的常用工具。其中我最推荐的工具就是 Jupyter Notebook，这个工具在深度学习领域里常常会用到，后面课程里涉及图片生成或者结果展示的环节，我们也会用到它。

![img](https://static001.geekbang.org/resource/image/ba/c9/babe27a41b8fb46699475bd7b0c521c9.jpg?wh=2944x1837)


## 02 | NumPy（上）：核心数据结构详解
为什么我们要先拿下 NumPy 呢？我相信，无论你正在从事或打算入门机器学习，不接触 NumPy 几乎不可能。现在的主流深度学习框架 PyTorch 与 TensorFlow 中最基本的计算单元 **Tensor**，**都与 NumPy 数组有着类似的计算逻辑**，可以说掌握了 NumPy 对学习这两种框架都有很大帮助。另外，NumPy 还被广泛用在 Pandas，SciPy 等其他数据科学与科学计算的 Python 模块中。而我们日常用得越来越多的人脸识别技术（属于计算机视觉领域），其原理本质上就是先把图片转换成 NumPy 的数组，然后再进行一系列处理。为了让你真正吃透 NumPy，我会用两节课的内容讲解 NumPy。这节课，**我们先介绍 NumPy 的数组、数组的关键属性以及非常重要的轴的概念。**

### 什么是 NumPy
NumPy 是用于 Python 中科学计算的一个基础包。它提供了一个多维度的数组对象（稍后展开），以及针对数组对象的各种快速操作，例如排序、变换，选择等。NumPy 的安装方式非常简单，可以使用 Conda (https://zhuanlan.zhihu.com/p/32925500)安装，命令如下：

```
conda install numpy
```
或使用 pip 进行安装，命令如下：

```
pip install numpy
```
### NumPy 数组
刚才所说的数组对象是 NumPy 中最核心的组成部分，这个数组叫做 ndarray，是“N-dimensional array”的缩写。其中的 N 是一个数字，指代维度，例如你常常能听到的 1-D 数组、2-D 数组或者更高维度的数组。在 NumPy 中，数组是由 numpy.ndarray 类来实现的，它是 NumPy 的核心数据结构。我们今天的内容就是围绕它进行展开的。学习一个新知识，我们常用的方法就是跟熟悉的东西做对比。NumPy 数组从逻辑上来看，与其他编程语言中的数组是一样的，索引也是从 0 开始。而 Python 中的列表，其实也可以达到与 NumPy 数组相同的功能，但它们又有差异，做个对比你就能体会到 NumPy 数组的特点了。

1. Python 中的列表可以动态地改变，而 NumPy 数组是不可以的，它在创建时就有**固定大小**了。改变 Numpy 数组长度的话，会新创建一个新的数组并且删除原数组。
2. NumPy 数组中的**数据类型必须是一样**的，而列表中的元素可以是多样的。
3. NumPy 针对 NumPy 数组一系列的运算进行了优化，使得其速度特别快，并且相对于 Python 中的列表，同等操作只需使用更少的内存。


#### 创建数组
好，那就让我们来看看 NumPy 数组是怎么创建的吧？最简单的方法就是**把一个列表传入到 np.array() 或 np.asarray()** 中，这个列表可以是任意维度的。**np.array() 属于深拷贝，np.asarray() 则是浅拷贝**，它们的区别我们下节课再细讲，这里你有个印象就行。我们可以先试着创建一个一维的数组，代码如下。

```
>>>import numpy as np
>>>#引入一次即可

>>>arr_1_d = np.asarray([1])
>>>print(arr_1_d)
[1]
```
再创建一个二维数组：

```
>>>arr_2_d = np.asarray([[1, 2], [3, 4]])
>>>print(arr_2_d)
[[1 2]
 [3 4]]
```
#### 数组的属性
作为一个数组，NumPy 有一些固有的属性，我们今天来介绍非常常用且关键的数组**维度、形状、size 与数据类型**。

ndim
ndim 表示数组维度（或轴）的个数。刚才创建的数组 arr_1_d 的轴的个数就是 1，arr_2_d 的轴的个数就是 2。

```
>>>arr_1_d.ndim
1
>>>arr_2_d.ndim
2
```
shape
shape 表示数组的维度或形状， 是一个整数的元组，元组的长度等于 ndim。arr_1_d 的形状就是（1，）（一个向量）， arr_2_d 的形状就是 (2, 2)（一个矩阵）。

```
>>>arr_1_d.shape
(1,)
>>>arr_2_d.shape
(2, 2)
```
shape 这个属性在实际中用途还是非常广的。比如说，我们现在有这样的数据 (B, W, H, C)，熟悉深度学习的同学肯定会知道，这代表一个 batch size 为 B 的（W，H，C）数据。现在我们需要根据（W，H，C）对数据进行变形或者其他处理，这时我们可以直接使用 **input_data.shape[1:3]**获取到数据的形状，而不需要直接在程序中硬编程、直接写好输入数据的宽高以及通道数。在实际的工作当中，我们经常需要对数组的形状进行变换，就可以使用 **arr.reshape() 函数**，**在不改变数组元素内容的情况下变换数组的形状**。但是你**需要注意的是，变换前与变换后数组的元素个数需要是一样的**，请看下面的代码。

```
>>>arr_2_d.shape
(2, 2)
>>>arr_2_d
[[1 2]
 [3 4]]
# 将arr_2_d reshape为(4，1)的数组
>>>arr_2_d.reshape((4，1))
array([[1],
       [2],
       [3],
       [4]])
```
我们还可以使用 **np.reshape(a, newshape, order)** 对数组 a 进行 reshape，新的形状在 newshape 中指定。这里需要注意的是，reshape 函数有个 **order** 参数，它是指以什么样的顺序读写元素，其中有这样几个参数。

‘C’：默认参数，使用类似 C-like 语言（行优先）中的索引方式进行读写。‘F’：使用类似 Fortran-like 语言（列优先）中的索引方式进行读写。‘A’：原数组如果是按照‘C’的方式存储数组，则用‘C’的索引对数组进行 reshape，否则使用’F’的索引方式。

reshape 的过程你可以这样理解，首先需要根据指定的方式 (‘C’或’F’) 将原数组展开，然后再根据指定的方式写入到新的数组中。这是什么意思呢？先看一个简单的 2 维数组的例子。

```
>>>a = np.arange(6).reshape(2,3)
array([[0, 1, 2],
       [3, 4, 5]])
```
我们要将数组 a，按照’C’的方式 reshape 成 (3,2)，可以这样操作。首先将原数组展开，对于‘C’的方式来说是行优先，最后一个维度最优先改变，所以展开结果如下，序号那一列代表展开顺序。

![img](https://static001.geekbang.org/resource/image/46/e1/46dc5efc0fc1ff8yya419d459349cde1.jpg?wh=1185x621)

所以，reshape 后的数组，是按照 0，1，2，3，4，5 这个序列进行写入数据的。reshape 后的数组如下表所示，序号代表写入顺序。

![img](https://static001.geekbang.org/resource/image/a2/1e/a2e4259d27eae29196616dece4b46d1e.jpg?wh=1240x615)

接下来，再看看将数组 a，按照’F’的方式 reshape 成 (3,2) 要如何处理。对于行优先的方式，我们应该是比较熟悉的，而‘F’方式是列优先的方式，这一点对于没有使用过列优先的同学来说，可能比较难理解一点。首先是按列优先展开原数组，列优先意味着最先变化的是数组的第一个维度。下表是展开后的结果，序号是展开顺序，这里请注意下坐标的变换方式（第一个维度最先变化）。

![img](https://static001.geekbang.org/resource/image/fe/72/fe21a81ab58523edc0d1a84f15yyf372.jpg?wh=1185x621)

所以，reshape 后的数组，是按照 0，3，1，4，2，5 这个序列进行写入数据的。reshape 后的数组如下表所示，序号代表写入顺序，为了显示直观，我将相同行以同样颜色显示了。

![img](https://static001.geekbang.org/resource/image/26/6b/26dbe3e14fded552bd8a0515858a476b.jpg?wh=1227x606)

这里我给你留一个小练习，你可以试试对多维数组的 reshape 吗？不过，大部分时候还是使用’C’的方式比较多，也就是行优先的形式。至少目前为止我还没有使用过’F’与’A’的方式。


size
size，也就是数组元素的总数，它就等于 shape 属性中元素的乘积。请看下面的代码，arr_2_d 的 size 是 4。

```
>>>arr_2_d.size
4
```
dtype
最后要说的是 dtype，它是一个描述数组中元素类型的对象。使用 dtype 属性可以查看数组所属的数据类型。NumPy 中大部分常见的数据类型都是支持的，例如 int8、int16、int32、float32、float64 等。dtype 是一个常见的属性，在创建数组，数据类型转换时都可以看到它。首先我们看看 arr_2_d 的数据类型：

```
>>>arr_2_d.dtype
dtype('int64')
```
你可以回头看一下刚才创建 arr_2_d 的时候，我们并没有指定数据类型，如果没有指定数据类型，NumPy 会自动进行判断，然后给一个默认的数据类型。我们再看下面的代码，我们在创建 arr_2_d 时，对数据类型进行了指定。

```
>>>arr_2_d = np.asarray([[1, 2], [3, 4]], dtype='float')
>>>arr_2_d.dtype
dtype('float64')
```
数组的数据类型当然也可以改变，我们可以使用 astype() 改变数组的数据类型，不过改变数据类型会创建一个新的数组，而不是改变原数组的数据类型。请看后面的代码。

```
>>>arr_2_d.dtype
dtype('float64')
>>>arr_2_d.astype('int32')
array([[1, 2],
       [3, 4]], dtype=int32)
>>>arr_2_d.dtype
dtype('float64')
# 原数组的数据类型并没有改变
>>>arr_2_d_int = arr_2_d.astype('int32')
>>>arr_2_d_int.dtype
dtype('int32')
```
但是，我想提醒你，不能通过直接修改数据类型来修改数组的数据类型，这样代码虽然不会报错，但是数据会发生改变，请看下面的代码：

```
>>>arr_2_d.dtype
dtype('float64')
>>>arr_2_d.size
4
>>>arr_2_d.dtype='int32'
>>>arr_2_d
array([[         0, 1072693248,          0, 1073741824],
       [         0, 1074266112,          0, 1074790400]], dtype=int32)
```
1 个 float64 相当于 2 个 int32，所以原有的 4 个 float32，会变为 8 个 int32，然后直接输出这个 8 个 int32。


#### 其他创建数组的方式
除了使用 np.asarray 或 np.array 来创建一个数组之外，NumPy 还提供了一些按照既定方式来创建数组的方法，我们只需按照要求，提供一些必要的参数即可。

np.ones() 与 np.zeros()
np.ones() 用来创建一个全 1 的数组，必须参数是指定数组的形状，可选参数是数组的数据类型，你可以结合下面的代码进行理解。

```
>>>np.ones()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: ones() takes at least 1 argument (0 given)
# 报错原因是没有给定形状的参数
>>>np.ones(shape=(2,3))
array([[1., 1., 1.],
       [1., 1., 1.]])
>>>np.ones(shape=(2,3), dtype='int32')
array([[1, 1, 1],
       [1, 1, 1]], dtype=int32)
```
创建全 0 的数组是 np.zeros()，用法与 np.ones() 类似，我们就不举例了。那这两个函数一般什么时候用呢？例如，如果需要初始化一些权重的时候就可以用上，比如说生成一个 2x3 维的数组，每个数值都是 0.5，可以这样做。

```
>>>np.ones((2, 3)) * 0.5
array([[0.5, 0.5, 0.5],
       [0.5, 0.5, 0.5]]
```
np.arange()
我们还可以使用 np.arange([start, ]stop, [step, ]dtype=None) 创建一个在[start, stop) 区间的数组，元素之间的跨度是 step。start 是可选参数，默认为 0。stop 是必须参数，区间的终点，请注意，刚才说的区间是一个左闭右开区间，所以数组并不包含 stop。step 是可选参数，默认是 1。

```
# 创建从0到4的数组
>>>np.arange(5)
array([0, 1, 2, 3, 4])
# 从2开始到4的数组
>>>np.arange(2, 5)
array([2, 3, 4])
# 从2开始，到8的数组，跨度是3
>>>np.arange(2, 9, 3)
array([2, 5, 8])
```
np.linspace()
最后，我们也可以用 np.linspace（start, stop, num=50, endpoint=True, retstep=False, dtype=None）创建一个数组，具体就是创建一个从开始数值到结束数值的等差数列。

start：必须参数，序列的起始值。stop：必须参数，序列的终点。num：序列中元素的个数，默认是 50。endpoint：默认为 True，如果为 True，则数组最后一个元素是 stop。retstep：默认为 False，如果为 True，则返回数组与公差。

```
# 从2到10，有3个元素的等差数列
>>>np.linspace(start=2, stop=10, num=3)
```
np.arange 与 np.linspace 也是比较常见的函数，比如你要作图的时候，可以用它们生成 x 轴的坐标。例如，我要生成一个 y=x2 的图片，x 轴可以用 np.linespace() 来生成。

```
import numpy as np
import matplotlib.pyplot as plt

X = np.arange(-50, 51, 2)
Y = X ** 2

plt.plot(X, Y, color='blue')
plt.legend()
plt.show()
```
![img](https://static001.geekbang.org/resource/image/0c/b4/0c752f2b6777a95d8a373649e4a3a2b4.jpg?wh=1800x1146)


### 数组的轴
这是一个非常重要的概念，也是 NumPy 数组中最不好理解的一个概念。它经常出现在 np.sum()、np.max() 这样关键的聚合函数中。我们用这样一个问题引出，同一个函数如何根据轴的不同来获得不同的计算结果呢？比如现在有一个 (4,3) 的矩阵，存放着 4 名同学关于 3 款游戏的评分数据。

```
>>>interest_score = np.random.randint(10, size=(4, 3))
>>>interest_score
array([[4, 7, 5],
       [4, 2, 5],
       [7, 2, 4],
       [1, 2, 4]])
```
第一个需求是，计算每一款游戏的评分总和。这个问题如何解决呢，我们一起分析一下。数组的轴即数组的维度，它是从 0 开始的。对于我们这个二维数组来说，有两个轴，分别是代表行的 0 轴与代表列的 1 轴。如下图所示。

![img](https://static001.geekbang.org/resource/image/e1/de/e14a4f5d6ba946665b7ccf01c58a2dde.jpg?wh=1233x790)

我们的问题是要计算每一款游戏的评分总和，也就是沿着 0 轴的方向进行求和。所以，我们只需要在求和函数中指定沿着 0 轴的方向求和即可。

```
>>> np.sum(interest_score, axis=0)
array([16, 13, 18])
```
计算方向如绿色箭头所示：

![img](https://static001.geekbang.org/resource/image/3a/80/3a6bd04c4708d3635e9c92092612e380.jpg?wh=1207x812)


第二个问题是要计算每名同学的评分总和，也就是要沿着 1 轴方向对二维数组进行操作。所以，我们只需要将 axis 参数设定为 1 即可。

```
>>> np.sum(interest_score, axis=1)
array([16, 11, 13,  7])
```
计算方向如绿色箭头所示。

![img](https://static001.geekbang.org/resource/image/d6/b4/d60ed120c370e376253bee7b362590b4.jpg?wh=1196x790)

二维数组还是比较好理解的，那多维数据该怎么办呢？你有没有发现，其实当 axis=i 时，就是按照第 i 个轴的方向进行计算的，或者可以理解为第 i 个轴的数据将会被折叠或聚合到一起。形状为 (a, b, c) 的数组，沿着 0 轴聚合后，形状变为 (b, c)；沿着 1 轴聚合后，形状变为 (a, c)；沿着 2 轴聚合后，形状变为 (a, b)；更高维数组以此类推。接下来，我们再看一个多维数组的例子。对数组 a，求不同维度上的最大值。


```
>>> a = np.arange(18).reshape(3,2,3)
>>> a
array([[[ 0,  1,  2],
        [ 3,  4,  5]],

       [[ 6,  7,  8],
        [ 9, 10, 11]],

       [[12, 13, 14],
        [15, 16, 17]]])
```
我们可以将同一个轴上的数据看做同一个单位，那聚合的时候，我们只需要在同级别的单位上进行聚合就可以了。如下图所示，绿框代表沿着 0 轴方向的单位，蓝框代表着沿着 1 轴方向的单位，红框代表着 2 轴方向的单位。

![img](https://static001.geekbang.org/resource/image/0a/b9/0af604dc4661e5512515781bbd7be3b9.jpg?wh=977x838)

当 axis=0 时，就意味着将三个绿框的数据聚合在一起，结果是一个（2，3）的数组，数组内容为：[ [(max(a000​,a100​,a200​),max(a001​,a101​,a201​),max(a002​,a102​,a202​))],[(max(a010​,a110​,a210​),max(a011​,a111​,a211​),max(a012​,a112​,a212​))] ]​代码如下：

```
>>> a.max(axis=0)
array([[12, 13, 14],
       [15, 16, 17]])
```
当 axis=1 时，就意味着每个绿框内的蓝框聚合在一起，结果是一个（3，3）的数组，数组内容为：[ [(max(a000​,a010​),max(a001​,a011​),max(a002​,a012​))],[(max(a100​,a110​),max(a101​,a111​),max(a102​,a112​))],[(max(a200​,a210​),max(a201​,a211​),max(a202​,a212​))], ]​代码如下：

```
>>> a.max(axis=1)
array([[ 3,  4,  5],
       [ 9, 10, 11],
       [15, 16, 17]])

```
当 axis=2 时，就意味着每个蓝框中的红框聚合在一起，结果是一个（3，2）的数组，数组内容如下所示：[ [(max(a000​,a001​,a002​),max(a010​,a011​,a012​))],[(max(a100​,a101​,a102​),max(a110​,a111​,a112​))],[(max(a200​,a201​,a202​),max(a210​,a211​,a212​))], ]​代码如下：

```
>>> a.max(axis=2)
array([[ 2,  5],
       [ 8, 11],
       [14, 17]])
```
axis 参数非常常见，不光光出现在刚才介绍的 sum 与 max，还有很多其他的聚合函数也会用到，例如 min、mean、argmin（求最小值下标）、argmax（求最大值下标）等。


### 小结
恭喜你完成了这节课的学习。其实你只要有一些其他语言的编程基础，学 Numpy 还是非常容易的。这里我想再次强调一下为什么 NumPy 这道前菜必不可少。其实 Numpy 的很多知识点是与 PyTorch 融会贯通的，例如 PyTorch 中的 Tensor。而且 Numpy 在机器学习中常常被用到，很多模块都要基于 NumPy 展开，尤其是在数据的预处理和膜后处理中。NumPy 是用于 Python 中科学计算的一个基础包。它提供了一个多维度的数组对象，以及针对数组对象的各种快速操作。为了让你有更直观的体验，我们学习了创建数组的四种方式。其中你重点要掌握的方法，就是如何使用 np.asarray 创建一个数组。这里涉及数组属性（ndim、shape、dtype、size）的灵活使用，特别是数组的形状变化与数据类型转换。最后，我为你介绍了数组轴的概念，我们需要在数组的聚合函数中灵活运用它。虽然这个概念十分常用，但却不好理解，建议你根据我课程里的例子仔细揣摩一下，从 2 维数组一步步推理到多维数组，根据轴的不同，数组聚合的方向是如何变化的。下一节课，我们要继续学习 NumPy 中常用且重要的功能。



## 03 | NumPy（下）：深度学习中的常用操作


通过上节课的学习，我们已经对 NumPy 数组有了一定的了解，正所谓实践出真知，今天我们就以一个图像分类的项目为例，看看 NumPy 的在实际项目中都有哪些重要功能。我们先从一个常见的工作场景出发，互联网教育推荐平台，每天都有千万量级的文字与图片的广告信息流入。为了给用户提供更加精准的推荐，你的老板交代你设计一个模型，让你把包含各个平台 Logo（比如包含极客时间 Logo）的图片自动找出来。

想要解决这个图片分类问题，我们可以分解成数据加载、训练与模型评估三部分（其实基本所有深度学习的项目都可以这样划分）。其中数据加载跟模型评估中，就经常会用到 NumPy 数组的相关操作。那么我们先来看看数据的加载。


### 数据加载阶段
这个阶段我们要做的就是把训练数据读进来，然后给模型训练使用。训练数据不外乎这三种：图片、文本以及类似二维表那样的结构化数据。不管使用 PyTorch 还是 TensorFlow，或者是传统机器学习的 scikit-learn，我们在读入数据这一块，都会先把数据转换成 NumPy 的数组，然后再进行后续的一系列操作。对应到我们这个项目中，需要做的就是把训练集中的图片读入进来。对于图片的处理，我们一般会使用 Pillow 与 OpenCV 这两个模块。虽然 Pillow 和 OpenCV 功能看上去都差不多，但还是有区别的。在 PyTorch 中，很多图片的操作都是基于 Pillow 的，所以当使用 PyTorch 编程出现问题，或者要思考、解决一些图片相关问题时，要从 Pillow 的角度出发。下面我们先以单张图片为例，将极客时间的那张 Logo 图片分别用 Pillow 与 OpenCV 读入，然后转换为 NumPy 的数组。

#### Pillow 方式
首先，我们需要使用 Pillow 中的下述代码读入上面的图片。

```
from PIL import Image
im = Image.open('jk.jpg')
im.size
输出: 318, 116
```
Pillow 是以二进制形式读入保存的，那怎么转为 NumPy 格式呢？这个并不难，我们只需要利用 NumPy 的 asarray 方法，就可以将 Pillow 的数据转换为 NumPy 的数组格式。
```
import numpy as np

im_pillow = np.asarray(im)

im_pillow.shape
输出：(116, 318, 3)
```
#### OpenCV 方式：
OpenCV 的话，不再需要我们手动转格式，它直接读入图片后，就是以 NumPy 数组的形式来保存数据的，如下面的代码所示。

```
import cv2
im_cv2 = cv2.imread('jk.jpg')
type(im_cv2)
输出：numpy.ndarray

im_cv2.shape
输出：(116, 318, 3)
```
结合代码输出可以发现，我们读入后的数组的最后一个维度是 3，这是因为图片的格式是 RGB 格式，表示有 R、G、B 三个通道。对于计算视觉任务来说，绝大多数处理的图片都是 RGB 格式，如果不是 RGB 格式的话，要记得事先转换成 RGB 格式。这里有个地方需要你关注，Pillow 读入后通道的顺序就是 R、G、B，而 OpenCV 读入后顺序是 B、G、R。模型训练时的通道顺序需与预测的通道顺序要保持一致。也就是说使用 Pillow 训练，使用 OpenCV 读入图片直接进行预测的话，不会报错，但结果会不正确，所以大家一定要注意。


接下来，我们就验证一下 Pillow 与 OpenCV 读入数据通道的顺序是否如此，借此引出有关 Numpy 数组索引与切片、合并等常见问题。怎么验证这条结论呢？只需要将 R、G、B 三个通道的数据单独提取出来，然后令另外两个通道的数据全为 0 即可。这里我给你说说为什么这样做。RGB 色彩模式是工业界的一种颜色标准，RGB 分别代表红、绿、蓝三个通道的颜色，将这三种颜色混合在一起，就形成了我们眼睛所能看到的所有颜色。RGB 三个通道各有 256 个亮度，分别用数字 0 到 255 表示，数字越高代表亮度越强，数字 0 则是代表最弱的亮度。在我们的例子中，如果一个通道的数据再加另外两个全 0 的通道（相当于关闭另外两个通道），最终图像以红色格调（可以先看一下后文中的最终输出结果）呈现出来的话，我们就可以认为该通道的数据是来源于 R 通道，G 与 B 通道的证明同样可以如此。好，首先我们提取出 RGB 三个通道的数据，这可以从数组的索引与切片说起。

#### 索引与切片
如果你了解 Python，那么索引和切片的概念你应该不陌生。就像图书目录里的索引，我们可以根据索引标注的页码快速找到需要的内容，而 Python里的索引也是同样的功能，它用来定位数组中的某一个值。而切片意思就相当于提取图书中从某一页到某一页的内容。NumPy 数组的索引方式与 Python 的列表的索引方式相同，也同样支持切片索引。这里需要你注意的是在 NumPy 数组中经常会出现用冒号来检索数据的形式，如下所示：


```
im_pillow[:, :, 0]
```
这是什么意思呢？我们一起来看看。“：”代表全部选中的意思。我们的图片读入后，会以下图的状态保存在数组中。

![img](https://static001.geekbang.org/resource/image/20/01/20ayy454079771245f44f983b2130e01.jpg?wh=1920x1391)

上述代码的含义就是取第三个维度索引为 0 的全部数据，换句话说就是，取图片第 0 个通道的所有数据。这样的话，通过下面的代码，我们就可以获得每个通道的数据了。

```
im_pillow_c1 = im_pillow[:, :, 0]
im_pillow_c2 = im_pillow[:, :, 1]
im_pillow_c3 = im_pillow[:, :, 2]
```
获得了每个通道的数据，接下来就需要生成一个全 0 数组，该数组要与 im_pillow 具有相同的宽高。全 0 数组你还记得怎么生成吗？可以自己先思考一下，生成的代码如下所示。

```
zeros = np.zeros((im_pillow.shape[0], im_pillow.shape[1], 1))
zeros.shape
输出：(116, 318, 1)
```
然后，我们只需要将全 0 的数组与 im_pillow_c1、im_pillow_c2、im_pillow_c3 进行拼接，就可以获得对应通道的图像数据了。


#### 数组的拼接
刚才我们拿到了单独通道的数据，接下来就需要把一个分离出来的数据跟一个全 0 数组拼接起来。如下图所示，红色的可以看作单通道数据，白色的为全 0 数据。

![img](https://static001.geekbang.org/resource/image/ee/c1/eedf20bf55a6c0521309d7c102719bc1.jpg?wh=1920x899)

NumPy 数组为我们提供了 np.concatenate((a1, a2, …), axis=0) 方法进行数组拼接。其中，a1，a2, …就是我们要合并的数组；axis 是我们要沿着哪一个维度进行合并，默认是沿着 0 轴方向。对于我们的问题，是要沿着 2 轴的方向进行合并，也是我们最终的目标是要获得下面的三幅图像。

![img](https://static001.geekbang.org/resource/image/68/7e/68bcd8107bf71ef876a339350a10c77e.jpg?wh=1603x653)


那么，我们先将 im_pillow_c1 与全 0 数组进行合并，生成上图中最左侧的数组，有了图像的数组才能获得最终图像。合并的代码跟输出结果如下：

```
im_pillow_c1_3ch = np.concatenate((im_pillow_c1, zeros, zeros),axis=2)
---------------------------------------------------------------------------
AxisError                                 Traceback (most recent call last)
<ipython-input-21-e3d53c33c94d> in <module>
----> 1 im_pillow_c1_3ch = np.concatenate((im_pillow_c1, zeros, zeros),axis=2)
<__array_function__ internals> in concatenate(*args, **kwargs)
AxisError: axis 2 is out of bounds for array of dimension 2
```
看到这里你可能很惊讶，竟然报错了？错误的原因是在 2 维数组中，axis 如果等于 2 的话会越界。我们看看 im_pillow_c1 与 zeros 的形状。


```
im_pillow_c1.shape
输出：(116, 318)
zeros.shape
输出：(116, 318, 1)
```
原来是我们要合并的两个数组维度不一样啊。那么如何统一维度呢？将 im_pillow_c1 变成 (116, 318, 1) 即可。


#### 方法一：使用 np.newaxis
我们可以使用 np.newaxis 让数组增加一个维度，使用方式如下。

```
im_pillow_c1 = im_pillow_c1[:, :, np.newaxis]
im_pillow_c1.shape
输出：(116, 318, 1)
```
运行上面的代码，就可以将 2 个维度的数组转换为 3 个维度的数组了。这个操作在你看深度学习相关代码的时候经常会看到，只不过 PyTorch 中的函数名 unsqueeze(), TensorFlow 的话是与 NumPy 有相同的名字，直接使用 tf.newaxis 就可以了。然后我们再次将 im_pillow_c1 与 zeros 进行合并，这时就不会报错了，代码如下所示：

```
im_pillow_c1_3ch = np.concatenate((im_pillow_c1, zeros, zeros),axis=2)
im_pillow_c1_3ch.shape
输出：(116, 318, 3)
```
#### 方法二：直接赋值
增加维度的第二个方法就是直接赋值，其实我们完全可以生成一个与 im_pillow 形状完全一样的全 0 数组，然后将每个通道的数值赋值为 im_pillow_c1、im_pillow_c2 与 im_pillow_c3 就可以了。我们用这种方式生成上图中的中间与右边图像的数组。

```
im_pillow_c2_3ch = np.zeros(im_pillow.shape)
im_pillow_c2_3ch[:,:,1] = im_pillow_c2

im_pillow_c3_3ch = np.zeros(im_pillow.shape)
im_pillow_c3_3ch[:,:,2] = im_pillow_c3
```
这样的话，我们就可以将三个通道的 RGB 图片打印出来了。关于绘图，你可以使用 matplotlib 进行绘图，它是 NumPy 的绘图库。如果你需要绘图，可以在[这个网站](https://matplotlib.org/stable/gallery/index.html)上找到各种各样的例子，然后根据它提供的代码进行修改，具体如何绘图我就不展开了。说回我们的通道顺序验证问题，完成前面的操作后，你可以用下面的代码将原图、R 通道、G 通道与 B 通道的 4 幅图打印出来，你看是不是 RGB 顺序的呢？

```
from matplotlib import pyplot as plt
plt.subplot(2, 2, 1)
plt.title('Origin Image')
plt.imshow(im_pillow)
plt.axis('off')
plt.subplot(2, 2, 2)
plt.title('Red Channel')
plt.imshow(im_pillow_c1_3ch.astype(np.uint8))
plt.axis('off')
plt.subplot(2, 2, 3)
plt.title('Green Channel')
plt.imshow(im_pillow_c2_3ch.astype(np.uint8))
plt.axis('off')
plt.subplot(2, 2, 4)
plt.title('Blue Channel')
plt.imshow(im_pillow_c3_3ch.astype(np.uint8))
plt.axis('off')
plt.savefig('./rgb_pillow.png', dpi=150)
```
![img](https://static001.geekbang.org/resource/image/12/10/1264b5c889bfyy5e0d00668cfa205110.png?wh=900x600)

#### 深拷贝（副本）与浅拷贝（视图）

刚才我们通过获取图片通道数据的练习，不过操作确实比较繁琐，介绍这些方法也主要是为了让你掌握切片索引和数组拼接的知识点。其实我们还有一种更加简单的方式获得三个通道的 BGR 数据，只需要将图片读入后，直接将其中的两个通道赋值为 0 即可。代码如下所示：

```
from PIL import Image
import numpy as np

im = Image.open('jk.jpg')
im_pillow = np.asarray(im)
im_pillow[:,:,1:]=0
输出：
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-146-789bda58f667> in <module>
      4 im = Image.open('jk.jpg')
      5 im_pillow = np.asarray(im)
----> 6 im_pillow[:,:,1:-1]=0
ValueError: assignment destination is read-only
```
运行刚才的代码，报错提示说数组是只读数组，没办法进行修改。那怎么办呢？我们可以使用 copy 来复制一个数组。说到 copy() 的话，就要说到浅拷贝与深拷贝的概念，上节课我们说到创建数组时就提过，np.array() 属于深拷贝，np.asarray() 则是浅拷贝。简单来说，浅拷贝或称视图，指的是与原数组共享数据的数组，请注意，只是数据，没有说共享形状。视图我们通常使用 view() 来创建。常见的切片操作也会返回对原数组的浅拷贝。请看下面的代码，数组 a 与 b 的数据是相同的，形状确实不同，但是修改 b 中的数据后，a 的数据同样会发生变化。

```
a = np.arange(6)
print(a.shape)
输出：(6,)
print(a)
输出：[0 1 2 3 4 5]

b = a.view()
print(b.shape)
输出：(6,)
b.shape = 2, 3
print(b)
输出：[[0 1 2]
 [3 4 5]]
b[0,0] = 111
print(a)
输出：[111   1   2   3   4   5]
print(b)
输出：[[111   1   2]
 [  3   4   5]]
```
而深拷贝又称副本，也就是完全复制原有数组，创建一个新的数组，修改新的数组不会影响原数组。深拷贝使用 copy() 方法。所以，我们将刚才报错的程序修改成下面的形式就可以了。

```
im_pillow = np.array(im)
im_pillow[:,:,1:]=0
```
可别小看深拷贝和浅拷贝的区别。这里讲一个我以前遇到的坑吧，我曾经要开发一个部署在手机端的人像分割模型。为了提高模型的分割效果，我考虑了新的实验方法——将前一帧的数据也作为当前帧的输入进行考虑，训练阶段没有发生问题，但是在调试阶段发现模型的效果非常差。后来经过研究，我才发现了问题的原因。原因是我为了可视化分割效果，我将前一帧的数据进行变换打印出来。同时，我错误的采用了浅拷贝的方式，将前一帧的数据传入当前帧，所以说传入到当前帧的数据是经过变化的，而不是原始的输出。这时再传入当前帧，自然无法得到正确结果。当时因为这个坑，差点产生要放弃这个实验的想法，后面改成深拷贝才解决了问题。好了，讲到这里，你是否可以用上述的方法对 OpenCV 读取图片读入通道顺序进行一下验证呢？

### 模型评估
在模型评估时，我们一般会将模型的输出转换为对应的标签。

假设现在我们的问题是将图片分为 2 个类别，包含极客时间的图片与不包含的图片。模型会输出形状为 (2, ) 的数组，我们把它叫做 probs，它存储了两个概率，我们假设索引为 0 的概率是包含极客时间图片的概率，另一个是其它图片的概率，它们两个概率的和为 1。如果极客时间对应的概率大，则可以推断该图片为包含极客时间的图片，否则为其他图片。简单的做法就是判断 probs[0]是否大于 0.5，如果大于 0.5，则可以认为图片是我们要寻找的。这种方法固然可以，但是如果我们需要判断图片的类别有很多很多种呢？例如，有 1000 个类别的 ImageNet。也许你会想到遍历这个数组，求出最大值对应的索引。那如果老板让你找出概率最大的前 5 个类别呢？有没有更简单点的方法？我们继续往下看。

#### Argmax Vs Argmin：
求最大 / 最小值对应的索引NumPy 的 argmax(a, axis=None) 方法可以为我们解决求最大值索引的问题。如果不指定 axis，则将数组默认为 1 维。对于我们的问题，使用下述代码即可获得拥有最大概率值的图片。

```
np.argmax(probs)
```
Argmin 的用法跟 Argmax 差不多，不过它的作用是获得具有最小值的索引。

#### Argsort：数组排序后返回原数组的索引
那现在我们再把问题升级一下，比如需要你将图片分成 10 个类别，要找到具有最大概率的前三个类别。模型输出的概率如下：

```
probs = np.array([0.075, 0.15, 0.075, 0.15, 0.0, 0.05, 0.05, 0.2, 0.25])
```
这时，我们就可以借助 argsort(a, axis=-1, kind=None) 函数来解决该问题。np.argsort 的作用是对原数组进行从小到大的排序，返回的是对应元素在原数组中的索引。


np.argsort 包括后面这几个关键参数：a 是要进行排序的原数组；axis 是要沿着哪一个轴进行排序，默认是 -1，也就是最后一个轴；kind 是采用什么算法进行排序，默认是快速排序，还有其他排序算法，具体你可以看看数据结构的排序算法。

我们还是结合例子来理解，你可以看看下面的代码，它描述了我们使用 argsort 对 probs 进行排序，然后返回对应坐标的全过程。


```
probs_idx_sort = np.argsort(-probs)  #注意，加了负号，是按降序排序
probs_idx_sort
输出：array([8, 7, 1, 3, 0, 2, 5, 6, 4])
#概率最大的前三个值的坐标
probs_idx_sort[:3]
输出：array([8, 7, 1])
```
### 小结
恭喜你，完成了这一节课的学习。这一节介绍了一些常用且重要的功能。几乎在所有深度学习相关的项目中，你都会常常用到这些函数，当你阅读别人的代码的时候也会经常看到。让我们一起来复习一下今天学到的这些函数，我画了一张表格，给你总结了它们各自的关键功能和使用要点。

![img](https://static001.geekbang.org/resource/image/29/yd/296b503a7c2fb89987695035c0184yyd.jpg?wh=1709x798)


我觉得 NumPy 最难懂的还是上节课的轴，如果你把轴的概念理解清楚之后，理解今天的内容会更加轻松。理解了原理之后，关键还是动手练习。


## 04 | Tensor：PyTorch中最基础的计算单元

在上节课中，我们一起学习了 NumPy 的主要使用方法和技巧，有了 NumPy 我们可以很好地处理各种类型的数据。而在深度学习中，数据的组织则更进一步，从数据的组织，到模型内部的参数，都是通过一种叫做张量的数据结构进行表示和处理。今天我们就来一块儿了解一下张量（Tensor），学习一下 Tensor 的常用操作。

### 什么是 Tensor
Tensor 是深度学习框架中极为基础的概念，也是 PyTroch、TensorFlow 中最重要的知识点之一，它是一种数据的存储和处理结构。回忆一下我们目前知道的几种数据表示：

标量，也称 Scalar，是一个只有大小，没有方向的量，比如 1.8、e、10 等。向量，也称 Vector，是一个有大小也有方向的量，比如 (1,2,3,4) 等。矩阵，也称 Matrix，是多个向量合并在一起得到的量，比如[(1,2,3),(4,5,6)]等。

为了帮助你更好理解标量、向量和矩阵，我特意准备了一张示意图，你可以结合图片理解。

![img](https://static001.geekbang.org/resource/image/a8/85/a85883cc14171ff5361346dd65776085.jpg?wh=1920x1090)

不难发现，几种数据表示其实都是有着联系的，标量可以组合成向量，向量可以组合成矩阵。那么，我们可否将它们看作是一种数据形式呢？答案是可以的，这种统一的数据形式，在 PyTorch 中我们称之为张量 (Tensor)。从标量、向量和矩阵的关系来看，你可能会觉得它们就是不同“维度”的 Tensor，这个说法对，也不全对。说它不全对是因为在 Tensor 的概念中，我们更愿意使用 Rank（秩）来表示这种“维度”，比如标量，就是 Rank 为 0 阶的 Tensor；向量就是 Rank 为 1 阶的 Tensor；矩阵就是 Rank 为 2 阶的 Tensor。也有 Rank 大于 2 的 Tensor。当然啦，你如果说维度其实也没什么错误，平时很多人也都这么叫。说完 Tensor 的含义，我们一起看一下 Tensor 的类型，以及如何创建 Tensor。

### Tensor 的类型、创建及转换
在不同的深度学习框架下，Tensor 呈现的特点大同小异，我们使用它的方法也差不多。这节课我们就以 PyTorch 中的使用方法为例进行学习。

#### Tensor 的类型
在 PyTorch 中，Tensor 支持的数据类型有很多种，这里列举较为常用的几种格式：

![img](https://static001.geekbang.org/resource/image/e6/08/e6af6a3b2172ee08db8c564146ae2108.jpg?wh=1680x933)

一般来说，torch.float32、torch.float64、torch.uint8 和 torch.int64 用得相对较多一些，但是也不是绝对，还是要根据实际情况进行选择。这里你有个印象就行，后面课程用到时我还会进一步讲解。


#### Tensor 的创建
PyTorch 对于 Tensor 的操作已经非常友好了，你可以通过多种不同的方式创建一个任意形状的 Tensor，而且每种方式都很简便，我们一起来看一下。

直接创建
首先来看直接创建的方法，这也是最简单创建的方法。我们需要用到下面的 torch.tensor 函数直接创建。


```
torch.tensor(data, dtype=None, device=None,requires_grad=False)
```
结合代码，我们看看其中的参数是什么含义。我们从左往右依次来看，首先是 data，也就是我们要传入模型的数据。PyTorch 支持通过 list、 tuple、numpy array、scalar 等多种类型进行数据传入，并转换为 tensor。接着是 dtype，它声明了你需要返回一个怎样类型的 Tensor，具体类型可以参考前面表格里列举的 Tensor 的 8 种类型。然后是 device，这个参数指定了数据要返回到的设备，目前暂时不需要关注，缺省即可。最后一个参数是 requires_grad，用于说明当前量是否需要在计算中保留对应的梯度信息。在 PyTorch 中，只有当一个 Tensor 设置 requires_grad 为 True 的情况下，才会对这个 Tensor 以及由这个 Tensor 计算出来的其他 Tensor 进行求导，然后将导数值存在 Tensor 的 grad 属性中，便于优化器来更新参数。


**所以，你需要注意的是，把 requires_grad 设置成 true 或者 false 要灵活处理。如果是训练过程就要设置为 true，目的是方便求导、更新参数。而到了验证或者测试过程，我们的目的是检查当前模型的泛化能力，那就要把 requires_grad 设置成 Fasle，避免这个参数根据 loss 自动更新。**


从 NumPy 中创建
还记得之前的课程中，我们一同学习了 NumPy 的使用，在实际应用中，我们在处理数据的阶段多使用的是 NumPy，而数据处理好之后想要传入 PyTorch 的深度学习模型中，则需要借助 Tensor，所以 PyTorch 提供了一个从 NumPy 转到 Tensor 的语句：


```
torch.from_numpy(ndarry)
```
有时候我们在开发模型的过程中，需要用到一些特定形式的矩阵 Tensor，比如全是 0 的，或者全是 1 的。这时我们就可以用这个方法创建，比如说，先生成一个全是 0 的 NumPy 数组，然后转换成 Tensor。但是这样也挺麻烦的，因为这意味着你要引入更多的包（NumPy），也会使用更多的代码，这会增加出错的可能性。不过你别担心，PyTorch 内部已经提供了更为简便的方法，我们接着往下看。


#### 创建特殊形式的 Tensor
我们一块来看一下后面的几个常用函数，它们都是在 PyTorch 模型内部使用的。

创建零矩阵 Tensor：零矩阵顾名思义，就是所有的元素都为 0 的矩阵。

```
torch.zeros(*size, dtype=None...)
```
其中，我们用得比较多的就是 size 参数和 dtype 参数。size 定义输出张量形状的整数序列。这里你可能注意到了，在函数参数列表中我加入了省略号，这意味着 torch.zeros 的参数有很多。不过。咱们现在是介绍零矩阵的概念，形状相对来说更重要。其他的参数（比如前面提到的 requires_grad 参数）与此无关，现阶段我们暂时不关注。

创建单位矩阵 Tensor：单位矩阵是指主对角线上的元素都为 1 的矩阵。

```
torch.eye(size, dtype=None...)
```
创建全一矩阵 Tensor：全一矩阵顾名思义，就是所有的元素都为 1 的矩阵。
```
torch.ones(size, dtype=None...)
```
创建随机矩阵 Tensor：在 PyTorch 中有几种较为经常使用的随机矩阵创建方式，分别如下。
```
torch.rand(size)
torch.randn(size)
torch.normal(mean, std, size)
torch.randint(low, high, size）
```
这些方式各自有不同的用法，你可以根据自己的需要灵活使用。


torch.rand 用于生成数据类型为浮点型且维度指定的随机 Tensor，随机生成的浮点数据在 0~1 区间均匀分布。torch.randn 用于生成数据类型为浮点型且维度指定的随机 Tensor，随机生成的浮点数的取值满足均值为 0、方差为 1 的标准正态分布。torch.normal 用于生成数据类型为浮点型且维度指定的随机 Tensor，可以指定均值和标准差。torch.randint 用于生成随机整数的 Tensor，其内部填充的是在[low,high) 均匀生成的随机整数。


#### Tensor 的转换
在实际项目中，我们接触到的数据类型有很多，比如 Int、list、NumPy 等。为了让数据在各个阶段畅通无阻，不同数据类型与 Tensor 之间的转换就非常重要了。接下来我们一起来看看 int、list、NumPy 是如何与 Tensor 互相转换的。

Int 与 Tensor 的转换：
```
a = torch.tensor(1)
b = a.item()
```
我们通过 torch.Tensor 将一个数字（或者标量）转换为 Tensor，又通过 item() 函数，将 Tensor 转换为数字（标量），item() 函数的作用就是将 Tensor 转换为一个 python number。

list 与 tensor 的转换：


```
a = [1, 2, 3]
b = torch.tensor(a)
c = b.numpy().tolist()
```
在这里对于一个 list a，我们仍旧直接使用 torch.Tensor，就可以将其转换为 Tensor 了。而还原回来的过程要多一步，需要我们先将 Tensor 转为 NumPy 结构，之后再使用 tolist() 函数得到 list。

NumPy 与 Tensor 的转换：有了前面两个例子，你是否能想到 NumPy 怎么转换为 Tensor 么？对，我们仍旧 torch.Tensor 即可，是不是特别方便。

CPU 与 GPU 的 Tensor 之间的转换：

```
CPU->GPU: data.cuda()
GPU->CPU: data.cpu()
```
#### Tensor 的常用操作
好，刚才我们一起了解了 Tensor 的类型，如何创建 Tensor，以及如何实现 Tensor 和一些常见的数据类型之间的相互转换。其实 Tensor 还有一些比较常用的功能，比如获取形状、维度转换、形状变换以及增减维度，接下来我们一起来看看这些功能。

获取形状
在深度学习网络的设计中，我们需要时刻对 Tensor 的情况做到了如指掌，其中就包括获取 Tensor 的形式、形状等。为了得到 Tensor 的形状，我们可以使用 shape 或 size 来获取。两者的不同之处在于，shape 是 torch.tensor 的一个属性，而 size() 则是一个 torch.tensor 拥有的方法。
```
>>> a=torch.zeros(2, 3, 5)
>>> a.shape
torch.Size([2, 3, 5])
>>> a.size()
torch.Size([2, 3, 5])
```
![img](https://static001.geekbang.org/resource/image/8d/24/8d5a9954126f8a737e58ba2f8afdc624.jpg?wh=1920x1080)

知道了 Tensor 的形状，我们就能知道这个 Tensor 所包含的元素的数量了。具体的计算方法就是直接将所有维度的大小相乘，比如上面的 Tensor a 所含有的元素的个数为 235=30 个。这样似乎有点麻烦，我们在 PyTorch 中可以使用 numel() 函数直接统计元素数量。

```
>>> a.numel()
30
```
#### 矩阵转秩 (维度转换）
在 PyTorch 中有两个函数，分别是 permute() 和 transpose() 可以用来实现矩阵的转秩，或者说交换不同维度的数据。比如在调整卷积层的尺寸、修改 channel 的顺序、变换全连接层的大小的时候，我们就要用到它们。其中，用 permute 函数可以对任意高维矩阵进行转置，但只有 tensor.permute() 这个调用方式，我们先看一下代码：

```
>>> x = torch.rand(2,3,5)
>>> x.shape
torch.Size([2, 3, 5])
>>> x = x.permute(2,1,0)
>>> x.shape
torch.Size([5, 3, 2])
```
![img](https://static001.geekbang.org/resource/image/02/84/025985c8635f3896d45d15e1ea381c84.jpg?wh=1920x1080)

有没有发现，原来的 Tensor 的形状是[2,3,5]，我们在 permute 中分别写入原来索引位置的新位置，x.permute(2,1,0)，2 表示原来第二个维度现在放在了第零个维度；同理 1 表示原来第一个维度仍旧在第一个维度；0 表示原来第 0 个维度放在了现在的第 2 个维度，形状就变成了[5,3,2]而另外一个函数 transpose，不同于 permute，它每次只能转换两个维度，或者说交换两个维度的数据。我们还是来看一下代码：

```
>>> x.shape
torch.Size([2, 3, 4])
>>> x = x.transpose(1,0)
>>> x.shape
torch.Size([3, 2, 4])
```
需要注意的是，经过了 transpose 或者 permute 处理之后的数据，变得不再连续了，什么意思呢？还是接着刚才的例子说，我们使用 torch.rand(2,3,4) 得到的 tensor，在内存中是连续的，但是经过 transpose 或者 permute 之后呢，比如 transpose(1,0)，内存虽然没有变化，但是我们得到的数据“看上去”是第 0 和第 1 维的数据发生了交换，现在的第 0 维是原来的第 1 维，所以 Tensor 都会变得不再连续。那你可能会问了，不连续就不连续呗，好像也没啥影响吧？这么想你就草率了，我们继续来看看 Tensor 的形状变换，学完以后你就知道 Tensor 不连续的后果了。

#### 形状变换
在 PyTorch 中有两种常用的改变形状的函数，分别是 view 和 reshape。我们先来看一下 view。

```
>>> x = torch.randn(4, 4)
>>> x.shape
torch.Size([4, 4])
>>> x = x.view(2,8)
>>> x.shape
torch.Size([2, 8])
```
我们先声明了一个[4, 4]大小的 Tensor，然后通过 view 函数，将其修改为[2, 8]形状的 Tensor。我们还是继续刚才的 x，再进行一步操作，代码如下：

```
>>> x = x.permute(1,0)
>>> x.shape
torch.Size([8, 2])
>>> x.view(4, 4)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

```
结合代码可以看到，利用 permute，我们将第 0 和第 1 维度的数据进行了变换，得到了[8, 2]形状的 Tensor，在这个新 Tensor 上进行 view 操作，忽然就报错了，为什么呢？其实就是因为 view 不能处理内存不连续 Tensor 的结构。那这时候要怎么办呢？我们可以使用另一个函数，reshape：

```
>>> x = x.reshape(4, 4)
>>> x.shape
torch.Size([4, 4])
```
这样问题就迎刃而解了。其实 reshape 相当于进行了两步操作，先把 Tensor 在内存中捋顺了，然后再进行 view 操作。


#### 增减维度
有时候我们需要对 Tensor 增加或者删除某些维度，比如删除或者增加图片的几个通道。PyTorch 提供了 squeeze() 和 unsqueeze() 函数解决这个问题。我们先来看 squeeze()。如果 dim 指定的维度的值为 1，则将该维度删除，若指定的维度值不为 1，则返回原来的 Tensor。为了方便你理解，我还是结合例子来讲解。

```
>>> x = torch.rand(2,1,3)
>>> x.shape
torch.Size([2, 1, 3])
>>> y = x.squeeze(1)
>>> y.shape
torch.Size([2, 3])
>>> z = y.squeeze(1)
>>> z.shape
torch.Size([2, 3])
```
结合代码我们可以看到，我们新建了一个维度为[2, 1, 3]的 Tensor，然后将第 1 维度的数据删除，得到 y，squeeze 执行成功是因为第 1 维度的大小为 1。然而在 y 上我们打算进一步删除第 1 维度的时候，就会发现删除失败了，这是因为 y 此刻的第 1 维度的大小为 3，suqeeze 不能删除。unsqueeze()：这个函数主要是对数据维度进行扩充。给指定位置加上维数为 1 的维度，我们同样结合代码例子来看看。

```
>>> x = torch.rand(2,1,3)
>>> y = x.unsqueeze(2)
>>> y.shape
torch.Size([2, 1, 1, 3])
```
这里我们新建了一个维度为[2, 1, 3]的 Tensor，然后在第 2 维度插入一个维度，这样就得到了一个[2,1,1,3]大小的 tensor。


### 小结
之前我们学习了 NumPy 相关的操作，如果把 NumPy 和 Tensor 做对比，就不难发现它们之间有很多共通的内容，共性就是两者都是数据的表示形式，都可以看作是科学计算的通用工具。但是 NumPy 和 Tensor 的用途是不一样的，NumPy 不能用于 GPU 加速，Tensor 则可以。这节课我们一同学习了 Tensor 的创建、类型、转换、变换等常用功能，通过这几个功能，我们就可以对 Tensor 进行最基本也是最常用的操作，这些都是必须要牢记的内容。此外，在实际上，真正的项目实战中还有个非常多的操作种类，其中较为重要的是数学计算操作，比如加减乘除、合并、连接等。但是这些操作如果一个一个列举出来，数量极其繁多，你也会感觉很枯燥，所以在后续的课程中，咱们会在具体的实战环节来学习相关的数学操作。下一节课的内容，咱们会对 Tensor 的变形、切分等高级操作进行学习，这是一个很好玩儿的内容，敬请期待。

## 05 | Tensor变形记：快速掌握Tensor切分、变形等方法

上节课我们一起学习了 Tensor 的基础概念，也熟悉了创建、转换、维度变换等操作，掌握了这些基础知识，你就可以做一些简单的 Tensor 相关的操作了。不过，要想在实际的应用中更灵活地用好 Tensor，Tensor 的连接、切分等操作也是必不可少的。今天这节课，咱们就通过一些例子和图片来一块学习下。虽然这几个操作比较有难度，但只要你耐心听我讲解，然后上手练习，还是可以拿下的。
### Tensor 的连接操作
在项目开发中，深度学习某一层神经元的数据可能有多个不同的来源，那么就需要将数据进行组合，这个组合的操作，我们称之为连接。

cat
连接的操作函数如下。

```
torch.cat(tensors, dim = 0, out = None)
```
cat 是 concatnate 的意思，也就是拼接、联系的意思。该函数有两个重要的参数需要你掌握。第一个参数是 tensors，它很好理解，就是若干个我们准备进行拼接的 Tensor。第二个参数是 dim，我们回忆一下 Tensor 的定义，Tensor 的维度（秩）是有多种情况的。比如有两个 3 维的 Tensor，可以有几种不同的拼接方式（如下图），dim 参数就可以对此作出约定。

![img](https://static001.geekbang.org/resource/image/61/3c/61bd88f3yy8d0ca07799f36540d3473c.jpg?wh=1285x862)

看到这里，你可能觉得上面画的图是三维的，看起来比较晦涩，所以咱们先从简单的二维的情况说起，我们先声明两个 3x3 的矩阵，代码如下：
```
>>> A=torch.ones(3,3)
>>> B=2*torch.ones(3,3)
>>> A
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
>>> B
tensor([[2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]])
```
我们先看看 dim=0 的情况，拼接的结果是怎样的：


```
>>> C=torch.cat((A,B),0)
>>> C
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]])
```
你会发现，两个矩阵是按照“行”的方向拼接的。我们接下来再看看，dim=1 的情况是怎样的：
```
>>> D=torch.cat((A,B),1)
>>> D
tensor([[1., 1., 1., 2., 2., 2.],
        [1., 1., 1., 2., 2., 2.],
        [1., 1., 1., 2., 2., 2.]])
```
显然，两个矩阵，是按照“列”的方向拼接的。那如果 Tensor 是三维甚至更高维度的呢？其实道理也是一样的，dim 的数值是多少，两个矩阵就会按照相应维度的方向链接两个 Tensor。看到这里你可能会问了，cat 实际上是将多个 Tensor 在已有的维度上进行连接，那如果想增加新的维度进行连接，又该怎么做呢？这时候就需要 stack 函数登场了。

stack
为了让你加深理解，我们还是结合具体例子来看看。假设我们有两个二维矩阵 Tensor，把它们“堆叠”放在一起，构成一个三维的 Tensor，如下图：

![img](https://static001.geekbang.org/resource/image/9d/66/9d991a0d571e2733ba15d67566f65166.jpg?wh=1160x770)

这相当于原来的维度（秩）是 2，现在变成了 3，变成了一个立体的结构，增加了一个维度。你需要注意的是，这跟前面的 cat 不同，cat 中示意图的例子，原来就是 3 维的，cat 之后仍旧是 3 维的，而现在咱们是从 2 维变成了 3 维。在实际图像算法开发中，咱们有时候需要将多个单通道 Tensor（2 维）合并，得到多通道的结果（3 维）。而实现这种增加维度拼接的方法，我们把它叫做 stack。stack 函数的定义如下：


```
torch.stack(inputs, dim=0)
```
其中，inputs 表示需要拼接的 Tensor，dim 表示新建立维度的方向。那 stack 如何使用呢？我们一块来看一个例子：
```
>>> A=torch.arange(0,4)
>>> A
tensor([0, 1, 2, 3])
>>> B=torch.arange(5,9)
>>> B
tensor([5, 6, 7, 8])
>>> C=torch.stack((A,B),0)
>>> C
tensor([[0, 1, 2, 3],
        [5, 6, 7, 8]])
>>> D=torch.stack((A,B),1)
>>> D
tensor([[0, 5],
        [1, 6],
        [2, 7],
        [3, 8]])
```
结合代码，我们可以看到，首先我们构建了两个 4 元素向量 A 和 B，它们的维度是 1。然后，我们在 dim=0，也就是“行”的方向上新建一个维度，这样维度就成了 2，也就得到了 C。而对于 D，我们则是在 dim=1，也就是“列”的方向上新建维度。


### Tensor 的切分操作
学完了连接操作之后，我们再来看看连接的逆操作：切分。切分就是连接的逆过程，有了刚才的经验，你很容易就会想到，切分的操作也应该有很多种，比如切片、切块等。没错，切分的操作主要分为三种类型：chunk、split、unbind。乍一看有不少，其实是因为它们各有特点，适用于不同的使用情景，让我们一起看一下。

chunk
chunk 的作用就是将 Tensor 按照声明的 dim，进行尽可能平均的划分。比如说，我们有一个 32channel 的特征，需要将其按照 channel 均匀分成 4 组，每组 8 个 channel，这个切分就可以通过 chunk 函数来实现。具体函数如下：
```
torch.chunk(input, chunks, dim=0)
```
我们挨个来看看函数中涉及到的三个参数：首先是 input，它表示要做 chunk 操作的 Tensor。接着，我们看下 chunks，它代表将要被划分的块的数量，而不是每组的数量。请注意，**chunks 必须是整型**。最后是 dim，想想这个参数是什么意思呢？对，就是按照哪个维度来进行 chunk。还是跟之前一样，我们通过几个代码例子直观感受一下。我们从一个简单的一维向量开始：


```
>>> A=torch.tensor([1,2,3,4,5,6,7,8,9,10])
>>> B = torch.chunk(A, 2, 0)
>>> B
(tensor([1, 2, 3, 4, 5]), tensor([ 6,  7,  8,  9, 10]))
```
这里我们通过 chunk 函数，将原来 10 位长度的 Tensor A，切分成了两个一样 5 位长度的向量。（注意，B 是两个切分结果组成的 tuple）。那如果 chunk 参数不能够整除的话，结果会是怎样的呢？我们接着往下看：
```
>>> B = torch.chunk(A, 3, 0)
>>> B
```
(tensor([1, 2, 3, 4]), tensor([5, 6, 7, 8]), tensor([ 9, 10]))
我们发现，10 位长度的 Tensor A，切分成了三个向量，长度分别是 4，4，2 位。这是怎么分的呢，不应该是 3，3，4 这样更为平均的方式么？想要解决问题，就得找到规律。让我们再来看一个更大一点的例子，将 A 改为 17 位长度。
```
>>> A=torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17])
>>> B = torch.chunk(A, 4, 0)
>>> B
(tensor([1, 2, 3, 4, 5]), tensor([ 6,  7,  8,  9, 10]), tensor([11, 12, 13, 14, 15]), tensor([16, 17]))
```
17 位长度的 Tensor A，切分成了四个分别为 5，5，5，2 位长度的向量。这时候你就会发现，其实在计算每个结果元素个数的时候，chunk 函数是先做除法，然后再向上取整得到每组的数量。比如上面这个例子，17/4=4.25，向上取整就是 5，那就先逐个生成若干个长度为 5 的向量，最后不够的就放在一块，作为最后一个向量（长度 2）。那如果 chunk 参数大于 Tensor 可以切分的长度，又要怎么办呢？我们实际操作一下，代码如下：
```
>>> A=torch.tensor([1,2,3])
>>> B = torch.chunk(A, 5, 0)
>>> B
(tensor([1]), tensor([2]), tensor([3]))
```
显然，被切分的 Tensor 只能分成若干个长度为 1 的向量。由此可以推论出二维的情况，我们再举一个例子， 看看二维矩阵 Tensor 的情况 ：


```
>>> A=torch.ones(4,4)
>>> A
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
>>> B = torch.chunk(A, 2, 0)
>>> B
(tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.]]), 
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.]]))

```
还是跟前面的 cat 一样，这里的 dim 参数，表示的是第 dim 维度方向上进行切分。刚才介绍的 chunk 函数，是按照“切分成确定的份数”来进行切分的，那如果想按照“每份按照确定的大小”来进行切分，该怎样做呢？PyTorch 也提供了相应的方法，叫做 split。


split
split 的函数定义如下，跟前面一样，我们还是分别看看这里涉及的参数。

```
torch.split(tensor, split_size_or_sections, dim=0)
```
首先是 tensor，也就是待切分的 Tensor。然后是 split_size_or_sections 这个参数。当它为整数时，表示将 tensor 按照每块大小为这个整数的数值来切割；当这个参数为列表时，则表示将此 tensor 切成和列表中元素一样大小的块。最后同样是 dim，它定义了要按哪个维度切分。同样的，我们举几个例子来看一下 split 的具体操作。首先是 split_size_or_sections 是整数的情况。


```
>>> A=torch.rand(4,4)
>>> A
tensor([[0.6418, 0.4171, 0.7372, 0.0733],
        [0.0935, 0.2372, 0.6912, 0.8677],
        [0.5263, 0.4145, 0.9292, 0.5671],
        [0.2284, 0.6938, 0.0956, 0.3823]])
>>> B=torch.split(A, 2, 0)
>>> B
(tensor([[0.6418, 0.4171, 0.7372, 0.0733],
        [0.0935, 0.2372, 0.6912, 0.8677]]), 
tensor([[0.5263, 0.4145, 0.9292, 0.5671],
        [0.2284, 0.6938, 0.0956, 0.3823]]))
```
在这个例子里，我们看到，原来 4x4 大小的 Tensor A，沿着第 0 维度，也就是沿“行”的方向，按照每组 2“行”的大小进行切分，得到了两个 2x4 大小的 Tensor。那么问题来了，如果 split_size_or_sections 不能整除对应方向的大小的话，会有怎样的结果呢？我们将代码稍作修改就好了：

```
>>> C=torch.split(A, 3, 0)
>>> C
(tensor([[0.6418, 0.4171, 0.7372, 0.0733],
        [0.0935, 0.2372, 0.6912, 0.8677],
        [0.5263, 0.4145, 0.9292, 0.5671]]), 
tensor([[0.2284, 0.6938, 0.0956, 0.3823]]))
```
根据刚才的代码我们就能发现，原来，PyTorch 会尽可能凑够每一个结果，使得其对应 dim 的数据大小等于 split_size_or_sections。如果最后剩下的不够，那就把剩下的内容放到一块，作为最后一个结果。接下来，我们再看一下 split_size_or_sections 是列表时的情况。刚才提到了，当 split_size_or_sections 为列表的时候，表示将此 tensor 切成和列表中元素大小一样的大小的块，我们来看一段对应的代码：

```
>>> A=torch.rand(5,4)
>>> A
tensor([[0.1005, 0.9666, 0.5322, 0.6775],
        [0.4990, 0.8725, 0.5627, 0.8360],
        [0.3427, 0.9351, 0.7291, 0.7306],
        [0.7939, 0.3007, 0.7258, 0.9482],
        [0.7249, 0.7534, 0.0027, 0.7793]])
>>> B=torch.split(A,(2,3),0)
>>> B
(tensor([[0.1005, 0.9666, 0.5322, 0.6775],
        [0.4990, 0.8725, 0.5627, 0.8360]]), 
tensor([[0.3427, 0.9351, 0.7291, 0.7306],
        [0.7939, 0.3007, 0.7258, 0.9482],
        [0.7249, 0.7534, 0.0027, 0.7793]]))
```
这部分代码怎么解释呢？其实也很好理解，就是将 Tensor A，沿着第 0 维进行切分，每一个结果对应维度上的尺寸或者说大小，分别是 2（行），3（行）。


unbind
通过学习前面的几个函数，咱们知道了怎么按固定大小做切分，或者按照索引 index 来进行选择。现在我们想象一个应用场景，如果我们现在有一个 3 channel 图像的 Tensor，想要逐个获取每个 channel 的数据，该怎么做呢？假如用 chunk 的话，我们需要将 chunks 设为 3；如果用 split 的话，需要将 split_size_or_sections 设为 1。虽然它们都可以实现相同的目的，但是如果 channel 数量很大，逐个去取也比较折腾。这时候，就需要用到另一个函数：unbind，它的函数定义如下：

```
torch.unbind(input, dim=0)

```
其中，input 表示待处理的 Tensor，dim 还是跟前面的函数一样，表示切片的方向。我们结合例子来理解：

```
>>> A=torch.arange(0,16).view(4,4)
>>> A
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15]])
>>> b=torch.unbind(A, 0)
>>> b
(tensor([0, 1, 2, 3]), 
tensor([4, 5, 6, 7]), 
tensor([ 8,  9, 10, 11]), 
tensor([12, 13, 14, 15]))
```
在这个例子中，我们首先创建了一个 4x4 的二维矩阵 Tensor，随后我们从第 0 维，也就是“行”的方向进行切分 ，因为矩阵有 4 行，所以就会得到 4 个结果。接下来，我们看一下：如果从第 1 维，也就是“列”的方向进行切分，会是怎样的结果呢：

```
>>> b=torch.unbind(A, 1)
>>> b
(tensor([ 0,  4,  8, 12]), 
tensor([ 1,  5,  9, 13]), 
tensor([ 2,  6, 10, 14]), 
tensor([ 3,  7, 11, 15]))
```
不难发现，这里是按照“列”的方向进行拆解的。所以，unbind 是一种降维切分的方式，相当于删除一个维度之后的结果。

### Tensor 的索引操作
你有没有发现，刚才我们讲的 chunk 和 split 操作，我们都是将数据整体进行切分，并获得全部结果。但有的时候，我们只需要其中的一部分，这要怎么做呢？一个很自然的想法就是，直接告诉 Tensor 我想要哪些部分，这种方法我们称为索引操作。索引操作有很多方式，有提供好现成 API 的，也有用户自行定制的操作，其中最常用的两个操作就是 index_select 和 masked_select，我们分别去看看用法。

index_select
这里就需要 index_select 这个函数了，其定义如下：

```
torch.index_select(tensor, dim, index)
```
这里的 tensor、dim 跟前面函数里的一样，不再赘述。我们重点看一看 index，它表示从 dim 维度中的哪些位置选择数据，这里需要注意，index是 torch.Tensor 类型。还是跟之前一样，我们来看几个示例代码：


```
>>> A=torch.arange(0,16).view(4,4)
>>> A
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15]])
>>> B=torch.index_select(A,0,torch.tensor([1,3]))
>>> B
tensor([[ 4,  5,  6,  7],
        [12, 13, 14, 15]])
>>> C=torch.index_select(A,1,torch.tensor([0,3]))
>>> C
tensor([[ 0,  3],
        [ 4,  7],
        [ 8, 11],
        [12, 15]])
```
在这个例子中，我们先创建了一个 4x4 大小的矩阵 Tensor A。然后，我们从第 0 维选择第 1（行）和 3（行）的数据，并得到了最终的 Tensor B，其大小为 2x4。随后我们从 Tensor A 中选择第 0（列）和 3（列）的数据，得到了最终的 Tensor C，其大小为 4x2。怎么样，是不是非常简单？


masked_select
刚才介绍的 indexed_select，它是基于给定的索引来进行数据提取的。但有的时候，我们还想通过一些判断条件来进行选择，比如提取深度学习网络中某一层中数值大于 0 的参数。这时候，就需要用到 PyTorch 提供的 masked_select 函数了，我们先来看它的定义：


```
torch.masked_select(input, mask, out=None) 
```
这里我们只需要关心前两个参数，input 和 mask。input 表示待处理的 Tensor。mask 代表掩码张量，也就是满足条件的特征掩码。这里你需要注意的是，mask 须跟 input 张量有相同数量的元素数目，但形状或维度不需要相同。你是不是还感觉有些云里雾里？让我来举一个例子，你看了之后，一下子就能明白。你在平时的练习中有没有想过，如果我们让 Tensor 和数字做比较，会有什么样的结果？比如后面这段代码，我们随机生成一个 5 位长度的 Tensor A：

```
>>> A=torch.rand(5)
>>> A
tensor([0.3731, 0.4826, 0.3579, 0.4215, 0.2285])
>>> B=A>0.3
>>> B
tensor([ True,  True,  True,  True, False])
```
在这段代码里，我们让 A 跟 0.3 做比较，得到了一个新的 Tensor，内部每一个数值表示的是 A 中对应数值是否大于 0.3。比如第一个数值原来是 0.3731，大于 0.3，所以是 True；最后一个数值 0.2285 小于 0.3，所以是 False。这个新的 Tensor 其实就是一个掩码张量，它的每一位表示了一个判断条件是否成立的结果。然后，我们继续写一段代码，看看基于掩码 B 的选择是怎样的结果 ：

```
>>> C=torch.masked_select(A, B)
>>> C
tensor([0.3731, 0.4826, 0.3579, 0.4215])
```
你会发现，C 实际上得到的就是：A 中“满足 B 里面元素值为 True 的”对应位置的数据。好了，这下你应该知道了 masked_select 的作用了吧？其实就是我们根据要筛选的条件，得到一个掩码张量，然后用这个张量去提取 Tensor 中的数据。根据这个思路，上面的例子就可以简化为：

```
>>> A=torch.rand(5)
>>> A
tensor([0.3731, 0.4826, 0.3579, 0.4215, 0.2285])
>>> C=torch.masked_select(A, A>0.3)
>>> C
tensor([0.3731, 0.4826, 0.3579, 0.4215])
```
### 小结
恭喜你完成了这节课的学习。这节课，我们一同学习了 Tensor 里更加高级的操作，包括 Tensor 之间的连接操作，Tensor 内部的切分操作，以及基于索引或者筛选条件的数据选择操作。当然了，在使用这些函数的时候，你最需要关注的就是边界的数值大小，具体来说就是维度和大小相关的参数，一定要提前仔细计算好，要不然就会产生错误的结果。结合众多的例子，我相信你一定可以拿下这些操作。这里我特意给你梳理了一张表格，总结归纳了 Tensor 中的主要函数跟用法。不过这些参数咱们也不用死记硬背，我们在使用的时候，根据需要灵活查询相关的参数列表即可。

![img](https://static001.geekbang.org/resource/image/d1/ba/d195706087f784c8e1e1c7c7b25a22ba.jpg?wh=3020x2455)


## 模型训练

## 06 | Torchvision（上）：数据读取，训练开始的第一步


今天起我们进入模型训练篇的学习。如果将模型看作一辆汽车，那么它的开发过程就可以看作是一套完整的生产流程，环环相扣、缺一不可。这些环节包括数据的读取、网络的设计、优化方法与损失函数的选择以及一些辅助的工具等。未来你将尝试构建自己的豪华汽车，或者站在巨人的肩膀上对前人的作品进行优化。试想一下，如果你对这些基础环节所使用的方法都不清楚，你还能很好地进行下去吗？所以通过这个模块，我们的目标是先把基础打好。通过这模块的学习，对于 PyTorch 都为我们提供了哪些丰富的 API，你就会了然于胸了。Torchvision 是一个和 PyTorch 配合使用的 Python 包，包含很多图像处理的工具。我们先从数据处理入手，开始 PyTorch 的学习的第一步。这节课我们会先介绍 Torchvision 的常用数据集及其读取方法，在后面的两节课里，我再带你了解常用的图像处理方法与 Torchvision 其它有趣的功能。


### PyTorch 中的数据读取
训练开始的第一步，首先就是数据读取。PyTorch 为我们提供了一种十分方便的数据读取机制，即使用 Dataset 类与 DataLoader 类的组合，来得到数据迭代器。在训练或预测时，数据迭代器能够输出每一批次所需的数据，并且对数据进行相应的预处理与数据增强操作。下面我们分别来看下 Dataset 类与 DataLoader 类。

#### Dataset 类
PyTorch 中的 Dataset 类是一个抽象类，它可以用来表示数据集。我们通过继承 Dataset 类来自定义数据集的格式、大小和其它属性，后面就可以供 DataLoader 类直接使用。其实这就表示，无论使用自定义的数据集，还是官方为我们封装好的数据集，其本质都是继承了 Dataset 类。而在继承 Dataset 类时，至少需要重写以下几个方法：
```
__init__()：构造函数，可自定义数据读取方法以及进行数据预处理；
__len__()：返回数据集大小；
__getitem__()：索引数据集中的某一个数据。
```
光看原理不容易理解，下面我们来编写一个简单的例子，看下如何使用 Dataset 类定义一个 Tensor 类型的数据集。

```
import torch
from torch.utils.data import Dataset

class MyDataset(Dataset):
    # 构造函数
    def __init__(self, data_tensor, target_tensor):
        self.data_tensor = data_tensor
        self.target_tensor = target_tensor
    # 返回数据集大小
    def __len__(self):
        return self.data_tensor.size(0)
    # 返回索引的数据与标签
    def __getitem__(self, index):
        return self.data_tensor[index], self.target_tensor[index]
```
结合代码可以看到，我们定义了一个名字为 MyDataset 的数据集，在构造函数中，传入 Tensor 类型的数据与标签；在 `__len__` 函数中，直接返回 Tensor 的大小；在 `__getitem__` 函数中返回索引的数据与标签。下面，我们来看一下如何调用刚才定义的数据集。首先随机生成一个 10*3 维的数据 Tensor，然后生成 10 维的标签 Tensor，与数据 Tensor 相对应。利用这两个 Tensor，生成一个 MyDataset 的对象。查看数据集的大小可以直接用 len() 函数，索引调用数据可以直接使用下标。

```
# 生成数据
data_tensor = torch.randn(10, 3)
target_tensor = torch.randint(2, (10,)) # 标签是0或1

# 将数据封装成Dataset
my_dataset = MyDataset(data_tensor, target_tensor)

# 查看数据集大小
print('Dataset size:', len(my_dataset))
'''
输出：
Dataset size: 10
'''

# 使用索引调用数据
print('tensor_data[0]: ', my_dataset[0])
'''
输出:
tensor_data[0]:  (tensor([ 0.4931, -0.0697,  0.4171]), tensor(0))
'''
```
#### DataLoader 类
在实际项目中，如果数据量很大，考虑到内存有限、I/O 速度等问题，在训练过程中不可能一次性的将所有数据全部加载到内存中，也不能只用一个进程去加载，所以就需要多进程、迭代加载，而 DataLoader 就是基于这些需要被设计出来的。DataLoader 是一个迭代器，最基本的使用方法就是传入一个 Dataset 对象，它会根据参数 batch_size 的值生成一个 batch 的数据，节省内存的同时，它还可以实现多进程、数据打乱等处理。DataLoader 类的调用方式如下：

```
from torch.utils.data import DataLoader
tensor_dataloader = DataLoader(dataset=my_dataset, # 传入的数据集, 必须参数
                               batch_size=2,       # 输出的batch大小
                               shuffle=True,       # 数据是否打乱
                               num_workers=0)      # 进程数, 0表示只有主进程

# 以循环形式输出
for data, target in tensor_dataloader: 
    print(data, target)
'''
输出:
tensor([[-0.1781, -1.1019, -0.1507],
        [-0.6170,  0.2366,  0.1006]]) tensor([0, 0])
tensor([[ 0.9451, -0.4923, -1.8178],
        [-0.4046, -0.5436, -1.7911]]) tensor([0, 0])
tensor([[-0.4561, -1.2480, -0.3051],
        [-0.9738,  0.9465,  0.4812]]) tensor([1, 0])
tensor([[ 0.0260,  1.5276,  0.1687],
        [ 1.3692, -0.0170, -1.6831]]) tensor([1, 0])
tensor([[ 0.0515, -0.8892, -0.1699],
        [ 0.4931, -0.0697,  0.4171]]) tensor([1, 0])
'''
 
# 输出一个batch
print('One batch tensor data: ', iter(tensor_dataloader).next())
'''
输出:
One batch tensor data:  [tensor([[ 0.9451, -0.4923, -1.8178],
        [-0.4046, -0.5436, -1.7911]]), tensor([0, 0])]
'''
```
结合代码，我们梳理一下 DataLoader 中的几个参数，它们分别表示：dataset：Dataset 类型，输入的数据集，必须参数；batch_size：int 类型，每个 batch 有多少个样本；shuffle：bool 类型，在每个 epoch 开始的时候，是否对数据进行重新打乱；num_workers：int 类型，加载数据的进程数，0 意味着所有的数据都会被加载进主进程，默认为 0。


### 什么是 Torchvision
PyTroch 官方为我们提供了一些常用的图片数据集，如果你需要读取这些数据集，那么无需自己实现，只需要利用 Torchvision 就可以搞定。Torchvision 是一个和 PyTorch 配合使用的 Python 包。它不只提供了一些常用数据集，还提供了几个已经搭建好的经典网络模型，以及集成了一些图像数据处理方面的工具，主要供数据预处理阶段使用。简单地说，Torchvision 库就是**常用数据集 + 常见网络模型 + 常用图像处理方法**。Torchvision 的安装方式同样非常简单，可以使用 conda 安装，命令如下：

```
conda install torchvision -c pytorch
```
或使用 pip 进行安装，命令如下：

```
pip install torchvision
```
Torchvision 中默认使用的图像加载器是 PIL，因此为了确保 Torchvision 正常运行，我们还需要安装一个 Python 的第三方图像处理库——Pillow 库。Pillow 提供了广泛的文件格式支持，强大的图像处理能力，主要包括图像储存、图像显示、格式转换以及基本的图像处理操作等。使用 conda 安装 Pillow 的命令如下：
```
conda install pillow
```
使用 pip 安装 Pillow 的命令如下：

```
pip install pillow
```
### 利用 Torchvision 读取数据
安装好 Torchvision 之后，我们再来接着看看。Torchvision 库为我们读取数据提供了哪些支持。Torchvision 库中的torchvision.datasets包中提供了丰富的图像数据集的接口。常用的图像数据集，例如 MNIST、COCO 等，这个模块都为我们做了相应的封装。下表中列出了torchvision.datasets包所有支持的数据集。各个数据集的说明与接口，详见链接https://pytorch.org/vision/stable/datasets.html。

![img](https://static001.geekbang.org/resource/image/5f/44/5fa4d9067fa79b140d9e7646e7f28544.jpg?wh=1920x1162)

这里我想提醒你注意，torchvision.datasets这个包本身并不包含数据集的文件本身，它的工作方式是先从网络上把数据集下载到用户指定目录，然后再用它的加载器把数据集加载到内存中。最后，把这个加载后的数据集作为对象返回给用户。为了让你进一步加深对知识的理解，我们以 MNIST 数据集为例，来说明一下这个模块具体的使用方法。

#### MNIST 数据集简介
MNIST 数据集是一个著名的手写数字数据集，因为上手简单，在深度学习领域，手写数字识别是一个很经典的学习入门样例。MNIST 数据集是 NIST 数据集的一个子集，MNIST 数据集你可以通过[这里](http://yann.lecun.com/exdb/mnist/)下载。它包含了四个部分，我用表格的方式为你做了梳理。

![img](https://static001.geekbang.org/resource/image/b6/9d/b6f465e8d27ca7f7abc27932da46309d.jpg?wh=1920x1157)


MNIST 数据集是 ubyte 格式存储，我们先将“训练集图片”解析成图片格式，来直观地看一看数据集具体是什么样子的。具体怎么解析，我在后面数据预览再展开。

![img](https://static001.geekbang.org/resource/image/08/15/08977ccc74a3d2055434174e545d0515.jpg?wh=1920x844)



#### 数据读取
接下来，我们看一下如何使用 Torchvision 来读取 MNIST 数据集。对于torchvision.datasets所支持的所有数据集，它都内置了相应的数据集接口。例如刚才介绍的 MNIST 数据集，torchvision.datasets就有一个 MNIST 的接口，接口内封装了从下载、解压缩、读取数据、解析数据等全部过程。这些接口的工作方式差不多，都是先从网络上把数据集下载到指定目录，然后再用加载器把数据集加载到内存中，最后将加载后的数据集作为对象返回给用户。以 MNIST 为例，我们可以用如下方式调用：

```
# 以MNIST为例
import torchvision
mnist_dataset = torchvision.datasets.MNIST(root='./data',
                                       train=True,
                                       transform=None,
                                       target_transform=None,
                                       download=True)
```
torchvision.datasets.MNIST是一个类，对它进行实例化，即可返回一个 MNIST 数据集对象。构造函数包括包含 5 个参数：root：是一个字符串，用于指定你想要保存 MNIST 数据集的位置。如果 download 是 Flase，则会从目标位置读取数据集；download：是布尔类型，表示是否下载数据集。如果为 True，则会自动从网上下载这个数据集，存储到 root 指定的位置。如果指定位置已经存在数据集文件，则不会重复下载；train：是布尔类型，表示是否加载训练集数据。如果为 True，则只加载训练数据。如果为 False，则只加载测试数据集。**这里需要注意，并不是所有的数据集都做了训练集和测试集的划分，这个参数并不一定是有效参数，具体需要参考官方接口说明文档**；transform：用于对图像进行预处理操作，例如数据增强、归一化、旋转或缩放等。这些操作我们会在下节课展开讲解；target_transform：用于对图像标签进行预处理操作。


运行上述的代码，我们可以得到下图所示的效果。从图中我们可以看出，程序首先去指定的网址下载了 MNIST 数据集，然后进行了解压缩等操作。如果你再次运行相同的代码，则不会再有下载的过程。

![img](https://static001.geekbang.org/resource/image/dc/8a/dcec80c2aa0e63f5450c85b7cda5c88a.png?wh=1920x1387)

看到这，你可能还有疑问，好奇我们得到的 mnist_dataset 是什么呢？如果你用 type 函数查看一下 mnist_dataset 的类型，就可以得到torchvision.datasets.mnist.MNIST ，而这个类是之前我们介绍过的 Dataset 类的派生类。相当于torchvision.datasets ，它已经帮我们写好了对 Dataset 类的继承，完成了对数据集的封装，我们直接使用即可。这里我们主要以 MNIST 为例，进行了说明。其它的数据集使用方法类似，调用的时候你只要需要将类名“MNIST”换成其它数据集名字即可。对于不同的数据集，数据格式都不尽相同，而torchvision.datasets则帮助我们完成了各种不同格式的数据的解析与读取，可以说十分便捷。而对于那些没有官方接口的图像数据集，我们也可以使用以torchvision.datasets.ImageFolder接口来自行定义，在图像分类的实战篇中，就是使用 ImageFolder 进行数据读取的，你可以到那个时候再看一看。


#### 数据预览
完成了数据读取工作，我们得到的是对应的 mnist_dataset，刚才已经讲过了，这是一个封装了的数据集。如果想要查看 mnist_dataset 中的具体内容，我们需要把它转化为列表。（如果 IOPub data rate 超限，可以只加载测试集数据，令 train=False）

```
mnist_dataset_list = list(mnist_dataset)
print(mnist_dataset_list)
```
执行结果如下图所示。

![img](https://static001.geekbang.org/resource/image/9c/12/9c7838a309b6e9ffa8yy33a44b00d312.png?wh=1920x434)


从运行结果中可以看出，转换后的数据集对象变成了一个元组列表，每个元组有两个元素，第一个元素是图像数据，第二个元素是图像的标签。这里图像数据是 PIL.Image.Image 类型的，这种类型可以直接在 Jupyter 中显示出来。显示一条数据的代码如下。

```
display(mnist_dataset_list[0][0])
print("Image label is:", mnist_dataset_list[0][1])
```
运行结果如下图所示。可以看出，数据集 mnist_dataset 中的第一条数据是图片手写数字“7”，对应的标签是“7”。

![img](https://static001.geekbang.org/resource/image/21/c3/211289da00fc13fd21f72573aee049c3.png?wh=1466x242)

好，如果你也得到了上面的运行结果，说明你的操作没问题，恭喜你成功完成了读取操作。

### 小结
恭喜你完成了这节课的学习。我们已经迈出了模型训练的第一步，学会了如何读取数据。今天的重点就是**掌握两种读取数据的方法，也就是自定义和读取常用图像数据集**。最通用的数据读取方法，就是自己定义一个 Dataset 的派生类。而读取常用的图像数据集，就可以利用 PyTorch 提供的视觉包 Torchvision。Torchvision 库为我们读取数据提供了丰富的图像数据集的接口。我用手写数字识别这个经典例子，给你示范了如何使用 Torchvision 来读取 MNIST 数据集。torchvision.datasets继承了 Dataset 类，它在预定义许多常用的数据集的同时，还预留了数据预处理与数据增强的接口。在下一节课中，我们就会接触到这些数据增强函数，并学习如何进行数据增强。



## 07 | Torchvision（中）：数据增强，让数据更加多样性

上一节课，我们一同迈出了训练开始的第一步——数据读取，初步认识了 Torchvision，学习了如何利用 Torchvision 读取数据。不过仅仅将数据集中的图片读取出来是不够的，在训练的过程中，神经网络模型接收的数据类型是 Tensor，而不是 PIL 对象，因此我们还需要对数据进行预处理操作，比如图像格式的转换。与此同时，加载后的图像数据可能还需要进行一系列图像变换与增强操作，例如裁切边框、调整图像比例和大小、标准化等，以便模型能够更好地学习到数据的特征。这些操作都可以使用torchvision.transforms工具完成。今天我们就来学习一下，利用 Torchvision 如何进行数据预处理操作，如何进行图像变换与增强。

### 图像处理工具之 torchvision.transforms
Torchvision 库中的torchvision.transforms包中提供了常用的图像操作，包括对 Tensor 及 PIL Image 对象的操作，例如随机切割、旋转、数据类型转换等等。按照torchvision.transforms 的功能，大致分为以下几类：数据类型转换、对 PIL.Image 和 Tensor 进行变化和变换的组合。下面我们依次来学习这些类别中的操作。


#### 数据类型转换
在上一节课中，我们学习了读取数据集中的图片，读取到的数据是 PIL.Image 的对象。而在模型训练阶段，需要传入 Tensor 类型的数据，神经网络才能进行运算。那么如何将 PIL.Image 或 Numpy.ndarray 格式的数据转化为 Tensor 格式呢？这需要用到transforms.ToTensor() 类。而反之，将 Tensor 或 Numpy.ndarray 格式的数据转化为 PIL.Image 格式，则使用transforms.ToPILImage(mode=None) 类。它则是 ToTensor 的一个逆操作，它能把 Tensor 或 Numpy 的数组转换成 PIL.Image 对象。其中，参数 mode 代表 PIL.Image 的模式，如果 mode 为 None（默认值），则根据输入数据的维度进行推断：

输入为 3 通道：mode 为’RGB’；输入为 4 通道：mode 为’RGBA’；输入为 2 通道：mode 为’LA’;输入为单通道：mode 根据输入数据的类型确定具体模式。

说完用法，我们来看一个具体的例子加深理解。以极客时间的 LOGO 图片（文件名为：jk.jpg）为例，进行一下数据类型的相互转换。具体代码如下。

```
from PIL import Image
from torchvision import transforms 



img = Image.open('jk.jpg') 
display(img)
print(type(img)) # PIL.Image.Image是PIL.JpegImagePlugin.JpegImageFile的基类
'''
输出: 
<class 'PIL.JpegImagePlugin.JpegImageFile'>
'''

# PIL.Image转换为Tensor
img1 = transforms.ToTensor()(img)
print(type(img1))
'''
输出: 
<class 'torch.Tensor'>
'''

# Tensor转换为PIL.Image
img2 = transforms.ToPILImage()(img1)  #PIL.Image.Image
print(type(img2))
'''
输出: 
<class 'PIL.Image.Image'>
'''
```
首先用读取图片，查看一下图片的类型为 PIL.JpegImagePlugin.JpegImageFile，这里需要注意，PIL.JpegImagePlugin.JpegImageFile 类是 PIL.Image.Image 类的子类。然后，用transforms.ToTensor() 将 PIL.Image 转换为 Tensor。最后，再将 Tensor 转换回 PIL.Image。


#### 对 PIL.Image 和 Tensor 进行变换
torchvision.transforms 提供了丰富的图像变换方法，例如：改变尺寸、剪裁、翻转等。并且这些图像变换操作可以接收多种数据格式，不仅可以直接对 PIL 格式的图像进行变换，也可以对 Tensor 进行变换，无需我们再去做额外的数据类型转换。下面我们依次来看一看。

Resize
将输入的 PIL Image 或 Tensor 尺寸调整为给定的尺寸，具体定义为：

```
torchvision.transforms.Resize(size, interpolation=2)
```
我们依次看下相关的参数：size：期望输出的尺寸。如果 size 是一个像 (h, w) 这样的元组，则图像输出尺寸将与之匹配。如果 size 是一个 int 类型的整数，图像较小的边将被匹配到该整数，另一条边按比例缩放。interpolation：插值算法，int 类型，默认为 2，表示 PIL.Image.BILINEAR。

有关 Size 中是 tuple 还是 int 这一点请你一定要注意。让我说明一下，在我们训练时，通常要把图片 resize 到一定的大小，比如说 128x128，256x256 这样的。如果直接给定 resize 后的高与宽，是没有问题的。但如果设定的是一个 int 型，较长的边就会按比例缩放。在 resize 之后呢，一般会接一个 crop 操作，crop 到指定的大小。对于高与宽接近的图片来说，这么做问题不大，但是高与宽的差距较大时，就会 crop 掉很多有用的信息。关于这一点，我们在后续的图像分类部分还会遇到，到时我在详细展开。我们还是以极客时间的 LOGO 图片为例，一起看一下 Resize 的效果。

```
from PIL import Image
from torchvision import transforms 

# 定义一个Resize操作
resize_img_oper = transforms.Resize((200,200), interpolation=2)

# 原图
orig_img = Image.open('jk.jpg') 
display(orig_img)

# Resize操作后的图
img = resize_img_oper(orig_img)
display(img)
```
首先定义一个 Resize 操作，设置好变换后的尺寸为 (200, 200)，然后对极客时间 LOGO 图片进行 Resize 变换。原图以及 Resize 变换后的效果如下表所示。

![img](https://static001.geekbang.org/resource/image/56/09/5611e53aaed88bb079909992db5c6d09.jpg?wh=1232x505)



剪裁
torchvision.transforms提供了多种剪裁方法，例如中心剪裁、随机剪裁、四角和中心剪裁等。我们依次来看下它们的定义。先说中心剪裁，顾名思义，在中心裁剪指定的 PIL Image 或 Tensor，其定义如下：


```
torchvision.transforms.CenterCrop(size)

```
其中，size 表示期望输出的剪裁尺寸。如果 size 是一个像 (h, w) 这样的元组，则剪裁后的图像尺寸将与之匹配。如果  size  是  int  类型的整数，剪裁出来的图像是  (size, size)  的正方形。然后是随机剪裁，就是在一个随机位置剪裁指定的 PIL Image 或 Tensor，定义如下：

```
torchvision.transforms.RandomCrop(size, padding=None)
```
其中，size 代表期望输出的剪裁尺寸，用法同上。而 padding 表示图像的每个边框上的可选填充。默认值是 None，即没有填充。通常来说，不会用 padding 这个参数，至少对于我来说至今没用过。最后要说的是 FiveCrop，我们将给定的 PIL Image  或 Tensor ，分别从四角和中心进行剪裁，共剪裁成五块，定义如下：


```
torchvision.transforms.FiveCrop(size)
```
size 可以是 int 或 tuple，用法同上。掌握了各种剪裁的定义和参数用法以后，我们来看一下这些剪裁操作具体如何调用，代码如下。

```
from PIL import Image
from torchvision import transforms 

# 定义剪裁操作
center_crop_oper = transforms.CenterCrop((60,70))
random_crop_oper = transforms.RandomCrop((80,80))
five_crop_oper = transforms.FiveCrop((60,70))

# 原图
orig_img = Image.open('jk.jpg') 
display(orig_img)

# 中心剪裁
img1 = center_crop_oper(orig_img)
display(img1)
# 随机剪裁
img2 = random_crop_oper(orig_img)
display(img2)
# 四角和中心剪裁
imgs = five_crop_oper(orig_img)
for img in imgs:
    display(img)
```
流程和 Resize 类似，都是先定义剪裁操作，然后对极客时间 LOGO 图片进行不同的剪裁。具体剪裁效果如下表所示。

![img](https://static001.geekbang.org/resource/image/60/b5/60ca577c5f08eef4ca727c1f0aac9cb5.jpg?wh=1384x896)

翻转
接下来，我们来看一看翻转操作。torchvision.transforms提供了两种翻转操作，分别是：以某一概率随机水平翻转图像和以某一概率随机垂直翻转图像。我们分别来看它们的定义。以概率 p 随机水平翻转图像，定义如下：


```
torchvision.transforms.RandomHorizontalFlip(p=0.5)
```
以概率 p 随机垂直翻转图像，定义如下：

```
torchvision.transforms.RandomVerticalFlip(p=0.5)
```
其中，p 表示随机翻转的概率值，默认为 0.5。这里的随机翻转，是为数据增强提供方便。如果想要必须执行翻转操作的话，将 p 设置为 1 即可。以极客时间的 LOGO 图片为例，图片翻转的代码如下。


```
from PIL import Image
from torchvision import transforms 

# 定义翻转操作
h_flip_oper = transforms.RandomHorizontalFlip(p=1)
v_flip_oper = transforms.RandomVerticalFlip(p=1)

# 原图
orig_img = Image.open('jk.jpg') 
display(orig_img)

# 水平翻转
img1 = h_flip_oper(orig_img)
display(img1)
# 垂直翻转
img2 = v_flip_oper(orig_img)
display(img2)
```
翻转效果如下表所示。

![img](https://static001.geekbang.org/resource/image/0d/84/0dc2543bb7bdfyy7803c353f2030f184.jpg?wh=1386x675)



### 只对 Tensor 进行变换
目前版本的 Torchvision（v0.10.0）对各种图像变换操作已经基本同时支持 PIL Image 和 Tensor 类型了，因此只针对 Tensor 的变换操作很少，只有 4 个，分别是 LinearTransformation（线性变换）、Normalize（标准化）、RandomErasing（随机擦除）、ConvertImageDtype（格式转换）。这里我们重点来看最常用的一个操作：标准化，其他 3 个你可以查阅官方文档。


#### 标准化
标准化是指每一个数据点减去所在通道的平均值，再除以所在通道的标准差，数学的计算公式如下：

$$output=(input−mean)/std$$

而对图像进行标准化，就是对图像的每个通道利用均值和标准差进行正则化。这样做的目的，是为了保证数据集中所有的图像分布都相似，这样在训练的时候更容易收敛，既加快了训练速度，也提高了训练效果。让我来解释一下：首先，标准化是一个常规做法，可以理解为无脑进行标准化后再训练的效果，大概率要好于不进行标准化。我把极客时间的 LOGO 读入后，所有像素都减去 50，获得下图。

![img](https://static001.geekbang.org/resource/image/3c/a1/3c3f30cee39ec09cc08fa91b4925e3a1.png?wh=640x234)

对于我们人来说是可以分辨出，这也是极客时间的 LOGO。但是计算机（也就是卷积神经网络）就不一定能分辨出来了，因为卷积神经网络是通过图像的像素进行提取特征的，这两张图片像素的数值都不一样，凭什么还让神经网络认为是一张图片？而标准化后的数据就会避免这一问题，标准化后会将数据映射到同一区间中，一个类别的图片虽说有的像素值可能有差异，但是它们分布都是类似的分布。torchvision.transforms提供了对 Tensor 进行标准化的函数，定义如下。

```
torchvision.transforms.Normalize(mean, std, inplace=False)
```
其中，每个参数的含义如下所示：mean：表示各通道的均值；std：表示各通道的标准差；inplace：表示是否原地操作，默认为否。

以极客时间的 LOGO 图片为例，我们来看看以 (R, G, B) 均值和标准差均为 (0.5, 0.5, 0.5) 来标准化图片后，是什么效果。

```
from PIL import Image
from torchvision import transforms 

# 定义标准化操作
norm_oper = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))

# 原图
orig_img = Image.open('jk.jpg') 
display(orig_img)

# 图像转化为Tensor
img_tensor = transforms.ToTensor()(orig_img)

# 标准化
tensor_norm = norm_oper(img_tensor)

# Tensor转化为图像
img_norm = transforms.ToPILImage()(tensor_norm)
display(img_norm)
```
上面代码的过程是，首先定义了均值和标准差均为 (0.5, 0.5, 0.5) 的标准化操作，然后将原图转化为 Tensor，接着对 Tensor 进行标准化，最后再将 Tensor 转化为图像输出。标准化的效果如下表所示。

![img](https://static001.geekbang.org/resource/image/f5/05/f58f3662e60501e02b31b12fa9f4e905.jpg?wh=1244x515)


#### 变换的组合
其实前面介绍过的所有操作都可以用 Compose 类组合起来，进行连续操作。Compose 类是将多个变换组合到一起，它的定义如下。

```
torchvision.transforms.Compose(transforms)
```
其中，transforms 是一个 Transform 对象的列表，表示要组合的变换列表。我们还是结合例子动手试试，如果我们想要将图片变为 200*200 像素大小，并且随机裁切成 80 像素的正方形。那么我们可以组合 Resize 和 RandomCrop 变换，具体代码如下所示。

```
from PIL import Image
from torchvision import transforms 

# 原图
orig_img = Image.open('jk.jpg') 
display(orig_img)

# 定义组合操作
composed = transforms.Compose([transforms.Resize((200, 200)),
                               transforms.RandomCrop(80)])

# 组合操作后的图
img = composed(orig_img)
display(img)
```
运行的结果如下表所示，也推荐你动手试试看。

![img](https://static001.geekbang.org/resource/image/6b/c1/6b3ce280815cff443734c9b8180fc6c1.jpg?wh=1046x505)

### 结合 datasets 使用
Compose 类是未来我们在实际项目中经常要使用到的类，结合torchvision.datasets包，就可以在读取数据集的时候做图像变换与数据增强操作。下面让我们一起来看一看。还记得上一节课中，在利用torchvision.datasets 读取 MNIST 数据集时，有一个参数“transform”吗？它就是用于对图像进行预处理操作的，例如数据增强、归一化、旋转或缩放等。这里的“transform”就可以接收一个torchvision.transforms操作或者由 Compose 类所定义的操作组合。上节课中，我们在读取 MNIST 数据集时，直接读取出来的图像数据是 PIL.Image.Image 类型的。但是遇到要训练手写数字识别模型这类的情况，模型接收的数据类型是 Tensor，而不是 PIL 对象。这时候，我们就可以利用“transform”参数，使数据在读取的同时做类型转换，这样读取出的数据直接就可以是 Tensor 类型了。不只是数据类型的转换，我们还可以增加归一化等数据增强的操作，只需要使用上面介绍过的 Compose 类进行组合即可。这样，在读取数据的同时，我们也就完成了数据预处理、数据增强等一系列操作。我们还是以读取 MNIST 数据集为例，看下如何在读取数据的同时，完成数据预处理等操作。具体代码如下。


```
from torchvision import transforms
from torchvision import datasets

# 定义一个transform
my_transform = transforms.Compose([transforms.ToTensor(),
                                   transforms.Normalize((0.5), (0.5))
                                  ])
# 读取MNIST数据集 同时做数据变换
mnist_dataset = datasets.MNIST(root='./data',
                               train=False,
                               transform=my_transform,
                               target_transform=None,
                               download=True)

# 查看变换后的数据类型
item = mnist_dataset.__getitem__(0)
print(type(item[0]))
'''
输出：
<class 'torch.Tensor'>
'''
```
当然，MNIST 数据集非常简单，根本不进行任何处理直接读入的话，效果也非常好，但是它确实适合学习来使用，你可以在利用它进行各种尝试。我们下面先来看看，在图像分类实战中使用的 transform，可以感受一下实际使用的 transforms 是什么样子：


```
transform = transforms.Compose([
    transforms.RandomResizedCrop(dest_image_size),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
```
这也是我在项目中使用的 transform。数据增强的方法有很多，不过根据我的经验来看，并不是用的越多，效果越好。


### 小结
恭喜你完成了这节课的学习，我来给你做个总结。今天的重点内容就是torchvision.transforms工具的使用。包括常用的图像处理操作，以及如何与torchvision.datasets结合使用。常用的图像处理操作包括数据类型转换、图像尺寸变化、剪裁、翻转、标准化等等。Compose 类还可以将多个变换操作组合成一个 Transform 对象的列表。torchvision.transforms与torchvision.datasets结合使用，可以在数据加载的同时进行一系列图像变换与数据增强操作，不仅能够直接将数据送入模型训练，还可以加快模型收敛速度，让模型更好地学习到数据特征。当然，我们在实际的项目中会有自己的数据，而不会使用 torchvision.datasets 中提供的公开数据集，我们今天讲的torchvision.transforms 同样可以在我们自定义的数据集中使用，关于这一点，我会在图像分类的实战中继续讲解。下节课中，我们会介绍 Torchvision 中其他有趣的功能。包括经典网络模型的实例化与其他有用的函数。



## 08 | Torchvision（下）：其他有趣的功能

在前面的课程中，我们已经学习了 Torchvision 的数据读取与常用的图像变换方法。其实，Torchvision 除了帮我们封装好了常用的数据集，还为我们提供了深度学习中各种经典的网络结构以及训练好的模型，只要直接将这些经典模型的类实例化出来，就可以进行训练或使用了。我们可以利用这些训练好的模型来实现图片分类、物体检测、视频分类等一系列应用。今天，我们就来学习一下经典网络模型的实例化与 Torchvision 中其他有趣的功能。

### 常见网络模型
Torchvision 中的各种经典网络结构以及训练好的模型，都放在了torchvision.models模块中，下面我们来看一看torchvision.models 具体为我们提供了什么支持，以及这些功能如何使用。


#### torchvision.models 模块
torchvision.models 模块中包含了常见网络模型结构的定义，这些网络模型可以解决以下四大类问题：图像分类、图像分割、物体检测和视频分类。图像分类、物体检测与图像分割的示意图如下图所示。

![img](https://static001.geekbang.org/resource/image/42/b3/4211c2d8cd27db3e903e6125122f47b3.jpg?wh=1920x1204)

图像分类，指的是单纯把一张图片判断为某一类，例如将上图左侧第一张判断为 cat。目标检测则是说，首先检测出物体的位置，还要识别出对应物体的类别。如上图中间的那张图，不仅仅要找到猫、鸭子、狗的位置，还有给出给定物体的类别信息。我们看一下图里最右侧的例子，它表示的是分割。分割即是对图像中每一个像素点进行分类，确定每个点的类别，从而进行区域划分。在早期的 Torchvision 版本中，torchvision.models模块中只包含了图片分类中的一部分网络，例如 AlexNet、VGG 系列、ResNet 系列、Inception 系列等。这里你先有个印象就行，具体网络特点，我后面会在图像分类中详细讲解。到了现在，随着深度学习技术的不断发展，人工智能应用更为广泛，torchvision.models模块中所封装的网络模型也在不断丰富。比如在当前版本（v0.10.0）的 Torchvision 中，新增了图像语义分割、物体检测和视频分类的相关网络，并且在图像分类中也新增了 GoogLeNet、ShuffleNet 以及可以使用于移动端的 MobileNet 系列。这些新模型，都能让我们站在巨人的肩膀上看世界。

#### 实例化一个 GoogLeNet 网络
如果我们直接把一个网络模型的类实例化，就会得到一个网络模型。而这个网络模型的类可以是我们自己定义的结构，也可以是按照经典模型的论文设计出来的结构。其实你自己按照经典模型的论文写一个类，然后实例化一下，这和从 Torchvision 中直接实例化一个网络效果是相同的。下面我们就以 GoogLeNet 网络为例，来说说如何使用torchvision.models模块实例化一个网络。GoogLeNet 是 Google 推出的基于 Inception 模块的深度神经网络模型。你可别小看这个模型，GoogLeNet 获得了 2014 年的 ImageNet 竞赛的冠军，并且相比之前的 AlexNet、VGG 等结构能更高效地利用计算资源。GoogLeNet 也被称为 Inception V1，在随后的两年中它一直在改进，形成了 Inception V2、Inception V3 等多个版本。

我们可以使用随机初始化的权重，创建一个 GoogLeNet 模型，具体代码如下：

```
import torchvision.models as models
googlenet = models.googlenet()
```
这时候的 GoogLeNet 模型，相当于只有一个实例化好的网络结构，里面的参数都是随机初始化的，需要经过训练之后才能使用，并不能直接用于预测。torchvision.models模块除了包含了定义好的网络结构，还为我们提供了预训练好的模型，我们可以直接导入训练好的模型来使用。导入预训练好的模型的代码如下：


```
import torchvision.models as models
googlenet = models.googlenet(pretrained=True)

```
可以看出，我们只是在实例化的时候，引入了一个参数“pretrained=True”，即可获得预训练好的模型，因为所有的工作torchvision.models模块都已经帮我们封装好了，用起来很方便。torchvision.models模块中所有预训练好的模型，都是在 ImageNet 数据集上训练的，它们都是由 PyTorch 的torch.utils.model_zoo模块所提供的，并且我们可以通过参数  pretrained=True  来构造这些预训练模型。如果之前没有加载过带预训练参数的网络，在实例化一个预训练好的模型时，模型的参数会被下载至缓存目录中，下载一次后不需要重复下载。这个缓存目录可以通过环境变量 TORCH_MODEL_ZOO 来指定。当然，你也可以把自己下载好的模型，然后复制到指定路径中。下图是运行了上述实例化代码的结果，可以看到，GoogLeNet 的模型参数被下载到了缓存目录 /root/.cache/torch 下面。

![img](https://static001.geekbang.org/resource/image/16/49/16975c6f4071ee1dacc9a41a28f93c49.png?wh=1920x270)

torchvision.models模块也包含了 Inception V3 和其他常见的网络结构，在实例化时，只需要修改网络的类名，即可做到举一反三。torchvision.models模块中可实例化的全部模型详见这个[网页](https://pytorch.org/vision/stable/models.html)。


#### 模型微调
完成了刚才的工作，你可能会疑惑，实例化了带预训练参数的网络有什么用呢？其实它除了可以直接用来做预测使用，还可以基于它做网络模型的微调，也就是“fine-tuning”。那什么是“fine-tuning”呢？举个例子，假设你的老板给布置了一个有关于图片分类的任务，数据集是关于狗狗的图片，让你区分图片中狗的种类，例如金毛、柯基、边牧等等。问题是数据集中狗的类别很多，但数据却不多。你发现从零开始训练一个图片分类模型，但这样模型效果很差，并且很容易过拟合。这种问题该如何解决呢？于是你想到了使用迁移学习，可以用已经在 ImageNet 数据集上训练好的模型来达成你的目的。例如上面我们已经实例化的 GoogLeNet 模型，只需要使用我们自己的数据集，重新训练网络最后的分类层，即可得到区分狗种类的图片分类模型。这就是所谓的“fine-tuning”方法。


模型微调，简单来说就是先在一个比较通用、宽泛的数据集上进行大量训练得出了一套参数，然后再使用这套预训练好的网络和参数，在自己的任务和数据集上进行训练。使用经过预训练的模型，要比使用随机初始化的模型训练效果更好，更容易收敛，并且训练速度更快，在小数据集上也能取得比较理想的效果。那新的问题又来了，为什么模型微调如此有效呢？因为我们相信同样是处理图片分类任务的两个模型，网络的参数也具有某种相似性。因此，把一个已经训练得很好的模型参数迁移到另一个模型上，同样有效。即使两个模型的工作不完全相同，我们也可以在这套预训练参数的基础上，经过微调性质的训练，同样能取得不错的效果。ImageNet 数据集共有 1000 个类别，而狗的种类远远达不到 1000 类。因此，加载了预训练好的模型之后，还需要根据你的具体问题对模型或数据进行一些调整，通常来说是调整输出类别的数量。假设狗的种类一共为 10 类，那么我们自然需要将 GoogLeNet 模型的输出分类数也调整为 10。对预训练模型进行调整对代码如下：


```
import torch
import torchvision.models as models

# 加载预训练模型
googlenet = models.googlenet(pretrained=True)

# 提取分类层的输入参数
fc_in_features = googlenet.fc.in_features
print("fc_in_features:", fc_in_features)

# 查看分类层的输出参数
fc_out_features = googlenet.fc.out_features
print("fc_out_features:", fc_out_features)

# 修改预训练模型的输出分类数(在图像分类原理中会具体介绍torch.nn.Linear)
googlenet.fc = torch.nn.Linear(fc_in_features, 10)
'''
输出：
fc_in_features: 1024
fc_out_features: 1000
'''
```
首先，你需要加载预训练模型，然后提取预训练模型的分类层固定参数，最后修改预训练模型的输出分类数为 10。根据输出结果，我们可以看到预训练模型的原始输出分类数是 1000。



### 其他常用函数
之前在torchvision.transforms中，我们学习了很多有关于图像处理的函数，Torchvision 还提供了几个常用的函数，make_grid 和 save_img，让我们依次来看一看它们又能实现哪些有趣的功能。


make_grid
make_grid 的作用是将若干幅图像拼成在一个网格中，它的定义如下。

```
torchvision.utils.make_grid(tensor, nrow=8, padding=2) 
```
定义中对应的几个参数含义如下：tensor：类型是 Tensor 或列表，如果输入类型是 Tensor，其形状应是 (B x C x H x W)；如果输入类型是列表，列表中元素应为相同大小的图片。nrow：表示一行放入的图片数量，默认为 8。padding：子图像与子图像之间的边框宽度，默认为 2 像素。

make_grid 函数主要用于展示数据集或模型输出的图像结果。我们以 MNIST 数据集为例，整合之前学习过的读取数据集以及图像变换的内容，来看一看 make_grid 函数的效果。下面的程序利用 make_grid 函数，展示了 MNIST 的测试集中的 32 张图片。


```
import torchvision
from torchvision import datasets
from torchvision import transforms
from torch.utils.data import DataLoader

# 加载MNIST数据集
mnist_dataset = datasets.MNIST(root='./data',
                               train=False,
                               transform=transforms.ToTensor(),
                               target_transform=None,
                               download=True)
# 取32张图片的tensor
tensor_dataloader = DataLoader(dataset=mnist_dataset,
                               batch_size=32)
data_iter = iter(tensor_dataloader)
img_tensor, label_tensor = data_iter.next()
print(img_tensor.shape)
'''
输出：torch.Size([32, 1, 28, 28])
'''
# 将32张图片拼接在一个网格中
grid_tensor = torchvision.utils.make_grid(img_tensor, nrow=8, padding=2)
grid_img = transforms.ToPILImage()(grid_tensor)
display(grid_img)
```
结合代码我们可以看到，程序首先利用torchvision.datasets加载 MNIST 的测试集，然后利用 DataLoader 类的迭代器一次获取到 32 张图片的 Tensor，最后利用 make_grid 函数将 32 张图片拼接在了一幅图片中。MNIST 的测试集中的 32 张图片，如下图所示，这里我要特别说明一下，因为 MNIST 的尺寸为 28x28，所以测试集里的手写数字图片像素都比较低，但这并不影响咱们动手实践。你可以参照我给到的示范，自己动手试试看。

![img](https://static001.geekbang.org/resource/image/bb/yy/bb73d6bdf49fc876d983cfa48569dcyy.png?wh=242x122)

save_img
一般来说，在保存模型输出的图片时，需要将 Tensor 类型的数据转化为图片类型才能进行保存，过程比较繁琐。Torchvision 提供了 save_image 函数，能够直接将 Tensor 保存为图片，即使 Tensor 数据在 CUDA 上，也会自动移到 CPU 中进行保存。save_image 函数的定义如下。

```
torchvision.utils.save_image(tensor, fp, **kwargs)
```
这些参数也很好理解：tensor：类型是 Tensor 或列表，如果输入类型是 Tensor，直接将 Tensor 保存；如果输入类型是列表，则先调用 make_grid 函数生成一张图片的 Tensor，然后再保存。fp：保存图片的文件名；**kwargs：make_grid 函数中的参数，前面已经讲过了。

我们接着上面的小例子，将 32 张图片的拼接图直接保存，代码如下。


```
# 输入为一张图片的tensor 直接保存
torchvision.utils.save_image(grid_tensor, 'grid.jpg')

# 输入为List 调用grid_img函数后保存
torchvision.utils.save_image(img_tensor, 'grid2.jpg', nrow=5, padding=2)
```
当输入为一张图片的 Tensor 时，直接保存，保存的图片如下所示。

![img](https://static001.geekbang.org/resource/image/bb/yy/bb73d6bdf49fc876d983cfa48569dcyy.png?wh=242x122)

当输入为 List 时，则会先调用 make_grid 函数，make_grid 函数的参数直接加在后面即可，代码中令 nrow=5，保存的图片如下所示。这时我们可以看到图片中，每行中有 5 个数字，最后一行不足的数字，已经自动填充了空图像。

![img](https://static001.geekbang.org/resource/image/21/a6/21435a2115ca4704bc51496f8a1c8da6.png?wh=152x212)
### 小结
恭喜你完成了这节课的学习。至此，Torchvision 的全部内容我们就学完了。今天的重点内容是torchvision.models模块的使用，包括如何实例化一个网络与如何进行模型的微调。torchvision.models模块为我们提供了深度学习中各种经典的网络结构以及训练好的模型，我们不仅可以实例化一个随机初始化的网络模型，还可以实例化一个预训练好的网络模型。模型微调可以让我们在自己的小数据集上快速训练模型，并取得比较理想的效果。但是我们需要根据具体问题对预训练模型或数据进行一些修改，你可以灵活调整输出类别的数量，或者调整输入图像的大小。除了模型微调，我还讲了两个 Torchvision 中有趣的函数，make_grid 和 save_img，我还结合之前我们学习过的读取数据集以及图像变换的内容，为你做了演示。相信 Torchvision 工具配合 PyTorch 使用，一定能够使你事半功倍。

## 09 | 卷积（上）：如何用卷积为计算机“开天眼”？

现在刷脸支付的场景越来越多，相信人脸识别你一定不陌生，你有没有想过，在计算机识别人脸之前，我们人类是如何判断一个人是谁的呢？我们眼睛看到人脸的时候，会先将人脸的一些粗粒度特征提取出来，例如人脸的轮廓、头发的颜色、头发长短等。然后这些信息会一层一层地传入到某一些神经元当中，每经过一层神经元就相当于特征提取。我们大脑最终会将最后的特征进行汇总，类似汇总成一张具体的人脸，用这张人脸去大脑的某一个地方与存好的人名进行匹配。那落实到我们计算机呢？其实这个过程是一样的，在计算机中进行特征提取的功能，就离不开我们今天要讲的卷积。可以说，没有卷积的话，深度学习在图像领域不可能取得今天的成就。 那么，就让我们来看看什么是卷积，还有它在 PyTorch 中的实现吧。


### 卷积
在使用卷积之前，人们尝试了很多人工神经网络来处理图像问题，但是人工神经网络的参数量非常大，从而导致非常难训练，所以计算机视觉的研究一直停滞不前，难以突破。直到卷积神经网络的出现，它的两个优秀特点：稀疏连接与平移不变性，这让计算机视觉的研究取得了长足的进步。什么是稀疏连接与平移不变性呢？简单来说，就是稀疏连接可以让学习的参数变得很少，而平移不变性则不关心物体出现在图像中什么位置。稀疏连接与平移不变性是卷积的两个重要特点，如果你想从事计算机视觉相关的工作，这两个特点必须该清楚，但不是本专栏的重点，这里就不展开了，有兴趣你可以自己去了解。下面我们直接来看看卷积是如何计算的。


#### 最简单的情况
我们先看最简单的情况，输入是一个 4x4 的特征图，卷积核的大小为 2x2。卷积核是什么呢？其实就是我们卷积层要学习到的参数，就像下图中红色的示例，下图中的卷积核是最简单的情况，只有一个通道。

![img](https://static001.geekbang.org/resource/image/ac/5d/ac84c162ee165d535fcbf465572faf5d.jpg?wh=1075x728)

输入特征与卷积核计算时，计算方式是卷积核与输入特征按位做乘积运算然后再求和，其结果为输出特征图的一个元素，下图为计算输出特征图第一个元素的计算方式：

![img](https://static001.geekbang.org/resource/image/78/20/787c8b346de00dayyd7e2d3504c33320.jpg?wh=1561x891)

完成了第一个元素的计算，我们接着往下看，按以从左向右，从上至下的顺序进行滑动卷积核，分别与输入的特征图进行计算，请看下图，下图为上图计算完毕之后，向右侧滑动一个单元的计算方式：![img](https://static001.geekbang.org/resource/image/5y/b5/5yy249d2f1221e21a1bdc7d8756f4fb5.jpg?wh=1544x862)



第一行第三个单元的计算以此类推。说完了同一行的移动，我们再看看，第一行计算完毕，向下滑动的计算方式是什么样的。

![img](https://static001.geekbang.org/resource/image/cf/bb/cf4aa3yy8ac31b06f153f9090d3bcebb.jpg?wh=1552x929)

第一行计算完毕之后，卷积核会回到行首，然后向下滑动一个单元，再重复以上从左至右的滑动计算。这里我再给你补充一个知识点，什么是步长？卷积上下左右滑动的长度，我们称为步长，用 stride 表示。上述例子中的步长就是 1，根据问题的不同，会取不同的步长，但通常来说步长为 1 或 2。不管是刚才说的最简单的卷积计算，还是我们后面要讲的标准卷积，都要用到这个参数。

#### 标准的卷积
好啦，前面只是最简单的情况，现在我们将最简单的卷积计算方式延伸到标准的卷积计算方式。我们先将上面的例子描述为更加通用的形式，输入的特征有 m 个通道，宽为 w，高为 h；输出有 n 个特征图，宽为 w′，高为 h′；卷积核的大小为 kxk。在刚才的例子中 m、n、k、w、h、w′、h′ 的值分别为 1、1、2、4、4、3、3。而现在，我们需要把一个输入为 (m，h，w) 的输入特征图经过卷积计算，生成一个输出为 (n, h′, w′) 的特征图。

那我们来看看可以获得这个操作的卷积是什么样子的。输出特征图的通道数由卷积核的个数决定的，所以说卷积核的个数为 n。根据卷积计算的定义，输入特征图有 m 个通道，所以每个卷积核里要也要有 m 个通道。所以，我们的需要 n 个卷积核，每个卷积核的大小为 (m, k, k)。为了帮你更好地理解刚才所讲的内容，我画了示意图，你可以对照一下：

![img](https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080)



结合上面的图解可以看到，卷积核 1 与全部输入特征进行卷积计算，就获得了输出特征图中第 1 个通道的数据，卷积核 2 与全部输入特征图进行计算获得输出特征图中第 2 个通道的数据。以此类推，最终就能计算 n 个输出特征图。在开篇的例子中，输入只有 1 个通道，现在有多个通道了，那我们该如何计算呢？其实计算方式类似，输入特征的每一个通道与卷积核中对应通道的数据按我们之前讲过的方式进行卷积计算，也就是输入特征图中第 i 个特征图与卷积核中的第 i 个通道的数据进行卷积。这样计算后会生成 m 个特征图，然后将这 m 个特征图按对应位置求和即可，求和后 m 个特征图合并为输出特征中一个通道的特征图。我们可以用后面的公式表示当输入有多个通道时，每个卷积核是如何与输入进行计算的。

Outputi 表示计算第 i 个输出特征图，i 的取值为 1 到 n；kernelk 表示 1 个卷积核里的第 k 个通道的数据；inputk 表示输入特征图中的第 k 个通道的数据；biask 为偏移项，我们在训练时一般都会默认加上；⋆ 为卷积计算；

$$
\text { Output }_{i}=\sum_{k=0}^{m} \text { kernel }_{k} \star \text { input }_{k}+\text { bias }_{i}, \quad i=1,2, \ldots, n
$$

我来解释一下为什么要加 bias。就跟回归方程一样，如果不加 bias 的话，回归方程为 y=wx 不管 w 如何变化，回归方程都必须经过原点。如果加上 bias 的话，回归方程变为 y=wx+b，这样就不是必须经过原点，可以变化的更加多样。好啦，卷积计算方式的讲解到这里就告一段落了。下面我们看看在卷积层中有关卷积计算的另外一个重要参数。

#### Padding
让我们回到开头的例子，可以发现，输入的尺寸是 4x4，输出的尺寸是 3x3。你有没有发现，输出的特征图变小了？没错，在有多层卷积层的神经网络中，特征图会越来越小。但是，有的时候我们为了让特征图变得不是那么小，可以对特征图进行补零操作。这样做主要有两个目的：


1. 有的时候需要输入与输出的特征图保持一样的大小；
2. 让输入的特征保留更多的信息。

这里我举个例子，带你看看，一般什么情况下会希望特征图变得不那么小。通过刚才的讲解我们知道，如果不补零且步长（stride）为 1 的情况下，当有多层卷积层时，特征图会一点点变小。如果我们希望有更多层卷积层来提取更加丰富的信息时，就可以让特征图变小的速度稍微慢一些，这个时候就可以考虑补零。这个补零的操作就叫做 padding，padding 等于 1 就是补一圈的零，等于 2 就是补两圈的零，如下图所示：

![img](https://static001.geekbang.org/resource/image/99/08/99dc22a96df665e93e881yy3cf358d08.jpg?wh=1520x792)



在 Pytorch 中，padding 这个参数可以是字符串、int 和 tuple。我们分别来看看不同参数类型怎么使用：当为字符串时只能取 ′valid′ 与 ′same′。当给定整型时，则是说要在特征图外边补多少圈 0。如果是 tuple 的时候，则是表示在特征图的行与列分别指定补多少零。我们重点看一下字符串的形式，相比于直接给定补多少零来说，我认为字符串更加常用。其中，′valid′ 就是没有 padding 操作，就像开头的例子那样。′same′ 则是让输出的特征图与输入的特征图获得相同的大小。那当 padding 为 same 时，到底是怎么计算的呢？我们继续用开篇的例子说明，现在 padding 为 ′same′ 了。

![img](https://static001.geekbang.org/resource/image/yy/9c/yy51a5c4a35ffa8a06e7d7415aba339c.jpg?wh=1417x736)

当滑动到特征图最右侧时，发现输出的特征图的宽与输入的特征图的宽不一致，它会自动补零，直到输出特征图的宽与输入特征图的宽一致为止。如下图所示：

![img](https://static001.geekbang.org/resource/image/52/c7/52f8d0ba39b49e9a2736c1a0afb38cc7.jpg?wh=1463x720)

高的计算和宽的计算同理，当计算到特征图的底部时，发现输出特征图的高与输入特征图的高不一致时，它同样会自动补零，直到输入和输出一致为止，如下图所示。

![img](https://static001.geekbang.org/resource/image/c4/81/c48614da6c7bbcd5abdaf942ea45b481.jpg?wh=1476x735)



完成上述操作，我们就可以获得与输入特征图有相同高、宽的输出特征图了。理论讲完了，我们还是要学以致用，在实践中深入体会。在下面的练习中，我们会实际考察一下当 padding 为 same 时，是否像我们说的这样计算。


### PyTorch 中的卷积
卷积操作定义在 torch.nn 模块中，torch.nn 模块为我们提供了很多构建网络的基础层与方法。在 torch.nn 模块中，关于今天介绍的卷积操作有 nn.Conv1d、nn.Conv2d 与 nn.Conv3d 三个类。请注意，我们上述的例子都是按照 nn.Conv2d 来介绍的，nn.Conv2d 也是用的最多的，而 nn.Conv1d 与 nn.Conv3d 只是输入特征图的维度有所不一样而已，很少会被用到。让我们先看看创建一个 nn.Conv2d 需要哪些必须的参数：

```
# Conv2d类
class torch.nn.Conv2d(in_channels, 
                      out_channels, 
                      kernel_size, 
                      stride=1, 
                      padding=0, 
                      dilation=1, 
                      groups=1, 
                      bias=True, 
                      padding_mode='zeros', 
                      device=None, 
                      dtype=None)

```
我们挨个说说这些参数。首先是跟通道相关的两个参数：in_channels 是指输入特征图的通道数，数据类型为 int，在标准卷积的讲解中 in_channels 为 m；out_channels 是输出特征图的通道数，数据类型为 int，在标准卷积的讲解中 out_channels 为 n。kernel_size 是卷积核的大小，数据类型为 int 或 tuple，需要注意的是只给定卷积核的高与宽即可，在标准卷积的讲解中 kernel_size 为 k。stride 为滑动的步长，数据类型为 int 或 tuple，默认是 1，在前面的例子中步长都为 1。


padding 为补零的方式，注意当 padding 为’valid’或’same’时，stride 必须为 1。对于 kernel_size、stride、padding 都可以是 tuple 类型，当为 tuple 类型时，第一个维度用于 height 的信息，第二个维度时用于 width 的信息。bias 是否使用偏移项。还有两个参数：dilation 与 groups，具体内容下节课我们继续展开讲解，你先有个印象就行。


#### 验证 same 方式
接下来，我们做一个练习，验证 padding 为 same 时，计算方式是否像我们所说的那样。过程并不复杂，一共三步，分别是创建输入特征图、设置卷积以及输出结果。先来看第一步，我们创建好例子中的（4，4，1）大小的输入特征图，代码如下：

```
import torch
import torch.nn as nn

input_feat = torch.tensor([[4, 1, 7, 5], [4, 4, 2, 5], [7, 7, 2, 4], [1, 0, 2, 4]], dtype=torch.float32)
print(input_feat)
print(input_feat.shape)

# 输出：
tensor([[4., 1., 7., 5.],
        [4., 4., 2., 5.],
        [7., 7., 2., 4.],
        [1., 0., 2., 4.]])
torch.Size([4, 4])
```
第二步，创建一个 2x2 的卷积，根据刚才的介绍，输入的通道数为 1，输出的通道数为 1，padding 为’same’，所以卷积定义为：
```
conv2d = nn.Conv2d(1, 1, (2, 2), stride=1, padding='same', bias=True)
# 默认情况随机初始化参数
print(conv2d.weight)
print(conv2d.bias)
# 输出：
Parameter containing:
tensor([[[[ 0.3235, -0.1593],
          [ 0.2548, -0.1363]]]], requires_grad=True)
Parameter containing:
tensor([0.4890], requires_grad=True)
```
需要注意的是，默认情况下是随机初始化的。一般情况下，我们不会人工强行干预卷积核的初始化，但是为了验证今天的例子，我们对卷积核的参数进行干预。请注意下面代码中卷积核的注释，代码如下：

```
conv2d = nn.Conv2d(1, 1, (2, 2), stride=1, padding='same', bias=False)
# 卷积核要有四个维度(输入通道数，输出通道数，高，宽)
kernels = torch.tensor([[[[1, 0], [2, 1]]]], dtype=torch.float32)
conv2d.weight = nn.Parameter(kernels, requires_grad=False)
print(conv2d.weight)
print(conv2d.bias)
# 输出：
Parameter containing:
tensor([[[[1., 0.],
          [2., 1.]]]])
None
```
完成之后就进入了第三步，现在我们已经准备好例子中的输入数据与卷积数据了，下面只需要计算一下，然后输出就可以了，代码如下：
```
output = conv2d(input_feat)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/var/folders/pz/z8t8232j1v17y01bkhyrl01w0000gn/T/ipykernel_29592/2273564149.py in <module>
----> 1 output = conv2d(input_feat)
~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []
~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/conv.py in forward(self, input)
    441 
    442     def forward(self, input: Tensor) -> Tensor:
--> 443         return self._conv_forward(input, self.weight, self.bias)
    444 
    445 class Conv3d(_ConvNd):
~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias)
    437                             weight, bias, self.stride,
    438                             _pair(0), self.dilation, self.groups)
--> 439         return F.conv2d(input, weight, bias, self.stride,
    440                         self.padding, self.dilation, self.groups)
    441 
RuntimeError: Expected 4-dimensional input for 4-dimensional weight[1, 1, 2, 2], but got 2-dimensional input of size [4, 4] instead
```
结合上面代码，你会发现这里报错了，提示信息是输入的特征图需要是一个 4 维的，而我们的输入特征图是一个 4x4 的 2 维特征图。这是为什么呢？**请你记住，Pytorch 输入 tensor 的维度信息是 (batch_size, 通道数，高，宽)**，但是在我们的例子中只给定了高与宽，没有给定 batch_size（在训练时，不会将所有数据一次性加载进来训练，而是以多个批次进行读取的，每次读取的量成为 batch_size）与通道数。所以，我们要回到第一步将输入的 tensor 改为 (1,1,4,4) 的形式。你还记得我在之前的讲解中提到过怎么对数组添加维度吗？在 Pytorch 中 unsqueeze() 对 tensor 的维度进行修改。代码如下：

```
input_feat = torch.tensor([[4, 1, 7, 5], [4, 4, 2, 5], [7, 7, 2, 4], [1, 0, 2, 4]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
print(input_feat)
print(input_feat.shape)
# 输出：
tensor([[[[4., 1., 7., 5.],
          [4., 4., 2., 5.],
          [7., 7., 2., 4.],
          [1., 0., 2., 4.]]]])
torch.Size([1, 1, 4, 4])
```
这里，unsqueeze() 中的参数是指在哪个位置添加维度。好，做完了修改，我们再次执行代码。

```
output = conv2d(input_feat)
输出：
tensor([[[[16., 11., 16., 15.],
          [25., 20., 10., 13.],
          [ 9.,  9., 10., 12.],
          [ 1.,  0.,  2.,  4.]]]])
```
你可以看看，跟我们在例子中推导的结果一不一样？


### 总结
恭喜你完成了今天的学习。今天所讲的卷积非常重要，它是各种计算机视觉应用的基础，例如图像分类、目标检测、图像分割等。卷积的计算方式是你需要关注的重点。具体过程如下图所示，输出特征图的通道数由卷积核的个数决定的，下图中因为有 n 个卷积核，所以输出特征图的通道数为 n。输入特征图有 m 个通道，所以每个卷积核里要也要有 m 个通道。

![img](https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080)

其实卷积背后的理论比较复杂，但在 PyTorch 中实现却很简单。在卷积计算中涉及的几大要素：输入通道数、输出通道数、步长、padding、卷积核的大小，分别对应的就是 PyTorch 中 nn.Conv2d 的关键参数。所以，就像前面讲的那样，我们要熟练用好 nn.Conv2d()。之后，我还带你做了一个验证 same 方式的练习，动手跑跑代码会帮你形成直观印象，快速掌握这部分内容。当然，对于卷积来说不光光有今天介绍的这种比较标准的卷积，还有各种变形。例如，今天没有讲到的 dilation 参数与 groups 参数，基于这两个参数实现的卷积操作，我会在下一节课中为展开，敬请期待。


## 10 | 卷积（下）：如何用卷积为计算机“开天眼”？

经过上一节课的学习，相信你已经对标准的卷积计算有所了解。虽然标准卷积基本上可以作为主力 Carry 全场，但是人们还是基于标准卷积，提出了一些其它的卷积方式，这些卷积方式在应对不同问题时能够发挥不同的作用，这里我为你列举了一些。

![img](https://static001.geekbang.org/resource/image/8a/4b/8a3f6bba138f36c1b31e52aeb3e2604b.jpg?wh=1920x977)

在上一节课中，我们学习了 conv2d 的 in_channels、out_channels、kernel_size、stride、padding 与 bias 参数。其中，PyTorch 中 conv2d 中剩余的两个参数，它们分别对应着两种不同的卷积，分别是深度可分离卷积和空洞卷积，让我们一起来看看。

### 深度可分离卷积（Depthwise Separable Convolution）
我们首先看看依托 groups 参数实现的深度可分离卷积。随着深度学习技术的不断发展，许多很深、很宽的网络模型被提出，例如，VGG、ResNet、SENet、DenseNet 等，这些网络利用其复杂的结构，可以更加精确地提取出有用的信息。同时也伴随着硬件算力的不断增强，可以将这些复杂的模型直接部署在服务器端，在工业中可以落地的项目中都取得了非常优秀的效果。但这些模型具有一个通病，就是速度较慢、参数量大，这两个问题使得这些模型无法被直接部署到移动终端上。而移动端的各种应用无疑是当今最火热的一个市场，这种情况下这些深而宽的复杂网络模型就不适用了。


因此，很多研究将目光投入到寻求更加轻量化的模型当中，这些轻量化模型的要求是速度快、体积小，精度上允许比服务器端的模型稍微降低一些。深度可分离卷积就是谷歌在 MobileNet v1 中提出的一种轻量化卷积。**简单来说，深度可分离卷积就是我们刚才所说的在效果近似相同的情况下，需要的计算量更少**。接下来，我们先来看看深度可分离卷积是如何计算的，然后再对比一下计算量到底减少了多少。


深度可分离卷积（Depthwise Separable Convolution）由 Depthwise（DW）和 Pointwise（PW）这两部分卷积组合而成的。我们先来复习一下标准卷积，然后再来讲解一下获得同样输出特征图的深度可分离卷积是如何工作的。你还记得下面这张图吗？这是我们上节课讲到的标准卷积计算方式，它描述的是：输入 m 个尺寸为 h, w 的特征图，通过卷积计算获得 n 个通道尺寸为 h′ 与 w′ 的特征图的计算过程。

![img](https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080)


我们将特征图与一个卷积核计算的过程展开一下，请看下图。一个卷积核中的 m 个卷积分别与输入特征图的 m 个通道数据进行卷积计算，生成一个中间结果，然后 m 个中间结果按位求和，最终就能获得 n 个输出特征图中的一个特征图。

![img](https://static001.geekbang.org/resource/image/6a/3d/6abd2bd18e767b7f0f01cab2fdc7023d.jpeg?wh=1920x1080)


### Depthwise（DW）卷积
那什么是 DW 卷积呢？DW 卷积就是有 m 个卷积核的卷积，每个卷积核中的通道数为 1，这 m 个卷积核分别与输入特征图对应的通道数据做卷积运算，所以 DW 卷积的输出是有 m 个通道的特征图。通常来说，DW 卷积核的大小是 3x3 的。DW 卷积的过程如下图所示：

![img](https://static001.geekbang.org/resource/image/6b/e9/6baeb5b36cea5c04555ea6186de577e9.jpeg?wh=1920x1080)



### Pointwise（PW）卷积
通常来说，深度可分离卷积的目标是轻量化标准卷积计算的，所以它是可以来替换标准卷积的，这也意味着原卷积计算的输出尺寸是什么样，替换后的输出尺寸也要保持一致。所以，在深度可分离卷积中，我们最终要获得一个具有 n 个通道的输出特征图，而刚才介绍的 DW 卷积显然没有达到，并且 DW 卷积也忽略了输入特征图通道之间的信息。所以，在 DW 之后我们还要加一个 PW 卷积。PW 卷积也叫做逐点卷积。PW 卷积的主要作用就是将 DW 输出的 m 个特征图结合在一起考虑，再输出一个具有 n 个通道的特征图。在卷积神经网络中，我们经常可以看到使用 1x1 的卷积，1x1 的卷积主要作用就是升维与降维。所以，在 DW 的输出之后的 PW 卷积，就是 n 个卷积核的 1x1 的卷积，每个卷积核中有 m 个通道的卷积数据。为了帮你理解刚才我描述的这个过程，我还是用图解的方式为你描述一下，你可以对照下图看一看：

![img](https://static001.geekbang.org/resource/image/7e/17/7e2c1a874d7b467bc96d4243813f3017.jpeg?wh=1920x1080)

经过这样的 DW 与 PW 的组合，我们就可以获得一个与标准卷积有同样输出尺寸的轻量化卷积啦。既然是轻量化，那么我们下面就来看看，深度可分离卷积的计算量相对于标准卷积减少了多少呢？

#### 计算量
我们的原问题是有 m 个通道的输入特征图，卷积核尺寸为 kxk，输出特征图的尺寸为 (n,h′,w′)，那么标准的卷积的计算量为：

$$
k \times k \times m \times n \times h^{\prime} \times w^{\prime}
$$

我们是怎么得出这个结果的呢？你可以从输出特征图往回思考。

![img](https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080)

上图输出特征图中每个点的数值是由 n 个卷积核与输入特征图计算出来的吧，这个计算量是 k×k×m×n，那输出特征图有多少个点？没错，一共有 h′×w′ 个。所以，我们自然就得出上面的计算方式了。如果采用深度可分离卷积，DW 的计算量为：k×k×m×h′×w′，而 PW 的计算量为：1×1×m×n×h′×w′。我们不难得出标准卷积与深度可分离卷积计算量的比值为：

$$
\begin{gathered}
\frac{k \times k \times m \times h^{\prime} \times w^{\prime}+1 \times 1 \times m \times n \times h^{\prime} \times w^{\prime}}{k \times k \times m \times n \times h^{\prime} \times w^{\prime}} \\
=\frac{1}{n}+\frac{1}{k \times k}
\end{gathered}
$$

所以，深度可分离卷积的计算量大约为普通卷积计算量的 $1/k^2$​。那深度可分离卷积落实到 PyTorch 中是怎么实现的呢？

#### PyTorch 中的实现
在 PyTorch 中实现深度可分离卷积的话，我们需要分别实现 DW 与 PW 两个卷积。我们先看看 DW 卷积，实现 DW 卷积的话，就会用到 nn.Conv2d 中的 groups 参数。groups 参数的作用就是控制输入特征图与输出特征图的分组情况。当 groups 等于 1 的时候，就是我们上一节课讲的标准卷积，而 groups=1 也是 nn.Conv2d 的默认值。当 groups 不等于 1 的时候，会将输入特征图分成 groups 个组，每个组都有自己对应的卷积核，然后分组卷积，获得的输出特征图也是有 groups 个分组的。需要注意的是，groups 不为 1 的时候，groups 必须能整除 in_channels 和 out_channels。

当 groups 等于 in_channels 时，就是我们的 DW 卷积啦。好，下面我们一起动手操作一下，看看如何实现一个 DW 卷积。首先我们来生成一个三通道的 5x5 输入特征图，然后经过深度可分离卷积，输出一个 4 通道的特征图。DW 卷积的实现代码如下：

```
import torch
import torch.nn as nn

# 生成一个三通道的5x5特征图
x = torch.rand((3, 5, 5)).unsqueeze(0)
print(x.shape)
# 输出：
torch.Size([1, 3, 5, 5])
# 请注意DW中，输入特征通道数与输出通道数是一样的
in_channels_dw = x.shape[1]
out_channels_dw = x.shape[1]
# 一般来讲DW卷积的kernel size为3
kernel_size = 3
stride = 1
# DW卷积groups参数与输入通道数一样
dw = nn.Conv2d(in_channels_dw, out_channels_dw, kernel_size, stride, groups=in_channels_dw)
```
你需要注意以下几点内容：1.DW 中，输入特征通道数与输出通道数是一样的；2. 一般来讲，DW 的卷积核为 3x3；3.DW 卷积的 groups 参数与输出通道数是一样的。

好啦，DW 如何实现我们已经写好了，接下来就是 PW 卷积的实现。其实 PW 卷积的实现就是我们上一节课介绍的标准卷积，只不过卷积核为 1x1。需要注意的是，PW 卷积的 groups 就是默认值了。具体代码如下所示：

```
in_channels_pw = out_channels_dw
out_channels_pw = 4
kernel_size_pw = 1
pw = nn.Conv2d(in_channels_pw, out_channels_pw, kernel_size_pw, stride)
out = pw(dw(x))
print(out.shape)
```
好了，groups 以及深度可分离卷积就讲完了，接下来我们看看最后一个 dilation 参数，它是用来实现空洞卷积的。


### 空洞卷积
空洞卷积经常用于图像分割任务当中。图像分割任务的目的是要做到 pixel-wise 的输出，也就是说，对于图片中的每一个像素点，模型都要进行预测。对于一个图像分割模型，通常会采用多层卷积来提取特征的，随着层数的不断加深，感受野也越来越大。这里有个新名词——“感受野”，这个我稍后再解释。我们先把空洞卷积的作用说完。但是对于图像分割模型有个问题，经过多层的卷积与 pooling 操作之后，特征图会变小。为了做到每个像素点都有预测输出，我们需要对较小的特征图进行上采样或反卷积，将特征图扩大到一定尺度，然后再进行预测。要知道，从一个较小的特征图恢复到一个较大的特征图，这显然会带来一定的信息损失，特别是较小的物体，这种损失是很难恢复的。那问题来了，能不能既保证有比较大的感受野，同时又不用缩小特征图呢？估计你已经猜到了，空洞卷积就是解决这个问题的杀手锏，它最大的优点就是不需要缩小特征图，也可以获得更大的感受野。


#### 感受野
现在让我来解释一下什么是感受野。感受野是计算机视觉领域中经常会看到的一个概念。因为伴随着不断的 pooling（这是卷积神经网络中的一种操作，通常是在一定区域的特征图内取最大值或平均值，用最大值或平均值代替这个区域的所有数据，pooling 操作会使特征图变小）或者卷积操作，在卷积神经网络中不同层的特征图是越来越小的。这就意味着在卷积神经网络中，相对于原图来说，不同层的特征图，其计算区域是不一样的，这个区域就是感受野。感受野越大，代表着包含的信息更加全面、语义信息更加抽象，而感受野越小，则代表着包含更加细节的语义信息。光说理论不容易理解，我们还是结合例子看一看。请看下图，原图是 6x6 的图像，第一层卷积层为 3x3，这时它输出的感受野就是 3，因为输出的特征图中每个值都是由原图中 3x3 个区域计算而来的。

![img](https://static001.geekbang.org/resource/image/ed/f1/edb2bd243872364fe032yy7fb7999bf1.jpg?wh=1920x886)

再看下图，卷积层 2 也为 3x3 的卷积，输出为 2x2 的特征图。这时卷积层 2 的感受野就会变为 5（输入特征图中蓝色加橘黄色部分）。

![img](https://static001.geekbang.org/resource/image/2f/yy/2f0e685e8894db8dd8919dee8400e6yy.jpg?wh=1920x647)



配合图解，我相信你很容易就能明白感受野的含义了。

#### 计算方式
好，那么我们再来看看空洞卷积具体是如何计算的。用语言来描述空洞卷积的计算方式比较抽象，我们不妨看一下它的动态示意图（这个[GitHub](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)中有各种卷积计算的动态图，非常直观，我们借助它来学习一下空洞卷积）。首先，我们先来看看上节课讲的标准卷积是如何计算的。

![img](https://static001.geekbang.org/resource/image/77/63/77643b049ac3cd241980b151e0f32063.gif?wh=395x449)


对照上图，下面的蓝图为输入特征图，滑动的阴影为卷积核，绿色的为输出特征图。然后我们再对照一下的空洞卷积示意图。

![img](https://static001.geekbang.org/resource/image/49/53/4959201e816888c6648f2e78cccfd253.gif?wh=395x381)



结合示意图我们会发现，计算方式与普通卷积一样，只不过是将卷积核以一定比例拆分开来。实现起来呢，就是用 0 来充填卷积核。这个分开的比例，我们一般称之为扩张率，就是 Conv2d 中的 dilation 参数。dilation 参数默认为 1，同样也是可以为 int 或者 tuple。当为 tuple 时，第一位代表行的信息，第二位代表列的信息。

### 总结
恭喜你，完成了今天的学习。今天我们在实现 PyTorch 卷积操作的同时，学习了两个特殊的卷积，深度可分离卷积与空洞卷积。对于空洞卷积，你最需要掌握的是感受野这个概念，以及空洞卷积的计算方式。感受野就是能在原始图像中反应的区域。深度可分离卷积主要用于轻量化的模型，而空洞卷积主要用于图像分割任务中。这里分享一下我的经验：如果说你需要轻量化你的模型，让你的模型变得更小、更快，你可以考虑将卷积层替换为深度可分离卷积。如果你在做图像分割项目的话，可以考虑将网络靠后的层替换为空洞卷积，看看效果是否能有所提高。最后，我们再来总结一下 PyTorch 中卷积操作的各个重要参数，我用表格的方式帮你做了总结归纳，你可以把它作为自己的工具包，时常翻看。

![img](https://static001.geekbang.org/resource/image/c4/bf/c4e0f1fbf77dd63ccb2ab36ce9e0fdbf.jpg?wh=1920x1002)


## 11 | 损失函数：如何帮助模型学会“自省”？

在前面的课程中，我们一同拿下了深度学习实战所需的预备基础知识，包括 PyTorch 的基础操作、NumPy、Tensor 的特性跟使用方法等，还一起学习了基于 Torchvision 的数据相关操作与特性。恭喜你走到这里，基础打好以后，我们距离实战关卡又进了一步。有了基础预备知识，我们就要开始学习深度学习的几个重要的概念了。一个深度学习项目包括了模型的设计、损失函数的设计、梯度更新的方法、模型的保存与加载、模型的训练过程等几个主要模块。每个模块都在整个深度学习项目搭建中意义重大，我特意为你画了一个示意图，方便你整体把握它们的功能。

![img](https://static001.geekbang.org/resource/image/d7/fc/d76e19dd8d8a5a1bfdb4f4b1a17078fc.jpg?wh=1896x910)


这节课咱们先从损失函数开始说起。损失函数是一把衡量模型学习效果的尺子，甚至可以说，训练模型的过程，实际就是优化损失函数的过程。如果你去面试机器学习岗位，常常会被问到前向传播、神经网络等内容，其实这些知识的考察都不可避免地会涉及到损失函数的相关概念。今天，我就从识别劳斯莱斯这个例子，带你了解损失函数的工作原理和常见类型。


### 一个简单的例子
回想一下我们学习新知识的大致过程，比如现在让你背一个单词，我举一个夸张的例子：Pneumonoultramicroscopicsilicovolcanoconiosis（矽肺病）。为了背会这个单词，你要反复地去看去记，第一次可能记住了开头的几个字母，第二次又记住了中间的几个字母，第三次又记住了结尾的几个字母，然后不断地反复学习，才能掌握这个单词的准确组成。为了检验你的学习成果，老师还会让你默写单词，跟标准拼写进行对照。刚才的例子用的是自然语言，那么如果视觉问题呢？比如我现在给你一个劳斯莱斯汽车的照片，让你记住，这就是这辈子都买不起的劳斯莱斯。

![img](https://static001.geekbang.org/resource/image/6c/19/6cf904ede85961a462c90cb555545e19.png?wh=1080x802)

你会怎么去记住它呢？对，你会下意识去寻找最具有代表性的内容，比如车前脸的方形格栅、车前面的立起来的小金人，方方正正的车体等。等你以后见到了有了具有以上特征的汽车，你就知道，它是你要躲远点的劳斯莱斯了。不过呢，如果这些特征发生了变化，你又要犹豫或者怀疑它是不是别的品牌的汽车了。其实，模型的学习也是一样的，模型最开始的时候就是一张白纸，它什么都不知道。我们作为研发人员，就要不断地给模型提供要学习的数据。模型拿到数据之后就要有一个非常重要的环节：把模型自己的判断结果和数据真实的情况做比较。如果偏差或者差异特别大，那么模型就要去纠正自己的判断，用某种方式去减少这种偏差，然后反复这个过程，直到最后模型能够对数据进行正确的判断。衡量这种偏差的方式很重要，也是模型学习进步的关键所在。这种减少偏差的过程，我们称之为拟合。接下来我们一同看看拟合的几种情况。


### 过拟合与欠拟合
我们先来学习第一组概念，也就是过拟合和欠拟合。为了方便你理解，我们结合函数曲线的例子来看看。首先假设在一个二维坐标系中有若干个点，我们需要让一个函数（模型）通过学习去尽可能地拟合这些点。那么拟合的结果都有哪几种可能呢？我们看看下面的图片：

![img](https://static001.geekbang.org/resource/image/9e/c3/9e8d67c31fa00051c29a369f4135bdc3.jpg?wh=1068x636)



在第一张图中，蓝色的曲线是我们学习到的第一个模型函数（H1）。我们发现，H1 好像没有很好地学习到这些点的拟合，或者说，函数跟样本点的拟合效果较差，只有一个大致符合的趋势。这种情况，我们称之为“欠拟合”。

![img](https://static001.geekbang.org/resource/image/d7/e2/d70c67323bc970e8d52610e02fdddee2.jpg?wh=1044x640)



既然有“欠”就有“过”，我们继续看第二张图。在这张图中，红色的曲线是我们学习到的第二个模型函数（H2），在这个结果上，我们看到函数曲线可以很好地拟合所有的点。但是，这里存在两个问题：第一，曲线对应的函数有点太过复杂了，不像 H1 那样简单明了；第二，如果我们在 H2 的曲线附近再增加一个点，这条 H2 对应的曲线就很难去拟合好。这种情况就叫做“过拟合”，实在是太过了。

![img](https://static001.geekbang.org/resource/image/d8/e9/d8c507e14c00e5d7c399a7063b3642e9.jpg?wh=1082x626)



那么我们再来看第三张图，这张图的曲线就比较靠谱了，这个函数不是太复杂，同时也能较好拟合绝大部分的点。看到这里，你可能会有疑惑，为什么我们会如此的在意“复杂”这个问题呢？其实你可以这样想，有这样两个函数：
$$
y 1=3 x^{2}+2, y 2=3 x^{7}+7 x^{6}+6 x^{2}+4 x+18
$$
y1 无论是从可解释性上，还是在简洁程度、计算量方面，都要比 y2 好得多。越复杂的函数，在实际工作中就需要越多的计算资源和时间消耗。当然了，我们也不能一味的追求简单，否则就会欠拟合。


### 损失函数与代价函数
过拟合和欠拟合的概念实际上就是模型的表现效果。接下来，我们再来看看损失函数和代价函数，这组概念就是我们刚才说用来衡量“偏差”、“效果”的方法。我们还是延续之前的思路，用函数举例子。假设刚才的二维空间中，任意一个点对应的真实函数为 F(x)。我们通过模型的学习拟合出来的函数为 f(x)。根据刚才提到的学习过程，我们会知道 F(x) 和 f(x) 之间存在一个误差，我们定义为 L(x)，于是有：
$$
L(x)=(F(x)−f(x))^2
$$
这里 F(x) 和 f(x) 的差距我们做了一个平方和，是为了保证两者的误差是一个正值，方便后续的计算。当然，你也可以做成绝对值的形式，后面课程里我们还会讲到梯度更新，那时你就会发现，平方和要比绝对值更为方便。这里你先有个印象就好，让我们言归正传。有了 L(x)，我们就有了一个评价拟合函数表现效果“好坏”的度量指标，这个指标函数我们称作损失函数（loss fuction)。根据公式可知，损失函数越小，拟合函数对于真实情况的拟合效果就越好。这里有一点需要你注意，损失函数的种类有很多种，L(x) 只是我们学习到的第一个损失函数。接下来，我们将数据从刚才的任意一个点，扩大到所有的点，那么这些点实际上就是一个训练集合。把集合所有的点对应的拟合误差做平均，就会得到如下公式：

$$
\frac{1}{N} \sum_{i=0}^{N}(F(x)-f(x))^{2}
$$

这个函数叫做代价函数（cost function），即在训练样本集合上，所有样本的拟合误差的平均值。代价函数我们也称作经验风险。其实，在实际的应用中，我们并不会严格区分损失函数和代价函数。你只需要知道，损失函数是单个样本点的误差，代价函数是所有样本点的误差。明白了这些，你哪怕混着叫，也没什么问题。

### 常见损失函数
在了解了损失函数的定义之后，我们来看一下常用的损失函数都有哪些。其实，严格来说，损失函数的种类是无穷多的。这是因为损失函数是用来度量模型拟合效果和真实值之间的差距，而度量方式要根据问题的特点或者需要优化的方面具体定制，所以损失函数的种类是无穷无尽的。作为初学者，我推荐你从一些常用的损失函数做开始学习。今天我们一块来看看 5 种最基本的损失函数。

#### 0-1 损失函数
假定我们要一个判断类型的问题，比如让模型判断用户输入的文字是不是数字。那么模型判断的结果只有两种：是和不是。于是，我们很容易就会想到一个最为简单的评估方式：如果模型预测对了，损失函数的值就为 0，因为没有误差；如果模型预测错了，那么损失函数的值就为 1。这就是最简单的 0-1 损失函数，这个函数的公式表示如下：

$$
L(F(x), f(x))= \begin{cases}0 & \text { if } F(x) \neq f(x) \\ 1 & \text { if } F(x)=f(x)\end{cases}
$$

![img](https://static001.geekbang.org/resource/image/80/67/805e3ce996e95132392643d1b6140a67.jpg?wh=2958x654)


其中，F(x) 是输入数据的真实类别，f(x) 是模型预测的类别。是不是很简单？但是，0-1 损失函数的使用频率是非常少的，这是为什么呢？因为模型训练中经常用到的梯度更新和反向传播都需要能够求导的损失函数，可是 0-1 损失函数的导数值是 0（常数的导数为 0），所以它应用不多。尽管如此，我们也一定要了解 0-1 损失函数，因为它是最简单的损失函数，有着很重要的意义。



#### 平方损失函数
前面讲损失函数的定义时，我们曾举了一个例子 L(x)=(F(x)−f(x))2，这个函数的正式名称叫做平方损失函数。有时候，我们会在损失函数中加入一个 1/2 的系数，这是为了求导的时候能够跟平方项的系数约掉。平方损失函数是可求导的损失函数中最简单的一种，它直接度量了模型拟合结果和真实结果之间的距离。在实际项目中，很多简单的问题，比如手写分类、花卉识别等，都可以使用这种简单的损失函数。


#### 均方差损失函数和平均绝对误差损失函数
在正式讲解均方差损失函数之前，我们先补充一个重要的背景知识：机器学习分为有监督学习和无监督学习两大类。其中有监督学习是从标签化训练数据集中，推断出函数的机器学习任务，也就是说：模型通过标注好的数据，就像一个学生（模型）一样，被老师（数据）“指导”和“监督”着去学习。有监督学习问题主要可以划分为两类，分类和回归。其中回归问题是根据数据预测一个数值。而均方误差（Mean Squared Error，MSE）是回归问题损失函数中最常用的一个，也称作 L2 损失函数。它是预测值与目标值之间差值的平方和。它的定义如下：

$$
M S E=\frac{\sum_{i=1}^{n}\left(s_{i}-y_{i}^{p}\right)^{2}}{n}
$$

其中 s 为目标值的向量表示，y 为预测值的向量表示。细心的你会发现，平方损失函数好像也是差不多一个样子呀？没错，这两种形式本质上是等价的。只是 MSE 计算得到的值是把整个样本的误差做了平均，也就是加起来之后除了一个 n。误差平方和以及均方差的公式中有系数 1/2，这是为了求导后，系数被约去。而平均绝对误差损失函数（Mean Absolute Error, MAE）是另一种常用于回归问题的损失函数，它的目标是度量真实值和预测值差异的绝对值之和，定义如下：

$$
M A E=\frac{\sum_{i=1}^{n}\left|y_{i}-y_{i}^{p}\right|}{n}
$$

#### 交叉熵损失函数
接下来，我们再了解一下交叉熵损失函数。熵这个概念有的小伙伴可能有些陌生，跟刚才一样，让我们先来简单了解一下什么是熵。熵最开始是物理学中的一个术语，它表示了一个系统的混乱程度或者说无序程度。如果一个系统越混乱，那么它的熵越大。后来，信息论创始人香农把这个概念引申到信道通信的过程中，开创了信息论，所以这里的熵又称为信息熵。信息熵的公式化可以表示为：
$$
H \S=-\sum_{i} p\left(x_{i}\right) \log p\left(x_{i}\right)
$$
其中，x 表示随机变量，与之相对应的是所有可能输出的集合。P(x) 表示输出概率函数。变量的不确定性越大，熵也就越大，把变量搞清楚所需要的信息量也就越大。当我们将函数变为如下格式，将 log p 改为 log q，即：

$$
-\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right)
$$
其中，𝑝(𝑥) 表示真实概率分布，𝑞(𝑥) 表示预测概率分布。这个函数就是交叉熵损失函数（Cross entropy loss）。也就意味着，这个公式同时衡量了真实概率分布和预测概率分布两方面。所以，这个函数实际上就是通过衡量并不断去尝试缩小两个概率分布的误差，使预测的概率分布尽可能达到真实概率分布。


#### softmax 损失函数
softmax 是深度学习中使用非常频繁的一个函数。在某些场景下，一些数值大小范围分布非常广，而为了方便计算，或者使梯度更好的更新（后续我们还会学习梯度更新），我们需要把输入的这些数值映射为 0-1 之间的实数，并且归一化后能够保证几个数的和为 1。它的公式化表示为：

$$
S_{j}=\frac{e^{a_{j}}}{\sum_{k=1}^{T} e^{a_{k}}}
$$
回到刚才的交叉熵损失函数，公式中的 q(xi)，也就是预测的概率分布，如果我们换成 softmax 方式的表示，即：

$$
\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(S_{i}\right)
$$
之后我们就得到了一个成为 softmax 损失函数（softmax loss）的新函数，也称为 softmax with cross-entropy loss，它是交叉熵损失函数的一个特例。损失函数的种类非常多，这里我选择了最常用的几种。咱们在后续的实战环节，将会遇到更多的损失函数，到时候我再为你详细展开。


### 小结
这节课我们一同学习了损失函数的原理。对于模型来说，损失函数就是一个衡量其效果表现的尺子，有了这把尺子，模型就知道了自己在学习过程中是否有偏差，以及偏差到底有多大，从而做到“三省吾身”。今天所讲的公式虽然数量不少，但并不需要你背下来。我想提醒你的是，这些公式有必要先过一遍，有了基本的理解，才能知道原理。否则，没有这些公式做基础，后面你根本无法区分不同的损失函数。在实际的研发中，损失函数的设定是非常重要的，其地位甚至比得上模型网络设计。因为如果没有好的损失函数做指导的话，一切的功夫都白做了。就比如我们做最简单的手写体识别，损失函数每次计算模型和真实值的区别，通过这个损失函数，我们的模型才能知道自己学对了还是学错了，才能真正的有效学习。后面咱们就要开始学习如何通过损失函数来更新模型参数的方法了，这也是非常有意思的一个话题，敬请期待。


## 12 | 计算梯度：网络的前向与反向传播

在上节课，我们一同学习了损失函数的概念以及一些常用的损失函数。你还记得我们当时说的么：模型有了损失函数，才能够进行学习。那么问题来了，模型是如何通过损失函数进行学习的呢？在接下来的两节课中，我们将会学习前馈网络、导数与链式法则、反向传播、优化方法等内容，掌握了这些内容，我们就可以将模型学习的过程串起来作为一个整体，彻底搞清楚怎样通过损失函数训练模型。下面我们先来看看最简单的前馈网络。


### 前馈网络
前馈网络，也称为前馈神经网络。顾名思义，是一种“往前走”的神经网络。它是最简单的神经网络，其典型特征是一个单向的多层结构。简化的结构如下图：

![img](https://static001.geekbang.org/resource/image/2c/27/2c89c49e8a4d46724113874c2e8d8f27.jpg?wh=1241x790)


结合上面的示意图，我带你具体看看前馈网络的结构。这个图中，你会看到最左侧的绿色的一个个神经元，它们相当于第 0 层，一般适用于接收输入数据的层，所以我们把它们叫做输入层。比如我们要训练一个 y=f(x) 函数的神经网络，x 作为一个向量，就需要通过这个绿色的输入层进入模型。那么在这个网络中，输入层有 5 个神经元，这意味着它可以接收一个 5 维长度的向量。结合图解，我们继续往下看，网络的中间有一层红色的神经元，它们相当于模型的“内部”，一般来说对外不可见，或者使用者并不关心的非结果部分，我们称之为隐藏层。在实际的网络模型中，隐藏层会有非常多的层数，它们是网络最为关键的内部核心，也是模型能够学习知识的关键部分。在图的右侧，蓝色的神经元是网络的最后一层。模型内部计算完成之后，就需要通过这一层输出到外部，所以也叫做输出层。

需要说明的是，神经元之间的连线，表示神经元之间连接的权重，通过权重就会知道网络中每个节点的重要程度。那么现在我们回头再来看看前馈神经网络这个名字，是不是就很好理解了。在前馈网络中，数据从输入层进入到隐藏层的第一层，然后传播到第二层，第三层……一直到最后通过输出层输出。数据的传播是单向的，无法后退，只能前行。



### 导数、梯度与链式法
则既然有了前向的数据传播，自然也会有反向的数据传播过程。说到反向传播，我们常常还会把梯度下降、链式法则这些词挂在嘴边。不过初次接触的话，这些生词你直接搜定义，常常还是一头雾水。其实并不是这些概念很复杂，而是你的学习路径有问题。所以，接下来我会带你重温高数学过的导数、偏导数，搞懂这些前置知识，你就能对反向传播所需的知识做一个回顾，也能更好地理解反向传播的原理。


#### 导数
导数，也叫做导函数值。还记得高中数学我们曾学习过的斜率么？例如一个函数 $F=2x^2$，它的导数 F’=4x。其实斜率就是一种特殊情况下的导数。

更普遍的情况也很容易推导，我们以 F=3x 为例，在 x=3 的时候，函数值为 3x=3*3=9。现在我们给 x 一个非常小的增量Δx，那么就有了 F(x+Δx)=3(x+Δx)，也就是说函数值也有了一个非常小的增量，我们记为Δy。当函数值增量Δy 与变量 x 的增量Δx 的比值，在Δx 趋近于 0 时，如果极限 a 存在，我们就称 a 为函数 F(x) 在 x 处的导数。

![img](https://static001.geekbang.org/resource/image/f1/a7/f1224d68a66eba8503f82b165aa117a7.jpg?wh=1251x979)

需要注意的是，Δx 一定要趋近于 0，而且极限 a 是要存在的。不过在这节课里，极限的定义以及如何去判断极限并非是核心内容，感兴趣的小伙伴有空可以自己查阅相关的内容。对照下面的公式，你会对导数的理解更加清晰，高中数学的斜率其实就是一种特殊的导数。导数我们一般采用如下的方式做描述：

$$
f^{\prime}\left(x_{0}\right)=\lim _{\Delta x \rightarrow 0} \frac{\Delta y}{\Delta x}=\lim _{\Delta x \rightarrow 0} \frac{f\left(x_{0}+\Delta x\right)-f\left(x_{0}\right)}{\Delta x}
$$

这里面 lim 就是极限的意思。另外，函数 y 关于 x 的导数也可以记为 ∂x/∂y​。



#### 偏导数
细心的小伙伴看到这里可能就会有疑问了，有的函数不止一个变量呀，比如 z=3x+2y，这个函数中就同时存在了 x 和 y 两种变量，那该怎么求它们的导数呢？别着急，这时我们就要让偏导数登场了。偏导数其实就是保持一个变量变化，而所有其他变量恒定不变的求导过程。

还是刚才的原理，假设有个函数 z=f(x,y)，当我们要求 x 方向的导数的时候，就可以给 x 一个非常小的增量Δx，同时保持 y 不变。反之，如果要求 y 方向的导数，则需要给 y 一个非常小的增量Δy，而 x 保持不变。于是就能得出如下的偏导数描述公式：

$$
\frac{\partial}{\partial x_{j}} f\left(x_{0}, x_{1}, \ldots, x_{n}\right)=\lim _{\Delta x \rightarrow 0} \frac{\Delta y}{\Delta x}=
\lim _{\Delta x \rightarrow 0} \frac{f\left(x_{0}, \ldots, x_{j}+\Delta x, \ldots, x_{n}\right)-f\left(x_{0}, \ldots, x_{j}, \ldots, x_{n}\right)}{\Delta x}
$$

上面的公式，看上去很复杂，其实仔细看，你就会发现只有 xj​ 这个变量有一个小小的Δx，也就是说在 x 的某一个维度 (j) 增加了一个小的增量。


我们举个具体的例子来加深理解。比如对于函数 $z=x^2+y^2$，∂x/∂z​=2x 表示函数 z 在 x 上的导数，∂y/∂z​=2y 表示函数 z 在 y 上的导数。


#### 梯度
当我们了解了导数和偏导数的概念之后，那么梯度的概念就会非常容易理解了。函数所有偏导数构成的向量就叫做梯度。是不是非常简单呢？我们一般使用 ∇f 来表述函数的梯度。它的描述公式为：

$$
\nabla f(x)=\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \ldots, \frac{\partial f}{\partial x_{i}}\right]
$$

关于梯度，后面这个结论你一定要牢记：**梯度向量的方向即为函数值增长最快的方向**。这是一个非常重要的结论，它贯穿了整个深度学习的全过程。模型要学习知识，就要用最快最好的方式来完成，其实就是需要借助梯度来进行。不过，这个结论涉及的证明过程以及数学知识点非常多，这里你只需要记住结论就够了。



#### 链式法则
深度学习的整个学习过程，其实就是一个更新网络节点之前权重的过程。这个权重就是刚才咱们在前馈网络中示意图中看到的节点之间的连线，权重我们一般使用 w 来进行表示。回忆一下上节课我们提到的损失函数，模型就是通过不断地减小损失函数值的方式来进行学习的。让损失函数最小化，通常就要采用梯度下降的方式，即：每一次给模型的权重进行更新的时候，都要按照梯度的反方向进行。为什么呢？因为梯度向量的方向即为函数值增长最快的方向，反方向则是减小最快的方向。上面这个自然段的内容非常非常核心，为了确保你学会，我们换个方式再说一次：模型通过梯度下降的方式，在梯度方向的反方向上不断减小损失函数值，从而进行学习。好，我们具体来看一个公式加深理解，假设我们把损失函数表示为：其中，Wij 表示第 i 层的第 j 个节点对应的权重值。则其梯度向量▽H 为：

$$
\left[\frac{\partial H}{\partial w_{11}}, \quad \frac{\partial H}{\partial w_{12}}, \ldots, \quad \frac{\partial H}{\partial w_{i j}}, \ldots, \quad \frac{\partial H}{\partial w_{m n}}\right]
$$
看到这里，你发现了什么问题？对，感觉这个公式好复杂啊，令人头秃。就比如第一项，w11 跟 H 的关系我哪知道呀，中间隔了那么多层。这时候，就需要链式法则隆重登场了：“两个函数组合起来的复合函数，导数等于里面函数代入外函数值的导数，乘以里面函数之导数。”这个法则包括了两种形式：
 1.  dx/dy​=f′(g(x))g′(x)
 2.  dx/dy​= du/dy​⋅ dx/du​

可能这时候的你仍旧还很懵，不过没关系，我们通过一个更具体的例子再解释一下，你就知道该如何去计算了。假设我们手中有函数 $f(x)=cos(x^2−1)$。我们可以把函数分解为：

 1. f(x)=cos(x) 
 2. $g(x)=x^2−1$

g(x) 的导数 g′(x)=2x，f(x) 的导数 f′(x)=−sin(x)，则 f′(x)=f′(g(x))g′(x)=−sin(x∧2−1)2x，相当于各自求导后再相乘。

说到这，你是不是有点感觉了？这个部分需要你结合公式和我提供的例子仔细看一看，相信你一定可以搞定它。

#### 反向传播
了解了前面的导数、偏导数、梯度、链式法则，反向传播必备的前置知识我们就搞定了。接下来正式进入反向传播的学习，你会发现前面咱们花的这些功夫都没有白费。反向传播算法（Backpropagation）是目前训练神经网络最常用且最有效的算法。模型就是通过反向传播的方式来不断更新自身的参数，从而实现了“学习”知识的过程。反向传播的主要原理是：

前向传播：数据从输入层经过隐藏层最后输出，其过程和之前讲过的前馈网络基本一致。计算误差并传播：计算模型输出结果和真实结果之间的误差，并将这种误差通过某种方式反向传播，即从输出层向隐藏层传递并最后到达输入层。迭代：在反向传播的过程中，根据误差不断地调整模型的参数值，并不断地迭代前面两个步骤，直到达到模型结束训练的条件。

其中最重要的环节有两个：一是通过某种方式反向传播；二是根据误差不断地调整模型的参数值。这两个环节，我们统称为优化方法，一般而言，多采用梯度下降的方法。这里就要使用到导数、梯度和链式法则相关的知识点，梯度下降我们将在下节课详细展开。反向传播的数学推导以及证明过程是非常复杂的，在实际的研发过程中反向传播的过程已经被 PyTorch、TensorFlow 等深度学习框架进行了完善的封装，所以我们不需要手动去写这个过程。不过作为深度学习的研发人员，你还是需要深入了解这个过程的运转方式，这样才能搞清楚深度学习中模型具体是如何学习的。

### 小结
这节课我们一块学习了前馈网络这种最简单的神经网络。虽然前馈网络很简单，但是它的思想贯穿了整个深度学习的过程，是非常重要的概念。同时我们又学习了导数、梯度和链式法则，这几个内容是模型做反向传播从而学习知识的最重要知识点，也是深度学习的内在核心内容，你一定要牢牢掌握。最后，我们初步了解了反向传播的大致过程和概念，这为我们后面正式学习如何计算反向传播奠定了基础。今天的内容里，我尽可能将相关数学知识点进行了简化，保留了最核心的内容。但实际上在深度学习的研究中，涉及的数学知识点非常多，如果感兴趣，你可以在课后查阅更多的相关资料，不断进步。下节课，我会带你学习优化函数，学会了优化函数之后，我们就可以正式开始计算反向传播的过程了。


## 13 | 优化方法：更新模型参数的方法

在上节课中，我们共同了解了前馈网络、导数、梯度、反向传播等概念。但是距离真正完全了解神经网络的学习过程，我们还差一个重要的环节，那就是优化方法。只有搞懂了优化方法，才能做到真的明白反向传播的具体过程。今天我们就来学习一下优化方法，为了让你建立更深入的理解，后面我还特意为你准备了一个例子，把这三节课的所有内容串联起来。

### 用下山路线规划理解优化方法
深度学习，其实包括了三个最重要的核心过程：模型表示、方法评估、优化方法。我们上节课学习的内容，都是为了优化方法做铺垫。优化方法，指的是一个过程，这个过程的目的就是，寻找模型在所有可能性中达到评估效果指标最好的那一个。我们举个例子，对于函数 f(x)，它包含了一组参数。这个例子中，优化方法的目的就是找到能够使得 f(x) 的值达到最小值对应的权重。换句话说，优化过程就是找到一个状态，这个状态能够让模型的损失函数最小，而这个状态就是模型的权重。常见的优化方法种类非常多，常见的有梯度下降法、牛顿法、拟牛顿法等，涉及的数学知识也更是不可胜数。同样的，PyTorch 也将优化方法进行了封装，我们在实际开发中直接使用即可，节省了大量的时间和劳动。不过，为了更好地理解深度学习特别是反向传播的过程，我们还是有必要对一些重要的优化方法进行了解。我们这节课要学习的梯度下降法，也是深度学习中使用最为广泛的优化方法。梯度下降其实很好理解，我给你举一个生活化的例子。假期你跟朋友去爬山，到了山顶之后忽然想上厕所，需要尽快到达半山腰的卫生间，这时候你就需要规划路线，该怎么规划呢？在不考虑生命危险的情况下，那自然是怎么快怎么走了，能跳崖我们绝不走平路，也就是说：越陡峭的地方，就越有可能快速到达目的地。所以，我们就有了一个送命方案：每走几步，就改变方向，这个方向就是朝着当前最陡峭的方向，即坡度下降最快的方向行走，并不断重复这个过程。这就是梯度下降的最直观的表示了。

在上节课中我们曾说过：梯度向量的方向即为函数值增长最快的方向，梯度的反方向则是函数减小最快的方向。梯度下降，就是梯度在深度学习中最重要的用途了。下面我们用相对严谨的方式来表述梯度下降。在一个多维空间中，对于任何一个曲面，我们都能够找到一个跟它相切的超平面。这个超平面上会有无数个方向（想想这是为什么？），但是这所有的方向中，肯定有一个方向是能够使函数下降最快的方向，这个方向就是梯度的反方向。每次优化的目标就是沿着这个最快下降的方向进行，就叫做梯度下降。


具体来说，在一个三维空间曲线中，任何一点我们都能找到一个与之相切的平面（更高维则是超平面），这个平面上就会有无穷多个方向，但是只有一个使曲线函数下降最快的梯度。再次叨叨一遍：每次优化就沿着梯度的反方向进行，就叫做梯度下降。使什么函数下降最快呢？答案就是损失函数。这下你应该将几个知识点串联起来了吧：为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值。这两节课的最终目的，就是让你牢牢记住这句话。我们继续回到刚才的例子。

![img](https://static001.geekbang.org/resource/image/69/3b/697af8ec29ae8fd9d54ce07f206de83b.png?wh=1920x986)


图中红色的线路，是一个看上去还不错的上厕所的路线。但是我们发现，还有别的路线可选。不过，下山就算是不要命地跑，也得讲究方法。就比如，步子大小很重要，太大的话你可能就按照上图中的黄色路线跑了，最后跑到了别的山谷中（函数的局部极小值而非整体最小值）或者在接近和远离卫生间的来回震荡过程中，结果可想而知。但是如果步伐太小了，则需要的时间就很久，可能你还没走到目的地，就坚持不住了（蓝色路线）。在算法中，这个步子的大小，叫做学习率（learning rate）。因为步长的原因，理论上我们是不可能精确地走到目的地的，而是最后在最小值的某一个范围内不断地震荡，也会存在一定的误差，不过这个误差是我们可以接受的。在实际的开发中，如果损失函数在一段时间内没有什么变化，我们就认为是到达了需要的“最低点”，就可以认为模型已经训练收敛了，从而结束训练。


### 常见的梯度下降方法
我们搞清楚了梯度下降的原理之后，下面具体来看几种最常用的梯度下降优化方法。


#### 批量梯度下降法（Batch Gradient Descent，BGD）
线性回归模型是我们最常用的函数模型之一。假设对于一个线性回归模型，y 是真实的数据分布函数，$h_θ​(x)=θ_1​x_1​+θ_2​x_2​+…+θ_n​x_n$​ 是我们通过模型训练得到的函数，其中θ是 h 的参数，也是我们要求的权值。损失函数 J(θ) 可以表述为如下公式：

$$
\text { cost }=J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{i}\right)-y^{i}\right)^{2}
$$

在这里，m 表示样本数量。既然要想损失函数的值最小，我们就要使用到梯度，还记得我们反复说的“梯度向量的方向即为函数值增长最快的方向”么？让损失函数以最快的速度减小，就得用梯度的反方向。首先我们对 J(θ) 中的θ求偏导数，这样就可以得到每个θ对应的梯度：

$$
\frac{\partial J(\theta)}{\partial \theta_{j}}=-\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{i}\right)-y^{i}\right) x_{j}^{i}
$$
得到了每个θ的梯度之后，我们就可以按照下降的方向去更新每个θ，即：
$$
\theta_{j}^{\prime}=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{i}\right)-y^{i}\right) x_{j}^{i}
$$


其中α就是我们刚才提到的学习率。更新θ之后，我们就得到了一个更新之后的损失函数，它的值肯定就会更小，那么我们的模型就更加接近于真实的数据分布了。在上面的公式中，你注意到了 m 这个数了吗？没错，这个方法是当所有的数据都经过了计算之后再整体除以它，即把所有样本的误差做平均。这里我想提醒你，在实际的开发中，往往有百万甚至千万数量级的样本，那这个更新的量就很恐怖了。所以就需要另一个办法，随机梯度下降法。

### 随机梯度下降（Stochastic Gradient Descent，SGD）
随机梯度下降法的特点是，每计算一个样本之后就要更新一次参数，这样参数更新的频率就变高了。其公式如下：
$$
\theta_{j}^{\prime}=\theta_{j}-\alpha\left(h_{\theta}\left(x^{i}\right)-y^{i}\right) x_{j}^{i}
$$

想想看，每训练一条数据就更新一条参数，会有什么好处呢？对，有的时候，我们只需要训练集中的一部分数据，就可以实现接近于使用全部数据训练的效果，训练速度也大大提升。然而，鱼和熊掌不可兼得，SGD 虽然快，也会存在一些问题。就比如，训练数据中肯定会存在一些错误样本或者噪声数据，那么在一次用到该数据的迭代中，优化的方向肯定不是朝着最理想的方向前进的，也就会导致训练效果（比如准确率）的下降。最极端的情况下，就会导致模型无法得到全局最优，而是陷入到局部最优。世间安得两全法，有的时候舍弃一些东西，我们才能获得想要的。**随机梯度下降方法选择了用损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了最终总体的优化效率的提高。**

当然这个过程中增加的迭代次数，还是要远远小于样本的数量的。那如果想尽可能折衷地去协调速度和效果，该怎么办呢？我们很自然就会想到，每次不用全部的数据，也不只用一条数据，而是用“一些”数据，这就是接下来我们要说的小批量梯度下降。

### 小批量梯度下降（Mini-Batch Gradient Descent, MBGD）
Mini-batch 的方法是目前主流使用最多的一种方式，它每次使用一个固定数量的数据进行优化。这个固定数量，我们称它为 batch size。batch size 较为常见的数量一般是 2 的 n 次方，比如 32、128、512 等，越小的 batch size 对应的更新速度就越快，反之则越慢，但是更新速度慢就不容易陷入局部最优。其实具体的数值设成为多少，也需要根据项目的不同特点，采用经验或不断尝试的方法去进行设置，比如图像任务 batch size 我们倾向于设置得稍微小一点，NLP 任务则可以适当的大一些。基于随机梯度下降法，人们又提出了包括 momentum、nesterov momentum 等方法，这部分知识同学们有兴趣点击[这里](https://ruder.io/optimizing-gradient-descent/)可以自行查阅。


### 一个简单的抽象例子
我们通过三节课（第 11 到 13 节课），分别学习了损失函数、反向传播和优化方法（梯度下降）的概念。这三个概念也是深度学习中最为重要的内容，其核心意义在于能够让模型真正做到不断学习和完善自己的表现。那么接下来我们将通过一个简单的抽样例子把三节课的内容汇总起来。需要注意的是，下面的例子不是一个能够运行的例子，而是旨在让我们更加明确一个最基本的 PyTorch 训练过程都需要哪些步骤，你可以当这是一次军训。有了这个演示例子，以后我们上战场，也就是实现真正可用的例子也会事半功倍。在一个模型中，我们要设置如下几个内容：

模型定义。损失函数定义。优化器定义。

通过下面的代码，我们来一块了解一下，上面三个内容在实际开发中应该怎么组合。当然，这个代码是一个抽象版本，目的是帮你快速领会思路。具体的代码填充，还是要根据实际项目来修改。


```
import LeNet #假定我们使用的模型叫做LeNet，首先导入模型的定义类
import torch.optim as optim #引入PyTorch自带的可选优化函数
...
net = LeNet() #声明一个LeNet的实例
criterion = nn.CrossEntropyLoss() #声明模型的损失函数，使用的是交叉熵损失函数
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
# 声明优化函数，我们使用的就是之前提到的SGD，优化的参数就是LeNet内部的参数，lr即为之前提到的学习率

#下面开始训练
for epoch in range(30): #设置要在全部数据上训练的次数
  
    for i, data in enumerate(traindata):
        #data就是我们获取的一个batch size大小的数据
  
        inputs, labels = data #分别得到输入的数据及其对应的类别结果
        # 首先要通过zero_grad()函数把梯度清零，不然PyTorch每次计算梯度会累加，不清零的话第二次算的梯度等于第一次加第二次的
        optimizer.zero_grad()
        # 获得模型的输出结果，也即是当前模型学到的效果
        outputs = net(inputs)
        # 获得输出结果和数据真正类别的损失函数
        loss = criterion(outputs, labels)
        # 算完loss之后进行反向梯度传播，这个过程之后梯度会记录在变量中
        loss.backward()
        # 用计算的梯度去做优化
        optimizer.step()
...
```
这个抽象框架是不是非常清晰？我们先设置好模型、损失函数和优化函数。然后针对每一批（batch）数据，求得输出结果，接着计算损失函数值，再把这个值进行反向传播，并利用优化函数进行优化。别看这个过程非常简单，但它是深度学习最根本、最关键的过程了，也是我们通过三节课学习到的最核心内容了。


### 总结
这节课，我们学习了优化方法以及梯度下降法，并通过一个例子将损失函数、反向传播、梯度下降做了串联。至此，我们就能够在给定一个模型的情况下，训练属于我们自己的深度学习模型了，恭喜你耐心看完。当你想不起来梯度下降原理的时候，不妨回顾一下我们下山路线规划的例子。我们的目标就是设置合理的学习率（步伐），尽可能接近咱们的目的地（达到较理想的拟合效果）。用严谨点的表达说，就是正文里咱们反复强调的：为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值。

![img](https://static001.geekbang.org/resource/image/69/3b/697af8ec29ae8fd9d54ce07f206de83b.png?wh=1920x986)

这里我再带你回顾一下这节课的要点：模型之所以使用梯度下降，其实是通过优化方法不断的去修正模型和真实数据的拟合差距。常用的三种梯度方法包括批量、随机和小批量，一般来说我们更多采用小批量梯度下降。最后我们通过一个抽象的框架，汇总了训练一个模型所需要的几个关键内容，如损失函数、优化函数等，这部分内容是深度学习最关键的过程，建议你重点关注。


## 加餐 | 机器学习其实就那么几件事


通过前面的学习，我们知道，PyTorch 是作为一种机器学习或深度学习的实现工具出现的，因此学习 PyTorch 的时候，免不了会碰到一些机器学习中的相关概念和名词。在专栏前期调研和上线之后，我收到了不少反馈、留言，希望可以在专栏里介绍一下机器学习的基本知识。今天这次加餐，我们就一起来看看什么是机器学习，它是怎么分类的，都有哪些常见名词。在补充了这些基础知识之后，我还会和你聊聊模型训练的本质是什么，你可以把它当作专栏更新过半的期中总结。好，让我们正式开始今天的学习。

### 人工智能、机器学习与深度学习
说到人工智能、机器学习还有深度学习这三个词，我们虽然很眼熟，但三者的关系总是理不清。其实，这三者的关系是一种包含的关系。人工智能包含机器学习，而机器学习又包含深度学习。

![img](https://static001.geekbang.org/resource/image/15/2d/15102e9da587b21792c0a624da0ba32d.jpg?wh=1920x1254)



人工智能的概念其实很早就有了，不过受到技术能力的限制，很少进入到人们的视线当中。当你在网络上搜索人工智能的概念时，可能每一条搜索结果都是用大段文字来解释。归根结底，人工智能的本质就是人们想让计算机像人一样思考，来帮助人们解决一些重复、繁重的工作。

人工智能的应用主要包括以下这几项：专家系统自然语言处理计算机视觉语音系统机器人其他

其中自然语言处理、计算机视觉与语音系统是现在大热的几个方向，从招聘信息中就可以看出来，例如去检索大厂的计算机视觉工程师、自然语言处理工程师等。这些领域中的问题，本质上都可以用传统的机器学习来解决，但依然是受到技术能力的限制，一直处于瓶颈。近十年，随着深度学习的发展，人们在这三个领域的研究中取得了长足的进步。越来越多的人工智能产品得以落地，让我们的生活变得更加便利、快捷。我们专栏所讲的 PyTorch 也活跃于这些领域当中。


### 机器学习（深度学习）
深度学习起源于机器学习中的人工神经网络，所以从工作机制上讲机器学习与深度学习是完全一致的，接下来我们就看看什么是机器学习与深度学习的分类与工作流程。（下文简称机器学习，省略深度学习）。正如前文所说，机器学习的目的是让机器能够像人一样思考。那么我们可以先想想看，人类是根据什么来思考问题的呢？很显然，我们思考问题时，通常会根据以往的一些经验对当前的问题作出判断。与人类归纳总结经验的过程类似，机器学习的主要目的**是把人类归纳经验的过程，转化为由计算机自己来归纳总结的过程。**

其中，人类积累的历史经验，在机器学习中体现为大量的数据。比如在图像分类的过程中我们给计算机提供了大量的图片，总结归纳这个过程，就是机器学习的训练过程，即计算机处理图片并“学习”其中潜在特征的过程。最终的“规律”，则体现为机器学习中的模型，模型也是我们机器学习任务中最终的一个产出。所以说，**机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。**

### 有监督学习 Vs 无监督学习
刚才我们说到，机器学习需要训练出模型。机器学习中的模型基本上可以分为有监督学习与无监督学习两大类，当然，基于这两大分类，下面还有很多小的细分类别，我们先不做讨论。这里我们先弄清楚，什么是有监督学习与无监督学习呢？


有监督学习与无监督学习最明显的区别就是，在训练的时候是否会使用数据真实的标签。为了让你快速理解，这里我结合一个人脸识别的例子来解释一下。首先来看有监督学习，我们现在要训练一个人脸识别模型，来自动识别人脸是 A 还是 B。那么，在训练的时候就要给模型看大量标记为 A 的 A 照片以及标记为 B 的 B 照片，让模型学习谁是 A，谁是 B。只有经过这样的训练之后，当我们进行预测的时候，模型才能正确判断出这张人脸图片是 A 还是 B。再来说说无监督学习，我们手机的相册中有这样的功能，它能自动把某一个人的照片汇聚在一起，但其实手机并不知道汇集到一起的照片是谁。这背后的模型训练原理是怎样的呢？其实训练的时候是把一堆图片给模型看，但是模型并不知道这些图片真实对应的标签，而是模型自己探索这些图片中的潜在特征。大多数我们可以体验到的深度学习应用，都属于有监督学习，例如人脸识别、图像分类、手势识别、人像分割、情感分析等。而最近几年特别流行的 GAN 就属于无监督学习。

### 常见名词讲解
我们在专栏中出现了很多专业的术语，在这里我们就一起汇总一下，解释一下都是什么意思。为了不让你觉得这部分像教科书那样照本宣科，所以我决定用一个例子把这些名词给串联起来。我们就像开篇所说的那样，机器学习的本质就是让机器像人一样的思考，所以，我就用学习这个专栏的过程来解释机器学习中的一些术语。


#### 训练集与验证集
在训练时使用的数据我们称之为训练集。评估模型时使用的数据称之为评估集、验证集或测试集。通过这个专栏的学习，会让你从无到有地掌握有关 PyTorch 的知识，在专栏结束的时候，我们还设置了期末测试题，用来帮助你衡量一下自己的学习成果。那么，这个专栏的内容就相当于训练集，测试题就是验证集（或称测试集）。训练集是用来训练模型的，而验证集是用来评估模型的。在模型训练的时候，要注意训练集与验证集一定是来自同一问题的不同数据。就像专栏学习的是 PyTorch，但是后面是 Python 的测试题，那显然不能反映出你真实的学习成果。


#### Epoch 与 Step
用所有数据训练一遍就是一个 Epoch，也就是把专栏学习一遍就叫做一个 Epoch。但受到硬件设备的限制，训练时不会一次性的读入所有数据，而是一次读入一部分进行训练，就像我们每周一、三、五更新一篇内容，然后你相应的去学习一部分内容一样。这里的“每次”就是对应的 Step 这个概念。那每次读入的数据量就是 batch_size。

![img](https://static001.geekbang.org/resource/image/dc/1c/dcd68825fee0fc135dfe27a9b528711c.jpg?wh=1920x446)


### 模型训练本质
刚才我们通过一个例子理顺了不少机器学习的关键名词。其实专栏更新到现在，我们已经讲完了使用 PyTorch 做模型训练的大部分内容了，恭喜你坚持到这里。其实我刚开始接触机器学习的时候，总是被它的那些算法弄得晕头转向，有一个阶段一直是摸不清头脑的迷茫状态。有的算法即使看明白了，我也不知道该如何使用。所幸坚持学习了一段时间后，我慢慢发现，机器学习其实就那么几件事，可谓万变不离其宗。接下来让我们一起回顾一下机器学习乃至深度学习开发的几个重要环节。首先看看机器学习开发的几个步骤，这我在之前的专栏也有提及，记不清的部分你可以温习回顾。


1. 数据处理：主要包括数据清理、数据预处理、数据增强等。总之，就是构建让模型使用的训练集与验证集。
2. 模型训练：确定网络结构，确定损失函数与设置优化方法。
3. 模型评估：使用各种评估指标来评估模型的好坏。



你现在可以想想，基本没有项目的开发能离开这三步吧。无论是深度学习中的深度模型还是机器学习中的浅层模型，它们的开发基本都离不开这三步。然后，我们再来看看其中的模型训练部分。各种模型纵有千万种变化，但是依然离不开以下几步：

1. 模型结构设计：例如，机器学习中回归算法、SVM 等，深度学习中的 VGG、ResNet、SENet 等各种网络结构，再或者你自己设计的自定义模型结构。
2. 给定损失函数：损失函数衡量的是当前模型预测结果与真实标签之间的差距。
3. 给定优化方法：与损失函数搭配，更新模型中的参数。

你现在再想想，是不是基本所有模型的训练都离不开这三步呢？其实上面讲的这 6 点，都来源于咱们前面讲过的内容。学习前面的内容就好比学会如何制造汽车的零部件，将这些零件组装起来就是完成了一辆汽车的完整生产，而这一步是我们后面要继续研究的。这里面变化最多的就是模型结构了，这一点除了多读读论文，看看相关博客来扩充知识面之外，没有什么捷径可走。然后呢，我们也不要小瞧了损失函数，不同的损失函数有不同的侧重点，当你模型训练处于瓶颈很难提升，或者解决不了现有问题的话，可以考虑考虑调整一下损失函数。在我看来，模型训练的本质就是确定网络结构、设定损失函数与优化方法。接下来，我们将一起学习如何将前面学习的各个环节组装起来，完成一个完整的模型训练。


## 14 | 构建网络：一站式实现模型搭建与训练

前面我们花了不少时间，既学习了数据部分的知识，还研究了模型的优化方法、损失函数以及卷积计算。你可能感觉这些知识还有些零零散散，但其实我们不知不觉中，已经拿下了模型训练的必学内容。今天这节课，也是一个中期小练习，是我们检验自己学习效果的好时机。我会带你使用 PyTorch 构建和训练一个自己的模型。具体我是这么安排的，首先讲解搭建网络必备的基础模块——nn.Module 模块，也就是如何自己构建一个网络，并且训练它，换句话说，就是搞清楚 VGG、Inception 那些网络是怎么训练出来的。然后我们再看看如何借助 Torchvision 的模型作为预训练模型，来训练我们自己的模型。


### 构建自己的模型
让我们直接切入主题，使用 PyTorch，自己构建并训练一个线性回归模型，来拟合出训练集中的走势分布。我们先随机生成训练集 X 与对应的标签 Y，具体代码如下：

```
import numpy as np
import random
from matplotlib import pyplot as plt

w = 2
b = 3
xlim = [-10, 10]
x_train = np.random.randint(low=xlim[0], high=xlim[1], size=30)

y_train = [w * x + b + random.randint(0,2) for x in x_train]

plt.plot(x_train, y_train, 'bo')
```
上述代码中生成的数据，整理成散点图以后，如下图所示：

![img](https://static001.geekbang.org/resource/image/f2/11/f2d24e9e7ea5737a78032b686282ca11.jpg?wh=900x621)

熟悉回归的同学应该知道，我们的回归模型为：y=wx+b。这里的 x 与 y，其实就对应上述代码中的 x_train 与 y_train，而 w 与 b 正是我们要学习的参数。好，那么我们看看如何构建这个模型。我们还是先看代码，再具体讲解。

```
import torch
from torch import nn

class LinearModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.weight = nn.Parameter(torch.randn(1))
    self.bias = nn.Parameter(torch.randn(1))

  def forward(self, input):
    return (input * self.weight) + self.bias
```
通过上面这个线性回归模型的例子，我们可以引出构建网络时的重要几个知识点。

1.必须继承 nn.Module 类。2.重写 `__init__()` 方法。通常来说要把有需要学习的参数的层放到构造函数中，例如，例子中的 weight 与 bias，还有我们之前学习的卷积层。我们在上述的 `__init__()` 中使用了 nn.Parameter()，它主要的作用就是作为 nn.Module 中可训练的参数使用。3.forward() 是必须重写的方法。看函数名也可以知道，它是用来定义这个模型是如何计算输出的，也就是前向传播。对应到我们的例子，就是获得最终输出 y=weight * x+bias 的计算结果。对于一些不需要学习参数的层，一般来说可以放在这里。例如，BN 层，激活函数还有 Dropout。


### nn.Module 模块
nn.Module 是所有神经网络模块的基类。当我们自己要设计一个网络结构的时候，就要继承该类。也就说，其实 Torchvison 中的那些模型，也都是通过继承 nn.Module 模块来构建网络模型的。

需要注意的是，模块本身是 callable 的，当调用它的时候，就是执行 forward 函数，也就是前向传播。我们还是结合代码例子直观感受一下。请看下面的代码，先创建一个 LinearModel 的实例 model，然后 model(x) 就相当于调用 LinearModel 中的 forward 方法。

```
model = LinearModel()
x = torch.tensor(3)
y = model(x)
```
在我们之前的课程里已经讲过，模型是通过前向传播与反向传播来计算梯度，然后更新参数的。我想学到这里，应该没有几个人会愿意去写反向传播和梯度更新之类的代码吧。这个时候 PyTorch 的优点就体现出来了，当你训练时，PyTorch 的求导机制会帮你自动完成这些令人头大的计算。除了刚才讲过的内容，关于初始化方法 `__init__`，你还需要关注的是，必须调用父类的构造方法才可以，也就是这行代码：

```
super().__init__()
```
因为在 nn.Module 的 __init__() 中，会初始化一些有序的字典与集合。这些集合用来存储一些模型训练过程的中间变量，如果不初始化 nn.Module 中的这些参数的话，模型就会报下面的错误。

```
AttributeError: cannot assign parameters before Module.__init__() call
```
### 模型的训练
我们的模型定义好之后，还没有被训练。要想训练我们的模型，就需要用到损失函数与优化方法，这一部分前面课里（如果你感觉陌生的话，可以回顾 11～13 节课）已经学过了，所以现在我们直接看代码就可以了。这里选择的是 MSE 损失与 SGD 优化方法。


```
model = LinearModel()
# 定义优化器
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)

y_train = torch.tensor(y_train, dtype=torch.float32)
for _ in range(1000):
    input = torch.from_numpy(x_train)
    output = model(input)
    loss = nn.MSELoss()(output, y_train)
    model.zero_grad()
    loss.backward()
    optimizer.step()
```
经过 1000 个 Epoch 的训练以后，我们可以打印出模型的 weight 与 bias，看看是多少。对于一个模型的可训练的参数，我们可以通过 named_parameters() 来查看，请看下面代码。

```
for parameter in model.named_parameters():
  print(parameter)
# 输出：
('weight', Parameter containing:
tensor([2.0071], requires_grad=True))
('bias', Parameter containing:
tensor([3.1690], requires_grad=True))
```
可以看到，weight 是 2.0071，bias 是 3.1690，你再回头对一下我们创建训练数据的 w 与 b，它们是不是一样呢？我们刚才说过，继承一个 nn.Module 之后，可以定义自己的网络模型。Module 同样可以作为另外一个 Module 的一部分，被包含在网络中。比如，我们要设计下面这样的一个网络：

![img](https://static001.geekbang.org/resource/image/25/c6/2574b93463fd3dbd0e97661d6a06ffc6.jpg?wh=1372x689)

观察图片很容易就会发现，在这个网络中有大量重复的结构。上图中的 3x3 与 2x2 的卷积组合，按照我们开篇的讲解的话，我们需要把每一层卷积都定义到 __init__()，然后再在 forward 中定义好执行方法就可以了，例如下面的伪代码：
```
class CustomModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding='same')
    self.conv1_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, padding='same')
    ...
    self.conv_m_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding='same')
    self.conv_m_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, padding='same')
    ...
    self.conv_n_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding='same')
    self.conv_n_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, padding='same')

  def forward(self, input):
    x = self.conv1_1(input)
    x = self.conv1_2(x)
    ...
    x = self.conv_m_1(x)
    x = self.conv_m_2(x)
    ...    
    x = self.conv_n_1(x)
    x = self.conv_n_2(x)
    ...
    return x
```
其实这部分重复的结构完全可以放在一个单独的 module 中，然后，在我们模型中直接调用这部分即可，具体实现你可以参考下面的代码：

```
class CustomLayer(nn.Module):
  def __init__(self, input_channels, output_channels):
    super().__init__()
    self.conv1_1 = nn.Conv2d(in_channels=input_channels, out_channels=3, kernel_size=3, padding='same')
    self.conv1_2 = nn.Conv2d(in_channels=3, out_channels=output_channels, kernel_size=2, padding='same')
    
  def forward(self, input):
    x = self.conv1_1(input)
    x = self.conv1_2(x)
    return x
```
然后呢，CustomModel 就变成下面这样了：
```
class CustomModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer1 = CustomLayer(1，1)
    ...
    self.layerm = CustomLayer(1，1)
    ...
    self.layern = CustomLayer(1，1)
  
  def forward(self, input):
    x = self.layer1(input)
    ...
    x = self.layerm(x)
    ...    
    x = self.layern(x)
    ...
    return x
```
熟悉深度学习的同学，一定听过残差块、Inception 块这样的多层的一个组合。你没听过也没关系，在图像分类中我还会讲到。这里你只需要知道，这种多层组合的结构是类似的，对于这种组合，我们就可以用上面的代码的方式实现。


### 模型保存与加载
我们训练好的模型最终的目的，就是要为其他应用提供服务的，这就涉及到了模型的保存与加载。模型保存与加载的话有两种方式。PyTorch 模型的后缀名一般是 pt 或 pth，这都没有关系，只是一个后缀名而已。我们接着上面的回归模型继续讲模型的保存与加载。

方式一：只保存训练好的参数第一种方式就是只保存训练好的参数。然后加载模型的时候，你需要通过代码加载网络结构，然后再将参数赋予网络。只保存参数的代码如下所示：
```
torch.save(model.state_dict(), './linear_model.pth')
```
第一个参数是模型的 state_dict，而第二个参数要保存的位置。代码中的 state_dict 是一个字典，在模型被定义之后会自动生成，存储的是模型可训练的参数。我们可以打印出线性回归模型的 state_dict，如下所示：

```
model.state_dict()
输出：OrderedDict([('weight', tensor([[2.0071]])), ('bias', tensor([3.1690]))])
```
加载模型的方式如下所示：

```
# 先定义网络结构
linear_model = LinearModel()
# 加载保存的参数
linear_model.load_state_dict(torch.load('./linear_model.pth'))
linear_model.eval()
for parameter in linear_model.named_parameters():
  print(parameter)
输出：
('weight', Parameter containing:
tensor([[2.0071]], requires_grad=True))
('bias', Parameter containing:
tensor([3.1690], requires_grad=True))
```
这里有个 model.eval() 需要你注意一下，因为有些层（例如，Dropout 与 BN）在训练时与评估时的状态是不一样的，当进入评估时要执行 model.eval()，模型才能进入评估状态。这里说的评估不光光指代评估模型，也包括模型上线时候时的状态。


方式二：保存网络结构与参数相比第一种方式，这种方式在加载模型的时候，不需要加载网络结构了。具体代码如下所示：

```
# 保存整个模型
torch.save(model, './linear_model_with_arc.pth')
# 加载模型，不需要创建网络了
linear_model_2 = torch.load('./linear_model_with_arc.pth')
linear_model_2.eval()
for parameter in linear_model_2.named_parameters():
  print(parameter)
# 输出：
('weight', Parameter containing:
tensor([[2.0071]], requires_grad=True))
('bias', Parameter containing:
tensor([3.1690], requires_grad=True))
```
这样操作以后，如果你成功输出了相应数值，而且跟之前保存的模型的参数一致，就说明加载对了。

### 使用 Torchvison 中的模型进行训练
我们前面说过，Torchvision 提供了一些封装好的网络结构，我们可以直接拿过来使用。但是并没有细说如何使用它们在我们的数据集上进行训练。今天，我们就来看看如何使用这些网络结构，在我们自己的数据上训练我们自己的模型。


再说微调
其实，Torchvision 提供的模型最大的作用就是当作我们训练时的预训练模型，用来加速我们模型收敛的速度，这就是所谓的微调。对于微调，最关键的一步就是之前讲的调整最后全连接层输出的数目。Torchvision 中只是对各大网络结构的复现，而不是对它们进行了统一的封装，所以在修改全连接层时，不同的网络有不同的修改方法。不过你也别担心，这个修改并不复杂，你只需要打印出网络结构，就可以知道如何修改了。我们接下来以 AlexNet 为例带你尝试一下如何微调。前面讲 Torchvision 的时候其实提到过一次微调，那个时候说的是固定整个网络的参数，只训练最后的全连接层。今天我再给你介绍另外一种微调的方式，那就是修改全连接层之后，整个网络都重新开始训练。只不过，这时候要使用预训练模型的参数作为初始化的参数，这种方式更为常用。接下来，我们就看看如何使用 Torchvision 中模型进行微调。首先，导入模型。代码如下：


```
import torchvision.models as models
alexnet = models.alexnet(pretrained=True)
```
这一步如果你不能“科学上网”的话，可能会比较慢。你可以先根据命令中提示的 url 手动下载，然后使用今天讲的模型加载的方式加载预训练模型，代码如下所示：
```
import torchvision.models as models
alexnet = models.alexnet()
alexnet.load_state_dict(torch.load('./model/alexnet-owt-4df8aa71.pth'))
```
为了验证加载是否成功，我们让它对下图进行预测：

![img](https://static001.geekbang.org/resource/image/66/20/666181b44c23e9075debe0daf6126b20.jpg?wh=1920x1867)

代码如下：


```
from PIL import Image
import torchvision
import torchvision.transforms as transforms

im = Image.open('dog.jpg')

transform = transforms.Compose([
    transforms.RandomResizedCrop((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

input_tensor = transform(im).unsqueeze(0)
alexnet(input_tensor).argmax()
输出：263
```
运行了前面的代码之后，对应到 ImageNet 的类别标签中可以找到，263 对应的是 Pembroke（柯基狗），这就证明模型已经加载成功了。这个过程中有两个重点你要留意。

首先，因为 Torchvision 中所有图像分类的预训练模型，它们都是在 ImageNet 上训练的。所以，输入数据需要是 3 通道的数据，也就是 shape 为 (B, 3, H, W) 的 Tensor，B 为 batchsize。我们需要使用均值为[0.485, 0.456, 0.406]，标准差为[0.229, 0.224, 0.225]对数据进行正规化。另外，从理论上说，大部分的经典卷积神经最后采用全连接层（也就是机器学习中的感知机）进行分类，这也导致了网络的输入尺寸是固定的。但是，在 Torchvision 的模型可以接受任意尺寸的输入的。这是因为 Torchvision 对模型做了优化，有的网络是在最后的卷积层采用了全局平均，或者采用的是全卷积网络。这两种方式都可以让网络接受在最小输入尺寸基础之上，任意尺度的输入。这一点，你现在可能认识得还不够清楚，不过别担心，以后我们学习完图像分类理论之后，你会理解得更加透彻。我们回到微调这个主题。正如刚才所说，训练一个 AlexNet 需要的数据必须是三通道数据。所以，在这里我使用了 CIFAR-10 公开数据集举例。CIFAR-10 数据集一共有 60000 张图片构成，共 10 个类别，每一类包含 6000 图片。每张图片为 32x32 的 RGB 图片。其中 50000 张图片作为训练集，10000 张图片作为测试集。可以说 CIFAR-10 是非常接近真实项目数据的数据集了，因为真实项目中的数据通常是 RGB 三通道数据，而 CIFAR-10 同样是三通道数据。我们用之前讲的 make_grid 方法，将 CIFAR-10 的数据打印出来，代码如下：


```
cifar10_dataset = torchvision.datasets.CIFAR10(root='./data',
                                       train=False,
                                       transform=transforms.ToTensor(),
                                       target_transform=None,
                                       download=True)
# 取32张图片的tensor
tensor_dataloader = DataLoader(dataset=cifar10_dataset,
                               batch_size=32)
data_iter = iter(tensor_dataloader)
img_tensor, label_tensor = data_iter.next()
print(img_tensor.shape)
grid_tensor = torchvision.utils.make_grid(img_tensor, nrow=16, padding=2)
grid_img = transforms.ToPILImage()(grid_tensor)
display(grid_img)

```
请注意，上述代码中的 transform，我为了打印图片只使用了 transform.ToTensor() 输出图片，结果如下所示：

![img](https://static001.geekbang.org/resource/image/f0/f2/f0b5f4ed8c24fbb81c7b7c71a907a6f2.jpg?wh=546x546)

这里我特别说明一下，因为这个训练集的数据都是 32x32 的，所以你现在看到的就是原图效果，图片大小并不影响咱们的学习。下面我们要做的是修改全连接层，直接 print 就可以打印出网络结构，代码如下：
```
print(alexnet)
输出：
AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
```
可以看到，最后全连接层输入是 4096 个单元，输出是 1000 个单元，我们要把它修改为输出是 10 个单元的全连接层（CIFR10 有 10 类）。代码如下：
```
# 提取分类层的输入参数
fc_in_features = alexnet.classifier[6].in_features

# 修改预训练模型的输出分类数
alexnet.classifier[6] = torch.nn.Linear(fc_in_features, 10)
print(alexnet)
输出：
AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=4096, out_features=10, bias=True)
  )
)
```
这时，你可以发现输出就变为 10 个单元了。接下来就是在 CIFAR-10 上，使用 AlexNet 作为预训练模型训练我们自己的模型了。首先是数据读入，代码如下：

```
transform = transforms.Compose([
    transforms.RandomResizedCrop((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
cifar10_dataset = torchvision.datasets.CIFAR10(root='./data',
                                       train=False,
                                       transform=transform,
                                       target_transform=None,
                                       download=True)
dataloader = DataLoader(dataset=cifar10_dataset, # 传入的数据集, 必须参数
                               batch_size=32,       # 输出的batch大小
                               shuffle=True,       # 数据是否打乱
                               num_workers=2)      # 进程数, 0表示只有主进程
```
这里需要注意的是，我更改了 transform，并且将图片 resize 到 224x224 大小。这个尺寸是 Torchvision 中推荐的一个最小训练尺寸。模型就是我们修改后的 AlexNet，之后的训练跟我们之前讲的是一样的。先定义优化器，代码如下：

```
optimizer = torch.optim.SGD(alexnet.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)
```
然后开始模型训练，是不是感觉后面的代码很眼熟，没错，它跟我们之前讲的一样：

```
# 训练3个Epoch
for epoch in range(3):
    for item in dataloader: 
        output = alexnet(item[0])
        target = item[1]
        # 使用交叉熵损失函数
        loss = nn.CrossEntropyLoss()(output, target)
        print('Epoch {}, Loss {}'.format(epoch + 1 , loss))
        #以下代码的含义，我们在之前的文章中已经介绍过了
        alexnet.zero_grad()
        loss.backward()
        optimizer.step()
```
这里用到的微调方式，就是所有参数都需要进行重新训练。而第一种方式（固定整个网络的参数，只训练最后的全连接层），只需要在读取完预训练模型之后，将全连接层之前的参数全部锁死即可，也就是让他们无法训练，我们模型训练时，只训练全连接层就行了，其余一切都不变。代码如下所示：

```
alexnet = models.alexnet()
alexnet.load_state_dict(torch.load('./model/alexnet-owt-4df8aa71.pth'))
for param in alexnet.parameters():
    param.requires_grad = False

```
说到这里，我们的模型微调就讲完了，你可以自己动手试试看。


### 总结
今天的内容，主要是围绕如何自己搭建一个网络模型，我们介绍了 nn.Module 模块以及围绕它的一些方法。根据这讲我分享给你的思路，之后如果你有什么想法时，就可以快速搭建一个模型进行训练和验证。其实，实际的开发中，我们很少会自己去构建一个网络，绝大多数都是直接使用前人已经构建好的一些经典网络，例如，Torchvision 中那些模型。当你去看一些还没有被封装到 PyTorch 的模型的时候，今天所学的内容就能够帮你直接借鉴前人的工作结果，训练属于自己的模型。最后，我再结合自己的学习研究经验，给有兴趣了解更多深度学习知识的同学提供一些学习线索。目前我们只讲了卷积层，对于一个网络还有很多其余层，比如 Dropout、Pooling 层、BN 层、激活函数等。Dropout 函数、Pooling 层、激活函数相对比较好理解，BN 层可能稍微复杂一些。另外，细心的小伙伴应该发现了，我们在打印 AlexNet 网络结构中的时候，它的一部分是使用 nn.Sequential 构建的。nn.Sequential 是一种快速构建网络的方式，有了这节课的知识作储备，弄懂这个方式你会觉得非常简单，也推荐你去看看。

## 15 | 可视化工具：如何实现训练的可视化监控？
上节课中，我们以线性回归模型为例，学习了模型从搭建到训练的全部过程。在深度学习领域，模型训练是一个必须的环节，而在训练过程中，我们常常需要对模型的参数、评价指标等信息进行可视化监控。今天我们主要会学习两种可视化工具，并利用它们实现训练过程的可视化监控。在 TensorFlow 中，最常使用的可视化工具非 Tensorboard 莫属，而 TensorboardX 工具使得 PyTorch 也享受到 Tensorboard 的便捷功能。另外，FaceBook 也为 PyTorch 开发了一款交互式可视化工具 Visdom，它可以对实时数据进行丰富的可视化，帮助我们实时监控实验过程。让我们先从 TensorboardX 说起。

### TensorboardX
Tensorboard 是 TensorFlow 的一个附加工具，用于记录训练过程的模型的参数、评价指标与图像等细节内容，并通过 Web 页面提供查看细节与过程的功能，用浏览器可视化的形式展现，帮助我们在实验时观察神经网络的训练过程，把握训练趋势。既然 Tensorboard 工具这么方便，TensorFlow 外的其它深度学习框架自然也想获取 Tensorboard 的便捷功能，于是，TensorboardX 应运而生。

安装
安装 Tensorboard 很容易，我们可以使用 pip 进行安装，命令如下：
```
pip install tensorboard
```
如果你已经安装过 TensorFlow，那么就无需额外安装 Tensorboard 了。接下来，我们需要安装 TensorboardX。这里需要注意的是，PyTorch 1.8 之后的版本自带 TensorboardX，它被放在torch.utils.tensorboard中，因此无需多余配置。如果你用的是 PyTorch 1.8 之前的版本，TensorboardX 安装起来也非常简单。我们依然使用 pip 命令安装：

```
pip install tensorboardX
```
使用与启动
为了使用 TensorboardX，我们首先需要创建一个 SummaryWriter 的实例，然后再使用add_scalar方法或add_image方法，将数字或图片记录到 SummaryWriter 实例中。SummaryWriter 类的定义如下：

```
torch.utils.tensorboard.writer.SummaryWriter(log_dir=None)
```
其中的 log_dir 表示保存日志的路径，默认会保存在“runs/ 当前时间 _ 主机名”文件夹中。实例创建好之后，我们来看add_scalar方法，这个方法用来记录数字常量，它的定义如下：

```
add_scalar(tag, scalar_value, global_step=None, walltime=None)
```
根据定义，我们依次说说其中的参数：tag：字符串类型，表示数据的名称，不同名称的数据会使用不同曲线展示；scalar_value：浮点型，表示要保存的数值；global_step：整型，表示训练的 step 数；walltime：浮点型，表示记录发生的时间，默认为 time.time()。

我们一般会使用add_scalar方法来记录训练过程的 loss、accuracy、learning rate 等数值的变化，这样就能直观地监控训练过程。add_image方法用来记录单个图像数据（需要 Pillow 库的支持），它的定义如下：

```
add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')
```
tag、global_step 和 walltime 的含义跟add_scalar方法里一样，所以不再赘述，我们看看其他新增的参数都是什么含义。
- img_tensor：PyTorch 的 Tensor 类型或 NumPy 的 array 类型，表示图像数据；
- dataformats：字符串类型，表示图像数据的格式，默认为“CHW”，即 Channel x Height x Width，还可以是“CHW”、“HWC”或“HW”等。

我们来看一个例子加深理解，具体代码如下。


```
from torch.utils.tensorboard import SummaryWriter
# PyTorch 1.8之前的版本请使用：
# from tensorboardX import SummaryWriter
import numpy as np

# 创建一个SummaryWriter的实例
writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
    
img = np.zeros((3, 100, 100))
img[0] = np.arange(0, 10000).reshape(100, 100) / 10000
img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000

writer.add_image('my_image', img, 0)
writer.close()
```
我给你梳理一下这段代码都做了什么。首先创建一个 SummaryWriter 的实例，这里注意，PyTorch 1.8 之前的版本请使用“from tensorboardX import SummaryWriter”，PyTorch 1.8 之后的版本请使用“from torch.utils.tensorboard import SummaryWriter”。然后，我们随机生成一些随机数，用来模拟训练与预测过程中的 Loss 和 Accuracy，并且用add_scalar方法进行记录。最后生成了一个图像，用add_image方法来记录。上述代码运行后，会在当前目录下生成一个“runs”文件夹，里面存储了我们需要记录的数据。然后，我们在当前目录下执行下面的命令，即可启动 Tensoboard。

```
tensorboard --logdir=runs
```
启动后，在浏览器中输入“http://127.0.0.1:6006/”（Tensorboard 的默认端口为 6006），即可对刚才我们记录的数据进行可视化。Tensorboard 的界面如下图所示。图片中右侧部分就是刚刚用add_scalar方法记录的 Loss 和 Accuracy。你看，Tensorboard 已经帮我们按照迭代 step 绘制成了曲线图，可以非常直观地监控训练过程。

![img](https://static001.geekbang.org/resource/image/33/e8/3393e81d9a34c51cdecd4a99e460dde8.png?wh=1920x1360)

在“IMAGES”的标签页中，可以显示刚刚用add_image方法记录的图像数据，如下图所示。

![img](https://static001.geekbang.org/resource/image/86/3a/86bcefa7e732ccceef230a888b3f3a3a.png?wh=1432x1114)

### 训练过程可视化
好，进行到这里，我们已经装好了 TensorboardX 并启动，还演示了这个工具如何使用。那么如何在我们实际的训练过程中来进行可视化监控呢？我们用上节课构建并训练的线性回归模型为例，来进行实践。下面的代码上节课讲过，作用是定义一个线性回归模型，并随机生成训练集 X 与对应的标签 Y。
```
import random
import numpy as np
import torch
from torch import nn

# 模型定义
class LinearModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.weight = nn.Parameter(torch.randn(1))
    self.bias = nn.Parameter(torch.randn(1))

  def forward(self, input):
    return (input * self.weight) + self.bias

# 数据
w = 2
b = 3
xlim = [-10, 10]
x_train = np.random.randint(low=xlim[0], high=xlim[1], size=30)
y_train = [w * x + b + random.randint(0,2) for x in x_train]
```
然后我们在训练的过程中，加入刚才讲过的 SummaryWriter 实例与add_scalar方法，具体的代码如下。


```
# Tensorboard
from torch.utils.tensorboard import SummaryWriter

# 训练
model = LinearModel()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)
y_train = torch.tensor(y_train, dtype=torch.float32)

writer = SummaryWriter()

for n_iter in range(500):
    input = torch.from_numpy(x_train)
    output = model(input)
    loss = nn.MSELoss()(output, y_train)
    model.zero_grad()
    loss.backward()
    optimizer.step()
    writer.add_scalar('Loss/train', loss, n_iter)
```
通过上面这段代码，我们记录了训练过程中的 Loss 的变换过程。具体的趋势如下图所示。

![img](https://static001.geekbang.org/resource/image/df/48/df021d742b09180fb267652fca440d48.png?wh=750x656)

可以看到 Loss 是一个下降的趋势，说明随着训练过程，模型越来越拟合我们的训练数据了。进行到这里，我们已经走完了利用 TensorboardX 工具，实现训练可视化监控的整个过程。TensorboardX 除了包括上述的常用方法之外，还有许多其他方法如add_histogram、add_graph、add_embedding、add_audio 等，感兴趣的同学可以参考[官方文档](https://pytorch.org/docs/stable/tensorboard.html)。相信参考已经学习过的两个 add 方法，你一定能够举一反三，很快熟练调用其它的方法。

### Visdom
Visdom 是 Facebook 开源的一个专门用于 PyTorch 的交互式可视化工具。它为实时数据提供了丰富的可视化种类，可以在浏览器中进行查看，并且可以很容易地与其他人共享可视化结果，帮助我们实时监控在远程服务器上进行的科学实验。


安装与启动Visdom 的安装非常简单，可直接使用 pip 进行安装，具体的命令如下：

```
pip install visdom
python -m visdom.server
```
Visdom 的默认端口是 8097，如果需要修改，可以使用 -p 选项。启动成功后，在浏览器中输入“http://127.0.0.1:8097/”，进入 Visdom 的主界面。Visdom 的主界面如下图所示。

![img](https://static001.geekbang.org/resource/image/99/a8/99d3a7d1ayya17cb625a6f8a567e2aa8.jpg?wh=1920x460)

请你注意，Visdom 的使用与 Tensorboard 稍有不同。Tensorboard 是在生成记录文件后，启动可视化界面。而 Visdom 是先启动可视化界面，当有数据进入 Visdom 的窗口时，会实时动态地更新并绘制数据。

快速上手
下面我们就来动手试一下，看看 Visdom 如何绘制数据。具体过程分四步走：首先，我们需要将窗口类 Visdom 实例化；然后，利用 line() 方法创建一个线图窗口并初始化；接着，利用生成的一组随机数数据来更新线图窗口。最后，通过 image() 方法来绘制一张图片。上述过程的具体代码如下。

```
from visdom import Visdom
import numpy as np
import time

# 将窗口类实例化
viz = Visdom() 
# 创建窗口并初始化
viz.line([0.], [0], win='train_loss', opts=dict(title='train_loss'))

for n_iter in range(10):
    # 随机获取loss值
    loss = 0.2 * np.random.randn() + 1
    # 更新窗口图像
    viz.line([loss], [n_iter], win='train_loss', update='append')
    time.sleep(0.5)

img = np.zeros((3, 100, 100))
img[0] = np.arange(0, 10000).reshape(100, 100) / 10000
img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000
# 可视化图像
viz.image(img)
```
可以看出，使用过程与 Tensorboard 基本一致，只是函数调用上的不同。绘制线图的结果如下图所示。

![img](https://static001.geekbang.org/resource/image/c0/13/c0057dd5935b0ed2f6606a8c148f4f13.gif?wh=368x336)

对应的绘制图片结果如下。可以看出，Visodm 绘制数据时，是动态更新的。



![img](https://static001.geekbang.org/resource/image/a6/88/a66f16a1be56456c009da3873c949888.jpg?wh=676x482)


训练可视化监控
同样地，我们学习可视化工具的使用主要是为了监控我们的训练过程。我们还是以构建并训练的线性回归模型为例，来进行实践。Visdom 监控训练过程大致分为三步：实例化一个窗口；初始化窗口的信息；更新监听的信息。

定义模型与生成训练数据的过程跟前面一样，我就不再重复了。在训练过程中实例化并初始化 Visdom 窗口、实时记录 Loss 的代码如下。

```
# Visdom
from visdom import Visdom
import numpy as np

# 训练
model = LinearModel()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)
y_train = torch.tensor(y_train, dtype=torch.float32)

# 实例化一个窗口
viz = Visdom(port=8097)
# 初始化窗口的信息
viz.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))

for n_iter in range(500):
    input = torch.from_numpy(x_train)
    output = model(input)
    loss = nn.MSELoss()(output, y_train)
    model.zero_grad()
    loss.backward()
    optimizer.step()
    # 更新监听的信息
    viz.line([loss.item()], [n_iter], win='train_loss', update='append')
```
在 Visdom 的界面中，我们可以看到 Loss 的变化趋势如下图所示。Visdom 不会像 Tensorboard 自动对曲线进行缩放或平滑，因此可以看到 50 轮之后，由于 Loss 值变化范围比较小，图像的抖动趋势被压缩得非常不明显。

![img](https://static001.geekbang.org/resource/image/54/e0/54665114801a49e36a0211b5b3dbdce0.jpg?wh=1007x763)



### 小结
这节课我带你学习了两种可视化工具：TensorboardX 和 Visdom。相信通过一节课的讲解和练习，这两种可视化工具如何安装、启动，还有如何用它们绘制线图和图片这些基本的操作，相信你都已经掌握了。学习使用可视化工具的主要目的，是为了帮助我们在深度学习模型的训练过程中，实时监控一些数据，例如损失值、评价指标等等。对这些数据进行可视化监控，可以帮助我们感知各个参数与指标的变化，实时把握训练趋势。因此，如何将可视化工具应用于模型训练过程中，是我们学习的重点。TensorboardX 和 Visdom 还有其它诸如绘制散点图、柱状图、热力图等等多种多样的功能，如果你感兴趣，可以参考官方文档，类比我们今天学习的方法动手试一试，经过练习，一定可以熟练使用它们。



## 16｜分布式训练：如何加速你的模型训练？
在之前的课程里，我们一起学习了深度学习必备的内容，包括构建网络、损失函数、优化方法等，这些环节掌握好了，我们就可以训练很多场景下的模型了。但是有的时候，我们的模型比较大，或者训练数据比较多，训练起来就会比较慢，该怎么办呢？这时候牛气闪闪的分布式训练登场了，有了它，我们就可以极大地加速我们的训练过程。这节课我就带你入门分布式训练，让你吃透分布式训练的工作原理，最后我还会结合一个实战项目，带你小试牛刀，让你在动手过程中加深对这部分内容的理解。

### 分布式训练原理
在具体介绍分布式训练之前，我们需要先简要了解一下为什么深度学习要使用 GPU。在我们平时使用计算机的时候，程序都是将进程或者线程的数据资源放在内存中，然后在 CPU 进行计算。通常的程序中涉及到了大量的 if else 等分支逻辑操作，这也是 CPU 所擅长的计算方式。而在深度学习中，模型的训练与计算过程则没有太多的分支，基本上都是矩阵或者向量的计算，而这种暴力又单纯的计算形式非常适合用 GPU 处理，GPU 的整个处理过程就是一个流式处理的过程。但是再好的车子，一个缸的发动机也肯定比不过 12 个缸的，同理单单靠一个 GPU，速度肯定还是不够快，于是就有了多个 GPU 协同工作的办法，即分布式训练。分布式训练，顾名思义就是训练的过程是分布式的，重点就在于后面这两个问题：

1. 谁分布了？答案有两个：数据与模型。
2. 怎么分布？答案也有两个：单机多卡与多机多卡。

也就是说，为了实现深度学习的分布式训练，我们需要采用单机多卡或者多机多卡的方式，让分布在不同 GPU 上的数据和模型协同训练。那么接下来，我们先从简单的单机单卡入手，了解一下 GPU 的训练过程。

#### 单机单卡
想象一下，如果让你把数据或者模型推送到 GPU 上，需要做哪几步操作呢？让我们先从单 GPU 的情况出发。第一步，我们需要知道手头有多少 GPU。PyTorch 中使用 torch.cuda.is_available() 函数来判断当前的机器是否有可用的 GPU，而函数 torch.cuda.device_count() 则可以得到目前可用的 GPU 的数量。第二步，获得 GPU 的一个实例。例如下面的语句：

```
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```
这里 torch.device 代表将 torch.Tensor 分配到的设备，是一个设备对象实例，也就是 GPU。其中 cuda: 0 表示我们使用的是第一块 GPU。当然你也可以不用声明“:0”，默认就从第一块开始。如果没有 GPU（torch.cuda.is_available()），那就只能使用 CPU 了。第三步，将数据或者模型推到 GPU 上去，这个过程我们称为迁移。在 PyTorch 中，这个过程的封装程度非常高，换句话说，我们只需要保证即将被推到 GPU 的内容是张量（Tensor）或者模型（Module），就可以用 to() 函数快速进行实现。例如：
```
data = torch.ones((3, 3))
print(data.device)
# Get: cpu

# 获得device
device = torch.device("cuda: 0")

# 将data推到gpu上
data_gpu = data.to(device)
print(data_gpu.device)
# Get: cuda:0
```
在上面这段代码中，我们首先创建了一个常规的张量 data，通过 device 属性，可以看到 data 现在是在 CPU 上的。随后，我们通过 to() 函数将 data 迁移到 GPU 上，同样也能通过 device 属性看到 data 确实已经存在于 GPU 上了。那么对于模型，是否也是一样的操作呢？答案是肯定的，我们接下来看一个例子：

```
net = nn.Sequential(nn.Linear(3, 3))
net.to(device)

```
这里仍旧使用 to() 函数即可。单机单卡的模式，相当于有一批要处理加工的产品，只分给了一个工人和一台机器来完成，这种情况下数量少了还可以，但是一旦产品太多了，就得加人、加机器才能快速交工了。深度学习也是一样，在很多场景中，比如推荐算法模型、语言模型等，往往都有着百万、千万甚至上亿的训练数据，这样如果只用一张卡的话肯定是搞不定了。于是就有了单机多卡和多机多卡的解决方案。


#### 单机多卡
那么，在 PyTorch 中，单机多卡的训练是如何进行的呢？其实 PyTorch 提供了好几种解决方案，咱们先看一个最简单也是最常用的办法：nn.DataParallel()。其具体定义如下：
```
torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)
```
在这里，module 就是你定义的模型，device_ids 即为训练模型时用到的 GPU 设备号，output_device 表示输出结果的 device，默认为 0 也就是第一块卡。我们可以使用 nvidia-smi 命令查看 GPU 使用情况。如果你足够细心就会发现，使用多个卡做训练的时候，output_device 的卡所占的显存明显大一些。继续观察你还会发现，使用 DataParallel 时，数据的使用是并行的，每张卡获得的数据都一样多，但是输出的 loss 则是所有的卡的 loss 都会在第 output_device 块 GPU 进行计算，这导致了 output_device 卡的负载进一步增加。

![img](https://static001.geekbang.org/resource/image/7f/08/7f8e9a83fa6a91yyf931565c55f0a708.png?wh=1130x522)

就这么简单？对，就这么简单，只需要一个 DataParallel 函数就可以将模型分发到多个 GPU 上。但是我们还是需要了解这内部的运行逻辑，因为只有了解了这个逻辑，在以后的开发中遇到了诸如时间计算、资源预估、优化调试问题的时候，你才可以更好地运用 GPU，让多 GPU 的优势真正发挥出来。在模型的前向计算过程中，数据会被划分为多个块，被推送到不同的 GPU 进行计算。但是不同的是，模型在每个 GPU 中都会复制一份。我们看一下后面的代码：


```
class ASimpleNet(nn.Module):
    def __init__(self, layers=3):
        super(ASimpleNet, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(3, 3, bias=False) for i in range(layers)])
    def forward(self, x):
        print("forward batchsize is: {}".format(x.size()[0]))
        x = self.linears(x)
        x = torch.relu(x)
        return x
        
batch_size = 16
inputs = torch.randn(batch_size, 3)
labels = torch.randn(batch_size, 3)
inputs, labels = inputs.to(device), labels.to(device)
net = ASimpleNet()
net = nn.DataParallel(net)
net.to(device)
print("CUDA_VISIBLE_DEVICES :{}".format(os.environ["CUDA_VISIBLE_DEVICES"]))

for epoch in range(1):
    outputs = net(inputs)

# Get:
# CUDA_VISIBLE_DEVICES : 3, 2, 1, 0
# forward batchsize is: 4
# forward batchsize is: 4
# forward batchsize is: 4
# forward batchsize is: 4

```
在上面的程序中，我们通过 CUDA_VISIBLE_DEVICES 得知了当前程序可见的 GPU 数量为 4，而我们的 batch size 为 16，输出每个 GPU 上模型 forward 函数内部的 print 内容，验证了每个 GPU 获得的数据量都是 4 个。这表示，DataParallel 会自动帮我们将数据切分、加载到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总。

#### 多机多卡
多机多卡一般都是基于集群的方式进行大规模的训练，需要涉及非常多的方面，咱们这节课只讨论最基本的原理和方法。在具体实践中，你可能还会遇到其它网络或环境等问题，届时需要具体问题具体解决。

DP 与 DDP
刚才我们已经提到，对于单机多卡训练，有一个最简单的办法：DataParallel。其实 PyTorch 的数据并行还有一个主要的 API，那就是 DistributedDataParallel。而 DistributedDataParallel 也是我们实现多机多卡的关键 API。

DataParallel 简称为 DP，而 DistributedDataParallel 简称为 DDP。我们来详细看看 DP 与 DDP 的区别。先看 DP，DP 是单进程控制多 GPU。从之前的程序中，我们也可以看出，DP 将输入的一个 batch 数据分成了 n 份（n 为实际使用的 GPU 数量），分别送到对应的 GPU 进行计算。在网络前向传播时，模型会从主 GPU 复制到其它 GPU 上；在反向传播时，每个 GPU 上的梯度汇总到主 GPU 上，求得梯度均值更新模型参数后，再复制到其它 GPU，以此来实现并行。由于主 GPU 要进行梯度汇总和模型更新，并将计算任务下发给其它 GPU，所以主 GPU 的负载与使用率会比其它 GPU 高，这就导致了 GPU 负载不均衡的现象。再说说 DDP，DDP 多进程控制多 GPU。系统会为每个 GPU 创建一个进程，不再有主 GPU，每个 GPU 执行相同的任务。DDP 使用分布式数据采样器（DistributedSampler）加载数据，确保数据在各个进程之间没有重叠。在反向传播时，各 GPU 梯度计算完成后，各进程以广播的方式将梯度进行汇总平均，然后每个进程在各自的 GPU 上进行梯度更新，从而确保每个 GPU 上的模型参数始终保持一致。由于无需在不同 GPU 之间复制模型，DPP 的传输数据量更少，因此速度更快。

DistributedDataParallel 既可用于单机多卡也可用于多机多卡，它能够解决 DataParallel 速度慢、GPU 负载不均衡等问题。因此，官方更推荐使用 DistributedDataParallel 来进行分布式训练，也就是接下来要说的 DDP 训练。

DDP 训练
DistributedDataParallel 主要是为多机多卡而设计的，不过单机上也同样可以使用。想要弄明白 DPP 的训练机制，我们先要弄明白这几个分布式中的概念：
- group：即进程组。默认情况下，只有一个组，即一个 world。
- world_size ：表示全局进程个数。
- rank：表示进程序号，用于进程间通讯，表示进程优先级。rank=0 的主机为主节点。

使用 DDP 进行分布式训练的具体流程如下。接下来，我们就按步骤分别去实现。

![img](https://static001.geekbang.org/resource/image/27/7d/2730a8d7e7e1fe21574918a2dc48c67d.jpg?wh=1920x1009)

第一步，初始化进程组。我们使用 init_process_group 函数来进行分布式初始化，其定义如下：
```
torch.distributed.init_process_group(backend, init_method=None,, world_size=-1, rank=-1, group_name='')
```
我们分别看看定义里的相关参数：backend：是通信所用的后端，可以是“nccl”或“gloo”。一般来说，nccl 用于 GPU 分布式训练，gloo 用于 CPU 进行分布式训练。init_method：字符串类型，是一个 url，用于指定进程初始化方式，默认是 “env://”，表示从环境变量初始化，还可以使用 TCP 的方式或共享文件系统  。world_size：执行训练的所有的进程数，表示一共有多少个节点（机器）。rank：进程的编号，也是其优先级，表示当前节点（机器）的编号。group_name：进程组的名字。

使用 nccl 后端的代码如下。


```
torch.distributed.init_process_group(backend="nccl")
```
完成初始化以后，第二步就是模型并行化。正如前面讲过的，我们可以使用 DistributedDataParallel，将模型分发至多 GPU 上，其定义如下：
```
torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0）
```
DistributedDataParallel 的参数与 DataParallel 基本相同，因此模型并行化的用法只需将 DataParallel 函数替换成 DistributedDataParallel 即可，具体代码如下。

```
net = torch.nn.parallel.DistributedDataParallel(net)
```
最后就是创建分布式数据采样器。在多机多卡情况下，分布式训练数据的读取也是一个问题，不同的卡读取到的数据应该是不同的。DP 是直接将一个 batch 的数据划分到不同的卡，但是多机多卡之间进行频繁的数据传输会严重影响效率，这时就要用到分布式数据采样器 DistributedSampler，它会为每个子进程划分出一部分数据集，从而使 DataLoader 只会加载特定的一个子数据集，以避免不同进程之间有数据重复。创建与使用分布式数据采样器的代码如下。

```
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
data_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)
```
结合代码我给你解读一下。首先，我们将 train_dataset 送到了 DistributedSampler 中，并创建了一个分布式数据采样器 train_sampler。然后在构造 DataLoader 的时候,  参数中传入了一个 sampler=train_sampler，即可让不同的进程节点加载属于自己的那份子数据集。也就是说，使用 DDP 时，不再是从主 GPU 分发数据到其他 GPU 上，而是各 GPU 从自己的硬盘上读取属于自己的那份数据。为什么要使用分布式训练以及分布式训练的原理我们就讲到这里。相信你已经对数据并行与模型并行都有了一个初步的认识。


### 小试牛刀
下面我们将会讲解一个[官方的 ImageNet 的示例](https://github.com/pytorch/examples/blob/main/imagenet/main.py)，以后你可以把这个小项目当做分布式训练的一个模板来使用。这个示例可对使用 DP 或 DDP 进行选配，下面我们就一起来看核心代码。

```
if args.distributed:
     if args.dist_url == "env://" and args.rank == -1:
         args.rank = int(os.environ["RANK"])
     if args.multiprocessing_distributed:
         # For multiprocessing distributed training, rank needs to be the
         # global rank among all the processes
         args.rank = args.rank * ngpus_per_node + gpu
     dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                             world_size=args.world_size, rank=args.rank)
```
这里你可以重点关注示例代码中的“args.distributed”参数，args.distributed 为 True，表示使用 DDP，反之表示使用 DP。我们来看 main_worker 函数中这段针对 DDP 的初始化代码，如果使用 DDP，那么使用 init_process_group 函数初始化进程组。ngpus_per_node 表示每个节点的 GPU 数量。我们再来看 main_worker 函数中的这段逻辑代码。

```
if not torch.cuda.is_available():
    print('using CPU, this will be slow')
elif args.distributed:
    # For multiprocessing distributed, DistributedDataParallel constructor
    # should always set the single device scope, otherwise,
    # DistributedDataParallel will use all available devices.
    if args.gpu is not None:
        torch.cuda.set_device(args.gpu)
        model.cuda(args.gpu)
        # When using a single GPU per process and per
        # DistributedDataParallel, we need to divide the batch size
        # ourselves based on the total number of GPUs we have
        args.batch_size = int(args.batch_size / ngpus_per_node)
        args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
    else:
        model.cuda()
        # DistributedDataParallel will divide and allocate batch_size to all
        # available GPUs if device_ids are not set
        model = torch.nn.parallel.DistributedDataParallel(model)
elif args.gpu is not None:
    torch.cuda.set_device(args.gpu)
    model = model.cuda(args.gpu)
else:
    # DataParallel will divide and allocate batch_size to all available GPUs
    if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):
        model.features = torch.nn.DataParallel(model.features)
        model.cuda()
    else:
        model = torch.nn.DataParallel(model).cuda()

```
这段代码是对使用 CPU 还是使用 GPU、如果使用 GPU，是使用 DP 还是 DDP 进行了逻辑选择。我们可以看到，这里用到了 DistributedDataParallel 函数或 DataParallel 函数，对模型进行并行化。并行化之后就是创建分布式数据采样器，具体代码如下。

```
if args.distributed:
    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
else:
    train_sampler = None

train_loader = torch.utils.data.DataLoader(
    train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),
    num_workers=args.workers, pin_memory=True, sampler=train_sample

```
这里需要注意的是，在建立 Dataloader 的过程中，如果 sampler 参数不为 None，那么 shuffle 参数不应该被设置。最后，我们需要为每个机器节点上的每个 GPU 启动一个进程。PyTorch 提供了 torch.multiprocessing.spawn 函数，来在一个节点启动该节点所有进程，具体的代码如下。


```
 ngpus_per_node = torch.cuda.device_count()
 if args.multiprocessing_distributed:
     # Since we have ngpus_per_node processes per node, the total world_size
     # needs to be adjusted accordingly
     args.world_size = ngpus_per_node * args.world_size
     # Use torch.multiprocessing.spawn to launch distributed processes: the
     # main_worker process function
     mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))
 else:
     # Simply call main_worker function
     main_worker(args.gpu, ngpus_per_node, args)
```
对照代码我们梳理一下其中的要点。之前我们提到的 main_worker 函数，就是每个进程中，需要执行的操作。ngpus_per_node 是每个节点的 GPU 数量（每个节点 GPU 数量相同），如果是多进程，ngpus_per_node * args.world_size 则表示所有的节点中一共有多少个 GPU，即总进程数。一般情况下，进程 0 是主进程，比如我们会在主进程中保存模型或打印 log 信息。当节点数为 1 时，实际上就是单机多卡，所以说 DDP 既可以支持多机多卡，也可以支持单机多卡。main_worker 函数的调用方法如下。

```
main_worker(args.gpu, ngpus_per_node, args)
```
其中，args.gpu 表示当前所使用 GPU 的 id。而通过 mp.spawn 调用之后，会为每个节点上的每个 GPU 都启动一个进程，每个进程运行 main_worker(i, ngpus_per_node, args)，其中 i 是从 0 到 ngpus_per_node-1。模型保存的代码如下。

```
if not args.multiprocessing_distributed or (args.multiprocessing_distributed
         and args.rank % ngpus_per_node == 0):
     save_checkpoint({
         'epoch': epoch + 1,
         'arch': args.arch,
         'state_dict': model.state_dict(),
         'best_acc1': best_acc1,
         'optimizer' : optimizer.state_dict(),
     }, is_best)
```
这里需要注意的是，使用 DDP 意味着使用多进程，如果直接保存模型，每个进程都会执行一次保存操作，此时只使用主进程中的一个 GPU 来保存即可。好，说到这，这个示例中有关分布式训练的重点内容我们就讲完了。

### 小结
恭喜你走到这里，这节课我们一起完成了分布式训练的学习，最后咱们一起做个总结。今天我们不但学习了为什么要使用分布式训练以及分布式训练的原理，还一起学习了一个分布式训练的实战项目。


在分布式训练中，主要有 DP 与 DDP 两种模式。其中 DP 并不是完整的分布式计算，只是将一部分计算放到了多张 GPU 卡上，在计算梯度的时候，仍然是“一卡有难，八方围观”，因此 DP 会有负载不平衡、效率低等问题。而 DDP 刚好能够解决 DP 的上述问题，并且既可以用于单机多卡，也可以用于多机多卡，因此它是更好的分布式训练解决方案。你可以将今天讲解的示例当做分布式训练的一个模板来使用。它包括了 DP 与 DPP 的完整使用过程，并且包含了如何在使用 DDP 时保存模型。不过这个示例中的代码里其实还有更多的细节，建议你留用课后空余时间，通过精读代码、查阅资料，多动手、多思考来巩固今天的学习成果。

## 实战
## 17 | 图像分类（上）：图像分类原理与图像分类模型

通过前面的学习，我们已经掌握了 PyTorch 有关深度学习的不少知识。为了避免纸上谈兵，我们正式进入实战环节，分别从计算机视觉与自然语言处理这两个落地项目最多的深度学习应用展开，看看业界那些常见深度学习应用都是如何实现的。完成这个模块的学习以后，我想你不仅仅会巩固之前学习的内容，还会进一步地落实到细分的领域去看待问题、解决问题。说到计算机视觉，很常见的一种应用方向就是图像分类。关于图像分类，其实离我们并不遥远。你有没有发现，现在很多智能手机，照相的时候都会自动给照片内容打上标签。举个例子，你看后面的截图，就是我用手机拍照的时候，手机自动对摄像头的内容进行了识别，打上了“多云”这个标签。



![img](https://static001.geekbang.org/resource/image/75/7c/75e6ec9c616da2c5c5907e0d11184d7c.jpeg?wh=1920x886)


然后你会发现，手机还能根据识别到的内容，为你推荐一些美化的方案。那这是怎么做到的呢？其实这就是卷积神经网络最常用、最广泛且最基本的一个应用：图像分类。今天咱们就来一探究竟，看看图像分类到底是怎么一回事。我会用两节课的篇幅，带你学习图像分类。这节课我们先学习理论知识，掌握图像分类原理和常见的卷积神经网络。下节课，我们再基于今天学到的原理，一块完成一个完整的图像分类项目实践。

### 图像分类原理
我们还是“书接上文”，沿用第 3 节课 NumPy 的那个例子。现在线上每天都有大量的图片被上传，老板交代你设计一个模型，把有关极客时间 Logo 的图片自动找出来。把这个需求翻译一下就是：建立一个图像分类模型，提供自动识别有极客时间 Logo 图片的功能。我们来梳理一下这个模型的功能，我们这个模型会接收一张图片，然后会输出一组概率，分别是该图片为 Logo 的概率与该图片为其他图片的概率，从而通过概率来判断这张图片是 Logo 类还是 Other 类，如下图所示：

![img](https://static001.geekbang.org/resource/image/f4/68/f4b226497cb6aae5e0dcde4f65e46a68.png?wh=1392x604)


#### 感知机
我们将上面的模型进一步拆分，看看如何才能获得这样的一组输出。其中输入的图片，就是输入 X，将其展开后，可以获得输入 X 为 X=x1​,x2​,…,xn​，而模型可以看做有两个节点，每个节点都会有一个输出，分别代表着对输入为 Logo 和 Other 的判断，但这里的输出暂时还不是概率，只是模型输出的一组数值。这一部分内容如下图所示：

![img](https://static001.geekbang.org/resource/image/03/29/0322747253dbbffe80a92004ea12be29.png?wh=1732x660)

上图这个结构其实就是感知机了，中间绿色的节点叫做神经元，是感知机的最基本组成单元。上图中的感知机只有中间一层（绿色的神经元），如果有多层神经元的话，我们就称之为多层感知机。那什么是神经元呢？神经元是关于输入的一个线性变换，每一个输入 x 都会有一个对应的权值，上图中的 y 的计算方式为：

$$
y_{i}=\delta\left(w_{i 1} x_{1}+w_{i 2} x_{2}+\ldots+w_{i_{n}} x_{n}+b_{i}\right), \quad i=1,2
$$


其中，$w_{i1}​,w_{i2}​,…,w_{in}$ 是神经元的权重，bi​ 为神经元的偏移项。权重与偏移项都是通过模型学习到的参数。δ 为激活函数，激活函数是一个可选参数。那如何将一组数值，也就是 y1​ 与 y2​ 转换为一组对应的概率呢？这个时候 Softmax 函数就要登场了。它的作用就是将一组数值转换为对应的概率，概率和为 1。Softmax 的计算公式如下：

$$
\delta\left(x_{j}\right)=\frac{e^{x_{j}}}{\sum_{j=1}^{m} e^{x_{j}}}
$$
请看下面的代码，我们用 Softmax 函数对原始的输入 y 做个转化，将 y 中的数值转化为一组对应的概率：


```
import torch
import torch.nn as nn

# 2个神经元的输出y的数值为
y = torch.randn(2)
print(y)
输出：tensor([0.2370, 1.7276])
m = nn.Softmax(dim=0)
out = m(y)
print(out)
输出：tensor([0.1838, 0.8162])
```
你看，经过 Softmax 之后，原始的输出 y 是不是转换成一组概率，并且概率的和为 1 呢。原始 y 中最大的 y 具有最大的概率。当然，Softmax 也不是每一个问题都会使用。我们根据问题的不同可以采用不同的函数，例如，有的时候也会使用 sigmoid 激活函数，sigmoid 激活函数是将 1 个数值转换为 0 到 1 之间的概率。现在，我们将上述的过程补充到前面的模型里，如下图所示。

![img](https://static001.geekbang.org/resource/image/92/32/92ec877yy99cd31b5c9c9fc46f78c832.png?wh=1708x608)


#### 全连接层
其实，上面那张示意图，就是图像的分类原理了。其中绿色那一层。在卷积神经网络中称为全连接层，Full Connection Layer，简称 fc 层。一般都是放在网络的最后端，用来获得最终的输出，也就是各个类别的概率。因为全连接层中的神经元的个数是固定的，所以说在有全连接层的网络中，输入图片是必须固定尺寸的。而现实里我们线上收集到的图片会有不同的尺寸，所以需要先把图片尺寸统一起来，PyTorch 才能进一步处理。我们假设将前面的输入图片 resize 到 128x128，然后看看全连接层推断的过程在 PyTorch 中是如何实现的。

```
x = torch.randint(0, 255, (1, 128*128), dtype=torch.float32)
fc = nn.Linear(128*128, 2)
y = fc(x)
print(y)
输出：tensor([[  72.1361, -120.3565]], grad_fn=<AddmmBackward>)
# 注意y的shape是(1, 2)
output = nn.Softmax(dim=1)(y)
print(output)
输出：tensor([[1., 0.]], grad_fn=<SoftmaxBackward>)
```
结合代码不难看出，PyTorch 中全连接层用 nn.Linear 来实现。我们分别看看里面的重要参数有哪些：in_features：输入特征的个数，在本例中为 128x128；out_features：输出的特征数，在本例中为 2；bias：是否需要偏移项，默认为 True。

全连接层的输入，也不是原始图片数据，而是经过多层卷积提取的特征。前面我们曾说过，有的网络是可以接收任意尺度的输入的。在上文中的设计中，全连接层的输入 x1 到 xn 是固定的，数目等于最后一层特征图所有元素的数目。如下图所示：

![img](https://static001.geekbang.org/resource/image/af/5b/af7f9971ea5564d93c0a0089d3f5d75b.png?wh=1490x678)

我们将上述结构稍作调整，就可以接收任意尺度的输入了。只需要在最后的特征图后面加一个全局平均即可，也就是将每个特征图进行求平均，用平均值代替特征图，这样无论输入的尺度是多少，进入全连接层的数据量都是固定的。如下图所示，黄色的圈就是全局平均的结果。

![img](https://static001.geekbang.org/resource/image/e0/e0/e0a62554422d28601af056809873d8e0.png?wh=1760x730)

我们下一节课介绍的 EfficientNet 就是采用这种方式，使得网络可以使用任意尺度的图片进行训练。

### 卷积神经网络
其实刚才说的多层感知机就是卷积神经网络的前身，由于自身的缺陷（参数量大、难以训练），使其在历史上有段时间一直是停滞不前，直到卷积神经网络的出现，打破了僵局。卷积神经网络的最大作用就是提取出输入图片的丰富信息，然后再对接上层的一些应用，比如前面提到的图片分类。把卷积神经网络应用到图像分类原理中，得到的模型如下图所示：

![img](https://static001.geekbang.org/resource/image/8b/1a/8bbc16d51yydca581cb1d88274ec161a.png?wh=1728x664)


你需要注意的是示意图中各个层的定义，不同层有不同的名称。在上图中，整个模型或者网络的重点全都在卷积神经网络那块，所以这也是我们的工作重点。那如何找到一个合适的卷积神经网络呢？在实际工作中，我们几乎不会自己去设计一个神经网络网的（因为不可控的变量太多），而是直接选择一些大神设计好的网络直接使用。那网络模型那么多，我们如何验证大神们提出的网络确实是可靠、可用的呢？



#### ImageNet
在业界中有个标杆——ImageNet，大家都用它来评价提出模型的好与坏。ImageNet 本身包含了一个非常大的数据集，并且从 2010 年开始，每年都会举办一次著名的 ImageNet 大规模视觉识别挑战赛（The ImageNet Large Scale Visual Recognition Challenge ，ILSVRC），比赛包含了图像分类、目标检测与图像分割等任务。其中，图像分类比赛使用的数据集是一份有 1000 个类别的庞大数据集，只要能在这个比赛中脱颖而出的模型，都是我们所说的经典网络结构，这些网络在实际项目中基本都是我们的首选。从 2012 年开始，伴随着深度学习的发展，几乎每一年都有非常经典的网络结构诞生，下表为历年来 ImageNet 上 Top-5 的错误率。

![img](https://static001.geekbang.org/resource/image/da/73/da4f8fe982d066b8541f63231d257c73.jpg?wh=1920x818)

你可能会有疑问，了解这么多网络模型真的有必要么？我想说的是，磨刀不误砍柴工，机器学习这个领域始终是依靠研究驱动的。工作当中，我们很少从 0 到 1 自创一个网络模型，常常是在经典设计基础上做一些自定义配置，所以你最好对这些经典网络都有所了解。接下来，我们就挑选几个经典的神经网络来看看。

#### VGG
[VGG](https://arxiv.org/abs/1409.1556)取得了 ILSVRC 2014 比赛分类项目的第 2 名和定位项目的第 1 名的优异成绩。当年的 VGG 一共提供了 A 到 E6 种不同的 VGG 网络（字母不同，只是表示层数不一样）。VGG19 的效果虽说最好，但是综合模型大小等指标，在实际项目中 VGG16 用得更加多一点。具体的网络结构你可以看看论文。我们来看看 VGG 突破的一些重点：证明了随着模型深度的增加，模型效果也会越来越好。使用较小的 3x3 的卷积，代替了 AlexNet 中的 11x11、7x7 以及 5x5 的大卷积核。

关于第二点，VGG 中将 5x5 的卷积用 2 层 3x3 的卷积替换；将 7x7 的卷积用 3 层 3x3 的卷积替换。这样做首先可以减少网络的参数，其次是可以在相同感受野的前提下，加深网络的层数，从而提取出更加多样的非线性信息。



#### GoogLeNet
2014 年分类比赛的冠军是[GoogLeNet](https://arxiv.org/abs/1409.4842)（VGG 同年）。GoogLeNet 的核心是 Inception 模块。这个时期的 Inception 模块是 v1 版本，后续还有 v2、v3 以及 v4 版本。我们先来看看 GoogLeNet 解决了什么样的问题。研究人员发现，对于同一个类别的图片，主要物体在不同图片中，所占的区域大小均有不同，例如下图所示。

![img](https://static001.geekbang.org/resource/image/c4/a7/c4bed5998c8yy9d4e4661c8a5520fba7.jpg?wh=2561x992)

如果使用 AlexNet 或者 VGG 中标准的卷积的话，每一层只能以相同的尺寸的卷积核来提取图片中的特征。但是正如上图所示，很可能物体以不同的尺寸出现在图片中，那么能否以不同尺度的卷积来提取不同的特征呢？沿着这个想法，Inception 模块应运而生，如下图示：

![img](https://static001.geekbang.org/resource/image/91/a1/911eee05256f145209fae76d3yy23fa1.png?wh=2718x1298)



结合图示我们发现，这里是将原来的相同尺寸卷积提取特征的方式拆分为，使用 1x1、3x3、5x5 以及 3x3 的 max pooling 同时进行特征提取，然后再合并到一起。这样就做到了以多尺度的方式提取图片中的特征。作者为了降低网络的计算成本，将上述的 Inception 模块做了一步改进，在 3x3、5x5 之前与 pooling 之后添加了 1x1 卷积用来降维，从而获得了 Inception 模块的最终形态。

![img](https://static001.geekbang.org/resource/image/8f/21/8fd81403acd0d70fb5ae4a857177ee21.png?wh=2700x1312)

这里有个额外的小知识点，如果是面试，经常会被问到为什么采用 1x1 的卷积或者 1x1 卷积的作用。1x1 卷积的作用就是用来升维或者降维的。GooLeNet 就是由以上的 Inception 模块构成的一个 22 层网络。别看网络层数有 22 层，但是它参数量却比 AlexNet 与 VGG 都要少，这带来的优势就是，搭建起来的模型就很小，占的存储空间也小。具体的网络结构你可以参考它的论文。



#### ResNet
ResNet 中文意思是残差神经网络。在 2015 年的 ImageNet 比赛中，模型的分类能力首次超越人眼，1000 类图片 top-5 的错误率降低到 3.57%。在[论文](https://arxiv.org/abs/1512.03385)中作者给出了 18 层、34 层、50 层、101 层与 152 层的 ResNet。101 层的与 152 层的残差神经网络效果最好，但是受硬件设备以及推断时间的限制，50 层的残差神经网络在实际项目中更为常用。具体的网络结构你感兴趣的话可以自己看看论文全文，这里我着重带你看看这个网络的主要突破点。



#### 网络退化问题
虽说研究已经证明，随着网络深度的不断增加，网络的整体性能也会提升。如果只是单纯的增加网络，就会引起以下两个问题：第一，模型容易过拟合；第二，产生梯度消失、梯度爆炸的问题。虽然随着研究的不断发展，以上两个问题都可以被解决掉，但是 ResNet 网络的作者发现，以上两个问题被规避之后，简单的堆叠卷积层，依然不能获得很好的效果。为了验证刚才的观点，作者做了这样的一个实验。通过搭建一个普通的 20 层卷积神经网络与一个 56 层的卷积神经网络，在 CIFAR-10 数据集上进行了验证。无论训练集误差还是测试集误差，56 层的网络均高于 20 层的网络。下图来源于论文。

![img](https://static001.geekbang.org/resource/image/85/75/8503d95991270ea2d4a3ff80622af375.png?wh=2784x966)



出现这样的情况，作者认为这是网络退化造成的。网络退化是指当一个网络可以开始收敛时，随着网络层数的增加，网络的精度逐渐达到饱和，并且会迅速降低。这里精度降低的原因并不是过拟合造成的，因为如果是过拟合，上图中 56 层的在训练集上的精度应该高于 20 层的精度。作者认为这一现象并不合理，假设 20 层是一个最优的网络，通过加深到 56 层之后，理论上后面的 36 层是可以通过学习到一个恒等映射的，也就是说理论上不会学习到一个比 26 层还差的网络。所以，作者猜测网络不能很容易地学习到恒等映射 (恒等映射就是 f(x)=x)。


#### 残差学习
正如刚才所说，从网络退化问题中可以发现，通过简单堆叠卷积层似乎很难学会到恒等映射。为了改善网络退化问题，论文作者何凯明提出了一种深度残差学习的框架。因为网络不容易学习到恒等映射，所以就让它强制添加一个恒等映射，如下图所示（下图来源于论文）。

![img](https://static001.geekbang.org/resource/image/27/3b/27c8c4a22782ab29e77c36d0131f5e3b.png?wh=2034x806)


具体实现是通过一种叫做 shortcut connection 的机制来完成的。在残差神经网络中 shortcut connection 就是恒等变换，就是上图中带有 x identity 的那条曲线，包含 shortcut connection 的几层网络我们称之为残差块。残差块被定义为如下形式：

$y=F(x,W_i​)+x$

F 可以是 2 层的卷积层。也可以是 3 层的卷积层。最后作者发现，通过残差块，就可以训练出更深、更加优秀的卷积神经网络了。

### 小结
恭喜你完成了这节课的学习，让我们回顾一下这节课的主要内容。首先我们从多层感知机说起，带你认识了这个卷积神经网络的前身。之后我们一起推导出了图像分类原理的基础模型。你需要注意的是，整个模型或者网络的重点全都在卷积神经网络那块，所以这也是我们的工作重点。

![img](https://static001.geekbang.org/resource/image/8b/1a/8bbc16d51yydca581cb1d88274ec161a.png?wh=1728x664)

之后我们结合业界标杆 ImageNet 的评选情况，一起学习了一些经典的网络结构：VGG、GoogLeNet、ResNet。这里为了让你快速抓住重点，我是从每个网络解决了什么问题，各自有什么突破点展开的。也建议你课余时间多读读相关论文，做更为详细深入的了解。纵观网络结构的发展，我们不难发现，一直都是长江后浪推前浪，一代更比一代强。掌握了这些网络结构，你就是深度学习未来的弄潮儿。下节课我们再一起实践一个图像分类项目，加深你对图像分类的理解，敬请期待。



## 18 | 图像分类（下）：如何构建一个图像分类模型?


我相信经过上节课的学习，你已经了解了图像分类的原理，还初步认识了一些经典的卷积神经网络。正所谓“纸上得来终觉浅，绝知此事要躬行”，今天就让我们把上节课的理论知识应用起来，一起从数据的准备、模型训练以及模型评估，从头至尾一起来完成一个完整的图像分类项目实践。课程代码你可以从[这里](https://github.com/syuu1987/geekTime-image-classification)下载。

### 问题回顾
我们先来回顾一下问题背景，我们要解决的问题是，在众多图片中自动识别出极客时间 Logo 的图片。想要实现自动识别，首先需要分析数据集里的图片是啥样子的。那我们先来看一张包含极客时间 Logo 的图片，如下所示。

![img](https://static001.geekbang.org/resource/image/1d/2c/1d221d4d170c54625dc8d124bcc6df2c.jpeg?wh=1242x2209)

你可以看到，Logo 占整张图片的比例还是比较小的，所以说，如果这个项目是真实存在的，目标检测其实更加合适。不过，我们可以将问题稍微修改一下，修改成自动识别极客时间宣传海报，这其实就很适合图像分类任务了。

### 数据准备
相比目标检测与图像分割来说，图像分类的数据准备还是比较简单的。在图像分类中，我们只需要将每个类别的图片放到指定的文件夹里就行了。下图是我的图片组织方式，文件夹就是图片所属的类别。

![img](https://static001.geekbang.org/resource/image/cf/8e/cf664db8d071979583a7cec69a45168e.png?wh=922x334)

logo 文件夹中存放的是 10 张极客时间海报的图片。

![img](https://static001.geekbang.org/resource/image/46/27/460af80104ec4550ff1b745a1f9f6627.png?wh=1516x704)



而 others 中，理论上应该是各种其它类型的图片，但这里为了简化问题，我这个文件夹中存放的都是小猫的图片。

![img](https://static001.geekbang.org/resource/image/e6/b0/e6275aac026ce5d626c1e6ebb1fde9b0.png?wh=1494x480)


### 模型训练
好啦，数据准备就绪，我们现在进入模型训练阶段。今天我想向你介绍一个在最近 2 年非常受欢迎的一个网络——EfficientNet。它为我们提供了 B0～B7，一共 8 个不同版本的模型，这 8 个版本有着不同的参数量，在同等参数量的模型中，它的精度都是首屈一指的。因此，这 8 个版本的模型可以解决你的大多数问题。

#### EfficientNet
我先给你解读一下[EfficientNet](https://arxiv.org/pdf/1905.11946.pdf)的这篇论文，这里我着重分享论文的核心思路还有我的理解，学有余力的同学可以在课后自行阅读原文。EfficientNet 一共有 B0 到 B7，8 个模型，参数量由少到多，精度也越来越高，具体你可以看看后面的评价指标。在之前的那些网络，要么从网络的深度出发，要么从网络的宽度出发来优化网络的性能，但从来没有人将这些方向结合在一起考虑。而 EfficientNet 就做了这样的尝试，它探索了网络深度、网络宽度、图像分辨率之间的最优组合。

EfficientNet 利用一种复合的缩放手段，对网络的深度 depth、宽度 width 和分辨率 resolution 同时进行缩放（按照一定的缩放规律），来达到精度和运算复杂度 FLOPS 的权衡。但即使只探索这三个维度，搜索空间仍然很大，所以作者规定只在 B0（作者提出的 EfficientNet 的一个 Baseline）上进行放大。首先，作者比较了单独放大这三个维度中的任意一个维度效果如何。得出结论是放大网络深度或网络宽度或图像分辨率，均可提升模型精度，但是越放大，精度增加越缓慢，如下图所示：

![img](https://static001.geekbang.org/resource/image/7f/64/7ff4750599323623bb148ed8b2222b64.png?wh=1920x591)



然后，作者做了第二个实验，尝试在不同的 r（分辨率），d（深度）组合下变动 w（宽度），得到下图：

![img](https://static001.geekbang.org/resource/image/de/46/dec67f3868ddcc44e503yy13a09c1e46.png?wh=1734x1310)



结论是，得到更高的精度以及效率的关键是平衡网络宽度，网络深度，图像分辨率三个维度的缩放倍率 (d, r, w)。因此，作者提出了混合维度放大法，该方法使用一个 ϕ（混合稀疏）来决定三个维度的放大倍率。深度 depth：$d=α^ϕ$宽度 width：$w=β^ϕ$分辨率 resolution: $r=γ^ϕ$

$$
\text { s.t. } \alpha \cdot \beta^{2} \cdot \gamma^{2} \approx 2 \alpha \geq 1, \beta \geq 1, \gamma \geq 1
$$


第一步，固定 ϕ 为 1，也就是计算量为 2 倍，使用网格搜索，得到了最佳的组合，也就是 α=1.2,β=1.1,γ=1.15。第二步，固定 α=1.2,β=1.1,γ=1.15，使用不同的混合稀疏 ϕ，得到了 B1~B7。整体评估效果如下图所示：

![img](https://static001.geekbang.org/resource/image/03/14/037cd03be0995f97caa71ba079078814.png?wh=1682x1324)



从评估结果上可以看到，EfficientNet 的各个版本均超过了之前的一些经典卷积神经网络。EfficientNet v2 也已经被提出来了，有时间的话你可以自己去看看。我们不妨借助一下 EfficientNet 的[GitHub](https://github.com/lukemelas/EfficientNet-PyTorch)，它里面有训练 ImageNet 的 demo(demo/imagenet/main.py)，接下来我们一起看看它的核心代码，然后精简一下代码，把它运行起来 (Torchvision 也提供了 EfficientNet 的模型，课后你也可以自己试一试)。这里我们再回顾一下，之前说的机器学习 3 件套：

1. 数据处理
2. 模型训练（构建模型、损失函数与优化方法）
3. 模型评估


接下来我们就挨个看看这些步骤。你需要先把https://github.com/lukemelas/EfficientNet-PyTorch给克隆下来，我们只使用 efficientnet_pytorch 中的内容，它包含着模型的网络结构。之后我们来创建一个叫做 geektime 的项目文件夹，然后把 efficientnet_pytorch 放进去。在开始之前，我先把程序需要的参数给你列一下，在下面的讲解中，我们就直接使用这些参数了。当你在实现今天代码的时候，需要将这些参数补充到代码中（可以使用 argparsem 模块）。

![img](https://static001.geekbang.org/resource/image/df/7a/df24d6aa865b645f1d2aa50716e7d17a.jpg?wh=1739x1027)



好，下面让我们正式开始动手。

### 加载数据
首先是数据加载的环节，我们创建一个 dataset.py 文件，用来存储与数据有关的内容。dataset.py 如下（我省略了模块的引入）。


```
# 作者给出的标准化方法
def _norm_advprop(img):
    return img * 2.0 - 1.0

def build_transform(dest_image_size):
    normalize = transforms.Lambda(_norm_advprop)

    if not isinstance(dest_image_size, tuple):
        dest_image_size = (dest_image_size, dest_image_size)
    else:
        dest_image_size = dest_image_size

    transform = transforms.Compose([
        transforms.RandomResizedCrop(dest_image_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        normalize
    ])

    return transform

def build_data_set(dest_image_size, data):
    transform = build_transform(dest_image_size) 
    dataset=datasets.ImageFolder(data, transform=transform, target_transform=None) 

    return dataset
```
这部分代码完成的工作是，通过 build_data_set 构建数据集。这里我们使用了 torchvision.datasets.ImageFolder 来创建 Dataset。ImageFolder 能将按文件夹形式的组织数据生成到一个 Dataset。在这个例子中，我传入的训练集路径为’./data/train’，你可以看看开篇的截图。ImageFolder 会自动的将同一文件夹内的数据打上一个标签，也就是说 logo 文件夹的数据，ImageFolder 会认为是来自同一类别，others 文件夹的数据，ImageFolder 会认为是来自另外一个类别。我们这个精简版只构建了训练集的 Dataset，当你看 Efficient 官方代码的时候，在验证集的构建过程中，你需要留意一下验证集的[transforms](https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/examples/imagenet/main.py#L240-L245)。

我认为，这里这么做是有点问题的，原因是 Resize 中 size 参数如果是个 tuple 类型，则直接按照 size 的尺寸进行 resize。如果是一个 int 的时候，如果图片的 height 大于 width，则按照 (size * height/width, size) 进行 resize。在作者的原始程序中，imag_size 是个 int，而不是 tuple。所以按照这种先 resize 再 crop 的方式处理一下，对长宽比比较大的图片来说，效果不是很好。让我们实际验证一下这个想法，我将开篇的例子（也就是那张海报图）的 image_size 设定为 224 后，用上述的方式进行处理后，获得下图。

![img](https://static001.geekbang.org/resource/image/a9/e2/a93417ee476234249d0e69fb5c5f04e2.jpg?wh=224x224)

你看，是不是缺少了很多信息？所以，如果在我们的例子中使用作者的程序，就需要做一下修改。把这里的代码逻辑修改为如果 image_size 不是 tuple，先将 image_size 转换为 tuple，并且也不需要 crop 了。代码如下所示：
```
if not isinstance(image_size, tuple):
    image_size = (image_size, image_size)
else:
    image_size = image_size

transform = transforms.Compose([
    transforms.Resize(image_size, interpolation=Image.BICUBIC),
    transforms.ToTensor(),
    normalize,
])
```
训练的主程序我们定义在 main.py 中，在 main.py 中的 main() 中，进行数据的加载，如下所示。然后，我们通过 for 循环一个一个 Epoch 的调用 train 方法进行训练就可以了。


```
# 省略了一些模块的引入
from efficientnet import EfficientNet
from dataset import build_data_set

def main():
    # part1: 模型加载 (稍后补充)
    # part2: 损失函数、优化方法(稍后补充)    
    train_dataset = build_data_set(args.image_size, args.train_data)

    train_loader = torch.utils.data.DataLoader(
        train_dataset, 
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.workers,
        )

    for epoch in range(args.epochs):
        # 调用train函数进行训练，稍后补充
        train(train_loader, model, criterion, optimizer, epoch, args)
        # 模型保存        
        if epoch % args.save_interval == 0:
            if not os.path.exists(args.checkpoint_dir):
                os.mkdir(args.checkpoint_dir)
            torch.save(model.state_dict(), os.path.join(args.checkpoint_dir,
                    'checkpoint.pth.tar.epoch_%s' % epoch))
```
### 创建模型
接下来，我们来看看如何创建模型，这一步我们直接使用作者给出的 Efficient 模型。在上面代码注释中的 part1 部分，用下述代码即可加载 EfficientNet 模型。

```
    args.classes_num = 2
    if args.pretrained:
        model = EfficientNet.from_pretrained(args.arch, num_classes=args.classes_num,
                advprop=args.advprop)
        print("=> using pre-trained model '{}'".format(args.arch))
    else:
        print("=> creating model '{}'".format(args.arch))
        model = EfficientNet.from_name(args.arch, override_params={'num_classes': args.classes_num})
    # 有GPU的话，加上cuda()
    #mode.cuda()

```
这段代码是说，如果 pretrained model 参数为 True，则自动下载并加载 pretrained model 后进行训练，否则是使用随机数初始化网络。from_pretrained 与 from_name 中，都需要修改一下 num_classes，将 EfficientNet 的全连接层修改我们项目对应的类别数，这里的 args.classes_num 为 2（logo 类与 others 类）。


### 模型微调
模型微调在第 8 节课和第 14 节课时说过，这个概念比较重要，我们一起再复习一下。

Pretrained model 一般是在 ImageNet（也有可能是 COCO 或 VOC，都是公开数据集）上训练过的模型，我们可以直接把它在 ImageNet 上训练好的模型参数直接拿过来，在其基础上训练我们自己的模型，这就是模型微调。所以说，如果有 Pretrained model，我们一定会使用 Pretrained model 进行训练，收敛速度会快。

使用 Pretrained model 的时候要注意一点，在 ImageNet 上训练后的全连接层一共有 1000 个节点，所以使用 Pretrained model 的时候只使用全连接层以外的参数。在上述代码的 EfficientNet.from_pretrained 中，会通调用 load_pretrained_weights 函数，调用之前 num_classes 已经被修改为 2（logo 与 others），所以说传入 load_pretrained_weights 的 load_fc 参数为 False，也就是说不会加载全连接层的参数。load_pretrained_weights 的调用如下所示：
```
load_pretrained_weights(model, model_name, load_fc=(num_classes == 1000), advprop=advprop)
```
load_pretrained_weights 函数中包含下面这段代码，就像刚才所说，如果不加载全连接层，则删除 _fc 的 weight 与 bias：


```
if load_fc:
    ret = model.load_state_dict(state_dict, strict=False)
    assert not ret.missing_keys, 'Missing keys when loading pretrained weights: {}'.format(ret.missing_keys)
else:
    state_dict.pop('_fc.weight')
    state_dict.pop('_fc.bias')
    ret = model.load_state_dict(state_dict, strict=False)
```
### 设定损失函数与优化方法
最后要做的就是设定损失函数与优化方法了，我们将下面的代码补充到 part2 部分：

```
criterion = nn.CrossEntropyLoss() # 有GPU的话加上.cuda()

optimizer = torch.optim.SGD(model.parameters(), args.lr,
                            momentum=args.momentum,
                            weight_decay=args.weight_decay)
```
到这里，我们就完成训练的所有准备了，只要再补充好 train 函数就可以了，代码如下。下面的代码的原理我们在第 13 节课中已经讲过了，记不清的可以去回顾一下。

```
def train(train_loader, model, criterion, optimizer, epoch, args):
    # switch to train mode
    model.train()

    for i, (images, target) in enumerate(train_loader):
        # compute output
        output = model(images)
        loss = criterion(output, target)
        print('Epoch ', epoch, loss)

        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```
不过在我的程序里，保存了若干个 Epoch 的模型，我们应该怎么选择呢？这就要说到模型的评估环节。

### 模型评估
对于分类模型的评估来说，有很多评价指标，例如准确率、精确率、召回率、F1-Score 等。其中，我认为最直观、最有说服力的就是精确率与召回率，这也是我在项目中观察的主要是指标。下面我们依次来看看。

#### 混淆矩阵
在讲解精确率与召回率之前，我们先看看混淆矩阵这个概念。其实精确率与召回率就是通过它计算出来的。下表就是一个混淆矩阵，正例就是 logo 类，负例就是 others 类。

![img](https://static001.geekbang.org/resource/image/57/8b/5756d1fe45493d69ayy534da3d20088b.jpg?wh=1920x847)


根据预测结果和真实类别的组合，一共有四种情况：1.TP 是说真实类别为 Logo，模型也预测为 Logo；2.FP 是说真实类别为 Others，但模型预测为 Logo；3.FN 是说真实类别为 Logo，但模型预测为 Others；4.TN 是说真实类别为 Others，模型也预测为 Others；

精确率的计算方法为：precision=TP/(TP+FP)
召回率的计算方式为：recall=TP/(TP+FN)


精确率与召回率分别衡量了模型的不同表现，精确率说的是，如果模型认为一张图片是 Logo 类，那有多大概率是 Logo 类。而召回率衡量的是，在整个验证集中，模型能找到多少 Logo 图片。那问题来了，怎样根据这两个指标来选择模型呢？业务需求不同，我们侧重的指标就不一样。比如在我们的这个项目中，如果老板允许一部分 Logo 图片没有被识别，但是模型必须非常准，模型说一张图片是 Logo 类，那图片真实类别就有非常大的概率是 Logo 类图片，那应该侧重的就是精确率；如果老板希望把线上 Logo 类尽可能地识别出来，允许一部分图片被误识别，那应该侧重的就是召回率。在计算精确率与召回率的时候，给你分享一下我的经验。在实际项目中，我习惯把模型对每张图片的预测结果保存到一个 txt 中，这样可以比较直观地筛选一些模型的 badcase，并且验证集如果非常大，又需要调整的时候，直接更改 txt 就可以了，不需要再次让模型预测整个验证集。下面是 txt 文件的一部分，分别记录了 logo 类的概率、others 类的概率、真实类别是否为 logo、真实类别是否为 others、预测类别是否为 logo、预测类别是否为 ohters、图片名。14.jpeg 是开篇例子的那张图片，模型认为它是 Logo 的概率是 0.58476，others 类的概率是 0.41524。


```
...
0.64460 0.35540 1 0 1 0 ./data/val/logo/13.jpeg
0.58476 0.41524 1 0 1 0 ./data/val/logo/14.jpeg
...
```
下图是我训练了 10 个 Epoch 的 B0 模型，在验证集 (这里我用训练集充当了一下验证集) 上的评价效果。

![img](https://static001.geekbang.org/resource/image/95/00/95a4b9f3e9eddb32b3bc30e85dfa2500.png?wh=966x730)

通过混淆矩阵可以看到，整个验证集一共有 8+0 张图片被预测为 logo 类，所以 logo 类的精确率为 8 / (8 + 0 ) = 1；logo 类一共有 8+2 张图片，有两张预测错了，所以召回率为 8 / (8 +2) = 0.8。others 类别的计算类似，你可以自己算算看。


### 小结
恭喜你，完成了今天的学习任务。今天我们一起完成了一个图像分类项目的实践。虽然项目规模较小，但是在真实项目中的每一个环节都包含在内了，可以说是麻雀虽小，五脏俱全。下面我们回顾一下每个环节上的关键要点和实操经验。数据准备其实是最关键的一步，数据的质量直接决定了模型好坏。所以，在开始训练之前你应该对你的数据集有十足的了解才可以。例如，验证集还是否可以反映出训练集、数据中有没有脏数据、数据分布有没有偏等等。完成数据准备之后就到了模型训练，图像分类任务其实基本上都是采用主流的卷积神经网络了，很少对模型结构做一些更改。最后的模型评估环节要侧重业务场景，看业务上需要高精确还是高召回，然后再对你的模型做调整。


## 19 | 图像分割（上）：详解图像分割原理与图像分割模型

在前两节课我们完成了有关图像分类的学习与实践。今天，让我们进入到计算机视觉另外一个非常重要的应用场景——图像分割。你一定用过或听过腾讯会议或者 Zoom 之类的产品吧? 在进行会议的时候，我们可以选择对背景进行替换，如下图所示。

![img](https://static001.geekbang.org/resource/image/30/82/3066670d30116f462e54fd50376f5882.png?wh=1282x878)



在华为手机中也曾经有过人像留色的功能。

![img](https://static001.geekbang.org/resource/image/b3/ec/b33ecbc167f2bbf66902cb35cc9e3eec.png?wh=1272x862)



这些应用背后的实现都离不开今天要讲的图像分割。我们同样用两节课的篇幅进行学习，这节课主攻分割原理，下节课再把这些技能点活用到实战上，从头开始搭建一个图像分割模型。

### 图像分割
我们不妨用对比的视角，先从概念理解一下图像分割是什么。图像分类是将一张图片自动分成某一类别，而图像分割是需要将图片中的每一个像素进行分类。图像分割可以分为语义分割与实例分割，两者的区别是语义分割中只需要把每个像素点进行分类就可以了，不需要区分是否来自同一个实例，而实例分割不仅仅需要对像素点进行分类，还需要判断来自哪个实例。如下图所示，左侧为语义分割，右侧为实例分割。我们这两节课都会以语义分割来展开讲解。

![img](https://static001.geekbang.org/resource/image/75/81/75d04920aa9208d0108fd4e35332e281.png?wh=1622x540)


### 语义分割原理
语义分割原理其实与图像分类大致类似，主要有两点区别。首先是分类端（这是我自己起的名字，就是经过卷积提取特征后分类的那一块）不同，其次是网络结构有所不同。先看第一点，也就是分类端的不同。

#### 分类端
我们先回想一下图像分类的原理。你可以结合下面的示意图做理解。

![img](https://static001.geekbang.org/resource/image/39/8a/39a58yy024069706401e741b63bd808a.jpg?wh=1699x914)





输入图片经过卷积层提取特征后，最终会生成若干特征图，然后在这些特征图之后会接一个全连接层（上图中红色的圆圈），全连接层中的节点数就对应着要将图片分为几类。我们将全连接层的输出送入到 softmax 中，就可以获得每个类别的概率，然后通过概率就可以判断输入图片属于哪一个类别了。在图像分割中，同样是利用卷积层来提取特征，最终生成若干特征图。只不过最后生成的特征图的数目对应着要分割成的类别数。举一个例子，假设我们想要将输入的小猫分割出来，也就是说，这个图像分割模型有两个类别，分别是小猫与背景，如下图所示。

![img](https://static001.geekbang.org/resource/image/32/66/325b9e7e91200c1e9fba3e6a985dbc66.jpg?wh=1392x875)



最终的两个特征图中，通道 1 代表的小猫的信息，通道 2 对应着背景的信息。这里我给你再举一个例子，来说明一下如何判断每个像素的类别。假设，通道 1 中（0,0）这个位置的输出是 2，通道 2 中（0,0）这个位置的输出是 30。经过 softmax 转为概率后，通道 1（0, 0）这个位置的概率为 0，而对应通道 2 中 (0,0) 这个位置的概率为 1，我们通过概率可以判断出，在（0，0）这个位置是背景，而不是小猫。



### 网络结构
在分割网络中最终输出的特征图的大小要么是与输入的原图相同，要么就是接近输入。这么做的原因是，我们要对原图中的每个像素进行判断。当输出特征图与原图尺寸相同时，可以直接进行分割判断。当输出特征图与原图尺寸不相同时，需要将输出的特征图 resize 到原图大小。如果是从一个比较小的特征图 resize 到一个比较大的尺寸的时候，必定会丢失掉一部分信息的。所以，输出特征图的大小不能太小。这也是图像分割网络与图像分类网络的第二个不同点，在图像分类中，经过多层的特征提取，最后生成的特征图都是很小的。而在图像分割中，最后生成的特征图通常来说是接近原图的。

前文也说过，图像分割网络也是通过卷积进行提取特征的，按照之前的理论特征提取后，特征图尺寸是减小的。如果说把特征提取看做 Encoder 的话，那在图像分割中还有一步是 Decoder。Decoder 的作用就是对特征图尺寸进行还原的过程，将尺寸还原到一个比较大的尺寸。这个还原的操作对应的就是上采样。而在上采样中我们通常使用的是转置卷积。

#### 转置卷积
接下来我就带你研究一下转置卷积的计算原理，这也是这节课的重点内容。我们看下面图这个卷积计算，padding 为 0，stride 为 1。

![img](https://static001.geekbang.org/resource/image/ac/c7/ac5dfca8d13e88b3e78fb3f8b34016c7.jpg?wh=1520x865)

从之前的学习我们可以知道，卷积操作是一个多对一的运算，输出中的每一个 y 都与输入中的 4 个 x 有关。其实，转置卷积从逻辑上是卷积的一个逆过程，而不是卷积的逆运算。也就是说，转置卷积并不是使用上图中的输出 Y 与卷积核 Kernel 来获得上图中的输入 X，转置卷积只能还原出一个与输入特征图尺寸相同的特征图。我们将转置卷积中的卷积核用 k’表示，那么一个 y 会与四个 k’进行还原，如下所示：

![img](https://static001.geekbang.org/resource/image/9d/2d/9d24b6a621f2b74648248dbce723a52d.jpg?wh=1593x745)

还原尺寸的过程如下所示，下图中每个还原后的结果都对应着原始 3x3 的输入。

![img](https://static001.geekbang.org/resource/image/1c/f1/1c18d2dbc330062410fddbcc911f78f1.jpg?wh=1920x1080)

通过观察你可以发现，有些部分是重合的，对于重合部分把它们加起来就可以了，最终还原后的特征图如下：

![img](https://static001.geekbang.org/resource/image/41/01/41833998c1510d0a6c1fe239a0557101.jpg?wh=1920x1110)

将上图的结果稍作整理，整理为下面的结果，也没有做什么特殊处理，只是补了一些零：

![img](https://static001.geekbang.org/resource/image/b7/0f/b7078cdd1ce155d1bb853ab2a88c000f.jpg?wh=1920x1163)



上面的结果，我们又可以通过下面的卷积获得：



![img](https://static001.geekbang.org/resource/image/64/bb/640099fe3138893e0a9f6941c0d49bbb.jpg?wh=1920x980)



你有没有发现一件很神奇的事情，转置卷积计算又变回了卷积计算。所以，我们一起梳理一下，转置卷积的计算过程如下：

1. 对输入特征图进行补零操作。
2. 将转置卷积的卷积核上下、左右变换作为新的卷积核。
3. 利用新的卷积核在 1 的基础上进行步长为 1，padding 为 0 的卷积操作。

我们先来看一下，PyTorch 中转置卷积以及它的主要参数，再根据参数解释一下第一步 1 是如何补零的。

```
class torch.nn.ConvTranspose2d(in_channels, 
                               out_channels, 
                               kernel_size, 
                               stride=1, 
                               padding=0,
                               groups=1,
                               bias=True,
                               dilation=1)
```
其中，in_channels、out_channels、kernel_size、groups、bias 以及 dilation 与我们之前讲卷积时的参数含义是一样的（你可以回顾卷积的第9、10两节课），这里我们就不赘述了。首先，我们看一下 stride。因为转置卷积是卷积的一个逆向过程，所以这里的 stride 指的是在原图上的 stride。在我们刚才的例子里，stride 是等于 1 的，如果等于 2 时，按照同样的套路，可以转换为如下的卷积变换。 同时，我们也可以得到结论，上文中第一步，补零的操作是，在输入的特征图的行与列之间补 stride-1 个行与列的零。

![img](https://static001.geekbang.org/resource/image/39/ca/3913cdcc21afe55ccb8511951c4512ca.jpg?wh=1920x1011)



再来看 padding 操作，padding 是指要在输入特征图的周围补 dilation * (kernel_size - 1) - padding 圈零。这里用到了 dliation 参数，但是通常在转置卷积中 dilation、groups 参数使用的比较少。以上就是转置卷积的补零操作了，图片和文字双管齐下，我相信你一定能够理解它。通过上述的讲解，我们可以推导出输出特征图尺寸与输入特征图尺寸的关系：

$$
\begin{aligned}
&h_{\text {out }}=\left(h_{\text {in }}-1\right) * \text { stride }[0]-\text { padding }[0]+\text { kernel_size }[0] \\
&w_{\text {out }}=\left(w_{\text {in }}-1\right) * \text { stride }[1]-\text { padding }[1]+\text { kernel_size }[1]
\end{aligned}
$$
下面，我们借助代码来验证一下，我们讲的转置卷积是否是向我们所说的那样计算。现在有特征图 input_feat:
```
import torch
import torch.nn as nn
import numpy as np
input_feat = torch.tensor([[[[1, 2], [3, 4]]]], dtype=torch.float32)
input_feat
输出：
tensor([[[[1., 2.],
          [3., 4.]]]])
```
卷积核 k：

```
kernels = torch.tensor([[[[1, 0], [1, 1]]]], dtype=torch.float32)
kernels
输出：
tensor([[[[1., 0.],
          [1., 1.]]]])
```
stride 为 1，padding 为 0 的转置卷积：

```
convTrans = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=1, padding=0, bias = False)
convTrans.weight=nn.Parameter(kernels)
```
按照我们刚才讲的，第一步是补零操作，输入的特征图补零后为：


$$
\text { input-feat }=\left[\begin{array}{llll}
0 & 0 & 0 & 0 \\
0 & 1 & 2 & 0 \\
0 & 3 & 4 & 0 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

然后再与变换后的卷积核：
$$
\left[\begin{array}{ll}
1 & 1 \\
0 & 1
\end{array}\right]
$$

做卷积运算后，获得输出：

$$
\text { output }=\left[\begin{array}{lll}
1 & 2 & 0 \\
4 & 7 & 2 \\
3 & 7 & 4
\end{array}\right]
$$

我们再看看代码的输出，如下所示：


```
convTrans(input_feat)
输出：
tensor([[[[1., 2., 0.],
          [4., 7., 2.],
          [3., 7., 4.]]]], grad_fn=<SlowConvTranspose2DBackward>)
```
你看看是不是一样呢？


### 损失函数
说完网络结构，我们再开启图像分割里的另一个话题：损失函数。在图像分割中依然可以使用在图像分类中经常使用的交叉熵损失。在图像分类中，一张图片有一个预测结果，预测结果与真实值就可以计算出一个 Loss。而在图像分割中，真实的标签是一张二维特征图，这张特征图记录着每个像素的真实分类结果。在分割中，含有像素类别的特征图，我们一般称为 Mask。我们结合一张小猫图片的例子解释一下。对于下图中的小猫进行标记，标记后会生成它的 GT，这个 GT 就是一个 Mask。GT 是 Ground Truth 的缩写，在图像分割中我们经常使用这个词。在图像分类中与之对应的就是数据的真实标签，在图像分割中则 GT 是每个像素的真实分类，如下面的例子所示。

![img](https://static001.geekbang.org/resource/image/c2/db/c258c4f2ffd1f819c662aa1e9f6a8cdb.jpg?wh=1024x640)



GT 如下所示：

![img](https://static001.geekbang.org/resource/image/1a/a0/1a35623ceccb0750cd8058568d847fa0.png?wh=1024x640)



那在我们模型预测的 Mask 中，每个位置都会有一个预测结果，这个预测结果与 GT 中的 Mask 做比较，然后会生成一个 Loss。当然，在图像分割中不光有交叉熵损失可以用，还可以用更加有针对性的 Dice Loss，下节课我再继续展开。

### 公开数据集
刚才我们也看到了，图像分割的数据标注还是比较耗时的，具体如何标注一张语义分割所需要的图片，下节课我们再一起通过实践探索。除此之外业界也有很多比较有权威性且质量很高的公开数据集。最著名的就是 COCO 了，链接如下：https://cocodataset.org/#detection-2016。一共有 80 个类别，超过 2 万张图片。感兴趣的话，课后你可以尝试着使用它训练来看看。



### 小结
恭喜你完成了今天的学习。今天我们首先明确了语义分割要解决的问题是什么，它可以对图像中的每个像素都进行分类。然后我们对比图像分类原理，说明了语义分割的原理。它与图像分类主要有两个不同点：1. 在分类端有所不同，在图像分类中，经过卷积的特征提取后，最后会以若干个神经元的形式作为输出，每个神经元代表着对一个类别的判断情况。而语义分割，则是会输出若干的特征图，每个特征图代表着对应类别判断。2. 在图像分类的网络中，特征图是不断减小的。但是在语义分割的网络中，特征图还会有 decoder 这一步，它是将特征图进行放大的过程。实现 decoder 的方式称为上采样，在上采样中我们最常使用的就是转置卷积。

对于转置卷积，我们除了要知道它是怎么计算的之外，最重要的是要记住它不是卷积的逆运算，只是能将特征图的大小进行放大的一种卷积运算。


## 20 | 图像分割（下）：如何构建一个图像分割模型？

今天我们就从头开始，来完成一个图像分割项目。项目的内容是，对图片中的小猫进行语义分割。为了实现这个项目，我会引入一个简单但实用的网络结构：UNet。通过这节课的学习，你不但能再次体验一下完整机器学习的模型实现过程，还能实际训练一个语义分割模型。课程代码你可以从[这里](https://github.com/syuu1987/geekTime-semantic-segmentation/tree/main)下载。

### 数据部分
我们还是从机器学习开发三件套：数据、训练、评估说起。首先是数据准备部分，我们先对训练数据进行标记，然后完成数据读取工作。


#### 分割图像的标记
之前也提到过，图像分割的准备相比图像分类的准备更加复杂。那我们如何标记语义分割所需要的图片呢？在图像分割中，我们使用的每张图片都要有一张与之对应的 Mask，如下所示：

![img](https://static001.geekbang.org/resource/image/c2/db/c258c4f2ffd1f819c662aa1e9f6a8cdb.jpg?wh=1024x640)



![img](https://static001.geekbang.org/resource/image/1a/a0/1a35623ceccb0750cd8058568d847fa0.png?wh=1024x640)





上节课我们说过，Mask 就是含有像素类别的特征图。结合这里的示例图片，我们可以看到，Mask 就是原图所对应的一张图片，它的每个位置都记录着原图每个位置对应的像素类别。对于 Mask 的标记，我们需要使用到 Labelme 工具。标记的方法一共包括七步，我们挨个看一下。


第一步，下载安装[Labelme](https://github.com/wkentaro/labelme)。我们按照 Github 中的安装方式进行安装即可。如果安装比较慢的话，你可以使用国内的镜像（例如清华的）进行安装。第二步，我们要将需要标记的图⽚放到⼀个⽂件夹中。这里我是将所有猫的图片放入到 cats 文件夹中了。

![img](https://static001.geekbang.org/resource/image/f3/04/f3c8cc99959c74f363ec290558a51d04.png?wh=1722x882)


第三步，我们事先准备好⼀个 label.txt 的⽂件，⾥⾯每⼀⾏写好的需要标记的类别。我的 label.txt 如下：

```
__ignore__
_background_
cat
```
这里我要提醒你的是，前两行最好这么写。不这样写的话，使用 label2voc.py 转换就会报错，但 label2voc.py 不是唯一的数据转换方式（还可以使用 labelme_json_to_dataset，但推荐你使用 label2voc.py）。从第三行开始，表示要标记的类别。

第四步，执行后面的这条命令，就会自动启动 Labelme。


```
labelme --labels labels.txt --nodata

```
第五步，点我们击左侧的 Open Dir，选择第二步中的文件夹，就会自动导入需要标记的图片。在右下角选择需要标记的文件后，会自动显示出来，如下图所示。

![img](https://static001.geekbang.org/resource/image/0d/81/0d37591417e44d51a21bac11f409c381.png?wh=1780x1470)



第六步：点击左侧的 Create Polygons。就可以开始标注了。标记的方式就是将小猫沿着它的边界给圈出来，当形成一个闭环的时候，Labelme 会自动提示你输入类别，我们选择 cat 类即可。标记成功后，结果如下图所示。

![img](https://static001.geekbang.org/resource/image/88/y6/888dc5b576ccc9629cd1f3fd2d9cbyy6.png?wh=1676x1438)

当标记完成后，我们需要保存一下，保存之后会生成标记好的 json 文件。如下所示：

```
fangyuan@geektime data $ ls cats
1.jpeg  1.json  10.jpeg 10.json 2.jpeg  3.jpeg  4.jpeg  4.json
```
第七步，执行下面的代码，将标记好的数据转换成 Mask。


```
python label2voc.py cats cats_output --label label.txt 
```
上面代码里用到的 label2voc.py，你可以通过后面这个链接获取它：https://github.com/wkentaro/labelme/blob/main/examples/semantic_segmentation/labelme2voc.py。其中，cats 为标记好的数据，cats_output 为输出文件夹。在 cats_output 下会自动生成 4 个文件夹，我们只需要两个文件夹，分别是 JPEGImages（训练原图）与 SegmentationClassPNG（转换后的 Mask）。到此为止，我们的数据就准备好了。我一共标记了 8 张图片，如下所示。当然了，在实际的项目中需要大量标记好的图片，这里主要是为了方便演示。

![img](https://static001.geekbang.org/resource/image/7c/1d/7ca1ecafd3ce893610c5eb89yy7ca51d.png?wh=686x285)



![img](https://static001.geekbang.org/resource/image/53/67/53e088956a41de56ea1010af8a2a6d67.png?wh=703x324)


到此为止，标记工作宣告完成。

#### 数据读取
完成了标记工作之后，我们就要用 PyTorch 把这些数据给读入进来了，我们把数据相关的写在 dataset.py 中。具体还是和之前讲的一样，要继承 Dataset 类，然后实现 __init__、__len__ 和 __getitem__ 方法。dataset.py 的代码如下所示，我已经在代码中写好注释了，相信结合注释你很容易就能领会意思。

```
import os
import torch
import numpy as np

from torch.utils.data import Dataset
from PIL import Image 


class CatSegmentationDataset(Dataset):
    
    # 模型输入是3通道数据
    in_channels = 3
    # 模型输出是1通道数据
    out_channels = 1

    def __init__(
        self,
        images_dir,
        image_size=256,
    ):

        print("Reading images...")
        # 原图所在的位置
        image_root_path = images_dir + os.sep + 'JPEGImages'
        # Mask所在的位置
        mask_root_path = images_dir + os.sep + 'SegmentationClassPNG'
        # 将图片与Mask读入后，分别存在image_slices与mask_slices中
        self.image_slices = []
        self.mask_slices = []
        for im_name in os.listdir(image_root_path):
            # 原图与mask的名字是相同的，只不过是后缀不一样
            mask_name = im_name.split('.')[0] + '.png' 

            image_path = image_root_path + os.sep + im_name
            mask_path = mask_root_path + os.sep + mask_name

            im = np.asarray(Image.open(image_path).resize((image_size, image_size)))
            mask = np.asarray(Image.open(mask_path).resize((image_size, image_size)))
            self.image_slices.append(im / 255.)
            self.mask_slices.append(mask)

    def __len__(self):
        return len(self.image_slices)

    def __getitem__(self, idx):

        image = self.image_slices[idx] 
        mask = self.mask_slices[idx] 

        # tensor的顺序是（Batch_size, 通道，高，宽）而numpy读入后的顺序是(高，宽，通道)
        image = image.transpose(2, 0, 1)
        # Mask是单通道数据，所以要再加一个维度
        mask = mask[np.newaxis, :, :]

        image = image.astype(np.float32)
        mask = mask.astype(np.float32)

        return image, mask
```
然后，我们的训练代码写在 train.py 中，train.py 中的 main 函数为主函数，在 main 中，我们会调用 data_loaders 来加载数据。代码如下所示：
```
import torch

from torch.utils.data import DataLoader 
from torch.utils.data import DataLoader
from dataset import CatSegmentationDataset as Dataset

def data_loaders(args):
    dataset_train = Dataset(
        images_dir=args.images,
        image_size=args.image_size,
    )

    loader_train = DataLoader(
        dataset_train,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.workers,
    )

    return loader_train

# args是传入的参数
def main(args):
    loader_train = data_loaders(args)
```
以上就是数据处理的全部内容了。接下来，我们再来看看模型训练部分的内容。

### 模型训练
我们先来回忆一下，模型训练的老三样，分别是网络结构、损失函数和优化方法。先从网络结构说起，今天我要为你介绍一个叫做 UNet 的语义分割网络。


#### 网络结构：UNet
UNet是一个非常实用的网络。它是一个典型的 Encoder-Decoder 类型的分割网络，网络结构非常简单，如下图所示。

![img](https://static001.geekbang.org/resource/image/11/b9/1196c6fcff2fe8c601f608b01bf82ab9.jpg?wh=1920x1130)

它的网络结构虽然简单，但是效果并不“简单”，我在很多项目中都用它与一些主流的语义分割做对比，而 UNet 都取得了非常好的效果。整体网络结构跟论文中给出的示意图一样，我们重点去关注几个实现细节。第一点，图中横向蓝色的箭头，它们都是重复的相同结构，都是由两个 3x3 的卷积层组合而成的，在每层卷积之后会跟随一个 BN 层与 ReLU 的激活层。按照第 14 节课讲的，这一部分重复的组织是可以单独提取出来的。我们先来创建一个 unet.py 文件，用来定义网络结构。现在 unet.py 中创建 Block 类，它是用来定义刚才所说的重复的卷积块：

```
class Block(nn.Module):

    def __init__(self, in_channels, features):
        super(Block, self).__init__()

        self.features = features
        self.conv1 = nn.Conv2d(
                            in_channels=in_channels,
                            out_channels=features,
                            kernel_size=3,
                            padding='same',
                        )
        self.conv2 = nn.Conv2d(
                            in_channels=features,
                            out_channels=features,
                            kernel_size=3,
                            padding='same',
                        )

    def forward(self, input):
        x = self.conv1(input)
        x = nn.BatchNorm2d(num_features=self.features)(x)
        x = nn.ReLU(inplace=True)(x)
        x = self.conv2(x)
        x = nn.BatchNorm2d(num_features=self.features)(x)
        x = nn.ReLU(inplace=True)(x)

        return x

```
这里需要注意的是，同一个块内，特征图的尺寸是不变的，所以 padding 为 same。第二点，就是绿色向上的箭头，也就是上采样的过程。这块的实现就是采用上一节课所讲的转置卷积来实现的。最后一点，我们现在是要对小猫进行分割，也就是说一共有两个类别——猫与背景。对于二分类的问题，我们可以直接输出一张特征图，然后通过概率来进行判断是正例（猫）还是负例（背景），也就是下面代码中的第 71 行。同时，下述代码也补全了 unet.py 中的所有代码。


```
import torch
import torch.nn as nn

class Block(nn.Module):
    ...
class UNet(nn.Module):

    def __init__(self, in_channels=3, out_channels=1, init_features=32):
        super(UNet, self).__init__()

        features = init_features
        self.conv_encoder_1 = Block(in_channels, features)
        self.conv_encoder_2 = Block(features, features * 2)
        self.conv_encoder_3 = Block(features * 2, features * 4)
        self.conv_encoder_4 = Block(features * 4, features * 8)

        self.bottleneck = Block(features * 8, features * 16)

        self.upconv4 = nn.ConvTranspose2d(
            features * 16, features * 8, kernel_size=2, stride=2
        )
        self.conv_decoder_4 = Block((features * 8) * 2, features * 8)
        self.upconv3 = nn.ConvTranspose2d(
            features * 8, features * 4, kernel_size=2, stride=2
        )
        self.conv_decoder_3 = Block((features * 4) * 2, features * 4)
        self.upconv2 = nn.ConvTranspose2d(
            features * 4, features * 2, kernel_size=2, stride=2
        )
        self.conv_decoder_2 = Block((features * 2) * 2, features * 2)
        self.upconv1 = nn.ConvTranspose2d(
            features * 2, features, kernel_size=2, stride=2
        )
        self.decoder1 = Block(features * 2, features)

        self.conv = nn.Conv2d(
            in_channels=features, out_channels=out_channels, kernel_size=1
        )

    def forward(self, x):
        conv_encoder_1_1 = self.conv_encoder_1(x)
        conv_encoder_1_2 = nn.MaxPool2d(kernel_size=2, stride=2)(conv_encoder_1_1)

        conv_encoder_2_1 = self.conv_encoder_2(conv_encoder_1_2)
        conv_encoder_2_2 = nn.MaxPool2d(kernel_size=2, stride=2)(conv_encoder_2_1)

        conv_encoder_3_1 = self.conv_encoder_3(conv_encoder_2_2)
        conv_encoder_3_2 = nn.MaxPool2d(kernel_size=2, stride=2)(conv_encoder_3_1)

        conv_encoder_4_1 = self.conv_encoder_4(conv_encoder_3_2)
        conv_encoder_4_2 = nn.MaxPool2d(kernel_size=2, stride=2)(conv_encoder_4_1)

        bottleneck = self.bottleneck(conv_encoder_4_2)

        conv_decoder_4_1 = self.upconv4(bottleneck)
        conv_decoder_4_2 = torch.cat((conv_decoder_4_1, conv_encoder_4_1), dim=1)
        conv_decoder_4_3 = self.conv_decoder_4(conv_decoder_4_2)

        conv_decoder_3_1 = self.upconv3(conv_decoder_4_3)
        conv_decoder_3_2 = torch.cat((conv_decoder_3_1, conv_encoder_3_1), dim=1)
        conv_decoder_3_3 = self.conv_decoder_3(conv_decoder_3_2)

        conv_decoder_2_1 = self.upconv2(conv_decoder_3_3)
        conv_decoder_2_2 = torch.cat((conv_decoder_2_1, conv_encoder_2_1), dim=1)
        conv_decoder_2_3 = self.conv_decoder_2(conv_decoder_2_2)

        conv_decoder_1_1 = self.upconv1(conv_decoder_2_3)
        conv_decoder_1_2 = torch.cat((conv_decoder_1_1, conv_encoder_1_1), dim=1)
        conv_decoder_1_3 = self.decoder1(conv_decoder_1_2)

        return torch.sigmoid(self.conv(conv_decoder_1_3))

```
到这里，网络结构我们就搭建好了，然后我们来我看看损失函数。

### 损失函数：Dice Loss
这里我们来看一下语义分割中常用的损失函数，Dice Loss。想要知道这个损失函数如何生成，你需要先了解一个语义分割的评价指标（但更常用的还是后面要讲的的 mIoU），它就是 Dice 系数，常用于计算两个集合的相似度，取值范围在 0-1 之间。Dice 系数的公式如下。

$$
\text { Dice }=\frac{2|P \cap G|}{|P|+|G|}
$$

其中，∣P∩G∣ 是集合 P 与集合 G 之间交集元素的个数，∣P∣ 和 ∣G∣ 分别表示集合 P 和 G 的元素个数。分子的系数 2，这是为了抵消分母中 P 和 G 之间的共同元素。对语义分割任务而言，集合 P 就是预测值的 Mask，集合 G 就是真实值的 Mask。根据 Dice 系数我们就能设计出一种损失函数，也就是 Dice Loss。它的计算公式非常简单，如下所示。

$$
\text { DiceLoss }=1-\frac{2|P \cap G|}{|P|+|G|}
$$

从公式中可以看出，当预测值的 Mask 与 GT 越相似，损失就越小；当预测值的 Mask 与 GT 差异度越大，损失就越大。对于二分类问题，GT 只有 0 和 1 两个值。当我们直接使用模型输出的预测概率而不是使用阈值将它们转换为二值 Mask 时，这种损失函数就被称为 Soft Dice Loss。此时，∣P∩G∣ 的值近似为 GT 与预测概率矩阵的点乘。定义损失函数的代码如下。

```
import torch.nn as nn

class DiceLoss(nn.Module):
    def __init__(self):
        super(DiceLoss, self).__init__()
        self.smooth = 1.0

    def forward(self, y_pred, y_true):
        assert y_pred.size() == y_true.size()
        y_pred = y_pred[:, 0].contiguous().view(-1)
        y_true = y_true[:, 0].contiguous().view(-1)
        intersection = (y_pred * y_true).sum()
        dsc = (2. * intersection + self.smooth) / (
            y_pred.sum() + y_true.sum() + self.smooth
        )
        return 1. - dsc
```
其中，self.smooth 是一个平滑值，这是为了防止分子和分母为 0 的情况。


### 训练流程
最后，我们将模型、损失函数和优化方法串起来，看下整体的训练流程，训练的代码如下。

```
def main(args):
    makedirs(args)
    # 根据cuda可用情况选择使用cpu或gpu
    device = torch.device("cpu" if not torch.cuda.is_available() else args.device)
    # 加载训练数据
    loader_train = data_loaders(args)
    # 实例化UNet网络模型
    unet = UNet(in_channels=Dataset.in_channels, out_channels=Dataset.out_channels)
    # 将模型送入gpu或cpu中
    unet.to(device)
    # 损失函数
    dsc_loss = DiceLoss()
    # 优化方法
    optimizer = optim.Adam(unet.parameters(), lr=args.lr)

    loss_train = []
    step = 0
    # 训练n个Epoch
    for epoch in tqdm(range(args.epochs), total=args.epochs):
        unet.train()
        for i, data in enumerate(loader_train):
            step += 1
            x, y_true = data
            x, y_true = x.to(device), y_true.to(device)
            y_pred = unet(x)
            optimizer.zero_grad()
            loss = dsc_loss(y_pred, y_true)
            loss_train.append(loss.item())
            loss.backward()
            optimizer.step()
            if (step + 1) % 10 == 0:
                print('Step ', step, 'Loss', np.mean(loss_train))
                loss_train = []
        torch.save(unet, args.weights + '/unet_epoch_{}.pth'.format(epoch))
```
需要注意的点，我都在注释中进行了说明，你可以自己看一看。其实就是我们一直说的模型训练的那几件事情：数据加载、构建网络以及迭代更新网络参数。我用训练数据训练了若干个 Epoch，同时也保存了若干个模型，保存为 pth 格式。到这里就完成了模型训练的整个环节，我们可以使用保存好的模型进行预测，来看看分割效果如何。



### 模型预测
现在我们要用训练生成的模型来进行语义分割，看看结果是什么样子的。模型预测的代码如下。

```
import torch
import numpy as np

from PIL import Image

img_size = (256, 256)
# 加载模型
unet = torch.load('./weights/unet_epoch_51.pth')
unet.eval()
# 加载并处理输入图片
ori_image = Image.open('data/JPEGImages/6.jpg')
im = np.asarray(ori_image.resize(img_size))
im = im / 255.
im = im.transpose(2, 0, 1)
im = im[np.newaxis, :, :]
im = im.astype('float32')
# 模型预测
output = unet(torch.from_numpy(im)).detach().numpy()
# 模型输出转化为Mask图片
output = np.squeeze(output)
output = np.where(output>0.5, 1, 0).astype(np.uint8)
mask = Image.fromarray(output, mode='P')
mask.putpalette([0,0,0, 0,128,0])
mask = mask.resize(ori_image.size)
mask.save('output.png')

```
这段代码也很好理解。首先，用 torch.load 函数加载模型。接着加载一张待分割的图片，并进行数据预处理。然后将处理好的数据送入模型中，得到预测值 output。最后将预测值转化为可视化的 Mask 图片进行保存。输入图片也就是待分割的图片，如下左图所示。最终的输出，即可视化的 Mask 图片如下右图所示。

![img](https://static001.geekbang.org/resource/image/c2/db/c258c4f2ffd1f819c662aa1e9f6a8cdb.jpeg?wh=1024x640)



![img](https://static001.geekbang.org/resource/image/fb/61/fbfecd56d8c31589890fcd05c7995461.png?wh=1024x640)


在将预测值转化为 Mask 图片的过程中，最终预测值的概率卡了 0.5 的阈值，超过阈值的像素点，在 output 矩阵中的值为 1，表示猫的区域，没有超过阈值的像素点，在 output 矩阵中的值为 0，表示背景区域。为了将 output 矩阵输出为可视化的图像，我们使用 Image.fromarray 函数，将 Numpy 的 array 转化为 Image 格式，并将模式设置为“P”，即调色板模式。然后用 putpalette 函数来给 Image 对象上色。其中，putpalette 函数的参数是一个列表：[0, 0, 0, 0, 128, 0]，列表前三个数表示值为 0 的像素的 RGB（[0, 0, 0]表示黑色），列表后三个数表示值为 1 的像素的 RGB（[0, 128, 0]表示绿色）。这样，我们保存的 Mask 图片，黑色部分即为背景区域，绿色部分即为猫的区域。不过，这样分开的轮廓图，可能无法让我们很直观地看出语义分割的效果。所以我们将原图和 Mask 合成一张图片来看看效果。具体的代码如下。

```
image = ori_image.convert('RGBA')
mask = mask.convert('RGBA')
# 合成
image_mask = Image.blend(image, mask, 0.3)
image_mask.save("output_mask.png")
```
首先，我们将原图 image 和 Mask 图片都转换为’RGBA’带透明度的模式。然后使用 Image.blend 函数将两张图片合成一张图片，最后一个参数 0.3 表示 Mask 图片透明度为 30%，原图的透明度为 70%。最终的结果如下图所示。

![img](https://static001.geekbang.org/resource/image/4d/7b/4d804527a87cc92aab8173da85f0ff7b.png?wh=1024x640)

这样我们就可以直观地看出哪些地方预测得不准确了。


### 模型评估
在语义分割中，常用的评价指标是 mIoU。mIoU 全称为 mean Intersection over Union，即平均交并比。交并比是真实值和预测值的交集和并集之比。真实值就是我们刚刚用 labelme 标注的 Mask，也是 Ground Truth（GT）。如下左图所示。预测值就是模型预测出的 Mask，用 Prediction 表示。如后面右图所示。

![img](https://static001.geekbang.org/resource/image/61/0b/61afb79172dfa0bd652f237fd1c5bd0b.png?wh=1024x640)

![img](https://static001.geekbang.org/resource/image/c3/9d/c31ec50a2a67262728a9fd8e84a1729d.png?wh=1024x640)



交集是指真实值与预测值的交集，如下图黄色区域所示。并集是指真实值与预测值的并集，如下图蓝色区域所示。

![img](https://static001.geekbang.org/resource/image/53/29/53e81fae1ceb5f21a269c3a461c6b129.png?wh=1024x640)

通过上面几个图，我们很容易就能理解 mIoU 了。mIoU 的公式如下所示。

$$
m I o U=\frac{1}{k} \sum_{i=1}^{k} \frac{P \cap G}{P \cup G}
$$

其中，k 为所有类别数，在我们的例子中，只有“cat”一类，因此 k 为 1，我们通常不将背景计算到 mIoU 中；P 为预测值；G 是真实值。


### 小结
恭喜你，完成了今天的学习任务。这节课我们一起完成了一个图像分割项目的实践。首先，我带你了解了图像分割的数据准备，需要使用 Labelme 工具为图像做标记。数据质量的好坏决定了最终模型的质量，所以你要对数据的标注好好把握。在使用 Labelme 标记完成之后，我们可以使用 label2voc.py 将 json 转换为 Mask。之后我们学习了一种非常高效且实用的模型–UNet，并使用 PyTorch 实现了其网络结构。然后，我为你讲解了图像分割的评估指标 mIoU 和损失函数 Dice Loss。mIoU 的公式如下：

$$
m I o U=\frac{1}{k} \sum_{i=1}^{k} \frac{P \cap G}{P \cup G}
$$
mIoU 主要是从预测结果与 GT 的重合度这一角度，来衡量分割模型的好与坏的，它是图像分割中经常使用的评价指标。最后，我们使用训练好的模型进行预测，并对分割结果进行了可视化绘制。相信通过之前学习的图像分类项目与今天学习的图像分割项目，对于图像处理，你会获得更深层次的理解。


## 21 | NLP基础（上）：详解自然语言处理原理与常用算法

在之前的课程中，我们一同学习了图像分类、图像分割的相关方法，还通过实战项目小试牛刀，学完这部分内容，相信你已经对深度学习图像算法有了一个较为深入的理解。然而在实际的项目中，除了图像算法，还有一个大的问题类型，就是文字或者说语言相关的算法。这一类让程序理解人类语言表达的算法或处理方法，我们统称为自然语言处理（Natural Language Processing, NLP）。这节课，我们先来学习自然语言处理的原理和常用算法，通过这一部分的学习，以后你遇到一些常见的 NLP 问题，很容易就能想出自己的解决办法。不必担心原理、算法的内容太过理论化，我会结合自己的经验从实际应用的角度，为你建立对 NLP 的整体认知。


### NLP 的应用无处不在
NLP 研究的领域非常广泛，凡是跟语言学有关的内容都属于 NLP 的范畴。一般来说，较为多见的语言学的方向包括：词干提取、词形还原、分词、词性标注、命名实体识别、语义消歧、句法分析、指代消解、篇章分析等方面。看到这里，你可能感觉这些似乎有点太学术、太专业了，涉及语言的结构、构成甚至是性质方面的研究了。没错，这些都是 NLP 研究在语言学中的应用方面，就会给人一种比较偏研究的感觉。实际上，NLP 还有很多的研究内容是侧重“处理”和“应用”方面的，比如我们常见的就有：机器翻译、文本分类、问答系统、知识图谱、信息检索等等。

![img](https://static001.geekbang.org/resource/image/7a/69/7ac2b2fac77ac640e0bf67785b677769.jpg?wh=1920x1416)

我举一个例子，你就知道自然语言处理有多么重要了。平时我们经常会用搜索引擎，当你打开网页、在搜索框中输入自己想要了解的关键词之后，搜索引擎的后台算法逻辑就要开始一整套非常复杂的算法逻辑，这其中包括几个比较重要的方面，我们不妨结合例子来看看。在搜索引擎的输入框中，输入“亚洲的动 wu”文本，显示的内容如下图所示。别看只是一次简单的检索动作，搜索系统要完成的工作可不少。

![img](https://static001.geekbang.org/resource/image/17/f6/171aae97ee0b55eb1ce229d9b8e751f6.png?wh=1920x991)

首先，搜索引擎要对你输入的内容（query）进行解析，这就涉及到了之前提到的分词、命名实体识别、语义消歧等内容，当然还涉及到了 query 纠错，因为你错误地输入了拼音而非汉字，需要改写成正确的形式。通过一系列的算法之后，系统识别出你的需求是：寻找动物相关的搜索结果，这些结果的限定条件是它们要生活在亚洲。接着，系统就开始在数据库（或者是存储的集群中）搜索相关的实体，这些实体的查询和限制条件的过滤，就涉及信息检索、知识图谱等内容。最后，细心的同学对照搜索结果会发现，有的时候搜索引擎除了提供严格匹配的检索结果之外，还会提供一些相关内容的扩展结果，比如广告、新闻、视频等。而且很多搜索引擎的扩展搜索结果页都是个性化的，也就是根据用户的特点行为提供推荐，这些让我们的搜索结果更加丰富，体验更好。

仅仅只有这些了么？不，远远没有，因为刚才的这个过程，只是针对你这一个用户的一次检索所需要完成的一部分工作而已。更多的工作，实际是用户开始使用搜索引擎之前的构建准备阶段。为了构建搜索引擎，就需要对存储的内容进行解析，这就包括了篇章理解、文本处理、图片识别、音视频算法等环节，对每一个网页（内容）进行特征的提取，构建检索库、知识库等，这个工作量就会非常的大，涉及的面也非常广泛。由此可见，NLP 的应用真的深入到了互联网业务的方方面面，掌握了 NLP 的相关算法将会使我们的竞争力变得更强。接下来，针对自然语言处理的“应用”方面，我们一起聊聊 NLP 中文场景下的一些重要内容。

### NLP 的几个重要内容
想要让程序对文本内容进行理解，我们需要解决几个非常基础和重要的内容，分别是分词、文本表示以及关键词提取。


#### 分词
中文跟英文最大的不同在于，英文是由一个个单词构成的，单词与单词之间有空格隔断。但是中文不一样，中文单词和单词之间除了标点符号没有别的隔断。这就给程序理解文本带来了一定的难度，分词的需求也应运而生。尽管现在的深度学习已经对分词的依赖越来越小，可以通过 Word Embedding 等方式对字符（token）级的文字进行表示，但是分词的地位不会降低，单词、词组级别的文本表示仍旧有非常多的应用场景。因为我们的学习重在快速上手和实战应用，所以为了降低你的学习成本，这个专栏里我不会专门深入讲解各种分词算法细节，而是侧重于带你理解其特点，并教你学习如何用相应的工具包实现分词过程。目前网络上已经有了很多的开源或者免费的 NLP 分词工具，比如 jieba、HanLP、THULAC 等，包括腾讯、百度、阿里等公司也有相应的商业付费工具。贫穷使人理智，我们今天使用免费的 jieba 分词来做一个分词的例子，链接你可以从[这里](https://pypi.org/project/jieba/)获取。安装这个工具非常简单，只需要使用 pip 即可。

```
pip install jieba
```
jieba 的使用也很方便，我来演示一下：


```
import jieba
text = "极客时间棒呆啦"
# jieba.cut得到的是generator形式的结果
seg = jieba.cut(text)  
print(' '.join(seg)) 

# Get： 极客 时间 棒呆 啦
```
其实除了分词，jieba 还提供了词性标注的结果（pos）：
```
import jieba.posseg as posseg
text = "一天不看极客时间我就浑身难受"
# 形如pair('word, 'pos')的结果
seg = posseg.cut(text)  
print([se for se in seg]) 
# Get [pair('一天', 'm'), pair('不', 'd'), pair('看', 'v'), pair('极客', 'n'), pair('时间', 'n'), pair('我', 'r'), pair('就', 'd'), pair('浑身', 'n'), pair('难受', 'v')]
```
是不是非常简单？搞定了分词，我们接下来就要开始对文本进行表示了。



#### 文本表示的方法
在深度学习被广泛采用之前，很多传统机器学习算法结合自身的特点，使用了各种各样的文本表示。最经典的就是独热（One-hot）表示法了。在这种方法中，假定所有的文字一共有 N 个单词（也可以是字符），我们可以将每个单词赋予一个单独的序号 id，那么对于任意一个单词，我们都可以采用一个 N 位的列表（向量）对其进行表示。在这个表示中，只需要将这个单词对应序号 id 的位置为 1，其他位置为 0 即可。我还是举个例子来帮你加深理解。比方说，我们词典大小为 10000，“极客”这个单词的序号 id 为 666，那么我们就需要建立一个 10000 长度的向量，并将其中的第 666 位置为 1，其余位为 0。如下：

![img](https://static001.geekbang.org/resource/image/fb/0c/fbe6c8a89087cfa15a01689ae0f28a0c.jpg?wh=1920x602)


这时候你就会发现，在 UTF-8 编码中，中文字符有两万多个，词语数量更是天文数字，那么我们仅用字符的方式，每个字就需要两万多维度的数据来表示。推算一下，如果有一篇一万字的文章，这个数据量就很可怕了。为了进一步的压缩数据的体积，可以只使用一个向量表示文章中所有的单词，例如前面的例子，我们仍旧建立一个 10000 维的向量，把文章中出现过的所有单词的对应位置置为 1，其余为 0。

![img](https://static001.geekbang.org/resource/image/fa/91/fa4377a9886cff7179f6f6b12e9b6b91.jpg?wh=1920x602)

这样看上去，数据体积就少了很多，还有没有其他办法进一步缩减空间的占用呢？有的，例如 count-based 表示方法。在这种方法中，我们采用 v={index1: count1, index2: count2,…, index n: count n}的形式，对每一个出现的单词的序号 id 以及出现过的次数进行统计，这样一来，“极客时间”我们只需要两个 k-v 对的 dict 即可表示: {3:1, 665:1}。这种表示方法在 SVM、树模型等多个算法包中被广泛采用，因为客观来说，它确实能够大幅度地压缩空间的占用，生成起来也非常方便。但是你会发现前面这几种方法，不能表述单词的语序信息。

举个例子，“我 / 喜欢 / 你”和“你 / 喜欢 / 我”两个截然不同的意思，用前面的方法做分词的话，却会得到相同的表示结果。这时候如果搞错了，其实却是单相思的话，那岂不是很苦涩？好在现在深度学习的使用推动了 Word Embedding 的发展，基本上我们都会采用该方法进行文本表示。但是还是刚才的话，这并不意味着传统的文本表示方法就过时了。在一些小规模、轻量级的文本处理场景中，它们的作用仍旧非常大。关于文本表示中 Word Embedding 的部分，咱们在后续课程再展开讲解，这也是 NLP 深度学习的核心内容之一。让我们回到刚才的传统文本表示方法，为了实现对单词顺序信息的记录，该怎么办呢？这时我们要解决 NLP 中的一个重要问题：关键词的提取。



### 关键词的提取
关键词，顾名思义，就是能够表达文本中心内容的词语。关键词提取在检索系统、推荐系统等应用中有着极重要的地位。它是文本数据挖掘领域的一个分支，所以在摘要生成、文本分类 / 聚类等领域中也是非常基础的环节。关键词提取，主要分为有监督和无监督的方法，一般来说，我们采用无监督的方法较多一些，这是因为它不需要人工标注的语料，只需要对文本中的单词按照位置、频率、依存关系等信息进行判断，就可以实现关键词的识别和提取。无监督方法一般有三种类型，基于统计特征的方法、基于词图模型的方法，以及基于主题模型的方法，我们分别来看看。


#### 基于统计特征的方法
这种类型的方法最为经典的就是 TF-IDF（term frequency–inverse document frequency，词频 - 逆向文件频率）。该方法最核心的思想非常简单：一个单词在文件中出现的次数越多，它的重要性越高；但它在语料库中出现的频率越高，它的重要性反而越小。什么意思呢？就比如说我们有 10 篇文章，其中有 2 篇财经文章、5 篇科技、3 篇娱乐，对于单词“股票”，它在财经文章中的次数肯定非常多，但是在娱乐和科技中就非常少，这就意味着“股票”这个词就能够更好的“区分”文章的类别，那它的重要性自然也就非常高了。在 TF-IDF 中，词频（TF）表示关键字在文本中出现的频率。而逆向文件频率 (IDF)  是由包含该词语的文件的数目除以总文件数目得到的，一般情况下还会取对数对结果进行缩放。

![img](https://static001.geekbang.org/resource/image/d7/bc/d7da8269b3664f82e539e7e217730bbc.jpg?wh=1920x602)





![img](https://static001.geekbang.org/resource/image/e2/9d/e2259d8c5f87664689f3c0a433ed7e9d.png?wh=1920x602)



你可以自己先想想，这里为什么分母要加 1 呢？这是为了避免分母为 0 的情况。得到了 TF 和 IDF 之后，我们将两者相乘，就得到了 TF-IDF 了。通过 TF-IDF 不难看出，基于统计的方法的特点在于，对单词出现的次数以及分布进行数学上的统计，从而发现相应的规律和重要性（权重），并以此作为关键词提取的依据。跟分词一样，关键词的提取目前也有很多集成工具包，比如 NLTK（Natural Language Toolkit），它是一个非常著名的自然语言处理工具包，是 NLP 研究领域常用的 Python 库。我们仍旧可以使用 pip install nltk 命令来进行安装。使用 NLTK 来计算 TF-IDF 非常简单，代码如下：


```
from nltk import word_tokenize
from nltk import TextCollection

sents=['i like jike','i want to eat apple','i like lady gaga']
# 首先进行分词
sents=[word_tokenize(sent) for sent in sents]

# 构建语料库
corpus=TextCollection(sents)

# 计算TF
tf=corpus.tf('one',corpus)

# 计算IDF
idf=corpus.idf('one')

# 计算任意一个单词的TF-IDF
tf_idf=corpus.tf_idf('one',corpus)
```
你可以执行前面这段代码，看看 tf_idf 等于多少？



#### 基于词图模型的关键词提取
前面基于统计的方法采用的是对词语的频率计算的方式，但我们还可以有其他的提取思路，那就是基于词图模型的关键词提取。在这种方法中，我们首先要构建文本一个图结构，用来表示语言的词语网络。然后对语言进行网络图分析，在这个图上寻找具有重要作用的词或者短语，即关键词。该类方法中最经典的就是 TextRank 算法了，它脱胎于更为经典的网页排序算法 PageRank。关于 PageRank 算法，你可以参考这个 wiki（[戳我](https://en.wikipedia.org/wiki/PageRank)）。戳完 PageRank 之后，你就会知道，PageRank 算法的核心内容有两点：

如果一个网页被很多其他网页链接到的话，就说明这个网页比较重要，也就是 PageRank 值会相对较高。如果一个 PageRank 值很高的网页，链接到一个其他的网页，那么被链接到的网页的 PageRank 值会相应地因此而提高。而 TextRank 就非常好理解了。它跟 PageRank 的区别在于：

用句子代替网页任意两个句子的相似性可以采用类似网页转换概率的概念计算，但是也稍有不同，TextRank 用归一化的句子相似度代替了 PageRank 中相等的转移概率，所以在 TextRank 中，所有节点的转移概率不会完全相等。利用矩阵存储相似性的得分，类似于 PageRank 的矩阵 M。TextRank 的基本流程如下图所示。

![img](https://static001.geekbang.org/resource/image/44/43/44c7160354da4fcc7a93ed5f7e1bfc43.jpg?wh=1920x793)

看上去蛮复杂的，不过没有关系，刚才提到的 jieba 也有了相应的集成算法。在 jieba 中，我们可以使用如下的函数进行提取：

```
jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'), withFlag=False)
```
其中 sentence 是待处理的文本，topK 是选择最重要的 K 个关键词，基本上你用好这两个参数就足够了。

#### 基于主题模型的关键词提取
最后一种关键词提取方法就是基于主题模型的关键词提取。主题模型，这个名字看起来就高端了很多，实际上它也是一种基于统计的模型，只不过它会“发现”文档集合中出现的抽象的“主题”，并用于挖掘文本中隐藏的语义结构。LDA（Latent Dirichlet Allocation）文档主题生成模型，是最典型的基于主题模型的算法。有关 LDA 的算法的介绍，你随便搜索一下网络资料就能找到，我就不展开说了。而咱们在这节课中，将会利用已经集成好的工具包 gensim 来实现使用这个模型，代码也非常简单，我们一起来看一下：

```
from gensim import corpora, models
import jieba.posseg as jp
import jieba

input_content = [line.strip() for line open ('input.txt', 'r')]
# 老规矩，先分词
words_list = []
for text in texts:
  words = [w.word for w in jp.cut(text)]
  words_list.append(words)

# 构建文本统计信息, 遍历所有的文本，为每个不重复的单词分配序列id，同时收集该单词出现的次数
dictionary = corpora.Dictionary(words_list)

# 构建语料，将dictionary转化为一个词袋。
# corpus是一个向量的列表，向量的个数就是文档数。你可以输出看一下它内部的结构是怎样的。
corpus = [dictionary.doc2bow(words) for words in words_list]

# 开始训练LDA模型
lda_model = models.ldamodel.LdaModel(corpus=corpus, num_topics=8, id2word=dictionary, passes=10)
```
在训练环节中，num_topics 代表生成的主题的个数。id2word 即为 dictionary，它把 id 都映射成为字符串。passes 相当于深度学习中的 epoch，表示模型遍历语料库的次数。


### 小结
在这节课中，我带你一同了解了自然语言处理的应用场景以及三个经典的 NLP 基础问题。NLP 的三大经典问题包括分词、文本表示、关键词提取，正是因为这三个问题太过经典和基础，所以现在已经有了大量的集成工具供我们直接使用。但我还是那句话，有了工具，并不意味着我们不需要理解它内部的原理，学习要知其然，更需要知其所以然，这样在实际的工作中遇到问题的时候，我们才能游刃有余地解决。细心的你可能已经发现了，今天的课程里我们针对不同的问题使用了不同的工具包，分词我们使用了 jieba，关键词的提取我们使用了 gensim 和 NLTK，所以我希望你在课后有空的时候，也去了解一下这三个工具的具体使用和更多功能，因为它们真的很强大。在文本表示方法中，我们留了一个小尾巴，也就是 Word Embedding。随着深度学习的越来越广泛使用，词嵌入（Word Embedding）的方法也有了越来越多的算法和工具来实现。在后续的课程中，我会通过 BERT 的实战开发来向你介绍 Word Embedding 的训练生成和使用。


## 22 | NLP基础（下）：详解语言模型与注意力机制

在上节课中，我们一同了解了 NLP 任务中的几个经典问题，这些方法各有千秋，但是我们也发现，有的方法并不能很好地将文本中单词、词组的顺序关系或者语义关系记录下来，也就是说，不能很好地量化表示，也不能对语言内容不同部分的重要程度加以区分。那么，有没有一种方法，可以把语言变成一种数学计算过程，比如采用概率、向量等方式对语言的生成和分析加以表示呢？答案当然是肯定的，这就是这节课我们要讲到的语言模型。那如何区分语言不同部分的重要程度呢？我会从深度学习中最火热的注意力机制这一角度为你讲解。


### 语言模型
语言模型是根据语言客观事实而进行的语言抽象数学建模，是一种对应关系。很多 NLP 任务中，都涉及到一个问题：对于一个确定的概念或者表达，判断哪种表示结果是最有可能的。我们结合两个例子体会一下。

先看第一个例子，翻译文字是：今天天气很好。可能的结果是： res1 = Today is a fine day.  res2 = Today is a good day. 那么我们最后要的结果就是看概率 P(res1) 和 P(res2) 哪个更大。再比如说，问答系统提问：我什么时候才能成为亿万富翁。可能的结果有：ans1 = 白日做梦去吧。ans2 = 红烧肉得加点冰糖。那么，最后返回的答案就要选择最贴近问题内容本身的结果，这里就是前一个答案。对于上面例子中提到的问题，我们很自然就会联想到，可以使用概率统计的方法来建立一个语言模型，这种模型我们称之为统计语言模型。

#### 统计语言模型
统计语言模型的原理，简单来说就是计算一句话是自然语言（也就是一个正常句子）的概率。多年以来，专家学者构建出了非常多的语言模型，其中最为经典的就是基于马尔可夫假设 n-gram 语言模型，它也是被广泛采用的模型之一。接下来我们先从一个简单的抽象例子出发，来了解一下什么是统计语言模型。给定一个句子 S=w1,w2,w3,…,wn，则生成该句子的概率为：p(S)=p(w1,w2,w3,w4,w5,…,wn)，再由链式法则我们可以继续得到：p(S)=p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)。那么这个 p(S) 就是我们所要的统计语言模型。

那么问题来了，你会发现从 p(w1,w2,w3,w4,w5,…,wn) 到 p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1) 无非是一个概率传递的过程，有一个非常本质的问题并没有被解决，那就是语料中数据必定存在稀疏的问题，公式中的很多部分是没有统计值的，那就成了 0 了，而且参数量真的实在是太大了。怎么办呢？我们观察一下后面这句话：“我们本节课将会介绍统计语言模型及其定义”。其中“定义”这个词，是谁的定义呢？是“其”的。那“其”又是谁呢？是前面的“语言模型”的。于是我们发现，对于文本中的一个词，它出现的概率，很大程度上是由这个单词前面的一个或者几个单词决定的，这就是马尔可夫假设。

有了马尔可夫假设，我们就可以把前面的公式中的 p(wn|w1,w2,…,wn-1) 进一步简化，简化的程度取决于你认为一个单词是由前面的几个单词所决定的，如果只由前面的一个单词决定，那它就是 p(wn|wn-1)，我们称之为 bigram。如果由前面两个单词决定，则变为 p(wn|wn-2wn-1)，我们称之为 trigram。当然了，如果你认为单词的出现仅由其本身决定的，与其他单词无关，就变成了最简单的形式：p(wn)，我们称之为 unigram（一元模型）。那么现在我们知道了，基于马尔可夫链的统计语言模型，其核心就在于基于统计的条件概率。为了计算一个句子的生成概率，我们只需要统计每个词及其前面 n 个词的共现条件概率，再经过简单的乘法计算就可以得到最终结果了。


#### 神经网络语言模型
ngram 模型一定程度上减少了参数的数量，但是如果 n 比较大，或者相关语料比较少的时候，数据稀疏问题仍然不能得到很好地解决。这就好比我们把水浒传的文本放入模型中进行统计训练，最后却问模型林冲和潘金莲的关系，这就很难回答了。因为基于 ngram 的统计模型实在是收集不到两者共现的文本。这种稀疏问题靠统计肯定不行了。那么怎么办呢？这时候就轮到神经网络语言模型闪亮登场了。其实从本质上说，神经网络语言模型也是通过 ngram 来进行语言的建模，但是神经网络的学习不是通过计数统计的方法，而是通过神经网络内部神经元针对数据不断更新。具体是怎么做的呢？首先我们要定义一个向量空间，假定这个空间是一百维的，这就意味着，对于每个单词，我们可以用一个一百维的向量对其进行表示，比如 V(中国)=[0.2821289, 0.171265, 0.12378123,…,0.172364]。这样，对于任意两个单词，我们可以用距离计算的方式来评价它们之间的联系。比如我们使用 cosin 距离计算“中国”和“北京”两个单词的距离，就大概率要比“中国”和“西瓜”的距离要近得多。这样做有什么好处呢？首先，词与词之间的距离可以作为两个词之间相似性的度量。其次，向量空间隐含了很多的数学计算，比如经典的 V(国王)- V(皇后) = V(男人) - V(女人) ，这让词语之间有了更多的语义上的关联。除了维度，为了确定向量空间，我们还需要确定这个空间有多少个“点”，也就是词语的数量有多少。一般来说，我们是将语料库中出现超过一定阈值次数的单词保留，把这些留下来的单词的数量，作为空间点的量了。我们具体看看实际的操作过程中是怎么做的。我们只需要建立一个 M*N 大小的矩阵，并随机初始化里面的每一个数值，其中 M 表示的是词语的数量，N 表示词语的维度。我们把这样矩阵叫做词向量矩阵。

![img](https://static001.geekbang.org/resource/image/d9/b3/d96a7d9yy8961e7175d2a421160065b3.jpg?wh=1920x1219)

既然是随机初始化的，那么就意味着这个向量空间不能作为我们的语言模型使用。下面我们就要想办法让这个矩阵学到内容。如下图：

![img](https://static001.geekbang.org/resource/image/f2/33/f2a74ede4e9d3bca29c38580b1260d33.jpg?wh=1920x1205)



刚才我们说过，神经网络语言模型也是通过 ngram 来进行语言建模的。假定我们的 ngram 长度为 n，那么我们就从词向量矩阵中找到对应的前 n-1 个词的向量，经过若干层神经网络（包括激活函数），将这 n-1 个词的向量映射到对应的条件概率分布空间中。最后，模型就可以通过参数更新的方式，学习词向量的映射关系参数，以及上下文单词出现的条件概率参数了。简单来说就是，我们使用 n-1 个词，预测第 n 个词，并利用预测出来的词向量跟真实的词向量做损失函数并更新，就可以不断更新词向量矩阵，从而获得一个语言模型。这种类型的神经网络语言模型我们称之为前馈网络语言模型。

除了前馈网络语言模型，还有一种叫做基于 LSTM 的语言模型。下节课，我们将会通过 LSTM 完成情感分析任务的项目，进一步细化 LSTM 神经网络语言模型的训练过程，同时也会用到前面提到的词向量矩阵。现在，我们回过头来比较一下统计语言模型和神经网络语言模型的区别。统计语言模型的本质是基于词与词共现频次的统计，而神经网络语言模型则是给每个词分别赋予了向量空间的位置作为表征，从而计算它们在高维连续空间中的依赖关系。相对来说，神经网络的表示以及非线性映射，更加适合对自然语言进行建模。

### 注意力机制
如果你足够细心就会发现，在前面介绍的神经网络语言模型中，我们似乎漏掉了一个点，那就是，对于一个由 n 个单词组成的句子来说，不同位置的单词，重要性是不一样的。因此，我们需要让模型“注意”到那些相对更加重要的单词，这种方式我们称之为注意力机制，也称作 Attention 机制。既然是机制，它就不是一个算法，准确来说是一个构建网络的思路。关于注意力机制最经典的论文就是大名鼎鼎的[《Attention Is All You Need》](https://arxiv.org/abs/1706.03762)，如果你有兴趣的话，可以自行阅读这篇论文。


因为我们的专栏是以动手实践为主，重在实际应用各种机器学习的理论，所以不会对其内部的数学原理进行过多剖析，但是我们还是要知道它是怎么运作的。我们从一个例子入手，比如“我今天中午跑到了肯德基吃了仨汉堡”。这句话中，你一定对“我”、“肯德基”、“仨”、“汉堡”这几个词比较在意，不过，你是不是没注意到“跑”字？其实 Attention 机制要做的就是这件事：找到最重要的关键内容。它对网络中的输入（或者中间层）的不同位置，给予了不同的注意力或者权重，然后再通过学习，网络就可以逐渐知道哪些是重点，哪些是可以舍弃的内容了。在前面的神经网络语言模型中，对于一个确定的单词，它的向量是固定的，但是现在不一样了，因为 Attention 机制，对于同一个单词，在不同语境下它的向量表达是不一样的。下面这张图是 Attention 机制和 RNN 结合的例子。其中红色框中的是 RNN 的展开模式，我们可以看到，我 / 爱 / 极 / 客四个字的向量沿着绿色箭头的方向传递，每个字从 RNN 节点出来之后都会有一个隐藏状态 h，也就是输入节点上面的蓝色方框，在这个过程中每个状态的权重是一样的，不分大小。而蓝色框就是 Attention 机制所加入的部分，其中的每个α就是每个状态 h 的权重，有了这个权重，就可以将所有的状态 h，加权汇总到 softmax 中，然后求和得到最终输出 C。这个 C 就可以为后续的 RNN 判断权重，提供更多的计算依据。

![img](https://static001.geekbang.org/resource/image/65/54/652e45cc9bbf0643f8fb2b3d8e692e54.jpg?wh=1920x1080)

你看，这个注意力机制的原理其实很简单，但是也很巧妙。只需要增加很少的参数，就可以让模型自己弄清楚谁重要谁次要。那么下面我们来看一下抽象化之后的 Attention，如下图，这张图想必你应该在很多 attention 的相关介绍中见过了。

![img](https://static001.geekbang.org/resource/image/1e/72/1e496a1d9359774c05bf8452955b9172.jpg?wh=1920x844)



在这里输入是 query(Q), key(K), value(V)，输出是 attention value。跟刚才 Attention 与 RNN 结合的图类比，query 就是上一个时间节点传递进来的状态 Zt-1，而这个 Zt-1 就是上一个时间节点输出的编码。key 就是各个隐藏状态 h，value 也是隐藏状态 h（h1, h2…hn）。模型通过 Q 和 K 的匹配公式计算出权重，再同 V 结合就可以得到输出，这就相当于计算得到了当前的输出和所有输入的匹配度，公式如下：


$$
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}(\operatorname{sim}(Q, K)) V
$$

Attention 目前主要有两种，一种是 soft attention，一种是 hard attention。hard attention 关注的是当前词附近很小的一个区域，而 soft attention 则是关注了更大更广的范围，也更为常用。作为应用者，你了解到 Attention 的基本原理就足够使用了，因为现在已经有了很多基于 Attention 的预训练模型可以直接使用。如果你想了解更深入，可以跟我在留言区和交流群中进一步讨论。

### 小结
恭喜你完成了今天的学习任务。今天我们一起学习了语言模型的基本原理，了解了注意力机制如何给模型赋能，让模型更加“善解人意”，提高它抓取文本重点内容的能力。通过语言模型，我们可以将语言文字变成可以计算的形式，让文字之间有了更为直接的关联。有了注意力机制，我们可以让模型了解到哪些是应该被更加关注的内容，从而提高语言模型的效果。如果你对注意力机制的数学原理感兴趣，并需要更深层次的专项学习，推荐你阅读《Attention Is All You Need》这篇论文。至此我们通过两节课了解了 NLP 任务中的基础问题和重要内容，接下来的课程中，我们即将迎来动手操作环节。首先是基于 LSTM 的情感分析项目，通过这个项目，你可以了解语言模型的构建方法，并可以实现一个由情感感知能力构成的模型。之后，我们还会使用目前火热的 Bert 模型来构建一个效果非常给力的文本分类模型，敬请期待。




## 23 | 情感分析：如何使用LSTM进行情感分析？


欢迎来跟我一起学习情感分析，今天我们要讲的就是机器学习里的文本情感分析。文本情感分析又叫做观点提取、主题分析、倾向性分析等。光说概念，你可能会觉得有些抽象，我们一起来看一个生活中的应用，你一看就能明白了。比方说我们在购物网站上选购一款商品时，首先会翻阅一下商品评价，看看是否有中差评。这些评论信息表达了人们的各种情感色彩和情感倾向性，如喜、怒、哀、乐和批评、赞扬等。像这样根据评价文本，由计算机自动区分评价属于好评、中评或者说差评，背后用到的技术就是情感分析。如果你进一步观察，还会发现，在好评差评的上方还有一些标签，比如“声音大小合适”、“连接速度快”、“售后态度很好”等。这些标签其实也是计算机根据文本，自动提取的主题或者观点。

情感分析的快速发展得益于社交媒体的兴起，自 2000 年初以来，情感分析已经成长为自然语言处理（NLP）中最活跃的研究领域之一，它也被广泛应用在个性化推荐、商业决策、舆情监控等方面。今天这节课，我们将完成一个情感分析项目，一起来对影评文本做分析。


### 数据准备
现在我们手中有一批影评数据（IMDB 数据集），影评被分为两类：正面评价与负面评价。我们需要训练一个情感分析模型，对影评文本进行分类。这个问题本质上还是一个文本分类问题，研究对象是电影评论类的文本，我们需要对文本进行二分类。下面我们来看一看训练数据。IMDB（Internet Movie Database）是一个来自互联网电影数据库，其中包含了 50000 条严重两极分化的电影评论。数据集被划分为训练集和测试集，其中训练集和测试集中各有 25000 条评论，并且训练集和测试集都包含 50% 的正面评论和 50% 的消极评论。


### 如何用 Torchtext 读取数据集
我们可以利用 Torchtext 工具包来读取数据集。Torchtext 是一个包含常用的文本处理工具和常见自然语言数据集的工具包。我们可以类比之前学习过的 Torchvision 包来理解它，只不过，Torchvision 包是用来处理图像的，而 Torchtext 则是用来处理文本的。安装 Torchtext 同样很简单，我们可以使用 pip 进行安装，命令如下：

```
pip install torchtext
```
Torchtext 中包含了上面我们要使用的 IMDB 数据集，并且还有读取语料库、词转词向量、词转下标、建立相应迭代器等功能，可以满足我们对文本的处理需求。更为方便的是，Torchtext 已经把一些常见对文本处理的数据集囊括在了torchtext.datasets中，与 Torchvision 类似，使用时会自动下载、解压并解析数据。以 IMDB 为例，我们可以用后面的代码来读取数据集：
```
# 读取IMDB数据集
import torchtext
train_iter = torchtext.datasets.IMDB(root='./data', split='train')
next(train_iter)
```
torchtext.datasets.IMDB 函数有两个参数，其中：root：是一个字符串，用于指定你想要读取目标数据集的位置，如果数据集不存在，则会自动下载；split：是一个字符串或者元组，表示返回的数据集类型，是训练集、测试集或验证集，默认是  (‘train’, ‘test’)。torchtext.datasets.IMDB 函数的返回值是一个迭代器，这里我们读取了 IMDB 数据集中的训练集，共 25000 条数据，存入了变量 train_iter 中。


程序运行的结果如下图所示。我们可以看到，利用 next() 函数，读取出迭代器 train_iter 中的一条数据，每一行是情绪分类以及后面的评论文本。“neg”表示负面评价，“pos”表示正面评价。

![img](https://static001.geekbang.org/resource/image/e4/e6/e4625437cafc8bb29851fb57a9b3e8e6.png?wh=1920x616)


### 数据处理 pipelines
读取出了数据集中的评论文本和情绪分类，我们还需要将文本和分类标签处理成向量，才能被计算机读取。处理文本的一般过程是先分词，然后根据词汇表将词语转换为 id。Torchtext 为我们提供了基本的文本处理工具，包括分词器“tokenizer”和词汇表“vocab”。我们可以用下面两个函数来创建分词器和词汇表。get_tokenizer 函数的作用是创建一个分词器。将文本喂给相应的分词器，分词器就可以根据不同分词函数的规则完成分词。例如英文的分词器，就是简单按照空格和标点符号进行分词。build_vocab_from_iterator 函数可以帮助我们使用训练数据集的迭代器构建词汇表，构建好词汇表后，输入分词后的结果，即可返回每个词语的 id。创建分词器和构建词汇表的代码如下。首先我们要建立一个可以处理英文的分词器 tokenizer，然后再根据 IMDB 数据集的训练集迭代器 train_iter 建立词汇表 vocab。
```
# 创建分词器
tokenizer = torchtext.data.utils.get_tokenizer('basic_english')
print(tokenizer('here is the an example!'))
'''
输出：['here', 'is', 'the', 'an', 'example', '!']
'''

# 构建词汇表
def yield_tokens(data_iter):
    for _, text in data_iter:
        yield tokenizer(text)

vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_iter), specials=["<pad>", "<unk>"])
vocab.set_default_index(vocab["<unk>"])

print(vocab(tokenizer('here is the an example <pad> <pad>')))
'''
输出：[131, 9, 40, 464, 0, 0]
'''
```
在构建词汇表的过程中，yield_tokens 函数的作用就是依次将训练数据集中的每一条数据都进行分词处理。另外，在构建词汇表时，用户还可以利用 specials 参数自定义词表。上述代码中我们自定义了两个词语：“<pad>”和“<unk>”，分别表示占位符和未登录词。顾名思义，未登录词是指没有被收录在分词词表中的词。由于每条影评文本的长度不同，不能直接批量合成矩阵，因此需通过截断或填补占位符来固定长度。为了方便后续调用，我们使用分词器和词汇表来建立数据处理的 pipelines。文本 pipeline 用于给定一段文本，返回分词后的 id。标签 pipeline 用于将情绪分类转化为数字，即“neg”转化为 0，“pos”转化为 1。具体代码如下所示。

```
# 数据处理pipelines
text_pipeline = lambda x: vocab(tokenizer(x))
label_pipeline = lambda x: 1 if x == 'pos' else 0

print(text_pipeline('here is the an example'))
'''
输出：[131, 9, 40, 464, 0, 0 , ... , 0]
'''
print(label_pipeline('neg'))
'''
输出：0
'''
```
通过示例的输出结果，相信你很容易就能理解文本 pipeline 和标签 pipeline 的用法了。

### 生成训练数据
有了数据处理的 pipelines，接下来就是生成训练数据，也就是生成 DataLoader。这里还涉及到一个变长数据处理的问题。我们在将文本 pipeline 所生成的 id 列表转化为模型能够识别的 tensor 时，由于文本的句子是变长的，因此生成的 tensor 长度不一，无法组成矩阵。这时，我们需要限定一个句子的最大长度。例如句子的最大长度为 256 个单词，那么超过 256 个单词的句子需要做截断处理；不足 256 个单词的句子，需要统一补位，这里用“/”来填补。上面所说的这些操作，我们都可以放到 collate_batch 函数中来处理。

collate_batch 函数有什么用呢？它负责在 DataLoad 提取一个 batch 的样本时，完成一系列预处理工作：包括生成文本的 tensor、生成标签的 tensor、生成句子长度的 tensor，以及上面所说的对文本进行截断、补位操作。所以，我们将 collate_batch 函数通过参数 collate_fn 传入 DataLoader，即可实现对变长数据的处理。collate_batch 函数的定义，以及生成训练与验证 DataLoader 的代码如下。

```
# 生成训练数据
import torch
import torchtext
from torch.utils.data import DataLoader
from torch.utils.data.dataset import random_split
from torchtext.data.functional import to_map_style_dataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def collate_batch(batch):
    max_length = 256
    pad = text_pipeline('<pad>')
    label_list, text_list, length_list = [], [], []
    for (_label, _text) in batch:
         label_list.append(label_pipeline(_label))
         processed_text = text_pipeline(_text)[:max_length]
         length_list.append(len(processed_text))
         text_list.append((processed_text+pad*max_length)[:max_length])
    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = torch.tensor(text_list, dtype=torch.int64)
    length_list = torch.tensor(length_list, dtype=torch.int64)
    return label_list.to(device), text_list.to(device), length_list.to(device)

train_iter = torchtext.datasets.IMDB(root='./data', split='train')
train_dataset = to_map_style_dataset(train_iter)
num_train = int(len(train_dataset) * 0.95)
split_train_, split_valid_ = random_split(train_dataset, 
                                         [num_train, len(train_dataset) - num_train])
train_dataloader = DataLoader(split_train_, batch_size=8, shuffle=True, collate_fn=collate_batch)
valid_dataloader = DataLoader(split_valid_, batch_size=8, shuffle=False, collate_fn=collate_batch)
```
我们一起梳理一下这段代码的流程，一共是五个步骤。1. 利用 torchtext 读取 IMDB 的训练数据集，得到训练数据迭代器；2. 使用 to_map_style_dataset 函数将迭代器转化为 Dataset 类型；3. 使用 random_split 函数对 Dataset 进行划分，其中 95% 作为训练集，5% 作为验证集；4. 生成训练集的 DataLoader；5. 生成验证集的 DataLoader。

到此为止，数据部分已经全部准备完毕了，接下来我们来进行网络模型的构建。


### 模型构建
之前我们已经学过卷积神经网络的相关知识。卷积神经网络使用固定的大小矩阵作为输入（例如一张图片），然后输出一个固定大小的向量（例如不同类别的概率），因此适用于图像分类、目标检测、图像分割等等。但是除了图像之外，还有很多信息，其大小或长度并不是固定的，例如音频、视频、文本等。我们想要处理这些序列相关的数据，就要用到时序模型。比如我们今天要处理的文本数据，这就涉及一种常见的时间序列模型：循环神经网络（Recurrent Neural Network，RNN）。不过由于 RNN 自身的结构问题，在进行反向传播时，容易出现梯度消失或梯度爆炸。LSTM 网络在 RNN 结构的基础上进行了改进，通过精妙的门控制将短时记忆与长时记忆结合起来，一定程度上解决了梯度消失与梯度爆炸的问题。我们使用 LSTM 网络来进行情绪分类的预测。模型的定义如下。

```
# 定义模型
class LSTM(torch.nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional,
                 dropout_rate, pad_index=0):
        super().__init__()
        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)
        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional,
                            dropout=dropout_rate, batch_first=True)
        self.fc = torch.nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)
        self.dropout = torch.nn.Dropout(dropout_rate)
        
    def forward(self, ids, length):
        embedded = self.dropout(self.embedding(ids))
        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True, 
                                                            enforce_sorted=False)
        packed_output, (hidden, cell) = self.lstm(packed_embedded)
        output, output_length = torch.nn.utils.rnn.pad_packed_sequence(packed_output)
        if self.lstm.bidirectional:
            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))
        else:
            hidden = self.dropout(hidden[-1])
        prediction = self.fc(hidden)
        return prediction
```
网络模型的具体结构，首先是一个 Embedding 层，用来接收文本 id 的 tensor，然后是 LSTM 层，最后是一个全连接分类层。其中，bidirectional 为 True，表示网络为双向 LSTM，bidirectional 为 False，表示网络为单向 LSTM。网络模型的结构图如下所示。

![img](https://static001.geekbang.org/resource/image/f4/a8/f4013742ab70b0dc405948f07198cfa8.jpg?wh=619x404)


### 模型训练与评估
定义好网络模型的结构，我们就可以进行模型训练了。首先是实例化网络模型，参数以及具体的代码如下。

```
# 实例化模型
vocab_size = len(vocab)
embedding_dim = 300
hidden_dim = 300
output_dim = 2
n_layers = 2
bidirectional = True
dropout_rate = 0.5

model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)
model = model.to(device)
```
由于数据的情感极性共分为两类，因此这里我们要把 output_dim 的值设置为 2。接下来是定义损失函数与优化方法，代码如下。在之前的课程里也多次讲过了，所以这里不再重复。


```
# 损失函数与优化方法
lr = 5e-4
criterion = torch.nn.CrossEntropyLoss()
criterion = criterion.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=lr)
```
计算 loss 的代码如下。

```
import tqdm
import sys
import numpy as np

def train(dataloader, model, criterion, optimizer, device):
    model.train()
    epoch_losses = []
    epoch_accs = []
    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):
        (label, ids, length) = batch
        label = label.to(device)
        ids = ids.to(device)
        length = length.to(device)
        prediction = model(ids, length)
        loss = criterion(prediction, label) # loss计算
        accuracy = get_accuracy(prediction, label)
        # 梯度更新
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_losses.append(loss.item())
        epoch_accs.append(accuracy.item())
    return epoch_losses, epoch_accs

def evaluate(dataloader, model, criterion, device):
    model.eval()
    epoch_losses = []
    epoch_accs = []
    with torch.no_grad():
        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):
            (label, ids, length) = batch
            label = label.to(device)
            ids = ids.to(device)
            length = length.to(device)
            prediction = model(ids, length)
            loss = criterion(prediction, label) # loss计算
            accuracy = get_accuracy(prediction, label)
            epoch_losses.append(loss.item())
            epoch_accs.append(accuracy.item())
    return epoch_losses, epoch_accs
```
可以看到，这里训练过程与验证过程的 loss 计算，分别定义在了 train 函数和 evaluate 函数中。主要区别是训练过程有梯度的更新，而验证过程中不涉及梯度的更新，只计算 loss 即可。模型的评估我们使用 ACC，也就是准确率作为评估指标，计算 ACC 的代码如下。

```
def get_accuracy(prediction, label):
    batch_size, _ = prediction.shape
    predicted_classes = prediction.argmax(dim=-1)
    correct_predictions = predicted_classes.eq(label).sum()
    accuracy = correct_predictions / batch_size
    return accuracy

```
最后，训练过程的具体代码如下。包括计算 loss 和 ACC、保存 losses 列表和保存最优模型。

```
n_epochs = 10
best_valid_loss = float('inf')

train_losses = []
train_accs = []
valid_losses = []
valid_accs = []

for epoch in range(n_epochs):
    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)
    valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)
    train_losses.extend(train_loss)
    train_accs.extend(train_acc)
    valid_losses.extend(valid_loss)
    valid_accs.extend(valid_acc) 
    epoch_train_loss = np.mean(train_loss)
    epoch_train_acc = np.mean(train_acc)
    epoch_valid_loss = np.mean(valid_loss)
    epoch_valid_acc = np.mean(valid_acc)    
    if epoch_valid_loss < best_valid_loss:
        best_valid_loss = epoch_valid_loss
        torch.save(model.state_dict(), 'lstm.pt')   
    print(f'epoch: {epoch+1}')
    print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')
    print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')

```
我们还可以利用保存下来 train_losses 列表，绘制训练过程中的 loss 曲线，或使用第 15 课讲过的可视化工具来监控训练过程。至此，一个完整的情感分析项目已经完成了。从数据读取到模型构建与训练的方方面面，我都手把手教给了你，希望你能以此为模板，独立解决自己的问题。


### 小结
恭喜你，完成了今天的学习任务。今天我们一起完成了一个情感分析项目的实践，相当于是对自然语言处理任务的一个初探。我带你回顾一下今天学习的要点。在数据准备阶段，我们可以使用 PyTorch 提供的文本处理工具包 Torchtext。想要掌握 Torchtext 也不难，我们可以类比之前详细介绍过的 Torchvision，不懂的地方再对应去查阅[文档](https://pytorch.org/text/stable/index.html)，相信你一定可以做到举一反三。模型构建时，要根据具体的问题选择适合的神经网络。卷积神经网络常被用于处理图像作为输入的预测问题；循环神经网络常被用于处理变长的、序列相关的数据。而 LSTM 相较于 RNN，能更好地解决梯度消失与梯度爆炸的问题。在后续的课程中，我们还会讲解两大自然语言处理任务：文本分类和摘要生成，它们分别包括了判别模型和生成模型，相信那时你一定会在文本处理方面有更深层次的理解。每课一练


## 24 | 文本分类：如何使用BERT构建文本分类模型？


在第 22 节课我们一起学习了不少文本处理方面的理论，其实文本分类在机器学习领域的应用也非常广泛。比如说你现在是一个 NLP 研发工程师，老板啪地一下甩给你一大堆新闻文本数据，它们可能来源于不同的领域，比如体育、政治、经济、社会等类型。这时我们就需要对文本分类处理，方便用户快速查询自己感兴趣的内容，甚至按用户的需要定向推荐某类内容。这样的需求就非常适合用 PyTorch + BERT 处理。为什么会选择 BERT 呢？因为 BERT 是比较典型的深度学习 NLP 算法模型，也是业界使用最广泛的模型之一。接下来，我们就一起来搭建这个文本分类模型，相信我，它的效果表现非常强悍。


### 问题背景与分析
正式动手之前，我们不妨回顾一下历史。文本分类问题有很多经典解决办法。开始时就是最简单粗暴的关键词统计方法。之后又有了基于贝叶斯概率的分类方法，通过某些条件发生的概率推断某个类别的概率大小，并作为最终分类的决策依据。尽管这个思想很简单，但是意义重大，时至今日，贝叶斯方法仍旧是非常多应用场景下的好选择。之后还有支持向量机（SVM），很长一段时间，其变体和应用都在 NLP 算法应用的问题场景下占据统治地位。随着计算设备性能的提升、新的算法理论的产生等进步，一大批的诸如随机森林、LDA 主题模型、神经网络等方法纷纷涌现，可谓百家争鸣。既然有这么多方法，为什么这里我们这里推荐选用 BERT 呢？因为在很多情况下，尤其是一些复杂场景下的文本，像 BERT 这样具有强大处理能力的工具才能应对。比如说新闻文本就不好分类，因为它存在后面这些问题。1.类别多。在新闻资讯 App 中，新闻的种类是非常多的，需要产品经理按照统计、实用的原则进行文章分类体系的设计，使其类别能够覆盖所有的文本，一般来说都有 50 种甚至以上。不过为了让你把握重点，咱们先简化问题，假定文本的分类体系已经确定。2.数据不平衡。不难理解，在新闻中，社会、经济、体育、娱乐等类别的文章数量相对来说是比较多的，占据了很大的比例；而少儿、医疗等类别则相对较少，有的时候一天也没有几篇对应的文章。3.多语言。一般来说，咱们主要的语言除了中文，应该是大多数人只会英语了，不过为了考虑到新闻来源的广泛性，咱们也假定这批文本是多语言的。

刚才提到了，因为 Bert 是比较典型的深度学习 NLP 算法模型，也是业界使用最广泛的模型之一。如果拿下这么有代表性的模型，以后你学习和使用基于 Attention 的模型你也能举一反三，比如 GPT 等。想要用好 BERT，我们需要先了解它有哪些特点。



### BERT 原理与特点分析
BERT 的全称是 Bidirectional Encoder Representation from Transformers，即双向 Transformer 的 Encoder。作为一种基于 Attention 方法的模型，它最开始出现的时候可以说是抢尽了风头，在文本分类、自动对话、语义理解等十几项 NLP 任务上拿到了历史最好成绩。在第 22 节课（如果不熟悉可以回看），我们已经了解了 Attention 的基本原理，有了这个知识做基础，我们很容易就能快速掌握 BERT 的原理。这里我再快速给你回顾一下，BERT 的理论框架主要是基于论文《Attention is all you need》中提出的 Transformer，而后者的原理则是刚才提到的 Attention。其最为明显的特点，就是摒弃了传统的 RNN 和 CNN 逻辑，有效解决了 NLP 中的长期依赖问题。

![img](https://static001.geekbang.org/resource/image/57/e7/57129ea84051eaf5985535dcb97c1fe7.jpg?wh=1920x1269)


在 BERT 中，它的输入部分，也就是图片的左边，其实是由 N 个多头 Attention 组合而成。多头 Attention 是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，这有助于网络捕捉到更丰富的特征或者信息。（具体原理，一定要查阅[《Attention is all you need》](https://arxiv.org/abs/1706.03762)哦）。结合上图我们要注意的是，BERT 采用了基于 MLM 的模型训练方式，即 Mask Language Model。因为 BERT 是 Transformer 的一部分，即 encoder 环节，所以没有 decoder 的部分（其实就是 GPT）。为了解决这个问题，MLM 方式应运而生。它的思想也非常简单，就是在训练之前，随机将文本中一部分的词语（token）进行屏蔽（mask），然后在训练的过程中，使用其他没有被屏蔽的 token 对被屏蔽的 token 进行预测。

![img](https://static001.geekbang.org/resource/image/ae/84/aeed42d94750436f1dyye31f92c96584.jpg?wh=1920x700)

用过 Word2Vec 的小伙伴应该比较清楚，在 Word2Vec 中，对于同一个词语，它的向量表示是固定的，这也就是为什么会有那个经典的“国王 - 男人 + 女人 = 皇后”计算式了。但是有一个问题，“苹果”这个词，有可能是水果的苹果，也可能是电子产品的品牌，如果还是用同一个向量表示，这样就有可能产生偏差。而在 BERT 中则不一样，根据上下文的不同，对于同一个 token 给出的词向量是动态变化的，更加灵活。此外，BERT 还有多语言的优势。在以前的算法中，比如 SVM，如果要做多语言的模型，就要涉及分词、提取关键词等操作，而这些操作要求你对该语言有所了解。像阿拉伯文、日语等语言，咱们大概率是看不懂的，这会对我们最后的模型效果产生极大影响。BERT 则不需要担心这个问题，通过基于字符、字符片段、单词等不同粒度的 token 覆盖并作 WordPiece，能够覆盖上百种语言，甚至可以说，只要你能够发明出一种逻辑上自洽的语言，BERT 就能够处理。有关 WordPiece 的介绍，你可以通过[这里](https://paperswithcode.com/method/wordpiece)做拓展阅读。好，说了这么多，集高效、准确、灵活再加上用途广泛于一体的 BERT，自然而然就成为了咱们的首选，下面咱们开始正式构建一个文本分类模型。


### 安装与准备
工欲善其事，必先利其器，在开始构建模型之前，我们要安装相应的工具，然后下载对应的预先训练好的模型，同时还要了解数据的格式。

环境准备因为咱们要做的是一个基于 PyTorch 的 BERT 模型，那么就要安装对应的 python 包，这里我选择的是 hugging face 的 PyTorch 版本的 Transformers 包。你可以通过 pip 命令直接安装。


```
pip install Transformers
```
模型准备安装之后，我们打开 Transformers 的git 页面，并找到如下的文件夹。

```
src/Transformers/models/BERT
```
从这个文件夹里，我们需要找到两个很重要的文件，分别是 convert_BERT_original_tf2_checkpoint_to_PyTorch.py 和 modeling_BERT.py 文件。先来看第一个文件，你看看名字，是不是就能猜出来，它大概是用来做什么的了？没错，就是用来将原来通过 TensorfFlow 预训练的模型转换为 PyTorch 的模型。然后是 modeling_BERT.py 文件，这个文件实际上是给了你一个使用 BERT 的范例。下面，咱们开始准备模型，打开[这个地址](https://github.com/tensorflow/models/tree/master/official/nlp/bert)，你会发现在这个页面中，有几个预训练好的模型。


对照这节课的任务，我们选择的是“BERT-Base, Multilingual Cased”的版本。从 GitHub 的介绍可以看出，这个版本的 checkpoint 支持 104 种语言，是不是很厉害？当然，如果你没有多语言的需求，也可以选择其他版本的，它们的区别主要是网络的体积不同。转换完模型之后，你会发现你的本地多了三个文件，分别是 config.json、pytorch_model.bin 和 vocab.txt。我来分别给你说一说。

![img](https://static001.geekbang.org/resource/image/a8/00/a85bfecfb02108cf7e46d5bef74efe00.jpg?wh=1920x228)

1.config.json：顾名思义，该文件就是 BERT 模型的配置文件，里面记录了所有用于训练的参数设置。2.PyTorch_model.bin：模型文件本身。3.vocab.txt：词表文件。尽管 BERT 可以处理一百多种语言，但是它仍旧需要词表文件用于识别所支持语言的字符、字符串或者单词。

格式准备
现在模型准备好了，我们还要看看跟模型匹配的格式。BERT 的输入不算复杂，但是也需要了解其形式。在训练的时候，我们输入的数据不能是直接把词塞到模型里，而是要转化成后面这三种向量。1.Token embeddings：词向量。这里需要注意的是，Token embeddings 的第一个开头的 token 一定得是“[CLS]”。[CLS]作为整篇文本的语义表示，用于文本分类等任务。2.Segment embeddings。这个向量主要是用来将两句话进行区分，比如问答任务，会有问句和答句同时输入，这就需要一个能够区分两句话的操作。不过在咱们此次的分类任务中，只有一个句子。3.Position embeddings。记录了单词的位置信息。

### 模型构建
准备工作已经一切就绪，我们这就来搭建一个基于 BERT 的文本分类网络模型。这包括了网络的设计、配置、以及数据准备，这个过程也是咱们的核心过程。

网络设计
从上面提到的 modeling_BERT.py 文件中，我们可以看到，作者实际上已经给我们提供了很多种类的 NLP 任务的示例代码，咱们找到其中的“BERTForSequenceClassification”，这个分类网络我们可以直接使用，它也是最最基础的 BERT 文本分类的流程。这个过程包括了利用 BERT 得到文本的 embedding 表示、将 embedding 放入全连接层得到分类结果两部分。我们具体看一下代码。

```
class BERTForSequenceClassification(BERTPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels//类别标签数量
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)//还记得Dropout是用来做什么的吗？对，可以一定程度防止过拟合。
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)//BERT输出的embedding传入一个MLP层做分类。
        self.init_weights()

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]//这个就是经过BERT得到的中间输出。

        pooled_output = self.dropout(pooled_output)//对，就是为了减少过拟合和增加网络的健壮性。
        logits = self.classifier(pooled_output)//多层MLP输出最后的分类结果。
```
对照前面的代码，可以发现，接收到输入信息之后，BERT 返回了一个 outputs，outputs 包括了模型计算之后的全部结果，不仅有每个 token 的信息，也有整个文本的信息，这个输出具体包括以下信息。last_hidden_state 是模型最后一层输出的隐藏层状态序列。shape 是 (batch_size, sequence_length, hidden_size)。其中 hidden_size=768，这个部分的状态，就相当于利用 sequence_length * 768 维度的矩阵，记录了整个文本的计算之后的每一个 token 的结果信息。pooled_output，代表序列的第一个 token 的最后一个隐藏层的状态。shape 是 (batch_size, hidden_size)。所谓的第一个 token，就是咱们刚才提到的[CLS]标签。除了上面两个信息，还有 hidden_states、attentions、cross attentions。有兴趣的小伙伴可以去查一下，它们有何用途。通常的任务中，我们用得比较多的是 last_hidden_state 对应的信息，我们可以用 pooled_output = outputs[1]来进行获取。

至此，我们已经有了经过 BERT 计算的文本向量表示，然后我们将其输入到一个 linear 层中进行分类，就可以得到最后的分类结果了。为了提高模型的表现，我们往往会在 linear 层之前，加入一个 dropout 层，这样可以减少网络的过拟合的可能性，同时增强神经元的独立性。

模型配置
设计好网络，我们还要对模型进行配置。还记得刚才提到的 config.json 文件么？这里面就记录了 BERT 模型所需的所有配置信息，我们需要对其中的几个内容进行调整，这样模型就能知道我们到底是要做什么事情了。后面这几个字段我专门说一下。id2label：这个字段记录了类别标签和类别名称的映射关系。label2id：这个字段记录了类别名称和类别标签的映射关系。num_labels_cate：类别的数量。

数据准备
模型网络设计好了，配置文件也搞定了，下面我们就要开始数据准备这一步了。这里的数据准备是指将文本转换为 BERT 能够识别的形式，即前面提到的三种向量，在代码中，对应的就是 input_ids、token_type_ids、attention_mask。为了生成这些数据，我们需要在 git 中找到“src/Transformers/data/processors/utils.py”文件，在这个文件中，我们要用到以下几个内容。1.InputExample：它用于记录单个训练数据的文本内容的结构。2.DataProcessor：通过这个类中的函数，我们可以将训练数据集的文本，表示为多个 InputExample 组成的数据集合。3.get_features：用于把 InputExample 数据转换成 BERT 能够理解的数据结构的关键函数。我们具体来看一下各个数据都怎么生成的。

input_ids 记录了输入 token 对应在 vocab.txt 的 id 序号，它是通过如下的代码得到的。


```
input_ids = tokenizer.encode(
                  example.text_a,
                  add_special_tokens=True,
                  max_length=min(max_length, tokenizer.max_len),
              )
```
而 attention_mask 记录了属于第一个句子的 token 信息，通过如下代码得到。


```
attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)
attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)
```
另外，不要忘记记录文本类别的信息（label）。你可以自己想想看，能否按照 utils.py 文件中的声明方式，构建出对应的 label 信息呢？

### 模型训练
到目前为止，我们有了网络结构定义（BERTForSequenceClassification）、数据集合（get_features），现在就可以开始编写实现训练过程的代码了。

选择优化器
首先我们来选择优化器，代码如下。我们要对网络中的所有权重参数进行设置，这样优化器就可以知道哪些参数是要进行优化的。然后我们将参数 list 放到优化器中，BERT 使用的是 AdamW 优化器。


```
param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
        ]
optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
```
这部分的代码，主要是为了选择一个合适咱们模型任务的优化器，并将网络中的参数设定好学习率。

构建训练过程逻辑
训练的过程逻辑是非常简单的，只需要两个 for 循环，分别代表 epoch 和 batch，然后在最内部增加一个训练核心语句，以及一个梯度更新语句，这就足够了。可以看到，PyTorch 在工程代码的实现上，封装得非常完善和简练。

```
for epoch in trange(0, args.num_train_epochs):
  model.train()//一定别忘了要把模型设置为训练状态。
  for step, batch in enumerate(tqdm(train_dataLoader, desc='Iteration')):
    step_loss = training_step(batch)//训练的核心环节
    tr_loss += step_loss[0]
    optimizer.step()
    optimizer.zero_grad()
```
训练的核心环节
训练的核心环节，你需要关注两个部分，分别是通过网络得到预测输出，也就是 logits，以及基于 logits 计算得到的 loss，loss 是整个模型使用梯度更新需要用到的数据。

```
def training_step(batch):
  input_ids, token_type_ids, attention_mask, labels = batch
  input_ids = input_ids.to(device)//将数据发送到GPU
  token_type_ids = token_type_ids.to(device)
  attention_mask = attention_mask.to(device)
  labels = labels_voc.to(device)
        
  logits = model(input_ids,
        token_type_ids=token_type_ids, 
        attention_mask=attention_mask, 
        labels=labels)
  loss_fct = BCEWithLogitsLoss()
  loss = loss_fct(logits.view(-1, num_labels_cate), labels.view(-1, num_labels_cate).float())
  loss.backward()
```
至此，咱们已经快速构建出了一个 BERT 分类器所需的所有关键代码。但是仍旧有一些小小的环节需要你来完善，比如 training_step 代码块中的 device，是怎么得到的呢？回顾一下咱们之前学习的内容，相信你一定可以做得到。

### 小结
恭喜你完成了这节课的学习，尽管现在 GitHub 上已经有了很多已经封装得非常完善的 BERT 代码，你也可以很快实现一个最基本的 NLP 算法流程，但是我仍希望你能够抽出时间，好好看一下 Transformer 中的模型代码，这会对你的技术提升有非常大的助益。这节课我们学习了如何用 PyTorch 快速构建一个基本的文本分类模型，想要实现这个过程，你需要了解 BERT 的预训练模型的获取以及转化、分类网络的设计方法、训练过程的编写。整个过程不难，但是却可以让你快速上手，了解 PyTorch 在 NLP 方面如何应用。除了技术本身，业务方面的考虑我们也要注意。比如新闻文本的多语言、数据不平衡等问题，模型有时不能解决所有的问题，因此你还需要学习一些数据预处理的技巧，这包括很多技术和算法方面的内容。即使我列出一份长长的学习清单，也可能会挂一漏万，所以数据预处理方面的知识我建议你重点关注以下内容：建议你需要花一些时间去学习 NumPy 和 Pandas 的使用，这样才能更加得心应手地处理数据；你还可以多学习一些常见的数据挖掘算法（比如决策树、KNN、支持向量机等）；另外，深度学习的广泛使用，其实仍旧非常需要传统机器学习算法的背后支撑，也建议你多多了解。



## 25 | 摘要：如何快速实现自动文摘生成？

当我们打开某个新闻 APP 或者某个网站时，常常被这样的标题所吸引：“震惊了十亿人”、“一定要读完，跟你的生命有关！”等。但是当我们点进去却发现都是标题党，实际内容大相径庭！这时候你可能会想，如果有一种工具能帮助我们提炼文章的关键内容，那我们就不会再受到标题党的影响了。其实想要实现这个工具并不复杂，用自动文摘技术就能解决。自动文摘充斥着我们生活的方方面面，它可用于热点新闻聚合、新闻推荐、语音播报、APP 消息 Push、智能写作等场景。今天我们要讲的这个自然语言处理任务，就是自动文摘生成。


### 问题背景
自动文摘技术，就是自动提炼出一些句子来概括整篇文章的大意，用户通过读摘要就可以了解到原文要表达的意思。

### 抽取与生成
自动文摘有两种解决方案：一种是抽取式（Extractive）的，就是从原文中提取一些关键的句子，组合成一篇摘要；另外一种是生成式（Abstractive）的，也是这节课我们重点要讲的内容，这种方式需要计算机通读原文后，在理解整篇文章内容的基础上，使用简短连贯的语言将原文的主要内容表达出来，即会产生原文中没有出现的词和句子。现阶段，抽取式的摘要目前已经相对成熟，但是抽取质量及内容流畅度都不够理想。随着深度学习的研究，生成式摘要的质量和流畅度都有很大提升，但目前也受到原文本长度过长、抽取内容不佳等限制，生成的摘要与人工摘要相比，还有相当的差距。语言的表达方式多种多样，机器生成的摘要可能和人工摘要并不相同，那么如何衡量自动摘要的好坏呢？这就涉及到摘要的评价指标。

### 评价指标
评价自动摘要的效果通常使用 ROUGE（Recall Oriented Understudy for Gisting Evaluation）评价。ROUGE 评价法参考了机器翻译自动评价方法，并且考虑了 N-gram 共同出现的程度。这个方法具体是这样设计的：首先由多个专家分别生成人工摘要，构成标准摘要集；然后对比系统生成的自动摘要与人工生成的标准摘要，通过统计二者之间重叠的基本单元（n 元语法、词序或词对）的数目，来评价摘要的质量。通过与多专家人工摘要的对比，提高评价系统的稳定性和健壮性。


ROUGE 主要包括以下 4 种评价指标：1.ROUGE-N，基于 n-gram 的共现统计；2.ROUGE-L，基于最长公共子串；3.ROUGE-S，基于顺序词对统计；4.ROUGE-W，在 ROUGE-L 的基础上，考虑串的连续匹配。 

了解了自动文摘的种类与评价指标，下面我们再来认识一个用于自动文摘生成的模型——BART。它的名字和上节课讲过的 BERT 非常像，我们先来看看它有哪些特点。


### BART 原理与特点分析
BART 的全称是 Bidirectional and Auto-Regressive Transformers（双向自回归变压器）。它是由 Facebook AI 在 2019 年提出的一个新的预训练模型，结合了双向 Transformer 和自回归 Transformer，在文本生成相关任务中达到了 SOTA 的结果。你可以通过这个链接查看[相关论文](https://arxiv.org/abs/1910.13461)。我们已经熟知了论文《Attention is all you need》中提出的 Transformer。Transformer 左半边为 Encoder，右半边为 Decoder。Encoder 和 Decoder 的结构分别如下图（a）、（b）所示。

![img](https://static001.geekbang.org/resource/image/02/c7/02d7541cd7b6f0a8b8c35efb3e4d74c7.jpg?wh=1920x901)

Encoder 负责将原始文本进行 self-attention，并获得句子中每个词的词向量，最经典的 Encoder 架构就是上节课所学习的 BERT，但是单独 Encoder 结构不适用于文本生成任务。Decoder 的输入与输出之间错开一个位置，这是为了模拟文本生成时，不能让模型看到未来的词，这种方式称为 Auto-Regressive（自回归）。例如 GPT 等基于 Decoder 结构的模型通常适用于做文本生成任务，但是无法学习双向的上下文语境信息。BART 模型就是将 Encoder 和 Decoder 结合在一起的一种 sequence-to-sequence 结构，它的主要结构如下图所示。

![img](https://static001.geekbang.org/resource/image/a6/99/a663e08e28803d6059aae93fea1a0699.png?wh=1898x778)


BART 模型的结构看似与 Transformer 没什么不同，主要区别在于 BART 的预训练阶段。首先在 Encoder 端使用多种噪声对原始文本进行破坏，然后再使用 Decoder 重建原始文本。由于 BART 本身就是在 sequence-to-sequence 的基础上构建并且进行预训练，它天然就适合做序列生成的任务，例如：问答、文本摘要、机器翻译等。在生成任务上获得进步的同时，在一些文本理解类任务上它也可以取得很好的效果。下面我们进入实战阶段，利用 BART 来实现自动文摘生成。


### 快速文摘生成
这里我们还是使用 hugging face 的 Transformers 工具包。具体的安装过程，上一节课已经介绍过了。Transformers 工具包为快速使用自动文摘生成模型提供了 pipeline API。pipeline 聚合了文本预处理步骤与训练好的自动文摘生成模型。利用 Transformers 的 pipeline，我们只需短短几行代码，就可以快速生成文本摘要。下面是一个使用 pipeline 生成文摘的例子，代码如下。

```
from transformers import pipeline

summarizer = pipeline("summarization")

ARTICLE = """ New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.
A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.
Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared "I do" five more times, sometimes only within two weeks of each other.
In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her "first and only" marriage.
Barrientos, now 39, is facing two criminal counts of "offering a false instrument for filing in the first degree," referring to her false statements on the
2010 marriage license application, according to court documents.
Prosecutors said the marriages were part of an immigration scam.
On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.
After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective
Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.
All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.
Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.
Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.
The case was referred to the Bronx District Attorney\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\'s
Investigation Division. Seven of the men are from so-called "red-flagged" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.
Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.
If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.
"""

print(summarizer(ARTICLE, max_length=130, min_length=30))
'''
输出:
[{'summary_text': ' Liana Barrientos, 39, is charged with two counts of "offering a false instrument for filing in
the first degree" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and
2002 . At one time, she was married to eight men at once, prosecutors say .'}]
'''
```
第 3 行代码的作用是构建一个自动文摘的 pipeline，pipeline 会自动下载并缓存训练好的自动文摘生成模型。这个自动文摘生成模型是 BART 模型在 CNN/Daily Mail 数据集上训练得到的。第 5~22 行代码是待生成摘要的文章原文。第 24 行代码是针对文摘原文自动生成文摘，其中参数 max_length 和 min_length 限制了文摘的最大和最小长度，输出的结果如上面代码注释所示。如果你不想使用 Transformers 提供的预训练模型，而是想使用自己的模型或其它任意模型也很简单。具体代码如下。

```
py
from transformers import BartTokenizer, BartForConditionalGeneration

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

inputs = tokenizer([ARTICLE], max_length=1024, return_tensors='pt')

# 生成文摘
summary_ids = model.generate(inputs['input_ids'], max_length=130, early_stopping=True)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(summary)
```
流程是一共包括四步，我们分别看一下。第一步是实例化一个 BART 的模型和分词器对象。BartForConditionalGeneration 类是 BART 模型用于摘要生成的类，BartTokenizer 是 BART 的分词器，它们都有 from_pretrained() 方法，可以加载预训练模型。from_pretrained() 函数需要传入一个字符串作为参数，这个字符串可以是本地模型的路径，也可以是上传到 Hugging Face 模型库中的模型名字。这里“facebook/bart-large-cnn”是 Facebook 利用 CNN/Daily Mail 数据集训练的 BART 模型，模型具体细节你可以参考这里。接下来是第二步，对原始文本进行分词。我们可以利用分词器对象 tokenizer 对原始文本 ARTICLE 进行分词，并得到词语 id 的 Tensor。return_tensors='pt’表示返回值是 PyTorch 的 Tensor。第三步，使用 generate() 方法生成摘要。其中参数 max_length 限制了生成摘要的最大长度，early_stopping 表示生成过程是否可提前停止。generate() 方法的输出是摘要词语的 id。最后一步，利用分词器解码得到最终的摘要文本。利用 tokenizer.decode() 函数，将词语 id 转换为词语文本。其中参数 skip_special_tokens 表示是否去掉“”、"<\s>"等一些特殊 token。


### Fine-tuning BART
下面我们来看一看如何用自己的数据集来训练 BART 模型。


#### 模型加载
模型加载部分和之前讲的一样，不再过多重复。这里我们要利用 BartForConditionalGeneration 类的 from_pretrained() 函数，加载一个 BART 模型。模型加载的代码如下。这里我们会在 Facebook 训练好的摘要模型上，继续 Fine-tuning。


```
from transformers import BartTokenizer, BartForConditionalGeneration

tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
model = BartForConditionalGeneration.from_pretrained('facebook/facebook/bart-large-cnn')
```
#### 数据准备
接下来，是数据准备。我们先来回顾一下，之前学习过的读取文本数据集的方式。在第 6 课中，我们学习过使用 PyTorch 原生的的 Dataset 类读取数据集；在第 23 课中，我们学习了使用 Torchtext 工具torchtext.datasets来读取数据集。今天，我们还要学习一种新的数据读取工具：Datasets 库。Datasets 库也是由 hugging face 团队开发的，旨在轻松访问与共享数据集。官方的文档在[这里](https://huggingface.co/docs/datasets/index.html)，有兴趣了解更多的同学可以去看看。Datasets 库的安装同样非常简单。可以使用 pip 安装：

```
pip install datasets
```
或使用 conda 进行安装：

```
conda install -c huggingface -c conda-forge datasets
```
Datasets 库中同样包括常见数据集，而且帮我们封装好了读取数据集的操作。我们来看一个读取 IMDB 数据集（第 23 课讲过）的训练数据的示例：

```
import datasets
train_dataset = datasets.load_dataset("imdb", split="train")
print(train_dataset.column_names)
'''
输出：
['label', 'text']
'''
```
用 load_dataset() 函数来加载数据集，它的参数是数据集的名字或本地文件的路径，split 参数用于指定加载训练集、测试集或验证集。我们还可以从不止一个 csv 文件中加载数据：

```
data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)
print(datasets)
'''
示例输出：(实际输出与此不同)
{train: Dataset({
    features: ['idx', 'text', 'summary'],
    num_rows: 3668
})
test: Dataset({
    features: ['idx', 'text', 'summary'],
    num_rows: 1725
})
}
'''
```
通过参数 data_files 指定训练集、测试集或验证集所需加载的文件路径即可。我们可以使用 map() 函数来对数据集进行一些预处理操作，示例如下：


```
def add_prefix(example):
    example['text'] = 'My sentence: ' + example['text']
    return example
updated_dataset = dataset.map(add_prefix)
updated_dataset['train']['text'][:5]
'''
示例输出：
['My sentence: Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',
'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
]
'''
```
我们首先定义了一个 add_prefix() 函数，其作用是为数据集的“text”字段加上一个前缀“My sentence: ”。然后调用数据集 dataset 的 map 方法，可以看到输出中“text”字段的内容前面都增加了指定前缀。下面我们来看一看，使用自定义的数据集 fine-tuning BART 模型应该怎么做。具体的代码如下：


```
from transformers.modeling_bart import shift_tokens_right

dataset = ... # Datasets的对象，数据集需有'text'和'summary'字段，并包含训练集和验证集

def convert_to_features(example_batch):
    input_encodings = tokenizer.batch_encode_plus(example_batch['text'], pad_to_max_length=True, max_length=1024, truncation=True))
    target_encodings = tokenizer.batch_encode_plus(example_batch['summary'], pad_to_max_length=True, max_length=1024, truncation=True))
    
    labels = target_encodings['input_ids']
    decoder_input_ids = shift_tokens_right(labels, model.config.pad_token_id)
    labels[labels[:, :] == model.config.pad_token_id] = -100
    
    encodings = {
        'input_ids': input_encodings['input_ids'],
        'attention_mask': input_encodings['attention_mask'],
        'decoder_input_ids': decoder_input_ids,
        'labels': labels,
    }

    return encodings

dataset = dataset.map(convert_to_features, batched=True)
columns = ['input_ids', 'labels', 'decoder_input_ids','attention_mask',] 
dataset.set_format(type='torch', columns=columns)
```
首先需要加载自定义的数据集，你要注意的是，这个数据集需要包含原文和摘要两个字段，并且包含训练集和验证集。加载数据集的方法可以用我们刚刚讲过的 load_dataset() 函数。由于加载的数据需要经过一系列预处理操作，比如通过分词器进行分词等等的处理后，才能送入到模型中，因此我们需要定义一个函数 convert_to_features() 来处理原文和摘要文本。convert_to_features() 函数中的主要操作就是调用 tokenizer 来将文本转化为词语 id。需要注意的是，代码第 10 行中有一个 shift_tokens_right() 函数，它的作用就是我们在原理中介绍过的 Auto-Regressive，目的是将 Decoder 的输入向后移一个位置。然后我们需要调用 dataset.map() 函数来对数据集进行预处理操作，参数 batched=True 表示支持在 batch 数据上操作。最后再利用 set_format() 函数生成选择训练所需的数据字段，并生成 PyTroch 的 Tensor。到这里，数据准备的工作就告一段落了。


#### 模型训练
做好了前面的准备工作，最后我们来看模型训练部分。Transformers 工具已经帮我们封装了用于训练文本生成模型的 Seq2SeqTrainer 类，无需我们自己再去定义损失函数与优化方法了。具体的训练代码如下。

```
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    output_dir='./models/bart-summarizer',# 模型输出目录
    num_train_epochs=1, # 训练轮数
    per_device_train_batch_size=1, # 训练过程bach_size
    per_device_eval_batch_size=1, # 评估过程bach_size
    warmup_steps=500, # 学习率相关参数
    weight_decay=0.01, # 学习率相关参数
    logging_dir='./logs', # 日志目录
)

trainer = Seq2SeqTrainer(
    model=model,                       
    args=training_args,                  
    train_dataset=dataset['train'],        
    eval_dataset=dataset['validation']   
)

trainer.train()
```
首先我们要定义一个训练参数的对象，关于训练的相关参数都通过 Seq2SeqTrainingArguments 类进行定义。然后再实例化一个 Seq2SeqTrainer 类的对象，将模型和训练数据作为参数传入其中。最后调用 train() 方法，即可一键开始训练。


### 小结
恭喜你完成了今天的学习任务，同时也完成了 PyTorch 的全部学习内容。这节课我们先一起了解了 BART 模型的原理与特点，这个模型是一个非常实用的预训练模型，能够帮助我们实现文本摘要生成。然后我们结合实例，学习了如何用 PyTorch 快速构建一个自动文摘生成项目，包括利用 Transformers 的 pipeline 快速生成文本摘要和 Fine-tuning BART 模型。因为 BART 模型具有自回归 Transformer 的结构，所以它不只可以用于摘要生成，还适用于其它文本生成类的项目，例如机器翻译、对话生成等。相信理解了它的基本原理与模型 Fine-tuning 的基本流程，你可以很容易地利用 BART 完成文本生成类的任务，期待你举一反三，亲手做更多的实验。通过实战篇的学习，我们一共探讨、实现了 2 个图像项目和 3 个自然语言处理项目。如何基于 PyTorch 搭建自己的深度学习网络，相信你已经了然于胸了。当我们解决实际的问题时，首先要从原理出发，选择适合的模型，PyTorch 只是一个工具，辅助我们实现自己需要的网络。除了自动摘要外，其他四个项目的共通思路都是把问题转化为分类问题。图像、文本分类不必细说，图像分割其实是判别一个像素是属于哪一个类别，情感分析则是判别文本是积极类还是消极类。而自动摘要则是生成模型，通常是基于 sequence-to-sequence 的结构来实现。这些是我通过一系列的实战训练，最终希望你领会到的模型搭建思路。





































































































































































































































































































































































































































































































































































































































































