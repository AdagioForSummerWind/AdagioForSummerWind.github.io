---
title: "MachineLlearning_base_01"
date: 2021-11-07T18:32:54+08:00
lastmod: 2022-01-01
tags: [machine_learning]
categories: [School courses]
slug: machine learning review
draft: true

---

# 复习

- 概念题、简答题、计算题（老师讲的计算题很少，PPT上的计算题，**注意**作业题以及上课例子）、算法补全。
- 每一章核心算法、核心知识点
- 重要地是掌握前面几章，后面几章更偏向算法的。
- 基本概念估计占分较大

## 1. 绪论：

- 什么是学习？什么是机器学习？（什么是学习什么是机器学习）机器学习的应用。
- 对于某类任务 T 和性能度量 P，如果一个计算机程序在 T 上以 P 衡量的性能随着经验 E 而自我完善，那么我们称这个计算机程序在从经验 E 学习。
- 机器学习的设计方法
  - 选择训练经验
  - 选择目标函数
  - 选择目标函数的表示
  - 选择函数逼近算法
  - 最终完成设计

## 2. 概念学习（两算法、两概念）：

- 概念的定义，概念学习的定义（实例集X、假设集H、目标概念c、训练样例集D）。两个重要算法：**要知道两个算法的基本步骤，看懂给的例子，在例子上出题**。

- 概念学习是指从有关某个布尔函数的输入输出训练样例中，推断出该布尔函数
- 假设的一般到特殊序
- Find-S:

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226211459.png)   ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226211551.png)

- Find-S输出的假设只是H中能够拟合训练样例的多个假设中的一个，而在候选消除算法中，输出的是与训练样例一致的所有假设的集合。

- 候选消除算法在描述这一集合时不需要明确列举其所有成员（而是用特殊和一般边界确定一个假设的集合），这也归功于more-general-than偏序结构,来维护一个一致假设集合的简洁表示。

- Find-S 和候选消除算法在训练数据有噪声时性能较差

- **变型空间**：与训练样例一致的所有假设构成的集合$\mathrm{VS}_{\mathrm{H}, \mathrm{D} }=\{\mathrm{h} \in \mathrm{H} \mid$ Consistent $(\mathrm{h}, \mathrm{D})\}$
  - 什么是一致、什么是满足（），其差别
    - ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20220104184200.png)

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20220101081722.png)正例 S → 一般化
  反例 G → 特殊化

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20220101082759.png)



- 归纳偏置：……附加前提的集合
  - 无偏学习器：能够表达实例集X的所有可能的子集
- **必考，什么是无偏学习的无用性？**:学习器如果不对目标概念的形式做预先的假定，它从根本上无法对未见实例进行分类
- **必考，候选消除算法的归纳偏置(某种形式的预先假定)是什么？**:目标概念 c 必须包含在给定的假设空间 H 中。

## 3. 决策树学习：

- 什么是决策树学习，决策树学习的**四个特点**、**熵、信息增益**、决策树的构造最重要的是：**如何进行节点的特征变量选择，进行分裂，分裂时必须知道熵和信息增益。ID3、必考，看懂例子，知道如何去计算。ID3算法和候选消除算法的差别**。限定偏置和优选偏置。**什么是过拟合现象（文字不会画个图给个公式也行），如何避免：增加序列样本、剪枝（预剪枝，后剪枝两算法：错误率降低修剪，规则后修剪）**。决策树的归纳偏置（选择较短的树）。

- 决策树学习是一种逼近离散值目标函数的方法，在这种方法中学习到的函数被表示为一棵决策树。（**用学习的方式构造一棵决策树，离散目标预测**）

- 决策树学习的**四个特点**
  - 通过在树中从根结点到叶结点对实例进行排序从而实现分类
  - 叶子结点提供分类
  - 每个结点指定一种属性  
  - 每个分支都对应一个属性值

- 通常决策树学习最适合具有以下特征的问题:
  
  - 实例是由“属性-值”对（pair）表示的。
  - 目标函数具有离散的输出值
  - 可能需要析取的描述
  - 训练数据可以包含错误
  - 训练数据可以包含缺少属性值的实例

- 决策树模型的三个关键过程
  
  - 特征变量的选择，通过信息增益等方法来选择合适的分裂变量
  - 决策树的生成，例如ID3, CART等算法提供了决策树的一套完整算法
  - 决策树的剪枝，通过剪枝来避免过度拟合，提升在未见数据上的预测效果

- 从信息量到信息熵
  
  - 信息中包含信息量的大小与该消息所表达的事件发生概率有关
  - 如果是必然事件（100%出现），则该消息所包含的信息量为0
  - 如果不可能发生(或概率极低)，则该消息的信息量为无穷大
  - 熵(Entropy)概念最早出现在热力学,热熵表示分子状态混乱程度的物理量。在这就是一个用来描述离散变量不确定性的概念
  - 香农将热力学的熵引入到信息论领域，并提出“信息熵”概念，用于表示信息不确定性的一种度量。无论是热熵还是信息熵都是表达事物的混乱程度，越高越混乱，越低与有序
  - ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226214658.png)

- 信息增益表示在知道某个特征之后使得的不确定性减少的程度
  
  - 知道某个特征之前的熵与知道某个特征之后的熵的差
  - 根据某个变量将样本划分为多个子集，分割前后样本数据的熵之差为信息增益，也即不确定性的减少量
  - 信息增益越高，表示该变量对样本数据的分类效果越好

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226215849.png)

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226220130.png)

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226220956.png)

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226223025.png)

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226223121.png)

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226223141.png)

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226223219.png)

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226223435.png)

- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211226223457.png)

- **ID3 vs. 候选消除算法(C-E)**
  - ID3搜索一个完整的假设空间，但不完全搜索这个空间。
    - C-E搜索一个不完整的H，但它完全地搜索这个空间
  - ID3的归纳偏差是假设按其搜索策略排序的结果(搜索策略）
    - C-E的归纳偏差是其假设表征的表达能力的结果(搜索空间)

- 限定偏置和优选偏置  
  
  - ID3 的归纳偏置是对某种假设（例如，对于较短的假设）胜过其他假设的一种优选（preference），它对最终可列举的假设没有硬性限制。这种类型的偏置通常被称为优选偏置（preference bias）（或叫搜索偏置（search bias））。
  - 相反，候选消除算法的偏置是对待考虑假设的一种限定（restriction）。这种形式的偏置通常被称为限定偏置（或者叫语言偏置（language bias））。

- 过拟合现象：
  - **什么是过拟合，必考**：
    - 假设 h属于H 过拟合 训练数据，如果存在一个可替换其他假设h’属于H 满足
    - ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20220104192047.png)
  - 什么导致了过拟合
    - 训练样例中的随机错误或噪声
    - 无噪声，但少量样例被关联到叶子结点
      - 发生巧合的规律性，使得一些属性恰巧可以很好地分割样例，但却与实际的目标函数无关
  - **如何避免过度拟合?**
    - 增加数据规模：数据包含足够多的差异性，但不现实
    - 剪枝的策略
      - 预剪枝：在对训练数据进行完美划分之前，及早停止增长树
      - 后剪枝：允许树过度拟合训练数据，然后对树进行再修剪
      - 第一种剪枝方法似乎更直接，但很难精确估计何时停止增长，第二种剪枝方法在实践中更为成功

- 后剪枝两算法：错误率降低修剪，规则后修剪
  
  - 错误率降低修剪
    
    - 修剪一个结点的步骤
      - 修剪(每一个结点作为候选对象)
        - 删除以该结点为根的子树，使其成为叶结点
      - 指定结点的分类
        - 把和该结点关联的训练样例的最常见分类赋给它
      - 删除一个结点
        - 如果修剪后的树在验证集上的性能不比原始树差，才删剪
      - 重复这一步骤，直到进一步修剪有害为止  
    - 可用数据被划分为3个子集
      - 训练样本、用于修剪树的验证实例
      - 用于估计实际精度的测试实例
      - 这种方法需要大量的数据
  
  - 规则后修剪法
    
    - 步骤
      
      - (1) 从训练推断出一棵树
        
        - 增长树尽可能地拟合训练数据，并允许发生过拟合
      
      - (2) 将树转换成等价的规则集合
        
        - 为从根到叶子的每个路径创建一条规则
      
      - (3) 修剪(泛化)每一条规则
        
        - 删除任何能够导致其估计精度提高的先行词(先决条件)
      
      - (4) 对修剪后的规则进行排序
        
        - 根据他们所估计的准确性进行排序
        
        - 按照这个顺序，应用这些规则对后续实例进行分类

## 4. 神经网络：

- 定义，来源（生物神经系统）。单多层感知器模型及其能解决什么问题等（线性不可分的时候单个感知器是无法解决的）。两大类优化做法：感知器法则和Delta算法。梯度下降法则（标准（计算时采用所有计算数据的误差，计算完之后做回传）、随机（三段式下降，每次只更新每一个数据））。**必考，实际问题需要知道：怎么去计算梯度，怎么去推导（作业）（可能有大题：给一个实际问题目标函数，推导，怎么去得到最终的解。最优解是什么样子的）**。**标准梯度下降和随机梯度下降的三点不同。**多层网络：可阈值单元（**sigmoid函数，其导数，损失函数的定义，梯度计算**）反向传播算法：见总结PPT。BP算法：**过拟合现象，怎么去解决过拟合现象。前馈网络的表征能力**

- 神经网络提供了一种逼近实值、离散值和向量值函数的健壮方法
  
  - 对于某些类型的问题，例如学习解释复杂的现实世界的传感器数据，神经网络是非常有效的

- 单多层感知器模型：
  
  - 单层感知器只拥有一层M-P神经元, 即只包含输入层和输出层，输入层接受外界输入信号后传递给输出层, 输出层是M-P神经元，进行激活处理。
  - 单层感知器可以表示所有的原子布尔函数
    - 布尔函数:m-of-n函数(和或)
      - AND OR NAND NOR
  - 一些布尔函数无法用单层的感知器表示
    - 线性不可分的布尔函数(例如异或)是不可表示的
  - 要解决非线性可分问题，就需要使用多层功能神经元。比如两层感知器，输入层与输出层之间的一层神经元，被称为隐含层，隐含层和输出层神经元都是拥有激活函数的功能神经元。

- 感知器法则：![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211227151138.png)

- 感知器法则vsDelta法则：![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20211231163532.png)

- 多层前馈神经网络的定义：
  
  - 多层前馈神经网络又名多层前馈全互连接网
    - 多层是指：除了输入层和输出层以外，还存在一个或者多个隐含层。每层神经元与下一层神经元全互联, 神经元之间不存在同层连接也不存在跨层连接。
    - 前馈是指：外界信号从输入层，隐含层与输出层神经元对信号进行加工, 最终结果由输出层神经元输出，不存在信号的逆向传播。
    - 全连接神经网络是指：任意两个相邻层之间的神经元全部互相连接，每层神经元与下层神经元全互连接。
    - 学习：根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的“阈值”
    - 多层网络：包含隐层的网络

- **标准GD和SGD的关键区别（书上讲3点）**
  
  - 权值更新的训练样例数量：GD(汇总所有数据误差)SGD(每个实例)
  - 每个权值更新的步长：GD更大
  - SGD逃离局部极小值的机会更大

- 前馈网络的表征能力：
  
  - 布尔函数: 使用两层单元的网络
    - 表示任意布尔函数的通用方案
      - 对于每个输入创建不同的隐单元，设置其权值使仅在特定的向量输入到网络时，这个单元被激活，即输出单元实现为一个仅由所希望输入模式激活的或门（OR gate）
  - 有界的连续函数(Cybenko 1989,Hornik 1989)
    - 两层单元网络：隐藏层sigmoid单元+ 输出层线性单元网络
    - 任意小的误差逼近
  - 任意函数(Cybenko 1988)
    - 三层单元的网络(两层sigmoid单元+ 一个线性输出层)
    - 任意精度逼近
  - **注意**：梯度下降是从一个初始的权值开始的，因此搜索范围里的网络权向量可能不包含所有的权向量

## 5. 贝叶斯学习：

- 贝叶斯的基础知识：先验概率，后验概率，类条件概率。贝叶斯法则：贝叶斯公式。极大后验（怎么推导出来的），极大似然（条件是什么）。一致学习器：**什么是一致学习器**。极大似然与最小误差平方假设：他们的关系是什么，怎么推导出来的，**了解即可**。
用于预测概率的极大似然假设：**知道什么是交叉熵损失，结论：神经网络学习所学习预测概率是基于预测概率的的极大似然假设，理解即可**。**两个分类器需要会计算，给一个实例知道如何计算，课堂例子，目测会考。一定要手推一遍**
- $P(h \mid D)=\frac{P(D \mid h) P(h)}{P(D)}$ 
- 极大后验假设（MAP）：确定 MAP 假设的方法是用贝叶斯公式计算每个候选假设的后验概率。
  - $h_{M A P} \equiv \underset{h \in H}{\arg \max } P(h \mid D)$
  - $=\underset{h \in H}{\arg \max } \frac{P(D \mid h) P(h)}{P(D)}$
  - $=\underset{h \in H}{\arg \max } P(D \mid h) P(h)$
  - 最后一步我们去掉了 P(D)，因为它是不依赖于 h 的常量
- 极大似然假设：P(D|h)常称为给定 h 时数据 D 的似然度（likelihood），而使 P(D|h)最大的假设被称为极大似然（maximum likelihood，ML）假设 $h_{ML}$。
  - $h_{M L} \equiv \underset{h \in H}{\arg \max } P(D \mid h)$
- 一致学习器：
  - 某学习算法被称为一致学习器，说明它输出的假设在训练例上有零错误率。
  - 如果假定 H 上有均匀的先验概率（即P(hi)=P(hj)，对所有的i,j），且训练数据是确定性的和无噪声的（即当D和h一致时，P(D|h)=1），否则为 0）时，任意一致学习器将输出一个 MAP 假设。
- 三道作业题
- ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20220101104945.png)

## 6. 基于实例的学习:

- 最近邻算法，K近邻算法。局部加权回归（解释清楚什么是局部，加权，回归，**一般方法**） 。径向基函数（一个核函数，用于函数逼近），它和神经网络之间的关系（了解即可）。**消极学习和积极学习的概念，定义，对应的实例**。
- 最近邻算法基本思想：以全部训练样本作为“代表点”，计算测试样本与这些“代表点”，即所有样本的距离，并以最近邻者的类别作为决策。
- 最近邻法的一个明显的推广是k-近邻法：
  - k近邻一般采用k为奇数
  - 训练样本集的数量总是有限的，有时候多一个或者少一个训练样本将会对测试样本分类的结果产生较大的影响，因此k近邻法的错误率是比较难以计算
  - 算法思想：取未知样本 $x_q$ 的k个近邻，看这k个近邻中多数属于哪一类，就把 $x_q$ 归为哪一类
- k-近邻改进
  - 根据距离对k-近邻样本进行加权
  - 距离近的近邻赋予较大的权值
- 距离加权k-NN对噪声有鲁棒性
  - 可以平滑掉孤立的噪声样例的影响
- k-近邻的归纳偏置
  - $x_q$的类别与在欧式空间中它附近的实例的类别相似
- k-近邻实践中的问题: 维度灾难(curse of dimensionality)
  - 基于实例的所有属性计算距离
  - 实际中分类相关属性只占少数
    - 依赖于全属性的相似性度量会误导k-近邻分类
    - 存在很多不相关属性所导致的难题
- 局部加权回归：
  - **使用附近的或距离加权的训练样例来构造目标函数f的局部逼近**
  - 是对k-近邻的推广
  - 局部：目标函数的逼近仅仅根据查询点附近的数据
  - 加权：逼近贡献是距离加权
  - 回归：逼近实数值函数的问题
  - 一般方法：
    - 构造一个逼近函数 𝑓መ 拟合xq邻域内的训练样例
    - 计算𝑓መ(𝑥𝑞) ，即xq估计的目标输出
    - 删除𝑓መ(𝑥𝑞) 
- 径向基函数：
  - 用于函数逼近、类似于局部加权回归、ANN
  - ![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20220101111531.png)
- 消极学习和积极学习：
  - 三种消极学习（lazy learning）方法：k-近邻算法、局部加权回归和基于案例的推理
  - 之所以称这些方法是消极的，是因为它们延迟了如何从训练数据中泛化的决策，直到遇到一个新的查询。
  - 一种积极学习方法：学习径向基函数网络的方法。
  - 之所以称这种方法是积极的，是因为它在见到新的查询之前就做好了泛化的工作——在训练时提交了定义其目标函数逼近的网络结构和权值
  - 同样的理解，本书其他章节讨论的所有其他算法都是积极学习算法（例如，反向传播算法、C4.5）
- 消极方法在训练时一般需要较少的计算，但在预测新查询的目标值时需要更多的计算时间。
- 在归纳偏置方面消极和积极方法是否有实质性的差异：
  - 消极方法在决定如何从训练数据 D 中泛化时考虑查询实例 xq。
  - 积极方法不能做到这一点，因为在见到查询实例 xq 前，它们已经选取了对目标函数的（全局）逼近。

## 7. 遗传算法（GA）:
- 定义及演化。基本思路与基本架构。研究了什么问题。最佳假设拥有最优的**适应度**。如何设计适应度函数。怎么去评估。遗传算法的两个算子（**交叉、变异，表达的是什么意思**）。**算法流程（重点）**。遗传算法基本原型：**流程图需要背下来，核心点**表示假设，遗传算子。适应度函数和假设选择。**什么是拥挤，怎么解决拥挤**
- 遗传算法
  - 算法基本流程：![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20220101113051.png)
  - 研究的问题：搜索候选假设空间并确定最佳假设
  - 最佳假设：最优的适应度
  - 共同结构：迭代更新一个假设池，评估适应度、生成新群体
    - 举例（PPTp16）：二进制编码、生成群体、选择、交叉、变异、回到选择、直至收敛
- 遗传算法原型
  - 基本算法（初始化群体、评估、迭代（选择、交叉、变异、更新、评估）、返回适应度最高假设）
  - 表示假设
    - 假设经常用二进制位串表示
      - 便于用变异和交叉遗传算子来操作
  - 遗传算子
    - 交叉
      - 二进制交叉：是指二进制编码情况下所采用的交叉操作，它主要包括单点交叉、两点交叉和均匀交叉等方法
      - 实值交叉
      - **交叉算子**：![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20220101114047.png)
    - 变异
      - 二进制变异：该变异方法是先随机地产生一个变异位，然后将该变异位置上的基因值由“0”变为“1”，或由“1”变为“0”，产生一个新的个体。例如：设变异前的个体为A=0 0 1 1 0 1，若随机产生的变异位置是2，则该个体的第2位由“0”变为“1”。 变异后的新的个体是A'= 0 1 1 1 0 1 。
      - 变异算子：
        - 对单亲的位串产生随机的小变化
        - 方法是选取一个位，然后取反
        - 变异经常是应用了交叉之后进行的
  - 适应度函数和假设选择方式
    - 适应度函数定义了候选假设的排序标准
    - 选择某假设概率的一个可能的方法：![](https://raw.githubusercontent.com/JF-011101/Image_hosting_rep/main/20220101114251.png)
- 假设空间搜索（拥挤）
  - 降低拥挤的策略
    - 修改选择函数
    - 适应度共享
    - 对可重组生成后代的个体种类进行限制
## 8. 特征选择：

- 定义，意义，分类（冗余特征不讲）。方法（简单的图，要知道）。**重点**：子集搜索（前向（从空集开始搜），后向（把所有数据点作为初始点/候选集进行搜索，搜索到最优），双向三者相应算法）。子集评价：通过计算属性子集的信息增益，评价子集的好坏。特征选择的三大类（需要区分开）：过滤式（训练过程和分离器/学习器无关）包裹式（直接把最终使用的学习器性能作为特征子集的评价准则）嵌入式（线性回归和稀释表示），及对应算法(**有什么样的代表性算法知道就可以**)。

## 9.线性分类器：

- 什么是线性可分不可分（**要会形容，形容不出来画张图也行**）。线性判别函数的形式：一般形式、齐次形式。**重点，**注意**几何意义和几何解释，需要知道二维、三维是怎么解释的，画一张图让你去解释一下**。线性分类器设计的主要步骤。**多类决策问题非常重要，拆分策略：一对余，一对一，多对多。出一个问题，怎么去判断属于哪一类别，是否出现在了不可预测的区域里边，看懂例子**。两个准则（知道即可）：最小距离准则、感知器准测、最小平方误差准则。比较难的地方（知道就行）：解区、解区的限制。两个求解去记一下（梯度下降算法求解，最小平方误差准则函数求解）。
  
## 10. 回归学习：

- 定义，线性回归的概念，回归的概念（线性函数，代价函数，知道概念，后面设计算法的时候可能用到）。逻辑回归和softmax回归都是线性回归的变形，是重点。**正则化的意义是什么**。
  
## 11. 无监督学习（相对比较难的内容）：

- **完全理解K-均值算法。完全掌握**，影响因素较难知道即可。考试不会过多涉及K指的选择、初始划分、聚类计算……

- **有监督和无监督的区别和关系。聚类的概念和聚类的目的（重点）**，聚类的表示（聚类的一般表示方法(聚类中心) & 分类模型 & 最常见的值 & 任意形状的聚类）。层次聚类：合并（自下而上）聚类，分裂（自上而下）聚类，知道思想即可。聚类簇之间的距离计算（掌握前三类即可，单链接方法（最近）全链接方法（最远）平均链接方法（多个点）聚类中心方法（聚类中心））
