# Cp_practice

# 编译原理实战课

## 开篇词 | 在真实世界的编译器中游历
宫文学 2020-06-01

你好，我是宫文学，一名技术创业者，现在是北京物演科技的 CEO，很高兴在这里跟你见面。我在 IT 领域里已经工作有 20 多年了。这其中，我个人比较感兴趣的，也是倾注时间精力最多的，是做基础平台类的软件，比如国内最早一批的 BPM 平台、BI 平台，以及低代码 / 无代码开发平台（那时还没有这个名字）等。这些软件之所以会被称为平台，很重要的原因就是拥有很强的定制能力，比如流程定制、界面定制、业务逻辑定制，等等。而这些定制能力，依托的就是编译技术。

在前几年，我参与了一些与微服务有关的项目。我发现，前些年大家普遍关注那些技术问题，比如有状态的服务（Stateful Service）的横向扩展问题，在云原生、Serverless、FaaS 等新技术满天飞的时代，不但没能被很好地解决，反而更恶化了。究其原因就是，状态管理还是被简单地交给数据库，而云计算的场景使得数据库的压力更大了，数据库原来在性能和扩展能力上的短板，就更加显著了。


而比较好的解决思路之一，就是大胆采用新的计算范式，发明新的计算机语言，所以我也有意想自己动手搞一下。

我从去年开始做设计，已经鼓捣了一阵了，采用了一些很前卫的理念，比如**云原生的并发调度、基于 Actor 的数据管理等**。总的目标，是要让开发云原生的、有状态的应用，像开发一个简单的单机应用一样容易。那我们就最好能把云架构和状态管理的细节给抽象掉，从而极大地降低成本、减少错误。而为编程提供更高的抽象层次，从来就是编译技术的职责。

Serverless 和 FaaS 已经把无状态服务的架构细节透明掉了。但针对有状态的服务，目前还没有答案。对我而言，这是个有趣的课题。
在我比较熟悉的企业应用领域，ERP 的鼻祖 SAP、SaaS 的鼻祖 SalesForce，都用自己的语言开发应用，很可惜国内的企业软件厂商还没有做到这一点。而在云计算时代，设计这样一门语言绕不过去的一个问题，就是解决有状态服务的云化问题。我希望能为解决这个问题提供一个新工具。当然，这个工具必须是开源的。

### 为什么要解析真实编译器？


第一，研究这些语言的编译机制，能直接提高我们的技术水平。一方面，深入了解自己使用的语言的编译器，会有助于你吃透这门语言的核心特性，更好地运用它，从而让自己向着专家级别的工程师进军。举个例子，国内某互联网公司的员工，就曾经向 Oracle 公司提交了 HotSpot 的高质量补丁，因为他们在工作中发现了 JVM 编译器的一些不足。那么，你是不是也有可能把一门语言吃得这么透呢？另一方面，IT 技术的进化速度是很快的，作为技术人，我们需要迅速跟上技术更迭的速度。而这些现代语言的编译器，往往就是整合了最前沿的技术。比如，Java 的 JIT 编译器和 JavaScript 的 V8 编译器，它们都不约而同地采用了“Sea of Nodes”的 IR 来做优化，这是为什么呢？这种 IR 有什么优势呢？这些问题我们都需要迅速弄清楚。

第二，阅读语言编译器的源码，是高效学习编译原理的重要路径。传统上，我们学习编译原理，总是要先学一大堆的理论和算法，理解起来非常困难，让人望而生畏。这个方法本身没有错，因为我们学习任何知识，都要掌握其中的原理。不过，这样可能离实现一款实用的编译器还有相当的距离。那么根据我的经验，学习编译原理的一个有效途径，就是阅读真实世界中编译器的源代码，跟踪它的执行过程，弄懂它的运行机制。因为只要你会写程序，就能读懂代码。既然能读懂代码，那为什么不直接去阅读编译器的源代码呢？在开源的时代，源代码就是一个巨大的知识宝库。面对这个宝库，我们为什么不进去尽情搜刮呢？想带走多少就带走多少，没人拦着。


当然，你可能会犯嘀咕：编译器的代码一般都比较难吧？以我的水平，能看懂吗？是会有这个问题。当我们面对一大堆代码的时候，很容易迷路，抓不住其中的重点和核心逻辑。不过没关系，有我呢。在本课程中，我会给你带路，并把地图准备好，带你走完这次探险之旅。而当你确实把握了编译器的脉络以后，你对自己的技术自信心会提升一大截。这些计算机语言，就被你摘掉了神秘的面纱。俗话说“读万卷书，行万里路”。如果说了解编译原理的基础理论和算法是读书的过程，那么探索真实世界里的编译器是什么样子，就是行路的过程了。根据我的体会，当你真正了解了身边的语言的编译器是怎样编写的之后，那些抽象的理论就会变得生动和具体，你也就会在编译技术领域里往前跨出一大步了。


### 我们可以解析哪些语言的编译器？
那你可能要问了，在本课程中，我都选择了哪些语言的编译器呢？选择这些编译器的原因又是什么呢？

这次，我要带你解析的编译器还真不少，包括了 Java 编译器（javac）、Java 的 JIT 编译器（Graal）、Python 编译器（CPython）、JavaScript 编译器（V8）、Julia 语言的编译器、Go 语言的编译器（gc），以及 MySQL 的编译器，并且在讲并行的时候，还涉及了 Erlang 的编译器。

我选择剖析这些语言的编译器，有三方面的原因：第一，它们足够有代表性，是你在平时很可能会用到的。这些语言中，除了 Julia 比较小众外，都比较流行。而且，虽然 Julia 没那么有名，但它使用的 LLVM 工具很重要。因为 LLVM 为 Swift、Rust、C++、C 等多种语言提供了优化和后端的支持，所以 Julia 也不缺乏代表性。第二，它们采用了各种不同的编译技术。这些编译器，有的是编译静态类型的语言，有的是动态类型的语言；有的是即时编译（JIT），有的是提前编译（AOT）；有高级语言，也有 DSL（SQL）；解释执行的话，有的是用栈机（Stack Machine），有的是用寄存器机，等等。不同的语言特性，就导致了编译器采用的技术会存在各种差异，从而更加有利于你开阔视野。第三，通过研究多种编译器，你可以多次迭代对编译器的认知过程，并通过分析对比，发现这些编译器之间的异同点，探究其中的原因，激发出更多的思考，从而得到更全面的、更深入的认知。

看到这里，你可能会有所疑虑：有些语言我没用过，不怎么了解，怎么办？其实没关系。因为现代的高级语言，其实相似度很高。一方面，对于不熟悉的语言，虽然你不能熟练地用它们来做项目，但是写一些基本的、试验性的程序，研究它的实现机制，是没有什么问题的。另一方面，学习编译原理的人会练就一项基本功，那就是更容易掌握一门语言的本质。特别是我这一季的课程，就是要帮你成为钻到了铁扇公主肚子里的孙悟空。研究某一种语言的编译器，当然有助于你通过“捷径”去深入地理解它。

### 我是如何规划课程模块的？
这门课程的目标，是要让你对现代语言的编译器的结构、所采用的算法以及设计上的权衡，都获得比较真切的认识。其最终结果是，如果要你使用编译技术来完成一个项目，你会心里非常有数，知道应该在什么地方使用什么技术。因为你不仅懂得原理，更有很多实际编译器的设计和实现的思路作为你的决策依据。为了达到本课程的目标，我仔细规划了课程的内容，将其划分为预备知识篇、真实编译器解析篇和现代语言设计篇三部分。

在预备知识篇，我会简明扼要地帮你重温一下编译原理的知识体系，让你对这些关键概念的理解变得更清晰。磨刀不误砍柴工，你学完预备知识篇后，再去看各种语言编译器的源代码和相关文档时，至少不会被各种名词、术语搞晕，也能更好地建立具体实现跟原理之间的关联，能互相印证它们。在真实编译器解析篇，我会带你研究语言编译器的源代码，跟踪它们的运行过程，分析编译过程的每一步是如何实现的，并对有特点的编译技术点加以分析和点评。这样，我们在研究了 Java、Java JIT、Python、JavaScript、Julia、Go、MySQL 这 7 个编译器以后，就相当于把编译原理印证了 7 遍。在现代语言设计篇，我会带你分析和总结前面已经研究过的编译器，进一步提升你对相关编译技术的认知高度。学完这一模块以后，你对于如何设计编译器的前端、中端、后端、运行时，都会有比较全面的了解，知道如何在不同的技术路线之间做取舍。

## 01 | 编译的全过程都悄悄做了哪些事情？

**编译，其实就是把源代码变成目标代码的过程。如果源代码编译后要在操作系统上运行，那目标代码就是汇编代码，我们再通过汇编和链接的过程形成可执行文件，然后通过加载器加载到操作系统里执行。如果编译后是在解释器里执行，那目标代码就可以不是汇编代码，而是一种解释器可以理解的中间形式的代码即可**。我举一个很简单的例子。这里有一段 C 语言的程序，我们一起来看看它的编译过程。


```
int foo(int a){
  int b = a + 3;
  return b;
}
```
这段源代码，如果把它编译成汇编代码，大致是下面这个样子：

```
    .section    __TEXT,__text,regular,pure_instructions
    .globl  _foo                    ## -- Begin function foo
_foo:                                   ## @foo
    pushq   %rbp
    movq    %rsp, %rbp
    movl    %edi, -4(%rbp)
    movl    -4(%rbp), %eax
    addl    $3, %eax
    movl    %eax, -8(%rbp)
    movl    -8(%rbp), %eax
    popq    %rbp
    retq
```
你可以看出，源代码和目标代码之间的差异还是很大的。那么，我们怎么实现这个翻译呢？其实，编译和把英语翻译成汉语的大逻辑是一样的。前提是你要懂这两门语言，这样你看到一篇英语文章，在脑子里理解以后，就可以把它翻译成汉语。编译器也是一样，你首先需要让编译器理解源代码的意思，然后再把它翻译成另一种语言。表面上看，好像从英语到汉语，一下子就能翻译过去。但实际上，大脑一瞬间做了很多个步骤的处理，包括识别一个个单词，理解语法结构，然后弄明白它的意思。同样，编译器翻译源代码，也需要经过多个处理步骤，如下图所示。

![img](https://static001.geekbang.org/resource/image/0a/d4/0aa7939fbdb80f923ae7a8090ec7f3d4.jpg?wh=2284*1000)

我来解释一下各个步骤。
### 词法分析（Lexical Analysis）
首先，编译器要读入源代码。在编译之前，源代码只是一长串字符而已，这显然不利于编译器理解程序的含义。所以，编译的第一步，就是要像读文章一样，先把里面的单词和标点符号识别出来。程序里面的单词叫做 Token，它可以分成关键字、标识符、字面量、操作符号等多个种类。把字符串转换为 Token 的这个过程，就叫做词法分析。

![img](https://static001.geekbang.org/resource/image/d8/9a/d80623403c912ab43c328623df8a0e9a.jpg?wh=2284*880)

图 2：把字符串转换为 Token（注意：其中的空白字符，代表空格、tab、回车和换行符，EOF 是文件结束符）

### 语法分析（Syntactic Analysis）
识别出 Token 以后，离编译器明白源代码的含义仍然有很长一段距离。下一步，我们需要让编译器像理解自然语言一样，理解它的语法结构。这就是第二步，语法分析。上语文课的时候，老师都会让你给一个句子划分语法结构。比如说：“我喜欢又聪明又勇敢的你”，它的语法结构可以表示成下面这样的树状结构。

![img](https://static001.geekbang.org/resource/image/6c/a7/6cca65a7d46f1d96e278bd53027a12a7.jpg?wh=2284*1580)

图 3：把一个句子变成语法树

那么在编译器里，语法分析阶段也会把 Token 串，转换成一个体现语法规则的、树状的数据结构，这个数据结构叫做抽象语法树（AST，Abstract Syntax Tree）。我们前面的示例程序转换为 AST 以后，大概是下面这个样子：

![img](https://static001.geekbang.org/resource/image/68/6b/6801cb9b637afae44e728fa08267756b.jpg?wh=2284*1480)

图 4：foo 函数对应的语法树

这样的一棵 AST 反映了示例程序的语法结构。比如说，我们知道一个函数的定义包括了返回值类型、函数名称、0 到多个参数和函数体等。这棵抽象语法树的顶部就是一个函数节点，它包含了四个子节点，刚好反映了函数的语法。再进一步，函数体里面还可以包含多个语句，如变量声明语句、返回语句，它们构成了函数体的子节点。然后，每个语句又可以进一步分解，直到叶子节点，就不可再分解了。而叶子节点，就是词法分析阶段生成的 Token（图中带边框的节点）。对这棵 AST 做深度优先的遍历，你就能依次得到原来的 Token。


### 语义分析（Semantic Analysis）
生成 AST 以后，程序的语法结构就很清晰了，编译工作往前迈进了一大步。但这棵树到底代表了什么意思，我们目前仍然不能完全确定。比如说，表达式“a+3”在计算机程序里的完整含义是：“获取变量 a 的值，把它跟字面量 3 的值相加，得到最终结果。”但我们目前只得到了这么一棵树，完全没有上面这么丰富的含义。

![img](https://static001.geekbang.org/resource/image/83/da/83e78e9e6ba8506b9e26a00fc77ef1da.jpg?wh=2284*400)

图 5：a+3 对应的 AST

这就好比西方的儿童，很小的时候就能够给大人读报纸。因为他们懂得发音规则，能念出单词来（词法分析），也基本理解语法结构（他们不见得懂主谓宾这样的术语，但是凭经验已经知道句子有不同的组成部分），可以读得抑扬顿挫（语法分析），但是他们不懂报纸里说的是什么，也就是不懂语义。这就是编译器解读源代码的下一步工作，语义分析。

那么，怎样理解源代码的语义呢？实际上，语言的设计者在定义类似“a+3”中加号这个操作符的时候，是给它规定了一些语义的，就是要把加号两边的数字相加。你在阅读某门语言的标准时，也会看到其中有很多篇幅是在做语义规定。在 ECMAScript（也就是 JavaScript）标准 2020 版中，Semantic 这个词出现了 657 次。下图是其中[加法操作的语义规则](https://tc39.es/ecma262/2020/#sec-additive-operators)，它对于如何计算左节点、右节点的值，如何进行类型转换等，都有规定。

![img](https://static001.geekbang.org/resource/image/bc/8b/bc77f55a2801542999b7eee10e6f1c8b.jpg?wh=1522*1224)

图 6：ECMAScript 标准中加法操作的语义规则

所以，我们可以在每个 AST 节点上附加一些语义规则，让它能反映语言设计者的本意。add 节点：把两个子节点的值相加，作为自己的值；变量节点（在等号右边的话）：取出变量的值；数字字面量节点：返回这个字面量代表的值。

这样的话，如果你深度遍历 AST，并执行每个节点附带的语义规则，就可以得到 a+3 的值。这意味着，我们正确地理解了这个表达式的含义。运用相同的方法，我们也就能够理解一个句子的含义、一个函数的含义，乃至整段源代码的含义。这也就是说，AST 加上这些语义规则，就能完整地反映源代码的含义。这个时候，你就可以做很多事情了。比如，你可以深度优先地遍历 AST，并且一边遍历，一边执行语法规则。那么这个遍历过程，就是解释执行代码的过程。你相当于写了一个基于 AST 的解释器。不过在此之前，编译器还要做点语义分析工作。那么这里的语义分析是要解决什么问题呢？给你举个例子，如果我把示例程序稍微变换一下，加一个全局变量的声明，这个全局变量也叫 a。那你觉得“a+3”中的变量 a 指的是哪个变量？


```
int a = 10;       //全局变量
int foo(int a){   //参数里有另一个变量a
  int b = a + 3;  //这里的a指的是哪一个？
  return b;
}
```
我们知道，编译程序要根据 C 语言在作用域方面的语义规则，识别出“a+3”中的 a，所以这里指的其实是函数参数中的 a，而不是全局变量的 a。这样的话，我们在计算“a+3”的时候才能取到正确的值。而把“a+3”中的 a，跟正确的变量定义关联的过程，就叫做引用消解（Resolve）。这个时候，变量 a 的语义才算是清晰了。变量有点像自然语言里的代词，比如说，“我喜欢又聪明又勇敢的你”中的“我”和“你”，指的是谁呢？如果这句话前面有两句话，“我是春娇，你是志明”，那这句话的意思就比较清楚了，是“春娇喜欢又聪明又勇敢的志明”。引用消解需要在上下文中查找某个标识符的定义与引用的关系，所以我们现在可以回答前面的问题了，语义分析的重要特点，就是做上下文相关的分析。

在语义分析阶段，编译器还会识别出数据的类型。比如，在计算“a+3”的时候，我们必须知道 a 和 3 的类型是什么。因为即使同样是加法运算，对于整型和浮点型数据，其计算方法也是不一样的。语义分析获得的一些信息（引用消解信息、类型信息等），会附加到 AST 上。这样的 AST 叫做带有标注信息的 AST（Annotated AST/Decorated AST），用于更全面地反映源代码的含义。

![img](https://static001.geekbang.org/resource/image/36/b7/3685791207d4430e5a48bc7d96aa99b7.jpg?wh=2284*1600)

图 7：带有标注信息的 AST

好了，前面我所说的，都是如何让编译器更好地理解程序的语义。不过在语义分析阶段，编译器还要做很多语义方面的检查工作。在自然语言里，我们可以很容易写出一个句子，它在语法上是正确的，但语义上是错误的。比如，“小猫喝水”这句话，它在语法和语义上都是对的；而“水喝小猫”这句话，语法是对的，语义上则是不对的。计算机程序也会存在很多类似的语义错误的情况。比如说，对于“int b = a+3”的这个语句，语义规则要求，等号右边的表达式必须返回一个整型的数据（或者能够自动转换成整型的数据），否则就跟变量 b 的类型不兼容。如果右边的表达式“a+3”的计算结果是浮点型的，就违背了语义规则，就要报错。总结起来，在语义分析阶段，编译器会做语义理解和语义检查这两方面的工作。词法分析、语法分析和语义分析，统称编译器的前端，它完成的是对源代码的理解工作。

做完语义分析以后，接下来编译器要做什么呢？本质上，编译器这时可以直接生成目标代码，因为编译器已经完全理解了程序的含义，并把它表示成了带有语义信息的 AST、符号表等数据结构。生成目标代码的工作，叫做后端工作。做这项工作有一个前提，就是编译器需要懂得目标语言，也就是懂得目标语言的词法、语法和语义，这样才能保证翻译的准确性。这是显而易见的，只懂英语，不懂汉语，是不可能做英译汉的。通常来说，目标代码指的是汇编代码，它是汇编器（Assembler）所能理解的语言，跟机器码有直接的对应关系。汇编器能够将汇编代码转换成机器码。熟练掌握汇编代码对于初学者来说会有一定的难度。但更麻烦的是，对于不同架构的 CPU，还需要生成不同的汇编代码，这使得我们的工作量更大。所以，我们通常要在这个时候增加一个环节：先翻译成中间代码（Intermediate Representation，IR）。


### 中间代码（Intermediate Representation）
中间代码（IR），是处于源代码和目标代码之间的一种表示形式。我们倾向于使用 IR 有两个原因。第一个原因，是很多解释型的语言，可以直接执行 IR，比如 Python 和 Java。这样的话，编译器生成 IR 以后就完成任务了，没有必要生成最终的汇编代码。第二个原因更加重要。我们生成代码的时候，需要做大量的优化工作。而很多优化工作没有必要基于汇编代码来做，而是可以基于 IR，用统一的算法来完成。

### 优化（Optimization）
那为什么需要做优化工作呢？这里又有两大类的原因。第一个原因，是源语言和目标语言有差异。源语言的设计目的是方便人类表达和理解，而目标语言是为了让机器理解。在源语言里很复杂的一件事情，到了目标语言里，有可能很简单地就表达出来了。比如“I want to hold your hand and with you I will grow old.” 这句话挺长的吧？用了 13 个单词，但它实际上是诗经里的“执子之手，与子偕老”对应的英文。这样看来，还是中国文言文承载信息的效率更高。同样的情况在编程语言里也有。以 Java 为例，我们经常为某个类定义属性，然后再定义获取或修改这些属性的方法：


```
Class Person{
  private String name;
  public String getName(){
    return name;
  }
  public void setName(String newName){
    this.name = newName
  }
}

```
如果你在程序里用“person.getName()”来获取 Person 的 name 字段，会是一个开销很大的操作，因为它涉及函数调用。在汇编代码里，实现一次函数调用会做下面这一大堆事情：

```
#调用者的代码
保存寄存器1   #保存现有寄存器的值到内存
保存寄存器2
...
保存寄存器n

把返回地址入栈
把person对象的地址写入寄存器，作为参数
跳转到getName函数的入口

#_getName 程序
在person对象的地址基础上，添加一个偏移量，得到name字段的地址
从该地址获取值，放到一个用于保存返回值的寄存器
跳转到返回地
```
你看了这段伪代码，就会发现，简单的一个 getName() 方法，开销真的很大。保存和恢复寄存器的值、保存和读取返回地址，等等，这些操作会涉及好几次读写内存的操作，要花费大量的时钟周期。但这个逻辑其实是可以简化的。怎样简化呢？就是跳过方法的调用。我们直接根据对象的地址计算出 name 属性的地址，然后直接从内存取值就行。这样优化之后，性能会提高好多倍。这种优化方法就叫做内联（inlining），也就是把原来程序中的函数调用去掉，把函数内的逻辑直接嵌入函数调用者的代码中。在 Java 语言里，这种属性读写的代码非常多。所以，Java 的 JIT 编译器（把字节码编译成本地代码）很重要的工作就是实现内联优化，这会让整体系统的性能提高很大的一个百分比！总结起来，我们在把源代码翻译成目标代码的过程中，没有必要“直译”，而是可以“意译”。这样我们完成相同的工作，对资源的消耗会更少。

第二个需要优化工作的原因，是程序员写的代码不是最优的，而编译器会帮你做纠正。比如下面这段代码中的 bar() 函数，里面就有多个地方可以优化。甚至，整个对 bar() 函数的调用，也可以省略，因为 bar() 的值一定是 101。这些优化工作都可以在编译期间完成。
```
int bar(){
    int a = 10*10;  //这里在编译时可以直接计算出100这个值，这叫做“常数折叠”
    int b = 20;     //这个变量没有用到，可以在代码中删除，这叫做“死代码删除”


    if (a>0){       //因为a一定大于0，所以判断条件和else语句都可以去掉
        return a+1; //这里可以在编译器就计算出是101
    }
    else{
        return a-1;
    }
}
int a = bar();      //这里可以直接换成 a=101
```
综上所述，在生成目标代码之前，需要做的优化工作可以有很多，这通常也是编译器在运行时，花费时间最长的一个部分。

![img](https://static001.geekbang.org/resource/image/26/82/26ad195418f2ce03155a625043348982.jpg?wh=2284*1280)

图 8：多个前端和多个后端，可以采用统一的 IR

而采用中间代码来编写优化算法的好处，是可以把大部分的优化算法，写成与具体 CPU 架构无关的形式，从而大大降低编译器适配不同 CPU 的工作量。并且，如果采用像 LLVM 这样的工具，我们还可以让多种语言的前端生成相同的中间代码，这样就可以复用中端和后端的程序了。


### 生成目标代码
编译器最后一个阶段的工作，是生成高效率的目标代码，也就是汇编代码。这个阶段，编译器也有几个重要的工作。第一，是要选择合适的指令，生成性能最高的代码。第二，是要优化寄存器的分配，让频繁访问的变量（比如循环变量）放到寄存器里，因为访问寄存器要比访问内存快 100 倍左右。第三，是在不改变运行结果的情况下，对指令做重新排序，从而充分运用 CPU 内部的多个功能部件的并行计算能力。目标代码生成以后，整个编译过程就完成了。

### 课程小结
本讲我从头到尾概要地讲解了编译的过程，希望你能了解每一个阶段存在的原因（Why），以及要完成的主要任务（What）。编译是一个比较复杂的过程，但如果我们能够分而治之，那么每一步的挑战就会降低很多。这样最后针对每个子任务，我们就都能找到解决的办法。我希望这一讲能帮你在大脑里建立起一个概要的地图。在后面几讲中，我会对编译过程的各个环节展开讨论，让你有越来越清晰的理解。

![img](https://static001.geekbang.org/resource/image/70/a5/706f4e0f50ab6ce77fd6246cb8cea0a5.jpg?wh=2284*2657)

## 02 | 词法分析：用两种方式构造有限自动机
上一讲，我带你把整个编译过程走了一遍。这样，你就知道了编译过程的整体步骤，每一步是做什么的，以及为什么要这么做。进一步地，你就可以研究一下每个环节具体是如何实现的、有哪些难点、有哪些理论和算法。通过这个过程，你不仅可以了解每个环节的原理，还能熟悉一些专有词汇。这样一来，你在读编译原理领域的相关资料时，就会更加顺畅了。不过，编译过程中涉及的算法和原理有些枯燥，所以我会用尽量通俗、直观的方式来给你解读，让你更容易接受。本讲，我主要跟你讨论一下词法分析（Lexical Analysis）这个环节。通过这节课，你可以掌握词法分析这个阶段是如何把字符串识别成一个个 Token 的。进而，你还会学到如何实现一个正则表达式工具，从而实现任意的词法解析。

### 词法分析的原理
首先，我们来了解一下词法分析的原理。通过上一讲，你已经很熟悉词法分析的任务了：输入的是字符串，输出的是 Token 串。所以，词法分析器在英文中一般叫做 Tokenizer。

![img](https://static001.geekbang.org/resource/image/d8/9a/d80623403c912ab43c328623df8a0e9a.jpg?wh=2284*880)

图 1：把字符串转换为 Token（注意：其中的空白字符，代表空格、tab、回车和换行符，EOF 是文件结束符）

但具体如何实现呢？这里要有一个计算模型，叫做有限自动机（Finite-state Automaton，FSA），或者叫做有限状态自动机（Finite-state Machine，FSM）。有限自动机这个名字，听上去可能比较陌生。但大多数程序员，肯定都接触过另一个词：状态机。假设你要做一个电商系统，那么订单状态的迁移，就是一个状态机。

![img](https://static001.geekbang.org/resource/image/91/62/91093cbeaeb516a9b7bb7b393a53dc62.jpg?wh=2284*880)

图 2：状态机的例子（订单的状态和迁移过程）

有限自动机就是这样的状态机，它的状态数量是有限的。当它收到一个新字符的时候，会导致状态的迁移。比如说，下面的这个状态机能够区分标识符和数字字面量。

![img](https://static001.geekbang.org/resource/image/ac/14/ac4f8932b2488ad0f3815853a4180114.jpg?wh=2284*1600)

图 3：一个能够识别标识符和数字字面量的有限自动机

在这样一个状态机里，我用单线圆圈表示临时状态，双线圆圈表示接受状态。接受状态就是一个合格的 Token，比如图 3 中的状态 1（数字字面量）和状态 2（标识符）。当这两个状态遇到空白字符的时候，就可以记下一个 Token，并回到初始态（状态 0），开始识别其他 Token。可以看出，词法分析的过程，其实就是对一个字符串进行模式匹配的过程。说起字符串的模式匹配，你能想到什么工具吗？对的，正则表达式工具。

大多数语言，以及一些操作系统的命令，都带有正则表达式工具，来帮助你匹配合适的字符串。比如下面的这个 Linux 命令，可以用来匹配所有包含“sa”“sb” … “sh”字符串的进程。


```
ps -ef | grep 's[a-h]'
```

在这个命令里，“s[a-h]”是用来描述匹配规则的，我们把它叫做一个正则表达式。同样地，正则表达式也可以用来描述词法规则。这种描述方法，我们叫做正则文法（Regular Grammar）。比如，数字字面量和标识符的正则文法描述是这样的：



```
IntLiteral : [0-9]+;          //至少有一个数字
Id : [A-Za-z][A-Za-z0-9]*;    //以字母开头，后面可以是字符或数字
```
与普通的正则表达式工具不同的是，词法分析器要用到很多个词法规则，每个词法规则都采用“Token 类型: 正则表达式”这样一种格式，用于匹配一种 Token。然而，当我们采用了多条词法规则的时候，有可能会出现词法规则冲突的情况。比如说，int 关键字其实也是符合标识符的词法规则的。

```
Int : int;   //int关键字
For : for;   //for关键字
Id : [A-Za-z][A-Za-z0-9]*; //以字母开头，后面可以是字符或数字
```
所以，词法规则里面要有优先级，比如排在前面的词法规则优先级更高。这样的话，我们就能够设计出区分 int 关键字和标识符的有限自动机了，可以画成下面的样子。其中，状态 1、2 和 3 都是标识符，而状态 4 则是 int 关键字。

![img](https://static001.geekbang.org/resource/image/31/1c/31d083e8e33b25c1b6ea721f20e7ce1c.jpg?wh=2284*2380)

图 4：一个能够识别 int 关键字和标识符的有限自动机


### 从正则表达式生成有限自动机
现在，你已经了解了如何构造有限自动机，以及如何处理词法规则的冲突。基本上，你就可以按照上面的思路来手写词法分析器了。但你可能觉得，这样手写词法分析器的步骤太繁琐了，我们能否只写出词法规则，就自动生成相对应的有限自动机呢？当然是可以的，实际上，正则表达式工具就是这么做的。此外，词法分析器生成工具 lex（及 GNU 版本的 flex）也能够基于规则自动生成词法分析器。它的具体实现思路是这样的：把一个正则表达式翻译成 NFA，然后把 NFA 转换成 DFA。对不起，我这里又引入了两个新的术语：NFA 和 DFA。先说说 DFA，它是“Deterministic Finite Automaton”的缩写，即确定的有限自动机。它的特点是：该状态机在任何一个状态，基于输入的字符，都能做一个确定的状态转换。前面例子中的有限自动机，都属于 DFA。再说说 NFA，它是“Nondeterministic Finite Automaton”的缩写，即不确定的有限自动机。它的特点是：该状态机中存在某些状态，针对某些输入，不能做一个确定的转换。

这又细分成两种情况：对于一个输入，它有两个状态可以转换。存在ε转换的情况，也就是没有任何字符输入的情况下，NFA 也可以从一个状态迁移到另一个状态。

比如，“a[a-zA-Z0-9]*bc”这个正则表达式，对字符串的要求是以 a 开头，以 bc 结尾，a 和 bc 之间可以有任意多个字母或数字。可以看到，在图 5 中，状态 1 的节点输入 b 时，这个状态是有两条路径可以选择的：一条是迁移到状态 2，另一条是仍然保持在状态 1。所以，这个有限自动机是一个 NFA。

![img](https://static001.geekbang.org/resource/image/41/d6/41e29db09305197dda72697e97aa83d6.jpg?wh=2284*720)

图 5：一个 NFA 的例子，识别“a[a-zA-Z0-9]*bc”的自动机

这个 NFA 还有引入ε转换的画法，如图 6 所示，它跟图 5 的画法是等价的。实际上，图 6 表示的 NFA 可以用我们下面马上要讲到的算法，通过正则表达式自动生成出来。

![img](https://static001.geekbang.org/resource/image/47/9a/4776f1393ea7d700668f42a6065e059a.jpg?wh=2284*640)

图 6：另一个 NFA 的例子，同样能识别“a[a-zA-Z0-9]*bc”，其中有ε转换

需要注意的是，无论是 NFA 还是 DFA，都等价于正则表达式。也就是说，所有的正则表达式都能转换成 NFA 或 DFA；而所有的 NFA 或 DFA，也都能转换成正则表达式。

理解了 NFA 和 DFA 以后，接下来我再大致说一下算法。首先，一个正则表达式可以机械地翻译成一个 NFA。它的翻译方法如下：

识别字符 i 的 NFA。当接受字符 i 的时候，引发一个转换，状态图的边上标注 i。其中，第一个状态（i，initial）是初始状态，第二个状态 (f，final) 是接受状态。

![img](https://static001.geekbang.org/resource/image/13/37/13c0df37440d701b4ec1484dd900ec37.jpg?wh=2284*320)

图 7：识别 i 的 NFA

转换“s|t”这样的正则表达式。它的意思是，或者 s，或者 t，二者选一。s 和 t 本身是两个子表达式，我们可以增加两个新的状态：开始状态和接受状态。然后，用ε转换分别连接代表 s 和 t 的子图。它的含义也比较直观，要么走上面这条路径，那就是 s，要么走下面这条路径，那就是 t：

![img](https://static001.geekbang.org/resource/image/b7/31/b7c2a649036b35b45b1f2af597b45e31.jpg?wh=2284*1280)

转换“st”这样的正则表达式。s 之后接着出现 t，转换规则是把 s 的开始状态变成 st 整体的开始状态，把 t 的结束状态变成 st 整体的结束状态，并且把 s 的结束状态和 t 的开始状态合二为一。这样就把两个子图衔接了起来，走完 s 接着走 t。

![img](https://static001.geekbang.org/resource/image/f5/03/f52985c394f156ff477f91a8b7f83303.jpg?wh=2284*440)

图 9：识别 st 的 NFA

对于“?”“*”和“+”这样的符号，它们的意思是可以重复 0 次、0 到多次、1 到多次，转换时要增加额外的状态和边。以“s*”为例，我们可以做下面的转换：

![img](https://static001.geekbang.org/resource/image/5b/09/5bb1470f0a07b6decfc8ac4fdbf5eb09.jpg?wh=2284*640)

图 10：识别 s* 的 NFA

你能看出，它可以从 i 直接到 f，也就是对 s 匹配 0 次，也可以在 s 的起止节点上循环多次。如果是“s+”，那就没有办法跳过 s，s 至少要经过一次：

![img](https://static001.geekbang.org/resource/image/54/de/543966ed03d9b25cec569d80f8b304de.jpg?wh=2284*600)

图 11：识别 s+ 的 NFA

通过这样的转换，所有的正则表达式，都可以转换为一个 NFA。基于 NFA，你仍然可以实现一个词法分析器，只不过算法会跟基于 DFA 的不同：当某个状态存在一条以上的转换路径的时候，你要先尝试其中的一条；如果匹配不上，再退回来，尝试其他路径。这种试探不成功再退回来的过程，叫做回溯（Backtracking）。小提示：下一讲的递归下降算法里，也会出现回溯现象，你可以对照着理解。

基于 NFA，你也可以写一个正则表达式工具。实际上，我在示例程序中已经写了一个简单的正则表达式工具，使用了[Regex.java](https://github.com/RichardGong/PlayWithCompiler/blob/master/lab/16-18/src/main/java/play/parser/Regex.java)中的 regexToNFA 方法。如下所示，我用了一个测试用的正则表达式，它能识别 int 关键字、标识符和数字字面量。在示例程序中，这个正则表达式首先被表示为一个内部的树状数据结构，然后可以转换成 NFA。

```
int | [a-zA-Z][a-zA-Z0-9]* | [0-9]*
```
示例程序也会将生成的 NFA 打印输出，下面的输出结果中列出了所有的状态，以及每个状态到其他状态的转换，比如“0 ε -> 2”的意思是从状态 0 通过 ε 转换，到达状态 2 ：



```
NFA states:
0  ε -> 2
  ε -> 8
  ε -> 14
2  i -> 3
3  n -> 5
5  t -> 7
7  ε -> 1
1  (end)
  acceptable
8  [a-z]|[A-Z] -> 9
9  ε -> 10
  ε -> 13
10  [0-9]|[a-z]|[A-Z] -> 11
11  ε -> 10
  ε -> 13
13  ε -> 1
14  [0-9] -> 15
15  ε -> 14
  ε -> 1
```

我用图片来直观展示一下输出结果，分为上、中、下三条路径，你能清晰地看出解析 int 关键字、标识符和数字字面量的过程：

![img](https://static001.geekbang.org/resource/image/38/71/38332d3ad4bbedc1979f5fe2936efc71.jpg?wh=2284*1218)

图 12：由算法自动生成的 NFA

那么生成 NFA 之后，我们要如何利用它，来识别某个字符串是否符合这个 NFA 代表的正则表达式呢？还是以图 12 为例，当我们解析“intA”这个字符串时，首先选择最上面的路径进行匹配，匹配完 int 这三个字符以后，来到状态 7，若后面没有其他字符，就可以到达接受状态 1，返回匹配成功的信息。

可实际上，int 后面是有 A 的，所以第一条路径匹配失败。失败之后不能直接返回“匹配失败”的结果，因为还有其他路径，所以我们要回溯到状态 0，去尝试第二条路径，在第二条路径中，我们尝试成功了。运行 Regex.java 中的 matchWithNFA() 方法，你可以用 NFA 来做正则表达式的匹配。其中，在匹配“intA”时，你会看到它的回溯过程：



```
NFA matching: 'intA'
trying state : 0, index =0
trying state : 2, index =0    //先走第一条路径，即int关键字这个路径
trying state : 3, index =1
trying state : 5, index =2
trying state : 7, index =3
trying state : 1, index =3    //到了末尾，发现还有字符'A'没有匹配上
trying state : 8, index =0    //回溯，尝试第二条路径，即标识符
trying state : 9, index =1
trying state : 10, index =1   //在10和11这里循环多次
trying state : 11, index =2
trying state : 10, index =2
trying state : 11, index =3
trying state : 10, index =3
true
```
从中你可以看到用 NFA 算法的特点：因为存在多条可能的路径，所以需要试探和回溯，在比较极端的情况下，回溯次数会非常多，性能会变得非常差。特别是当处理类似“s*”这样的语句时，因为 s 可以重复 0 到无穷次，所以在匹配字符串时，可能需要尝试很多次。NFA 的运行可能导致大量的回溯，那么能否将 NFA 转换成 DFA，让字符串的匹配过程更简单呢？如果能的话，那整个过程都可以自动化，从正则表达式到 NFA，再从 NFA 到 DFA。方法是有的，这个算法就是子集构造法。不过我这里就不展开介绍了，如果你想继续深入学习的话，可以去看看本讲最后给出的参考资料。总之，只要有了准确的正则表达式，是可以根据算法自动生成对字符串进行匹配的程序的，这就是正则表达式工具的基本原理，也是有些工具（比如 ANTLR 和 flex）能够自动给你生成一个词法分析器的原理。

### 课程小结
本讲涵盖了词法分析所涉及的主要知识点。词法分析跟你日常使用的正则表达式关系很密切，你可以用正则表达式来表示词法规则。在实际的编译器中，词法分析器一般都是手写的，依据的基本原理就是构造有限自动机。不过有一些地方也会用手工编码的方式做一些优化（如 javac 编译器），有些编译器会做用一些特别的技巧来提升解析速度（如 JavaScript 的 V8 编译器），你在后面的课程中会看到。基于正则表达式构造 NFA，再去进行模式匹配，是一个很好的算法思路，它不仅仅可以用于做词法分析，其实还可以用于解决其他问题（比如做语法分析），值得你去做举一反三的思考。

![img](https://static001.geekbang.org/resource/image/90/bb/90af03a17ffc5fb441327198a753ecbb.jpg?wh=2284*1660)


## 03 | 语法分析：两个基本功和两种算法思路

通过第 1 讲的学习，现在你已经清楚了语法分析阶段的任务：依据语法规则，把 Token 串转化成 AST。今天，我就带你来掌握语法分析阶段的核心知识点，也就是两个基本功和两种算法思路。理解了这些重要的知识点，对于语法分析，你就不是外行了。

两个基本功：第一，必须能够阅读和书写语法规则，也就是掌握上下文无关文法；第二，必须要掌握递归下降算法。两种算法思路：一种是自顶向下的语法分析，另一种则是自底向上的语法分析。

### 上下文无关文法（Context-Free Grammar）
在开始语法分析之前，我们要解决的第一个问题，就是如何表达语法规则。在上一讲中，你已经了解了，我们可以用正则表达式来表达词法规则，语法规则其实也差不多。我还是以下面这个示例程序为例，里面用到了变量声明语句、加法表达式，我们看看语法规则应该怎么写：

```
 int a = 2;
 int b = a + 3;
 return b;
```
第一种写法是下面这个样子，它看起来跟上一讲的词法规则差不多，都是左边是规则名称，右边是正则表达式。


```
start：blockStmts ;               //起始
block : '{' blockStmts '}' ;      //语句块
blockStmts : stmt* ;              //语句块中的语句
stmt = varDecl | expStmt | returnStmt | block;   //语句
varDecl : type Id varInitializer？ ';' ;         //变量声明
type : Int | Long ;                              //类型
varInitializer : '=' exp ;                       //变量初始化
expStmt : exp ';' ;                              //表达式语句
returnStmt : Return exp ';' ;                    //return语句
exp : add ;                                      //表达式       
add : add '+' mul | mul;                         //加法表达式
mul : mul '*' pri | pri;                         //乘法表达式
pri : IntLiteral | Id | '(' exp ')' ;            //基础表达式 
```
在语法规则里，我们把冒号左边的叫做非终结符（Non-terminal），又叫变元（Variable）。非终结符可以按照右边的正则表达式来逐步展开，直到最后都变成标识符、字面量、运算符这些不可再展开的符号，也就是终结符（Terminal）。终结符其实也是词法分析过程中形成的 Token。

提示：
1. 在本课程，非终结符以小写字母开头，终结符则以大写字母开头，或者是一个原始的字符串格式。
2. 在谈论语法分析的时候，我们可以把 Token 和终结符这两个术语互换使用。

像这样左边是非终结符，右边是正则表达式的书写语法规则的方式，就叫做扩展巴科斯范式（EBNF）。你在 ANTLR 这样的语法分析器生成工具中，经常会看到这种格式的语法规则。


对于 EBNF 的严格定义，你可以去参考Wikipedia上的解释。

在教科书中，我们还经常采用另一种写法，就是产生式（Production Rule），又叫做替换规则（Substitution Rule）。产生式的左边是非终结符（变元），它可以用右边的部分替代，中间通常会用箭头连接。

为了避免跟 EBNF 中的“*”号、“+”号等冲突，在本节课中，凡是采用 EBNF 格式，就给字符串格式的终结符加引号，左右两边用“::=”或冒号分隔开；凡是采用产生式，字符串就不加引号，并且采用“->”分隔产生式的左右两侧。

```
add -> add + mul
add -> mul
mul -> mul * pri
mul -> pri
```
也有个偷懒的写法，就是把同一个变元的多个产生式写在一起，用竖线分隔（但这时候，如果产生式里面原本就要用到“|”终结符，那么就要加引号来进行区分）。但也就仅此为止了，不会再引入“*”和“+”等符号，否则就成了 EBNF 了。
```
add -> add + mul | mul
mul -> mul * pri | pri
```
产生式不用“ * ”和“+”来表示重复，而是用迭代，并引入“ε”（空字符串）。所以“blockStmts : stmt*”可以写成下面这个样子：
```
blockStmts -> stmt blockStmts | ε
```
总结起来，语法规则是由 4 个部分组成的：一个有穷的非终结符（或变元）的集合；一个有穷的终结符的集合；一个有穷的产生式集合；一个起始非终结符（变元）。


那么符合这四个特点的文法规则，就叫做上下文无关文法（Context-Free Grammar，CFG）。你可能会问，上下文无关文法和词法分析中用到的正则文法是否有一定的关系？是的，正则文法是上下文无关文法的一个子集。其实，正则文法也可以写成产生式的格式。比如，数字字面量（正则表达式为“[0-9]+”）可以写成：

```
IntLiteral -> Digit IntLiteral1
IntLiteral1 -> Digit IntLiteral1 
IntLiteral1 -> ε
Digit -> [0-9]
```
但是，在上下文无关文法里，产生式的右边可以放置任意的终结符和非终结符，而正则文法只是其中的一个子集，叫做线性文法（Linear Grammar）。它的特点是产生式的右边部分最多只有一个非终结符，比如 X->aYb，其中 a 和 b 是终结符。

![img](https://static001.geekbang.org/resource/image/99/29/99a69f477f20f1a4eae194116adb7829.jpg?wh=2284*1080)

图 1：正则文法是上下文无关文法的子集

你可以试一下，把上一讲用到的正则表达式“a[a-zA-Z0-9]*bc”写成产生式的格式，它就符合线性文法的特点。

```
S0 -> aS1bc          
S1 -> [a-zA-Z0-9]S1  
S1 -> ε
```

但对于常见的语法规则来说，正则文法是不够的。比如，你最常用的算术表达式的规则，就没法用正则文法表示，因为有的产生式需要包含两个非终结符（如“add + mul”）。你可以试试看，能把“2+3”“2+3*5”“2+3+4+5”等各种可能的算术表达式，用一个正则表达式写出来吗？实际是不可能的。

```
add -> add + mul
add -> mul
mul -> mul * pri
mul -> pri
```

好，现在你已经了解了上下文无关文法，以及它与正则文法的区别。可是，为什么它会叫“上下文无关文法”这样一个奇怪的名字呢？难道还有上下文相关的文法吗？答案的确是有的。举个例子来说，在高级语言里，本地变量必须先声明，才能在后面使用。这种制约关系就是上下文相关的。不过，在语法分析阶段，我们一般不管上下文之间的依赖关系，这样能使得语法分析的任务更简单。而对于上下文相关的情况，则放到语义分析阶段再去处理。好了，现在你已经知道，用上下文无关文法可以描述程序的语法结构。学习编译原理，阅读和书写语法规则是一项基本功。针对高级语言中的各种语句，你要都能够手写出它们的语法规则来才可以。接下来，我们就要依据语法规则，编写语法分析程序，把 Token 串转化成 AST。语法分析的算法有很多，但有一个算法也是你必须掌握的一项基本功，这就是递归下降算法。



### 递归下降算法（Recursive Descent Parsing）
递归下降算法其实很简单，它的基本思路就是按照语法规则去匹配 Token 串。比如说，变量声明语句的规则如下：

```
varDecl : types Id varInitializer？ ';' ;        //变量声明
varInitializer : '=' exp ;                       //变量初始化
exp : add ;                                      //表达式       
add : add '+' mul | mul;                         //加法表达式
mul : mul '*' pri | pri;                         //乘法表达式
pri : IntLiteral | Id | '(' exp ')' ;            //基础表达式
```
如果写成产生式格式，是下面这样：

```
varDecl -> types Id varInitializer ';' 
varInitializer -> '=' exp              
varInitializer -> ε
exp -> add
add -> add + mul
add -> mul
mul -> mul * pri
mul -> pri
pri -> IntLiteral
pri -> Id
pri -> ( exp )
```

而基于这个规则做解析的算法如下：



```
匹配一个数据类型(types)
匹配一个标识符(Id)，作为变量名称
匹配初始化部分(varInitializer)，而这会导致下降一层，使用一个新的语法规则：
   匹配一个等号
   匹配一个表达式(在这个步骤会导致多层下降：exp->add->mul->pri->IntLiteral)
   创建一个varInitializer对应的AST节点并返回
如果没有成功地匹配初始化部分，则回溯，匹配ε，也就是没有初始化部分。
匹配一个分号   
创建一个varDecl对应的AST节点并返回
```

用上述算法解析“int a = 2”，就会生成下面的 AST：

![img](https://static001.geekbang.org/resource/image/31/ed/3102dff3c43e5bcd40ddf6442947dced.jpg?wh=2284*1476)

图 2：“int a = 2”对应的 AST

那么总结起来，递归下降算法的特点是：对于一个非终结符，要从左到右依次匹配其产生式中的每个项，包括非终结符和终结符。在匹配产生式右边的非终结符时，要下降一层，继续匹配该非终结符的产生式。如果一个语法规则有多个可选的产生式，那么只要有一个产生式匹配成功就行。如果一个产生式匹配不成功，那就回退回来，尝试另一个产生式。这种回退过程，叫做回溯（Backtracking）。

所以说，递归下降算法是非常容易理解的。它能非常有效地处理很多语法规则，但是它也有两个缺点。第一个缺点，就是著名的左递归（Left Recursion）问题。比如，在匹配算术表达式时，产生式的第一项就是一个非终结符 add，那么按照算法，要下降一层，继续匹配 add。这个过程会一直持续下去，无限递归下去。

```
add -> add + mul
```

所以，递归下降算法是无法处理左递归问题的。那么有什么解决办法吗？你可能会说，把产生式改成右递归不就可以了吗？也就是 add 这个递归项在右边：



```
add -> mul + add
```
这样确实可以避免左递归问题，但它同时也会导致结合性的问题。举个例子来说，我们按照上面的语法规则来解析“2+3+4”这个表达式，会形成如下所示的 AST。

![img](https://static001.geekbang.org/resource/image/08/20/08df3cff28b8b53a4b3dd1e30d282820.jpg?wh=2284*540)

图 3：结合性错误的 AST

它会先计算“3+4”，而不是先计算“2+3”。这破坏了加法的结合性规则，加法运算本来应该是左结合的。其实有一个标准的方法，能避免左递归问题。我们可以改写原来的语法规则，也就是引入add'，把左递归变成右递归：

```
add -> mul add'
add' -> + mul add' | ε
```
接下来，我们用刚刚改写的规则再次解析一下 “2+3+4”这个表达式，会得到下图中的 AST：

![img](https://static001.geekbang.org/resource/image/86/46/861f47308498c402dfab6798c3b7d246.jpg?wh=2284*1240)

图 4：基于改写后的文法所生成的 AST

你能看出，这种改写方法虽然能够避免左递归问题，但由于add'的规则是右递归的，采用标准的递归下降算法，仍然会出现运算符结合性的错误。那么针对这点，我们有没有解决办法呢？有的，方法就是把递归调用转化成循环。这里利用了很多同学都知道的一个原理，即递归调用可以转化为循环。其实我把上面的规则换成用 EBNF 方式来表达就很清楚了。在 EBNF 格式里，允许用“*”号和“+”号表示重复：

```
add ： mul ('+' mul)*  ；
```
所以说，对于('+'mul)*这部分，我们其实可以写成一个循环。而在循环里，我们可以根据结合性的要求，手工生成正确的 AST。它的伪代码如下：

```
左子节点 = 匹配一个mul
while(下一个Token是+){
  消化掉+
  右子节点 = 匹配一个mul
  用左、右子节点创建一个add节点
  左子节点 = 该add节点
}
```
采用上面的算法，就可以创建正确的 AST，如下图所示：

![img](https://static001.geekbang.org/resource/image/fd/5b/fdf3da5d525ddd949e318b1a6fa5895b.jpg?wh=2284*540)

图 5：结合性正确的 AST

递归下降算法的第二个缺点，就是当产生式匹配失败的时候，必须要“回溯”，这就可能导致浪费。

这个时候，我们有个针对性的解决办法，就是预读后续的一个 Token，判断该选择哪个产生式。以 stmt 变元为例，考虑它的三个产生式，分别是变量声明语句、表达式语句和 return 语句。那么在递归下降算法中，我们可以在这里预读一个 Token，看看能否根据这个 Token 来选择某个产生式。经过仔细观察，你发现如果预读的 Token 是 Int 或 Long，就选择变量声明语句；如果是 IntLiteral、Id 或左括号，就选择表达式语句；而如果是 Return，则肯定是选择 return 语句。因为这三个语句开头的 Token 是不重叠的，所以你可以很明确地做出选择。如果我们手写递归下降算法，可以用肉眼识别出每次应该基于哪个 Token，选择用哪个产生式。但是，对于一些比较复杂的语法规则，我们要去看好几层规则，这样比较辛苦。那么能否有一个算法，来自动计算出选择不同产生式的依据呢？当然是有的，这就是 LL 算法家族。

### LL 算法：计算 First 和 Follow 集合
LL 算法的要点，就是计算 First 和 Follow 集合。First 集合是每个产生式开头可能会出现的 Token 的集合。就像 stmt 有三个产生式，它的 First 集合如下表所示。

![img](https://static001.geekbang.org/resource/image/63/b3/6316103438404e64f89e402ef28498b3.jpg?wh=2284*1080)

而 stmt 的 First 集合，就是三个产生式的 First 集合的并集，也是 Int Long IntLiteral Id ( Return。总体来说，针对非终结符 x，它的 First 集合的计算规则是这样的：

如果产生式以终结符开头，那么把这个终结符加入 First(x)；如果产生式以非终结符 y 开头，那么把 First(y) 加入 First(x);如果 First(y) 包含ε，那要把下一个项的 First 集合也加入进来，以此类推；如果 x 有多个产生式，那么 First(x) 是每个产生式的并集。

在计算 First 集合的时候，具体可以采用“不动点法”。相关细节这里就不展开了，你可以参考示例程序[FirstFollowSet](https://github.com/RichardGong/PlayWithCompiler/blob/master/lab/16-18/src/main/java/play/parser/FirstFollowSet.java)类的 CalcFirstSets() 方法，运行示例程序能打印各个非终结符的 First 集合。

不过，这样是不是就万事大吉了呢？其实还有一种特殊情况我们需要考虑，那就是对于某个非终结符，它自身会产生ε的情况。比如说，示例文法中的 blockStmts，它是可能产生ε的，也就是块中一个语句都没有。

```
block : '{' blockStmts '}' ;                 //语句块
blockStmts : stmt* ;                         //语句块中的语句
stmt = varDecl | expStmt | returnStmt;       //语句
```
语法解析器在这个时候预读的下一个 Token 是什么呢？是右花括号。这证明 blockStmts 产生了ε，所以才读到了后续跟着的花括号。对于某个非终结符后面可能跟着的 Token 的集合，我们叫做 Follow 集合。如果预读到的 Token 在 Follow 中，那么我们就可以判断当前正在匹配的这个非终结符，产生了ε。

Follow 的算法也比较简单，以非终结符 x 为例：

扫描语法规则，看看 x 后面都可能跟着哪些符号；对于后面跟着的终结符，都加到 Follow(x) 集合中去；如果后面是非终结符 y，就把 First(y) 加 Follow(x) 集合中去；最后，如果 First(y) 中包含ε，就继续往后找；如果 x 可能出现在程序结尾，那么要把程序的终结符 $ 加入到 Follow(x) 中去。

这样在计算了 First 和 Follow 集合之后，你就可以通过预读一个 Token，来完全确定采用哪个产生式。这种算法，就叫做 LL(1) 算法。LL(1) 中的第一个 L，是 Left-to-right 的缩写，代表从左向右处理 Token 串。第二个 L，是 Leftmost 的缩写，意思是最左推导。最左推导是什么呢？就是它总是先把产生式中最左侧的非终结符展开完毕以后，再去展开下一个。这也就相当于对 AST 从左子节点开始的深度优先遍历。LL(1) 中的 1，指的是预读一个 Token。

### LR 算法：移进和规约
前面讲的递归下降和 LL 算法，都是自顶向下的算法。还有一类算法，是自底向上的，其中的代表就是 LR 算法。自顶向下的算法，是从根节点逐层往下分解，形成最后的 AST；而 LR 算法的原理呢，则是从底下先拼凑出 AST 的一些局部拼图，并逐步组装成一棵完整的 AST。所以，其中的关键之处在于如何“拼凑”。假设我们采用下面的上下文无关文法，来推演一个实例，具体语法规则如下所示：

```
start->add
add->add+mul
add->mul
mul->mul*pri
mul->pri
pri->Int
pri->(add)
```
如果用于解析“2+3*5”，最终会形成下面的 AST：

![img](https://static001.geekbang.org/resource/image/c7/8e/c7dc8d39cdbd32785e785c53e21e738e.jpg?wh=2284*1300)

图 6：2+3*5 对应的 AST

那算法是怎么从底部凑出这棵 AST 来的呢？LR 算法和 LL 算法一样，也是从左到右地消化掉 Token。在第 1 步，它会取出“2”这个 Token，放到一个栈里，这个栈是用来组装 AST 的工作区。同时，它还会预读下一个 Token，也就是“+”号，用来帮助算法做判断。在下面的示意图里，我画了一条橙色竖线，竖线的左边是栈，右边是预读到的一个 Token。在做语法解析的过程中，竖线会不断地往右移动，把 Token 放到栈里，这个过程叫做“移进”（Shift）。

![img](https://static001.geekbang.org/resource/image/6c/97/6c6ecc3391053afdbff110a4ada60d97.jpg?wh=2284*1487)

图 7：第 1 步，移进一个 Token

注意，我在图 7 中还用虚线框推测了 AST 的其他部分。也就是说，如果第一个 Token 遇到的是整型字面量，而后面跟着一个 + 号，那么这两个 Token 就决定了它们必然是这棵推测出来的 AST 的一部分。而图中右边就是它的推导过程，其中的每个步骤，都使用了一个产生式加了一个点（如“.add”）。这个点，就相当于图中左边的橙色竖线。所以你就可以根据这棵假想的 AST，也就是依据假想的推导过程，给它反推回去。把 Int 还原为 pri。这个还原过程，就叫做“规约”（Reduce）。工作区里的元素也随之更新成 pri。

![img](https://static001.geekbang.org/resource/image/a1/81/a1189673c2bc6964b43f32b7db2aa781.jpg?wh=2284*1216)

图 8：第 2 步，Int 规约为 pri

按照这样的思路，不断地移进和规约，这棵 AST 中推测出来的节点会不断地被证实。而随着读入的 Token 越来越多，这棵 AST 也会长得越来越高，整棵树变得更大。下图是推导过程中间的一个步骤。

![img](https://static001.geekbang.org/resource/image/41/82/412af5303189b7c80b0de5960cf85982.jpg?wh=2284*1487)

图 9：移进和规约过程中的一个步骤



最后，整个 AST 构造完毕，而工作区里也就只剩了一个 Start 节点。



![img](https://static001.geekbang.org/resource/image/5a/b7/5ae146b8f2b622741daff8ca97f995b7.jpg?wh=2284*1480)

图 10：最后一步，add 规约为 start

通过上面的介绍，你应该已经建立了对 LR 算法的直觉认识。如果要把这个推导过程写成严密的算法，你可以参考《编译原理之美》的第 18 讲。从示例中，你应该已经看出来了，相对于 LL 算法，LR 算法的优点是能够处理左递归文法。但它也有缺点，比如不利于输出全面的编译错误信息。因为在没有解析完毕之前，算法并不知道最后的 AST 是什么样子，所以也不清楚当前的语法错误在整体 AST 中的位置。最后我再提一下 LR 的意思，来帮你更完整地理解 LR 算法。L 还是代表从左到右读入 Token，而 R 是最右推导（Rightmost）的意思。我把“2+3*5”最右推导的过程写在了下面，而如果你从最后一行往前一步步地看，它恰好就是规约的过程。

![img](https://static001.geekbang.org/resource/image/ec/6f/ec26627ee1ca27094227c53a42d7476f.jpg?wh=2284*1080)

如果你见到 LR(k)，那它的意思就是会预读 k 个 Token，我们在示例中采用的是 LR(1)。

### 课程小结
今天花了一讲的时间，把语法分析的要点给你讲解了一下。对于上下文无关的文法，你要知道产生式、非终结符、终结符、EBNF 这几个基本概念，能够熟练阅读各种语言的语法规则，这是一个基本功。递归下降算法是另一项基本功，所以也一定要掌握。你要注意，递归下降是深度优先的，只有最左边的子树都生成完了，才会往右生成它的兄弟节点。有的同学会在没有把左侧的非终结符匹配完毕的情况下，就开始匹配右边的项，从而不自觉地采用了宽度优先的思路，这是我发现很多同学会容易陷入的一个思维误区。对于 LL 算法和 LR 算法，我只做了简单的讲解，目的是为了帮助你建立直观的理解。我们在后面的课程中，还会遇到使用它们的实际例子，到时你可以与这一讲的内容相互印证。

![img](https://static001.geekbang.org/resource/image/24/18/249343a116119f7d9e1e1e803d6c5318.jpg?wh=2284*2849)


## 04 | 语义分析：让程序符合语义规则
对计算机程序语义的研究，是一个专门的学科。要想很简单地把它讲清楚，着实不是太容易的事情。但我们可以退而求其次，只要能直观地去理解什么是语义就可以了。语义，就是程序要表达的意思。因为计算机最终是用来做计算的，那么理解程序表达的意思，就是要知道让计算机去执行什么计算动作，这样才好翻译成目标代码。那具体来说，语义分析要做什么工作呢？我们在第 1 讲中说过，每门计算机语言的标准中，都会定义很多语义规则，比如对加法运算要执行哪些操作。而在语义分析阶段，就是去检查程序是否符合这些语义规则，并为后续的编译工作收集一些语义信息，比如类型信息。再具体一点，这些语义规则可以分为两大类。

第一类规则与上下文有关。因为我们说了，语法分析只能处理与上下文无关的工作。而与上下文有关的工作呢，自然就放到了语义分析阶段。第二类规则与类型有关。在计算机语言中，类型是语义的重要载体。所以，语义分析阶段要处理与类型有关的工作。比如，声明新类型、类型检查、类型推断等。在做类型分析的时候，我们会用到一个工具，就是属性计算，也是需要你了解和掌握的。

补充：某些与类型有关的处理工作，还必须到运行期才能去做。比如，在多态的情况，调用一个方法时，到底要采用哪个子类的实现，只有在运行时才会知道。这叫做动态绑定。


在语义分析过程中，会使用两个数据结构。一个还是 AST，但我们会把语义分析时获得的一些信息标注在 AST 上，形成带有标注的 AST。另一个是符号表，用来记录程序中声明的各种标识符，并用于后续各个编译阶段。那今天这一讲，我就会带你看看如何完成与上下文有关的分析、与类型有关的处理，并带你认识符号表和属性计算。首先，我们来学习如何处理与上下文有关的工作。


### 上下文相关的分析
那什么是与上下文有关的工作呢？在解析一个程序时，会有非常多的分析工作要结合上下文来进行。接下来，我就以控制流检查、闭包分析和引用消解这三个场景和你具体分析下。

场景 1：控制流检查像 return、break 和 continue 等语句，都与程序的控制流有关，它们必须符合控制流方面的规则。在 Java 这样的语言中，语义规则会规定：如果返回值不是 void，那么在退出函数体之前，一定要执行一个 return 语句，那么就要检查所有的控制流分支，是否都以 return 语句结尾。


场景 2：闭包分析很多语言都支持闭包。而要正确地使用闭包，就必须在编译期知道哪些变量是自由变量。这里的自由变量是指在本函数外面定义的变量，但被这个函数中的代码所使用。这样，在运行期，编译器就会用特殊的内存管理机制来管理这些变量。所以，对闭包的分析，也是上下文敏感的。

场景 3：引用消解我们重点说一下引用消解，以及相关的作用域问题。引用消解（Reference Resolution），有时也被称作名称消解（Name Resolution）或者标签消解（Label Resolution）。对变量名称、常量名称、函数名称、类型名称、包名称等的消解，都属于引用消解。因此，引用消解是一种非常重要的上下文相关的语义规则，我来重点讲解下。

在高级语言里，我们会做变量、函数（或方法）和类型的声明，然后在其他地方使用它们。这个时候，我们要找到定义和使用之间的正确引用关系。我们来看一个例子。在语法分析阶段，对于“int b = a + 3”这样一条语句，无论 a 是否提前声明过，在语法上都是正确的。而在实际的计算机语言中，如果引用某个变量，这个变量就必须是已经声明过的。同时，当前这行代码，要处于变量 a 的作用域中才行。

![img](https://static001.geekbang.org/resource/image/50/03/5092c6f4103a967ea0609dceea873d03.jpg?wh=2284*1520)

图 1：变量引用的消解

对于变量来说，为了找到正确的引用，就需要用到作用域（Scope）这个概念。在编译技术里面，作用域这个词，有两个稍微有所差异的使用场景。作用域的第一个使用场景，指的是变量、函数等标识符可以起作用的范围。下图列出了三个变量的作用域，每个变量声明完毕以后，它的下一句就可以引用它。

![img](https://static001.geekbang.org/resource/image/a8/ba/a82fbf11da007a038ed2ee282b30c9ba.jpg?wh=2284*880)

图 2：变量的作用域

作用域的第二个使用场景，是词法作用域（Lexical Scope），也就是程序中的不同文本区域。比如，一个语句块、参数列表、类定义的主体、函数（方法）的主体、模块主体、整个程序等。到这里，咱们来总结下这两个使用场景。标识符和词法的作用域的差异在于：一个本地变量（标识符）的作用域，虽然属于某个词法作用域（如某个函数体），但其作用范围只是在变量声明之后的语句。而类的成员变量（标识符）的作用域，跟词法作用域是一致的，也就是整个类的范围，跟声明的位置无关。如果这个成员变量不是私有的，它的作用域还会覆盖到子类。那具体到不同的编程语言，它们的作用域规则是不同的。比如，C 语言里允许你在一个 if 语句块里定义一个变量，覆盖外部的变量，而 Java 语言就不允许这样。所以，在给 Java 做语义分析时，我们要检查出这种错误。

```
void foo(){
  int a = 2;
  if (...){
    int a = 3;   //在C语言里允许，在Java里不允许
    ...
  }
}
```
在做引用消解的时候，为了更好地查找变量、类型等定义信息，编译器会使用一个辅助的数据结构：符号表。


### 符号表（Symbol Table）
在写程序的时候，我们会定义很多标识符，比如常量名称、变量名称、函数名称、类名称，等等。在编译器里，我们又把这些标识符叫做符号（Symbol）。用来保存这些符号的数据结构，就叫做符号表。比如，对于变量 a 来说，符号表中的基本信息可以包括：

名称：a分类：变量类型：int作用域：foo 函数体其他必要的信息。

符号表的具体实现，每个编译器可能都不同。比如，它可能是一张线性的表格，也可能是按照作用域形成的一种有层次的表格。以下面这个程序为例，它包含了两个函数，每个函数里面都定义了多个变量：

```
void foo(){
  int a；
  int b；
  if (a>0){
    int c;
    int d;
  }
  else{
    int e;
    int f;
  }
}

void bar(){
  int g;
  {
    int h;
    int i;
  }
}
```
它的符号表可能是下面这样的，分成了多个层次，每个层次对应了一个作用域。在全局作用域，符号表里包含 foo 和 bar 两个函数。在 foo 函数体里，有两个变量 a 和 b，还有两个内部块，每个块里各有两个变量。

![img](https://static001.geekbang.org/resource/image/6e/6c/6ef69555fc2fc1abe06e206fab52ac6c.jpg?wh=2284*1056)

图 3：一种层次化的符号表



那针对引用消解，其实就是从符号表里查找被引用的符号的定义，如下图所示：

![img](https://static001.geekbang.org/resource/image/1a/4e/1aca158938cf6e9895dce3f954b3db4e.jpg?wh=2283*2020)

图 4：利用符号表帮助做引用消解

更进一步地，符号表除了用于引用消解外，还可以辅助完成语义分析的其他工作。比如，在做类型检查的时候，我们可以从符号表里查找某个符号的类型，从而检查类型是否兼容。其实，不仅仅是在语义分析阶段会用到符号表，其他的编译阶段也会用到。比如，早在词法分析阶段，你就可以为符号表建立条目；在生成 IR、做优化和生成目标代码的时候，都会用到符号表里的信息。

![img](https://static001.geekbang.org/resource/image/36/31/3608f553a443cd9a4a59934b7e937e31.jpg?wh=2284*852)

图 5：编译过程中的每个阶段，都可能会使用符号表

有的编译器，在前期做语法分析的时候，如果不依赖符号表的话，它是不可能完整地做语法分析的。甚至，除了编译阶段，在链接阶段，我们也要用到符号表。比如，在 foo.c 中定义了一个函数 foo()，并编译成目标文件 foo.o，在 bar.c 中使用了这个 foo() 函数。那么在链接的时候，链接器需要找到 foo() 函数的地址。为了满足这个场景，你必须在目标文件中找到 foo 符号的相关信息。同样的道理，在 Java 的字节码文件里也需要保存符号信息，以便在加载后我们可以定位其中的类、方法和成员变量。好了，以上就是语义分析的第一项重要工作上下文相关的分析，以及涉及的数据结构符号表的重点内容了。我们再来考察一下语义分析中第二项重要的工作：类型分析和处理。


### 类型分析和处理
语义分析阶段的一个重要工作就是做类型检查，现代语言还普遍增加了类型推断的能力。那什么是类型呢？通常来说，在计算机语言里，类型是数据的一个属性，它的作用是来告诉编译器或解释器，程序可以如何使用这些数据。比如说，对于整型数据，它可能占 32 或者 64 位存储，我们可以对它做加减乘除操作。而对于字符串，它可能占很多个字节，并且通过一定的编码规则来表示字符。字符串可以做连接、查找、获取子字符串等操作，但不能像整数一样做算术运算。

一门语言的类型系统是包含了与类型有关的各种规则的一个逻辑系统。类型系统包含了一系列规则，规定了如何把类型用于变量、表达式和函数等程序元素，以及如何创建自定义类型，等等。比如，如果你定义了某个类有哪些方法，那你就只能通过调用这些方法来使用这个类，没有别的方法。这些强制规定减少了程序出错的可能性。所以在语义分析阶段，一个重要的工作就是做类型检查。那么，类型检查是怎样实现的呢？我们要如何做类型检查呢？关于类型检查，编译器一般会采用属性计算的方法，来计算出每个 AST 节点的类型属性，然后检查它们是否匹配。

### 属性计算
以“int b = a+3”为例，它的 AST 如下图所示。编译器会计算出 b 节点所需的类型和 init 节点的实际类型，比较它们是否一致（或者可以自动转换）。

![img](https://static001.geekbang.org/resource/image/16/60/16772e733d25f008332dcc507b544960.jpg?wh=2284*880)

图 6：“int b = a+3”对应的 AST

我们首先要计算等号右边“a+3”的类型。其中，3 是个整型字面量，我们可以据此把它的类型标注为整型；a 是一个变量，它的类型可以从符号表中查到，也是整型。那么“a+3”是什么类型呢？根据加法的语义，两个整型数据相加，结果仍然是整型，因此“a+3”这个表达式整体是整型的。因为 init 只有一个子节点（add），所以 init 的类型也一样是整型。在刚才这段推理中，我们实际上是依据“a+3”的 AST，从下级节点的类型计算出上级节点的类型。

那么，我们能否以同样的方法计算 b 节点的类型呢？答案是不可以。因为 b 根本没有子节点。但声明变量 b 的时候，有个 int 关键字，所以在 AST 中，b 有一个兄弟节点，就是 int 关键字。根据变量声明的语义，b 的类型就是 int，因此它的类型是从 AST 的兄弟节点中获得的。你看，同样是计算 AST 节点的类型，等号右边和左边的计算方法是不一样的。

实际上，我们刚才用的分析方法，就是属性计算。其中，有些属性是通过子节点计算出来的，这叫做 S 属性（Synthesized Attribute，综合出来的属性），比如等号右边的类型。而另一些属性，则要根据父节点或者兄弟节点计算而来，这种属性叫做 I 属性（Inherited Attribute，继承到的属性），比如等号左边的 b 变量的类型。计算出来的属性，我们可以标注在 AST 上，这就形成我第 1 讲曾经提过的带有标注信息的 AST，（Annotated Tree），也有人称之为 Decorated Tree，或者 Attributed Tree。虽然叫法有很多，但都是一个意思，都是向 AST 中添加了语义信息。

![img](https://static001.geekbang.org/resource/image/7a/a2/7a72cb24e044c68c4ccda78ee40fafa2.jpg?wh=2284*1520)

图 7：带有标注信息的 AST

属性计算的方法，就是基于语法规则，来定义一些属性计算的规则，在遍历 AST 的时候执行这些规则，我们就可以计算出属性值。这种基于语法规则定义的计算规则，被叫做属性文法（Attribute Grammar）。补充：基于属性计算的方法可以做类型检查，那其实也可以做类型推断。有些现代语言在声明一个变量的时候，可以不明确指定的类型，那么它的类型就可以通过变量声明语句的右边部分推断出来。你可能会问，属性计算的方法，除了计算类型，还可以计算什么属性呢？

根据不同语言的语义，可能有不同的属性需要计算。其实，value（值）也可以看做是一个属性，你可以给每个节点定义一个“value”属性。对表达式求值，也就是对 value 做属性计算，比如，“a + 3”的值，我们就可以自下而上地计算出来。这样看起来，value 是一个 S 属性。针对 value 这个属性的属性文法，你可以参考下面这个例子，在做语法解析（或先解析成 AST，再遍历 AST）的时候，执行方括号中的规则，我们就可以计算出 AST 的值了。
```
add1 → add2 + mul [ add1.value = add2.value + mul.value ]
add → mul [ add.value = mul.value ]
mul1 → mul2 * primary [ mul1.value = mul2.value * primary.value ]
mul → primary [ mul.value = primary.value ]
primary → ( add ) [ primary.value =  add.value ]
primary → integer [ primary.value = strToInt(integer.str) ]

```
这种在语法规则上附加一系列动作，在解析语法的时候执行这些动作的方式，是一种编译方法，在龙书里有一个专门的名字，叫做语法制导的翻译（Syntax Directed Translation，SDT）。使用语法制导的翻译可以做很多事情，包括做属性计算、填充符号表，以及生成 IR。

### 课程小结
在实际的编译器中，语义分析相关的代码量往往要比词法分析和语法分析的代码量大。因为一门语言有很多语义规则，所以要做的语义分析和检查工作也很多。并且，因为每门语言之间的差别主要都体现在语义上，所以每门语言在语义处理方面的工作差异也比较大。比如，一门语言支持闭包，另一门语言不支持；有的语言支持泛型，另一门语言不支持；一门语言的面向对象特性是基于继承实现的，而另一门语言则是基于组合实现的，等等。不过，这没啥关系。我们主要抓住它们的共性就好了。这些共性，就是我们本讲的内容：

做好上下文相关的分析，比如对各种引用的消解、控制流的检查、闭包的分析等；做好与类型有关的分析和处理，包括类型检查、类型推断等；掌握属性计算这个工具，用于计算类型、值等属性；最后，把获得的语义信息保存到符号表和 AST 里。

我把本讲的知识点也整理成了脑图，供你参考：

![img](https://static001.geekbang.org/resource/image/f5/1e/f56cdb074915e743e9c21fd3c5eff11e.jpg?wh=2284*2644)


## 05 | 运行时机制：程序如何运行，你有发言权

![img](https://static001.geekbang.org/resource/image/f4/72/f4535fcf01b83075b24096091b0ddb72.jpg?wh=2284*3027)


## 06 | 中间代码：不是只有一副面孔

![img](https://static001.geekbang.org/resource/image/19/55/19a96336d4b579a86263f82d98c72255.jpg?wh=2284*2965)


## 07 | 代码优化：跟编译器做朋友，让你的代码飞起来

![img](https://static001.geekbang.org/resource/image/ec/a0/ec83e01a9471d19b285aac365694c3a0.jpg?wh=2284*3438)

## 08 | 代码生成：如何实现机器相关的优化？

![img](https://static001.geekbang.org/resource/image/b7/62/b71852806b0a7531c62c2ddc3fe51e62.jpg?wh=2284*2691)

## 知识地图 | 一起来复习编译技术核心概念与算法

![img](https://static001.geekbang.org/resource/image/a2/22/a260d574faae4a033b9ae3f616d45222.jpg?wh=2911*14671)


## 真实编译器解析篇
## 24 | Go语言编译器：把它当作教科书吧
宫文学 2020-07-27

你好，我是宫文学。今天这一讲，我来带你研究一下 Go 语言自带的编译器，它可以被简称为 gc。我之所以要来带你研究 Go 语言的编译器，一方面是因为 Go 现在确实非常流行，很多云端服务都用 Go 开发，Docker 项目更是巩固了 Go 语言的地位；另一方面，我希望你能把它当成编译原理的教学参考书来使用。这是因为：


Go 语言的编译器完全用 Go 语言本身来实现，它完全实现了从前端到后端的所有工作，而不像 Java 要分成多个编译器来实现不同的功能模块，不像 Python 缺少了后端，也不像 Julia 用了太多的语言。所以你研究它所采用的编译技术会更方便。Go 编译器里基本上使用的都是经典的算法：**经典的递归下降算法、经典的 SSA 格式的 IR 和 CFG、经典的优化算法、经典的 Lower 和代码生成**，因此你可以通过一个编译器就把这些算法都贯穿起来。除了编译器，你还可以学习到一门语言的其他构成部分的实现思路，包括**运行时（垃圾收集器、并发调度机制等）、标准库和工具链，甚至连链接器都是用 Go 语言自己实现的**，从而对实现一门语言所需要做的工作有更完整的认识。最后，Go 语言的实现继承了从 Unix 系统以来形成的一些良好的设计哲学，因为 Go 语言的核心设计者都是为 Unix 的发展，做出过重要贡献的极客。因此了解了 Go 语言编译器的实现机制，会提高你的软件设计品味。

扩展：每种语言都有它的个性，而这个个性跟语言设计者的背景密切相关。Go 语言的核心设计者，是 Unix 领域的极客，包括 Unix 的创始人和 C 语言的共同发明人之一，Ken Tompson。Rob Pike 也是 Unix 的核心作者。Go 语言的作者们显然希望新的语言体现出他们的设计哲学和口味。比如，致力于像 Unix 那样的简洁和优雅，并且致力于让 Go 再次成为一款经典作品。

所以，在已经研究了多个高级语言的编译器之后，我们可以拿 Go 语言的编译器，把整个编译过程再重新梳理和印证一遍。好了，现在就开始我们今天探索的旅途吧。首先，我们来看看 Go 语言编译器的前端。

重要提示：照例，你要下载 Go 语言的源代码，本讲采用的是 1.14.2 版本。并且，你最好使用一个 IDE，便于跟踪调试编译器的执行过程。Go 的源代码中附带的介绍编译器的文档，写得很好、很清晰，你可以参考一下。

### 词法分析和语法分析
Go 的编译器的词法分析和语法分析功能的实现，是在 cmd/compile/internal/syntax 目录下。词法分析器是 scanner.go。其实大部分编程语言的词法分析器的算法，都已经很标准了，我们在Java 编译器里就曾经分析过。甚至它们处理标识符和关键字的方式也都一致，都是先作为标识符识别出来，然后再查表挑出关键字来。Go 的词法分析器并没有像 V8 那样在不遗余力地压榨性能，它跟你平常编码的方式是很一致的，非常容易阅读。

语法分析器是 parser.go。它是一个标准的手写的递归下降算法。在解析二元表达式的时候，Go 的语法分析器也是采用了**运算符优先级算法**，这个已经是我们第 N 次见到这个算法了，所以你一定要掌握！不过，每个编译器的实现都不大一样，而 Go 的实现方式相当的简洁，你可以去自己看一下，或者用调试器来跟踪一下它的执行过程。

![img](https://static001.geekbang.org/resource/image/1e/4c/1e0b9ae47048ed77329d941c4a8e374c.jpg?wh=1110*788)

图 1：用 IDE 工具 Goland 跟踪调试编译过程

Go 的 AST 的节点，是在 nodes.go 中定义的，它异常简洁，可以说简洁得让你惊讶。你可以欣赏一下。

Go 的语法分析器还有一个很有特色的地方，就是对错误的处理。它在处理编译错误时，有一个原则，就是不要遇到一个错误就停止编译，而是要尽可能跳过当前这个出错的地方，继续往下编译，这样可以一次多报几个语法错误。

parser.go 的处理方式是，当语法分析器在处理某个产生式的时候，如果发现了错误，那就记录下这个错误，并且往下跳过一些 Token，直到找到一个 Token 是属于这个产生式的 Follow 集合的。这个时候编译器就认为找到了这个产生式的结尾。这样分析器就可以跳过这个语法单元，继续处理下面的语法单元。

比如，在[解析函数声明语句](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/syntax/parser.go#L614)时，如果 Go 的语法分析器没有找到函数名称，就报错“expecting name or (”，然后往后找到“{”或者“;”，这样就跳过了函数名称的声明部分，继续去编译后面的函数体部分。

在 cmd/compile/internal/syntax 目录下，还有词法分析器和语法分析器的测试程序，你可以去运行测试一下。

最后，如果你还想对 Go 语言的语法分析有更加深入地了解，我建议你去阅读一下[Go 语言的规范](https://go.dev/ref/spec)，它里面对于每个语法单元，都有 EBNF 格式的语法规则定义，比如对[语句的定义](https://go.dev/ref/spec#Statements)。你通过看代码、看语言规范，积累语法规则的第一手经验，以后再看到一段程序，你的脑子里就能反映出它的语法规则，并且能随手画出 AST 了，这是你学习编译原理需要建立的硬功夫。比如说，这里我节选了一段 Go 语言的规范中针对语句的部分语法规则。



```go

Statement =
  Declaration | LabeledStmt | SimpleStmt |
  GoStmt | ReturnStmt | BreakStmt | ContinueStmt | GotoStmt |
  FallthroughStmt | Block | IfStmt | SwitchStmt | SelectStmt | 
    ForStmt | DeferStmt .

SimpleStmt = EmptyStmt | ExpressionStmt | SendStmt | IncDecStmt |    
    Assignment | ShortVarDecl .
```

好，在了解了 Go 语言编译器的语法分析工作以后，接下来，我们再来看看它的语义分析阶段。



### 语义分析（类型检查和 AST 变换）
语义分析的程序，是在 cmd/compile/internal/gc 目录下（注意，gc 的意思是 Go Compiler，不是垃圾收集的意思）。在入口代码 main.go 中，你能看到整个编译过程的主干步骤。语义分析的主要程序是在[typecheck.go](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/gc/typecheck.go)中。这里你要注意，不要被“typecheck”的名称所误导，它其实不仅是做类型检查，还做了名称消解（Name Resolution）和类型推导。

你已经知道，名称消解算法的特点，是分阶段完成。举个例子，在给表达式“a=b”中的变量 b 做引用消解之前，编译器必须先处理完 b 的定义，比如“var b Person”，这样才知道符号 b 指的是一个 Person 对象。

另外，在前面学习Java 编译器的时候，你已经知道，对方法体中的本地变量的消解，必须放在最后，才能保证变量的使用总是引用到在它前面的变量声明。Go 的编译器也是采用了相同的实现思路，你可以借此再回顾一下这个知识点，加深认识。

在语义分析阶段，Go 的编译器还做了一些 AST 变换的工作。其中就有[内联优化](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/gc/inl.go)和[逃逸分析](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/gc/escape.go)这两项工作。在我们之前解析的编译器当中，这两项工作都是基于专门做优化的 IR（比如 Sea of Nodes）来做的，而在 Go 的编译器里，却可以基于 AST 来做这两项优化。你看，是不是真实世界中的编译器，才能让你如此开阔眼界？

你可以用“-m”参数来编译程序，它会打印出与内联和逃逸方面有关的优化。你可以带上多个“-m”参数，打印出嵌套层次更深的算法步骤的决策。

```
go build -gcflags '-m -m' hello.go 
```
好了，现在我们借 gc 编译器，又复习了一遍语义分析中的一些关键知识点：**名称消解算法要分阶段**，在语义分析阶段会对 AST 做一些变换。我们继续来看 gc 编译器下一步的处理工作。

### 生成 SSA 格式的 IR
gc 编译器在做完语义分析以后，下一步就是生成 IR 了。并且，gc 的 IR 也是 SSA 格式的。你可以通过 gc，来进一步了解如何生成和处理 SSA 格式的 IR。好，首先，我们来看看 Go 语言的 IR 是什么样子的。针对下面的示例代码 foo.go，我们来看下它对应的 SSA 格式的 IR：

```
package main

func Foo(a int) int {
   var b int
   if (a > 10) {
      b = a
   } else {
      b = 10
   }
   return b
}
```
在命令行中输入下面的命令，让 gc 打印出为 foo 函数生成的 IR。在当前目录下，你能看到一个 ssa.html 文件，你可以在浏览器中打开它。
```
goSSAFUNC=Foo go build -gcflags '-S' foo.go
```
在这个文件当中，你能看到编译器对 IR 做了多步的处理，也能看到每次处理后所生成的 IR。gc 的 IR 是基于控制流图（CFG）的。一个函数会被分成多个基本块，基本块中包含了一行行的指令。点击某个变量，你能看到它的定义和使用情况（def-use 链，图中显示成绿色）。你还能看到，图中灰色的变量，根据定义和使用关系，会发现它们没有被使用，所以是死代码，可以删除。

![img](https://static001.geekbang.org/resource/image/a8/66/a8c648560c0b03b23c3b39a95cbd5b66.jpg?wh=1920*654)

图 2：foo 示例程序各个处理阶段的 IR

针对第一个阶段（Start 阶段），我来给你解释一下每行指令的含义（可参考 genericOps.go），帮助你了解 Go 语言的 IR 的设计特点。

![img](https://static001.geekbang.org/resource/image/b1/a5/b1d104eaac5d7f04a3dc59f7a6d2a2a5.jpg?wh=2284*2949)



你可以参考代码库中[介绍 SSA 的文档](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/README.md)，里面介绍了 Go 的 SSA 的几个主要概念。

下面我来给你解读一下。

首先是[值（Value）](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/value.go#L16)。Value 是 SSA 的最主要构造单元，它可以定义一次、使用多次。在定义一个 Value 的时候，需要一个标识符（ID）作为名称、产生该 Value 的操作码（[Op](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/op.go)）、一个类型（[Type](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/types/type.go#L118)，就是代码中 <> 里面的值），以及一些参数。

操作码有两类。一类是机器无关的，其定义在[genericOps.go](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/gen/genericOps.go)中；一类是机器相关的，它是面向特定的 CPU 架构的，其定义在 XXXOps.go 中。比如，[AMD64Ops.go](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/gen/AMD64Ops.go#L166)中是针对 AMD64 架构 CPU 的操作码信息。

在做 Lower 处理时，编译器会把机器无关的操作码转换为机器相关的操作码，有利于后序生成目标代码。机器无关的优化和机器相关的优化，分别作用在采用这两类不同操作码的 IR 上。

Value 的类型信息，通常就是 Go 语言中的类型。但有几个类型是只会在 SSA 中用到的[特殊类型](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/types/type.go#L1472)，就像上面语句中的，即内存 ([TypeMem](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/types/type.go#L1475)) 类型；以及 TypeFlags，也就是 CPU 的标志位类型。

这里我要特别讲一下内存类型。内存类型代表的是全局的内存状态。如果一个操作码带有一个内存类型的参数，那就意味着该操作码依赖该内存状态。如果一个操作码的类型是内存类型，则意味着它会影响内存状态。SSA 的介绍文档中有一个例子，能帮助你理解内存类型的用法。在这个例子中，程序首先会向地址 a 写入 3 这个值。这个时候，内存状态就修改了（从 v1 到了 v10）。接着，把地址 a 的值写入地址 b，内存状态又发生了一次修改。在 IR 中，第二行代码依赖第一行代码的内存状态（v10），因此就导致这行代码只能出现在定义了 v10 之后。

```
// *a = 3       //向a地址写入3
// *b = *a      //向b地址写入a的值
v10 = Store <mem> {int} v6 v8 v1
v14 = Store <mem> {int} v7 v8 v10
```
这里你需要注意，对内存的读和写（各种 IR 一般都是使用 Load 和 Store 这两个词汇）是一类比较特殊的指令。其他的 Value，我们都可以认为它们是在寄存器中的，是计算过程中的临时变量，所以它们在代码中的顺序只受数据流中依赖关系的制约。而一旦中间有读写内存的操作，那么代码顺序就会受到一定的限制。

我们可以跟在[Graal 编译器](https://time.geekbang.org/column/article/256914)中学到的知识印证一下。当你读写一个 Java 对象的属性的时候，也会涉及内存的读写，这些操作对应的 IR 节点，在顺序上也是受到限制的，我们把它们叫做固定节点。

此外，Value 结构中还包含了两个辅助信息字段：AuxInt 和 Aux。AuxInt 是一个整型值，比如，在使用 Const64 指令中，AuxInt 保存了常量的值；而 Aux 则可能是个复杂的结构体，用来保存每个操作码的个性化的信息。

在 IR 中你还能看到基本块（[Block](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/block.go)），这是第二个重要的数据结构。Go 编译器中的基本块有三种：简单（Plain）基本块，它只有一个后继基本块；退出（Exit）基本块，它的最后一个指令是一个返回指令；还有 if 基本块，它有一个控制值，并且它会根据该值是 true 还是 false，跳转到不同的基本块。

第三个数据结构是函数（[Func](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/func.go)）。函数是由多个基本块构成的。它必须有一个入口基本块（Entry Block），但可以有 0 到多个退出基本块，就像一个 Go 函数允许包含多个 Return 语句一样。

现在，你已经知道了 Go 的 IR 的关键概念和相关的数据结构了。Go 的 IR 在运行时就是保存在 Value、Block、Func 等内存结构中，就像 AST 一样。它不像 LLVM 的 bitcode 还有文本格式、二进制格式，可以保存在文件中。

那么接下来，编译器就可以基于 IR，来做优化了。



### 基于 SSA 格式的 IR 做优化
SSA 格式的 IR 对编译器做优化很有帮助。

以死代码删除为例，Value 结构中有一个 Uses 字段，记录了它的使用数。如果它出现在另一个 Value 的操作码的参数里，或者是某个基本块的控制变量，那么使用数就会加 1；而如果 Uses 字段的值是 0，那就证明这行代码没什么用，是死代码，可以删掉。

而你应该记得，在第 7 讲中曾提到过，我们需要对一个函数的所有基本块都扫描一遍甚至多遍，才能知道某个变量的活跃性，从而决定是否可以删除掉它。那相比起来，采用 SSA 格式，可以说简单太多了。

基于这样的 IR 来做优化，就是对 IR 做很多遍（Pass）的处理。在cmd/compile/internal/ssa/compile.go的代码里，列出了所有这些 Pass，有将近 50 个。你能看到每个处理步骤执行的是哪个优化函数，你还可以在 ssa.html 中，看到每个 Pass 之后，IR 都被做了哪些修改。

![img](https://static001.geekbang.org/resource/image/yy/f8/yy0a23bdf773d2yy1d7234d95578eff8.jpg?wh=1920*719)

图 3：compiler.go 中的 Pass



这些处理算法都是在cmd/compile/internal/ssa目录下。比如cse.go里面是消除公共子表达式的算法，而nilcheck.go是被用来消除冗余的 nil 检查代码。

有些算法还带了测试程序（如cse_test.go，nilcheck_test.go）。你可以去阅读一下，看看测试程序是如何构造测试数据的，并且你还可以通过 Debugger 来跟踪测试程序的执行过程，从而理解相关优化算法是如何实现的，这是一个很有效的学习方式。

另外，gc 还有一些比较简单的优化算法，它们是基于一些规则，对 IR 做一些重写（rewrite）。Go 的编译器使用了自己的一种 DSL，来描述这些重写规则：针对机器无关的操作码的重写规则，是在[generic.rules](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/gen/generic.rules)文件中；而针对机器有关的操作码的重写规则是在 XXX.rules 中，比如[AMD64.rules](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/gen/AMD64.rules)。

我们来看几个例子：在 generic.rules 中，有这样一个机器无关的优化规则，它是把 x*1 的运算优化为 x。

![img](https://static001.geekbang.org/resource/image/c0/fb/c091e058e96d6f166fa5737b8d9a80fb.jpg?wh=1438*186)

图 4：把 x*1 的运算优化为 x 的规则

在 AMD64.rules 中，有一个机器相关的优化规则，这个规则是把 MUL 指令转换为 LEA 指令，LEA 指令比 MUL 指令消耗的时钟周期更少。

```
(MUL(Q|L)const [ 3] x) -> (LEA(Q|L)2 x x)
```
generic.rules 中的规则会被[rulegen.go](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/gen/rulegen.go)解析，并生成 Go 代码[rewritegeneric.go](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/rewritegeneric.go)。而 AMD64.rules 中的规则，被解析后会生成[rewriteAMD64.go](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/rewriteAMD64.go)。其中，Lower 的过程，也就是把机器无关的操作码转换为机器相关的操作码，它也是用这种重写规则实现的。

通过 gc 这种基于规则做指令转换的方法，你应该产生一个感悟，也就是在写软件的时候，我们经常要设计自己的 DSL，让自己的软件更具灵活性。比如，gc 要增加一个新的优化功能，只需要增加一条规则就行了。我们还可以再拿 Graal 编译器印证一下。你还记得，Graal 在生成 LIR 的时候，要进行指令的选择，那些选择规则是用注解来生成的，而那些注解规则，也是一种 DSL。

好了，谈完了优化，我们继续往下看。


### 生成机器码
最后，编译器就可以调用 gc/ssa.go 中的[genssa](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/gc/ssa.go#L5899)方法，来生成汇编码了。在 ssa.html 的最右边一栏，就是调用 genssa 方法以后生成的汇编代码（采用的是 Go 编译器特有的格式，其中有些指令，如 PCDATA 和 FUNCDATA 是用来与垃圾收集器配合的）。

![img](https://static001.geekbang.org/resource/image/60/93/60798f64cbcd63d45671412712b49893.jpg?wh=1234*876)

你可能会问，编译器在生成机器码之前，不是还要做指令选择、寄存器分配、指令排序吗？那我们看看 gc 是如何完成这几项任务的。

寄存器分配（[regalloc.go](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/regalloc.go)）作为一个 Pass，已经在生成机器码之前执行了。它采用的是线性扫描算法（Linear Scan Register Allocator）。

指令选择会分为两部分的工作。一部分工作，是在优化算法中已经做了一些指令选择，我们前面提到的重写规则，就蕴含了根据 IR 的模式，来生成合适的指令的规则；另一部分工作，则放到了汇编器当中。

这就是 Go 的编译器与众不同的地方。原来，gc 生成的汇编代码，是一种“伪汇编”，它是一种半抽象的汇编代码。在生成特定 CPU 的机器码的时候，它还会做一些转换，这个地方可以完成另一些指令选择的工作。

至于指令排序，我没看到过在 gc 编译器中的实现。我请教了谷歌的一位研究员，他给我的信息是：像 AMD64 这样的 CPU，已经能够很好地支持乱序执行了，所以指令重排序给 gc 编译器的优化工作，带来的好处很有限。

而 gc 目前没有做指令排序，还有一个原因就是，指令重排序算法的实现代价比较高，而 gc 的一个重要设计目标，就是要求编译速度要快。

扩展：Go 语言的另外两个编译器，gccgo 和 GoLLVM 都具备指令重排序功能。

### 课程小结
这一讲，我给你介绍了 gc 编译器的主要特点。之所以能压缩在一讲里面，是因为你已经见识了好几款编译器，渐渐地可以触类旁通、举一反三了。

在 gc 里面，你能看到很多可以借鉴的成熟实践：语法分析：递归下降算法，加上针对二元表达式的运算符优先级算法；语义分析：分阶段的名称消解算法，以及对 AST 的转换；优化：采用了 SSA 格式的 IR、控制流图（CFG）、多个 Pass 的优化框架，以及通过 DSL 支持的优化规则。

所以在这一讲的开头，我还建议你把 Go 语言的编译器作为你学习编译原理的“教学参考书”，建议你在图形化的 IDE 界面里，来跟踪调试每一个功能，这样你就能很方便去观察它的算法执行过程。本讲的思维导图如下：

![img](https://static001.geekbang.org/resource/image/b4/47/b4d6d2e094c9d2485303065945781047.jpg?wh=2284*2521)


### 参考资料
[Introduction to the Go compiler](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/README.md) 官方文档，介绍了 gc 的主要结构。[Introduction to the Go compiler’s SSA backend](https://github.com/golang/go/blob/release-branch.go1.14/src/cmd/compile/internal/ssa/README.md) 官方文档，介绍了 gc 的 SSA。Go compiler internals: adding a new statement to Go - [Part 1](https://eli.thegreenplace.net/2019/go-compiler-internals-adding-a-new-statement-to-go-part-1/)、[Part2](https://eli.thegreenplace.net/2019/go-compiler-internals-adding-a-new-statement-to-go-part-2/)。在这两篇博客里，作者做了一个实验：如果往 Go 里面增加一条新的语法规则，需要做哪些事情。你能贯穿性地了解一个编译器的方法。[Go compiler: SSA optimization rules description language](https://quasilyte.dev/blog/post/go_ssa_rules/)这篇博客，详细介绍了 gc 编译器的 SSA 优化规则描述语言的细节。[A Primer on Go Assembly](https://github.com/teh-cmc/go-internals/blob/master/chapter1_assembly_primer/README.md)和[A Quick Guide to Go’s Assembler](https://go.dev/doc/asm) 。gc 编译器采用的汇编语言是它自己的一种格式，是“伪汇编”。这两篇文章中有 Go 汇编的细节。


## 25 | MySQL编译器（一）：解析一条SQL语句的执行过程

数据库系统能够接受 SQL 语句，并返回数据查询的结果，或者对数据库中的数据进行修改，可以说几乎每个程序员都使用过它。而 MySQL 又是目前使用最广泛的数据库。所以，解析一下 MySQL 编译并执行 SQL 语句的过程，一方面能帮助你加深对数据库领域的编译技术的理解；另一方面，由于 SQL 是一种最成功的 DSL（特定领域语言），所以理解了 MySQL 编译器的内部运作机制，也能加深你对所有使用数据操作类 DSL 的理解，比如文档数据库的查询语言。另外，解读 SQL 与它的运行时的关系，也有助于你在自己的领域成功地使用 DSL 技术。

那么，数据库系统是如何使用编译技术的呢？接下来，我就会花两讲的时间，带你进入到 MySQL 的内部，做一次全面的探秘。今天这一讲，我先带你了解一下如何跟踪 MySQL 的运行，了解它处理一个 SQL 语句的过程，以及 MySQL 在词法分析和语法分析方面的实现机制。好，让我们开始吧！

### 编译并调试 MySQL
按照惯例，你要下载MySQL 的源代码。我下载的是 8.0 版本的分支。源代码里的主要目录及其作用如下，我们需要分析的代码基本都在 sql 目录下，它包含了编译器和服务端的核心组件。

![img](https://static001.geekbang.org/resource/image/b8/c6/b8c9a108f1370bace3b1d8b3300b7ec6.jpg?wh=2284*569)

图 1：MySQL 的源代码包含的主要目录

MySQL 的源代码主要是.cc 结尾的，也就是说，MySQL 主要是用 C++ 编写的。另外，也有少量几个代码文件是用 C 语言编写的。为了跟踪 MySQL 的执行过程，你要用 Debug 模式编译 MySQL，具体步骤可以参考这篇[开发者文档](https://dev.mysql.com/doc/internals/en/cmake.html)。如果你用单线程编译，大约需要 1 个小时。编译好以后，先初始化出一个数据库来：

```
./mysqld --initialize --user=mysql
```

这个过程会为 root@localhost 用户，生成一个缺省的密码。接着，运行 MySQL 服务器：

```
./mysqld &
```
之后，通过客户端连接数据库服务器，这时我们就可以执行 SQL 了：

```
./mysql -uroot -p   #连接mysql server
```
最后，我们把 GDB 调试工具附加到 mysqld 进程上，就可以对它进行调试了。



```
gdb -p `pidof mysqld`  #pidof是一个工具，用于获取进程的id，你可以安装一下
```
提示：这一讲中，我是采用了一个 CentOS 8 的虚拟机来编译和调试 MySQL。我也试过在 macOS 下编译，并用 LLDB 进行调试，也一样方便。

注意，你在调试程序的时候，有两个设置断点的好地方：dispatch_command：在 sql/sql_parse.cc 文件里。在接受客户端请求的时候（比如一个 SQL 语句），会在这里集中处理。my_message_sql：在 sql/mysqld.cc 文件里。当系统需要输出错误信息的时候，会在这里集中处理。

这个时候，我们在 MySQL 的客户端输入一个查询命令，就可以从雇员表里查询姓和名了。在这个例子中，我采用的数据库是 MySQL 的一个示例数据库 [employees](https://github.com/datacharmer/test_db)，你可以根据它的文档来生成示例数据库。

```
mysql> select first_name, last_name from employees;    #从mysql库的user表中查询信息
```
这个命令被 mysqld 接收到以后，就会触发断点，并停止执行。这个时候，客户端也会老老实实地停在那里，等候从服务端传回数据。即使你在后端跟踪代码的过程会花很长的时间，客户端也不会超时，一直在安静地等待。给我的感觉就是，MySQL 对于调试程序还是很友好的。在 GDB 中输入 bt 命令，会打印出调用栈，这样你就能了解一个 SQL 语句，在 MySQL 中执行的完整过程。为了方便你理解和复习，这里我整理成了一个表格：

![img](https://static001.geekbang.org/resource/image/c8/5e/c8115701536a1d0ba093e804bf13735e.jpg?wh=2284*2247)

我也把 MySQL 执行 SQL 语句时的一些重要程序入口记录了下来，这也需要你重点关注。它反映了执行 SQL 过程中的一些重要的处理阶段，包括语法分析、处理上下文、引用消解、优化和执行。你在这些地方都可以设置断点。

![img](https://static001.geekbang.org/resource/image/82/90/829ff647ecefed1ca2653696085f7a90.jpg?wh=2284*1403)

好了，现在你就已经做好准备，能够分析 MySQL 的内部实现机制了。不过，由于 MySQL 执行的是 SQL 语言，它跟我们前面分析的高级语言有所不同。所以，我们先稍微回顾一下 SQL 语言的特点。


### SQL 语言：数据库领域的 DSL
SQL 是结构化查询语言（Structural Query Language）的英文缩写。举个例子，这是一个很简单的 SQL 语句：


```
select emp_no, first_name, last_name from employees;
```
其实在大部分情况下，SQL 都是这样一个一个来做语句执行的。这些语句又分为 DML（数据操纵语言）和 DDL（数据定义语言）两类。前者是对数据的查询、修改和删除等操作，而后者是用来定义数据库和表的结构（又叫模式）。我们平常最多使用的是 DML。而 DML 中，执行起来最复杂的是 select 语句。所以，在本讲，我都是用 select 语句来给你举例子。那么，SQL 跟我们前面分析的高级语言相比有什么不同呢？

第一个特点：SQL 是声明式（Declarative）的。这是什么意思呢？其实就是说，SQL 语句能够表达它的计算逻辑，但它不需要描述控制流。高级语言一般都有控制流，也就是详细规定了实现一个功能的流程：先调用什么功能，再调用什么功能，比如 if 语句、循环语句等等。这种方式叫做命令式（imperative）编程。更深入一点，声明式编程说的是“要什么”，它不关心实现的过程；而命令式编程强调的是“如何做”。前者更接近人类社会的领域问题，而后者更接近计算机实现。

第二个特点：SQL 是一种特定领域语言（DSL，Domain Specific Language），专门针对关系数据库这个领域的。SQL 中的各个元素能够映射成关系代数中的操作术语，比如选择、投影、连接、笛卡尔积、交集、并集等操作。它采用的是表、字段、连接等要素，而不需要使用常见的高级语言的变量、类、函数等要素。

所以，SQL 就给其他 DSL 的设计提供了一个很好的参考：采用声明式，更加贴近领域需求。比如，你可以设计一个报表的 DSL，这个 DSL 只需要描述报表的特征，而不需要描述其实现过程。采用特定领域的模型、术语，甚至是数学理论。比如，针对人工智能领域，你完全就可以用张量计算（力学概念）的术语来定义 DSL。

好了，现在我们分析了 SQL 的特点，从而也让你了解了 DSL 的一些共性特点。那么接下来，顺着 MySQL 运行的脉络，我们先来了解一下 MySQL 是如何做词法分析和语法分析的。


### 词法和语法分析
词法分析的代码是在 sql/sql_lex.cc 中，入口是 MYSQLlex() 函数。在 sql/lex.h 中，有一个 symbols[]数组，它定义了各类关键字、操作符。

MySQL 的词法分析器也是手写的，这给算法提供了一定的灵活性。比如，SQL 语句中，Token 的解析是跟当前使用的字符集有关的。使用不同的字符集，词法分析器所占用的字节数是不一样的，判断合法字符的依据也是不同的。而字符集信息，取决于当前的系统的配置。词法分析器可以根据这些配置信息，正确地解析标识符和字符串。

MySQL 的语法分析器是用 bison 工具生成的，bison 是一个语法分析器生成工具，它是 GNU 版本的 yacc。bison 支持的语法分析算法是 LALR 算法，而 LALR 是 LR 算法家族中的一员，它能够支持大部分常见的语法规则。bison 的规则文件是 sql/sql_yacc.yy，经过编译后会生成 sql/sql_yacc.cc 文件。

sql_yacc.yy 中，用你熟悉的 EBNF 格式定义了 MySQL 的语法规则。我节选了与 select 语句有关的规则，如下所示，从中你可以体会一下，SQL 语句的语法是怎样被一层一层定义出来的：

```
select_stmt:
          query_expression
        | ...
        | select_stmt_with_into
        ;
query_expression:
          query_expression_body opt_order_clause opt_limit_clause
        | with_clause query_expression_body opt_order_clause opt_limit_clause
        | ...
        ;
query_expression_body:
          query_primary
        | query_expression_body UNION_SYM union_option query_primary
        | ...
        ;
query_primary:
          query_specification
        | table_value_constructor
        | explicit_table
        ;
query_specification:
          ...
        | SELECT_SYM             /*select关键字*/
          select_options         /*distinct等选项*/
          select_item_list       /*select项列表*/
          opt_from_clause        /*可选：from子句*/
          opt_where_clause       /*可选：where子句*/
          opt_group_clause       /*可选：group子句*/
          opt_having_clause      /*可选：having子句*/
          opt_window_clause      /*可选：window子句*/
        ;
...
```
其中，query_expression 就是一个最基础的 select 语句，它包含了 SELECT 关键字、字段列表、from 子句、where 子句等。你可以看一下 select_options、opt_from_clause 和其他几个以 opt 开头的规则，它们都是 SQL 语句的组成部分。opt 是可选的意思，也就是它的产生式可能产生ε。

```
opt_from_clause:
          /* Empty. */ 
        | from_clause
        ;
```
另外，你还可以看一下表达式部分的语法。在 MySQL 编译器当中，对于二元运算，你可以大胆地写成左递归的文法。因为它的语法分析的算法用的是 LALR，这个算法能够自动处理左递归。一般研究表达式的时候，我们总是会关注编译器是如何处理结合性和优先级的。那么，bison 是如何处理的呢？原来，bison 里面有专门的规则，可以规定运算符的优先级和结合性。在 sql_yacc.yy 中，你会看到如下所示的规则片段：

![img](https://static001.geekbang.org/resource/image/4e/20/4e0d2706eb5e26143ae125c05bd2e720.jpg?wh=1388*916)

你可以看一下 bit_expr 的产生式，它其实完全把加减乘数等运算符并列就行了。

```
bit_expr : 
         ...
         | bit_expr '+' bit_expr %prec '+'
         | bit_expr '-' bit_expr %prec '-'
         | bit_expr '*' bit_expr %prec '*'
         | bit_expr '/' bit_expr %prec '/'
         ...
         | simple_expr
```

如果你只是用到加减乘除的运算，那就可以不用在产生式的后面加 %prec 这个标记。但由于加减乘除这几个还可以用在其他地方，比如“-a”可以用来表示把 a 取负值；减号可以用在一元表达式当中，这会比用在二元表达式中有更高的优先级。也就是说，为了区分同一个 Token 在不同上下文中的优先级，我们可以用 %prec，来说明该优先级是[上下文依赖](https://www.gnu.org/software/bison/manual/html_node/Contextual-Precedence.html)的。

好了，在了解了词法分析器和语法分析器以后，我们接着来跟踪一下 MySQL 的执行，看看编译器所生成的解析树和 AST 是什么样子的。在 sql_class.cc 的 sql_parser() 方法中，编译器执行完解析程序之后，会返回解析树的根节点 root，在 GDB 中通过 p 命令，可以逐步打印出整个解析树。你会看到，它的根节点是一个 PT_select_stmt 指针（见图 3）。解析树的节点是在语法规则中规定的，这是一些 C++ 的代码，它们会嵌入到语法规则中去。

下面展示的这个语法规则就表明，编译器在解析完 query_expression 规则以后，要创建一个 PT_query_expression 的节点，其构造函数的参数分别是三个子规则所形成的节点。对于 query_expression_body 和 query_primary 这两个规则，它们会直接把子节点返回，因为它们都只有一个子节点。这样就会简化解析树，让它更像一棵 AST。关于 AST 和解析树（也叫 CST）的区别，我在解析 Python 的编译器中讲过了，你可以回忆一下。



```
query_expression:
          query_expression_body
          opt_order_clause
          opt_limit_clause
          {
            $$ = NEW_PTN PT_query_expression($1, $2, $3); /*创建节点*/
          }
        | ...

query_expression_body:
          query_primary
          {
            $$ = $1;   /*直接返回query_primary的节点*/
          }
        | ...
        
query_primary:
          query_specification
          {
            $$= $1;  /*直接返回query_specification的节点*/
          }
        | ...
```
最后，对于“select first_name, last_name from employees”这样一个简单的 SQL 语句，它所形成的解析树如下：

![img](https://static001.geekbang.org/resource/image/00/26/007f91d9f3fe4c3349722201bec44226.jpg?wh=2284*1292)

图 3：示例 SQL 解析后生成的解析树

而对于“select 2 + 3”这样一个做表达式计算的 SQL 语句，所形成的解析树如下。你会看到，它跟普通的高级语言的表达式的 AST 很相似：

![img](https://static001.geekbang.org/resource/image/da/db/da090cf1095e2aef738a69a5851ffcdb.jpg?wh=2284*1528)

图 4：“select 2 + 3”对应的解析树

图 4 中的 PT_query_expression 等类，就是解析树的节点，它们都是 Parse_tree_node 的子类（PT 是 Parse Tree 的缩写）。这些类主要定义在 sql/parse_tree_nodes.h 和 parse_tree_items.h 文件中。其中，Item 代表了与“值”有关的节点，它的子类能够用于表示字段、常量和表达式等。你可以通过 Item 的 val_int()、val_str() 等方法获取它的值。

![img](https://static001.geekbang.org/resource/image/cf/04/cfa126a6144186deafe7d9caff56f304.jpg?wh=2284*1708)

图 5：解析树的树节点（部分）

由于 SQL 是一个个单独的语句，所以 select、insert、update 等语句，它们都各自有不同的根节点，都是 Parse_tree_root 的子类。

![img](https://static001.geekbang.org/resource/image/4a/2a/4a1152566c2ccab84d2f5022f44a022a.jpg?wh=2284*1480)

好了，现在你就已经了解了 SQL 的解析过程和它所生成的 AST 了。前面我说过，MySQL 采用的是 LALR 算法，因此我们可以借助 MySQL 编译器，来加深一下对 LR 算法家族的理解。

### 重温 LR 算法
你在阅读 yacc.yy 文件的时候，在注释里，你会发现如何跟踪语法分析器的执行过程的一些信息。你可以用下面的命令，带上“-debug”参数，来启动 MySQL 服务器：

```
mysqld --debug="d,parser_debug"
```
然后，你可以通过客户端执行一个简单的 SQL 语句：“select 2+3*5”。在终端，会输出语法分析的过程。这里我截取了一部分界面，通过这些输出信息，你能看出 LR 算法执行过程中的移进、规约过程，以及工作区内和预读的信息。

![img](https://static001.geekbang.org/resource/image/69/91/69e4644e93a5156a6695eff41d162891.jpg?wh=1054*1516)

我来给你简单地复现一下这个解析过程。第 1 步，编译器处于状态 0，并且预读了一个 select 关键字。你已经知道，LR 算法是基于一个 DFA 的。在这里的输出信息中，你能看到某些状态的编号达到了一千多，所以这个 DFA 还是比较大的。第 2 步，把 select 关键字移进工作区，并进入状态 42。这个时候，编译器已经知道后面跟着的一定是一个 select 语句了，也就是会使用下面的语法规则：

```
query_specification:
          ...
        | SELECT_SYM             /*select关键字*/
          select_options         /*distinct等选项*/
          select_item_list       /*select项列表*/
          opt_from_clause        /*可选：from子句*/
          opt_where_clause       /*可选：where子句*/
          opt_group_clause       /*可选：group子句*/
          opt_having_clause      /*可选：having子句*/
          opt_window_clause      /*可选：window子句*/
        ;

```
为了给你一个直观的印象，这里我画了 DFA 的局部示意图（做了一定的简化），如下所示。你可以看到，在状态 42，点符号位于“select”关键字之后、select_options 之前。select_options 代表了“distinct”这样的一些关键字，但也有可能为空。

![img](https://static001.geekbang.org/resource/image/47/0b/474af6c5761e157cb82987fcd87a3c0b.jpg?wh=2284*592)

图 7：移进 select 后的 DFA



第 3 步，因为预读到的 Token 是一个数字（NUM），这说明 select_options 产生式一定生成了一个ε，因为 NUM 是在 select_options 的 Follow 集合中。这就是 LALR 算法的特点，它不仅会依据预读的信息来做判断，还要依据 Follow 集合中的元素。所以编译器做了一个规约，也就是让 select_options 为空。也就是，编译器依据“select_options->ε”做了一次规约，并进入了新的状态 920。注意，状态 42 和 920 从 DFA 的角度来看，它们是同一个大状态。而 DFA 中包含了多个小状态，分别代表了不同的规约情况。

![img](https://static001.geekbang.org/resource/image/8f/e1/8f2444e7c1f100485d679cc543073de1.jpg?wh=2284*562)

图 8：基于“select_options->ε”规约后的 DFA

你还需要注意，这个时候，老的状态都被压到了栈里，所以栈里会有 0 和 42 两个状态。栈里的这些状态，其实记录了推导的过程，让我们知道下一步要怎样继续去做推导。

![img](https://static001.geekbang.org/resource/image/c3/76/c3a585e8a1c3753137ff83fac5368576.jpg?wh=2284*443)

图 9：做完前 3 步之后，栈里的情况

第 4 步，移进 NUM。这时又进入一个新状态 720。

![img](https://static001.geekbang.org/resource/image/04/b9/048df36542d61ba8f8f688c58e00a3b9.jpg?wh=2284*911)

图 10：移进 NUM 后的 DFA

而旧的状态也会入栈，记录下推导路径：

![img](https://static001.geekbang.org/resource/image/bc/51/bcd744e5d278ce37d0abb1583ceccb51.jpg?wh=2284*702)

图 11：移进 NUM 后栈的状态

第 5~8 步，依次依据 NUM_literal->NUM、literal->NUM_literal、simple_expr->literal、bit_expr->simple_expr 这四条产生式做规约。这时候，编译器预读的 Token 是 + 号，所以你会看到，图中的红点停在 + 号前。

![img](https://static001.geekbang.org/resource/image/33/7a/33b3f6b88214412b6d29b2ce2b03dc7a.jpg?wh=2284*995)

图 12：第 8 步之后的 DFA

第 9~10 步，移进 + 号和 NUM。这个时候，状态又重新回到了 720。这跟第 4 步进入的状态是一样的。

![img](https://static001.geekbang.org/resource/image/e3/8d/e3976970cd368c8e9c1547bbc2c6f48d.jpg?wh=2284*1367)

图 13：第 10 步之后的 DFA

而栈里的目前有 5 个状态，记录了完整的推导路径。

![img](https://static001.geekbang.org/resource/image/14/55/142e374173e90ba657579f67566bb755.jpg?wh=2284*879)

图 14：第 10 步之后栈的状态

到这里，其实你就已经了解了 LR 算法做移进和规约的思路了。不过你还可以继续往下研究。由于栈里保留了完整的推导路径，因此 MySQL 编译器最后会依次规约回来，把栈里的元素清空，并且形成一棵完整的 AST。


### 课程小结
这一讲，我带你初步探索了 MySQL 编译 SQL 语句的过程。你需要记住几个关键点：

掌握如何用 GDB 来跟踪 MySQL 的执行的方法。你要特别注意的是，我给你梳理的那些关键的程序入口，它是你理解 MySQL 运行过程的地图。SQL 语言是面向关系数据库的一种 DSL，它是声明式的，并采用了领域特定的模型和术语，可以为你设计自己的 DSL 提供启发。MySQL 的语法分析器是采用 bison 工具生成的。这至少说明，语法分析器生成工具是很有用的，连正式的数据库系统都在使用它，所以你也可以大胆地使用它，来提高你的工作效率。我在最后的参考资料中给出了 bison 的手册，希望你能自己阅读一下，做一些简单的练习，掌握 bison 这个工具。最后，你一定要知道 LR 算法的运行原理，知其所以然，这也会更加有助于你理解和用好工具。


我依然把本讲的内容给你整理成了一张知识地图，供你参考和复习回顾：

![img](https://static001.geekbang.org/resource/image/04/5b/04cc0ce4fb5d78d7d9aa18e03088f95b.jpg?wh=2284*1642)


### 参考资料
MySQL 的内行手册（[MySQL Internals Manual](https://dev.mysql.com/doc/internals/en/)）能提供一些重要的信息。但我发现文档内容经常跟源代码的版本不同步，比如介绍源代码的目录结构的信息就过时了，你要注意这点。bison 的[手册](https://www.gnu.org/software/bison/manual/)。

## 26 | MySQL编译器（二）：编译技术如何帮你提升数据库性能？


通过上一讲的学习，你已经了解了 MySQL 编译器是怎么做词法和语法分析的了。那么在做完语法分析以后，MySQL 编译器又继续做了哪些处理，才能成功地执行这个 SQL 语句呢？所以今天，我就带你来探索一下 MySQL 的实现机制，我会把重点放在 SQL 的语义分析和优化机制上。当你学完以后，你就能真正理解以下这些问题了：高级语言的编译器具有语义分析功能，那么 MySQL 编译器也会做语义分析吗？它有没有引用消解问题？有没有作用域？有没有类型检查？MySQL 有没有类似高级语言的那种优化功能呢？好，让我们开始今天的探究吧。不过，在讨论 MySQL 的编译过程之前，我想先带你了解一下 MySQL 会用到的一些重要的数据结构，因为你在解读代码的过程中经常会见到它们。


### 认识 MySQL 编译器的一些重要的数据结构
第一组数据结构，是下图中的几个重要的类或结构体，包括线程、保存编译上下文信息的 LEX，以及保存编译结果 SELECT_LEX_UNIT 和 SELECT_LEX。

![img](https://static001.geekbang.org/resource/image/cc/b7/ccd4a2dcae0c974b0b5254c51440f9b7.jpg?wh=2284*1215)

图 1：MySQL 编译器的中的几个重要的类和结构体

首先是 THD，也就是线程对象。对于每一个客户端的连接，MySQL 编译器都会启动一个线程来处理它的查询请求。THD 中的一个重要数据成员是 LEX 对象。你可以把 LEX 对象想象成是编译 SQL 语句的工作区，保存了 SQL 语句编译过程中的上下文信息，编译器会把编译的成果放在这里，而编译过程中所需要的信息也是从这里查找。在把 SQL 语句解析完毕以后，编译器会形成一些结构化的对象来表示一个查询。其中 SELECT_LEX_UNIT 结构体，就代表了一个查询表达式（Query Expression）。一个查询表达式可能包含了多个查询块，比如使用 UNION 的情况。而 SELECT_LEX 则代表一个基本的查询块（Query Block），它里面的信息包括了所有的列和表达式、查询用到的表、where 条件等。在 SELECT_LEX 中会保存查询块中涉及的表、字段和表达式等，它们也都有对应的数据结构。

第二组需要了解的数据结构，是表示表、字段等信息的对象。Table_ident 对象保存了表的信息，包括数据库名、表名和所在的查询语句（SELECT_LEX_UNIT 对象）。

![img](https://static001.geekbang.org/resource/image/10/d0/10567f1112bf04c0240c0ebc277067d0.jpg?wh=2061*428)

图 2：Table_indent 对象，代表一个表

而字段和表达式等表示一个值的对象，用 Item 及其子类来表示。SQL 语句中的每个字段、每个计算字段，最后都对应一个 Item。where 条件，其实也是用一个 Item 就能表示。具体包括：字段（Item_field）。各种常数，包括数字、字符和 null 等（Item_basic_constant）。能够产生出值的运算（Item_result_field），包括算术表达式（Item_num_op）、存储过程（Item_func_sp）、子查询（Item_subselect）等。在语法分析过程中产生的 Item（Parse_tree_item）。它们是一些占位符，因为在语法分析阶段，不容易一下子创建出真正的 Item，这些 Parse_tree_item 需要在上下文分析阶段，被替换成真正的 Item。

![img](https://static001.geekbang.org/resource/image/7a/c3/7aed46a44dcfb3bb6908db30cyy815c3.jpg?wh=2284*1413)

图 3：Item 及其子类

好了，上面这些就是 MySQL 会用到的最核心的一些数据结构了。接下来的编译工作，就会生成和处理上述的数据结构。

### 上下文分析
我们先来看一下 MySQL 编译器的上下文分析工作。你已经知道，语法分析仅仅完成的是上下文无关的分析，还有很多的工作，需要基于上下文来做处理。这些工作，就属于语义分析。

MySQL 编译器中，每个 AST 节点，都会有一个 contextualize() 方法。从这个方法的名称来看，你就能知道它是做上下文处理的（contextualize，置于上下文中）。对一个 Select 语句来说，编译器会调用其根节点 PT_select_stmt 的 contextualize() 方法，从而深度遍历整个 AST，并调用每个节点的 contextualize() 方法。

那么，MySQL 编译器的上下文处理，都完成了什么工作呢？首先，是检查数据库名、表名和字段名是否符合格式要求（在 table.cc 中实现）。

比如，MySQL 会规定表名、字段名等名称不能超过 64 个字符，字段名不能包含 ASCII 值为 255 的字符，等等。这些规则在词法分析阶段是不检查的，要留在语义分析阶段检查。

然后，创建并填充 SELECT_LEX_UNIT 和 SELECT_LEX 对象。

前面我提到了，SELECT_LEX_UNIT 和 SELECT_LEX 中，保存了查询表达式和查询块所需的所有信息，依据这些信息，MySQL 就可以执行实际的数据库查询操作。那么，在 contextualize 的过程中，编译器就会生成上述对象，并填充它们的成员信息。比如，对于查询中用到的表，在语法分析阶段就会生成 Table_ident 对象。但其中的数据库名称可能是缺失的，那么在上下文的分析处理当中，就会被编译器设置成当前连接所采用的默认数据库。这个信息可以从线程对象（THD）中获得，因为每个线程对应了一个数据库连接，而每个数据库连接是针对一个具体的数据库的。

好了，经过上下文分析的编译阶段以后，我们就拥有了可以执行查询的 SELECT_LEX_UNIT 和 SELECT_LEX 对象。可是，你可能会注意到一个问题：为什么在语义分析阶段，MySQL 没有做引用的消解呢？不要着急，接下来我就给你揭晓这个答案。

### MySQL 是如何做引用消解的？
我们在 SQL 语句中，会用到数据库名、表名、列名、表的别名、列的别名等信息，编译器肯定也需要检查它们是不是正确的。这就是引用消解（或名称消解）的过程。一般编译器是在语义分析阶段来做这项工作的，而 MySQL 是在执行 SQL 命令的时候才做引用消解。引用消解的入口是在 SQL 命令的的 prepare() 方法中，它会去检查表名、列名都对不对。通过 GDB 调试工具，我们可以跟踪编译器做引用消解的过程。你可以在 my_message_sql() 函数处设个断点，然后写个 SQL 语句，故意使用错误的表名或者列名，来看看 MySQL 是在什么地方检查出这些错误的。比如说，你可以执行“select * from fake_table”，其中的 fake_table 这个表，在数据库中其实并不存在。下面是打印出的调用栈。你会注意到，MySQL 在准备执行 SQL 语句的过程中，会试图去打开 fake_table 表，这个时候编译器就会发现这个表不存在。

![img](https://static001.geekbang.org/resource/image/9c/9a/9c2e987a6fc797926593cd1f6c001d9a.jpg?wh=1920*1091)

你还可以再试一下“select fake_column from departments”这个语句，也一样会查出，fake_column 并不是 departments 表中的一列。

![img](https://static001.geekbang.org/resource/image/a2/9f/a283dcc105f240c6760a78913a4d069f.jpg?wh=1920*1061)

那么，MySQL 是如何知道哪些表和字段合法，哪些不合法的呢？原来，它是通过查表的定义，也就是数据库模式信息，或者可以称为数据字典、元数据。MySQL 在一个专门的库中，保存了所有的模式信息，包括库、表、字段、存储过程等定义。你可以跟高级语言做一下类比。高级语言，比如说 Java 也会定义一些类型，类型中包含了成员变量。那么，MySQL 中的表，就相当于高级语言的类型；而表的字段（或列）就相当于高级语言的类型中的成员变量。所以，在这个方面，MySQL 和高级语言做引用消解的思路其实是一样的。

但是，高级语言在做引用消解的时候有作用域的概念，那么 MySQL 有没有类似的概念呢？有的。举个例子，假设一个 SQL 语句带了子查询，那么子查询中既可以引用本查询块中的表和字段，也可以引用父查询中的表和字段。这个时候就存在了两个作用域，比如下面这个查询语句：

```
select dept_name from departments where dept_no in 
    (select dept_no from dept_emp 
        where dept_name != 'Sales'  #引用了上一级作用域中的字段
        group by dept_no 
        having count(*)> 20000)
```
其中的 dept_name 字段是 dept_emp 表中所没有的，它其实是上一级作用域中 departments 表中的字段。提示：这个 SQL 当然写得很不优化，只是用来表现作用域的概念。好。既然要用到作用域，那么 MySQL 的作用域是怎么表示的呢？这就要用到 Name_resolution_context 对象。这个对象保存了当前作用域中的表，编译器可以在这些表里查找字段；它还保存了对外层上下文的引用（outer_context），这样 MySQL 就可以查找上一级作用域中的表和字段。

![img](https://static001.geekbang.org/resource/image/40/a8/40114ec45281b96a0c34fd0a237251a8.jpg?wh=2247*619)

图 4：MySQL 用来表示作用域的对象

好了，现在你就对 MySQL 如何做引用消解非常了解了。我们知道，对于高级语言的编译器来说，接下来它还会做一些优化工作。那么，MySQL 是如何做优化的呢？它跟高级语言编译器的优化工作相比，又有什么区别呢？



### MySQL 编译器的优化功能
MySQL 编译器的优化功能主要都在 sql_optimizer.cc 中。就像高级语言一样，MySQL 编译器会支持一些常见的优化。我来举几个例子。


第一个例子是常数传播优化（const propagation）。假设有一个表 foo，包含了 x 和 y 两列，那么 SQL 语句：“select * from foo where x = 12 and y=x”，会被优化成“select * from foo where x = 12 and y = 12”。你可以在 propagate_cond_constants() 函数上加个断点，查看常数传播优化是如何实现的。

第二个例子是死代码消除。比如，对于 SQL 语句：“select * from foo where x=2 and y=3 and x< y”，编译器会把它优化为“select * from foo where x=2 and y=3”，把“x< y”去掉了，这是因为 x 肯定是小于 y 的。该功能的实现是在 remove_eq_conds() 中。

第三个例子是常数折叠。这个优化工作我们应该很熟悉了，主要是对各种条件表达式做折叠，从而降低计算量。其实现是在 sql_const_folding.cc 中。

你要注意的是，上述的优化主要是针对条件表达式。因为 MySQL 在执行过程中，对于每一行数据，可能都需要执行一遍条件表达式，所以上述优化的效果会被放大很多倍，这就好比针对循环体的优化，是一个道理。


不过，MySQL 还有一种特殊的优化，是对查询计划的优化。比如说，我们要连接 employees、dept_emp 和 departments 三张表做查询，数据库会怎么做呢？最笨的办法，是针对第一张表的每条记录，依次扫描第二张表和第三张表的所有记录。这样的话，需要扫描多少行记录呢？是三张表的记录数相乘。基于我们的示例数据库的情况，这个数字是 8954 亿。上述计算其实是做了一个笛卡尔积，这会导致处理量的迅速上升。而在数据库系统中，显然不需要用这么笨的方法。



你可以用 explain 语句，让 MySQL 输出执行计划，下面我们来看看 MySQL 具体是怎么做的：


```
explain select employees.emp_no, first_name, 
               departments.dept_no dept_name
        from employees, dept_emp, departments
        where employees.emp_no = dept_emp.emp_no 
              and dept_emp.dept_no = departments.dept_no;
```
这是 MySQL 输出的执行计划：

![img](https://static001.geekbang.org/resource/image/b0/13/b0e1e8e2b1f9ef11dbd0150b8ae2cd13.jpg?wh=1920*272)

从输出的执行计划里，你能看出，MySQL 实际的执行步骤分为了 3 步：第 1 步，通过索引，遍历 departments 表；第 2 步，通过引用关系（ref），找到 dept_emp 表中，dept_no 跟第 1 步的 dept_no 相等的记录，平均每个部门在 dept_emp 表中能查到 3.7 万行记录；第 3 步，基于第 2 步的结果，通过等值引用（eq_ref）关系，在 employees 表中找到相应的记录，每次找到的记录只有 1 行。这个查找可以通过 employees 表的主键进行。

根据这个执行计划来完成上述的操作，编译器只需要处理大约 63 万行的数据。因为通过索引查数据相比直接扫描表来说，处理每条记录花费的时间比较长，所以我们假设前者花费的时间是后者的 3 倍，那么就相当于扫描了 63*3=189 万行表数据，这仍然仅仅相当于做笛卡尔积的 47 万分之一。我在一台虚拟机上运行该 SQL 花费的时间是 5 秒，而如果使用未经优化的方法，则需要花费 27 天！

通过上面的例子，你就能直观地理解做查询优化的好处了。MySQL 会通过一个 JOIN 对象，来为一个查询块（SELECT_LEX）做查询优化，你可以阅读 JOIN 的方法，来看看查询优化的具体实现。关于查询优化的具体算法，你需要去学习一下数据库的相关课程，我在本讲末尾也推荐了一点参考资料，所以我这里就不展开了。

从编译原理的角度来看，我们可以把查询计划中的每一步，看做是一条指令。MySQL 的引擎，就相当于能够执行这些指令的一台虚拟机。如果再做进一步了解，你就会发现，**MySQL 的执行引擎和存储引擎是分开的**。存储引擎提供了一些基础的方法（比如通过索引，或者扫描表）来获取表数据，而做连接、计算等功能，是在 MySQL 的执行引擎中完成的。好了，现在你就已经大致知道了，一条 SQL 语句从解析到执行的完整过程。但我们知道，普通的高级语言在做完优化后，生成机器码，这样性能更高。那么，是否可以把 SQL 语句编译成机器码，从而获得更高的性能呢？


### 能否把 SQL 语句编译成机器码？
MySQL 编译器在执行 SQL 语句的过程中，除了查找数据、做表间连接等数据密集型的操作以外，其实还有一些地方是需要计算的。比如：

where 条件：对每一行扫描到的数据都需要执行一次。计算列：有的列是需要计算出来的。聚合函数：像 sum、max、min 等函数，也是要对每一行数据做一次计算。

在研究 MySQL 的过程中，你会发现上述计算都是解释执行的。MySQL 做解释执行的方式，基本上就是深度遍历 AST。比如，你可以对代表 where 条件的 Item 求值，它会去调用它的下级节点做递归的计算。这种计算过程和其他解释执行的语言差不多，都是要在运行时判断数据的类型，进行必要的类型转换，最后执行特定的运算。因为很多的判断都要在运行时去做，所以程序运行的性能比较低。

另外，由于 MySQL 采用的是解释执行机制，所以它在语义分析阶段，其实也没有做类型检查。在编译过程中，不同类型的数据在运算的时候，会自动进行类型转换。比如，执行“select'2' + 3”，MySQL 会输出 5，这里就把字符串'2'转换成了整数。

那么，我们能否把这些计算功能编译成本地代码呢？因为我们在编译期就知道每个字段的数据类型了，所以编译器其实是可以根据这些类型信息，生成优化的代码，从而提升 SQL 的执行效率。这种思路理论上是可行的。不过，目前我还没有看到 MySQL 在这方面的工作，而是发现了另一个数据库系统 PostgreSQL，做了这方面的优化。PostgreSQL 的团队发现，如果解释执行下面的语句，表达式计算所用的时间，占到了处理一行记录所需时间的 56%。而基于 LLVM 实现 JIT 以后（编译成机器码执行），所用的时间只占到总执行时间的 6%，这就使得 SQL 执行的整体性能整整提高了一倍。


```
select count(*) from table_name where (x + y) > 100
```
中国用户对 MySQL 的用量这么大，如果能做到上述的优化，那么仅仅因此而减少的碳排放，就是一个很大的成绩！所以，你如果有兴趣的话，其实可以在这方面尝试一下！

### 课程小结
这一讲我们分析了 MySQL 做语义分析、优化和执行的原理，并探讨了一下能否把 SQL 编译成本地代码的问题。你要记住以下这些要点：MySQL 也会做上下文分析，并生成能够代表 SQL 语句的内部数据结构；MySQL 做引用消解要基于数据库模式信息，并且也支持作用域；MySQL 会采用常数传播等优化方法，来优化查询条件，并且要通过查询优化算法，形成高效的执行计划；把 SQL 语句编译成机器码，会进一步提升数据库的性能，并降低能耗。
我把相应的知识点总结成了思维导图，供你参考：

![img](https://static001.geekbang.org/resource/image/d8/61/d8dfbb4d207cd6a27fffe8ab860f6861.jpg?wh=2284*2265)

总结这两讲对 MySQL 所采用的编译技术介绍，你会发现这样几个很有意思的地方：第一，编译技术确实在数据库系统中扮演了很重要的作用。第二，数据库编译 SQL 语句的过程与高级语言有很大的相似性，它们都包含了词法分析、语法分析、语义分析和优化等处理。你对编译技术的了解，能够指导你更快地看懂 MySQL 的运行机制。另外，如果你也要设计类似的系统级软件，这就是一个很好的借鉴。

## 现代语言设计篇

## 27 | 课前导读：学习现代语言设计的正确姿势

**对于一门完整的语言来说，编译器只是其中的一部分。它通常还有两个重要的组成部分：一个是运行时，包括内存管理、并发机制、解释器等模块；还有一个是标准库，包含了一些标准的功能，如算术计算、字符串处理、文件读写，等等。**


C 语言是 Unix 系统的脚本，COBOL 是大型机的脚本，SQL 是数据库系统的脚本，JavaScript、Java 和 C# 都是浏览器的脚本，Swift 和 Objective-C 是苹果系统的脚本，Kotlin 是 Android 的脚本。让一门语言成为某个流行的技术系统的脚本，为这个生态提供编程支持，就是一种定位很清晰的需求。

![img](https://static001.geekbang.org/resource/image/f9/f7/f9034be8cf1e452808154bb16b3c7df7.jpg?wh=2284*1241)

作为对比，Go 语言的设计主要是用来编写服务端程序的，那么它的关键特性也是与这个定位相适应。

并发：服务端的软件最重要的一项能力就是支持大量并发任务。Go 在语言设计上把并发能力作为第一等级的语言要素。垃圾收集：由于垃圾收集会造成整个应用程序停下来，所以 Go 语言特别重视降低由于垃圾收集而产生的停顿。


## 32 | 运行时（二）：垃圾收集与语言的特性有关吗？


### 垃圾收集算法概述
垃圾收集主要有标记 - 清除（Mark and Sweep）、标记 - 整理（Mark and Compact）、停止 - 拷贝（Stop and Copy）、引用计数、分代收集、增量收集和并发收集等不同的算法，在这里我简要地和你介绍一下。首先，我们先学习一下什么是内存垃圾。内存垃圾，其实就是一些保存在堆里的、已经**无法从程序里访问的对象**。我们看一个具体的例子。

在堆中申请一块内存时（比如 Java 中的对象实例），我们会用一个变量指向这块内存。但是，如果给变量赋予一个新的地址，或者当栈桢弹出时，该栈桢的变量全部失效，这时，变量所指向的内存就没用了（如图中的灰色块）。

![img](https://static001.geekbang.org/resource/image/7a/c0/7a9d301d4b1c9091d99cfe721d01d9c0.jpg?wh=2231*579)

图 1：A 是内存垃圾

另外，如果 A 对象有一个成员变量指向 C 对象，那么 A 不可达，C 也会不可达，也就失效了。但 D 对象除了被 A 引用，还被 B 引用，仍然是可达的。

![img](https://static001.geekbang.org/resource/image/d7/91/d7a15d401fbb3b31b3bd63c014b21391.jpg?wh=2026*1035)

图 2：A 和 C 是内存垃圾

那么，所有不可达的内存就是垃圾。所以，垃圾收集的重点就是找到并清除这些垃圾。接下来，我们就看看不同的算法是怎么完成这个任务的。


### 标记 - 清除
标记 - 清除算法，是从 GC 根节点出发，顺着对象的引用关系，依次标记可达的对象。这里说的 GC 根节点，包括全局变量、常量、栈里的本地变量、寄存器里的本地变量等。从它们出发，就可以找到所有有用的对象。那么剩下的对象，就是内存垃圾，可以清除掉。


### 标记 - 整理
采用标记 - 清除算法，运行时间长了以后，会形成内存碎片。这样在申请内存的时候，可能会失败。

![img](https://static001.geekbang.org/resource/image/1b/d9/1b5af01923b40566d5fde1a25a0720d9.jpg?wh=1743*965)

图 3：内存碎片导致内存申请失败

为了避免内存碎片，你可以采用变化后的算法，也就是标记 - 整理算法：**在做完标记以后，做一下内存的整理，让存活的对象都移动到一边，消除掉内存碎片。**

![img](https://static001.geekbang.org/resource/image/5e/b0/5e7530840bfa8077d9f0944142d932b0.jpg?wh=2055*1009)

图 4：内存整理以后，可以更有效地利用内存


### 停止 - 拷贝
停止和拷贝算法把内存分为新旧空间两部分。你需要保持一个堆指针，指向自由空间开始的位置。申请内存时，把堆指针往右移动就行了。

![img](https://static001.geekbang.org/resource/image/f9/10/f9f8c968c6a4bb31c274b8eb0e0b6d10.jpg?wh=1983*899)

图 5：在旧空间中申请内存

**当旧空间内存不够了以后，就会触发垃圾收集。在收集时，会把可达的对象拷贝到新空间，然后把新旧空间互换。**

![img](https://static001.geekbang.org/resource/image/e9/c9/e98d5746653d7dd0927f2006270a7dc9.jpg?wh=2174*919)

图 6：新旧空间互换

**停止 - 拷贝算法，在分配内存的时候，不需要去查找一块合适的空闲内存；在垃圾收集完毕以后，也不需要做内存整理，因此速度是最快的。但它也有缺点，就是总有一半内存是闲置的。**


### 引用计数
**引用计数方法，是在对象里保存该对象被引用的数量。一旦这个引用数为零，那么就可以作为垃圾被收集走**。有时候，我们会把引用计数叫做自动引用计数（ARC），并把它作为跟垃圾收集（GC）相对立的一个概念。所以，如果你读到相关的文章，它把 ARC 和 GC 做了对比，也不要吃惊。引用计数实现起来简单，并且可以边运行边做垃圾收集，不需要为了垃圾收集而专门停下程序。可是，它也有缺陷，就是**不能处理循环引用（Reference Cycles）的情况**。在下图中，四个对象循环引用，但**没有 GC 根指向它们**。它们已经是垃圾，但计数却都为 1。

![img](https://static001.geekbang.org/resource/image/fe/08/fe6c0082c0ff95780f5b11e935ea9e08.jpg?wh=2060*485)

图 7：循环引用

**另外，由于在程序引用一个对象的前后，都要修改引用计数，并且还有多线程竞争的可能性，所以引用计数法的性能开销比较大。**


### 分代收集
在程序中，新创建的对象往往会很快死去，比如，你在一个方法中，使用临时变量指向一些新创建的对象，这些对象大多数在退出方法时，就没用了。这些数据叫做**新生代**。而如果一个对象被扫描多次，发现它还没有成为垃圾，那就会标记它为比较老的时代。这些对象可能 Java 里的静态数据成员，或者调用栈里比较靠近根部的方法所引用的，不会很快成为垃圾。对于新生代对象，可以更频繁地回收。而对于老一代的对象，则回收频率可以低一些。并且，**对于不同世代的对象，还可以用不同的回收方法。比如，新生代比较适合复制式收集算法，因为大部分对象会被收集掉，剩下来的不多；而老一代的对象生存周期比较长，拷贝的话代价太大，比较适合标记 - 清除算法，或者标记 - 整理算法。**

### 增量收集和并发收集
垃圾收集算法在运行时，通常会把程序停下。因为在垃圾收集的过程中，如果程序继续运行，可能会出错。**这种停下整个程序的现象，被形象地称作“停下整个世界（STW）”**。可是让程序停下来，会导致系统卡顿，用户的体验感会很不好。一些对实时性要求比较高的系统，根本不可能忍受这种停顿。所以，在自动内存管理领域的一个研究的重点，就是**如何缩短这种停顿时间**。增量收集和并发收集算法，就是在这方面的有益探索：

**增量收集可以每次只完成部分收集工作，没必要一次把活干完，从而减少停顿。并发收集就是在不影响程序执行的情况下，并发地执行垃圾收集工作。**

好了，理解了垃圾收集算法的核心原理以后，我们就可以继续去探索各门语言是怎么运用这些算法的了。首先，我们从 Python 的垃圾收集算法学起。

### Python 与引用计数算法

Python 语言选择的是引用计数的算法。除此之外，Swift 语言和方舟编译器，采用的也是引用计数，所以值得我们重视。


### Python 的内存管理和垃圾收集机制
首先我们来复习一下 Python 内存管理的特征。在 Python 里，每个数据都是对象，而这些对象又都是在堆上申请的。对比一下，在 C 和 Java 这样的语言里，很多计算可以用本地变量实现，而本地变量是在栈上申请的。这样，你用到一个整数的时候，只占用 4 个字节，而不像 Python 那样有一个对象头的固定开销。栈的优势还包括：不会产生内存碎片，数据的局部性又好，申请和释放的速度又超快。而在堆里申请太多的小对象，则会走向完全的反面：太多次系统调用，性能开销大；产生内存碎片；数据的局部性也比较差。

所以说，Python 的内存管理方案，就决定了它的内存占用大、性能低。这是 Python 内存管理的短板。而为了稍微改善一下这个短板，Python 采用了一套基于区域（Region-based）的内存管理方法，能够让小块的内存管理更高效。简单地说，就是 Python 每次都申请一大块内存，这一大块内存叫做 Arena。当需要较小的内存的时候，直接从 Arena 里划拨就好了，不用一次次地去操作系统申请。当用垃圾回收算法回收内存时，也不一定马上归还给操作系统，而是归还到 Arena 里，然后被循环使用。这个策略能在一定程度上提高内存申请的效率，并且减少内存碎片化。

接下来，我们就看看 Python 是如何做垃圾回收的。回忆一下，在第 19 讲分析 Python 的运行时机制时，其中提到了一些垃圾回收的线索。Python 里每个对象都是一个 PyObject，每个 PyObject 都有一个 ob_refcnt 字段用于记录被引用的数量。

在解释器执行字节码的时候，会根据不同的指令自动增加或者减少 ob_refcnt 的值。当一个 PyObject 对象的 ob_refcnt 的值为 0 的时候，意味着没有任何一个变量引用它，可以立即释放掉，回收对象所占用的内存。

现在你已经知道，采用引用计数方法，需要解决循环引用的问题。那 Python 是如何实现的呢？

Python 在 gc 模块里提供了一个循环检测算法。接下来我们通过一个示例，来看看这个算法的原理。在这个例子中，有一个变量指向对象 A。你能用肉眼看出，对象 A、B、C 不是垃圾，而 D 和 E 是垃圾。

![img](https://static001.geekbang.org/resource/image/5a/bc/5a8efc25bca7b594f6284131674b4ebc.jpg?wh=3285*1335)

图 8：把容器对象加入待扫描列表



在循环检测算法里，gc 使用了两个列表。一个列表保存所有待扫描的对象，另一个列表保存可能的垃圾对象。注意，这个算法只检测容器对象，比如列表、用户自定义的类的实例等。而像整数对象这样的，就不用检测了，因为它们不可能持有对其他对象的引用，也就不会造成循环引用。在这个算法里，我们首先让一个 gc_ref 变量等于对象的引用数。接着，算法假装去掉对象之间的引用。比如，去掉从 A 到 B 的引用，这使得 B 对象的 gc_ref 值变为了 0。在遍历完整个列表以后，除了 A 对象以外，其他对象的 gc_ref 都变成了 0。

![img](https://static001.geekbang.org/resource/image/d8/e8/d825f11087afcf1482dede2920c66ce8.jpg?wh=3217*1320)

图 9：扫描列表，修改 gc_ref 的值

gc_ref 等于零的对象，有的可能是垃圾对象，比如 D 和 E；但也有些可能不是，比如 B 和 C。那要怎么区分呢？我们先把这些对象都挪到另一个列表中，怀疑它们可能是垃圾。

![img](https://static001.geekbang.org/resource/image/3c/0e/3c0e14f3af25de89f96bc1e75fc57d0e.jpg?wh=3208*1321)

图 10：认为 gc_ref 为 0 的对象可能是垃圾

这个时候，待扫描对象区只剩下了对象 A。它的 gc_ref 是大于零的，也就是从 gc 根是可到达的，因此肯定不是垃圾对象。那么顺着这个对象所直接引用和间接引用到的对象，也都不是垃圾。而剩下的对象，都是从 gc 根不可到达的，也就是真正的内存垃圾。

![img](https://static001.geekbang.org/resource/image/2d/2b/2de8166a19741082b3aef0fb2b57502b.jpg?wh=3262*1348)

图 11：去除其中可达的对象，剩下的是真正的垃圾

另外，基于循环检测的垃圾回收算法是定期执行的，这会跟 Java 等语言的垃圾收集器一样，导致系统的停顿。所以，它也会像 Java 等语言的垃圾收集器一样，采用分代收集的策略来减少垃圾收集的工作量，以及由于垃圾收集导致的停顿。好了，以上就是 Python 的垃圾收集算法。我们前面提过，除了 Python 以外，Swift 和方舟编译器也使用了引用计数算法。另外，还有些分代的垃圾收集器，在处理老一代的对象时，也会采用引用计数的方法，这样就可以在引用计数为零的时候收回内存，而不需要一遍遍地扫描。




### 编译器如何配合引用计数算法？
对于 Python 来说，引用计数的增加和减少，是**由运行时来负责**的，编译器并不需要做额外的工作。它只需要生成字节码就行了。而对于 Python 的解释器来说，**在把一个对象赋值给一个变量的时候，要把对象的引用数加 1；而当该变量超出作用域的时候，要把对象的引用数减 1**。不过，对于编译成机器码的语言来说，就要由编译器介入了。它要负责生成相应的指令，来做引用数的增减。

不过，这只是高度简化的描述。实际实现时，还要解决很多细致的问题。比如，在多线程的环境下，对引用数的改变，必须要用到锁，防止超过一个线程同时修改引用数。这种频繁地对锁的使用，会导致性能的降低。这时候，我们之前学过的一些优化算法就可以派上用场了。比如，编译器可以做一下逃逸分析，对于没有逃逸或者只是参数逃逸的对象，就可以不使用锁，因为这些对象不可能被多个线程访问。这样就可以提高程序的性能。除了通过逃逸分析优化对锁的使用，编译器还可以进一步优化。比如，在一段程序中，一个对象指针被调用者通过参数传递给某个函数使用。在函数调用期间，由于调用者是引用着这个对象的，所以这个对象不会成为垃圾。而这个时候，就可以省略掉进入和退出函数时给对象引用数做增减的代码。还有不少类似上面的情况，需要编译器配合垃圾收集机制，生成高效的、正确的代码。你在研究 Swift 和方舟编译器时，可以多关注一下它们对引用计数做了哪些优化。
接下来，我们再看看其他语言是怎么做垃圾收集的。


### 其他语言是怎么做垃圾收集的？
除了 Python 以外，我们在第二个模块研究的其他几门语言，包括 Java、JavaScript（V8）和 Julia，都没有采用引用计数算法（除了在分代算法中针对老一代的对象），它们基本都采用了**分代收集**的策略。针对新生代，通常是采用**标记 - 清除或者停止拷贝算法**。

它们不采用引用计数的原因，其实我们可以先猜测一下，那就是因为引用计数的缺点。比如增减引用计数所导致的计算量比较多，在多线程的情况下要用到锁，就更是如此；再比如会导致内存碎片化、局部性差等。

而采用像**停止 - 拷贝这样的算法，在总的计算开销上会比引用计数的方法低**。Java 和 Go 语言主要是用于服务端程序开发的。尽量减少内存收集带来的性能损耗，当然是语言的设计者重点考虑的问题。


再进一步看，采用像停止 - 拷贝这样的算法，其实是用空间换时间，以更大的内存消耗换来性能的提升。如果你的程序需要 100M 内存，那么虚拟机需要给它准备 200M 内存，因为有一半空间是空着的。**这其实也是为什么 Android 手机比 iPhone 更加消耗内存的原因之一**。

在为 iPhone 开发程序的时候，无论是采用 Objective C 还是 Swift，都是采用引用计数的技术。并且，程序员还负责采用弱引用等技术，来避免循环引用，从而进一步消除了在运行时进行循环引用检测的开销。

通过上面的分析，我们能发现移动端应用和服务端应用有不同的特点，因此也会导致采用不同的垃圾收集算法。那么方舟编译器采用引用计数的方法，来编译原来的 Android 应用，是否也是借鉴了 iPhone 的经验呢？我没有去求证过，所以不得而知。但我们可以根据自己的知识去做一些合理的猜测。

好，回过头来，我们继续分析一下用 Java 和 Go 语言来写服务端程序对垃圾收集的需求。对于服务器端程序来说，垃圾收集导致的停顿，是一个令程序员们头痛的问题。有时候，一次垃圾收集会让整个程序停顿一段非常可观的时间（比如上百毫秒，甚至达到秒级），这对于实时性要求较高或并发量较大的系统来说，就会引起很大的问题。也因此，一些很关键的系统很长时间内无法采用 Java 和 Go 语言编写。

所以，Java 和 Go 语言一直在致力于减少由于垃圾收集而产生的停顿。最新的垃圾收集器，已经使得垃圾收集导致的停顿降低到了几毫秒内。

在这里，你需要理解的要点，是为什么在垃圾收集时，要停下整个程序？又有什么办法可以减少停顿的时间？

### 为什么在垃圾收集时，要停下整个程序？
其实，对于引用计数算法来说，是不需要停下整个程序的，每个对象的内存在计数为零的时候就可以收回。

而采用标记 - 清除算法时，你就必须要停下程序：首先做标记，然后做清除。在做标记的时候，你必须从所有的 GC 根出发，去找到所有存活的对象，剩下的才是垃圾。所以，看上去，这是一项完整的工作，程序要一直停顿到这项完整的工作做完。

让事情更棘手的是，**你不仅要停下当前的线程，扫描栈里的所有 GC 根，你还要停下其他的线程，因为其他线程栈里的对象，也可能引用了相同的对象。最后的结果，就是你停下了整个世界。**

当然也有例外，就是如果别的线程正在运行的代码，没有可能改变对象之间的引用关系，比如仅仅是在做一个耗费时间的数学计算，那么是不用停下来的。你可以参考 Julia 的gc 程序中的一段[注释](https://github.com/JuliaLang/julia/blob/v1.4.1/src/gc.c#L152)，来理解什么样的代码必须停下来。

更麻烦的是，不仅仅在扫描阶段你需要停下整个世界，如果垃圾收集算法需要做内存的整理或拷贝，那么这个时候仍然要停下程序。而且，程序必须停在一些叫做**安全点**（SafePoint）的地方。

在这些地方，修改对象的地址不会破坏程序数据的一致性。比如说，假设代码里有一段逻辑，是访问对象的某个成员变量，而这个成员变量的地址是根据对象的地址加上一个偏移量计算出来的。那么如果你修改了对象的地址，而这段代码仍然去访问原来的地址，那就出错了。而当代码停留在安全点上，就不会有这种不一致。

安全点是编译器插入到代码中一个片段。在查看[Graal 生成的汇编代码](https://time.geekbang.org/column/article/255730)时，我们曾经看到过这样的指令片段。

好了，到目前为止，你了解了为什么要停下整个世界，以及要停在哪里才合适。那么我们继续研究，如何能减少停顿时间。


### 如何能减少停顿时间？
第一招，分代收集可以减少垃圾收集的工作量，不用每次都去扫描所有的对象，因此也会减少停顿时间。像 Java、Julia 和 V8 的垃圾收集器都是分代的。第二招，可以尝试增量收集。你可能会问了，怎样才能实现增量呀？不是说必须扫描所有的 GC 根，才能确认一个对象是垃圾吗？

其实是有方法可以实现增量收集的，比如**三色标记（Tri-color Marking）法**。这种方法的原理，是用三种颜色来表示不同的内存对象的处理阶段：

**白色，表示算法还没有访问的对象。灰色，表示这个节点已经被访问过，但子节点还没有被访问过。黑色，表示这个节点已经被访问过，子节点也已经被访问过了（当然，没有子节点也是“子节点被访问过了”）。**

我们用一个例子来了解一下这个算法的原理。这个例子中有 8 个对象。你可以看出，其中三个对象是内存垃圾。在垃圾收集的时候，一开始所有对象都是白色的。

![img](https://static001.geekbang.org/resource/image/99/b0/990affb0e6d65610d768f4c6f471c5b0.jpg?wh=2327*1150)

然后，**扫描所有 GC 根所引用的对象**，把这些对象加入到一个工作区，并标记为灰色。在例子中，我们把 A 和 F 放入了灰色区域。

![img](https://static001.geekbang.org/resource/image/03/65/03472898d64854b6e55d013d81645365.jpg?wh=2357*1061)

如果这个对象的所有子节点都被访问过之后，就把它标记为黑色。在例子中，A 和 F 已经被标记为黑色，而 B、C、D 被标记为灰色。

![img](https://static001.geekbang.org/resource/image/31/6b/3127a489a54d01fb7ba1f9e9317abf6b.jpg?wh=2328*1105)

继续上面的过程，B、C、D 也被标记为黑色。这个时候，灰色区域已经没有对象了。那么剩下的白色对象 E、G 和 H 就能确定是垃圾了。

![img](https://static001.geekbang.org/resource/image/2e/01/2ed571bdaa8bca89bbdcf4d7f1c4a501.jpg?wh=2284*955)

回收掉 E、G 和 H 以后，就可以进入下一次循环。重新开始做增量收集。

![img](https://static001.geekbang.org/resource/image/01/e6/0171377626c9d83bb9b55d6cd68b38e6.jpg?wh=2284*853)

从上面的原理还可以看出这个算法的特点：黑色对象永远不能指向白色对象，顶多指向灰色对象。我们只要始终保证这一条，就可以去做增量式的收集。

具体来说，垃圾收集器可以做了一段标记工作后，就让程序再运行一段。如果在程序运行期间，一个黑色对象被修改了，比如往一个黑色对象 a 里新存储了一个指针 b，那么把 a 涂成灰色，或者把 b 涂成灰色，就可以了。等所有的灰色节点变为黑色以后，就可以做垃圾清理了。

总结起来，三色标记法中，黑色的节点是已经处理完毕的，灰色的节点是正在处理的。如果灰色节点都处理完，剩下的白色节点就是垃圾。**而如果在两次处理的间隙，有黑色对象又被改了，那么要重新处理。**

那在增量收集的过程中，需要编译器做什么配合？肯定是需要的，**编译器需要往生成的目标代码中插入读屏障（Read Barrier）和写屏障（Write Barrier）的代码**。也就是在程序读写对象的时候，要执行一些逻辑，保证三色的正确性。

好了，你已经理解了增量标识的原理，知道了它可以减少程序的整体停顿时间。那么，能否再**进一步减少停顿时间**呢？

这就涉及到第三招：**并发收集**。我们再仔细看上面的增量式收集算法：既然垃圾收集程序和主程序可以交替执行，那么是否可以一边运行主程序，一边用另一个或多个线程来做垃圾收集呢？

这是可以的。**实际上，除了少量的时候需要停下整个程序（比如一开头处理所有的 GC 根），其他时候是可以并发的，这样就进一步减少了总的停顿时间。**



### 课程小结
今天这一讲，我带你了解运行时中的一个重要组成部分：垃圾收集器。采用什么样的垃圾收集算法，是实现一门语言时要着重考虑的点。垃圾收集算法包含的内容有很多，我们这一讲并没有展开所有的内容，而是聚焦在介绍常用的几种算法（比如引用计数、分代收集、增量收集等）的原理，以及几种典型语言的编译器是如何跟选定的垃圾收集算法配合的。比如，在生成目标代码的时候，生成安全点、写屏障和读屏障的代码，修改引用数的代码，以及能够减少垃圾收集工作的一些优化工作。我把今天的知识点做成了思维导图，供你参考：

![img](https://static001.geekbang.org/resource/image/9f/b7/9fc46118d16472ab4bd5a718e32041b7.jpg?wh=2284*3402)

## 33 | 并发中的编译技术（一）：如何从语言层面支持线程？

现代的编程语言，开始越来越多地采用并发计算的模式。这也对语言的设计和编译技术提出了要求，需要能够更方便地利用计算机的多核处理能力。并发计算需求的增长跟两个趋势有关：一是，CPU 在制程上的挑战越来越大，逼近物理极限，主频提升也越来越慢，计算能力的提升主要靠核数的增加，比如现在的手机，核数越来越多，动不动就 8 核、12 核，用于服务器的 CPU 核数则更多；二是，现代应用对并发处理的需求越来越高，云计算、人工智能、大数据和 5G 都会吃掉大量的计算量。因此，在现代语言中，友好的并发处理能力是一项重要特性，也就需要编译技术进行相应的配合。现代计算机语言采用了多种并发技术，包括线程、协程、Actor 模式等。我会用三讲来带你了解它们，从而理解编译技术要如何与这些并发计算模式相配合。

这一讲，我们重点探讨线程模式，它是现代计算机语言中支持并发的基础模式。它也是讨论协程和 Actor 等其他话题的基础。不过在此之前，我们需要先了解一下并发计算的一点底层机制：并行与并发、进程和线程。


### 并发的底层机制：并行与并发、进程与线程
我们先来学习一下硬件层面对并行计算的支持。假设你的计算机有两颗 CPU，每颗 CPU 有两个内核，那么在同一时间，至少可以有 4 个程序同时运行。后来 CPU 厂商又发明了超线程（Hyper Threading）技术，让一个内核可以同时执行两个线程，增加对 CPU 内部功能单元的利用率，这有点像我们之前讲过的流水线技术(8讲)。这样一来，在操作系统里就可以虚拟出 8 个内核（或者叫做操作系统线程），在同一时间可以有 8 个程序同时运行。这种真正的同时运行，我们叫做并行（parallelism）。

![img](https://static001.geekbang.org/resource/image/48/77/487d4289970590947915689aeyy5a377.jpg?wh=2029*1235)

可是仅仅 8 路并行，也不够用呀。如果你去查看一下自己电脑里的进程数，会发现运行着几十个进程，而线程数就更多了。所以，操作系统会用分时技术，让一个程序执行一段时间，停下来，再让另一个程序运行。由于时间片切得很短，对于每一个程序来说，感觉上似乎一直在运行。这种“同时”能处理多个任务，但实际上并不一定是真正同时执行的，就叫做并发（Concurrency）。

实际上，哪怕我们的计算机只有一个内核，我们也可以实现多个任务的并发执行。这通常是由操作系统的一个调度程序（Scheduler）来实现的。但是有一点，操作系统在调度多个任务的时候，是有一定开销的：

**一开始是以进程为单位来做调度，开销比较大。在切换进程的时候，要保存当前进程的上下文，加载下一个进程的上下文，也会有一定的开销。由于进程是一个比较大的单位，其上下文的信息也比较多，包括用户级上下文（程序代码、静态数据、用户堆栈等）、寄存器上下文（各种寄存器的值）和系统级上下文（操作系统中与该进程有关的信息，包括进程控制块、内存管理信息、内核栈等）。**

相比于进程，线程技术就要轻量级一些。在一个进程内部，可以有多个线程，每个线程都共享进程的资源，包括内存资源（代码、静态数据、堆）、操作系统资源（如文件描述符、网络连接等）和安全属性（用户 ID 等），但拥有自己的栈和寄存器资源。这样一来，线程的上下文包含的信息比较少，所以切换起来开销就比较小，可以把宝贵的 CPU 时间用于执行用户的任务。

总结起来，线程是操作系统做并发调度的基本单位，并且可以跟同一个进程内的其他线程共享内存等资源。操作系统会让一个线程运行一段时间，然后把它停下来，把它所使用的寄存器保存起来，接着让另一个线程运行，这就是线程调度原理。你要在大脑里记下这个场景，这样对理解后面所探讨的所有并发技术都很有帮助。

![img](https://static001.geekbang.org/resource/image/13/2c/13597dc8c5ea3124a1b85b040072242c.jpg?wh=1973*1000)

图 2：进程的共享资源和线程私有的资源

我们通常把进程作为资源分配的基本单元，而把线程作为**并发执行**的基本单元。不过，有的时候，用进程作为并发的单元也是比较好的，**比如谷歌浏览器每打开一个 Tab 页，就新启动一个进程。这是因为，浏览器中多个进程之间不需要有互动。并且，由于各个进程所使用的资源是独立的，所以一个进程崩溃也不会影响到另一个。**

而如果采用线程模型的话，由于它比较轻量级，消耗的资源比较少，所以你可以在一个操作系统上启动几千个线程，这样就能执行更多的并发任务。所以，在一般的网络编程模型中，我们可以针对每个网络连接，都启动一条线程来处理该网络连接上的请求。在第二个模块中我们分析过的MySQL就是这样做的。你每次跟 MySQL 建立连接，它就会启动一条线程来响应你的查询请求。

采用线程模型的话，程序就可以在不同线程之间共享数据。比如，在数据库系统中，如果一个客户端提交了一条 SQL，那么这个 SQL 的编译结果可以被缓存起来。如果另一个用户恰好也执行了同一个 SQL，那么就可以不用再编译一遍，因为两条线程可以访问共享的内存。

但是共享内存也会带来一些问题。当多个线程访问同样的数据的时候，会出现数据处理的错误。如果使用并发程序会造成错误，那当然不是我们所希望的。所以，我们就要采用一定的技术去消除这些错误。

### java的并发机制


……


## 34 | 并发中的编译技术（二）：如何从语言层面支持协程？
上一讲我们提到了线程模式是当前计算机语言支持并发的主要方式。不过，在有些情况下，线程模式并不能满足要求。当需要运行大量并发任务的时候，线程消耗的内存、线程上下文切换的开销都太大。这就限制了程序所能支持的并发任务的数量。在这个背景下，一个很“古老”的技术重新焕发了青春，这就是协程（Coroutine）。它能以非常低的代价、友好的编程方式支持大量的并发任务。像 Go、Python、Kotlin、C# 等语言都提供了对协程的支持。今天这一讲，我们就来探究一下如何在计算机语言中支持协程的奇妙功能，它与编译技术又是怎样衔接的。首先，我们来认识一下协程。


### 协程（Coroutine）的特点与使用场景
我说协程“古老”，是因为这个概念是在 1958 年被马尔文 · 康威（Melvin Conway）提出来、在 20 世纪 60 年代又被高德纳（Donald Ervin Knuth）总结为两种子过程（Subroutine）的模式之一。一种是我们常见的函数调用的方式，而另一种就是协程。在当时，计算机的性能很低，完全没有现代的多核计算机。而采用协程就能够在这样低的配置上实现并发计算，可见它是多么的轻量级。有的时候，协程又可能被称作绿色线程、纤程等，所采用的技术也各有不同。但总的来说，它们都有一些共同点。

首先，协程占用的资源非常少。你可以在自己的笔记本电脑上随意启动几十万个协程，而如果你启动的是几十万个线程，那结果就是不可想象的。比如，在 JVM 中，缺省会为每个线程分配 1MB 的内存，用于线程栈等。这样的话，几千个线程就要消耗掉几个 GB 的内存，而几十万个线程理论上需要消耗几百 GB 的内存，这还没算程序在堆中需要申请的内存。当然，由于底层操作系统和 Java 应用服务器的限制，你也无法启动这么多线程。

其次，协程是用户自己的程序所控制的并发。也就是说，协程模式，一般是程序交出运行权，之后又被另外的程序唤起继续执行，整个过程完全是由用户程序自己控制的。而线程模式就完全不同了，它是由操作系统中的调度器（Scheduler）来控制的。

我们看个 Python 的例子：
```
py
def running_avg():
    total = 0.0
    count = 0
    avg = 0
    while True:
        num = yield avg
        total += num
        count += 1
        avg = total/count

#生成协程，不会有任何输出
ra = running_avg()
#运行到yield
next(ra)     

print(ra.send(2))  
print(ra.send(3))  
print(ra.send(4)) 
print(ra.send(7))  
print(ra.send(9))  
print(ra.send(11)) 

#关掉协程
ra.close
```
可以看到，使用协程跟我们平常使用函数几乎没啥差别，对编程人员很友好。实际上，它可以认为是跟函数并列的一种子程序形式。和函数的区别是，函数调用时，调用者跟被调用者之间像是一种上下级的关系；而在协程中，调用者跟被调用者更像是互相协作的关系，比如一个是生产者，一个是消费者。这也是“协程”这个名字直观反映出来的含义。我们用一张图来对比下函数和协程中的调用关系。

![img](https://static001.geekbang.org/resource/image/5d/4d/5daca741e22f31520ff0d39b5acc8f4d.jpg?wh=2529*1568)

细想一下，编程的时候，这种需要子程序之间互相协作的场景有很多，我们一起看两种比较常见的场景。

第一种比较典型的场景，就是生产者和消费者模式。如果你用过 Unix 管道或者消息队列编程的话，会非常熟悉这种模式。但那是在多个进程之间的协作。如果用协程的话，在一个进程内部就能实现这种协作，非常轻量级。就拿编译器前端来说，词法分析器（Tokenizer）和语法分析器（Parser）就可以是这样的协作关系。也就是说，为了更好的性能，没有必要一次把词法分析完毕，而是语法分析器消费一个，就让词法分析器生产一个。因此，这个过程就没有必要做成两个线程了，否则就太重量级了。这种场景，我们可以叫做生成器（Generator）场景：主程序调用生成器，给自己提供数据。



特别适合使用协程的第二种场景是 IO 密集型的应用。比如做一个网络爬虫同时执行很多下载任务，或者做一个服务器同时响应很多客户端的请求，这样的任务大部分时间是在等待网络传输。如果用同步阻塞的方式来做，一个下载任务在等待的时候就会把整个线程阻塞掉。而用异步的方式，协程在发起请求之后就把控制权交出，调度程序接收到数据之后再重新激活协程，这样就能高效地完成 IO 操作，同时看上去又是用同步的方式编程，不需要像异步编程那样写一大堆难以阅读的回调逻辑。

这样的场景在微服务架构的应用中很常见，我们来简化一个实际应用场景，分析下如何使用协程。在下面的示例中，应用 A 从客户端接收大量的并发请求，而应用 A 需要访问应用 B 的服务接口，从中获得一些信息，然后返回给客户端。

![img](https://static001.geekbang.org/resource/image/a3/12/a3a7744f2f11c7ec57f4250d927a4112.jpg?wh=1667*1111)

要满足这样的场景，我们最容易想到的就是，编写同步通讯的程序，其实就是同步调用。假设应用 A 对于每一个客户端的请求，都会起一个线程做处理。而你呢，则在这个线程里发起一个针对应用 B 的请求。在等待网络返回结果的时候，当前线程会被阻塞住。

![img](https://static001.geekbang.org/resource/image/f0/25/f04b7058db480194fa80edd6849cyy25.jpg?wh=2099*1239)

这个架构是最简单的，你如果采用 Java 的 Servlet 容器来编写程序的话，很可能会采用这个结构。但它有一些缺陷：

对于每个客户端请求，都要起一个线程。如果请求应用 B 的时延比较长，那么在应用 A 里会积压成千上万的线程，从而浪费大量的服务器资源。而且，当线程超过一定数量，应用服务器就会拒绝后续的请求。大量的请求毫无节制地涌向应用 B，使得应用 B 难以承受负载，从而导致响应变慢，甚至宕机。

因为同步调用的这种缺点，近年来异步编程模型得到了更多的应用，典型的就是 Node.js。在异步编程模型中，网络通讯等 IO 操作不必阻塞线程，而是通过回调来让主程序继续执行后续的逻辑。

![img](https://static001.geekbang.org/resource/image/d6/83/d69d2d5e6c220286716c9bc073523d83.jpg?wh=2156*1451)

上图中，我们只用到了 4 个线程，对应操作系统的 4 个真线程，可以减少线程切换的开销。在每个线程里，维护一个任务队列。首先，getDataFromApp2() 会被放到任务队列；当数据返回以后，系统的调度器会把 sendBack() 函数放进任务队列。

这个例子比较简单，只有一层回调，你还能读懂它的逻辑。但是，采用这种异步编程模式，经常会导致多层回调，让代码很难阅读。这种现象，被叫做“回调地狱（Callback Hell）”。



这时候，就显示出协程的优势了。协程可以让你用自己熟悉的命令式编程的风格，来编写异步的程序。比如，对于上面的示例程序，用协程可以这样写，看上去跟编写同步调用的代码没啥区别。

```
requestHandler(){
  ...;
  await getDataFromApp2();
  ...;
  sendBack(); 
}
```

当然，我要强调一下，在协程用于同步和异步编程的时候，其调度机制是不同的。跟异步编程配合的时候，要把异步 IO 机制与协程调度机制关联起来。好了，现在你已经了解了协程的特点和适用场景。那么问题来了，如何让一门语言支持协程呢？要回答这个问题，我们就要先学习一下协程的运行原理。



### 协程的运行原理
当我们使用函数的时候，简单地保持一个调用栈就行了。当 fun1 调用 fun2 的时候，就往栈里增加一个新的栈帧，用于保存 fun2 的本地变量、参数等信息；这个函数执行完毕的时候，fun2 的栈帧会被弹出（恢复栈顶指针 sp），并跳转到返回地址（调用 fun2 的下一条指令），继续执行调用者 fun1 的代码。

![img](https://static001.geekbang.org/resource/image/c1/ea/c1a859d94c48bbb245e3e3a51ee9f3ea.jpg?wh=2001*1012)

但如果调用的是协程 coroutine1，该怎么处理协程的栈帧呢？因为协程并没有执行完，显然还不能把它简单地丢掉。这种情况下，程序可以从堆里申请一块内存，保存协程的活动记录，包括本地变量的值、程序计数器的值（当前执行位置）等等。这样，当下次再激活这个协程的时候，可以在栈帧和寄存器中恢复这些信息。

![img](https://static001.geekbang.org/resource/image/0d/7e/0d6986ebfe379694759b84b57eb2877e.jpg?wh=2341*1216)

图 6：调用协程时的控制流和栈桢管理

把活动记录保存到堆里，是不是有些眼熟？其实，这有点像闭包的运行机制。程序在使用闭包的时候，也需要在堆里保存闭包中的自由变量的信息，并且在下一次调用的时候，从堆里恢复。只不过，闭包不需要保存本地变量，只保存自由变量就行了；也不需要保存程序计数器的值，因为再一次调用闭包函数的时候，还是从头执行，而协程则是接着执行 yield 之后的语句。fun1 通过 resume 语句，让协程继续运行。这个时候，协程会去调用一个普通函数 fun2，而 fun2 的栈帧也会加到栈上。

![img](https://static001.geekbang.org/resource/image/f5/de/f52a8758cbe86006a7400b29aa9bdede.jpg?wh=2385*1581)

图 7：在协程里调用普通函数时的栈桢情况

如果 fun2 执行完毕，那么就会返回到协程。而协程也会接着执行下一个语句，这个语句是一个专门针对协程的返回语句，我们叫它 co_return 吧，以便区别于传统的 return。在执行了 co_return 以后，协程就结束了，无法再 resume。这样的话，保存在堆里的活动记录也就可以销毁了。

![img](https://static001.geekbang.org/resource/image/df/a1/dfd44eac96784a283eac49750c6ebaa1.jpg?wh=2402*1548)

图 8：协程结束时对栈桢的处理

通过上面的例子，你应该已经了解了协程的运行原理。那么我们学习编译原理会关心的问题是：**实现协程的调度，包括协程信息的保存与恢复、指令的跳转，需要编译器的帮忙吗？还是用一个库就可以实现？**实际上，对于 C 和 C++ 这样的语言来说，确实用一个库就可以实现。因为 C 和 C++ 比较灵活，比如可以用 setjmp、longjmp 等函数，跨越函数的边界做指令的跳转。但如果用库实现，通常要由程序管理哪些状态信息需要被保存下来。为此，你可能要专门设计一个类型，来参与实现协程状态信息的维护。而如果用编译器帮忙，那么就可以自动确定需要保存的协程的状态信息，并确定需要申请的内存大小。一个协程和函数的区别，就仅仅在于是否使用了 yield 和 co_return 语句而已，减轻了程序员编程的负担。好了，刚才我们讨论了，在实现协程的时候，要能够正确保存协程的活动记录。在具体实现上，有 Stackful 和 Stackless 两种机制。采用不同的机制，对于协程能支持的特性也很有关系。所以接下来，我带你再进一步地分析一下 Stackful 和 Stackless 这两种机制。



### Stackful 和 Stackless 的协程
到目前为止，看上去协程跟普通函数（子程序）的差别也不大嘛，你看：

都是由一个主程序调用，运行一段时间以后再把控制流交回给主程序；都使用栈来管理本地变量和参数等信息，**只不过协程在没有完全运行完毕时，要用堆来保存活动记录**；在协程里也可以调用其他的函数。

可是，在有的情况下，我们没有办法直接在 coroutine1 里确定是否要暂停线程的执行，可能需要在下一级的子程序中来确定。比如说，coroutine1 函数变得太大，我们重构后，把它的功能分配到了几个子程序中。那么暂停协程的功能，也会被分配到子程序中。

![img](https://static001.geekbang.org/resource/image/93/dd/9317554e9243f9ef4708858fe22322dd.jpg?wh=2428*1556)

图 9：在辅助函数里暂停协程时的控制流和栈桢情况

这个时候，在 helper() 中暂停协程，会让控制流回到 fun1 函数。而当在 fun1 中调用 resume 的时候，控制流应该回到 helper() 函数中 yield 语句的下一条，继续执行。coroutine1() 和 helper() 加在一起，起到了跟原来只有一个 coroutine1() 一样的效果。这个时候，在栈里不仅要加载 helper() 的活动记录，还要加载它的上一级，也就是 coroutine1() 的活动记录，这样才能维护正确的调用顺序。当 helper() 执行完毕的时候，控制流会回到 coroutine1()，继续执行里面的逻辑。在这个场景下，不仅要从堆里恢复多个活动记录，还要维护它们之间的正确顺序。上面的示例中，还只有两级调用。如果存在多级的调用，那就更麻烦了。那么，怎么解决这个技术问题呢？你会发现，其实协程的逐级调用过程，形成了自己的调用栈，这个调用栈需要作为一个整体来使用，不能拆成一个个单独的活动记录。

既然如此，那我们就加入一个辅助的运行栈好了。这个栈通常被叫做 Side Stack。每个协程，都有一个自己专享的协程栈。

![img](https://static001.geekbang.org/resource/image/2b/c0/2bf6a16cf5d7e85ee1db921efe0d0bc0.jpg?wh=2298*942)

图 10：协程的 Side Stack

好了，现在是时候给你介绍两个术语了：这种需要一个辅助的栈来运行协程的机制，叫做 Stackful Coroutine；而在主栈上运行协程的机制，叫做 Stackless Coroutine。



对于 Stackless 的协程来说，只能在顶层的函数里把控制权交回给调用者。如果这个协程调用了其他函数或者协程，必须等它们返回后，才能去执行暂停协程的操作。从这种角度看，Stackless 的特征更像一个函数。而对于 Stackful 的协程来说，可以在协程栈的任意一级，暂停协程的运行。从这个角度看，Stackful 的协程像是一个线程，不管有多少级的调用，随时可以让这个协程暂停，交出控制权。除此之外，我们再仔细去想，因为设计上的不同，Stackless 和 Stackful 的协程其实还会产生其他的差别：

Stackless 的协程用的是主线程的栈，也就是说它基本上会被绑定在创建它的线程上了。而 Stackful 的协程，可以从一个线程脱离，附加到另一个线程上。Stackless 的协程的生命周期，一般来说受制于它的创建者的生命周期。而 Stackful 的协程的生命周期，可以超过它的创建者的生命周期。

好了，以上就是对 Stackless 和 Stackful 的协程的概念和区别了。其实，对于协程，我们可能还会听说一种分类方法，就是对称的和非对称的。到目前为止，我们讲到的协程都是非对称的。有一个主程序，而协程像是子程序。主程序和子程序控制程序执行的原语是不同的，一个用于激活协程，另一个用于暂停协程。而对称的协程，相互之间是平等的关系，它们使用相同的原语在协程之间移交控制权。那么，C++、Python、Java、JavaScript、Julia 和 Go 这些常见语言中，哪些是支持协程的？是 Stackless 的 ，还是 Stackful 的？是对称的，还是非对称的？需要编译器做什么配合？

接下来，我们就一起梳理下。


### 不同语言的协程实现和差异


#### Python 语言的协程实现
我们前面讲协程的运行原理用的示例程序，就是用 Python 写的。这是 Python 的一种协程的实现，支持的是同步处理，叫做 generator 模式。3.4 版本之后，Python 支持一种异步 IO 的协程模式，采用了 async/await 关键字，能够以同步的语法编写异步程序。总体来说，Python 是一种解释型的语言，而且内部所有成员都是对象，所以实现协程的机制是很简单的，保存协程的执行状态也很容易。只不过，你不可能把 Python 用于像刚才微信那样高并发的场景，因为解释型语言对资源的消耗太高了。尽管如此，在把 Python 当作脚本语言使用的场景中，比如编写网络爬虫，采用它提供的协程加异步编程的机制，还是能够带来很多好处的。

#### Julia 和 Go 语言的协程实现
Julia 语言的协程机制，跟以上几种语言都不同。它提供的是对称的协程机制。多个协程可以通过 channel 通讯，当从 channel 里取不出信息时，或者 channel 已满不能再发信息时，自然就停下来了。

当我谈到 channel 的时候，熟悉 Go 语言的同学马上就会想到 Goroutine。Goroutine 是 Go 语言的协程机制，也是用 channel 实现协程间的协作的。

**我把对 Go 语言协程机制的介绍放在最后，是因为 Goroutine 实在是很强大。我觉得，所有对并发编程有兴趣的同学，都要看一看 Goroutine 的实现机制，都会得到很大的启发。**

我的感受是，Goroutine 简直是实现轻量级并发功能的集大成者，几乎考虑到了你能想到的所有因素。介绍 Goroutine 的文章有很多，我就不去重复已有的内容了，你可以看看“[How Stacks are Handled in Go](https://blog.cloudflare.com/how-stacks-are-handled-in-go/)”这篇文章。现在，我就顺着本讲的知识点，对 Goroutine 的部分特点做一点介绍。

首先我们来看一下，Goroutine 是 Stackful 还是 Stackless？答案是 Stackful 的。就像我们前面已经总结过的，Stackful 协程的特点主要是两点：**协程的生命周期可以超过其创建者，以及协程可以从一个线程转移到另一个线程。**后者在 Goroutine 里特别有用。当一个协程调用了一个系统功能，导致线程阻塞的时候，那么排在这条线程上的其他 Goroutine 岂不是也要被迫等待？为了避免这种尴尬，Goroutine 的调度程序会把被阻塞的线程上的其他 Goroutine 迁移到其他线程上。

我们讲 libco 的时候还讲过，Stackful 的缺点是要预先分配比较多的内存用作协程的栈空间，比如 libco 要为每个协程分配 128K 的栈。而 Go 语言只需要为每个 Goroutine 分配 2KB 的栈。你可能会问了，万一空间不够了怎么办，不会导致内存访问错误吗？

不会的。Go 语言的函数在运行的时候，会有一小块**序曲代码**，用来检查栈空间够不够用。如果不够用，就马上申请新的内存。需要注意的是，像这样的机制，必须有编译器的配合才行，**编译器可以为每个函数生成这样的序曲代码**。如果你用库来实现协程，就无法实现这样的功能。

通过这个例子，**你也可以体会到把某个特性做成语言原生的，以及用库去实现的差别**。

我想说的 Go 语言协程机制的第二个特点，就是 channel 机制。channel 提供了 Goroutine 之间互相通讯，从而能够协调行为的机制。Go 语言的运行时保证了在同一个时刻，只有一个 Goroutine 能够读写 channel，这就避免了我们前一讲提到的，用锁来保证多个线程访问共享数据的难题。当然，**channel 在底层也采用了锁的机制，毕竟现在不需要程序员去使用这么复杂且容易出错的机制了。**

Go 语言协程机制的第三个特点，是关于协程的调度时机。今天这一讲，我们其实看到了两种调度时机：**对于 generator 类型的协程，基本上是同步调度的，协程暂停以后，控制立即就回到主程序；第二个调度机制，是跟异步 IO 机制配合。**

而我关心的，是能否实现像线程那样的抢占式（preemptive）的调度。操作系统的线程调度器，在进行调度的时候，可以不管当前线程运行到了什么位置，直接中断它的运行，并把相关的寄存器的值保存下来，然后去运行另一个线程。这种抢占式的调度的一个最大的好处，是不会让某个程序霸占 CPU 资源不放，而是公平地分配给各个程序。而协程也存在着类似的问题。如果一个协程长时间运行，那么排在这条线程上的其他协程，就被剥夺了运行的机会。

Goroutine 在解决这个问题上也做了一些努力。比如，在下面的示例程序中，foo 函数中的循环会一直运行。这时候，编译器就可以在 bar() 函数的序曲中，插入一些代码，检查当前协程是否运行时间太久，从而主动让出控制权。不过，**如果 bar() 函数被内联了，处理方式就要有所变化**。但总的来说，由于有编译器的参与，这种类似抢占的逻辑是可以实现的。

```go
func foo(){
  while true{
    bar();   //可以在bar函数的序曲中做检查。
  }
}
```
在 Goroutine 实现了各种丰富的调度机制以后，它已经变得不完全由用户的程序来主导协程的调度了，而是能够更加智能、更加优化地实现协程的调度，**由操作系统的线程调度器、Go 语言的调度器和用户程序三者配合实现**。这也是 Go 语言的一个重要优点。

那么，我们从 C、C++、Python、Java、JavaScript、Julia 和 Go 语言中，就能总结出协程实现上的特点了：

除了 Julia 和 Go，其他语言采用的都是非对称的协程机制。Go 语言是采用协程最彻底的。在采用了协程以后，已经不需要用过去的线程。像 C++、Go 这样编译成机器码执行的语言，对协程栈的良好管理，能够大大降低内存占用，增加支持的协程的数量。协程与异步 IO 结合是一个趋势。


### 课程小结
今天这一讲，我们学习了协程的定义、使用场景、实现原理和不同语言的具体实现机制。我们特别从编译技术的角度，关注了协程对栈的使用机制，看看它与传统的程序有什么不同。在这个过程中，一方面，你会通过今天的课程对协程产生深入的认识；另一方面，你会更加深刻地认识到编译技术是如何跟语言特性的设计和运行时紧密配合的。协程可以用库实现，也可以借助编译技术成为一门语言的原生特性。采用编译技术，能帮助我们自动计算活动记录的大小，实现自己独特的栈管理机制，实现抢占式调度等功能。本讲的思维导图我也放在了下面，供你参考：

![img](https://static001.geekbang.org/resource/image/89/38/89f40bc89yyf16f0d855d43e85d9c838.jpg?wh=2284*3409)


## 35 | 并发中的编译技术（三）：Erlang语言厉害在哪里？

不论是线程模型、还是协程模型，当涉及到多个线程访问共享数据的时候，都会出现竞争问题，从而需要用到锁。锁会让其他需要访问该数据的线程等待，从而导致系统整体处理能力的降低。并且，编程人员还要特别注意，避免出现死锁。比如，线程 A 持有了锁 x，并且想要获得锁 y；而线程 B 持有了锁 y，想要获得锁 x，结果这两个线程就会互相等待，谁也进行不下去。像数据库这样的系统，检测和消除死锁是一项重要的功能，以防止互相等待的线程越来越多，对数据库操作不响应，并最终崩溃掉。

既然使用锁这么麻烦，那在并发计算中，能否不使用锁呢？这就出现了 Actor 模型。那么，什么是 Actor 模型？为什么它可以不用锁就实现并发？这个并发模型有什么特点？需要编译技术做什么配合？


### 什么是 Actor 模型？
在线程和协程模型中，之所以用到锁，是因为两个线程共享了内存，而它们会去修改同一个变量的值。那，如果避免共享内存，是不是就可以消除这个问题了呢？没错，这就是 Actor 模型的特点。Actor 模型是 1973 年由 Carl Hewitt 提出的。在 Actor 模型中，并发的程序之间是不共享内存的。它们通过互相发消息来实现协作，很多个一起协作的 Actor 就构成了一个支持并发计算的系统。我们看一个有三个 Actor 的例子。

![img](https://static001.geekbang.org/resource/image/d1/fb/d1494936866024b3c9a687da3a9de1fb.jpg?wh=2236*1350)

你会注意到，每个 Actor 都有一个邮箱，用来接收其他 Actor 发来的消息；每个 Actor 也都可以给其他 Actor 发送消息。这就是 Actor 之间交互的方式。Actor A 给 Actor B 发完消息后就返回，并不会等着 Actor B 处理完毕，所以它们之间的交互是异步的。如果 Actor B 要把结果返回给 A，也是通过发送消息的方式。

这就是 Actor 大致的工作原理了。因为 Actor 之间只是互发消息，没有共享的变量，当然也就不需要用到锁了。

但是，你可能会问：如果不共享内存，能解决传统上需要对资源做竞争性访问的需求吗？比如，卖电影票、卖火车票、秒杀或者转账的场景。我们以卖电影票为例讲解一下。

在用传统的线程或者协程来实现卖电影票功能的时候，对票的状态进行修改，需要用锁的机制实现同步互斥，以保证同一个时间段只有一个线程可以去修改票的状态、把它分配给某个用户，从而避免多个线程同时访问而出现一张票卖给多个人的情况。这种情况下，多个程序是串行执行的，所以系统的性能就很差。

如果用 Actor 模式会怎样呢？

你可以把电影院的前半个场地和后半个场地的票分别由 Actor B 和 C 负责销售：Actor A 在接收到定前半场座位的请求的时候，就发送给 Actor B，后半场的就发送给 Actor C，Actor B 和 C 依次处理这些请求；如果 Actor B 或 C 接收到的两个信息都想要某个座位，那么针对第二个请求会返回订票失败的消息。

![img](https://static001.geekbang.org/resource/image/31/2d/31838a322d9bddd811a53b44e63ac82d.jpg?wh=2083*1216)

你发现没有？在这个场景中，Actor B 和 C 仍然是顺序处理各个请求。但因为是两个 Actor 并发地处理请求，所以系统整体的性能会提升到原来的两倍。甚至，你可以让每排座位、每个座位都由一个 Actor 负责，使得系统的性能更高。因为在系统中创建一个 Actor 的成本是很低的。Actor 跟协程类似，很轻量级，一台服务器里创建几十万、上百万个 Actor 也没有问题。如果每个 Actor 负责一个座位，那一台服务器也能负责几十万、上百万个座位的销售，也是可以接受的。



当然，实际的场景要比这个复杂，比如一次购买多张相邻的票等，但原理是一样的。用这种架构，可以大大提高并发能力，处理海量订票、秒杀等场景不在话下。其实，我个人比较喜欢 Actor 这种模式，因为它跟现实世界里的分工协作很相似。比如，餐厅里不同岗位的员工，他们通过互相发信息来实现协作，从而并发地服务很多就餐的顾客。分析到这里，我再把 Actor 模式跟你非常熟悉的一个概念，面向对象编程（Object Oriented Programming，OOP）关联起来。你可能会问：Actor 和面向对象怎么还有关联？

是的。面向对象语言之父阿伦 · 凯伊（Alan Kay），Smalltalk 的发明人，在谈到面向对象时是这样说的：对象应该像生物的细胞，或者是网络上的计算机，它们只能通过消息互相通讯。对我来说 OOP 仅仅意味着消息传递、本地保留和保护以及隐藏状态过程，并且尽量推迟万物之间的绑定关系。

总结起来，Alan 对面向对象的理解，强调消息传递、封装和动态绑定，没有谈多态、继承等。对照这个理解，你会发现 Actor 模式比现有的流行的面向对象编程语言，更加接近面向对象的实现。

无论如何，通过把 Actor 和你熟悉的面向对象做关联，我相信能够拉近你跟 Actor 之间的距离，甚至会引发你以新的视角来审视目前流行的面向对象范式。

好了，到现在，你可以说是对 Actor 模型比较熟悉了，也可以这么理解：Actor 有点像面向对象程序里的对象，里面可以封装一些数据和算法；但你不能调用它的方法，只能给它发消息，它会异步地、并发地处理这些消息。

但是，你可能会提出一个疑问：Actor 模式不用锁的机制就能实现并发程序之间的协作，这一点很好，那么它有没有什么缺点呢？

我们知道，任何设计方案都是一种取舍。一个方案有某方面的优势，可能就会有其他方面的劣势。采用 Actor 模式，会有两方面的问题。

第一，由于 Actor 之间不共享任何数据，因此不仅增加了数据复制的时间，还增加了内存占用量。但这也不完全是缺点：一方面，你可以通过在编写程序时，尽量降低消息对象的大小，从而减少数据复制导致的开销；另一方面，消息传递的方式对于本机的 Actor 和集群中的 Actor 是一样的，这就使得编写分布式的云端应用更简单，从而在云计算时代可以获得更好的应用。

第二，基于消息的并发机制，基本上是采用异步的编程模式，这就和通常程序的编程风格有很大的不同。你发出一个消息，并不会马上得到结果，而要等待另一个 Actor 发送消息回来。这对于习惯于编写同步代码的同学，可能是一个挑战。

好了，我们已经讨论了 Actor 机制的特点。接下来我们再看看，什么语言和框架实现了 Actor 模式。



### 支持 Actor 模型的语言和框架
支持 Actor 的最有名的语言是 Erlang。Erlang 是爱立信公司发明的，它的正式版本是在 1987 年发布，其核心设计者是乔 · 阿姆斯特朗（Joe Armstrong），最早是用于开发电信领域的软件系统。在 Erlang 中，每个 Actor 叫作一个进程（Process）。但这个“进程”其实不是操作系统意义上的进程，而是 Erlang 运行时的并发调度单位。Erlang 有两个显著的优点：首先，对并发的支持非常好，所以它也被叫做面向并发的编程语言（COP）。第二，用 Erlang 可以编写高可靠性的软件，可以达到 9 个 9。这两个优点都与 Actor 模式有关：

Erlang 的软件由很多 Actor 构成；这些 Actor 可以分布在多台机器上，相互之间的通讯跟在同一台机器上没有区别；某个 Actor 甚至机器出现故障，都不影响整体系统，可以在其他机器上重新启动该 Actor；Actor 的代码可以在运行时更新。


所以，由 Actor 构成的系统真的像一个生命体，每个 Actor 像一个细胞。细胞可以有新陈代谢，而生命体却一直存在。可以说，用 Erlang 编写的基于 Actor 模式的软件，非常好地体现了复杂系统的精髓。到这里，你是不是就能解释“Erlang 语言厉害在哪里”这个问题了。鉴于 Actor 为 Erlang 带来的并发能力和高可靠性，有一些比较流行的开源系统就是用 Erlang 编写的。比如，消息队列系统 RabbitMQ、分布式的文档数据库系统 CouchDB，都很好地体现了 Erlang 的并发能力和健壮性。除了 Erlang 以外，Scala 语言也提供了对 Actor 的支持，它是通过 Akka 库实现的，运行在 JVM 上。我还关注了微软的一个 Orleans 项目，它在.NET 平台上支持 Actor 模式，并进一步做了一些有趣的创新。那接下来我们继续探讨一下，这些语言和框架是如何实现 Actor 机制的，以及需要编译器做什么配合。


### Actor 模型的实现
在上一讲研究过协程的实现机制以后，我们现在再分析 Actor 的实现机制时，其实就应该会把握要点了。比如说，我们会去看它的调度机制和内存管理机制等。鉴于 Erlang 算是支持 Actor 的最有名、使用最多的语言，接下来我会以 Erlang 的实现机制带你学习 Actor 机制是如何实现的。首先，我们知道，肯定要有个调度器，把海量的 Actor 在多个线程上调度。


### 并发调度机制
那我们需要细究一下：对于 Actor，该如何做调度呢？什么时候把一个 Actor 停下，让另一个 Actor 运行呢？协程也好，Actor 也好，都是在应用级做调度，而不是像线程那样，在应用完全不知道的情况下，就被操作系统调度了。对于协程，我们是通过一些像 yield 这样的特殊语句，触发调度机制。那，Actor 在什么时候调度比较好呢？前面我们也讲过了，Actor 的运行规律，是每次从邮箱取一条消息并进行处理。那么，我们自然会想到，一个可选的调度时机，就是让 Actor 每处理完一条消息，就暂停一下，让别的 Actor 有机会运行。当然，如果处理一条消息所花费的时间太短，比如有的消息是可以被忽略的，那么处理多条消息，累积到一定时间再去调度也行。了解了调度时机，我们再挑战第二个比较难的话题：如果处理一条消息就要花费很长时间怎么办呢？能否实现抢占式的调度呢，就像 Goroutine 那样？


当然可以，但这个时候就肯定需要编译器和运行时的配合了。Erlang 的运行机制，是基于一个寄存器机解释执行。这使得调度器可以在合适的时机，去停下某个 Actor 的运行，调度其他 Actor 过来运行。Erlang 做抢占式调度的机制是对 Reduction 做计数，Reduction 可以看作是占时不长的一小块工作量。如果某个 Actor 运行了比较多的 Reduction，那就可以对它做调度，从而提供了软实时的能力（具体可以参考[这篇文章](https://blog.stenmans.org/theBeamBook/#_scheduling_non_preemptive_reduction_counting)）。



在比较新的版本中，Erlang 也加入了编译成本地代码的特性，那么在生成的本地代码中，也需要编译器加入对 Reduction 计数的代码，这就有点像 Goroutine 了。这也是 Erlang 和 Scala/Akka 的区别。Akka 没有得到编译器和 JVM 在底层的支持，也就没办法实现抢占式的调度。这有可能让某些特别耗时的 Actor 影响了其他 Actor，使得系统的响应时间不稳定。最后一个涉及调度的话题，是 I/O 与调度的关系。这个关系如果处理得不好，那么对系统整体的性能影响会很大。通常我们编写 I/O 功能时，会采用同步编程模式来获取数据。这个时候，操作系统会阻塞当前的线程，直到成功获取了数据以后，才可以继续执行。


```
getSomeData();    //操作系统会阻塞住线程，直到获得了数据。
do something else //继续执行
```
采用这种模式开发一个服务端程序，会导致大量线程被阻塞住，等待 I/O 的结果。由于每个线程都需要不少的内存，并且线程切换的成本也比较高，因此就导致一台服务器能够服务的客户端数量大大降低。如果这时候，你在运行时查看服务程序的状态，就会发现大量线程在等待，CPU 利用率也不高，而新的客户端又连接不上来，造成服务器资源的浪费。并且，如果采用协程等应用级的并发机制，一个线程被阻塞以后，排在这个线程上的其他协程也只能等待，从而导致服务响应时间变得不可靠，有时快，有时慢。我们在前一讲了解过 Goroutine 的调度器。它在遇到这种情况的时候，就会把这条线程上的其他 Goroutine 挪到没被阻塞的线程上，从而尽快得到运行机会。


由于阻塞式 I/O 的缺点，现在很多语言也提供了非阻塞 I/O 的机制。在这种机制下，程序在做 I/O 请求的时候并不能马上获得数据。当操作系统准备好数据以后，应用程序可以通过轮询或被回调的方式获取数据。Node.js 就是采用这种 I/O 模式的典型代表。上一讲提到的 C++ 协程库 libco，也把非阻塞的网络通讯机制和协程机制做了一个很好的整合，大大增加了系统的整体性能。而 Erlang 在很早以前就解决了这个问题。在 Erlang 的最底层，所有的 I/O 都是用事件驱动的方式来实现的。系统收到了一块数据，就调用应用来处理，整个过程都是非阻塞的。说完了并发调度机制，我们再来看看运行时的另一个重要特征，内存管理机制。

### 内存管理机制
内存管理机制要考虑栈、堆都怎么设计，以及垃圾收集机制等内容。

![img](https://static001.geekbang.org/resource/image/a3/fe/a36035d18b24bcebb8a4d0a0b191a3fe.jpg?wh=2350*1031)

图 3：Erlang 的内存模型

首先说栈。每个 Actor 也需要有自己的栈空间，在执行 Actor 里面的逻辑的时候，用于保存本地变量。这跟上一节讲过的 Stateful 的协程很像。再来看看堆。Erlang 的堆与其他语言有很大的区别，它的每个 Actor 都有自己的堆空间，而不是像其他编程模型那样，不同的线程共享堆空间。这也很容易理解，因为 Actor 模型的特点，就是并发的程序之间没有共享的内存，所以当然也就不需要共享的堆了。再进一步，由于每个 Actor 都有自己的堆，因此会给垃圾收集带来很大的便利：

因为整个程序划分成了很多个 Actor，每个 Actor 都有自己的堆，所以每个 Actor 的垃圾都比较少，不用一次回收整个应用的垃圾，所以回收速度会很快。由于没有共享内存，所以垃圾收集器不需要停下整个应用，而只需要停下被收集的 Actor。这就避免了“停下整个世界（STW）”问题，而这个问题是 Java、Go 等语言面临的重大技术挑战。如果一个 Actor 的生命周期结束，那么它占用的内存会被马上释放掉。这意味着，对于有些生命周期比较短的 Actor 来说，可能压根儿都不需要做垃圾收集。

好了，基于 Erlang，我们学习了 Actor 的运行时机制的两个重要特征：一是并发调度机制，二是内存管理机制。那么，与此相配合，需要编译器做什么工作呢？


### 编译器的配合工作
我们说过，Erlang 首先是解释执行的，是用一个寄存器机来运行字节码。那么，编译器的任务，就是生成正确的字节码。之前我们已经分别研究过 Graal、Python 和 V8 Ignition 的字节码了。我们知道，字节码的设计很大程度上体现了语言的设计特点，体现了与运行时的交互过程。Erlang 的字节码设计当然也是如此。比如，针对消息的发送和接收，它专门提供了 send 指令和 receive 指令，这体现了 Erlang 的并发特征。再比如，Erlang 还提供了与内存管理有关的指令，比如分配一个新的栈桢等，体现了 Erlang 在内存管理上的特点。

不过，我们知道，仅仅以字节码的方式解释执行，不能满足计算密集型的需求。所以，Erlang 也正在努力提供编译成机器码运行的特性，这也需要编译器的支持。那你可以想象出，生成的机器码，一定也会跟运行时配合，来实现 Erlang 特有的并发机制和内存管理机制。


### 课程小结
今天这一讲，我们介绍了另一种并发模型：Actor 模型。Actor 模型的特点，是避免在并发的程序之间共享任何信息，从而程序就不需要使用锁机制来保证数据的一致性。但是，采用 Actor 机制也会因为数据拷贝导致更大的开销，并且你需要习惯异步的编程风格。Erlang 是实现 Actor 机制的典型代表。它被称为面向并发的编程语言，并且能够提供很高的可靠性。这都源于它善用了 Actor 的特点：由 Actor 构成的系统更像一个生命体一般的复杂系统。在实现 Actor 模型的时候，你要在运行时里实现独特的调度机制和内存管理机制，这些也需要编译器的支持。本讲的思维导图我也放在了下面，供你参考：

![img](https://static001.geekbang.org/resource/image/c0/5d/c04c32c93280afbea3fdc112285a085d.jpg?wh=2284*2623)

好了，今天这一讲加上第 33和34 讲，我们用了三讲，介绍了不同计算机语言是如何实现并发机制的。不难看出，并发机制确实是计算机语言设计中的一个重点。不同的并发机制，会非常深刻地影响计算机语言的运行时的实现，以及所采用的编译技术。



## 37 | 高级特性（二）：揭秘泛型编程的实现机制

对泛型的支持，是现代语言中的一个重要特性。它能有效地降低程序员编程的工作量，避免重复造轮子，写很多雷同的代码。像 C++、Java、Scala、Kotlin、Swift 和 Julia 这些语言都支持泛型。至于 Go 语言，它的开发团队也对泛型技术方案讨论了很久，并可能会在 2021 年的版本中正式支持泛型。可见，泛型真的是成为各种强类型语言的必备特性了。那么，泛型有哪些特点？在设计和实现上有哪些不同的方案？编译器应该进行什么样的配合呢？今天这一讲，我就带你一起探讨泛型的实现原理，借此加深你对编译原理相关知识点的认知，让你能够在自己的编程中更好地使用泛型技术。首先，我们来了解一下什么是泛型。


### 什么是泛型？
在日常编程中，我们经常会遇到一些代码逻辑，它们除了类型不同，其他逻辑是完全一样的。你可以看一下这段示例代码，里面有两个类，其中一个类是保存 Integer 的列表，另一个类是保存 Student 对象的列表。

```
public class IntegerList{
    List data = new ArrayList();
    public void add(Integer elem){
        data.add(elem);
    }
    public Integer get(int index){
        return (Integer) data.get(index);
    }
}

public class StudentList{
    List data = new ArrayList();
    public void add(Student elem){
        data.add(elem);
    }
    public Student get(int index){
        return (Student) data.get(index);
    }
}
```
我们都知道，程序员是很不喜欢重复的代码的。像上面这样的代码，如果要为每种类型都重新写一遍，简直会把人逼疯！泛型的典型用途是针对集合类型，能够更简单地保存各种类型的数据，比如 List、Map 这些。在 Java 语言里，如果用通用的集合类来保存特定类型的对象，就要做很多强制转换工作。而且，我们还要小心地做类型检查。比如：


```
List strList = new ArrayList();       //字符串列表
strList.add("Richard");
String name = (String)strList.get(i); //类型转换
for (Object obj in strList){
  String str = (String)obj;           //类型转换
  ...
}

strList.add(Integer.valueOf(1));      //类型错误
```
而 Java 里的泛型功能，就能完全消除这些麻烦工作，让程序更整洁，并且也可以减少出错机会。

```
List<String> strList = new ArrayList<String>(); //字符串列表
strList.add("Richard");
String name = strList.get(i);                   //类型转换
for (String str in strList){                    //无需类型转换
  ...
}

strList.add(Integer.valueOf(1));                //编译器报错
```
像示例程序里用到的List< String>，是在常规的类型后面加了一个参数，使得这个列表变成了专门存储字符串的列表。如果你再查看一下 List 和 ArrayList 的源代码，会发现它们比普通的接口和类的声明多了一个类型参数< E>，而这个参数可以用在接口和方法的内部所有需要类型的地方：变量的声明、方法的参数和返回值、类所实现的接口，等等。

```
public interface List<E> extends Collection<E>{
  E get(int index);
  boolean add(E e);
  ... 
}
```
所以说，泛型就是把类型作为参数，出现在类 / 接口 / 结构体、方法 / 函数和变量的声明中。由于类型是作为参数出现的，因此泛型也被称作参数化类型。参数化类型还可以用于更复杂的情况。比如，你可以使用 1 个以上的类型参数，像 Map 就可以使用两个类型参数，一个是 key 的类型（K），一个是 value 的类型（V）。


```
public interface Map<K,V> {
  ...
}
```
另外，你还可以对类型参数添加约束条件。比如，你可以要求类型参数必须是某个类型的子类，这是指定了上界（Upper Bound）；你还可以要求类型参数必须是某个类型的一个父类，这是指定了下界（Lower Bound）。实际上，从语言设计的角度来看，你可以对参数施加很多可能的约束条件，比如必须是几个类型之一，等等。**基于泛型的程序，由于传入的参数不同，程序会实现不同的功能。这也被叫做一种多态现象，叫做参数化多态（Parametric Polymorphism）。**它跟面向对象中的多态一样，都能让我们编写更加通用的程序。


好了，现在我们已经了解了泛型的含义了。那么，它们是如何在语言中实现的呢？需要用到什么编译技术？



### 泛型的实现
接下来，我们一起来看一看几种有代表性的语言实现泛型的技术，包括 Java、C#、C++ 等。


### 类型擦除技术
在 Java 里，泛型是通过类型擦除（Type Erasure）技术来实现的。前面在分析Java 编译器时，你就能发现，其实类型参数只存在于编译过程中，用于做类型检查和类型推断。在此之后，这些类型信息就可以被擦除。ArrayList 和ArrayList< String>对应的字节码是一样的，在运行时没有任何区别。所以，我们可以说，在 Java 语言里，泛型其实是一种语法糖，有助于减少程序员的编程负担，并能提供额外的类型检查功能。除了 Java 以外，其他基于 JVM 的语言，比如 Scala 和 Kotlin，其泛型机制基本上都是类型擦除技术。

类型擦除技术的优点是实现起来特别简单。运用我们学过的属性计算、类型检查和推断等相关技术基本就够用了。不过类型擦除技术也有一定的局限性。

问题之一，是它只能适用于引用类型，也就是对象，而不适用于值类型，也就是 Java 中的基础数据类型（Primitive Type）。比如，你不能声明一个List<int>，来保存单纯的整型数据，你在列表里只能保存对象化的 Integer。而我们学习过 Java 对象的内存模型，知道一个 Integer 对象所占的内存，是一个 int 型基础数据的好几倍，因为对象头要有十几个字节的固定开销。再加上由此引起的对象创建和垃圾收集的性能开销，导致用 Java 的集合对象来保存大量的整型、浮点型等基础数据是非常不划算的。我们在这种情况下，还是要退回到使用数组才行。

问题之二，就是因为类型信息在编译期被擦除了，所以程序无法在运行时使用这些类型信息。比如，在下面的示例代码中，如果你想要根据传入的类型 T 创建一个新实例，就会导致编译错误。

```
public static <T> void append(ArrayList<T> a) {
  T b= new T(); // 编译错误
  a.add(b); 
}
```
同样，由于在运行期没有类型信息，所以如果要用反射机制来调用程序的时候，我们也没有办法像在编译期那样进行类型检查。所以，你完全可以往一个旨在保存 String 的列表里添加一个 Interger 对象。而缺少类型检查，可能会导致程序在执行过程中出错。另外，还有一些由于类型擦除而引起的问题。比如，在使用参数化类型的情况下，方法的重载（Overload）会失败。再比如，下面的示例代码中，两个 foo 方法看似参数不同。但如果进行了类型擦除以后，它们就没什么区别，所以是不能共存的。

```
public void foo(List<Integer> p) { ... }
public void foo(List<Double> p)  { ... }
```
你要注意，不仅仅是 Java 语言的泛型技术有这样的缺点，其他基于 JVM 实现的语言也有类似的缺点（比如没有办法在运行时使用参数化类型的信息）。这其实是由于 JVM 的限制导致的。为了理解这个问题，我们可以看一下基于.NET 平台的语言 ，比如 C# 所采用的泛型技术。C# 使用的不是类型擦除技术，而是一种叫做具体化（reification）的技术。

### 具体化技术（Reification）
说起来，C# 语言的设计者，安德斯 · 海尔斯伯格（Anders Hejlsberg），是一位令人尊敬的传奇人物。像我这一代的程序员，差不多都使用过他在 DOS 操作系统上设计的 Pascal 编译器。后来他在此基础上，设计出了 Delphi，也是深受很多人喜爱的一款开发工具。出于对语言设计者的敬佩，虽然我自己从没用 C# 写过程序，但我从来没有低估过 C# 的技术。在泛型方面，C# 的技术方案成功地避免了 Java 泛型的那些缺点。C# 语言编译也会形成 IR，然后在.NET 平台上运行。在 C# 语言中，对应于 Java 字节码的 IR 被叫做 IL，是中间语言（Intermediate Language）的缩写。

我们知道了，在 Java 的泛型实现中，编译完毕以后类型信息就会被擦除。而在 C# 生成的 IL 中，则保留了类型参数的类型信息。所以，List< Student>和List< Teacher>是两个完全不同的类型。也因为 IL 保存了类型信息，因此我们可以在运行时使用这些类型信息，比如根据类型参数创建对象；而且如果通过反射机制来运行 C# 程序的话，也会进行类型检查。还有很重要的一点，就是 C# 的泛型能够支持值类型，比如基础的整型、浮点型数据；再比如，针对List< int>和List< long>，C# 的泛型能够真的生成一份完全不同的可运行的代码。它也不需要把值类型转换成对象，从而导致额外的内存开销和性能开销。

把参数化类型变成实际的类型的过程，是在运行时通过 JIT 技术实现的。这就是具体化（Reification）的含义。把一个参数化的类型，变成一个运行时真实存在的类型，它可以跟非参数化的类型起到完全相同的作用。不过，为了支持泛型，其实.NET 扩展了 C# 生成的 IL，以便在 IL 里能够记录参数化类型信息。而 JVM 则没有改变它的字节码，从而完全是靠编译器来处理泛型。好了，现在我们已经见识到了两种不同的泛型实现机制。还有一种泛型实现机制，也是经常被拿来比较的，这就是 C++ 的泛型机制，它的泛型机制依托的是模板元编程技术。


### 基于元编程技术来支持泛型
在上一讲，我们介绍过 C++ 的模板元编程技术。模板元编程很强大，程序里的很多要素都可以模板化，那么类型其实也可以被模板化。你已经知道，元编程技术是把程序本身作为处理对象的。采用 C++ 的模板元编程技术，我们实际上是为每一种类型参数都生成了新的程序，编译后生成的目标代码也是不同的。


所以，C++ 的模板技术也能做到 Java 的类型擦除技术所做不到的事情，比如提供对基础数据类型的支持。在 C++ 的标准模板库（STL）中，提供了很多容器类型。它们能像保存对象一样保存像整型、浮点型这样的基础数据类型。不过使用模板技术来实现泛型也有一些缺点。因为本质上，模板技术有点像宏，它是把程序中某些部分进行替换，来生成新的程序。在这个过程中，它并不会检查针对参数类型执行的某些操作是否是合法的。编译器只会针对生成后的程序做检查，并报错。这个时候，错误信息往往是比较模糊的，不太容易定位。这也是模板元编程技术固有的短板。究其原因，是模板技术不是单单为了泛型的目的而实现的。不过，如果了解了泛型机制的原理，你会发现，其实可以通过增强 C++ 编译器，来提升它的类型检查能力。甚至，对类型参数指定上界和下界等约束条件，也是可以的。不过这要看 C++ 标准委员会的决定了。总的来说，C++ 的泛型技术像 Java 的一样，都是在运行期之前就完成了所有的工作，而不像.NET 那样，在运行期针对某个参数化的类型产生具体的本地代码。好了，了解了泛型的几种实现策略以后，接下来，我们接着讨论一个更深入的话题：把类型参数化以后，对于计算机语言的类型系统有什么挑战？这个问题很重要，因为在语义分析阶段，我们已经知道如何做普通类型的分析和处理。而要处理参数化的类型，我们还必须更加清楚支持参数化以后，类型体系会有什么变化。


### 泛型对类型系统的增强
在现代语言中，通常会建立一个层次化的类型系统，其中一些类型是另一些类型的子类型。什么是子类型呢？就是在任何一个用到父类型的地方，都可以用其子类型进行替换。比如，Cat 是 Animal 的子类型，在任何用到 Animal 的地方，都可以用 Cat 来代替。不过，当类型可以带有参数之后，类型之间的关系就变得复杂了。比如说：

Collection< Cat>和List< Cat>是什么关系呢？
List< Animal>和List< Cat>之间又是什么关系呢？
```
对于第一种情况，其实它们的类型参数是一样的，都是 Cat。而 List 本来是 Collection 的子类型，那么List<Cat>也是Collection<Cat>的子类型，我们永远可以用List<Cat>来替换Collection<Cat>。这种情况比较简单。

但是对于第二种情况，List<Cat>是否是List<Animal>的子类型呢？这个问题就比较难了。不同语言的实现是不一样的。在 Java、Julia 等语言中，List<Cat>和List<Animal>之间没有任何的关系。
```
在由多个类型复合而形成的类型中（比如泛型），复合类型之间的关系随其中的成员类型的关系而变化的方式，分为不变（Invariance）、协变（Covariance）和逆变（Contravariance）三种情况。理解清楚这三种变化，对于我们理解引入泛型后的类型体系非常重要，这也是编译器进行正确的类型计算的基础。首先说说不变。在 Java 语言中，List<Animal>和List<Cat>之间并没有什么关系，在下面的示例代码中，如果我们把List<Cat>赋值给List<Animal>，编译器会报错。因此，我们说List<T>基于 T 是不变的。

```
List<Cat> catList = new ArrayList<>();
List<Animal> animalList = catList;  //报错，不是子类型
```
那么协变是什么呢？就是复合类型的变化方向，跟成员类型是相同的。我给你举两个在 Java 语言中关于协变的例子。第一个例子。假设 Animal 有个 reproduce() 方法，也就是繁殖。而 Cat 覆盖（Override）了这个方法，但这个方法的返回值是 Cat 而不是 Animal。因为猫肯定繁殖出的是小猫，而不是其他动物。这样，当我们调用 Cat.reproduce() 方法的时候，就不用对其返回值做强制转换。这个时候，我们说 reproduce() 方法的返回值与它所在类的类型，是协变的，也就是一起变化。

```
class Animal{
    public abstract Animal reproduce();
}

class Cat extends Animal{
    @Override
    public Cat reproduce() {  //方法的返回值可以是Animal的子类型
        ...
    }
}
```
第二个例子。在 Java 语言中，数组是协变的。也就是Cat[]其实是Animal[]的子类型，在下面的示例代码中，一个猫的数组可以赋值给一个动物数组。

```
Cat[] cats = {new Cat(), new Cat()}; //创建Cat数组
Animal[] animals = cats;             //赋值给Animal数组
animals[0] = new Dog();              //修改第一个元素的值
Cat aCat = cats[0];                  //运行时错误
```
但你接下来会看到，Animal 数组中的值可以被修改为 Dog，这会导致 Cat 数组中的元素类型错误。至于为什么 Java 语言要把数组设计为协变的，以及由此导致的一些问题，我们暂且不管它。我们要问的是，List<T>这样的泛型可以变成协变关系吗？答案是可以的。我前面也提过，我们可以在类型参数中指定上界。List<Cat>是List<? Extends Animal>的子类型，List<? Extends Animal>的意思，是任何以 Animal 为祖先的子类。我们可以把一个List<Cat>赋值给List<? Extends Animal>。你可以看一下示例代码：

```
List<Cat> catList = new ArrayList<>();
List<? extends Animal> animalList = catList;  //子类型
catList.add(new Cat());
Animal animal = animalList.get(0);
```
实际上，不仅仅List<Cat>是List<? extends Animal>的子类型，连List<Animal>也是List<? extends Animal>的子类型。你可以自己测试一下。

我们再来说说逆变。逆变的意思是：虽然 Cat 是 Animal 的子类型，但包含了 Cat 的复合类型，竟然是包含了 Animal 的复合类型的父类型！它们颠倒过来了？这有点违反直觉。在真实世界里有这样的例子吗？当然有。比如，假设有两个函数，getWeight<Cat>()函数是返回 Cat 的重量，getWeight<Animal>()函数是返回 Animal 的重量。你知道，从函数式编程的观点，每个函数也都是有类型的。那么这两个函数，谁是谁的子类型呢？实际上，求 Animal 重量的函数，其实是求 Cat 重量的函数的子类型。怎么说呢？来假设一下。如果你想用一个 getTotalWeight() 函数，求一群 Cat 的总重量，你会怎么办呢？你可以把求 Cat 重量的函数作为参数传进去，这肯定没问题。但是，你也可以把求 Animal 重量的函数传进去。因为既然它能返回普通动物的重量，那么也一定能返回猫的重量。

```
//伪代码，求Cat的总重量
getTotalWeight(List<Cat> cats, function fun)
```
而根据类型理论，如果类型 B 能顶替类型 A 的位置，那么 B 就是 A 的子类型。所以，getWeigh<Animal>()反倒是getWeight<Cat>()的子类型，这种情况就叫做逆变。总的来说，加入泛型以后，计算机语言的类型体系变得更加复杂了。我们在编写编译器的时候，一定要弄清楚这些变化关系，这样才能执行正确的类型计算。那么，在了解了加入泛型以后对类型体系的影响后，我们接着借助 Julia 语言，来进一步验证一下如何进行正确的类型计算。


### Julia 中的泛型和类型计算
Julia 设计了一个精巧的类型体系。这个类型体系有着共同的根，也就是 Any。在这个类型层次中，橙色的类型是叶子节点，它们是具体的类型，也就是可以创建具体的实例。而中间层次的节点（蓝色），都是抽象的，主要是用于类型的计算。

![img](https://static001.geekbang.org/resource/image/4b/38/4b88f681a305ecf5010606e3b3a68c38.jpg?wh=2850*2027)



Julia的类型体系你在第 22 讲中，已经了解到了 Julia 做函数编译的特点。在编写函数的时候，你可以根本不用指定参数的类型，编译器会根据传入的参数的实际类型，来编译成相应版本的机器码。另外，你也可以为函数编写多个版本的方法，每个版本的参数采用不同的类型。编译器会根据实际参数的类型，动态分派到不同的版本。而这个动态分派机制，就需要用到类型的计算。比如说，有一个函数 foo()，定义了三个版本的方法，其参数分别是没有指定类型（也就是 Any）、Real 类型和 Float64 类型。如果参数是 Float64 类型，那它当然会被分派到第三个方法。如果是 Float32 类型，那么就会被分派到第二个方法。如果是一个字符串类型呢，则会被分派到第一个方法。

```
julia> function foo(x)          #方法1
           ...
       end
       
julia> function foo(x::Real)    #方法2  
           ...
       end
       
julia> function foo(x::Float64) #方法3  
           ...
       end
```

再进一步，Julia 还支持在定义结构体和函数的时候使用泛型。比如，下面的一个 Point 结构中，坐标 x 和 y 的类型是参数化的。

```
julia> struct Point{T}
           x::T
           y::T
       end


julia> Point{Float64}
Point{Float64}

julia> Point{Float64} <: Point     #在Julia里，如果一个类型更具体，则<:为真
true

julia> Point{Float64} <: Point{Real} #Invariant
false

julia> p1 = Point(1.0,2.3)   #创建一个Point实例
Point{Float64}(1.0, 2.3)     #自动推断出类型
```

如果我们再为 foo() 函数添加几个方法，其参数类型分别是 Point 类型、Point{Real}类型和 Point{Float64}类型，那动态分派的算法也必须能够做正确的分派。所以，在这里，我们就必须能够识别出带有参数的类型之间的关系。

```
julia> function foo(x::Point)          #方法4
           ...
       end
       
julia> function foo(x::Point{Real})    #方法5  
           ...
       end
       
julia> function foo(x::Point{Float64}) #方法6  
           ...
       end
```

通过以上的示例代码你可以看到，Point{Float64} <: Point，也就是 Point{Float64}是 Point 的子类型。这个关系是有意义的。Julia 的逻辑是，Point{Float64} 比 Point 更具体，能够在程序里替代 Point。而 Point{Float64} 和 Point{Real}之间是没有什么关系的，虽然 Float64 是 Real 的子类型。这说明，Point{T}基于 T 是不变的（Invariant），这跟 Java 语言的泛型处理是一样的。所以，在 Julia 编译的时候，如果我们给 foo() 传递一个 Point{Float64}参数，那么应该被分派到方法 6。而如果传递一个 Point{Float32}参数呢？分派算法不会选择方法 5，因为 Point{Float32}不是 Point{Real}的子类型。因此，分配算法会选择方法 4，因为 Point{Float32}是 Point 的子类型。那么，如何让 Point{T}基于 T 协变呢？这样我们就可以针对 Real 类型写一些通用的算法，让采用 Float32、Float16 等类型的 Point，都按照这个算法去编译了。

答案就是需要指定上界。我们可以把 Point{Real}改为 Point{<:Real}，它是 Point{Float32}、Point{Float16}等的父类型。好，总结起来，Julia 的泛型和类型计算是很有特点的。泛型提供的参数化多态（Parametric Polymorphism）跟 Julia 原来的方法多态（Method Polymorphism）很好地融合在了一起，让我们能够最大程度地去编写通用的程序。而被泛型增强后的类型体系，也对动态分派算法提出了更高的要求。



### 课程小结
这一讲，我们学习了泛型这个现代语言中非常重要的特性的实现机制。在实现泛型机制的时候，我们首先必须弄清楚引入泛型以后，对类型体系的影响。你要掌握不变、协变和逆变这三个基本概念和它们的应用场景，从而能够正确地用于类型计算的过程中。在泛型的具体实现机制上，有类型擦除、具体化和模板元编程等不同的方法。好的实现机制应该有能力同时兼顾值类型和复合类型，同时又便于调试。按照惯例，我也把本讲的内容总结成了思维导图，供你参考：

![img](https://static001.geekbang.org/resource/image/c7/d7/c7bf4642ebd4a0253b9ec3b174ef71d7.jpg?wh=2284*3134)



