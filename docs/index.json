[{"categories":["Advanced learning"],"content":"趣谈Linux操作系统 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:0:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"入门准备 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:1:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"开篇词 | 为什么要学习Linux操作系统？ 刘超 2019-03-25 我们大学里上过操作系统的课，而且每天都在用操作系统，为什么还要专门学一遍呢？尽管我的操作系统课成绩不错，但是在大学的时候，我和你的看法一样，我觉得这门课没有什么用，现在回想起来可能有这样几个原因。 第一，大学里普遍使用的操作系统是 Windows，老师大多也用 Windows。Windows 的优势是界面友好，很容易上手，于是我们就养成了要配置东西了就去菜单找，用鼠标点点的习惯，似乎会攒电脑、装系统、配软件就能搞定一切问题。第二，一种操作系统对应的是一系列的软件生态，而大学里很多课程都是围绕 Windows 软件生态展开的。例如学 C++ 用的是 Vistual Studio，学数据库用的是 SQL Server，做网站用的是 IIS 等等。第三，大学里的操作系统课往往都是纯讲理论，讲了很多原理，但是压根儿没法和平时用的 Windows 系统的行为关联起来，也根本弄不清操作系统在底层到底是怎么做的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:2:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"打开 Linux 操作系统这扇门，你才是合格的软件工程师 根据 2018 年 W3Techs 的数据统计，对于服务器端，Unix-Like OS 占的比例近 70%，其中 Linux 可以称得上是中流砥柱。随着移动互联网的发展，客户端基本上以 Android 和 iOS 为主。Android 是基于 Linux 内核的，因而客户端也进入了 Linux 阵营。可以说，在编程世界中，Linux 就是主流，不会 Linux 你就会格格不入。 那些火得不行的技术，什么云计算、虚拟化、容器、大数据、人工智能，几乎都是基于 Linux 技术的。那些牛得不行的系统，团购、电商、打车、快递，都是部署在服务端，也几乎都是基于 Linux 技术的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:2:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"研究 Linux 内核代码，你能学到数据结构与设计模式的落地实践 Linux 最大的优点就是开源。作为程序员，有了代码，啥都好办了。只要有足够的耐心，我们就可以一层一层看下去，看内核调度函数，看内存分配过程。理论理解起来不容易，但是一行行的“if-else”却不会产生歧义。 在 Linux 内核里，你会看到数据结构和算法的经典使用案例；你甚至还会看到并发情况下的保护这种复杂场景；在实践中遇到问题的时候，你可以直接参考内核中的实现。 例如，平时看起来最简单的文件操作，通过阅读 Linux 代码，你能学到从应用层、系统调用层、进程文件操作抽象层、虚拟文件系统层、具体文件系统层、缓存层、设备 I/O 层的完美分层机制，尤其是虚拟文件系统对于接入多种类型文件系统的抽象设计，在很多复杂的系统里面，这个思想都能用得上。再如，当你写代码的时候，大部分情况下都可以使用现成的数据结构和算法库，但是有些场景对于内存的使用需要限制到很小，对于搜索的时间需要限制到很小的时候，我们需要定制化一些数据结构，这个时候内核里面这些实现就很有参考意义了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:2:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"了解 Linux 操作系统生态，能让你事半功倍地学会新技术 数据库 MySQL、PostgreSQL，消息队列 RabbitMQ、Kafka，大数据 Hadoop、Spark，虚拟化 KVM、Openvswitch，容器 Kubernetes、Docker，这些软件都会默认提供 Linux 下的安装、使用、运维手册，都会默认先适配 Linux。 操作系统是干什么的呢？我们都知道，一台物理机上有很多硬件，最重要的就是 CPU、内存、硬盘、网络。同时，一台物理机上也要跑很多程序，这些资源应该给谁用呢？当然是大家轮着用，谁也别独占，谁也别饿着。为了完成资源分配这件事，操作系统承担了一个“大管家”的作用。它将硬件资源分配给不同的用户程序使用，并且在适当的时间将这些资源拿回来，再分配给其他的用户进程。 假设，我们现在就是在做一家外包公司，我们的目标是把这家公司做上市。其中，操作系统就是这家外包公司的老板。我们把这家公司的发展阶段分为这样几个时期： 初创期：这个老板基于开放的营商环境（x86 体系结构），创办一家外包公司（系统的启动）。因为一开始没有其他员工，老板需要亲自接项目（实模式）。 发展期：公司慢慢做大，项目越接越多（保护模式、多进程），为了管理各个外包项目，建立了项目管理体系（进程管理）、会议室管理体系（内存管理）、文档资料管理系统（文件系统）、售前售后体系（输入输出设备管理）。 壮大期：公司越来越牛，开始促进内部项目的合作（进程间通信）和外部公司合作（网络通信）。 集团化：公司的业务越来越多，会成立多家子公司（虚拟化），或者鼓励内部创业（容器化），这个时候公司就变成了集团。大管家的调度能力不再局限于一家公司，而是集团公司（Linux 集群），从而成功上市（从单机操作系统到数据中心操作系统）。 第二个原则就是图解。Linux 操作系统中的概念非常多，数据结构也很多，流程也复杂，一般人在学习的过程中很容易迷路。所谓“一图胜千言”，我希望能够通过图的方式，将这些复杂的概念、数据结构、流程表现出来，争取用一张图串起一篇文章的知识点。最终，整个专栏下来，你如果能把这些图都掌握了，你的知识就会形成体系和连接。在此基础上再进行深入学习，就会如鱼得水、易如反掌。 例如，这张图就表示了文件操作在各个层的数据结构的关联。只要你学完之后，能对着这张图将它们之间的关系讲清楚，对于文件系统的部分，你就会了然于心了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:2:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"02 | 学习路径：爬过这六个陡坡，你就能对Linux了如指掌 Windows 的基本使用模式是“图形化界面 + 菜单”。也就是说，无论我们做什么事情，首先要找一个图形化的界面。在这里面，“开始”菜单是统一的入口，无论是运行程序，还是做系统设置，你都能找到一个界面，界面上会有各种各样的输入框和菜单。我们只要挨个儿看过去，总能找到想操作的功能。实在不行，还有杀手锏，就是右键菜单，挨个儿一项一项看下去，最终也能实现想做的操作。 如果你刚刚上手 Linux，就会发现，情况完全不一样。你基本是这也找不着，那也找不着，觉得 Linux 十分难用，从而“从入门到放弃”。Linux 上手难，学习曲线陡峭，所以它的学习过程更像一个爬坡模式。这些坡看起来都很陡，但是一旦爬上一阶，就会一马平川。你会惊叹 Linux 的设计之美，而 Linux 的灵活性也会使得你有 N 多种方法解决问题，从而事半功倍，你就会有一切尽在掌握的感觉。只可惜，大部分同学都停留在了山脚下。那怎样才能掌握这项爬坡技能呢？我们首先需要明确，我们要爬哪些坡。 我总结了一下，在整个 Linux 的学习过程中，要爬的坡有六个，分别是：熟练使用 Linux 命令行、使用 Linux 进行程序设计、了解 Linux 内核机制、阅读 Linux 内核代码、实验定制 Linux 组件，以及最后落到生产实践上。以下是我为你准备的爬坡秘籍以及辅助的书单弹药。 第一个坡：抛弃旧的思维习惯，熟练使用 Linux 命令行 上手 Linux 的第一步，要先从 Windows 的思维习惯，切换成 Linux 的“命令行 + 文件”使用模式。 在 Linux 中，无论我们做什么事情，都会有相应的命令工具。虽然这些命令一般会在 bin 或者 sbin 目录下面，但是这些命令的数量太多了。如果你事先不知道该用哪个命令，很难通过枚举的方式找到。因此，在这样没有统一入口的情况下，就需要你对最基本的命令有所掌握。一旦找到某个命令行工具，替代输入框的是各种各样的启动参数。这些参数怎么填，一般可以通过 -h 查看 help，挨个儿看过去，就能找到相应的配置项；还可以通过 man 命令，查看文档。无论是什么命令行工具，最终的配置一般会落到一个文件上，只要找到了那个文件，文件中会有注释，也可以挨个儿看下去，基本就知道如何配置了。这个过程可能非常痛苦，在没有足够熟练地掌握命令行之前，你会发现干个非常小的事情都需要搜索半天，读很多文档，即便如此还不一定能得到期望的结果。这个时候你一定不要气馁，坚持下去，继续看文档、查资料，慢慢你就会发现，大部分命令的行为模式都很像，你几乎不需要搜索就能完成大部分操作了。恭喜你，这个时候你已经爬上第一个坡了。这个时候，你能看到一些很美丽的风景，例如一些很有技巧的命令 sed 和 awk、很神奇的正则表达式、灵活的管道和 grep、强大的 bash。你可以自动化地做一些事情了，例如处理一些数据，会比你使用 Excel 要又快又准，关键是不用框框点点，在后台就能完成一系列操作。在处理数据的同时，你还可以干别的事情，半夜处理数据，第二天早上发个邮件报告，这都是 Excel 很难做到的事情。不过，在这个专栏里，命令行并不是我们的重点，但是考虑到一些刚起步的同学，在第一部分我会简单介绍一些能够让你快速上手 Linux 的命令行。专栏每一模块的第一节，我都会有针对性地讲解这一模块的常用命令，足够你把 Linux 用起来。 如果你想全面学习 Linux 命令，推荐你阅读《鸟哥的 Linux 私房菜》。如果想再深入一点，推荐你阅读《Linux 系统管理技术手册》。这本砖头厚的书，可以说是 Linux 运维手边必备。 第二个坡：通过系统调用或者 glibc，学会自己进行程序设计 命令行工具也是程序，只不过是别人写的程序。从用别人写的程序，到自己能够写程序，通过程序来操作 Linux，这是第二个要爬的坡。 用代码操作 Linux，可以直接使用 Linux 系统调用，也可以使用 glibc 的库。Linux 的系统调用非常多，而且每个函数都非常复杂，传入的参数、返回值、调用的方式等等都有很多讲究。这里面需要掌握很多 Linux 操作系统的原理，否则你会无法理解为什么应该这样调用。刚开始学 Linux 程序设计的时候，你会发现它比命令行复杂得多。因为你的角色再次变化，这是为啥呢？我这么说，估计你就能理解了。 如果说使用命令行的人是吃馒头的，那写代码操作命令行的人就是做馒头的。看着简简单单的一个馒头，可能要经过 N 个工序才能蒸出来。同样，你会发现，你平时用的一个简单的命令行，却需要 N 个系统调用组合才能完成。其中每个系统调用都要进行深入地学习、读文档、做实验。 经过一段时间的学习，你啃下了这些东西，恭喜你，又爬上了一个坡。这时候，你已经很接近操作系统的原理了，你能看到另一番风景了。大学里学的那些理论，你再回去看，现在就会开始有感觉了。你本来不理解进程树，调用了 fork，就明白了；你本来不理解进程同步机制，调用了信号量，也明白了；你本来分不清楚网络应用层和传输层的分界线，调用了 socket，都明白了。同样，专栏的第一模块，我会简单介绍一下 Linux 有哪些系统调用，每一模块的第一节，我还会讲解这一模块的常用系统调用，以及如何编程调用这些系统调用。这样可以使你对 Linux 程序设计入个门，但是这对于实战肯定是远远不够的。如果要进一步学习 Linux 程序设计，推荐你阅读《UNIX 环境高级编程》，这本书有代码，有介绍，有原理，非常实用。 第三个坡：了解 Linux 内核机制，反复研习重点突破 当你已经会使用代码操作 Linux 的时候，你已经很希望揭开这层面纱，看看系统调用背后到底做了什么。 这个时候，你的角色要再次面临变化，就像你蒸馒头时间长了，发现要蒸出更好吃的馒头，就必须要对面粉有所研究。怎么研究呢？当然你可以去面粉厂看人家的加工过程，但是面粉厂的流水线也很复杂，很多和你蒸馒头没有直接关系，直接去看容易蒙圈，所以这时候你最好先研究一下，面粉制造工艺与馒头口味的关系。对于 Linux 也是一样的，进一步了解内核的原理，有助于你更好地使用命令行和进行程序设计，能让你的运维和开发水平上升一个层次，但是我不建议你直接看代码，因为 Linux 代码量太大，很容易迷失，找不到头绪。最好的办法是，先了解一下 Linux 内核机制，知道基本的原理和流程就可以了。一旦学起来的时候，你会发现，Linux 内核机制也非常复杂，而且其中相互关联。比如说，进程运行要分配内存，内存映射涉及文件的关联，文件的读写需要经过块设备，从文件中加载代码才能运行起来进程。这些知识点要反复对照，才能理清。 但是一旦爬上这个坡，你会发现 Linux 这个复杂的系统开始透明起来。无论你是运维，还是开发，你都能大概知道背后发生的事情，并在出现异常的情况时，比较准确地定位到问题所在。Linux 内核机制是我们这个专栏重点要讲述的部分，我会基于最新 4.x 的内核进行讲解，当然我也意识到了内核机制的复杂性，所以我选择通过故事性和图形化的方式，帮助你了解并记住这些机制。这块内容的辅助学习，我推荐一本《深入理解 LINUX 内核》。这本书言简意赅地讲述了主要的内核机制。看完这本书，你会对 Linux 内核有总体的了解。不过这本书的内核版本有点老，不过对于了解原理来讲，没有任何问题。 第四个坡：阅读 Linux 内核代码，聚焦核心逻辑和场景 在了解内核机制的时候，你肯定会遇到困惑的地方，因为理论的描述和提炼虽然能够让你更容易看清全貌，但是容易让你忽略细节。我在看内核原理的书的时候也遇到过这种问题，有的地方实在是难以理解，或者不同的书说的不一样，这时候该怎么办呢？其实很好办，Linux 是开源的呀，我们可以看代码呀，代码是精准的。哪里有问题，找到那段代码看一看，很多问题就有方法了。另外，当你在工作中需要重点研究某方面技术的时候，如果涉及内核，这个时候仅仅了解原理已经不够了，你需要看这部分的代码。 但是开源软件代码纷繁复杂，一开始看肯定晕，找不着北。这里有一个诀窍，就是**一开始阅读代码不要纠结一城一池的得失，不要每一行都一定要搞清楚它是干嘛的，而要聚焦于核心逻辑和使用场景。**一旦爬上这个坡，对于操作系统的原理，你应该就掌握得比较清楚了。就像蒸馒头的人已经将面粉加工流程烂熟于心。这个时候，你就可以有针对性地去做课题，把所学和你现在做的东西结合起来重点突破。例如你是研究虚拟化的，就重点看 KVM 的部分；如果你是研究网络的，就重点看内核协议栈的部分。在专栏里，我在讲述 Linux 原理的同时，也会根据场景和主要流程来分析部分代码，例如创建进程、分配内存、打开文件、读写文件、收发网络包等等。考虑到大量代码粘贴会让你看起来比较费劲，也会占用大量篇幅，所以我采取只叙述主要流程，只放必要的代码，大部分的逻辑和相互关系，尽量通过图的方式展现出来，给你讲解。这里也推荐一本书，《LINUX 内核源代码情景分析》。这本书最大的优点是结合场景进行分析，看得见、摸得着，非常直观，唯一的缺点还是内核版本比较老。 第五个坡：实验定制化 Linux 组件，已经没人能阻挡你成为内核开发工程师了 纸上得来终觉浅，绝知此事要躬行。从只看内核代码，到上手修改内核代码，这又是一个很大的坎。这相当于蒸馒头的人为了定制口味，要开始修改面粉生产流程了。因为 Linux 有源代码，很多地方可以参考现有的实现，定制化自己的模块。例如，你可以自己实现一个设备驱动程序，实现一个自己的系统调用，或者实现一个自己的文件系统等等。 这个难度比较","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:3:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 今天，我把爬坡的过程，分解成了六个阶段，并给你分享了我的私家爬坡宝典。你都记住了吗？我把今天的内容总结成了下面这张图。建议你牢牢记住这张图，在接下来的四个月中，按照这个路径稳步前进，攻克 Linux 操作系统。 Linux 操作系统爬坡路线图 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:3:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"核心原理 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:4:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"03 | 你可以把Linux内核当成一家软件外包公司的老板 那操作系统到底在背后默默地做了哪些事情，才能让我们轻松地使用这些电子设备呢？要想回答这个问题，我们需要把眼光放回到自己攒电脑的那个时代。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:5:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"电脑组装好就能直接用吗？ 那时候买电脑，经常是这样一个情景：三五个哥们儿一起来到电脑城，呼啦呼啦采购了一大堆硬件，有密密麻麻都是针脚的 CPU；有铺满各种复杂电路的一块板子，也就是主板；还需要买块显卡，用来连接显示器；还需要买个网卡，里面可以插网线；还要买块硬盘，将来用来存放文件；然后还需要一大堆线，将这些设备和主板连接起来；最终再来一个鼠标，一个键盘，还有一个显示器。设备差不多啦，准备开整！ 好不容易组装完这一大堆硬件，还是不能直接用，你还需要安装一个操作系统。安装操作系统也是一件非常复杂的事，一点儿也不亚于把刚才那堆东西组装起来。这个安装过程可能会涉及十几个步骤、几十项配置。每一步骤配置完了，点击下一步，会出现个进度条。伴随着一堆难以理解的描述，最终安装步骤到达百分之百，才出现你熟悉的那个界面。我这么说起来好像很容易，但是要把这事儿讲清楚估计得用一个专栏。这个复杂程度，咱们父母估计是上不了手了。所以，那个时候，能把这套东西都组装起来，是一件很拉风的事情。很多 IT 男甚至因为这项绝技“泡”到了妹子。当操作系统安装完毕的时候，我妈通常会要求我一定要装一个 QQ。看到妈妈在你装好的操作系统前愉快地和她的朋友聊天，这时候，经历过以上过程的你，多少应该能感受到操作系统的厉害了。 操作系统究竟是如何把这么多套复杂的东西管理起来，从而弄出来一个简单到父母都会用的东西呢？ 很多事情就怕细想。不知道你有没有产生过这些疑问： 桌面上的图标到底是啥？凭啥我在鼠标上一双击，就会出来一个美丽的画面？这都是从哪里跑出来的？凭什么我在键盘上噼里啪啦地敲，某个位置就会显示我想要的那些字符？电脑怎么知道我鼠标点击的是这个地方，又是怎么知道我要输入的是这个地方？我在键盘上点“a”，是谁在显示器上画出“a”这个图像呢？为什么我一回车，这些字符就发到遥远的另外一台机器上去了？ 对于普通用户来讲，其实只要会用就行了，但是咱们作为专业人士，要深入探究一下背后的答案。你别小看“双击鼠标打开聊天软件”这样一个简单的操作，它几乎涵盖了操作系统的所有功能。我们就从这个熟悉的操作，来认识陌生的操作系统。 操作系统其实就像一个软件外包公司，其内核就相当于这家外包公司的老板。所以接下来的整个课程中，请你将自己的角色切换成这家软件外包公司的老板，设身处地地去理解操作系统是如何协调各种资源，帮客户做成事情的。 想要学好咱们这门课，你要牢牢记住这段话，把这个概念牢牢扎根在心里，我之后的讲解都会基于此，帮你理解、记忆那些难搞的概念和原理。同时，为了防止你混淆，我这里先强调一下。今后我所说的“用户”，都是指操作系统的用户，“客户”则是指外包公司的客户，这两者是对应的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:5:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"“双击 QQ”这个过程，都需要用到哪些硬件？ 好，现在用户开始对着屏幕上的 QQ 图标双击鼠标了。 鼠标和键盘是计算机的输入设备。大部分的普通用户想要告诉计算机应该做什么，都是通过这两个设备。例如，用户移动了一下鼠标，鼠标就会通过鼠标线给电脑发消息，告知电脑，鼠标向某个方向移动了多少距离。如果是一家外包公司，怎么才能知道客户的需求呢？你需要配备销售、售前等角色，专门负责和客户对接，把客户需求拿回来，我们把这些人统称为客户对接员。你可以跟客户说，有什么事儿都找对接员。屏幕，也就是显示器，是计算机的输出设备，将计算机处理用户请求后的结果展现给客户，要不然用户无法知道自己的请求是不是到达并且执行了。显示器上面显示的东西是由显卡控制的。无论是显示器还是显卡，这里都有个“坐标”的概念，也就是说，什么图像在哪个坐标，都是定义好了才画上去的。本来在某个坐标画了一个鼠标箭头，当接到鼠标移动的事件之后，你应该按相同的方向，按照一定的比例（鼠标灵敏度），在屏幕的某个坐标再画一个鼠标箭头。 作为外包公司，当客户给你提了需求，不管你做还是不做，最终做成什么样，你都需要给客户反馈，所以你要配备交付人员，将做好的需求展示给他们看。在操作系统中，输入设备驱动其实就是客户对接员。有时候新插上一个鼠标的时候，会弹出一个通知你安装驱动，这就是操作系统这家外包公司给你配备对接人员呢。当客户告诉对接员需求的时候，对于操作系统来讲，输入设备会发送一个中断。这个概念很好理解。客户肯定希望外包公司把正在做的事情都停下来服务它。所以，这个时候客户发送的需求就被称为中断事件（Interrupt Event）。显卡会有显卡驱动，在操作系统中称为输出设备驱动，也就是上面说的交付人员。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:5:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"从点击 QQ 图标，看操作系统全貌 有了客户对接员和交付人员，外包公司就可以处理用户“在桌面上点击 QQ 图标”的事件了。 首先，鼠标双击会触发一个中断，这相当于客户告知客户对接员“有了新需求，需要处理一下”。你会事先把处理这种问题的方法教给客户对接员。在操作系统里面就是调用中断处理函数。操作系统发现双击的是一个图标，就明白了用户的原始诉求，准备运行 QQ 和别人聊天。你会发现，运行 QQ 是一件大事，因为将来的一段时间，用户要一直和 QQ 进行交互。这就相当于你们公司接了一个大单，而不是处理零星的客户需求，这个时候应该单独立项。一旦立了项，以后与这个项目有关的事情，都由这个项目组来处理。立项可不能随便立，一定要有一个项目执行计划书，说明这个项目打算怎么做，一步一步如何执行，遇到什么情况应该怎么办等等。换句话说，对 QQ 这个程序来说，它能做哪些事情，每件事情怎么做，先做啥后做啥，都已经作为程序逻辑写在程序里面，并且编译成为二进制了。这个程序就相当于项目执行计划书。电脑上的程序有很多，什么有道云笔记的程序、Word 程序等等，它们都以二进制文件的形式保存在硬盘上。硬盘是个物理设备，要按照规定格式化成为文件系统，才能存放这些程序。文件系统需要一个系统进行统一管理，称为文件管理子系统（File Management Subsystem）。 对于你们公司，项目立得多了，项目执行计划书也会很多，同样需要有个统一保存文件的档案库，而且需要有序地管理起来。当你从资料库里面拿到这个项目执行计划书，接下来就需要开始执行这个项目了。项目执行计划书是静态的，项目的执行是动态的。同理，当操作系统拿到 QQ 的二进制执行文件的时候，就可以运行这个文件了。QQ 的二进制文件是静态的，称为程序（Program），而运行起来的 QQ，是不断进行的，称为进程（Process）。 说了这么多，怎样才能立项呢？你会发现，一个项目要想顺畅进行，需要用到公司的各种资源，比如说盖个公章、开个证明、申请个会议室、打印个材料等等。这里有个两难的权衡，一方面，资源毕竟是有限的，甚至是涉及机密的，不能由项目组滥取滥用；另一方面，就是效率，咱是一个私营企业，保证项目申请资源的时候只跑一次，这样才能比较高效。为了平衡这一点，一方面涉及核心权限的资源，还是应该被公司严格把控，审批了才能用；另外一方面，为了提高效率，最好有个统一的办事大厅，明文列出提供哪些服务，谁需要可以来申请，然后就会有回应。在操作系统中，也有同样的问题，例如多个进程都要往打印机上打印文件，如果随便乱打印进程，就会出现同样一张纸，第一行是 A 进程输出的文字，第二行是 B 进程输出的文字，全乱套了。所以，打印机的直接操作是放在操作系统内核里面的，进程不能随便操作。但是操作系统也提供一个办事大厅，也就是系统调用（System Call）。 系统调用也能列出来提供哪些接口可以调用，进程有需要的时候就可以去调用。这其中，立项是办事大厅提供的关键服务之一。同样，任何一个程序要想运行起来，就需要调用系统调用，创建进程。一旦项目正式立项，就要开始执行，就要成立项目组，将开发人员分配到这个项目组，按照项目执行计划书一步一步执行。为了管理这个项目，我们还需要一个项目经理、一套项目管理流程、一个项目管理系统，例如程序员比较熟悉的 Jira。如果项目多，可能一个开发人员需要同时执行多个项目，这就要考验项目经理的调度能力了。在操作系统中，进程的执行也需要分配 CPU 进行执行，也就是按照程序里面的二进制代码一行一行地执行。于是，为了管理进程，我们还需要一个进程管理子系统（Process Management Subsystem）。如果运行的进程很多，则一个 CPU 会并发运行多个进程，也就需要 CPU 的调度能力了。 每个项目都有自己的私密资料，这些资料不能被其他项目组看到。这些资料主要是项目在执行的过程中，产生的很多中间成果，例如架构图、流程图。执行过程中，难免要在白板上或者本子上写写画画，如果不同项目的办公空间不隔离，一方面，项目的私密性不能得到保证，A 项目的细节，B 项目也能看到；另一方面，项目之间会相互干扰，A 项目组的人刚在白板上画了一个架构图，出去上个厕所，结果 B 项目组的人就给擦了。如果把不同的项目组分配到不同的会议室，就解决了这个问题。当然会议室是有限的，需要有人管理和分配，并且需要一个会议室管理系统。 在操作系统中，不同的进程有不同的内存空间，但是整个电脑内存就这么点儿，所以需要统一的管理和分配，这就需要内存管理子系统（Memory Management Subsystem）。 如果想直观地了解 QQ 如何使用 CPU 和内存，可以打开任务管理器，你就能看到 QQ 这个进程耗费的 CPU 和内存。项目执行的时候，有了一定的成果，就要给客户演示。例如客户说要做个应用，我们做出来了要给客户看看，如果客户说哪里需要改，可以根据客户的需求再改，这就需要交付人员了。QQ 启动之后，有一部分代码会在显示器上画一个对话框，并且将键盘的焦点放在了输入框里面。CPU 根据这些指令，就会告知显卡驱动程序，将这个对话框画出来。于是使用 QQ 的用户就会很开心地发现，他能和别人开始聊天了。 当用户通过键盘噼里啪啦打字的时候，键盘也是输入设备，也会触发中断，通知相应的输入设备驱动程序。我们假设用户输入了一个“a”。这就像客户提出了新的需求给客户对接员。客户对接员收到需求后，因为是对接这个项目的，所以就回来报告，客户提新需求了，项目组需要处理一下。项目执行计划书里面一般都会有当遇到何种需求应该怎么做的规定，项目组就按这个规定做了，然后让交付人员再去客户那里演示就行了。对于 QQ 来讲，由于键盘闪啊闪的焦点在 QQ 这个对话框上，因而操作系统知道，这个事件是给这个进程的。QQ 的代码里面肯定有遇到这种事件如何处理的代码，就会执行。一般是记录下客户的输入，并且告知显卡驱动程序，在那个地方画一个“a”。显卡画完了，客户看到了，就觉得自己的输入成功了。当用户输入完毕之后，回车一下，还是会通过键盘驱动程序告诉操作系统，操作系统还是会找到 QQ，QQ 会将用户的输入发送到网络上。QQ 进程是不能直接发送网络包的，需要调用系统调用，内核使用网卡驱动程序进行发送。这就像客户对接员接到一个需求，但是这个需求需要和其他公司沟通，这就需要依靠公司的对外合作部，对外合作部在办事大厅有专门的窗口，非常方便。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:5:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 到这里，一个外包公司大部分的职能部门都凑齐了。你可以对应着下图的操作系统内核体系结构，回顾一下它们是如何组成一家公司的。QQ 的运行过程，只是一个简单的比喻。在后面的章节中，我会展开讲述每个部分是怎么工作的，最后我会再将这个过程串起来，这样你就能了解操作系统的全貌了。 操作系统内核体系结构图 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:5:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"04 | 快速上手几个Linux命令：每家公司都有自己的黑话 如果你还没有上手用过 Linux，那么接下来的课程，你可能会感受到困惑。因为没有一手的体验，你可能很难将 Linux 的机制和你的使用行为关联起来。所以这一节，咱们先介绍几个上手 Linux 的命令，通过这些命令，我们试试先把 Linux 用起来。为什么我把 Linux 命令称为“黑话”呢？就像上一节我们介绍的，Linux 操作系统有很多功能，我们有很多种方式可以使用这些功能，其中最简单和直接的方式就是命令行（Command Line）。命令行就相当于你请求服务使用的专业术语。干任何事情，第一步就是学会使用正确的术语。这样，Linux 作为服务方，才能听懂。这些术语可不就是“黑话”吗？Window 系统你肯定很熟悉吧？现在，我就沿着你使用 Windows 的习惯，来给你介绍相应的 Linux 命令。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:6:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用户与密码 当我们打开一个新系统的时候，第一件要做的事就是登录。系统默认有一个 Administrator 用户，也就是系统管理员，它的权限很大，可以在这个系统上干任何事。Linux 上面也有一个类似的用户，我们叫 Root。同样，它也具有最高的操作权限。接下来，你需要输入密码了。密码从哪里来呢？对于 Windows 来讲，在你安装操作系统的过程中，会让你设置一下 Administrator 的密码；对于 Linux，Root 的密码同样也是在安装过程中设置的。 对于 Windows，你设好之后，可以多次修改这个密码。比如说，我们在控制面板的账户管理里面就可以完成这个操作。但是对于 Linux 呢？不好意思，没有这么一个统一的配置中心了。你需要使用命令来完成这件事情。这个命令很好记，passwd，其实就是 password 的简称。 # passwd Changing password for user root. New password: 按照这个命令，我们就可以输入新密码啦。在 Windows 里，除了 Administrator 之外，我们还可以创建一个以自己名字命名的用户。那在 Linux 里可不可以创建其他用户呢？当然可以了，我们同样需要一个命令useradd。 useradd cliu8 执行这个命令，一个用户就被创建了。它不会弹出什么让你输入密码之类的页面，就会直接返回了。因为接下来你需要自己调用 passwd cliu8 来设置密码，再进行登录。在 Windows 里设置用户的时候，用户有一个“组”的概念。你可能没注意过，不过我一说名字你估计就能想起来了，比如“Adminsitrator 组”“Guests 组”“Power User 组”等等。同样，Linux 里也是分组的。前面我们创建用户的时候，没有说加入哪个组，于是默认就会创建一个同名的组。能不能在创建用户的时候就指定属于哪个组呢？我们来试试。我们可以使用 -h 参数看一下，使用 useradd 这个命令，有没有相应的选项。 [root@deployer ~]# useradd -h Usage: useradd [options] LOGIN useradd -D useradd -D [options] Options: -g, --gid GROUP name or ID of the primary group of the new account 一看还真有这个选项。以后命令不会用的时候，就可以通过 -h 参数看一下，它的意思是 help。如果想看更加详细的文档，你可以通过 man useradd 获得，细细阅读。 上一节我们说过，Linux 里是“命令行 + 文件”模式。对于用户管理来说，也是一样的。咱们通过命令创建的用户，其实是放在 /etc/passwd 文件里的。这是一个文本文件。我们可以通过 cat 命令，将里面的内容输出在命令行上。组的信息我们放在 /etc/group 文件中。 # cat /etc/passwd root❌0:0:root:/root:/bin/bash ...... cliu8❌1000:1000::/home/cliu8:/bin/bash # cat /etc/group root❌0: ...... cliu8❌1000: 在 /etc/passwd 文件里，我们可以看到 root 用户和咱们刚创建的 cliu8 用户。x 的地方应该是密码，密码当然不能放在这里，不然谁都知道了。接下来是用户 ID 和组 ID，这和 /etc/group 里面就对应上了。/root 和 /home/cliu8 是什么呢？它们分别是 root 用户和 cliu8 用户的主目录。主目录是用户登录进去后默认的路径。其实 Windows 里面也是这样的。当我们打开文件夹浏览器的时候，左面会有“文档”“图片”“下载”等文件夹，路径在 C:\\Users\\cliu8 下面。要注意，同一台电脑，不同的用户情况会不一样。 /bin/bash 的位置是用于配置登录后的默认交互命令行的，不像 Windows，登录进去是界面，其实就是 explorer.exe。而 Linux 登录后的交互命令行是一个解析脚本的程序，这里配置的是 /bin/bash。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:6:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"浏览文件 终于登录进来啦，接下来你可以在文件系统里面随便逛一逛、看一看了。可以看到，Linux 的文件系统和 Windows 是一样的，都是用文件夹把文件组织起来，形成一个树形的结构。这一点没有什么差别。只不过在 Linux 下面，大多数情况，我们需要通过命令行来查看 Linux 的文件。其实在 Windows 下也有命令行，例如cd就是 change directory，就是切换目录；cd . 表示切换到当前目录；cd .. 表示切换到上一级目录；使用 dir，可以列出当前目录下的文件。Linux 基本也是这样，只不过列出当前目录下的文件我们用的是ls，意思是 list。 我们常用的是 ls -l，也就是用列表的方式列出文件。 # ls -l drwxr-xr-x 6 root root 4096 Oct 20 2017 apt -rw-r--r-- 1 root root 211 Oct 20 2017 hosts 其中第一个字段的第一个字符是文件类型。如果是“-”，表示普通文件；如果是 d，就表示目录。当然还有很多种文件类型，咱们后面遇到的时候再说，你现在先记住我说的这两个就行了。第一个字段剩下的 9 个字符是模式，其实就是权限位（access permission bits）。3 个一组，每一组 rwx 表示“读（read）”“写（write）”“执行（execute）”。如果是字母，就说明有这个权限；如果是横线，就是没有这个权限。这三组分别表示文件所属的用户权限、文件所属的组权限以及其他用户的权限。例如，上面的例子中，-rw-r–r– 就可以翻译为，这是一个普通文件，对于所属用户，可读可写不能执行；对于所属的组，仅仅可读；对于其他用户，也是仅仅可读。如果想改变权限，可以使用命令 chmod 711 hosts。第二个字段是硬链接（hard link）数目，这个比较复杂，讲文件的时候我会详细说。第三个字段是所属用户，第四个字段是所属组。第五个字段是文件的大小，第六个字段是文件被修改的日期，最后是文件名。你可以通过命令chown改变所属用户，chgrp改变所属组。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:6:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"安装软件 好了，你现在应该会浏览文件夹了，接下来应该做什么呢？当然是开始安装那些“装机必备”的软件啦！ 在 Windows 下面，在没有类似软件管家的软件之前，我们其实都是在网上下载 installer，然后再进行安装的。就以我们经常要安装的 JDK 为例子。应该去哪里下载呢？为了安全起见，一般去官网比较好。如果你去 JDK 的官网，它会给你一个这样的列表。 对于 Windows 系统，最方便的方式就是下载 exe，也就是安装文件。下载后我们直接双击安装即可。对于 Linux 来讲，也是类似的方法，你可以下载 rpm 或者 deb。这个就是 Linux 下面的安装包。为什么有两种呢？因为 Linux 现在常用的有两大体系，一个是 CentOS 体系，一个是 Ubuntu 体系，前者使用 rpm，后者使用 deb。在 Linux 上面，没有双击安装这一说，因此想要安装，我们还得需要命令。CentOS 下面使用rpm -i jdk-XXX_linux-x64_bin.rpm进行安装，Ubuntu 下面使用dpkg -i jdk-XXX_linux-x64_bin.deb。其中 -i 就是 install 的意思。 在 Windows 下面，控制面板里面有程序管理，我们可以查看目前安装了哪些软件，可以删除这些软件。 在 Linux 下面，凭借rpm -qa和dpkg -l就可以查看安装的软件列表，-q 就是 query，a 就是 all，-l 的意思就是 list。如果真的去运行的话，你会发现这个列表很长很长，很难找到你安装的软件。如果你知道要安装的软件包含某个关键词，可以用一个很好用的搜索工具 grep。 rpm -qa | grep jdk，这个命令是将列出来的所有软件形成一个输出。| 是管道，用于连接两个程序，前面 rpm -qa 的输出就放进管道里面，然后作为 grep 的输入，grep 将在里面进行搜索带关键词 jdk 的行，并且输出出来。grep 支持正则表达式，因此搜索的时候很灵活，再加上管道，这是一个很常用的模式。同理dpkg -l | grep jdk也是能够找到的。 如果你不知道关键词，可以使用rpm -qa | more和rpm -qa | less这两个命令，它们可以将很长的结果分页展示出来。这样你就可以一个个来找了。我们还是利用管道的机制。more 是分页后只能往后翻页，翻到最后一页自动结束返回命令行，less 是往前往后都能翻页，需要输入 q 返回命令行，q 就是 quit。如果要删除，可以用rpm -e和dpkg -r。-e 就是 erase，-r 就是 remove。我们刚才说的都是没有软件管家的情况，后来 Windows 上有了软件管家，就方便多了。我们直接搜索一下，然后点击安装就行了。 Linux 也有自己的软件管家，CentOS 下面是 yum，Ubuntu 下面是 apt-get。 你可以根据关键词搜索，例如搜索jdk、yum search jdk和apt-cache search jdk，可以搜索出很多很多可以安装的 jdk 版本。如果数目太多，你可以通过管道 grep、more、less 来进行过滤。选中一个之后，我们就可以进行安装了。你可以用yum install java-11-openjdk.x86_64和apt-get install openjdk-9-jdk来进行安装。安装以后，如何卸载呢？我们可以使用yum erase java-11-openjdk.x86_64和apt-get purge openjdk-9-jdk。 Windows 上的软件管家会有一个统一的服务端，来保存这些软件，但是我们不知道服务端在哪里。而 Linux 允许我们配置从哪里下载这些软件的，地点就在配置文件里面。对于 CentOS 来讲，配置文件在/etc/yum.repos.d/CentOS-Base.repo里。 [base] name=CentOS-$releasever - Base - 163.com baseurl=http://mirrors.163.com/centos/$releasever/os/$basearch/ gpgcheck=1 gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7 对于 Ubuntu 来讲，配置文件在/etc/apt/sources.list里。 deb http://mirrors.163.com/ubuntu/ xenial main restricted universe multiverse deb http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiverse deb http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiverse deb http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiverse deb http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse 这里为什么都是 163.com 呢？因为 Linux 服务器遍布全球，不能都从一个地方下载，最好选一个就近的地方下载，例如在中国，选择 163.com，就不用跨越重洋了。 其实无论是先下载再安装，还是通过软件管家进行安装，都是下载一些文件，然后将这些文件放在某个路径下，然后在相应的配置文件中配置一下。例如，在 Windows 里面，最终会变成 C:\\Program Files 下面的一个文件夹以及注册表里面的一些配置。对应 Linux 里面会放的更散一点。例如，主执行文件会放在 /usr/bin 或者 /usr/sbin 下面，其他的库文件会放在 /var 下面，配置文件会放在 /etc 下面。 所以其实还有一种简单粗暴的方法，就是将安装好的路径直接下载下来，然后解压缩成为一个整的路径。在 JDK 的安装目录中，Windows 有 jdk-XXX_Windows-x64_bin.zip，这是 Windows 下常用的压缩模式。Linux 有 jdk-XXX_linux-x64_bin.tar.gz，这是 Linux 下常用的压缩模式。如何下载呢？Linux 上面有一个工具 wget，后面加上链接，就能从网上下载了。下载下来后，我们就可以进行解压缩了。Windows 下可以有 winzip 之类的解压缩程序，Linux 下面默认会有 tar 程序。如果是解压缩 zip 包，就需要另行安装。 yum install zip.x86_64 unzip.x86_64 apt-get install zip unzip 如果是 tar.gz 这种格式的，通过 tar xvzf jdk-XXX_linux-x64_bin.tar.gz 就可以解压缩了。对于 Windows 上 jdk 的安装，如果采取这种下载压缩包的格式，需要在系统设置的环境变量配置里面设置JAVA_HOME和PATH。 在 Linux 也是一样的，通过 tar 解压缩之后，也需要配置环境变量，可以通过 export 命令来配置。 export JAVA_HOME=/root/jdk-XXX_linux-x64 export PATH=$JAVA_HOME/bin:$PATH export 命令仅在当前命令行的会话中管用，一旦退出重新登录进来，就不管用了，有没有一个地方可以像 Windows 里面可以配置永远管用呢？在当前用户的默认工作目录，例如 /root 或者 /home/cliu8 下面，有一个.bashrc 文件，这个文件是以点开头的，这个文件默认看不到，需要 ls -la 才能看到，a 就是 all。每次登录的时候，这个文件都会运行，因而把它放在这里。这样登录进来就会自动执行。当然也可以通过 source .bashrc 手动执行。要编辑.bashrc 文件，可以使用文本编辑器 vi，也可以使用更加友好的 vim。如果默认没有安装，可以通过 yum install vim 及 apt-get install vim 进行安装。 vim 就像 Windows 里面的 notepad 一样，是我们第一个要学会的工具。要不然编辑、查看配置文件，这些操作你都没办法完成。vim 是一个很复杂的工具，刚上手的时候，你只需要记住几个命令就行了。 vim hello，就是打开一个文件，名字叫 hello。如果没有这个文件，就先创建一个。我们其实就相当于打开了一个 notepad。如果文件有内容，就会显示出来。移动光标的位置，通过上下左右键就行。如果想要编辑，就把光标移动到相应的位置，输入i，意思是 insert。进入编辑模式，可以插入、删除字符，这些都和 notepad 很像。要想保存编辑的文本，我们使用esc键退出编辑模式，然后输入“:”，然后在“:”后面输入命令w，意思是 write，这样就可以保存文本，冒号后面输入q，意思是 quit，这样就会退出 vim。如果编辑了，还没保存，不想要了，可以输入q!。好了，掌握这些基本够用了，想了解更复杂的，你可以自己去看文档。通过 vim .ba","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:6:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"运行程序 好了，装好了程序，可以运行程序了。我们都知道 Windows 下的程序，如果后缀名是 exe，双击就可以运行了。Linux 不是根据后缀名来执行的。它的执行条件是这样的：只要文件有 x 执行权限，都能到文件所在的目录下，通过./filename运行这个程序。当然，如果放在 PATH 里设置的路径下面，就不用./ 了，直接输入文件名就可以运行了，Linux 会帮你找。这是 Linux 执行程序最常用的一种方式，通过 shell 在交互命令行里面运行。 这样执行的程序可能需要和用户进行交互，例如允许让用户输入，然后输出结果也打印到交互命令行上。这种方式比较适合运行一些简单的命令，例如通过 date 获取当前时间。这种模式的缺点是，一旦当前的交互命令行退出，程序就停止运行了。这样显然不能用来运行那些需要“永远“在线的程序。比如说，运行一个博客程序，我总不能老是开着交互命令行，博客才可以提供服务。一旦我要去睡觉了，关了命令行，我的博客别人就不能访问了，这样肯定是不行的。于是，我们就有了 Linux 运行程序的第二种方式，后台运行。 这个时候，我们往往使用nohup命令。这个命令的意思是 no hang up（不挂起），也就是说，当前交互命令行退出的时候，程序还要在。当然这个时候，程序不能霸占交互命令行，而是应该在后台运行。最后加一个 \u0026，就表示后台运行。另外一个要处理的就是输出，原来什么都打印在交互命令行里，现在在后台运行了，输出到哪里呢？输出到文件是最好的。最终命令的一般形式为nohup command \u003eout.file 2\u003e\u00261 \u0026。这里面，“1”表示文件描述符 1，表示标准输出，“2”表示文件描述符 2，意思是标准错误输出，“2\u003e\u00261”表示标准输出和错误输出合并了。合并到哪里去呢？到 out.file 里。那这个进程如何关闭呢？我们假设启动的程序包含某个关键字，那就可以使用下面的命令。 ps -ef |grep 关键字 |awk '{print $2}'|xargs kill -9 从这个命令中，我们多少能看出 shell 的灵活性和精巧组合。其中 ps -ef 可以单独执行，列出所有正在运行的程序，grep 上面我们介绍过了，通过关键字找到咱们刚才启动的程序。awk 工具可以很灵活地对文本进行处理，这里的 awk ‘{print $2}‘是指第二列的内容，是运行的程序 ID。我们可以通过 xargs 传递给 kill -9，也就是发给这个运行的程序一个信号，让它关闭。如果你已经知道运行的程序 ID，可以直接使用 kill 关闭运行的程序。在 Windows 里面还有一种程序，称为服务。这是系统启动的时候就在的，我们可以通过控制面板的服务管理启动和关闭它。 Linux 也有相应的服务，这就是程序运行的第三种方式，以服务的方式运行。例如常用的数据库 MySQL，就可以使用这种方式运行。例如在 Ubuntu 中，我们可以通过 apt-get install mysql-server 的方式安装 MySQL，然后通过命令systemctl start mysql启动 MySQL，通过systemctl enable mysql设置开机启动。之所以成为服务并且能够开机启动，是因为在 /lib/systemd/system 目录下会创建一个 XXX.service 的配置文件，里面定义了如何启动、如何关闭。在 CentOS 里有些特殊，MySQL 被 Oracle 收购后，因为担心授权问题，改为使用 MariaDB，它是 MySQL 的一个分支。通过命令yum install mariadb-server mariadb进行安装，命令systemctl start mariadb启动，命令systemctl enable mariadb设置开机启动。同理，会在 /usr/lib/systemd/system 目录下，创建一个 XXX.service 的配置文件，从而成为一个服务。systemd 的机制十分复杂，这里咱们不讨论。如果有兴趣，你可以自己查看相关文档。最后咱们要学习的是如何关机和重启。这个就很简单啦。shutdown -h now是现在就关机，reboot就是重启。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:6:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 好了，掌握这些基本命令足够你熟练操作 Linux 了。如果你是个初学者，这些命令估计看起来还是很多。我把今天这些基本的命令以及对应的操作总结了一下，方便你操作和查阅。你不用可以去死记硬背，按照我讲的这个步骤，从设置用户和密码、浏览文件、安装软件，最后到运行程序，自己去操作几遍，再自己整理一遍，手脑并用，加深理解，巩固记忆，效果可能会更好。 (建议保存查看清晰大图) ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:6:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"05 | 学会几个系统调用：咱们公司能接哪些类型的项目？ 上一节我们讲了几个重要的 Linux 命令行，只有通过这些命令，用户才能把 Linux 系统用起来，不知道你掌握得如何了？其实 Linux 命令也是一个程序，只不过代码是别人写好的，你直接用就可以了。你可以自己试着写写代码，通过代码把 Linux 系统用起来，这样印象会更深刻。不过，无论是别人写的程序，还是你写的程序，运行起来都是进程。如果你是一家外包公司，一个项目的运行要使用公司的服务，那就应该去办事大厅，也就是说，你写的程序应该使用系统调用。你看，系统调用决定了这个操作系统好用不好用、功能全不全。对应到咱们这个公司中，作为一个老板，你应该好好规划一下，你的办事大厅能够提供哪些服务，这决定了你这个公司会被打五星还是打差评。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"立项服务与进程管理 首先，我们得有个项目，那就要有立项服务。对应到 Linux 操作系统中就是创建进程。创建进程的系统调用叫fork。这个名字很奇怪，中文叫“分支”。为啥启动一个新进程叫“分支”呢？在 Linux 里，要创建一个新的进程，需要一个老的进程调用 fork 来实现，其中老的进程叫作父进程（Parent Process），新的进程叫作子进程（Child Process）。 前面我们说过，一个进程的运行是要有一个程序的，就像一个项目的执行，要有一个项目执行计划书。本来老的项目，按照项目计划书按部就班地来，项目执行到一半，突然接到命令，说是要新启动一个项目，这个时候应该怎么办呢？一个项目的执行是很复杂的，需要涉及公司各个部门的工作，比如说，项目管理部门需要给这个项目组开好 Jira 和 Wiki，会议室管理部要为这个项目分配会议室等等。所以，我们现在有两种方式，一种是列一个清单，清单里面写明每个新项目组都要开哪些账号。但是，这样每次有项目，都要重新配置一遍新的 Jira、Wiki，复杂得很。另一种方式就是咱们程序员常用的方式，CTRL/C + CTRL/V。也就是说，如果想为新项目建立一套 Jira，但又觉得一个个填 Jira 里面的选项太麻烦，那就可以拷贝一个别人的，然后根据新项目的实际情况，将相应的配置改改。 Linux 就是这样想的。当父进程调用 fork 创建进程的时候，子进程将各个子系统为父进程创建的数据结构也全部拷贝了一份，甚至连程序代码也是拷贝过来的。按理说，如果不进行特殊的处理，父进程和子进程都按相同的程序代码进行下去，这样就没有意义了。所以，我们往往会这样处理：对于 fork 系统调用的返回值，如果当前进程是子进程，就返回 0；如果当前进程是父进程，就返回子进程的进程号。这样首先在返回值这里就有了一个区分，然后通过 if-else 语句判断，如果是父进程，还接着做原来应该做的事情；如果是子进程，需要请求另一个系统调用execve来执行另一个程序，这个时候，子进程和父进程就彻底分道扬镳了，也就产生了一个分支（fork）了。 同样是“先拷贝，再修改”的策略，你可能会问，新进程都是父进程 fork 出来的，那到底谁是第一个呢？作为一个外包公司老板，有了新项目当然会分给手下做，但是当公司刚起步的时候呢？没有下属，只好自己上了。先建立项目运行体系，等后面再做项目的时候，就都按这个来。对于操作系统也一样，启动的时候先创建一个所有用户进程的“祖宗进程”。这个在讲系统启动的时候还会详细讲，我这里先不多说。有时候，父进程要关心子进程的运行情况，这毕竟是自己身上掉下来的肉。有个系统调用waitpid，父进程可以调用它，将子进程的进程号作为参数传给它，这样父进程就知道子进程运行完了没有，成功与否。所以说，所有子项目最终都是老板，也就是祖宗进程 fork 过来的，因而它要对整个公司的项目执行负最终的责任。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"会议室管理与内存管理 项目启动之后，每个项目组有独立的会议室，存放自己项目相关的数据。每个项目组都感觉自己有独立的办公空间。在操作系统中，每个进程都有自己的内存，互相之间不干扰，有独立的进程内存空间。那独立的办公空间里面，都放些什么呢？项目执行计划书肯定是要放进去的，因为执行过程中肯定要不断地看。对于进程的内存空间来讲，放程序代码的这部分，我们称为代码段（Code Segment）。 项目执行的过程中，会产生一些架构图、流程图，这些也放在会议室里面。有的画在白板上，讨论完了，进入下个主题就会擦了；有的画在纸和本子上，讨论的时候翻出来，不讨论的时候堆在那里，会保留比较长的一段时间，除非指明的确不需要了才会去销毁。对于进程的内存空间来讲，放进程运行中产生数据的这部分，我们称为数据段（Data Segment）。其中局部变量的部分，在当前函数执行的时候起作用，当进入另一个函数时，这个变量就释放了；也有动态分配的，会较长时间保存，指明才销毁的，这部分称为堆（Heap）。 一个进程的内存空间是很大的，32 位的是 4G，64 位的就更大了，我们不可能有这么多物理内存。就像一个公司的会议室是有限的，作为老板，你不可能事先都给项目组分配好。哪有这么多会议室啊，一定是需要的时候再分配。所以，进程自己不用的部分就不用管，只有进程要去使用部分内存的时候，才会使用内存管理的系统调用来登记，说自己马上就要用了，希望分配一部分内存给它，但是这还不代表真的就对应到了物理内存。只有真的写入数据的时候，发现没有对应物理内存，才会触发一个中断，现分配物理内存。 这里我们介绍两个在堆里面分配内存的系统调用，brk和mmap。 当分配的内存数量比较小的时候，使用 brk，会和原来的堆的数据连在一起，这就像多分配两三个工位，在原来的区域旁边搬两把椅子就行了。当分配的内存数量比较大的时候，使用 mmap，会重新划分一块区域，也就是说，当办公空间需要太多的时候，索性来个一整块。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"档案库管理与文件管理 项目执行计划书要保存在档案库里，有一些需要长时间保存，这样哪怕公司暂时停业，再次经营的时候还可以继续使用。同样，程序、文档、照片等，哪怕关机再开机也能不丢的，就需要放在文件系统里面。文件之所以能做到这一点，一方面是因为介质，另一方面是因为格式。公司之所以强调资料库，也是希望将一些知识固化为标准格式，放在一起进行管理，无论多少人来人走，都不影响公司业务。文件管理其实花样不多，拍着脑袋都能想出来，无非是创建、打开、读、写等。对于文件的操作，下面这六个系统调用是最重要的： 对于已经有的文件，可以使用open打开这个文件，close关闭这个文件；对于没有的文件，可以使用creat创建文件；打开文件以后，可以使用lseek跳到文件的某个位置；可以对文件的内容进行读写，读的系统调用是read，写是write。 但是别忘了，Linux 里有一个特点，那就是一切皆文件。 启动一个进程，需要一个程序文件，这是一个二进制文件。启动的时候，要加载一些配置文件，例如 yml、properties 等，这是文本文件；启动之后会打印一些日志，如果写到硬盘上，也是文本文件。但是如果我想把日志打印到交互控制台上，在命令行上唰唰地打印出来，这其实也是一个文件，是标准输出 stdout 文件。这个进程的输出可以作为另一个进程的输入，这种方式称为管道，管道也是一个文件。进程可以通过网络和其他进程进行通信，建立的 Socket，也是一个文件。进程需要访问外部设备，设备也是一个文件。文件都被存储在文件夹里面，其实文件夹也是一个文件。进程运行起来，要想看到进程运行的情况，会在 /proc 下面有对应的进程号，还是一系列文件。 每个文件，Linux 都会分配一个文件描述符（File Descriptor），这是一个整数。有了这个文件描述符，我们就可以使用系统调用，查看或者干预进程运行的方方面面。所以说，文件操作是贯穿始终的，这也是“一切皆文件”的优势，就是统一了操作的入口，提供了极大的便利。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"项目异常处理与信号处理 在项目运行过程中，不一定都是一帆风顺的，很可能遇到各种异常情况。作为老板，处理异常情况的能力是非常重要的，所以办事大厅也一定要包含这部分服务。当项目遇到异常情况，例如项目中断，做到一半不做了。这时候就需要发送一个信号（Signal）给项目组。经常遇到的信号有以下几种： 在执行一个程序的时候，在键盘输入“CTRL+C”，这就是中断的信号，正在执行的命令就会中止退出；如果非法访问内存，例如你跑到别人的会议室，可能会看到不该看的东西；硬件故障，设备出了问题，当然要通知项目组；用户进程通过kill函数，将一个用户信号发送给另一个进程。 当项目组收到信号的时候，项目组需要决定如何处理这些异常情况。对于一些不严重的信号，可以忽略，该干啥干啥，但是像 SIGKILL（用于终止一个进程的信号）和 SIGSTOP（用于中止一个进程的信号）是不能忽略的，可以执行对于该信号的默认动作。每种信号都定义了默认的动作，例如硬件故障，默认终止；也可以提供信号处理函数，可以通过sigaction系统调用，注册一个信号处理函数。提供了信号处理服务，项目执行过程中一旦有变动，就可以及时处理了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"项目组间沟通与进程间通信 当某个项目比较大的时候，可能分成多个项目组，不同的项目组需要相互交流、相互配合才能完成，这就需要一个项目组之间的沟通机制。项目组之间的沟通方式有很多种，我们来一一规划。首先就是发个消息，不需要一段很长的数据，这种方式称为消息队列（Message Queue）。由于一个公司内的多个项目组沟通时，这个消息队列是在内核里的，我们可以通过msgget创建一个新的队列，msgsnd将消息发送到消息队列，而消息接收方可以使用msgrcv从队列中取消息。 当两个项目组需要交互的信息比较大的时候，可以使用共享内存的方式，也即两个项目组共享一个会议室（这样数据就不需要拷贝来拷贝去）。大家都到这个会议室来，就可以完成沟通了。这时候，我们可以通过shmget创建一个共享内存块，通过shmat将共享内存映射到自己的内存空间，然后就可以读写了。但是，两个项目组共同访问一个会议室里的数据，就会存在“竞争”的问题。如果大家同时修改同一块数据咋办？这就需要有一种方式，让不同的人能够排他地访问，这就是信号量的机制 Semaphore。这个机制比较复杂，我这里说一种简单的场景。 对于只允许一个人访问的需求，我们可以将信号量设为 1。当一个人要访问的时候，先调用sem_wait。如果这时候没有人访问，则占用这个信号量，他就可以开始访问了。如果这个时候另一个人要访问，也会调用 sem_wait。由于前一个人已经在访问了，所以后面这个人就必须等待上一个人访问完之后才能访问。当上一个人访问完毕后，会调用sem_post将信号量释放，于是下一个人等待结束，可以访问这个资源了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"公司间沟通与网络通信 同一个公司不同项目组之间的合作搞定了，如果是不同公司之间呢？也就是说，这台 Linux 要和另一台 Linux 交流，这时候，我们就需要用到网络服务。不同机器的通过网络相互通信，要遵循相同的网络协议，也即 TCP/IP 网络协议栈。Linux 内核里有对于网络协议栈的实现。如何暴露出服务给项目组使用呢？ 网络服务是通过套接字 Socket 来提供服务的。Socket 这个名字很有意思，可以作“插口”或者“插槽”讲。虽然我们是写软件程序，但是你可以想象成弄一根网线，一头插在客户端，一头插在服务端，然后进行通信。因此，在通信之前，双方都要建立一个 Socket。我们可以通过 Socket 系统调用建立一个 Socket。Socket 也是一个文件，也有一个文件描述符，也可以通过读写函数进行通信。好了，我们分门别类地规划了这么多办事大厅的服务，如果这些都有了，足够我们成长为一个大型跨国公司了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:6","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"查看源代码中的系统调用 你如果问，这里的系统调用列举全了吗？其实没有，系统调用非常多。我建议你访问https://www.kernel.org下载一份 Linux 内核源代码。因为在接下来的整个课程里，我讲述的逻辑都是这些内核代码的逻辑。对于 64 位操作系统，找到 unistd_64.h 文件，里面对于系统调用的定义，就是下面这样。 #define __NR_restart_syscall 0 #define __NR_exit 1 #define __NR_fork 2 #define __NR_read 3 #define __NR_write 4 #define __NR_open 5 #define __NR_close 6 #define __NR_waitpid 7 #define __NR_creat 8 ...... ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:7","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"中介与 Glibc 如果你做过开发，你会觉得刚才讲的和平时咱们调用的函数不太一样。这是因为，平时你并没有直接使用系统调用。虽然咱们的办事大厅已经很方便了，但是为了对用户更友好，我们还可以使用中介 Glibc，有事情找它就行，它会转换成为系统调用，帮你调用。Glibc 是 Linux 下使用的开源的标准 C 库，它是 GNU 发布的 libc 库。Glibc 为程序员提供丰富的 API，除了例如字符串处理、数学运算等用户态服务之外，最重要的是封装了操作系统提供的系统服务，即系统调用的封装。 每个特定的系统调用对应了至少一个 Glibc 封装的库函数，比如说，系统提供的打开文件系统调用 sys_open 对应的是 Glibc 中的 open 函数。有时候，Glibc 一个单独的 API 可能调用多个系统调用，比如说，Glibc 提供的 printf 函数就会调用如 sys_open、sys_mmap、sys_write、sys_close 等等系统调用。也有时候，多个 API 也可能只对应同一个系统调用，如 Glibc 下实现的 malloc、calloc、free 等函数用来分配和释放内存，都利用了内核的 sys_brk 的系统调用。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:8","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 学了这么多系统调用，我们还是用一个图来总结一下。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:7:9","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"06 | x86架构：有了开放的架构，才能打造开放的营商环境 做生意的人最喜欢开放的营商环境，也就是说，我的这家公司，只要符合国家的法律，到哪里做生意，都能受到公平的对待，这样就不用为了适配各个地方的规则煞费苦心，只要集中精力优化自己的服务就可以了。作为 Linux 操作系统，何尝不是这样。如果下面的硬件环境千差万别，就会很难集中精力做出让用户易用的产品。毕竟天天适配不同的平台，就已经够头大了。x86 架构就是这样一个开放的平台。今天我们就来解析一下它。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:8:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"计算机的工作模式是什么样的？ 还记得咱们攒电脑时买的那堆硬件吗？虽然你可以根据经验，把那些复杂的设备和连接线安装起来，但是你真的了解它们为什么要这么连接吗？现在我就把硬件图和计算机的逻辑图对应起来，带你看看计算机的工作模式。 对于一个计算机来讲，最核心的就是 CPU（Central Processing Unit，中央处理器）。这是这台计算机的大脑，所有的设备都围绕它展开。对于公司来说，CPU 是真正干活的，将来执行项目都要靠它。CPU 就相当于咱们公司的程序员，我们常说，二十一世纪最缺的是什么？是人才！所以，大量水平高、干活快的程序员，才是营商环境中最重要的部分。CPU 和其他设备连接，要靠一种叫做总线（Bus）的东西，其实就是主板上密密麻麻的集成电路，这些东西组成了 CPU 和其他设备的高速通道。 在这些设备中，最重要的是内存（Memory）。因为单靠 CPU 是没办法完成计算任务的，很多复杂的计算任务都需要将中间结果保存下来，然后基于中间结果进行进一步的计算。CPU 本身没办法保存这么多中间结果，这就要依赖内存了。内存就相当于办公室，我们要看看方不方便租到办公室，有没有什么创新科技园之类的。有了共享的、便宜的办公位，公司就有注册地了。当然总线上还有一些其他设备，例如显卡会连接显示器、磁盘控制器会连接硬盘、USB 控制器会连接键盘和鼠标等等。 CPU 和内存是完成计算任务的核心组件，所以这里我们重点介绍一下 CPU 和内存是如何配合工作的。CPU 其实也不是单纯的一块，它包括三个部分，运算单元、数据单元和控制单元。运算单元只管算，例如做加法、做位移等等。但是，它不知道应该算哪些数据，运算结果应该放在哪里。运算单元计算的数据如果每次都要经过总线，到内存里面现拿，这样就太慢了，所以就有了数据单元。数据单元包括 CPU 内部的缓存和寄存器组，空间很小，但是速度飞快，可以暂时存放数据和运算结果。有了放数据的地方，也有了算的地方，还需要有个指挥到底做什么运算的地方，这就是控制单元。控制单元是一个统一的指挥中心，它可以获得下一条指令，然后执行这条指令。这个指令会指导运算单元取出数据单元中的某几个数据，计算出个结果，然后放在数据单元的某个地方。 每个项目都有一个项目执行计划书，里面是一行行项目执行的指令，这些都是放在档案库里面的。每个进程都有一个程序放在硬盘上，是二进制的，再里面就是一行行的指令，会操作一些数据。进程一旦运行，比如图中两个进程 A 和 B，会有独立的内存空间，互相隔离，程序会分别加载到进程 A 和进程 B 的内存空间里面，形成各自的代码段。当然真实情况肯定比我说的要复杂的多，进程的内存虽然隔离但不连续，除了简单的区分代码段和数据段，还会分得更细。 程序运行的过程中要操作的数据和产生的计算结果，都会放在数据段里面。那 CPU 怎么执行这些程序，操作这些数据，产生一些结果，并写入回内存呢？ CPU 的控制单元里面，有一个指令指针寄存器，它里面存放的是下一条指令在内存中的地址。控制单元会不停地将代码段的指令拿进来，先放入指令寄存器。当前的指令分两部分，一部分是做什么操作，例如是加法还是位移；一部分是操作哪些数据。要执行这条指令，就要把第一部分交给运算单元，第二部分交给数据单元。数据单元根据数据的地址，从数据段里读到数据寄存器里，就可以参与运算了。运算单元做完运算，产生的结果会暂存在数据单元的数据寄存器里。最终，会有指令将数据写回内存中的数据段。你可能会问，上面算来算去执行的都是进程 A 里的指令，那进程 B 呢？CPU 里有两个寄存器，专门保存当前处理进程的代码段的起始地址，以及数据段的起始地址。这里面写的都是进程 A，那当前执行的就是进程 A 的指令，等切换成进程 B，就会执行 B 的指令了，这个过程叫作进程切换（Process Switch）。这是一个多任务系统的必备操作，我们后面有专门的章节讲这个内容，这里你先有个印象。 到这里，你会发现，CPU 和内存来来回回传数据，靠的都是总线。其实总线上主要有两类数据，一个是地址数据，也就是我想拿内存中哪个位置的数据，这类总线叫地址总线（Address Bus）；另一类是真正的数据，这类总线叫数据总线（Data Bus）。所以说，总线其实有点像连接 CPU 和内存这两个设备的高速公路，说总线到底是多少位，就类似说高速公路有几个车道。但是这两种总线的位数意义是不同的。地址总线的位数，决定了能访问的地址范围到底有多广。例如只有两位，那 CPU 就只能认 00，01，10，11 四个位置，超过四个位置，就区分不出来了。位数越多，能够访问的位置就越多，能管理的内存的范围也就越广。而数据总线的位数，决定了一次能拿多少个数据进来。例如只有两位，那 CPU 一次只能从内存拿两位数。要想拿八位，就要拿四次。位数越多，一次拿的数据就越多，访问速度也就越快。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:8:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"x86 成为开放平台历史中的重要一笔 那 CPU 中总线的位数有没有个标准呢？如果没有标准，那操作系统作为软件就很难办了，因为软件层没办法实现通用的运算逻辑。这就像很多非标准的元器件一样，你烧你的电路板，我烧我的电路板，谁都不能用彼此的。早期的 IBM 凭借大型机技术成为计算机市场的领头羊，直到后来个人计算机兴起，苹果公司诞生。但是，那个时候，无论是大型机还是个人计算机，每家的 CPU 架构都不一样。如果一直是这样，个人电脑、平板电脑、手机等等，都没办法形成统一的体系，就不会有我们现在通用的计算机了，更别提什么云计算、大数据这些统一的大平台了。好在历史将 x86 平台推到了开放、统一、兼容的位置。我们继续来看 IBM 和 x86 的故事。 IBM 开始做 IBM PC 时，一开始并没有让最牛的华生实验室去研发，而是交给另一个团队。一年时间，软硬件全部自研根本不可能完成，于是他们采用了英特尔的 8088 芯片作为 CPU，使用微软的 MS-DOS 做操作系统。谁能想到 IBM PC 卖得超级好，好到因为垄断市场而被起诉。IBM 就在被逼的情况下公开了一些技术，使得后来无数 IBM-PC 兼容机公司的出现，也就有了后来占据市场的惠普、康柏、戴尔等等。能够开放自己的技术是一件了不起的事。从技术和发展的层面来讲，它会使得一项技术大面积铺开，形成行业标准。就比如现在常用的 Android 手机，如果没有开放的 Android 系统，我们也没办法享受到这么多不同类型的手机。对于当年的 PC 机来说，其实也是这样。英特尔的技术因此成为了行业的开放事实标准。由于这个系列开端于 8086，因此称为 x86 架构。后来英特尔的 CPU 数据总线和地址总线越来越宽，处理能力越来越强。但是一直不能忘记三点，一是标准，二是开放，三是兼容。因为要想如此大的一个软硬件生态都基于这个架构，符合它的标准，如果是封闭或者不兼容的，那谁都不答应。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:8:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"从 8086 的原理说起 说完了 x86 的历史，我们再来看 x86 中最经典的一款处理器，8086 处理器。虽然它已经很老了，但是咱们现在操作系统中的很多特性都和它有关，并且一直保持兼容。我们把 CPU 里面的组件放大之后来看。你可以看我画的这幅图。 我们先来看数据单元。为了暂存数据，8086 处理器内部有 8 个 16 位的通用寄存器，也就是刚才说的 CPU 内部的数据单元，分别是 AX、BX、CX、DX、SP、BP、SI、DI。这些寄存器主要用于在计算过程中暂存数据。这些寄存器比较灵活，其中 AX、BX、CX、DX 可以分成两个 8 位的寄存器来使用，分别是 AH、AL、BH、BL、CH、CL、DH、DL，其中 H 就是 High（高位），L 就是 Low（低位）的意思。这样，比较长的数据也能暂存，比较短的数据也能暂存。你可能会说 16 位并不长啊，你可别忘了，那是在计算机刚刚起步的时代。 接着我们来看控制单元。IP 寄存器就是指令指针寄存器（Instruction Pointer Register)，指向代码段中下一条指令的位置。CPU 会根据它来不断地将指令从内存的代码段中，加载到 CPU 的指令队列中，然后交给运算单元去执行。如果需要切换进程呢？每个进程都分代码段和数据段，为了指向不同进程的地址空间，有四个 16 位的段寄存器，分别是 CS、DS、SS、ES。其中，CS 就是代码段寄存器（Code Segment Register），通过它可以找到代码在内存中的位置；DS 是数据段的寄存器，通过它可以找到数据在内存中的位置。SS 是栈寄存器（Stack Register）。栈是程序运行中一个特殊的数据结构，数据的存取只能从一端进行，秉承后进先出的原则，push 就是入栈，pop 就是出栈。 凡是与函数调用相关的操作，都与栈紧密相关。例如，A 调用 B，B 调用 C。当 A 调用 B 的时候，要执行 B 函数的逻辑，因而 A 运行的相关信息就会被 push 到栈里面。当 B 调用 C 的时候，同样，B 运行相关信息会被 push 到栈里面，然后才运行 C 函数的逻辑。当 C 运行完毕的时候，先 pop 出来的是 B，B 就接着调用 C 之后的指令运行下去。B 运行完了，再 pop 出来的就是 A，A 接着运行，直到结束。如果运算中需要加载内存中的数据，需要通过 DS 找到内存中的数据，加载到通用寄存器中，应该如何加载呢？对于一个段，有一个起始的地址，而段内的具体位置，我们称为偏移量（Offset）。例如 8 号会议室的第三排，8 号会议室就是起始地址，第三排就是偏移量。 在 CS 和 DS 中都存放着一个段的起始地址。代码段的偏移量在 IP 寄存器中，数据段的偏移量会放在通用寄存器中。这时候问题来了，CS 和 DS 都是 16 位的，也就是说，起始地址都是 16 位的，IP 寄存器和通用寄存器都是 16 位的，偏移量也是 16 位的，但是 8086 的地址总线地址是 20 位。怎么凑够这 20 位呢？方法就是“起始地址 *16+ 偏移量”，也就是把 CS 和 DS 中的值左移 4 位，变成 20 位的，加上 16 位的偏移量，这样就可以得到最终 20 位的数据地址。从这个计算方式可以算出，无论真正的内存多么大，对于只有 20 位地址总线的 8086 来讲，能够区分出的地址也就 2^20=1M，超过这个空间就访问不到了。这又是为啥呢？如果你想访问 1M+X 的地方，这个位置已经超过 20 位了，由于地址总线只有 20 位，在总线上超过 20 位的部分根本是发不出去的，所以发出去的还是 X，最后还是会访问 1M 内的 X 的位置。 那一个段最大能有多大呢？因为偏移量只能是 16 位的，所以一个段最大的大小是 2^16=64k。是不是好可怜？对于 8086CPU，最多只能访问 1M 的内存空间，还要分成多个段，每个段最多 64K。尽管我们现在看来这不可想象得小，根本没法儿用，但是在当时其实够用了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:8:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"再来说 32 位处理器 当然，后来计算机的发展日新月异，内存越来越大，总线也越来越宽。在 32 位处理器中，有 32 根地址总线，可以访问 2^32=4G 的内存。使用原来的模式肯定不行了，但是又不能完全抛弃原来的模式，因为这个架构是开放的。“开放”，意味着有大量其他公司的软硬件是基于这个架构来实现的，不能为所欲为，想怎么改怎么改，一定要和原来的架构兼容，而且要一直兼容，这样大家才愿意跟着你这个开放平台一直玩下去。如果你朝令夕改，那其他厂商就惨了。如果是不开放的架构，那就没有问题。硬件、操作系统，甚至上面的软件都是自己搞的，你想怎么改就可以怎么改。 我们下面来说说，在开放架构的基础上，如何保持兼容呢？首先，通用寄存器有扩展，可以将 8 个 16 位的扩展到 8 个 32 位的，但是依然可以保留 16 位的和 8 位的使用方式。你可能会问，为什么高 16 位不分成两个 8 位使用呢？因为这样就不兼容了呀！其中，指向下一条指令的指令指针寄存器 IP，就会扩展成 32 位的，同样也兼容 16 位的。 而改动比较大，有点不兼容的就是段寄存器（Segment Register）。因为原来的模式其实有点不伦不类，因为它没有把 16 位当成一个段的起始地址，也没有按 8 位或者 16 位扩展的形式，而是根据当时的硬件，弄了一个不上不下的 20 位的地址。这样每次都要左移四位，也就意味着段的起始地址不能是任何一个地方，只是能整除 16 的地方。如果新的段寄存器都改成 32 位的，明明 4G 的内存全部都能访问到，还左移不左移四位呢？ 那我们索性就重新定义一把吧。CS、SS、DS、ES 仍然是 16 位的，但是不再是段的起始地址。段的起始地址放在内存的某个地方。这个地方是一个表格，表格中的一项一项是段描述符（Segment Descriptor）。这里面才是真正的段的起始地址。而段寄存器里面保存的是在这个表格中的哪一项，称为选择子（Selector）。这样，将一个从段寄存器直接拿到的段起始地址，就变成了先间接地从段寄存器找到表格中的一项，再从表格中的一项中拿到段起始地址。这样段起始地址就会很灵活了。当然为了快速拿到段起始地址，段寄存器会从内存中拿到 CPU 的描述符高速缓存器中。这样就不兼容了，咋办呢？好在后面这种模式灵活度非常高，可以保持将来一直兼容下去。前面的模式出现的时候，没想到自己能够成为一个标准，所以设计就没这么灵活。因而到了 32 位的系统架构下，我们将前一种模式称为实模式（Real Pattern），后一种模式称为保护模式（Protected Pattern）。 当系统刚刚启动的时候，CPU 是处于实模式的，这个时候和原来的模式是兼容的。也就是说，哪怕你买了 32 位的 CPU，也支持在原来的模式下运行，只不过快了一点而已。当需要更多内存的时候，你可以遵循一定的规则，进行一系列的操作，然后切换到保护模式，就能够用到 32 位 CPU 更强大的能力。这也就是说，不能无缝兼容，但是通过切换模式兼容，也是可以接受的。在接下来的几节，我们就来看一下，CPU 如何从启动开始，逐渐从实模式变为保护模式的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:8:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们讲了 x86 架构。在以后的操作系统讲解中，我们也是主要基于 x86 架构进行讲解，只有了解了底层硬件的基本工作原理，将来才能理解操作系统的工作模式。x86 架构总体来说还是很复杂的，其中和操作系统交互比较密切的部分，我画了个图。在这个图中，建议你重点牢记这些寄存器的作用，以及段的工作模式，后面我们马上就能够用到了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:8:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"07 | 从BIOS到bootloader：创业伊始，有活儿老板自己上 上一节我们说，x86 作为一个开放的营商环境，有两种模式，一种模式是实模式，只能寻址 1M，每个段最多 64K。这个太小了，相当于咱们创业的个体户模式。有了项目只能老板自己上，本小利微，万事开头难。另一种是保护模式，对于 32 位系统，能够寻址 4G。这就是大买卖了，老板要雇佣很多人接项目。几乎所有成功的公司，都是从个体户模式发展壮大的，因此，这一节咱们就从系统刚刚启动的个体户模式开始说起。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:9:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"BIOS 时期 当你轻轻按下计算机的启动按钮时，你的主板就加上电了。按照我们之前说的，这时候你的 CPU 应该开始执行指令了。你作为老板，同时也作为员工，要开始干活了。可是你发现，这个时候还没有项目执行计划书，所以你没啥可干的。也就是说，这个时候没有操作系统，内存也是空的，一穷二白。CPU 该怎么办呢？你作为这个创业公司的老板，由于原来没开过公司，对于公司的运营当然是一脸懵的。但是我们有一个良好的营商环境，其中的创业指导中心早就考虑到这种情况了。于是，创业指导中心就给了你一套创业公司启动指导手册。你只要按着指导手册来干就行了。 计算机系统也早有计划。在主板上，有一个东西叫 ROM（Read Only Memory，只读存储器）。这和咱们平常说的内存 RAM（Random Access Memory，随机存取存储器）不同。咱们平时买的内存条是可读可写的，这样才能保存计算结果。而 ROM 是只读的，上面早就固化了一些初始化的程序，也就是 BIOS（Basic Input and Output System，基本输入输出系统）。如果你自己安装过操作系统，刚启动的时候，按某个组合键，显示器会弹出一个蓝色的界面。能够调整启动顺序的系统，就是我说的 BIOS，然后我们就可以先执行它。 创业初期，你的办公室肯定很小。假如现在你有 1M 的内存地址空间。这个空间非常有限，你需要好好利用才行。 在 x86 系统中，将 1M 空间最上面的 0xF0000 到 0xFFFFF 这 64K 映射给 ROM，也就是说，到这部分地址访问的时候，会访问 ROM。当电脑刚加电的时候，会做一些重置的工作，将 CS 设置为 0xFFFF，将 IP 设置为 0x0000，所以第一条指令就会指向 0xFFFF0，正是在 ROM 的范围内。在这里，有一个 JMP 命令会跳到 ROM 中做初始化工作的代码，于是，BIOS 开始进行初始化的工作。创业指导手册第一条，BIOS 要检查一下系统的硬件是不是都好着呢。创业指导手册第二条，要有个办事大厅，只不过自己就是办事员。这个时期你能提供的服务很简单，但也会有零星的客户来提要求。这个时候，要建立一个中断向量表和中断服务程序，因为现在你还要用键盘和鼠标，这些都要通过中断进行的。这个时期也要给客户输出一些结果，因为需要你自己来，所以你还要充当客户对接人。你做了什么工作，做到了什么程度，都要主动显示给客户，也就是在内存空间映射显存的空间，在显示器上显示一些字符。 最后，政府领进门，创业靠个人。接下来就是你发挥聪明才智的时候了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:9:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"bootloader 时期 政府给的创业指导手册只能保证你把公司成立起来，但是公司如何做大做强，需要你自己有一套经营方法。你可以试着从档案库里面翻翻，看哪里能够找到《企业经营宝典》。通过这个宝典，可以帮你建立一套完整的档案库管理体系，使得任何项目的档案查询都十分方便。现在，什么线索都没有的 BIOS，做完自己的事情，只能从档案库门卫开始，慢慢打听操作系统的下落。操作系统在哪儿呢？一般都会在安装在硬盘上，在 BIOS 的界面上。你会看到一个启动盘的选项。启动盘有什么特点呢？它一般在第一个扇区，占 512 字节，而且以 0xAA55 结束。这是一个约定，当满足这个条件的时候，就说明这是一个启动盘，在 512 字节以内会启动相关的代码。 这些代码是谁放在这里的呢？在 Linux 里面有一个工具，叫 Grub2，全称 Grand Unified Bootloader Version 2。顾名思义，就是搞系统启动的。你可以通过 grub2-mkconfig -o /boot/grub2/grub.cfg 来配置系统启动的选项。你可以看到里面有类似这样的配置。 menuentry 'CentOS Linux (3.10.0-862.el7.x86_64) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-862.el7.x86_64-advanced-b1aceb95-6b9e-464a-a589-bed66220ebee' { load_video set gfxpayload=keep insmod gzio insmod part_msdos insmod ext2 set root='hd0,msdos1' if [ x$feature_platform_search_hint = xy ]; then search --no-floppy --fs-uuid --set=root --hint='hd0,msdos1' b1aceb95-6b9e-464a-a589-bed66220ebee else search --no-floppy --fs-uuid --set=root b1aceb95-6b9e-464a-a589-bed66220ebee fi linux16 /boot/vmlinuz-3.10.0-862.el7.x86_64 root=UUID=b1aceb95-6b9e-464a-a589-bed66220ebee ro console=tty0 console=ttyS0,115200 crashkernel=auto net.ifnames=0 biosdevname=0 rhgb quiet initrd16 /boot/initramfs-3.10.0-862.el7.x86_64.img } 这里面的选项会在系统启动的时候，成为一个列表，让你选择从哪个系统启动。最终显示出来的结果就是下面这张图。至于上面选项的具体意思，我们后面再说。 使用 grub2-install /dev/sda，可以将启动程序安装到相应的位置。grub2 第一个要安装的就是 boot.img。它由 boot.S 编译而成，一共 512 字节，正式安装到启动盘的第一个扇区。这个扇区通常称为 MBR（Master Boot Record，主引导记录 / 扇区）。BIOS 完成任务后，会将 boot.img 从硬盘加载到内存中的 0x7c00 来运行。由于 512 个字节实在有限，boot.img 做不了太多的事情。它能做的最重要的一个事情就是加载 grub2 的另一个镜像 core.img。引导扇区就是你找到的门卫，虽然他看着档案库的大门，但是知道的事情很少。他不知道你的宝典在哪里，但是，他知道应该问谁。门卫说，档案库入口处有个管理处，然后把你领到门口。core.img 就是管理处，它们知道的和能做的事情就多了一些。core.img 由 lzma_decompress.img、diskboot.img、kernel.img 和一系列的模块组成，功能比较丰富，能做很多事情。 boot.img 先加载的是 core.img 的第一个扇区。如果从硬盘启动的话，这个扇区里面是 diskboot.img，对应的代码是 diskboot.S。boot.img 将控制权交给 diskboot.img 后，diskboot.img 的任务就是将 core.img 的其他部分加载进来，先是解压缩程序 lzma_decompress.img，再往下是 kernel.img，最后是各个模块 module 对应的映像。这里需要注意，它不是 Linux 的内核，而是 grub 的内核。lzma_decompress.img 对应的代码是 startup_raw.S，本来 kernel.img 是压缩过的，现在执行的时候，需要解压缩。在这之前，我们所有遇到过的程序都非常非常小，完全可以在实模式下运行，但是随着我们加载的东西越来越大，实模式这 1M 的地址空间实在放不下了，所以在真正的解压缩之前，lzma_decompress.img 做了一个重要的决定，就是调用 real_to_prot，切换到保护模式，这样就能在更大的寻址空间里面，加载更多的东西。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:9:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"从实模式切换到保护模式 好了，管理处听说你要找宝典，知道你将来是要做老板的人。既然是老板，早晚都要雇人干活的。这不是个体户小打小闹，所以，你需要切换到老板角色，进入保护模式了，把哪些是你的权限，哪些是你可以授权给别人的，都分得清清楚楚。切换到保护模式要干很多工作，大部分工作都与内存的访问方式有关。第一项是启用分段，就是在内存里面建立段描述符表，将寄存器里面的段寄存器变成段选择子，指向某个段描述符，这样就能实现不同进程的切换了。第二项是启动分页。能够管理的内存变大了，就需要将内存分成相等大小的块，这些我们放到内存那一节详细再讲。 切换到了老板角色，也是为了招聘很多人，同时接多个项目，这时候就需要划清界限，懂得集权与授权。当了老板，眼界要宽多了，同理保护模式需要做一项工作，那就是打开 Gate A20，也就是第 21 根地址线的控制线。在实模式 8086 下面，一共就 20 个地址线，可访问 1M 的地址空间。如果超过了这个限度怎么办呢？当然是绕回来了。在保护模式下，第 21 根要起作用了，于是我们就需要打开 Gate A20。切换保护模式的函数 DATA32 call real_to_prot 会打开 Gate A20，也就是第 21 根地址线的控制线。现在好了，有的是空间了。接下来我们要对压缩过的 kernel.img 进行解压缩，然后跳转到 kernel.img 开始运行。切换到了老板角色，你可以正大光明地进入档案馆，寻找你的那本宝典。 kernel.img 对应的代码是 startup.S 以及一堆 c 文件，在 startup.S 中会调用 grub_main，这是 grub kernel 的主函数。在这个函数里面，grub_load_config() 开始解析，我们上面写的那个 grub.conf 文件里的配置信息。如果是正常启动，grub_main 最后会调用 grub_command_execute (“normal”, 0, 0)，最终会调用 grub_normal_execute() 函数。在这个函数里面，grub_show_menu() 会显示出让你选择的那个操作系统的列表。 同理，作为老板，你发现这类的宝典不止一本，经营企业的方式也有很多种，到底是人性化的，还是强纪律的，这个时候你要做一个选择。一旦，你选定了某个宝典，启动某个操作系统，就要开始调用 grub_menu_execute_entry() ，开始解析并执行你选择的那一项。接下来你的经营企业之路就此打开了。例如里面的 linux16 命令，表示装载指定的内核文件，并传递内核启动参数。于是 grub_cmd_linux() 函数会被调用，它会首先读取 Linux 内核镜像头部的一些数据结构，放到内存中的数据结构来，进行检查。如果检查通过，则会读取整个 Linux 内核镜像到内存。如果配置文件里面还有 initrd 命令，用于为即将启动的内核传递 init ramdisk 路径。于是 grub_cmd_initrd() 函数会被调用，将 initramfs 加载到内存中来。当这些事情做完之后，grub_command_execute (“boot”, 0, 0) 才开始真正地启动内核。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:9:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 启动的过程比较复杂，我这里画一个图，让你比较形象地理解这个过程。你可以根据我讲的，自己来梳理一遍这个过程，做到不管是从流程还是细节上，都能心中有数。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:9:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"08 | 内核初始化：生意做大了就得成立公司 上一节，你获得了一本《企业经营宝典》，完成了一件大事，切换到了老板角色，从实模式切换到了保护模式。有了更强的寻址能力，接下来，我们就要按照宝典里面的指引，开始经营企业了。内核的启动从入口函数 start_kernel() 开始。在 init/main.c 文件中，start_kernel 相当于内核的 main 函数。打开这个函数，你会发现，里面是各种各样初始化函数 XXXX_init。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:10:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"初始化公司职能部门 于是，公司要开始建立各种职能部门了。首先是项目管理部门。咱们将来肯定要接各种各样的项目，因此，项目管理体系和项目管理流程首先要建立起来。之前讲的创建项目都是复制老项目，现在咱们需要有第一个全新的项目。这个项目需要你这个老板来打个样。在操作系统里面，先要有个创始进程，有一行指令 set_task_stack_end_magic(\u0026init_task)。这里面有一个参数 init_task，它的定义是 struct task_struct init_task = INIT_TASK(init_task)。它是系统创建的第一个进程，我们称为 0 号进程。这是唯一一个没有通过 fork 或者 kernel_thread 产生的进程，是进程列表的第一个。 所谓进程列表（Process List），就是咱们前面说的项目管理工具，里面列着我们所有接的项目。第二个要初始化的就是办事大厅。有了办事大厅，我们就可以响应客户的需求。 这里面对应的函数是 trap_init()，里面设置了很多中断门（Interrupt Gate），用于处理各种中断。其中有一个 set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_32)，这是系统调用的中断门。系统调用也是通过发送中断的方式进行的。当然，64 位的有另外的系统调用方法，这一点我们放到后面的系统调用章节详细谈。接下来要初始化的是咱们的会议室管理系统。对应的，mm_init() 就是用来初始化内存管理模块。 项目需要项目管理进行调度，需要执行一定的调度策略。sched_init() 就是用于初始化调度模块。vfs_caches_init() 会用来初始化基于内存的文件系统 rootfs。在这个函数里面，会调用 mnt_init()-\u003einit_rootfs()。这里面有一行代码，register_filesystem(\u0026rootfs_fs_type)。在 VFS 虚拟文件系统里面注册了一种类型，我们定义为 struct file_system_type rootfs_fs_type。文件系统是我们的项目资料库，为了兼容各种各样的文件系统，我们需要将文件的相关数据结构和操作抽象出来，形成一个抽象层对上提供统一的接口，这个抽象层就是 VFS（Virtual File System），虚拟文件系统。这里的 rootfs 还有其他用处，下面我们会用到。 最后，start_kernel() 调用的是 rest_init()，用来做其他方面的初始化，这里面做了好多的工作。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:10:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"初始化 1 号进程 rest_init 的第一大工作是，用 kernel_thread(kernel_init, NULL, CLONE_FS) 创建第二个进程，这个是 1 号进程。 1 号进程对于操作系统来讲，有“划时代”的意义。因为它将运行一个用户进程，这意味着这个公司把一个老板独立完成的制度，变成了可以交付他人完成的制度。这个 1 号进程就相当于老板带了一个大徒弟，有了第一个，就有第二个，后面大徒弟开枝散叶，带了很多徒弟，形成一棵进程树。一旦有了用户进程，公司的运行模式就要发生一定的变化。因为原来你是老板，没有雇佣其他人，所有东西都是你的，无论多么关键的资源，第一，不会有人给你抢，第二，不会有人恶意破坏、恶意使用。但是现在有了其他人，你就要开始做一定的区分，哪些是核心资源，哪些是非核心资源；办公区也要分开，有普通的项目人员都能访问的项目工作区，还有职业核心人员能够访问的核心保密区。好在 x86 提供了分层的权限机制，把区域分成了四个 Ring，越往里权限越高，越往外权限越低。 操作系统很好地利用了这个机制，将能够访问关键资源的代码放在 Ring0，我们称为内核态（Kernel Mode）；将普通的程序代码放在 Ring3，我们称为用户态（User Mode）。你别忘了，现在咱们的系统已经处于保护模式了，保护模式除了可访问空间大一些，还有另一个重要功能，就是“保护”，也就是说，当处于用户态的代码想要执行更高权限的指令，这种行为是被禁止的，要防止他们为所欲为。如果用户态的代码想要访问核心资源，怎么办呢？咱们不是有提供系统调用的办事大厅吗？这里是统一的入口，用户态代码在这里请求就是了。办事大厅后面就是内核态，用户态代码不用管后面发生了什么，做完了返回结果就可以了。当一个用户态的程序运行到一半，要访问一个核心资源，例如访问网卡发一个网络包，就需要暂停当前的运行，调用系统调用，接下来就轮到内核中的代码运行了。首先，内核将从系统调用传过来的包，在网卡上排队，轮到的时候就发送。发送完了，系统调用就结束了，返回用户态，让暂停运行的程序接着运行。 这个暂停怎么实现呢？其实就是把程序运行到一半的情况保存下来。例如，我们知道，内存是用来保存程序运行时候的中间结果的，现在要暂时停下来，这些中间结果不能丢，因为再次运行的时候，还要基于这些中间结果接着来。另外就是，当前运行到代码的哪一行了，当前的栈在哪里，这些都是在寄存器里面的。所以，暂停的那一刻，要把当时 CPU 的寄存器的值全部暂存到一个地方，这个地方可以放在进程管理系统很容易获取的地方。在后面讨论进程管理数据结构的时候，我们还会详细讲。当系统调用完毕，返回的时候，再从这个地方将寄存器的值恢复回去，就能接着运行了。 这个过程就是这样的：用户态 - 系统调用 - 保存寄存器 - 内核态执行系统调用 - 恢复寄存器 - 返回用户态，然后接着运行。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:10:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"从内核态到用户态 我们再回到 1 号进程启动的过程。当前执行 kernel_thread 这个函数的时候，我们还在内核态，现在我们就来跨越这道屏障，到用户态去运行一个程序。这该怎么办呢？很少听说“先内核态再用户态”的。kernel_thread 的参数是一个函数 kernel_init，也就是这个进程会运行这个函数。在 kernel_init 里面，会调用 kernel_init_freeable()，里面有这样的代码： if (!ramdisk_execute_command) ramdisk_execute_command = \"/init\"; 先不管 ramdisk 是啥，我们回到 kernel_init 里面。这里面有这样的代码块： if (ramdisk_execute_command) { ret = run_init_process(ramdisk_execute_command); ...... } ...... if (!try_to_run_init_process(\"/sbin/init\") || !try_to_run_init_process(\"/etc/init\") || !try_to_run_init_process(\"/bin/init\") || !try_to_run_init_process(\"/bin/sh\")) return 0; 这就说明，1 号进程运行的是一个文件。如果我们打开 run_init_process 函数，会发现它调用的是 do_execve。这个名字是不是看起来很熟悉？前面讲系统调用的时候，execve 是一个系统调用，它的作用是运行一个执行文件。加一个 do_ 的往往是内核系统调用的实现。没错，这就是一个系统调用，它会尝试运行 ramdisk 的“/init”，或者普通文件系统上的“/sbin/init”“/etc/init”“/bin/init”“/bin/sh”。不同版本的 Linux 会选择不同的文件启动，但是只要有一个起来了就可以。 static int run_init_process(const char *init_filename) { argv_init[0] = init_filename; return do_execve(getname_kernel(init_filename), (const char __user *const __user *)argv_init, (const char __user *const __user *)envp_init); } 如何利用执行 init 文件的机会，从内核态回到用户态呢？我们从系统调用的过程可以得到启发，“用户态 - 系统调用 - 保存寄存器 - 内核态执行系统调用 - 恢复寄存器 - 返回用户态”，然后接着运行。而咱们刚才运行 init，是调用 do_execve，正是上面的过程的后半部分，从内核态执行系统调用开始。do_execve-\u003edo_execveat_common-\u003eexec_binprm-\u003esearch_binary_handler，这里面会调用这段内容： int search_binary_handler(struct linux_binprm *bprm) { ...... struct linux_binfmt *fmt; ...... retval = fmt-\u003eload_binary(bprm); ...... } 也就是说，我要运行一个程序，需要加载这个二进制文件，这就是我们常说的项目执行计划书。它是有一定格式的。Linux 下一个常用的格式是 ELF（Executable and Linkable Format，可执行与可链接格式）。于是我们就有了下面这个定义： static struct linux_binfmt elf_format = { .module = THIS_MODULE, .load_binary = load_elf_binary, .load_shlib = load_elf_library, .core_dump = elf_core_dump, .min_coredump = ELF_EXEC_PAGESIZE, }; 这其实就是先调用 load_elf_binary，最后调用 start_thread。 void start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp) { set_user_gs(regs, 0); regs-\u003efs = 0; regs-\u003eds = __USER_DS; regs-\u003ees = __USER_DS; regs-\u003ess = __USER_DS; regs-\u003ecs = __USER_CS; regs-\u003eip = new_ip; regs-\u003esp = new_sp; regs-\u003eflags = X86_EFLAGS_IF; force_iret(); } EXPORT_SYMBOL_GPL(start_thread); 看到这里，你是不是有点感觉了？struct pt_regs，看名字里的 register，就是寄存器啊！这个结构就是在系统调用的时候，内核中保存用户态运行上下文的，里面将用户态的代码段 CS 设置为 __USER_CS，将用户态的数据段 DS 设置为 __USER_DS，以及指令指针寄存器 IP、栈指针寄存器 SP。这里相当于补上了原来系统调用里，保存寄存器的一个步骤。最后的 iret 是干什么的呢？它是用于从系统调用中返回。这个时候会恢复寄存器。从哪里恢复呢？按说是从进入系统调用的时候，保存的寄存器里面拿出。好在上面的函数补上了寄存器。CS 和指令指针寄存器 IP 恢复了，指向用户态下一个要执行的语句。DS 和函数栈指针 SP 也被恢复了，指向用户态函数栈的栈顶。所以，下一条指令，就从用户态开始运行了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:10:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"ramdisk 的作用 init 终于从内核到用户态了。一开始到用户态的是 ramdisk 的 init，后来会启动真正根文件系统上的 init，成为所有用户态进程的祖先。为什么会有 ramdisk 这个东西呢？还记得上一节咱们内核启动的时候，配置过这个参数： initrd16 /boot/initramfs-3.10.0-862.el7.x86_64.img 就是这个东西，这是一个基于内存的文件系统。为啥会有这个呢？是因为刚才那个 init 程序是在文件系统上的，文件系统一定是在一个存储设备上的，例如硬盘。Linux 访问存储设备，要有驱动才能访问。如果存储系统数目很有限，那驱动可以直接放到内核里面，反正前面我们加载过内核到内存里了，现在可以直接对存储系统进行访问。但是存储系统越来越多了，如果所有市面上的存储系统的驱动都默认放进内核，内核就太大了。这该怎么办呢？我们只好先弄一个基于内存的文件系统。内存访问是不需要驱动的，这个就是 ramdisk。这个时候，ramdisk 是根文件系统。 然后，我们开始运行 ramdisk 上的 /init。等它运行完了就已经在用户态了。/init 这个程序会先根据存储系统的类型加载驱动，有了驱动就可以设置真正的根文件系统了。有了真正的根文件系统，ramdisk 上的 /init 会启动文件系统上的 init。接下来就是各种系统的初始化。启动系统的服务，启动控制台，用户就可以登录进来了。先别忙着高兴，rest_init 的第一个大事情才完成。我们仅仅形成了用户态所有进程的祖先。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:10:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"创建 2 号进程 用户态的所有进程都有大师兄了，那内核态的进程有没有一个人统一管起来呢？有的，rest_init 第二大事情就是第三个进程，就是 2 号进程。 kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES) 又一次使用 kernel_thread 函数创建进程。这里需要指出一点，函数名 thread 可以翻译成“线程”，这也是操作系统很重要的一个概念。它和进程有什么区别呢？为什么这里创建的是进程，函数名却是线程呢？从用户态来看，创建进程其实就是立项，也就是启动一个项目。这个项目包含很多资源，例如会议室、资料库等。这些东西都属于这个项目，但是这个项目需要人去执行。有多个人并行执行不同的部分，这就叫多线程（Multithreading）。如果只有一个人，那它就是这个项目的主线程。 但是从内核态来看，无论是进程，还是线程，我们都可以统称为任务（Task），都使用相同的数据结构，平放在同一个链表中。这些在进程的那一章节，我会更加详细地讲。这里的函数 kthreadd，负责所有内核态的线程的调度和管理，是内核态所有线程运行的祖先。这下好了，用户态和内核态都有人管了，可以开始接项目了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:10:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们讲了内核的初始化过程，主要做了以下几件事情： 各个职能部门的创建；用户态祖先进程的创建；内核态祖先进程的创建。 咱们还是用一个图来总结一下这个过程。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:10:6","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"09 | 系统调用：公司成立好了就要开始接项目 上一节，系统终于进入了用户态，公司由一个“皮包公司”进入正轨，可以开始接项目了。这一节，我们来解析 Linux 接项目的办事大厅是如何实现的，这是因为后面介绍的每一个模块，都涉及系统调用。站在系统调用的角度，层层深入下去，就能从某个系统调用的场景出发，了解内核中各个模块的实现机制。有的时候，我们的客户觉得，直接去办事大厅还是不够方便。没问题，Linux 还提供了 glibc 这个中介。它更熟悉系统调用的细节，并且可以封装成更加友好的接口。你可以直接用。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:11:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"glibc 对系统调用的封装 我们以最常用的系统调用 open，打开一个文件为线索，看看系统调用是怎么实现的。这一节我们仅仅会解析到从 glibc 如何调用到内核的 open，至于 open 怎么实现，怎么打开一个文件，留到文件系统那一节讲。现在我们就开始在用户态进程里面调用 open 函数。为了方便，大部分用户会选择使用中介，也就是说，调用的是 glibc 里面的 open 函数。这个函数是如何定义的呢？ int open(const char *pathname, int flags, mode_t mode) 在 glibc 的源代码中，有个文件 syscalls.list，里面列着所有 glibc 的函数对应的系统调用，就像下面这个样子： # File name Caller Syscall name Args Strong name Weak names open - open Ci:siv __libc_open __open open 另外，glibc 还有一个脚本 make-syscall.sh，可以根据上面的配置文件，对于每一个封装好的系统调用，生成一个文件。这个文件里面定义了一些宏，例如 #define SYSCALL_NAME open。glibc 还有一个文件 syscall-template.S，使用上面这个宏，定义了这个系统调用的调用方式。 T_PSEUDO (SYSCALL_SYMBOL, SYSCALL_NAME, SYSCALL_NARGS) ret T_PSEUDO_END (SYSCALL_SYMBOL) #define T_PSEUDO(SYMBOL, NAME, N) PSEUDO (SYMBOL, NAME, N) 这里的 PSEUDO 也是一个宏，它的定义如下： #define PSEUDO(name, syscall_name, args) \\ .text; \\ ENTRY (name) \\ DO_CALL (syscall_name, args); \\ cmpl $-4095, %eax; \\ jae SYSCALL_ERROR_LABEL 里面对于任何一个系统调用，会调用 DO_CALL。这也是一个宏，这个宏 32 位和 64 位的定义是不一样的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:11:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"32 位系统调用过程 我们先来看 32 位的情况（i386 目录下的 sysdep.h 文件）。 /* Linux takes system call arguments in registers: syscall number %eax call-clobbered arg 1 %ebx call-saved arg 2 %ecx call-clobbered arg 3 %edx call-clobbered arg 4 %esi call-saved arg 5 %edi call-saved arg 6 %ebp call-saved ...... */ #define DO_CALL(syscall_name, args) \\ PUSHARGS_##args \\ DOARGS_##args \\ movl $SYS_ify (syscall_name), %eax; \\ ENTER_KERNEL \\ POPARGS_##args 这里，我们将请求参数放在寄存器里面，根据系统调用的名称，得到系统调用号，放在寄存器 eax 里面，然后执行 ENTER_KERNEL。在 Linux 的源代码注释里面，我们可以清晰地看到，这些寄存器是如何传递系统调用号和参数的。这里面的 ENTER_KERNEL 是什么呢？ # define ENTER_KERNEL int $0x80 int 就是 interrupt，也就是“中断”的意思。int $0x80 就是触发一个软中断，通过它就可以陷入（trap）内核。在内核启动的时候，还记得有一个 trap_init()，其中有这样的代码： set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_32); 这是一个软中断的陷入门。当接收到一个系统调用的时候，entry_INT80_32 就被调用了。 ENTRY(entry_INT80_32) ASM_CLAC pushl %eax /* pt_regs-\u003eorig_ax */ SAVE_ALL pt_regs_ax=$-ENOSYS /* save rest */ movl %esp, %eax call do_syscall_32_irqs_on .Lsyscall_32_done: ...... .Lirq_return: INTERRUPT_RETURN 通过 push 和 SAVE_ALL 将当前用户态的寄存器，保存在 pt_regs 结构里面。进入内核之前，保存所有的寄存器，然后调用 do_syscall_32_irqs_on。它的实现如下： static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs) { struct thread_info *ti = current_thread_info(); unsigned int nr = (unsigned int)regs-\u003eorig_ax; ...... if (likely(nr \u003c IA32_NR_syscalls)) { regs-\u003eax = ia32_sys_call_table[nr]( (unsigned int)regs-\u003ebx, (unsigned int)regs-\u003ecx, (unsigned int)regs-\u003edx, (unsigned int)regs-\u003esi, (unsigned int)regs-\u003edi, (unsigned int)regs-\u003ebp); } syscall_return_slowpath(regs); } 在这里，我们看到，将系统调用号从 eax 里面取出来，然后根据系统调用号，在系统调用表中找到相应的函数进行调用，并将寄存器中保存的参数取出来，作为函数参数。如果仔细比对，就能发现，这些参数所对应的寄存器，和 Linux 的注释是一样的。根据宏定义，#define ia32_sys_call_table sys_call_table，系统调用就是放在这个表里面。至于这个表是如何形成的，我们后面讲。当系统调用结束之后，在 entry_INT80_32 之后，紧接着调用的是 INTERRUPT_RETURN，我们能够找到它的定义，也就是 iret。 #define INTERRUPT_RETURN iret iret 指令将原来用户态保存的现场恢复回来，包含代码段、指令指针寄存器等。这时候用户态进程恢复执行。这里我总结一下 32 位的系统调用是如何执行的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:11:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"64 位系统调用过程 我们再来看 64 位的情况（x86_64 下的 sysdep.h 文件）。 /* The Linux/x86-64 kernel expects the system call parameters in registers according to the following table: syscall number rax arg 1 rdi arg 2 rsi arg 3 rdx arg 4 r10 arg 5 r8 arg 6 r9 ...... */ #define DO_CALL(syscall_name, args) \\ lea SYS_ify (syscall_name), %rax; \\ syscall 和之前一样，还是将系统调用名称转换为系统调用号，放到寄存器 rax。这里是真正进行调用，不是用中断了，而是改用 syscall 指令了。并且，通过注释我们也可以知道，传递参数的寄存器也变了。syscall 指令还使用了一种特殊的寄存器，我们叫特殊模块寄存器（Model Specific Registers，简称 MSR）。这种寄存器是 CPU 为了完成某些特殊控制功能为目的的寄存器，其中就有系统调用。在系统初始化的时候，trap_init 除了初始化上面的中断模式，这里面还会调用 cpu_init-\u003esyscall_init。这里面有这样的代码： wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64); rdmsr 和 wrmsr 是用来读写特殊模块寄存器的。MSR_LSTAR 就是这样一个特殊的寄存器，当 syscall 指令调用的时候，会从这个寄存器里面拿出函数地址来调用，也就是调用 entry_SYSCALL_64。在 arch/x86/entry/entry_64.S 中定义了 entry_SYSCALL_64。 ENTRY(entry_SYSCALL_64) /* Construct struct pt_regs on stack */ pushq $__USER_DS /* pt_regs-\u003ess */ pushq PER_CPU_VAR(rsp_scratch) /* pt_regs-\u003esp */ pushq %r11 /* pt_regs-\u003eflags */ pushq $__USER_CS /* pt_regs-\u003ecs */ pushq %rcx /* pt_regs-\u003eip */ pushq %rax /* pt_regs-\u003eorig_ax */ pushq %rdi /* pt_regs-\u003edi */ pushq %rsi /* pt_regs-\u003esi */ pushq %rdx /* pt_regs-\u003edx */ pushq %rcx /* pt_regs-\u003ecx */ pushq $-ENOSYS /* pt_regs-\u003eax */ pushq %r8 /* pt_regs-\u003er8 */ pushq %r9 /* pt_regs-\u003er9 */ pushq %r10 /* pt_regs-\u003er10 */ pushq %r11 /* pt_regs-\u003er11 */ sub $(6*8), %rsp /* pt_regs-\u003ebp, bx, r12-15 not saved */ movq PER_CPU_VAR(current_task), %r11 testl $_TIF_WORK_SYSCALL_ENTRY|_TIF_ALLWORK_MASK, TASK_TI_flags(%r11) jnz entry_SYSCALL64_slow_path ...... entry_SYSCALL64_slow_path: /* IRQs are off. */ SAVE_EXTRA_REGS movq %rsp, %rdi call do_syscall_64 /* returns with IRQs disabled */ return_from_SYSCALL_64: RESTORE_EXTRA_REGS TRACE_IRQS_IRETQ movq RCX(%rsp), %rcx movq RIP(%rsp), %r11 movq R11(%rsp), %r11 ...... syscall_return_via_sysret: /* rcx and r11 are already restored (see code above) */ RESTORE_C_REGS_EXCEPT_RCX_R11 movq RSP(%rsp), %rsp USERGS_SYSRET64 这里先保存了很多寄存器到 pt_regs 结构里面，例如用户态的代码段、数据段、保存参数的寄存器，然后调用 entry_SYSCALL64_slow_pat-\u003edo_syscall_64。 __visible void do_syscall_64(struct pt_regs *regs) { struct thread_info *ti = current_thread_info(); unsigned long nr = regs-\u003eorig_ax; ...... if (likely((nr \u0026 __SYSCALL_MASK) \u003c NR_syscalls)) { regs-\u003eax = sys_call_table[nr \u0026 __SYSCALL_MASK]( regs-\u003edi, regs-\u003esi, regs-\u003edx, regs-\u003er10, regs-\u003er8, regs-\u003er9); } syscall_return_slowpath(regs); } 在 do_syscall_64 里面，从 rax 里面拿出系统调用号，然后根据系统调用号，在系统调用表 sys_call_table 中找到相应的函数进行调用，并将寄存器中保存的参数取出来，作为函数参数。如果仔细比对，你就能发现，这些参数所对应的寄存器，和 Linux 的注释又是一样的。所以，无论是 32 位，还是 64 位，都会到系统调用表 sys_call_table 这里来。在研究系统调用表之前，我们看 64 位的系统调用返回的时候，执行的是 USERGS_SYSRET64。定义如下： #define USERGS_SYSRET64 \\ swapgs; \\ sysretq; 这里，返回用户态的指令变成了 sysretq。我们这里总结一下 64 位的系统调用是如何执行的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:11:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"系统调用表 前面我们重点关注了系统调用的方式，都是最终到了系统调用表，但是到底调用内核的什么函数，还没有解读。现在我们再来看，系统调用表 sys_call_table 是怎么形成的呢？32 位的系统调用表定义在 arch/x86/entry/syscalls/syscall_32.tbl 文件里。例如 open 是这样定义的： 5 i386 open sys_open compat_sys_open 64 位的系统调用定义在另一个文件 arch/x86/entry/syscalls/syscall_64.tbl 里。例如 open 是这样定义的： 2 common open sys_open 第一列的数字是系统调用号。可以看出，32 位和 64 位的系统调用号是不一样的。第三列是系统调用的名字，第四列是系统调用在内核的实现函数。不过，它们都是以 sys_ 开头。系统调用在内核中的实现函数要有一个声明。声明往往在 include/linux/syscalls.h 文件中。例如 sys_open 是这样声明的： asmlinkage long sys_open(const char __user *filename, int flags, umode_t mode); 真正的实现这个系统调用，一般在一个.c 文件里面，例如 sys_open 的实现在 fs/open.c 里面，但是你会发现样子很奇怪。 SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) { if (force_o_largefile()) flags |= O_LARGEFILE; return do_sys_open(AT_FDCWD, filename, flags, mode); } SYSCALL_DEFINE3 是一个宏系统调用最多六个参数，根据参数的数目选择宏。具体是这样定义的： #define SYSCALL_DEFINE1(name, ...) SYSCALL_DEFINEx(1, _##name, __VA_ARGS__) #define SYSCALL_DEFINE2(name, ...) SYSCALL_DEFINEx(2, _##name, __VA_ARGS__) #define SYSCALL_DEFINE3(name, ...) SYSCALL_DEFINEx(3, _##name, __VA_ARGS__) #define SYSCALL_DEFINE4(name, ...) SYSCALL_DEFINEx(4, _##name, __VA_ARGS__) #define SYSCALL_DEFINE5(name, ...) SYSCALL_DEFINEx(5, _##name, __VA_ARGS__) #define SYSCALL_DEFINE6(name, ...) SYSCALL_DEFINEx(6, _##name, __VA_ARGS__) #define SYSCALL_DEFINEx(x, sname, ...) \\ SYSCALL_METADATA(sname, x, __VA_ARGS__) \\ __SYSCALL_DEFINEx(x, sname, __VA_ARGS__) #define __PROTECT(...) asmlinkage_protect(__VA_ARGS__) #define __SYSCALL_DEFINEx(x, name, ...) \\ asmlinkage long sys##name(__MAP(x,__SC_DECL,__VA_ARGS__)) \\ __attribute__((alias(__stringify(SyS##name)))); \\ static inline long SYSC##name(__MAP(x,__SC_DECL,__VA_ARGS__)); \\ asmlinkage long SyS##name(__MAP(x,__SC_LONG,__VA_ARGS__)); \\ asmlinkage long SyS##name(__MAP(x,__SC_LONG,__VA_ARGS__)) \\ { \\ long ret = SYSC##name(__MAP(x,__SC_CAST,__VA_ARGS__)); \\ __MAP(x,__SC_TEST,__VA_ARGS__); \\ __PROTECT(x, ret,__MAP(x,__SC_ARGS,__VA_ARGS__)); \\ return ret; \\ } \\ static inline long SYSC##name(__MAP(x,__SC_DECL,__VA_ARGS__) 如果我们把宏展开之后，实现如下，和声明的是一样的。 asmlinkage long sys_open(const char __user * filename, int flags, int mode) { long ret; if (force_o_largefile()) flags |= O_LARGEFILE; ret = do_sys_open(AT_FDCWD, filename, flags, mode); asmlinkage_protect(3, ret, filename, flags, mode); return ret; 声明和实现都好了。接下来，在编译的过程中，需要根据 syscall_32.tbl 和 syscall_64.tbl 生成自己的 unistd_32.h 和 unistd_64.h。生成方式在 arch/x86/entry/syscalls/Makefile 中。这里面会使用两个脚本，其中第一个脚本 arch/x86/entry/syscalls/syscallhdr.sh，会在文件中生成 #define __NR_open；第二个脚本 arch/x86/entry/syscalls/syscalltbl.sh，会在文件中生成 __SYSCALL(_NR_open, sys_open)。这样，unistd_32.h 和 unistd_64.h 是对应的系统调用号和系统调用实现函数之间的对应关系。在文件 arch/x86/entry/syscall_32.c，定义了这样一个表，里面 include 了这个头文件，从而所有的 sys 系统调用都在这个表里面了。 __visible const sys_call_ptr_t ia32_sys_call_table[__NR_syscall_compat_max+1] = { /* * Smells like a compiler bug -- it doesn't work * when the \u0026 below is removed. */ [0 ... __NR_syscall_compat_max] = \u0026sys_ni_syscall, #include \u003casm/syscalls_32.h\u003e }; 同理，在文件 arch/x86/entry/syscall_64.c，定义了这样一个表，里面 include 了这个头文件，这样所有的 sys_ 系统调用就都在这个表里面了。 /* System call table for x86-64. */ asmlinkage const sys_call_ptr_t sys_call_table[__NR_syscall_max+1] = { /* * Smells like a compiler bug -- it doesn't work * when the \u0026 below is removed. */ [0 ... __NR_syscall_max] = \u0026sys_ni_syscall, #include \u003casm/syscalls_64.h\u003e }; ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:11:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 系统调用的过程还是挺复杂的吧？如果加上上一节的内核态和用户态的模式切换，就更复杂了。这里我们重点分析 64 位的系统调用，我将整个完整的过程画了一张图，帮你总结、梳理一下。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:11:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"10 | 进程：公司接这么多项目，如何管？ 有了系统调用，咱们公司就能开始批量接项目啦！对应到 Linux 操作系统，就是可以创建进程了。在命令行那一节，我们讲了使用命令创建 Linux 进程的几种方式。现在学习了系统调用，你是不是想尝试一下，如何通过写代码使用系统调用创建一个进程呢？我们一起来看看。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:12:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"写代码：用系统调用创建进程 在 Linux 上写程序和编译程序，也需要一系列的开发套件，就像 Visual Studio 一样。运行下面的命令，就可以在 centOS 7 操作系统上安装开发套件。在以后的章节里面，我们的实验都是基于 centOS 7 操作系统进行的。 yum -y groupinstall \"Development Tools\" 接下来，我们要开始写程序了。在 Windows 上写的程序，都会被保存成.h 或者.c 文件，容易让人感觉这是某种有特殊格式的文件，但其实这些文件只是普普通通的文本文件。因而在 Linux 上，我们用 Vim 来创建并编辑一个文件就行了。我们先来创建一个文件，里面用一个函数封装通用的创建进程的逻辑，名字叫 process.c，代码如下： #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003csys/types.h\u003e #include \u003cunistd.h\u003e extern int create_process (char* program, char** arg_list); int create_process (char* program, char** arg_list) { pid_t child_pid; child_pid = fork (); if (child_pid != 0) return child_pid; else { execvp (program, arg_list); abort (); } } 这里面用到了咱们学过的 fork 系统调用，通过这里面的 if-else，我们可以看到，根据 fork 的返回值不同，父进程和子进程就此分道扬镳了。在子进程里面，我们需要通过 execvp 运行一个新的程序。接下来我们创建第二个文件，调用上面这个函数。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003csys/types.h\u003e #include \u003cunistd.h\u003e extern int create_process (char* program, char** arg_list); int main () { char* arg_list[] = { \"ls\", \"-l\", \"/etc/yum.repos.d/\", NULL }; create_process (\"ls\", arg_list); return 0; } 在这里，我们创建的子程序运行了一个最最简单的命令 ls。学过命令行的那一节之后，这里你应该很熟悉了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:12:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"进行编译：程序的二进制格式 程序写完了，是不是很简单？你可能要问了，这是不是就是我们所谓的项目执行计划书了呢？当然不是了，这两个文件只是文本文件，CPU 是不能执行文本文件里面的指令的，这些指令只有人能看懂，CPU 能够执行的命令是二进制的，比如“0101”这种，所以这些指令还需要翻译一下，这个翻译的过程就是编译（Compile）。编译好的二进制文件才是项目执行计划书。现在咱们是正规的公司了，接项目要有章法，项目执行计划书也要有统一的格式，这样才能保证无论项目交到哪个项目组手里，都能以固定的流程执行。按照里面的指令来，项目也能达到预期的效果。在 Linux 下面，二进制的程序也要有严格的格式，这个格式我们称为 ELF（Executeable and Linkable Format，可执行与可链接格式）。这个格式可以根据编译的结果不同，分为不同的格式。接下来我们看一下，如何从文本文件编译成二进制格式。 在上面两段代码中，上面 include 的部分是头文件，而我们写的这个.c 结尾的是源文件。接下来我们编译这两个程序。 gcc -c -fPIC process.c gcc -c -fPIC createprocess.c 在编译的时候，先做预处理工作，例如将头文件嵌入到正文中，将定义的宏展开，然后就是真正的编译过程，最终编译成为.o 文件，这就是 ELF 的第一种类型，可重定位文件（Relocatable File）。这个文件的格式是这样的： ELF 文件的头是用于描述整个文件的。这个文件格式在内核中有定义，分别为 struct elf32_hdr 和 struct elf64_hdr。接下来我们来看一个一个的 section，我们也叫节。这里面的名字有点晦涩，不过你可以猜一下它们是干什么的。这个编译好的二进制文件里面，应该是代码，还有一些全局变量、静态变量等等。没错，我们依次来看。 .text：放编译好的二进制可执行代码.data：已经初始化好的全局变量.rodata：只读数据，例如字符串常量、const 的变量.bss：未初始化全局变量，运行时会置 0.symtab：符号表，记录的则是函数和变量.strtab：字符串表、字符串常量和变量名 为啥这里只有全局变量呢？其实前面我们讲函数栈的时候说过，局部变量是放在栈里面的，是程序运行过程中随时分配空间，随时释放的，现在我们讨论的是二进制文件，还没启动呢，所以只需要讨论在哪里保存全局变量。这些节的元数据信息也需要有一个地方保存，就是最后的节头部表（Section Header Table）。在这个表里面，每一个 section 都有一项，在代码里面也有定义 struct elf32_shdr 和 struct elf64_shdr。在 ELF 的头里面，有描述这个文件的节头部表的位置，有多少个表项等等信息。 我们刚才说了可重定位，为啥叫可重定位呢？我们可以想象一下，这个编译好的代码和变量，将来加载到内存里面的时候，都是要加载到一定位置的。比如说，调用一个函数，其实就是跳到这个函数所在的代码位置执行；再比如修改一个全局变量，也是要到变量的位置那里去修改。但是现在这个时候，还是.o 文件，不是一个可以直接运行的程序，这里面只是部分代码片段。例如这里的 create_process 函数，将来被谁调用，在哪里调用都不清楚，就更别提确定位置了。所以，.o 里面的位置是不确定的，但是必须是可重新定位的，因为它将来是要做函数库的嘛，就是一块砖，哪里需要哪里搬，搬到哪里就重新定位这些代码、变量的位置。有的 section，例如.rel.text, .rel.data 就与重定位有关。例如这里的 createprocess.o，里面调用了 create_process 函数，但是这个函数在另外一个.o 里面，因而 createprocess.o 里面根本不可能知道被调用函数的位置，所以只好在 rel.text 里面标注，这个函数是需要重定位的。要想让 create_process 这个函数作为库文件被重用，不能以.o 的形式存在，而是要形成库文件，最简单的类型是静态链接库.a 文件（Archives），仅仅将一系列对象文件（.o）归档为一个文件，使用命令 ar 创建。 ar cr libstaticprocess.a process.o 虽然这里 libstaticprocess.a 里面只有一个.o，但是实际情况可以有多个.o。当有程序要使用这个静态连接库的时候，会将.o 文件提取出来，链接到程序中。 gcc -o staticcreateprocess createprocess.o -L. -lstaticprocess 在这个命令里，-L 表示在当前目录下找.a 文件，-lstaticprocess 会自动补全文件名，比如加前缀 lib，后缀.a，变成 libstaticprocess.a，找到这个.a 文件后，将里面的 process.o 取出来，和 createprocess.o 做一个链接，形成二进制执行文件 staticcreateprocess。这个链接的过程，重定位就起作用了，原来 createprocess.o 里面调用了 create_process 函数，但是不能确定位置，现在将 process.o 合并了进来，就知道位置了。形成的二进制文件叫可执行文件，是 ELF 的第二种格式，格式如下： 这个格式和.o 文件大致相似，还是分成一个个的 section，并且被节头表描述。只不过这些 section 是多个.o 文件合并过的。但是这个时候，这个文件已经是马上就可以加载到内存里面执行的文件了，因而这些 section 被分成了需要加载到内存里面的代码段、数据段和不需要加载到内存里面的部分，将小的 section 合成了大的段 segment，并且在最前面加一个段头表（Segment Header Table）。在代码里面的定义为 struct elf32_phdr 和 struct elf64_phdr，这里面除了有对于段的描述之外，最重要的是 p_vaddr，这个是这个段加载到内存的虚拟地址。在 ELF 头里面，有一项 e_entry，也是个虚拟地址，是这个程序运行的入口。当程序运行起来之后，就是下面这个样子： # ./staticcreateprocess # total 40 -rw-r--r--. 1 root root 1572 Oct 24 18:38 CentOS-Base.repo ...... 静态链接库一旦链接进去，代码和变量的 section 都合并了，因而程序运行的时候，就不依赖于这个库是否存在。但是这样有一个缺点，就是相同的代码段，如果被多个程序使用的话，在内存里面就有多份，而且一旦静态链接库更新了，如果二进制执行文件不重新编译，也不随着更新。因而就出现了另一种，动态链接库（Shared Libraries），不仅仅是一组对象文件的简单归档，而是多个对象文件的重新组合，可被多个程序共享。 gcc -shared -fPIC -o libdynamicprocess.so process.o 当一个动态链接库被链接到一个程序文件中的时候，最后的程序文件并不包括动态链接库中的代码，而仅仅包括对动态链接库的引用，并且不保存动态链接库的全路径，仅仅保存动态链接库的名称。 gcc -o dynamiccreateprocess createprocess.o -L. -ldynamicprocess 当运行这个程序的时候，首先寻找动态链接库，然后加载它。默认情况下，系统在 /lib 和 /usr/lib 文件夹下寻找动态链接库。如果找不到就会报错，我们可以设定 LD_LIBRARY_PATH 环境变量，程序运行时会在此环境变量指定的文件夹下寻找动态链接库。 # export LD_LIBRARY_PATH=. # ./dynamiccreateprocess # total 40 -rw-r--r--. 1 root root 1572 Oct 24 18:38 CentOS-Base.repo ...... 动态链接库，就是 ELF 的第三种类型，共享对象文件（Shared Object）。基于动态链接库创建出来的二进制文件格式还是 ELF，但是稍有不同。首先，多了一个.interp 的 Segment，这里面是 ld-linux.so，这是动态链接器，也就是说，运行时的链接动作都是它做的。另外，ELF 文件中还多了两个 section，一个是.plt，过程链接表（Procedure Linkage Table，PLT），一个是.got.plt，全局偏移量表（Global Offset Table，GOT）。它们是怎么工作的，使得程序运行的时候，可以将 so 文件动态链接到进程空间的呢？dynamiccreateprocess 这个程序要调用 libdynamicprocess.so 里的 create_process 函数。由于是运行时才去找，编译的时候，压根不知道这个函数在哪里，所以就在 PLT 里面建立一项 PLT[x]。这一项也是一些代码，有点像一个本地的代理，在二进制程序里面，不直接调用 create_process 函数，而是调用 PLT[x]里面的代理代码，这个代理代码会在运行的时候找真正的 create_process 函数。去哪里找代理代码呢？这就用到了 GOT，这里面也会为 create_process 函数创建一项 GOT[y]。这一项是运行时 creat","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:12:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"运行程序为进程 知道了 ELF 这个格式，这个时候它还是个程序，那怎么把这个文件加载到内存里面呢？在内核中，有这样一个数据结构，用来定义加载二进制文件的方法。 struct linux_binfmt { struct list_head lh; struct module *module; int (*load_binary)(struct linux_binprm *); int (*load_shlib)(struct file *); int (*core_dump)(struct coredump_params *cprm); unsigned long min_coredump; /* minimal dump size */ } __randomize_layout; 对于 ELF 文件格式，有对应的实现。 static struct linux_binfmt elf_format = { .module = THIS_MODULE, .load_binary = load_elf_binary, .load_shlib = load_elf_library, .core_dump = elf_core_dump, .min_coredump = ELF_EXEC_PAGESIZE, }; load_elf_binary 是不是你很熟悉？没错，我们加载内核镜像的时候，用的也是这种格式。还记得当时是谁调用的 load_elf_binary 函数吗？具体是这样的：do_execve-\u003edo_execveat_common-\u003eexec_binprm-\u003esearch_binary_handler。那 do_execve 又是被谁调用的呢？我们看下面的代码。 SYSCALL_DEFINE3(execve, const char __user *, filename, const char __user *const __user *, argv, const char __user *const __user *, envp) { return do_execve(getname(filename), argv, envp); } 学过了系统调用一节，你会发现，原理是 exec 这个系统调用最终调用的 load_elf_binary。exec 比较特殊，它是一组函数： 包含 p 的函数（execvp, execlp）会在 PATH 路径下面寻找程序；不包含 p 的函数需要输入程序的全路径；包含 v 的函数（execv, execvp, execve）以数组的形式接收参数；包含 l 的函数（execl, execlp, execle）以列表的形式接收参数；包含 e 的函数（execve, execle）以数组的形式接收环境变量。 在上面 process.c 的代码中，我们创建 ls 进程，也是通过 exec。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:12:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"进程树 既然所有的进程都是从父进程 fork 过来的，那总归有一个祖宗进程，这就是咱们系统启动的 init 进程。 在解析 Linux 的启动过程的时候，1 号进程是 /sbin/init。如果在 centOS 7 里面，我们 ls 一下，可以看到，这个进程是被软链接到 systemd 的。 /sbin/init -\u003e ../lib/systemd/systemd 系统启动之后，init 进程会启动很多的 daemon 进程，为系统运行提供服务，然后就是启动 getty，让用户登录，登录后运行 shell，用户启动的进程都是通过 shell 运行的，从而形成了一棵进程树。我们可以通过 ps -ef 命令查看当前系统启动的进程，我们会发现有三类进程。 [root@deployer ~]# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 2018 ? 00:00:29 /usr/lib/systemd/systemd --system --deserialize 21 root 2 0 0 2018 ? 00:00:00 [kthreadd] root 3 2 0 2018 ? 00:00:00 [ksoftirqd/0] root 5 2 0 2018 ? 00:00:00 [kworker/0:0H] root 9 2 0 2018 ? 00:00:40 [rcu_sched] ...... root 337 2 0 2018 ? 00:00:01 [kworker/3:1H] root 380 1 0 2018 ? 00:00:00 /usr/lib/systemd/systemd-udevd root 415 1 0 2018 ? 00:00:01 /sbin/auditd root 498 1 0 2018 ? 00:00:03 /usr/lib/systemd/systemd-logind ...... root 852 1 0 2018 ? 00:06:25 /usr/sbin/rsyslogd -n root 2580 1 0 2018 ? 00:00:00 /usr/sbin/sshd -D root 29058 2 0 Jan03 ? 00:00:01 [kworker/1:2] root 29672 2 0 Jan04 ? 00:00:09 [kworker/2:1] root 30467 1 0 Jan06 ? 00:00:00 /usr/sbin/crond -n root 31574 2 0 Jan08 ? 00:00:01 [kworker/u128:2] ...... root 32792 2580 0 Jan10 ? 00:00:00 sshd: root@pts/0 root 32794 32792 0 Jan10 pts/0 00:00:00 -bash root 32901 32794 0 00:01 pts/0 00:00:00 ps -ef 你会发现，PID 1 的进程就是我们的 init 进程 systemd，PID 2 的进程是内核线程 kthreadd，这两个我们在内核启动的时候都见过。其中用户态的不带中括号，内核态的带中括号。接下来进程号依次增大，但是你会看所有带中括号的内核态的进程，祖先都是 2 号进程。而用户态的进程，祖先都是 1 号进程。tty 那一列，是问号的，说明不是前台启动的，一般都是后台的服务。pts 的父进程是 sshd，bash 的父进程是 pts，ps -ef 这个命令的父进程是 bash。这样整个链条都比较清晰了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:12:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节我们讲了一个进程从代码到二进制到运行时的一个过程，我们用一个图总结一下。我们首先通过图右边的文件编译过程，生成 so 文件和可执行文件，放在硬盘上。下图左边的用户态的进程 A 执行 fork，创建进程 B，在进程 B 的处理逻辑中，执行 exec 系列系统调用。这个系统调用会通过 load_elf_binary 方法，将刚才生成的可执行文件，加载到进程 B 的内存中执行。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:12:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"11 | 线程：如何让复杂的项目并行执行？ ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:13:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"为什么要有线程？ 其实，对于任何一个进程来讲，即便我们没有主动去创建线程，进程也是默认有一个主线程的。线程是负责执行二进制指令的，它会根据项目执行计划书，一行一行执行下去。进程要比线程管的宽多了，除了执行指令之外，内存、文件系统等等都要它来管。所以，进程相当于一个项目，而线程就是为了完成项目需求，而建立的一个个开发任务。默认情况下，你可以建一个大的任务，就是完成某某功能，然后交给一个人让它从头做到尾，这就是主线程。但是有时候，你发现任务是可以拆解的，如果相关性没有非常大前后关联关系，就可以并行执行。例如，你接到了一个开发任务，要开发 200 个页面，最后组成一个网站。这时候你就可以拆分成 20 个任务，每个任务 10 个页面，并行开发。都开发完了，再做一次整合，这肯定比依次开发 200 个页面快多了。 那我们能不能成立多个项目组实现并行开发呢？当然可以了，只不过这样做有两个比较麻烦的地方。第一个麻烦是，立项。涉及的部门比较多，总是劳师动众。你本来想的是，只要能并行执行任务就可以，不需要把会议室都搞成独立的。另一个麻烦是，项目组是独立的，会议室是独立的，很多事情就不受你控制了，例如一旦有了两个项目组，就会有沟通问题。所以，使用进程实现并行执行的问题也有两个。第一，创建进程占用资源太多；第二，进程之间的通信需要数据在不同的内存空间传来传去，无法共享。除了希望任务能够并行执行，有的时候，你作为项目管理人员，肯定要管控风险，因此还会预留一部分人作为应急小分队，来处理紧急的事情。例如，主线程正在一行一行执行二进制命令，突然收到一个通知，要做一点小事情，应该停下主线程来做么？太耽误事情了，应该创建一个单独的线程，单独处理这些事件。另外，咱们希望自己的公司越来越有竞争力。要想实现远大的目标，我们不能把所有人力都用在接项目上，应该预留一些人力来做技术积累，比如开发一些各个项目都能用到的共享库、框架等等。在 Linux 中，有时候我们希望将前台的任务和后台的任务分开。因为有些任务是需要马上返回结果的，例如你输入了一个字符，不可能五分钟再显示出来；而有些任务是可以默默执行的，例如将本机的数据同步到服务器上去，这个就没刚才那么着急。因此这样两个任务就应该在不同的线程处理，以保证互不耽误。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:13:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"如何创建线程？ 看来多线程还是有很多好处的。接下来我们来看一下，如何使用线程来干一件大事。假如说，现在我们有 N 个非常大的视频需要下载，一个个下载需要的时间太长了。按照刚才的思路，我们可以拆分成 N 个任务，分给 N 个线程各自去下载。我们知道，进程的执行是需要项目执行计划书的，那线程是一个项目小组，这个小组也应该有自己的项目执行计划书，也就是一个函数。我们将要执行的子任务放在这个函数里面，比如上面的下载任务。这个函数参数是 void 类型的指针，用于接收任何类型的参数。我们就可以将要下载的文件的文件名通过这个指针传给它。 为了方便，我将代码整段都贴在这里，这样你把下面的代码放在一个文件里面就能成功编译。当然，这里我们不是真的下载这个文件，而仅仅打印日志，并生成一个一百以内的随机数，作为下载时间返回。这样，每个子任务干活的同时在喊：“我正在下载，终于下载完了，用了多少时间。” #include \u003cpthread.h\u003e #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #define NUM_OF_TASKS 5 void *downloadfile(void *filename) { printf(\"I am downloading the file %s!\\n\", (char *)filename); sleep(10); long downloadtime = rand()%100; printf(\"I finish downloading the file within %d minutes!\\n\", downloadtime); pthread_exit((void *)downloadtime); } int main(int argc, char *argv[]) { char files[NUM_OF_TASKS][20]={\"file1.avi\",\"file2.rmvb\",\"file3.mp4\",\"file4.wmv\",\"file5.flv\"}; pthread_t threads[NUM_OF_TASKS]; int rc; int t; int downloadtime; pthread_attr_t thread_attr; pthread_attr_init(\u0026thread_attr); pthread_attr_setdetachstate(\u0026thread_attr,PTHREAD_CREATE_JOINABLE); for(t=0;t\u003cNUM_OF_TASKS;t++){ printf(\"creating thread %d, please help me to download %s\\n\", t, files[t]); rc = pthread_create(\u0026threads[t], \u0026thread_attr, downloadfile, (void *)files[t]); if (rc){ printf(\"ERROR; return code from pthread_create() is %d\\n\", rc); exit(-1); } } pthread_attr_destroy(\u0026thread_attr); for(t=0;t\u003cNUM_OF_TASKS;t++){ pthread_join(threads[t],(void**)\u0026downloadtime); printf(\"Thread %d downloads the file %s in %d minutes.\\n\",t,files[t],downloadtime); } pthread_exit(NULL); } 一个运行中的线程可以调用 pthread_exit 退出线程。这个函数可以传入一个参数转换为 (void *) 类型。这是线程退出的返回值。接下来，我们来看主线程。在这里面，我列了五个文件名。接下来声明了一个数组，里面有五个 pthread_t 类型的线程对象。接下来，声明一个线程属性 pthread_attr_t。我们通过 pthread_attr_init 初始化这个属性，并且设置属性 PTHREAD_CREATE_JOINABLE。这表示将来主线程程等待这个线程的结束，并获取退出时的状态。接下来是一个循环。对于每一个文件和每一个线程，可以调用 pthread_create 创建线程。一共有四个参数，第一个参数是线程对象，第二个参数是线程的属性，第三个参数是线程运行函数，第四个参数是线程运行函数的参数。主线程就是通过第四个参数，将自己的任务派给子线程。任务分配完毕，每个线程下载一个文件，接下来主线程要做的事情就是等待这些子任务完成。当一个线程退出的时候，就会发送信号给其他所有同进程的线程。有一个线程使用 pthread_join 获取这个线程退出的返回值。线程的返回值通过 pthread_join 传给主线程，这样子线程就将自己下载文件所耗费的时间，告诉给主线程。好了，程序写完了，开始编译。多线程程序要依赖于 libpthread.so。 gcc download.c -lpthread 编译好了，执行一下，就能得到下面的结果。 # ./a.out creating thread 0, please help me to download file1.avi creating thread 1, please help me to download file2.rmvb I am downloading the file file1.avi! creating thread 2, please help me to download file3.mp4 I am downloading the file file2.rmvb! creating thread 3, please help me to download file4.wmv I am downloading the file file3.mp4! creating thread 4, please help me to download file5.flv I am downloading the file file4.wmv! I am downloading the file file5.flv! I finish downloading the file within 83 minutes! I finish downloading the file within 77 minutes! I finish downloading the file within 86 minutes! I finish downloading the file within 15 minutes! I finish downloading the file within 93 minutes! Thread 0 downloads the file file1.avi in 83 minutes. Thread 1 downloads the file file2.rmvb in 86 minutes. Thread 2 downloads the file file3.mp4 in 77 minutes. Thread 3 downloads the file file4.wmv in 93 minutes. Thread 4 downloads the file file5.flv in 15 minutes. 这里我们画一张图总结一下，一个普通线程的创建和运行过程。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:13:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"线程的数据 线程可以将项目并行起来，加快进度，但是也带来的负面影响，过程并行起来了，那数据呢？我们把线程访问的数据细分成三类。下面我们一一来看。 第一类是线程栈上的本地数据，比如函数执行过程中的局部变量。前面我们说过，函数的调用会使用栈的模型，这在线程里面是一样的。只不过每个线程都有自己的栈空间。栈的大小可以通过命令 ulimit -a 查看，默认情况下线程栈大小为 8192（8MB）。我们可以使用命令 ulimit -s 修改。对于线程栈，可以通过下面这个函数 pthread_attr_t，修改线程栈的大小。 int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize); 主线程在内存中有一个栈空间，其他线程栈也拥有独立的栈空间。为了避免线程之间的栈空间踩踏，线程栈之间还会有小块区域，用来隔离保护各自的栈空间。一旦另一个线程踏入到这个隔离区，就会引发段错误。第二类数据就是在整个进程里共享的全局数据。例如全局变量，虽然在不同进程中是隔离的，但是在一个进程中是共享的。如果同一个全局变量，两个线程一起修改，那肯定会有问题，有可能把数据改的面目全非。这就需要有一种机制来保护他们，比如你先用我再用。这一节的最后，我们专门来谈这个问题。那线程能不能像进程一样，也有自己的私有数据呢？如果想声明一个线程级别，而非进程级别的全局变量，有没有什么办法呢？虽然咱们都是一个大组，分成小组，也应该有点隐私。这就是第三类数据，线程私有数据（Thread Specific Data），可以通过以下函数创建： int pthread_key_create(pthread_key_t *key, void (*destructor)(void*)) 可以看到，创建一个 key，伴随着一个析构函数。key 一旦被创建，所有线程都可以访问它，但各线程可根据自己的需要往 key 中填入不同的值，这就相当于提供了一个同名而不同值的全局变量。我们可以通过下面的函数设置 key 对应的 value。 int pthread_setspecific(pthread_key_t key, const void *value) 我们还可以通过下面的函数获取 key 对应的 value。 void *pthread_getspecific(pthread_key_t key) 而等到线程退出的时候，就会调用析构函数释放 value。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:13:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"数据的保护 接下来，我们来看共享的数据保护问题。我们先来看一种方式，Mutex，全称 Mutual Exclusion，中文叫互斥。顾名思义，有你没我，有我没你。它的模式就是在共享数据访问的时候，去申请加把锁，谁先拿到锁，谁就拿到了访问权限，其他人就只好在门外等着，等这个人访问结束，把锁打开，其他人再去争夺，还是遵循谁先拿到谁访问。我这里构建了一个“转账”的场景。相关的代码我放到这里，你可以看看。 #include \u003cpthread.h\u003e #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #define NUM_OF_TASKS 5 int money_of_tom = 100; int money_of_jerry = 100; //第一次运行去掉下面这行 pthread_mutex_t g_money_lock; void *transfer(void *notused) { pthread_t tid = pthread_self(); printf(\"Thread %u is transfering money!\\n\", (unsigned int)tid); //第一次运行去掉下面这行 pthread_mutex_lock(\u0026g_money_lock); sleep(rand()%10); money_of_tom+=10; sleep(rand()%10); money_of_jerry-=10; //第一次运行去掉下面这行 pthread_mutex_unlock(\u0026g_money_lock); printf(\"Thread %u finish transfering money!\\n\", (unsigned int)tid); pthread_exit((void *)0); } int main(int argc, char *argv[]) { pthread_t threads[NUM_OF_TASKS]; int rc; int t; //第一次运行去掉下面这行 pthread_mutex_init(\u0026g_money_lock, NULL); for(t=0;t\u003cNUM_OF_TASKS;t++){ rc = pthread_create(\u0026threads[t], NULL, transfer, NULL); if (rc){ printf(\"ERROR; return code from pthread_create() is %d\\n\", rc); exit(-1); } } for(t=0;t\u003c100;t++){ //第一次运行去掉下面这行 pthread_mutex_lock(\u0026g_money_lock); printf(\"money_of_tom + money_of_jerry = %d\\n\", money_of_tom + money_of_jerry); //第一次运行去掉下面这行 pthread_mutex_unlock(\u0026g_money_lock); } //第一次运行去掉下面这行 pthread_mutex_destroy(\u0026g_money_lock); pthread_exit(NULL); } 这里说，有两个员工 Tom 和 Jerry，公司食堂的饭卡里面各自有 100 元，并行启动 5 个线程，都是 Jerry 转 10 元给 Tom，主线程不断打印 Tom 和 Jerry 的资金之和。按说，这样的话，总和应该永远是 200 元。在上面的程序中，我们先去掉 mutex 相关的行，就像注释里面写的那样。在没有锁的保护下，在 Tom 的账户里面加上 10 元，在 Jerry 的账户里面减去 10 元，这不是一个原子操作。我们来编译一下。 gcc mutex.c -lpthread 然后运行一下，就看到了下面这样的结果。 [root@deployer createthread]# ./a.out Thread 508479232 is transfering money! Thread 491693824 is transfering money! Thread 500086528 is transfering money! Thread 483301120 is transfering money! Thread 516871936 is transfering money! money_of_tom + money_of_jerry = 200 money_of_tom + money_of_jerry = 200 money_of_tom + money_of_jerry = 220 money_of_tom + money_of_jerry = 220 money_of_tom + money_of_jerry = 230 money_of_tom + money_of_jerry = 240 Thread 483301120 finish transfering money! money_of_tom + money_of_jerry = 240 Thread 508479232 finish transfering money! Thread 500086528 finish transfering money! money_of_tom + money_of_jerry = 220 Thread 516871936 finish transfering money! money_of_tom + money_of_jerry = 210 money_of_tom + money_of_jerry = 210 Thread 491693824 finish transfering money! money_of_tom + money_of_jerry = 200 money_of_tom + money_of_jerry = 200 可以看到，中间有很多状态不正确，比如两个人的账户之和出现了超过 200 的情况，也就是 Tom 转入了，Jerry 还没转出。接下来我们在上面的代码里面，加上 mutex，然后编译、运行，就得到了下面的结果。 [root@deployer createthread]# ./a.out Thread 568162048 is transfering money! Thread 576554752 is transfering money! Thread 551376640 is transfering money! Thread 542983936 is transfering money! Thread 559769344 is transfering money! Thread 568162048 finish transfering money! Thread 576554752 finish transfering money! money_of_tom + money_of_jerry = 200 money_of_tom + money_of_jerry = 200 money_of_tom + money_of_jerry = 200 Thread 542983936 finish transfering money! Thread 559769344 finish transfering money! money_of_tom + money_of_jerry = 200 money_of_tom + money_of_jerry = 200 Thread 551376640 finish transfering money! money_of_tom + money_of_jerry = 200 money_of_tom + money_of_jerry = 200 money_of_tom + money_of_jerry = 200 money_of_tom + money_of_jerry = 200 这个结果就正常了。两个账号之和永远是 200。这下你看到锁的作用了吧？使用 Mutex，首先要使用 pthread_mutex_init 函数初始化这个 mutex，初始化后，就可以用它来保护共享变量了。pthread_mutex_lock() 就是去抢那把锁的函数，如果抢到了，就可以执行下一行程序，对共享变量进行访问；如果没抢到，就被阻塞在那里等待。如果不想被阻塞，可以使用 pthread_mutex_trylock 去抢那把锁，如果抢到了，就可以执行下一行程序，对共享变量进行访问；如果没抢到，不会被阻塞，而是返回一个错误码。当共享数据访问结束了，别忘了使用 pthread_mutex_unlock 释放锁，让给其他人使用，最终调用 pthread_mutex_destroy 销毁掉这把锁。这里我画个图，总结一下 Mutex 的使用流程。 在使用 Mutex 的时候，有个问题是如果使用 pthread_mutex_lock()，那就需要一直在那里等着。如果是 pthread_mutex_trylock()，就可以不用等着，去干点儿别的，但是我怎么知道什么时候回来再试一下，是不是轮到我了呢？能不能在轮到我的时候，通知我一下呢？这其实就是条件变量，也就是说如果没事儿，就让大家歇着，有事儿了就去通知，别让人家没事儿就来问问，浪费大家的时间。但是当它接到了通知，来操作共享资源的时候，还是需要抢互斥锁，因为可能很多人都受","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:13:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们讲了如何创建线程，线程都有哪些数据，如何对线程数据进行保护。写多线程的程序是有套路的，我这里用一张图进行总结。你需要记住的是，创建线程的套路、mutex 使用的套路、条件变量使用的套路。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:13:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"12 | 进程数据结构（上）：项目多了就需要项目管理系统 前面两节，我们讲了如何使用系统调用，创建进程和线程。你是不是觉得进程和线程管理，还挺复杂的呢？如此复杂的体系，在内核里面应该如何管理呢？有的进程只有一个线程，有的进程有多个线程，它们都需要由内核分配 CPU 来干活。可是 CPU 总共就这么几个，应该怎么管理，怎么调度呢？你是老板，这个事儿得你来操心。首先，我们得明确，公司的项目售前售后人员，接来了这么多的项目，这是个好事儿。这些项目都通过办事大厅立了项的，有的需要整个项目组一起开发，有的是一个项目组分成多个小组并行开发。无论哪种模式，到你这个老板这里，都需要有一个项目管理体系，进行统一排期、统一管理和统一协调。这样，你才能对公司的业务了如指掌。那具体应该怎么做呢？还记得咱们平时开发的时候，用的项目管理软件 Jira 吧？它的办法对我们来讲，就很有参考意义。我们这么来看，其实，无论是一个大的项目组一起完成一个大的功能（单体应用模式），还是把一个大的功能拆成小的功能并行开发（微服务模式），这些都是开发组根据客户的需求来定的，项目经理没办法决定，但是从项目经理的角度来看，这些都是任务，需要同样关注进度、协调资源等等。同样在 Linux 里面，无论是进程，还是线程，到了内核里面，我们统一都叫任务（Task），由一个统一的结构 task_struct 进行管理。这个结构非常复杂，但你也不用怕，我们慢慢来解析。 接下来，我们沿着建立项目管理体系的思路，设想一下，Linux 的任务管理都应该干些啥？首先，所有执行的项目应该有个项目列表吧，所以 Linux 内核也应该先弄一个链表，将所有的 task_struct 串起来。 struct list_head tasks; 接下来，我们来看每一个任务都应该包含哪些字段。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:14:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"任务 ID 每一个任务都应该有一个 ID，作为这个任务的唯一标识。到时候排期啊、下发任务啊等等，都按 ID 来，就不会产生歧义。task_struct 里面涉及任务 ID 的，有下面几个： pid_t pid; pid_t tgid; struct task_struct *group_leader; 你可能觉得奇怪，既然是 ID，有一个就足以做唯一标识了，这个怎么看起来这么麻烦？这是因为，上面的进程和线程到了内核这里，统一变成了任务，这就带来两个问题。 第一个问题是，任务展示。 啥是任务展示呢？这么说吧，你作为老板，想了解的肯定是，公司都接了哪些项目，每个项目多少营收。什么项目执行是不是分了小组，每个小组是啥情况，这些细节，项目经理没必要全都展示给你看。前面我们学习命令行的时候，知道 ps 命令可以展示出所有的进程。但是如果你是这个命令的实现者，到了内核，按照上面的任务列表把这些命令都显示出来，把所有的线程全都平摊开来显示给用户。用户肯定觉得既复杂又困惑。复杂在于，列表这么长；困惑在于，里面出现了很多并不是自己创建的线程。 第二个问题是，给任务下发指令。 如果客户突然给项目组提个新的需求，比如说，有的客户觉得项目已经完成，可以终止；再比如说，有的客户觉得项目做到一半没必要再进行下去了，可以中止，这时候应该给谁发指令？当然应该给整个项目组，而不是某个小组。我们不能让客户看到，不同的小组口径不一致。这就好比说，中止项目的指令到达一个小组，这个小组很开心就去休息了，同一个项目组的其他小组还干的热火朝天的。Linux 也一样，前面我们学习命令行的时候，知道可以通过 kill 来给进程发信号，通知进程退出。如果发给了其中一个线程，我们就不能只退出这个线程，而是应该退出整个进程。当然，有时候，我们希望只给某个线程发信号。 所以在内核中，它们虽然都是任务，但是应该加以区分。其中，pid 是 process id，tgid 是 thread group ID。任何一个进程，如果只有主线程，那 pid 是自己，tgid 是自己，group_leader 指向的还是自己。但是，如果一个进程创建了其他线程，那就会有所变化了。线程有自己的 pid，tgid 就是进程的主线程的 pid，group_leader 指向的就是进程的主线程。好了，有了 tgid，我们就知道 tast_struct 代表的是一个进程还是代表一个线程了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:14:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"信号处理 这里既然提到了下发指令的问题，我就顺便提一下 task_struct 里面关于信号处理的字段。 /* Signal handlers: */ struct signal_struct *signal; struct sighand_struct *sighand; sigset_t blocked; sigset_t real_blocked; sigset_t saved_sigmask; struct sigpending pending; unsigned long sas_ss_sp; size_t sas_ss_size; unsigned int sas_ss_flags; 这里定义了哪些信号被阻塞暂不处理（blocked），哪些信号尚等待处理（pending），哪些信号正在通过信号处理函数进行处理（sighand）。处理的结果可以是忽略，可以是结束进程等等。信号处理函数默认使用用户态的函数栈，当然也可以开辟新的栈专门用于信号处理，这就是 sas_ss_xxx 这三个变量的作用。上面我说了下发信号的时候，需要区分进程和线程。从这里我们其实也能看出一些端倪。task_struct 里面有一个 struct sigpending pending。如果我们进入 struct signal_struct *signal 去看的话，还有一个 struct sigpending shared_pending。它们一个是本任务的，一个是线程组共享的。关于信号，你暂时了解到这里就够用了，后面我们会有单独的章节进行解读。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:14:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"任务状态 作为一个项目经理，另外一个需要关注的是项目当前的状态。例如，在 Jira 里面，任务的运行就可以分成下面的状态。 在 task_struct 里面，涉及任务状态的是下面这几个变量： volatile long state; /* -1 unrunnable, 0 runnable, \u003e0 stopped */ int exit_state; unsigned int flags; state（状态）可以取的值定义在 include/linux/sched.h 头文件中。 /* Used in tsk-\u003estate: */ #define TASK_RUNNING 0 #define TASK_INTERRUPTIBLE 1 #define TASK_UNINTERRUPTIBLE 2 #define __TASK_STOPPED 4 #define __TASK_TRACED 8 /* Used in tsk-\u003eexit_state: */ #define EXIT_DEAD 16 #define EXIT_ZOMBIE 32 #define EXIT_TRACE (EXIT_ZOMBIE | EXIT_DEAD) /* Used in tsk-\u003estate again: */ #define TASK_DEAD 64 #define TASK_WAKEKILL 128 #define TASK_WAKING 256 #define TASK_PARKED 512 #define TASK_NOLOAD 1024 #define TASK_NEW 2048 #define TASK_STATE_MAX 4096 从定义的数值很容易看出来，state 是通过 bitset 的方式设置的，也就是说，当前是什么状态，哪一位就置一。 TASK_RUNNING 并不是说进程正在运行，而是表示进程在时刻准备运行的状态。当处于这个状态的进程获得时间片的时候，就是在运行中；如果没有获得时间片，就说明它被其他进程抢占了，在等待再次分配时间片。在运行中的进程，一旦要进行一些 I/O 操作，需要等待 I/O 完毕，这个时候会释放 CPU，进入睡眠状态。在 Linux 中，有两种睡眠状态。 一种是 TASK_INTERRUPTIBLE，可中断的睡眠状态。这是一种浅睡眠的状态，也就是说，虽然在睡眠，等待 I/O 完成，但是这个时候一个信号来的时候，进程还是要被唤醒。只不过唤醒后，不是继续刚才的操作，而是进行信号处理。当然程序员可以根据自己的意愿，来写信号处理函数，例如收到某些信号，就放弃等待这个 I/O 操作完成，直接退出；或者收到某些信息，继续等待。 另一种睡眠是 TASK_UNINTERRUPTIBLE，不可中断的睡眠状态。这是一种深度睡眠状态，不可被信号唤醒，只能死等 I/O 操作完成。一旦 I/O 操作因为特殊原因不能完成，这个时候，谁也叫不醒这个进程了。你可能会说，我 kill 它呢？别忘了，kill 本身也是一个信号，既然这个状态不可被信号唤醒，kill 信号也被忽略了。除非重启电脑，没有其他办法。 因此，这其实是一个比较危险的事情，除非程序员极其有把握，不然还是不要设置成 TASK_UNINTERRUPTIBLE。于是，我们就有了一种新的进程睡眠状态，TASK_KILLABLE，可以终止的新睡眠状态。进程处于这种状态中，它的运行原理类似 TASK_UNINTERRUPTIBLE，只不过可以响应致命信号。从定义可以看出，TASK_WAKEKILL 用于在接收到致命信号时唤醒进程，而 TASK_KILLABLE 相当于这两位都设置了。 #define TASK_KILLABLE (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE) TASK_STOPPED 是在进程接收到 SIGSTOP、SIGTTIN、SIGTSTP 或者 SIGTTOU 信号之后进入该状态。TASK_TRACED 表示进程被 debugger 等进程监视，进程执行被调试程序所停止。当一个进程被另外的进程所监视，每一个信号都会让进程进入该状态。一旦一个进程要结束，先进入的是 EXIT_ZOMBIE 状态，但是这个时候它的父进程还没有使用 wait() 等系统调用来获知它的终止信息，此时进程就成了僵尸进程。 EXIT_DEAD 是进程的最终状态。EXIT_ZOMBIE 和 EXIT_DEAD 也可以用于 exit_state。上面的进程状态和进程的运行、调度有关系，还有其他的一些状态，我们称为标志。放在 flags 字段中，这些字段都被定义成为宏，以 PF 开头。我这里举几个例子。 #define PF_EXITING 0x00000004 #define PF_VCPU 0x00000010 #define PF_FORKNOEXEC 0x00000040 PF_EXITING 表示正在退出。当有这个 flag 的时候，在函数 find_alive_thread 中，找活着的线程，遇到有这个 flag 的，就直接跳过。PF_VCPU 表示进程运行在虚拟 CPU 上。在函数 account_system_time 中，统计进程的系统运行时间，如果有这个 flag，就调用 account_guest_time，按照客户机的时间进行统计。PF_FORKNOEXEC 表示 fork 完了，还没有 exec。在 _do_fork 函数里面调用 copy_process，这个时候把 flag 设置为 PF_FORKNOEXEC。当 exec 中调用了 load_elf_binary 的时候，又把这个 flag 去掉。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:14:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"进程调度 进程的状态切换往往涉及调度，下面这些字段都是用于调度的。为了让你理解 task_struct 进程管理的全貌，我先在这里列一下，咱们后面会有单独的章节讲解，这里你只要大概看一下里面的注释就好了。 //是否在运行队列上 int on_rq; //优先级 int prio; int static_prio; int normal_prio; unsigned int rt_priority; //调度器类 const struct sched_class *sched_class; //调度实体 struct sched_entity se; struct sched_rt_entity rt; struct sched_dl_entity dl; //调度策略 unsigned int policy; //可以使用哪些CPU int nr_cpus_allowed; cpumask_t cpus_allowed; struct sched_info sched_info; ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:14:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们讲述了进程管理复杂的数据结构，我还是画一个图总结一下。这个图是进程管理 task_struct 的结构图。其中红色的部分是今天讲的部分，你可以对着这张图说出它们的含义。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:14:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"13 | 进程数据结构（中）：项目多了就需要项目管理系统 上一节我们讲了，task_struct 这个结构非常长。由此我们可以看出，Linux 内核的任务管理是非常复杂的。上一节，我们只是讲了一部分，今天我们接着来解析剩下的部分。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:15:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"运行统计信息 作为项目经理，你肯定需要了解项目的运行情况。例如，有的员工很长时间都在做一个任务，这个时候你就需要特别关注一下；再如，有的员工的琐碎任务太多，这会大大影响他的工作效率。那如何才能知道这些员工的工作情况呢？在进程的运行过程中，会有一些统计量，具体你可以看下面的列表。这里面有进程在用户态和内核态消耗的时间、上下文切换的次数等等。 u64 utime;//用户态消耗的CPU时间 u64 stime;//内核态消耗的CPU时间 unsigned long nvcsw;//自愿(voluntary)上下文切换计数 unsigned long nivcsw;//非自愿(involuntary)上下文切换计数 u64 start_time;//进程启动时间，不包含睡眠时间 u64 real_start_time;//进程启动时间，包含睡眠时间 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:15:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"进程亲缘关系 从我们之前讲的创建进程的过程，可以看出，任何一个进程都有父进程。所以，整个进程其实就是一棵进程树。而拥有同一父进程的所有进程都具有兄弟关系。 struct task_struct __rcu *real_parent; /* real parent process */ struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */ struct list_head children; /* list of my children */ struct list_head sibling; /* linkage in my parent's children list */ parent 指向其父进程。当它终止时，必须向它的父进程发送信号。children 表示链表的头部。链表中的所有元素都是它的子进程。sibling 用于把当前进程插入到兄弟链表中。 通常情况下，real_parent 和 parent 是一样的，但是也会有另外的情况存在。例如，bash 创建一个进程，那进程的 parent 和 real_parent 就都是 bash。如果在 bash 上使用 GDB 来 debug 一个进程，这个时候 GDB 是 parent，bash 是这个进程的 real_parent。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:15:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"进程权限 了解了运行统计信息，接下来，我们需要关注一下项目组权限的控制。什么是项目组权限控制呢？这么说吧，我这个项目组能否访问某个文件，能否访问其他的项目组，以及我这个项目组能否被其他项目组访问等等，这都是项目组权限的控制范畴。在 Linux 里面，对于进程权限的定义如下： /* Objective and real subjective task credentials (COW): */ const struct cred __rcu *real_cred; /* Effective (overridable) subjective task credentials (COW): */ const struct cred __rcu *cred; 这个结构的注释里，有两个名词比较拗口，Objective 和 Subjective。事实上，所谓的权限，就是我能操纵谁，谁能操纵我。“谁能操作我”，很显然，这个时候我就是被操作的对象，就是 Objective，那个想操作我的就是 Subjective。“我能操作谁”，这个时候我就是 Subjective，那个要被我操作的就是 Objectvie。“操作”，就是一个对象对另一个对象进行某些动作。当动作要实施的时候，就要审核权限，当两边的权限匹配上了，就可以实施操作。其中，real_cred 就是说明谁能操作我这个进程，而 cred 就是说明我这个进程能够操作谁。这里 cred 的定义如下： struct cred { ...... kuid_t uid; /* real UID of the task */ kgid_t gid; /* real GID of the task */ kuid_t suid; /* saved UID of the task */ kgid_t sgid; /* saved GID of the task */ kuid_t euid; /* effective UID of the task */ kgid_t egid; /* effective GID of the task */ kuid_t fsuid; /* UID for VFS ops */ kgid_t fsgid; /* GID for VFS ops */ ...... kernel_cap_t cap_inheritable; /* caps our children can inherit */ kernel_cap_t cap_permitted; /* caps we're permitted */ kernel_cap_t cap_effective; /* caps we can actually use */ kernel_cap_t cap_bset; /* capability bounding set */ kernel_cap_t cap_ambient; /* Ambient capability set */ ...... } __randomize_layout; 从这里的定义可以看出，大部分是关于用户和用户所属的用户组信息。第一个是 uid 和 gid，注释是 real user/group id。一般情况下，谁启动的进程，就是谁的 ID。但是权限审核的时候，往往不比较这两个，也就是说不大起作用。第二个是 euid 和 egid，注释是 effective user/group id。一看这个名字，就知道这个是起“作用”的。当这个进程要操作消息队列、共享内存、信号量等对象的时候，其实就是在比较这个用户和组是否有权限。第三个是 fsuid 和 fsgid，也就是 filesystem user/group id。这个是对文件操作会审核的权限。一般说来，fsuid、euid，和 uid 是一样的，fsgid、egid，和 gid 也是一样的。因为谁启动的进程，就应该审核启动的用户到底有没有这个权限。但是也有特殊的情况。 例如，用户 A 想玩一个游戏，这个游戏的程序是用户 B 安装的。游戏这个程序文件的权限为 rwxr–r–。A 是没有权限运行这个程序的，所以用户 B 要给用户 A 权限才行。用户 B 说没问题，都是朋友嘛，于是用户 B 就给这个程序设定了所有的用户都能执行的权限 rwxr-xr-x，说兄弟你玩吧。于是，用户 A 就获得了运行这个游戏的权限。当游戏运行起来之后，游戏进程的 uid、euid、fsuid 都是用户 A。看起来没有问题，玩得很开心。用户 A 好不容易通过一关，想保留通关数据的时候，发现坏了，这个游戏的玩家数据是保存在另一个文件里面的。这个文件权限 rw——-，只给用户 B 开了写入权限，而游戏进程的 euid 和 fsuid 都是用户 A，当然写不进去了。完了，这一局白玩儿了。那怎么解决这个问题呢？我们可以通过 chmod u+s program 命令，给这个游戏程序设置 set-user-ID 的标识位，把游戏的权限变成 rwsr-xr-x。这个时候，用户 A 再启动这个游戏的时候，创建的进程 uid 当然还是用户 A，但是 euid 和 fsuid 就不是用户 A 了，因为看到了 set-user-id 标识，就改为文件的所有者的 ID，也就是说，euid 和 fsuid 都改成用户 B 了，这样就能够将通关结果保存下来。在 Linux 里面，一个进程可以随时通过 setuid 设置用户 ID，所以，游戏程序的用户 B 的 ID 还会保存在一个地方，这就是 suid 和 sgid，也就是 saved uid 和 save gid。这样就可以很方便地使用 setuid，通过设置 uid 或者 suid 来改变权限。除了以用户和用户组控制权限，Linux 还有另一个机制就是 capabilities。 原来控制进程的权限，要么是高权限的 root 用户，要么是一般权限的普通用户，这时候的问题是，root 用户权限太大，而普通用户权限太小。有时候一个普通用户想做一点高权限的事情，必须给他整个 root 的权限。这个太不安全了。于是，我们引入新的机制 capabilities，用位图表示权限，在 capability.h 可以找到定义的权限。我这里列举几个。 #define CAP_CHOWN 0 #define CAP_KILL 5 #define CAP_NET_BIND_SERVICE 10 #define CAP_NET_RAW 13 #define CAP_SYS_MODULE 16 #define CAP_SYS_RAWIO 17 #define CAP_SYS_BOOT 22 #define CAP_SYS_TIME 25 #define CAP_AUDIT_READ 37 #define CAP_LAST_CAP CAP_AUDIT_READ 对于普通用户运行的进程，当有这个权限的时候，就能做这些操作；没有的时候，就不能做，这样粒度要小很多。cap_permitted 表示进程能够使用的权限。但是真正起作用的是 cap_effective。cap_permitted 中可以包含 cap_effective 中没有的权限。一个进程可以在必要的时候，放弃自己的某些权限，这样更加安全。假设自己因为代码漏洞被攻破了，但是如果啥也干不了，就没办法进一步突破。cap_inheritable 表示当可执行文件的扩展属性设置了 inheritable 位时，调用 exec 执行该程序会继承调用者的 inheritable 集合，并将其加入到 permitted 集合。但在非 root 用户下执行 exec 时，通常不会保留 inheritable 集合，但是往往又是非 root 用户，才想保留权限，所以非常鸡肋。cap_bset，也就是 capability bounding set，是系统中所有进程允许保留的权限。如果这个集合中不存在某个权限，那么系统中的所有进程都没有这个权限。即使以超级用户权限执行的进程，也是一样的。这样有很多好处。例如，系统启动以后，将加载内核模块的权限去掉，那所有进程都不能加载内核模块。这样，即便这台机器被攻破，也做不了太多有害的事情。 cap_ambient 是比较新加入内核的，就是为了解决 cap_inheritable 鸡肋的状况，也就是，非 root 用户进程使用 exec 执行一个程序的时候，如何保留权限的问题。当执行 exec 的时候，cap_ambient 会被添加到 cap_permitted 中，同时设置到 cap_effective 中。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:15:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"内存管理 每个进程都有自己独立的虚拟内存空间，这需要有一个数据结构来表示，就是 mm_struct。这个我们在内存管理那一节详细讲述。这里你先有个印象。 struct mm_struct *mm; struct mm_struct *active_mm; ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:15:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"文件与文件系统 每个进程有一个文件系统的数据结构，还有一个打开文件的数据结构。这个我们放到文件系统那一节详细讲述。 /* Filesystem information: */ struct fs_struct *fs; /* Open file information: */ struct files_struct *files; ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:15:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们终于把进程管理复杂的数据结构基本讲完了，请你重点记住以下两点：进程亲缘关系维护的数据结构，是一种很有参考价值的实现方式，在内核中会多个地方出现类似的结构；进程权限中 setuid 的原理，这一点比较难理解，但是很重要，面试经常会考。 你可以对着下面这张图，看看自己是否真的理解了，进程树是如何组织的，以及如何控制进程的权限的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:15:6","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"14 | 进程数据结构（下）：项目多了就需要项目管理系统 上两节，我们解读了 task_struct 的大部分的成员变量。这样一个任务执行的方方面面，都可以很好地管理起来，但是其中有一个问题我们没有谈。在程序执行过程中，一旦调用到系统调用，就需要进入内核继续执行。那如何将用户态的执行和内核态的执行串起来呢？这就需要以下两个重要的成员变量： struct thread_info thread_info; void *stack; ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:16:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用户态函数栈 在用户态中，程序的执行往往是一个函数调用另一个函数。函数调用都是通过栈来进行的。我们前面大致讲过函数栈的原理，今天我们仔细分析一下。函数调用其实也很简单。如果你去看汇编语言的代码，其实就是指令跳转，从代码的一个地方跳到另外一个地方。这里比较棘手的问题是，参数和返回地址应该怎么传递过去呢？我们看函数的调用过程，A 调用 B、调用 C、调用 D，然后返回 C、返回 B、返回 A，这是一个后进先出的过程。有没有觉得这个过程很熟悉？没错，咱们数据结构里学的栈，也是后进先出的，所以用栈保存这些最合适。在进程的内存空间里面，栈是一个从高地址到低地址，往下增长的结构，也就是上面是栈底，下面是栈顶，入栈和出栈的操作都是从下面的栈顶开始的。我们先来看 32 位操作系统的情况。在 CPU 里，ESP（Extended Stack Pointer）是栈顶指针寄存器，入栈操作 Push 和出栈操作 Pop 指令，会自动调整 ESP 的值。另外有一个寄存器 EBP（Extended Base Pointer），是栈基地址指针寄存器，指向当前栈帧的最底部。例如，A 调用 B，A 的栈里面包含 A 函数的局部变量，然后是调用 B 的时候要传给它的参数，然后返回 A 的地址，这个地址也应该入栈，这就形成了 A 的栈帧。接下来就是 B 的栈帧部分了，先保存的是 A 栈帧的栈底位置，也就是 EBP。因为在 B 函数里面获取 A 传进来的参数，就是通过这个指针获取的，接下来保存的是 B 的局部变量等等。当 B 返回的时候，返回值会保存在 EAX 寄存器中，从栈中弹出返回地址，将指令跳转回去，参数也从栈中弹出，然后继续执行 A。对于 64 位操作系统，模式多少有些不一样。因为 64 位操作系统的寄存器数目比较多。rax 用于保存函数调用的返回结果。栈顶指针寄存器变成了 rsp，指向栈顶位置。堆栈的 Pop 和 Push 操作会自动调整 rsp，栈基指针寄存器变成了 rbp，指向当前栈帧的起始位置。改变比较多的是参数传递。rdi、rsi、rdx、rcx、r8、r9 这 6 个寄存器，用于传递存储函数调用时的 6 个参数。如果超过 6 的时候，还是需要放到栈里面。然而，前 6 个参数有时候需要进行寻址，但是如果在寄存器里面，是没有地址的，因而还是会放到栈里面，只不过放到栈里面的操作是被调用函数做的。 以上的栈操作，都是在进程的内存空间里面进行的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:16:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"内核态函数栈 接下来，我们通过系统调用，从进程的内存空间到内核中了。内核中也有各种各样的函数调用来调用去的，也需要这样一个机制，这该怎么办呢？这时候，上面的成员变量 stack，也就是内核栈，就派上了用场。Linux 给每个 task 都分配了内核栈。在 32 位系统上 arch/x86/include/asm/page_32_types.h，是这样定义的：一个 PAGE_SIZE 是 4K，左移一位就是乘以 2，也就是 8K。 #define THREAD_SIZE_ORDER 1 #define THREAD_SIZE (PAGE_SIZE \u003c\u003c THREAD_SIZE_ORDER) 内核栈在 64 位系统上 arch/x86/include/asm/page_64_types.h，是这样定义的：在 PAGE_SIZE 的基础上左移两位，也即 16K，并且要求起始地址必须是 8192 的整数倍。 #ifdef CONFIG_KASAN #define KASAN_STACK_ORDER 1 #else #define KASAN_STACK_ORDER 0 #endif #define THREAD_SIZE_ORDER (2 + KASAN_STACK_ORDER) #define THREAD_SIZE (PAGE_SIZE \u003c\u003c THREAD_SIZE_ORDER) 内核栈是一个非常特殊的结构，如下图所示： 这段空间的最低位置，是一个 thread_info 结构。这个结构是对 task_struct 结构的补充。因为 task_struct 结构庞大但是通用，不同的体系结构就需要保存不同的东西，所以往往与体系结构有关的，都放在 thread_info 里面。在内核代码里面有这样一个 union，将 thread_info 和 stack 放在一起，在 include/linux/sched.h 文件中就有。 union thread_union { #ifndef CONFIG_THREAD_INFO_IN_TASK struct thread_info thread_info; #endif unsigned long stack[THREAD_SIZE/sizeof(long)]; }; 这个 union 就是这样定义的，开头是 thread_info，后面是 stack。在内核栈的最高地址端，存放的是另一个结构 pt_regs，定义如下。其中，32 位和 64 位的定义不一样。 #ifdef __i386__ struct pt_regs { unsigned long bx; unsigned long cx; unsigned long dx; unsigned long si; unsigned long di; unsigned long bp; unsigned long ax; unsigned long ds; unsigned long es; unsigned long fs; unsigned long gs; unsigned long orig_ax; unsigned long ip; unsigned long cs; unsigned long flags; unsigned long sp; unsigned long ss; }; #else struct pt_regs { unsigned long r15; unsigned long r14; unsigned long r13; unsigned long r12; unsigned long bp; unsigned long bx; unsigned long r11; unsigned long r10; unsigned long r9; unsigned long r8; unsigned long ax; unsigned long cx; unsigned long dx; unsigned long si; unsigned long di; unsigned long orig_ax; unsigned long ip; unsigned long cs; unsigned long flags; unsigned long sp; unsigned long ss; /* top of stack page */ }; #endif 看到这个是不是很熟悉？咱们在讲系统调用的时候，已经多次见过这个结构。当系统调用从用户态到内核态的时候，首先要做的第一件事情，就是将用户态运行过程中的 CPU 上下文保存起来，其实主要就是保存在这个结构的寄存器变量里。这样当从内核系统调用返回的时候，才能让进程在刚才的地方接着运行下去。如果我们对比系统调用那一节的内容，你会发现系统调用的时候，压栈的值的顺序和 struct pt_regs 中寄存器定义的顺序是一样的。在内核中，CPU 的寄存器 ESP 或者 RSP，已经指向内核栈的栈顶，在内核态里的调用都有和用户态相似的过程。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:16:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"通过 task_struct 找内核栈 如果有一个 task_struct 的 stack 指针在手，你可以通过下面的函数找到这个线程内核栈： static inline void *task_stack_page(const struct task_struct *task) { return task-\u003estack; } 从 task_struct 如何得到相应的 pt_regs 呢？我们可以通过下面的函数： /* * TOP_OF_KERNEL_STACK_PADDING reserves 8 bytes on top of the ring0 stack. * This is necessary to guarantee that the entire \"struct pt_regs\" * is accessible even if the CPU haven't stored the SS/ESP registers * on the stack (interrupt gate does not save these registers * when switching to the same priv ring). * Therefore beware: accessing the ss/esp fields of the * \"struct pt_regs\" is possible, but they may contain the * completely wrong values. */ #define task_pt_regs(task) \\ ({ \\ unsigned long __ptr = (unsigned long)task_stack_page(task); \\ __ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING; \\ ((struct pt_regs *)__ptr) - 1; \\ }) 你会发现，这是先从 task_struct 找到内核栈的开始位置。然后这个位置加上 THREAD_SIZE 就到了最后的位置，然后转换为 struct pt_regs，再减一，就相当于减少了一个 pt_regs 的位置，就到了这个结构的首地址。这里面有一个 TOP_OF_KERNEL_STACK_PADDING，这个的定义如下： #ifdef CONFIG_X86_32 # ifdef CONFIG_VM86 # define TOP_OF_KERNEL_STACK_PADDING 16 # else # define TOP_OF_KERNEL_STACK_PADDING 8 # endif #else # define TOP_OF_KERNEL_STACK_PADDING 0 #endif 也就是说，32 位机器上是 8，其他是 0。这是为什么呢？因为压栈 pt_regs 有两种情况。我们知道，CPU 用 ring 来区分权限，从而 Linux 可以区分内核态和用户态。因此，第一种情况，我们拿涉及从用户态到内核态的变化的系统调用来说。因为涉及权限的改变，会压栈保存 SS、ESP 寄存器的，这两个寄存器共占用 8 个 byte。另一种情况是，不涉及权限的变化，就不会压栈这 8 个 byte。这样就会使得两种情况不兼容。如果没有压栈还访问，就会报错，所以还不如预留在这里，保证安全。在 64 位上，修改了这个问题，变成了定长的。好了，现在如果你 task_struct 在手，就能够轻松得到内核栈和内核寄存器。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:16:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"通过内核栈找 task_struct 那如果一个当前在某个 CPU 上执行的进程，想知道自己的 task_struct 在哪里，又该怎么办呢？这个艰巨的任务要交给 thread_info 这个结构。 struct thread_info { struct task_struct *task; /* main task structure */ __u32 flags; /* low level flags */ __u32 status; /* thread synchronous flags */ __u32 cpu; /* current CPU */ mm_segment_t addr_limit; unsigned int sig_on_uaccess_error:1; unsigned int uaccess_err:1; /* uaccess failed */ }; 这里面有个成员变量 task 指向 task_struct，所以我们常用 current_thread_info()-\u003etask 来获取 task_struct。 static inline struct thread_info *current_thread_info(void) { return (struct thread_info *)(current_top_of_stack() - THREAD_SIZE); } 而 thread_info 的位置就是内核栈的最高位置，减去 THREAD_SIZE，就到了 thread_info 的起始地址。但是现在变成这样了，只剩下一个 flags。 struct thread_info { unsigned long flags; /* low level flags */ }; 那这时候怎么获取当前运行中的 task_struct 呢？current_thread_info 有了新的实现方式。在 include/linux/thread_info.h 中定义了 current_thread_info。 #include \u003casm/current.h\u003e #define current_thread_info() ((struct thread_info *)current) #endif 那 current 又是什么呢？在 arch/x86/include/asm/current.h 中定义了。 struct task_struct; DECLARE_PER_CPU(struct task_struct *, current_task); static __always_inline struct task_struct *get_current(void) { return this_cpu_read_stable(current_task); } #define current get_current 到这里，你会发现，新的机制里面，每个 CPU 运行的 task_struct 不通过 thread_info 获取了，而是直接放在 Per CPU 变量里面了。多核情况下，CPU 是同时运行的，但是它们共同使用其他的硬件资源的时候，我们需要解决多个 CPU 之间的同步问题。Per CPU 变量是内核中一种重要的同步机制。顾名思义，Per CPU 变量就是为每个 CPU 构造一个变量的副本，这样多个 CPU 各自操作自己的副本，互不干涉。比如，当前进程的变量 current_task 就被声明为 Per CPU 变量。要使用 Per CPU 变量，首先要声明这个变量，在 arch/x86/include/asm/current.h 中有： DECLARE_PER_CPU(struct task_struct *, current_task); 然后是定义这个变量，在 arch/x86/kernel/cpu/common.c 中有： DEFINE_PER_CPU(struct task_struct *, current_task) = \u0026init_task; 也就是说，系统刚刚初始化的时候，current_task 都指向 init_task。当某个 CPU 上的进程进行切换的时候，current_task 被修改为将要切换到的目标进程。例如，进程切换函数 __switch_to 就会改变 current_task。 __visible __notrace_funcgraph struct task_struct * __switch_to(struct task_struct *prev_p, struct task_struct *next_p) { ...... this_cpu_write(current_task, next_p); ...... return prev_p; } 当要获取当前的运行中的 task_struct 的时候，就需要调用 this_cpu_read_stable 进行读取。 #define this_cpu_read_stable(var) percpu_stable_op(\"mov\", var) 好了，现在如果你是一个进程，正在某个 CPU 上运行，就能够轻松得到 task_struct 了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:16:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节虽然只介绍了内核栈，但是内容更加重要。如果说 task_struct 的其他成员变量都是和进程管理有关的，内核栈是和进程运行有关系的。我这里画了一张图总结一下 32 位和 64 位的工作模式，左边是 32 位的，右边是 64 位的。 在用户态，应用程序进行了至少一次函数调用。32 位和 64 的传递参数的方式稍有不同，32 位的就是用函数栈，64 位的前 6 个参数用寄存器，其他的用函数栈。在内核态，32 位和 64 位都使用内核栈，格式也稍有不同，主要集中在 pt_regs 结构上。在内核态，32 位和 64 位的内核栈和 task_struct 的关联关系不同。32 位主要靠 thread_info，64 位主要靠 Per-CPU 变量。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:16:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"15 | 调度（上）：如何制定项目管理流程？ 前几节，我们介绍了 task_struct 数据结构。它就像项目管理系统一样，可以帮项目经理维护项目运行过程中的各类信息，但这并不意味着项目管理工作就完事大吉了。task_struct 仅仅能够解决“看到”的问题，咱们还要解决如何制定流程，进行项目调度的问题，也就是“做到”的问题。公司的人员总是有限的。无论接了多少项目，公司不可能短时间增加很多人手。有的项目比较紧急，应该先进行排期；有的项目可以缓缓，但是也不能让客户等太久。所以这个过程非常复杂，需要平衡。对于操作系统来讲，它面对的 CPU 的数量是有限的，干活儿都是它们，但是进程数目远远超过 CPU 的数目，因而就需要进行进程的调度，有效地分配 CPU 的时间，既要保证进程的最快响应，也要保证进程之间的公平。这也是一个非常复杂的、需要平衡的事情。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:17:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"调度策略与调度类 在 Linux 里面，进程大概可以分成两种。一种称为实时进程，也就是需要尽快执行返回结果的那种。这就好比我们是一家公司，接到的客户项目需求就会有很多种。有些客户的项目需求比较急，比如一定要在一两个月内完成的这种，客户会加急加钱，那这种客户的优先级就会比较高。另一种是普通进程，大部分的进程其实都是这种。这就好比，大部分客户的项目都是普通的需求，可以按照正常流程完成，优先级就没实时进程这么高，但是人家肯定也有确定的交付日期。那很显然，对于这两种进程，我们的调度策略肯定是不同的。在 task_struct 中，有一个成员变量，我们叫调度策略。 unsigned int policy; 它有以下几个定义： #define SCHED_NORMAL 0 #define SCHED_FIFO 1 #define SCHED_RR 2 #define SCHED_BATCH 3 #define SCHED_IDLE 5 #define SCHED_DEADLINE 6 配合调度策略的，还有我们刚才说的优先级，也在 task_struct 中。 int prio, static_prio, normal_prio; unsigned int rt_priority; 优先级其实就是一个数值，对于实时进程，优先级的范围是 0～99；对于普通进程，优先级的范围是 100～139。数值越小，优先级越高。从这里可以看出，所有的实时进程都比普通进程优先级要高。毕竟，谁让人家加钱了呢。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:17:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"实时调度策略 对于调度策略，其中 SCHED_FIFO、SCHED_RR、SCHED_DEADLINE 是实时进程的调度策略。虽然大家都是加钱加急的项目，但是也不能乱来，还是需要有个办事流程才行。 例如，SCHED_FIFO 就是交了相同钱的，先来先服务，但是有的加钱多，可以分配更高的优先级，也就是说，高优先级的进程可以抢占低优先级的进程，而相同优先级的进程，我们遵循先来先得。另外一种策略是，交了相同钱的，轮换着来，这就是 SCHED_RR 轮流调度算法，采用时间片，相同优先级的任务当用完时间片会被放到队列尾部，以保证公平性，而高优先级的任务也是可以抢占低优先级的任务。还有一种新的策略是 SCHED_DEADLINE，是按照任务的 deadline 进行调度的。当产生一个调度点的时候，DL 调度器总是选择其 deadline 距离当前时间点最近的那个任务，并调度它执行。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:17:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"普通调度策略 对于普通进程的调度策略有，SCHED_NORMAL、SCHED_BATCH、SCHED_IDLE。既然大家的项目都没有那么紧急，就应该按照普通的项目流程，公平地分配人员。SCHED_NORMAL 是普通的进程，就相当于咱们公司接的普通项目。 SCHED_BATCH 是后台进程，几乎不需要和前端进行交互。这有点像公司在接项目同时，开发一些可以复用的模块，作为公司的技术积累，从而使得在之后接新项目的时候，能够减少工作量。这类项目可以默默执行，不要影响需要交互的进程，可以降低它的优先级。SCHED_IDLE 是特别空闲的时候才跑的进程，相当于咱们学习训练类的项目，比如咱们公司很长时间没有接到外在项目了，可以弄几个这样的项目练练手。上面无论是 policy 还是 priority，都设置了一个变量，变量仅仅表示了应该这样这样干，但事情总要有人去干，谁呢？在 task_struct 里面，还有这样的成员变量： const struct sched_class *sched_class; 调度策略的执行逻辑，就封装在这里面，它是真正干活的那个。sched_class 有几种实现： stop_sched_class 优先级最高的任务会使用这种策略，会中断所有其他线程，且不会被其他任务打断；dl_sched_class 就对应上面的 deadline 调度策略；rt_sched_class 就对应 RR 算法或者 FIFO 算法的调度策略，具体调度策略由进程的 task_struct-\u003epolicy 指定；fair_sched_class 就是普通进程的调度策略；idle_sched_class 就是空闲进程的调度策略。 这里实时进程的调度策略 RR 和 FIFO 相对简单一些，而且由于咱们平时常遇到的都是普通进程，在这里，咱们就重点分析普通进程的调度问题。普通进程使用的调度策略是 fair_sched_class，顾名思义，对于普通进程来讲，公平是最重要的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:17:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"完全公平调度算法 在 Linux 里面，实现了一个基于 CFS 的调度算法。CFS 全称 Completely Fair Scheduling，叫完全公平调度。听起来很“公平”。那这个算法的原理是什么呢？我们来看看。首先，你需要记录下进程的运行时间。CPU 会提供一个时钟，过一段时间就触发一个时钟中断。就像咱们的表滴答一下，这个我们叫 Tick。CFS 会为每一个进程安排一个虚拟运行时间 vruntime。如果一个进程在运行，随着时间的增长，也就是一个个 tick 的到来，进程的 vruntime 将不断增大。没有得到执行的进程 vruntime 不变。显然，那些 vruntime 少的，原来受到了不公平的对待，需要给它补上，所以会优先运行这样的进程。这有点像让你把一筐球平均分到 N 个口袋里面，你看着哪个少，就多放一些；哪个多了，就先不放。这样经过多轮，虽然不能保证球完全一样多，但是也差不多公平。你可能会说，不还有优先级呢？如何给优先级高的进程多分时间呢？ 这个简单，就相当于 N 个口袋，优先级高的袋子大，优先级低的袋子小。这样球就不能按照个数分配了，要按照比例来，大口袋的放了一半和小口袋放了一半，里面的球数目虽然差很多，也认为是公平的。在更新进程运行的统计量的时候，我们其实就可以看出这个逻辑。 /* * Update the current task's runtime statistics. */ static void update_curr(struct cfs_rq *cfs_rq) { struct sched_entity *curr = cfs_rq-\u003ecurr; u64 now = rq_clock_task(rq_of(cfs_rq)); u64 delta_exec; ...... delta_exec = now - curr-\u003eexec_start; ...... curr-\u003eexec_start = now; ...... curr-\u003esum_exec_runtime += delta_exec; ...... curr-\u003evruntime += calc_delta_fair(delta_exec, curr); update_min_vruntime(cfs_rq); ...... } /* * delta /= w */ static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se) { if (unlikely(se-\u003eload.weight != NICE_0_LOAD)) /* delta_exec * weight / lw.weight */ delta = __calc_delta(delta, NICE_0_LOAD, \u0026se-\u003eload); return delta; } 在这里得到当前的时间，以及这次的时间片开始的时间，两者相减就是这次运行的时间 delta_exec ，但是得到的这个时间其实是实际运行的时间，需要做一定的转化才作为虚拟运行时间 vruntime。转化方法如下：虚拟运行时间 vruntime += 实际运行时间 delta_exec * NICE_0_LOAD/ 权重 这就是说，同样的实际运行时间，给高权重的算少了，低权重的算多了，但是当选取下一个运行进程的时候，还是按照最小的 vruntime 来的，这样高权重的获得的实际运行时间自然就多了。这就相当于给一个体重 (权重)200 斤的胖子吃两个馒头，和给一个体重 100 斤的瘦子吃一个馒头，然后说，你们两个吃的是一样多。这样虽然总体胖子比瘦子多吃了一倍，但是还是公平的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:17:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"调度队列与调度实体 看来 CFS 需要一个数据结构来对 vruntime 进行排序，找出最小的那个。这个能够排序的数据结构不但需要查询的时候，能够快速找到最小的，更新的时候也需要能够快速地调整排序，要知道 vruntime 可是经常在变的，变了再插入这个数据结构，就需要重新排序。能够平衡查询和更新速度的是树，在这里使用的是红黑树。红黑树的的节点是应该包括 vruntime 的，称为调度实体。在 task_struct 中有这样的成员变量： struct sched_entity se;struct sched_rt_entity rt;struct sched_dl_entity dl; 这里有实时调度实体 sched_rt_entity，Deadline 调度实体 sched_dl_entity，以及完全公平算法调度实体 sched_entity。看来不光 CFS 调度策略需要有这样一个数据结构进行排序，其他的调度策略也同样有自己的数据结构进行排序，因为任何一个策略做调度的时候，都是要区分谁先运行谁后运行。而进程根据自己是实时的，还是普通的类型，通过这个成员变量，将自己挂在某一个数据结构里面，和其他的进程排序，等待被调度。如果这个进程是个普通进程，则通过 sched_entity，将自己挂在这棵红黑树上。对于普通进程的调度实体定义如下，这里面包含了 vruntime 和权重 load_weight，以及对于运行时间的统计。 struct sched_entity { struct load_weight load; struct rb_node run_node; struct list_head group_node; unsigned int on_rq; u64 exec_start; u64 sum_exec_runtime; u64 vruntime; u64 prev_sum_exec_runtime; u64 nr_migrations; struct sched_statistics statistics; ...... }; 下图是一个红黑树的例子。 所有可运行的进程通过不断地插入操作最终都存储在以时间为顺序的红黑树中，vruntime 最小的在树的左侧，vruntime 最多的在树的右侧。 CFS 调度策略会选择红黑树最左边的叶子节点作为下一个将获得 CPU 的任务。这棵红黑树放在哪里呢？就像每个软件工程师写代码的时候，会将任务排成队列，做完一个做下一个。CPU 也是这样的，每个 CPU 都有自己的 struct rq 结构，其用于描述在此 CPU 上所运行的所有进程，其包括一个实时进程队列 rt_rq 和一个 CFS 运行队列 cfs_rq，在调度时，调度器首先会先去实时进程队列找是否有实时进程需要运行，如果没有才会去 CFS 运行队列找是否有进程需要运行。 struct rq { /* runqueue lock: */ raw_spinlock_t lock; unsigned int nr_running; unsigned long cpu_load[CPU_LOAD_IDX_MAX]; ...... struct load_weight load; unsigned long nr_load_updates; u64 nr_switches; struct cfs_rq cfs; struct rt_rq rt; struct dl_rq dl; ...... struct task_struct *curr, *idle, *stop; ...... }; 对于普通进程公平队列 cfs_rq，定义如下： /* CFS-related fields in a runqueue */ struct cfs_rq { struct load_weight load; unsigned int nr_running, h_nr_running; u64 exec_clock; u64 min_vruntime; #ifndef CONFIG_64BIT u64 min_vruntime_copy; #endif struct rb_root tasks_timeline; struct rb_node *rb_leftmost; struct sched_entity *curr, *next, *last, *skip; ...... }; 这里面 rb_root 指向的就是红黑树的根节点，这个红黑树在 CPU 看起来就是一个队列，不断地取下一个应该运行的进程。rb_leftmost 指向的是最左面的节点。到这里终于凑够数据结构了，上面这些数据结构的关系如下图： ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:17:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"调度类是如何工作的？ 凑够了数据结构，接下来我们来看调度类是如何工作的。调度类的定义如下： struct sched_class { const struct sched_class *next; void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags); void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags); void (*yield_task) (struct rq *rq); bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt); void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags); struct task_struct * (*pick_next_task) (struct rq *rq, struct task_struct *prev, struct rq_flags *rf); void (*put_prev_task) (struct rq *rq, struct task_struct *p); void (*set_curr_task) (struct rq *rq); void (*task_tick) (struct rq *rq, struct task_struct *p, int queued); void (*task_fork) (struct task_struct *p); void (*task_dead) (struct task_struct *p); void (*switched_from) (struct rq *this_rq, struct task_struct *task); void (*switched_to) (struct rq *this_rq, struct task_struct *task); void (*prio_changed) (struct rq *this_rq, struct task_struct *task, int oldprio); unsigned int (*get_rr_interval) (struct rq *rq, struct task_struct *task); void (*update_curr) (struct rq *rq) 这个结构定义了很多种方法，用于在队列上操作任务。这里请大家注意第一个成员变量，是一个指针，指向下一个调度类。上面我们讲了，调度类分为下面这几种： extern const struct sched_class stop_sched_class; extern const struct sched_class dl_sched_class; extern const struct sched_class rt_sched_class; extern const struct sched_class fair_sched_class; extern const struct sched_class idle_sched_class; 它们其实是放在一个链表上的。这里我们以调度最常见的操作，取下一个任务为例，来解析一下。可以看到，这里面有一个 for_each_class 循环，沿着上面的顺序，依次调用每个调度类的方法。 /* * Pick up the highest-prio task: */ static inline struct task_struct * pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) { const struct sched_class *class; struct task_struct *p; ...... for_each_class(class) { p = class-\u003epick_next_task(rq, prev, rf); if (p) { if (unlikely(p == RETRY_TASK)) goto again; return p; } } } 这就说明，调度的时候是从优先级最高的调度类到优先级低的调度类，依次执行。而对于每种调度类，有自己的实现，例如，CFS 就有 fair_sched_class。 const struct sched_class fair_sched_class = { .next = \u0026idle_sched_class, .enqueue_task = enqueue_task_fair, .dequeue_task = dequeue_task_fair, .yield_task = yield_task_fair, .yield_to_task = yield_to_task_fair, .check_preempt_curr = check_preempt_wakeup, .pick_next_task = pick_next_task_fair, .put_prev_task = put_prev_task_fair, .set_curr_task = set_curr_task_fair, .task_tick = task_tick_fair, .task_fork = task_fork_fair, .prio_changed = prio_changed_fair, .switched_from = switched_from_fair, .switched_to = switched_to_fair, .get_rr_interval = get_rr_interval_fair, .update_curr = update_curr_fair, }; 对于同样的 pick_next_task 选取下一个要运行的任务这个动作，不同的调度类有自己的实现。fair_sched_class 的实现是 pick_next_task_fair，rt_sched_class 的实现是 pick_next_task_rt。我们会发现这两个函数是操作不同的队列，pick_next_task_rt 操作的是 rt_rq，pick_next_task_fair 操作的是 cfs_rq。 static struct task_struct * pick_next_task_rt(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) { struct task_struct *p; struct rt_rq *rt_rq = \u0026rq-\u003ert; ...... } static struct task_struct * pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) { struct cfs_rq *cfs_rq = \u0026rq-\u003ecfs; struct sched_entity *se; struct task_struct *p; ...... } 这样整个运行的场景就串起来了，在每个 CPU 上都有一个队列 rq，这个队列里面包含多个子队列，例如 rt_rq 和 cfs_rq，不同的队列有不同的实现方式，cfs_rq 就是用红黑树实现的。当有一天，某个 CPU 需要找下一个任务执行的时候，会按照优先级依次调用调度类，不同的调度类操作不同的队列。当然 rt_sched_class 先被调用，它会在 rt_rq 上找下一个任务，只有找不到的时候，才轮到 fair_sched_class 被调用，它会在 cfs_rq 上找下一个任务。这样保证了实时任务的优先级永远大于普通任务。下面我们仔细看一下 sched_class 定义的与调度有关的函数。 enqueue_task 向就绪队列中添加一个进程，当某个进程进入可运行状态时，调用这个函数；dequeue_task 将一个进程从就绪队列中删除；pick_next_task 选择接下来要运行的进程；put_prev_task 用另一个进程代替当前运行的进程；set_curr_task 用于修改调度策略；task_tick 每次周期性时钟到的时候，这个函数被调用，可能触发调度。 在这里面，我们重点看 fair_sched_class 对于 pick_next_task 的实现 pick_next_task_fair，获取下一个进程。调用路径如下：pick_next_task_fair-\u003epick_next_entity-\u003e__pick_first_entity。 struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq) { struct rb_node *left = rb_first_cached(\u0026cfs_rq-\u003etasks_timeline); if (!left) return NULL; return rb_entry(left, struct sched_entity, r","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:17:6","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 好了，这一节我们讲了调度相关的数据结构，还是比较复杂的。一个 CPU 上有一个队列，CFS 的队列是一棵红黑树，树的每一个节点都是一个 sched_entity，每个 sched_entity 都属于一个 task_struct，task_struct 里面有指针指向这个进程属于哪个调度类。 在调度的时候，依次调用调度类的函数，从 CPU 的队列中取出下一个进程。上面图中的调度器、上下文切换这一节我们没有讲，下一节我们讲讲基于这些数据结构，如何实现调度。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:17:7","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"16 | 调度（中）：主动调度是如何发生的？ 上一节，我们为调度准备了这么多的数据结构，这一节我们来看调度是如何发生的。所谓进程调度，其实就是一个人在做 A 项目，在某个时刻，换成做 B 项目去了。发生这种情况，主要有两种方式。 方式一：A 项目做着做着，发现里面有一条指令 sleep，也就是要休息一下，或者在等待某个 I/O 事件。那没办法了，就要主动让出 CPU，然后可以开始做 B 项目。方式二：A 项目做着做着，旷日持久，实在受不了了。项目经理介入了，说这个项目 A 先停停，B 项目也要做一下，要不然 B 项目该投诉了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:18:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"主动调度 我们这一节先来看方式一，主动调度。这里我找了几个代码片段。第一个片段是 Btrfs，等待一个写入。Btrfs（B-Tree）是一种文件系统，感兴趣你可以自己去了解一下。这个片段可以看作写入块设备的一个典型场景。写入需要一段时间，这段时间用不上 CPU，还不如主动让给其他进程。 static void btrfs_wait_for_no_snapshoting_writes(struct btrfs_root *root) { ...... do { prepare_to_wait(\u0026root-\u003esubv_writers-\u003ewait, \u0026wait, TASK_UNINTERRUPTIBLE); writers = percpu_counter_sum(\u0026root-\u003esubv_writers-\u003ecounter); if (writers) schedule(); finish_wait(\u0026root-\u003esubv_writers-\u003ewait, \u0026wait); } while (writers); } 另外一个例子是，从 Tap 网络设备等待一个读取。Tap 网络设备是虚拟机使用的网络设备。当没有数据到来的时候，它也需要等待，所以也会选择把 CPU 让给其他进程。 static ssize_t tap_do_read(struct tap_queue *q, struct iov_iter *to, int noblock, struct sk_buff *skb) { ...... while (1) { if (!noblock) prepare_to_wait(sk_sleep(\u0026q-\u003esk), \u0026wait, TASK_INTERRUPTIBLE); ...... /* Nothing to read, let's sleep */ schedule(); } ...... } 你应该知道，计算机主要处理计算、网络、存储三个方面。计算主要是 CPU 和内存的合作；网络和存储则多是和外部设备的合作；在操作外部设备的时候，往往需要让出 CPU，就像上面两段代码一样，选择调用 schedule() 函数。接下来，我们就来看 schedule 函数的调用过程。 asmlinkage __visible void __sched schedule(void) { struct task_struct *tsk = current; sched_submit_work(tsk); do { preempt_disable(); __schedule(false); sched_preempt_enable_no_resched(); } while (need_resched()); } 这段代码的主要逻辑是在 __schedule 函数中实现的。这个函数比较复杂，我们分几个部分来讲解。 static void __sched notrace __schedule(bool preempt) { struct task_struct *prev, *next; unsigned long *switch_count; struct rq_flags rf; struct rq *rq; int cpu; cpu = smp_processor_id(); rq = cpu_rq(cpu); prev = rq-\u003ecurr; ...... 首先，在当前的 CPU 上，我们取出任务队列 rq。task_struct *prev 指向这个 CPU 的任务队列上面正在运行的那个进程 curr。为啥是 prev？因为一旦将来它被切换下来，那它就成了前任了。接下来代码如下： next = pick_next_task(rq, prev, \u0026rf); clear_tsk_need_resched(prev); clear_preempt_need_resched(); 第二步，获取下一个任务，task_struct *next 指向下一个任务，这就是继任。pick_next_task 的实现如下： static inline struct task_struct * pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) { const struct sched_class *class; struct task_struct *p; /* * Optimization: we know that if all tasks are in the fair class we can call that function directly, but only if the @prev task wasn't of a higher scheduling class, because otherwise those loose the opportunity to pull in more work from other CPUs. */ if (likely((prev-\u003esched_class == \u0026idle_sched_class || prev-\u003esched_class == \u0026fair_sched_class) \u0026\u0026 rq-\u003enr_running == rq-\u003ecfs.h_nr_running)) { p = fair_sched_class.pick_next_task(rq, prev, rf); if (unlikely(p == RETRY_TASK)) goto again; /* Assumes fair_sched_class-\u003enext == idle_sched_class */ if (unlikely(!p)) p = idle_sched_class.pick_next_task(rq, prev, rf); return p; } again: for_each_class(class) { p = class-\u003epick_next_task(rq, prev, rf); if (p) { if (unlikely(p == RETRY_TASK)) goto again; return p; } } } 我们来看 again 这里，就是咱们上一节讲的依次调用调度类。但是这里有了一个优化，因为大部分进程是普通进程，所以大部分情况下会调用上面的逻辑，调用的就是 fair_sched_class.pick_next_task。根据上一节对于 fair_sched_class 的定义，它调用的是 pick_next_task_fair，代码如下： static struct task_struct * pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) { struct cfs_rq *cfs_rq = \u0026rq-\u003ecfs; struct sched_entity *se; struct task_struct *p; int new_tasks; 对于 CFS 调度类，取出相应的队列 cfs_rq，这就是我们上一节讲的那棵红黑树。 struct sched_entity *curr = cfs_rq-\u003ecurr; if (curr) { if (curr-\u003eon_rq) update_curr(cfs_rq); else curr = NULL; ...... } se = pick_next_entity(cfs_rq, curr); 取出当前正在运行的任务 curr，如果依然是可运行的状态，也即处于进程就绪状态，则调用 update_curr 更新 vruntime。update_curr 咱们上一节就见过了，它会根据实际运行时间算出 vruntime 来。接着，pick_next_entity 从红黑树里面，取最左边的一个节点。这个函数的实现我们上一节也讲过了。 p = task_of(se); if (prev != p) { struct sched_entity *pse = \u0026prev-\u003ese; ...... put_prev_entity(cfs_rq, pse); set_next_entity(cfs_rq, se); } return p task_of 得到下一个调度实体对应的 task_struct，如果发现继任和前任不一样，这就说明有一个更需要运行的进程了，就需要更新红黑树了。前面前任的 vruntime 更新过了，put_prev_entity 放回红黑树，会找到相应的位置，然后 set_next_entity 将继任者设为当前任务。第三步，当选出的继任者和前任不同，就要进行上下文切换，继任者进程正式进入运行。 if (likely(prev != next)) { rq-\u003enr_switches++; rq-\u003ecurr = next; ++*switch_count; ...... rq = context_switch(rq, prev, next, \u0026rf); ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:18:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"进程上下文切换 上下文切换主要干两件事情，一是切换进程空间，也即虚拟内存；二是切换寄存器和 CPU 上下文。我们先来看 context_switch 的实现。 /* * context_switch - switch to the new MM and the new thread's register state. */ static __always_inline struct rq * context_switch(struct rq *rq, struct task_struct *prev, struct task_struct *next, struct rq_flags *rf) { struct mm_struct *mm, *oldmm; ...... mm = next-\u003emm; oldmm = prev-\u003eactive_mm; ...... switch_mm_irqs_off(oldmm, mm, next); ...... /* Here we just switch the register state and the stack. */ switch_to(prev, next, prev); barrier(); return finish_task_switch(prev); } 这里首先是内存空间的切换，里面涉及内存管理的内容比较多。内存管理后面我们会有专门的章节来讲，这里你先知道有这么一回事就行了。接下来，我们看 switch_to。它就是寄存器和栈的切换，它调用到了 __switch_to_asm。这是一段汇编代码，主要用于栈的切换。对于 32 位操作系统来讲，切换的是栈顶指针 esp。 /* * %eax: prev task * %edx: next task */ ENTRY(__switch_to_asm) ...... /* switch stack */ movl %esp, TASK_threadsp(%eax) movl TASK_threadsp(%edx), %esp ...... jmp __switch_to END(__switch_to_asm) 对于 64 位操作系统来讲，切换的是栈顶指针 rsp。 /* * %rdi: prev task * %rsi: next task */ ENTRY(__switch_to_asm) ...... /* switch stack */ movq %rsp, TASK_threadsp(%rdi) movq TASK_threadsp(%rsi), %rsp ...... jmp __switch_to END(__switch_to_asm) 最终，都返回了 __switch_to 这个函数。这个函数对于 32 位和 64 位操作系统虽然有不同的实现，但里面做的事情是差不多的。所以我这里仅仅列出 64 位操作系统做的事情。 __visible __notrace_funcgraph struct task_struct * __switch_to(struct task_struct *prev_p, struct task_struct *next_p) { struct thread_struct *prev = \u0026prev_p-\u003ethread; struct thread_struct *next = \u0026next_p-\u003ethread; ...... int cpu = smp_processor_id(); struct tss_struct *tss = \u0026per_cpu(cpu_tss, cpu); ...... load_TLS(next, cpu); ...... this_cpu_write(current_task, next_p); /* Reload esp0 and ss1. This changes current_thread_info(). */ load_sp0(tss, next); ...... return prev_p; } 这里面有一个 Per CPU 的结构体 tss。这是个什么呢？在 x86 体系结构中，提供了一种以硬件的方式进行进程切换的模式，对于每个进程，x86 希望在内存里面维护一个 TSS（Task State Segment，任务状态段）结构。这里面有所有的寄存器。另外，还有一个特殊的寄存器 TR（Task Register，任务寄存器），指向某个进程的 TSS。更改 TR 的值，将会触发硬件保存 CPU 所有寄存器的值到当前进程的 TSS 中，然后从新进程的 TSS 中读出所有寄存器值，加载到 CPU 对应的寄存器中。下图就是 32 位的 TSS 结构。 图片来自 Intel® 64 and IA-32 Architectures Software Developer’s Manual Combined Volumes 但是这样有个缺点。我们做进程切换的时候，没必要每个寄存器都切换，这样每个进程一个 TSS，就需要全量保存，全量切换，动作太大了。于是，Linux 操作系统想了一个办法。还记得在系统初始化的时候，会调用 cpu_init 吗？这里面会给每一个 CPU 关联一个 TSS，然后将 TR 指向这个 TSS，然后在操作系统的运行过程中，TR 就不切换了，永远指向这个 TSS。TSS 用数据结构 tss_struct 表示，在 x86_hw_tss 中可以看到和上图相应的结构。 void cpu_init(void) { int cpu = smp_processor_id(); struct task_struct *curr = current; struct tss_struct *t = \u0026per_cpu(cpu_tss, cpu); ...... load_sp0(t, thread); set_tss_desc(cpu, t); load_TR_desc(); ...... } struct tss_struct { /* * The hardware state: */ struct x86_hw_tss x86_tss; unsigned long io_bitmap[IO_BITMAP_LONGS + 1]; } 在 Linux 中，真的参与进程切换的寄存器很少，主要的就是栈顶寄存器。于是，在 task_struct 里面，还有一个我们原来没有注意的成员变量 thread。这里面保留了要切换进程的时候需要修改的寄存器。 /* CPU-specific state of this task: */ struct thread_struct thread; 所谓的进程切换，就是将某个进程的 thread_struct 里面的寄存器的值，写入到 CPU 的 TR 指向的 tss_struct，对于 CPU 来讲，这就算是完成了切换。例如 __switch_to 中的 load_sp0，就是将下一个进程的 thread_struct 的 sp0 的值加载到 tss_struct 里面去。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:18:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"指令指针的保存与恢复 你是不是觉得，这样真的就完成切换了吗？是的，不信我们来盘点一下。从进程 A 切换到进程 B，用户栈要不要切换呢？当然要，其实早就已经切换了，就在切换内存空间的时候。每个进程的用户栈都是独立的，都在内存空间里面。那内核栈呢？已经在 __switch_to 里面切换了，也就是将 current_task 指向当前的 task_struct。里面的 void *stack 指针，指向的就是当前的内核栈。 内核栈的栈顶指针呢？在 __switch_to_asm 里面已经切换了栈顶指针，并且将栈顶指针在 __switch_to 加载到了 TSS 里面。用户栈的栈顶指针呢？如果当前在内核里面的话，它当然是在内核栈顶部的 pt_regs 结构里面呀。当从内核返回用户态运行的时候，pt_regs 里面有所有当时在用户态的时候运行的上下文信息，就可以开始运行了。唯一让人不容易理解的是指令指针寄存器，它应该指向下一条指令的，那它是如何切换的呢？这里有点绕，请你仔细看。这里我先明确一点，进程的调度都最终会调用到 __schedule 函数。为了方便你记住，我姑且给它起个名字，就叫“进程调度第一定律”。后面我们会多次用到这个定律，你一定要记住。 我们用最前面的例子仔细分析这个过程。本来一个进程 A 在用户态是要写一个文件的，写文件的操作用户态没办法完成，就要通过系统调用到达内核态。在这个切换的过程中，用户态的指令指针寄存器是保存在 pt_regs 里面的，到了内核态，就开始沿着写文件的逻辑一步一步执行，结果发现需要等待，于是就调用 __schedule 函数。这个时候，进程 A 在内核态的指令指针是指向 __schedule 了。这里请记住，A 进程的内核栈会保存这个 __schedule 的调用，而且知道这是从 btrfs_wait_for_no_snapshoting_writes 这个函数里面进去的。__schedule 里面经过上面的层层调用，到达了 context_switch 的最后三行指令（其中 barrier 语句是一个编译器指令，用于保证 switch_to 和 finish_task_switch 的执行顺序，不会因为编译阶段优化而改变，这里咱们可以忽略它）。 switch_to(prev, next, prev); barrier(); return finish_task_switch(prev); 当进程 A 在内核里面执行 switch_to 的时候，内核态的指令指针也是指向这一行的。但是在 switch_to 里面，将寄存器和栈都切换到成了进程 B 的，唯一没有变的就是指令指针寄存器。当 switch_to 返回的时候，指令指针寄存器指向了下一条语句 finish_task_switch。但这个时候的 finish_task_switch 已经不是进程 A 的 finish_task_switch 了，而是进程 B 的 finish_task_switch 了。这样合理吗？你怎么知道进程 B 当时被切换下去的时候，执行到哪里了？恢复 B 进程执行的时候一定在这里呢？这时候就要用到咱的“进程调度第一定律”了。当年 B 进程被别人切换走的时候，也是调用 __schedule，也是调用到 switch_to，被切换成为 C 进程的，所以，B 进程当年的下一个指令也是 finish_task_switch，这就说明指令指针指到这里是没有错的。接下来，我们要从 finish_task_switch 完毕后，返回 __schedule 的调用了。返回到哪里呢？按照函数返回的原理，当然是从内核栈里面去找，是返回到 btrfs_wait_for_no_snapshoting_writes 吗？当然不是了，因为 btrfs_wait_for_no_snapshoting_writes 是在 A 进程的内核栈里面的，它早就被切换走了，应该从 B 进程的内核栈里面找。 假设，B 就是最前面例子里面调用 tap_do_read 读网卡的进程。它当年调用 __schedule 的时候，是从 tap_do_read 这个函数调用进去的。当然，B 进程的内核栈里面放的是 tap_do_read。于是，从 __schedule 返回之后，当然是接着 tap_do_read 运行，然后在内核运行完毕后，返回用户态。这个时候，B 进程内核栈的 pt_regs 也保存了用户态的指令指针寄存器，就接着在用户态的下一条指令开始运行就可以了。假设，我们只有一个 CPU，从 B 切换到 C，从 C 又切换到 A。在 C 切换到 A 的时候，还是按照“进程调度第一定律”，C 进程还是会调用 __schedule 到达 switch_to，在里面切换成为 A 的内核栈，然后运行 finish_task_switch。这个时候运行的 finish_task_switch，才是 A 进程的 finish_task_switch。运行完毕从 __schedule 返回的时候，从内核栈上才知道，当年是从 btrfs_wait_for_no_snapshoting_writes 调用进去的，因而应该返回 btrfs_wait_for_no_snapshoting_writes 继续执行，最后内核执行完毕返回用户态，同样恢复 pt_regs，恢复用户态的指令指针寄存器，从用户态接着运行。到这里你是不是有点理解为什么 switch_to 有三个参数呢？为啥有两个 prev 呢？其实我们从定义就可以看到。 #define switch_to(prev, next, last) \\ do { \\ prepare_switch_to(prev, next); \\ \\ ((last) = __switch_to_asm((prev), (next))); \\ } while (0) 在上面的例子中，A 切换到 B 的时候，运行到 __switch_to_asm 这一行的时候，是在 A 的内核栈上运行的，prev 是 A，next 是 B。但是，A 执行完 __switch_to_asm 之后就被切换走了，当 C 再次切换到 A 的时候，运行到 __switch_to_asm，是从 C 的内核栈运行的。这个时候，prev 是 C，next 是 A，但是 __switch_to_asm 里面切换成为了 A 当时的内核栈。还记得当年的场景“prev 是 A，next 是 B”，__switch_to_asm 里面 return prev 的时候，还没 return 的时候，prev 这个变量里面放的还是 C，因而它会把 C 放到返回结果中。但是，一旦 return，就会弹出 A 当时的内核栈。这个时候，prev 变量就变成了 A，next 变量就变成了 B。这就还原了当年的场景，好在返回值里面的 last 还是 C。通过三个变量 switch_to(prev = A, next=B, last=C)，A 进程就明白了，我当时被切换走的时候，是切换成 B，这次切换回来，是从 C 回来的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:18:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节我们讲主动调度的过程，也即一个运行中的进程主动调用 __schedule 让出 CPU。在 __schedule 里面会做两件事情，第一是选取下一个进程，第二是进行上下文切换。而上下文切换又分用户态进程空间的切换和内核态的切换。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:18:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"17 | 调度（下）：抢占式调度是如何发生的？ 上一节，我们讲了主动调度，就是进程运行到一半，因为等待 I/O 等操作而主动让出 CPU，然后就进入了我们的“进程调度第一定律”。所有进程的调用最终都会走 __schedule 函数。那这个定律在这一节还是要继续起作用。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:19:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"抢占式调度 上一节我们讲的主动调度是第一种方式，第二种方式，就是抢占式调度。什么情况下会发生抢占呢？最常见的现象就是一个进程执行时间太长了，是时候切换到另一个进程了。那怎么衡量一个进程的运行时间呢？在计算机里面有一个时钟，会过一段时间触发一次时钟中断，通知操作系统，时间又过去一个时钟周期，这是个很好的方式，可以查看是否是需要抢占的时间点。时钟中断处理函数会调用 scheduler_tick()，它的代码如下： void scheduler_tick(void) { int cpu = smp_processor_id(); struct rq *rq = cpu_rq(cpu); struct task_struct *curr = rq-\u003ecurr; ...... curr-\u003esched_class-\u003etask_tick(rq, curr, 0); cpu_load_update_active(rq); calc_global_load_tick(rq); ...... } 这个函数先取出当前 CPU 的运行队列，然后得到这个队列上当前正在运行中的进程的 task_struct，然后调用这个 task_struct 的调度类的 task_tick 函数，顾名思义这个函数就是来处理时钟事件的。如果当前运行的进程是普通进程，调度类为 fair_sched_class，调用的处理时钟的函数为 task_tick_fair。我们来看一下它的实现。 static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued) { struct cfs_rq *cfs_rq; struct sched_entity *se = \u0026curr-\u003ese; for_each_sched_entity(se) { cfs_rq = cfs_rq_of(se); entity_tick(cfs_rq, se, queued); } ...... } 根据当前进程的 task_struct，找到对应的调度实体 sched_entity 和 cfs_rq 队列，调用 entity_tick。 static void entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued) { update_curr(cfs_rq); update_load_avg(curr, UPDATE_TG); update_cfs_shares(curr); ..... if (cfs_rq-\u003enr_running \u003e 1) check_preempt_tick(cfs_rq, curr); } 在 entity_tick 里面，我们又见到了熟悉的 update_curr。它会更新当前进程的 vruntime，然后调用 check_preempt_tick。顾名思义就是，检查是否是时候被抢占了。 static void check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr) { unsigned long ideal_runtime, delta_exec; struct sched_entity *se; s64 delta; ideal_runtime = sched_slice(cfs_rq, curr); delta_exec = curr-\u003esum_exec_runtime - curr-\u003eprev_sum_exec_runtime; if (delta_exec \u003e ideal_runtime) { resched_curr(rq_of(cfs_rq)); return; } ...... se = __pick_first_entity(cfs_rq); delta = curr-\u003evruntime - se-\u003evruntime; if (delta \u003c 0) return; if (delta \u003e ideal_runtime) resched_curr(rq_of(cfs_rq)); } check_preempt_tick 先是调用 sched_slice 函数计算出的 ideal_runtime。ideal_runtime 是一个调度周期中，该进程运行的实际时间。sum_exec_runtime 指进程总共执行的实际时间，prev_sum_exec_runtime 指上次该进程被调度时已经占用的实际时间。每次在调度一个新的进程时都会把它的 se-\u003eprev_sum_exec_runtime = se-\u003esum_exec_runtime，所以 sum_exec_runtime-prev_sum_exec_runtime 就是这次调度占用实际时间。如果这个时间大于 ideal_runtime，则应该被抢占了。除了这个条件之外，还会通过 __pick_first_entity 取出红黑树中最小的进程。如果当前进程的 vruntime 大于红黑树中最小的进程的 vruntime，且差值大于 ideal_runtime，也应该被抢占了。当发现当前进程应该被抢占，不能直接把它踢下来，而是把它标记为应该被抢占。为什么呢？因为进程调度第一定律呀，一定要等待正在运行的进程调用 __schedule 才行啊，所以这里只能先标记一下。标记一个进程应该被抢占，都是调用 resched_curr，它会调用 set_tsk_need_resched，标记进程应该被抢占，但是此时此刻，并不真的抢占，而是打上一个标签 TIF_NEED_RESCHED。 static inline void set_tsk_need_resched(struct task_struct *tsk) { set_tsk_thread_flag(tsk,TIF_NEED_RESCHED); } 另外一个可能抢占的场景是当一个进程被唤醒的时候。我们前面说过，当一个进程在等待一个 I/O 的时候，会主动放弃 CPU。但是当 I/O 到来的时候，进程往往会被唤醒。这个时候是一个时机。当被唤醒的进程优先级高于 CPU 上的当前进程，就会触发抢占。try_to_wake_up() 调用 ttwu_queue 将这个唤醒的任务添加到队列当中。ttwu_queue 再调用 ttwu_do_activate 激活这个任务。ttwu_do_activate 调用 ttwu_do_wakeup。这里面调用了 check_preempt_curr 检查是否应该发生抢占。如果应该发生抢占，也不是直接踢走当前进程，而是将当前进程标记为应该被抢占。 static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags, struct rq_flags *rf) { check_preempt_curr(rq, p, wake_flags); p-\u003estate = TASK_RUNNING; trace_sched_wakeup(p); 到这里，你会发现，抢占问题只做完了一半。就是标识当前运行中的进程应该被抢占了，但是真正的抢占动作并没有发生。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:19:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"抢占的时机 真正的抢占还需要时机，也就是需要那么一个时刻，让正在运行中的进程有机会调用一下 __schedule。你可以想象，不可能某个进程代码运行着，突然要去调用 __schedule，代码里面不可能这么写，所以一定要规划几个时机，这个时机分为用户态和内核态。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:19:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用户态的抢占时机 对于用户态的进程来讲，从系统调用中返回的那个时刻，是一个被抢占的时机。前面讲系统调用的时候，64 位的系统调用的链路位 do_syscall_64-\u003esyscall_return_slowpath-\u003eprepare_exit_to_usermode-\u003eexit_to_usermode_loop，当时我们还没关注 exit_to_usermode_loop 这个函数，现在我们来看一下。 static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags) { while (true) { /* We have work to do. */ local_irq_enable(); if (cached_flags \u0026 _TIF_NEED_RESCHED) schedule(); ...... } } 现在我们看到在 exit_to_usermode_loop 函数中，上面打的标记起了作用，如果被打了 _TIF_NEED_RESCHED，调用 schedule 进行调度，调用的过程和上一节解析的一样，会选择一个进程让出 CPU，做上下文切换。对于用户态的进程来讲，从中断中返回的那个时刻，也是一个被抢占的时机。在 arch/x86/entry/entry_64.S 中有中断的处理过程。又是一段汇编语言代码，你重点领会它的意思就行，不要纠结每一行都看懂。 common_interrupt: ASM_CLAC addq $-0x80, (%rsp) interrupt do_IRQ ret_from_intr: popq %rsp testb $3, CS(%rsp) jz retint_kernel /* Interrupt came from user space */ GLOBAL(retint_user) mov %rsp,%rdi call prepare_exit_to_usermode TRACE_IRQS_IRETQ SWAPGS jmp restore_regs_and_iret /* Returning to kernel space */ retint_kernel: #ifdef CONFIG_PREEMPT bt $9, EFLAGS(%rsp) jnc 1f 0: cmpl $0, PER_CPU_VAR(__preempt_count) jnz 1f call preempt_schedule_irq jmp 0b 中断处理调用的是 do_IRQ 函数，中断完毕后分为两种情况，一个是返回用户态，一个是返回内核态。这个通过注释也能看出来。咱们先来看返回用户态这一部分，先不管返回内核态的那部分代码，retint_user 会调用 prepare_exit_to_usermode，最终调用 exit_to_usermode_loop，和上面的逻辑一样，发现有标记则调用 schedule()。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:19:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"内核态的抢占时机 用户态的抢占时机讲完了，接下来我们看内核态的抢占时机。对内核态的执行中，被抢占的时机一般发生在 preempt_enable() 中。在内核态的执行中，有的操作是不能被中断的，所以在进行这些操作之前，总是先调用 preempt_disable() 关闭抢占，当再次打开的时候，就是一次内核态代码被抢占的机会。就像下面代码中展示的一样，preempt_enable() 会调用 preempt_count_dec_and_test()，判断 preempt_count 和 TIF_NEED_RESCHED 是否可以被抢占。如果可以，就调用 preempt_schedule-\u003epreempt_schedule_common-\u003e__schedule 进行调度。还是满足进程调度第一定律的。 #define preempt_enable() \\ do { \\ if (unlikely(preempt_count_dec_and_test())) \\ __preempt_schedule(); \\ } while (0) #define preempt_count_dec_and_test() \\ ({ preempt_count_sub(1); should_resched(0); }) static __always_inline bool should_resched(int preempt_offset) { return unlikely(preempt_count() == preempt_offset \u0026\u0026 tif_need_resched()); } #define tif_need_resched() test_thread_flag(TIF_NEED_RESCHED) static void __sched notrace preempt_schedule_common(void) { do { ...... __schedule(true); ...... } while (need_resched()) 在内核态也会遇到中断的情况，当中断返回的时候，返回的仍然是内核态。这个时候也是一个执行抢占的时机，现在我们再来上面中断返回的代码中返回内核的那部分代码，调用的是 preempt_schedule_irq。 asmlinkage __visible void __sched preempt_schedule_irq(void) { ...... do { preempt_disable(); local_irq_enable(); __schedule(true); local_irq_disable(); sched_preempt_enable_no_resched(); } while (need_resched()); ...... } preempt_schedule_irq 调用 __schedule 进行调度。还是满足进程调度第一定律的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:19:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 好了，抢占式调度就讲到这里了。我这里画了一张脑图，将整个进程的调度体系都放在里面。这个脑图里面第一条就是总结了进程调度第一定律的核心函数 __schedule 的执行过程，这是上一节讲的，因为要切换的东西比较多，需要你详细了解每一部分是如何切换的。第二条总结了标记为可抢占的场景，第三条是所有的抢占发生的时机，这里是真正验证了进程调度第一定律的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:19:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"18 | 进程的创建：如何发起一个新项目？ 前面我们学习了如何使用 fork 创建进程，也学习了进程管理和调度的相关数据结构。这一节，我们就来看一看，创建进程这个动作在内核里都做了什么事情。fork 是一个系统调用，根据咱们讲过的系统调用的流程，流程的最后会在 sys_call_table 中找到相应的系统调用 sys_fork。sys_fork 是如何定义的呢？根据 SYSCALL_DEFINE0 这个宏的定义，下面这段代码就定义了 sys_fork。 SYSCALL_DEFINE0(fork) { ...... return _do_fork(SIGCHLD, 0, 0, NULL, NULL, 0); } sys_fork 会调用 _do_fork。 long _do_fork(unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr, unsigned long tls) { struct task_struct *p; int trace = 0; long nr; ...... p = copy_process(clone_flags, stack_start, stack_size, child_tidptr, NULL, trace, tls, NUMA_NO_NODE); ...... if (!IS_ERR(p)) { struct pid *pid; pid = get_task_pid(p, PIDTYPE_PID); nr = pid_vnr(pid); if (clone_flags \u0026 CLONE_PARENT_SETTID) put_user(nr, parent_tidptr); ...... wake_up_new_task(p); ...... put_pid(pid); } ...... ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:20:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"fork 的第一件大事：复制结构 _do_fork 里面做的第一件大事就是 copy_process，咱们前面讲过这个思想。如果所有数据结构都从头创建一份太麻烦了，还不如使用惯用“伎俩”，Ctrl C + Ctrl V。这里我们再把 task_struct 的结构图拿出来，对比着看如何一个个复制。 static __latent_entropy struct task_struct *copy_process( unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *child_tidptr, struct pid *pid, int trace, unsigned long tls, int node) { int retval; struct task_struct *p; ...... p = dup_task_struct(current, node); dup_task_struct 主要做了下面几件事情：调用 alloc_task_struct_node 分配一个 task_struct 结构；调用 alloc_thread_stack_node 来创建内核栈，这里面调用 __vmalloc_node_range 分配一个连续的 THREAD_SIZE 的内存空间，赋值给 task_struct 的 void *stack 成员变量；调用 arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)，将 task_struct 进行复制，其实就是调用 memcpy；调用 setup_thread_stack 设置 thread_info。 到这里，整个 task_struct 复制了一份，而且内核栈也创建好了。我们再接着看 copy_process。 retval = copy_creds(p, clone_flags); 轮到权限相关了，copy_creds 主要做了下面几件事情：调用 prepare_creds，准备一个新的 struct cred *new。如何准备呢？其实还是从内存中分配一个新的 struct cred 结构，然后调用 memcpy 复制一份父进程的 cred；接着 p-\u003ecred = p-\u003ereal_cred = get_cred(new)，将新进程的“我能操作谁”和“谁能操作我”两个权限都指向新的 cred。 接下来，copy_process 重新设置进程运行的统计量。 p-\u003eutime = p-\u003estime = p-\u003egtime = 0; p-\u003estart_time = ktime_get_ns(); p-\u003ereal_start_time = ktime_get_boot_ns(); 接下来，copy_process 开始设置调度相关的变量。 retval = sched_fork(clone_flags, p); sched_fork 主要做了下面几件事情：调用 __sched_fork，在这里面将 on_rq 设为 0，初始化 sched_entity，将里面的 exec_start、sum_exec_runtime、prev_sum_exec_runtime、vruntime 都设为 0。你还记得吗，这几个变量涉及进程的实际运行时间和虚拟运行时间。是否到时间应该被调度了，就靠它们几个；设置进程的状态 p-\u003estate = TASK_NEW；初始化优先级 prio、normal_prio、static_prio；设置调度类，如果是普通进程，就设置为 p-\u003esched_class = \u0026fair_sched_class；调用调度类的 task_fork 函数，对于 CFS 来讲，就是调用 task_fork_fair。在这个函数里，先调用 update_curr，对于当前的进程进行统计量更新，然后把子进程和父进程的 vruntime 设成一样，最后调用 place_entity，初始化 sched_entity。这里有一个变量 sysctl_sched_child_runs_first，可以设置父进程和子进程谁先运行。如果设置了子进程先运行，即便两个进程的 vruntime 一样，也要把子进程的 sched_entity 放在前面，然后调用 resched_curr，标记当前运行的进程 TIF_NEED_RESCHED，也就是说，把父进程设置为应该被调度，这样下次调度的时候，父进程会被子进程抢占。 接下来，copy_process 开始初始化与文件和文件系统相关的变量。 retval = copy_files(clone_flags, p); retval = copy_fs(clone_flags, p); copy_files 主要用于复制一个进程打开的文件信息。这些信息用一个结构 files_struct 来维护，每个打开的文件都有一个文件描述符。在 copy_files 函数里面调用 dup_fd，在这里面会创建一个新的 files_struct，然后将所有的文件描述符数组 fdtable 拷贝一份。copy_fs 主要用于复制一个进程的目录信息。这些信息用一个结构 fs_struct 来维护。一个进程有自己的根目录和根文件系统 root，也有当前目录 pwd 和当前目录的文件系统，都在 fs_struct 里面维护。copy_fs 函数里面调用 copy_fs_struct，创建一个新的 fs_struct，并复制原来进程的 fs_struct。接下来，copy_process 开始初始化与信号相关的变量。 init_sigpending(\u0026p-\u003epending); retval = copy_sighand(clone_flags, p); retval = copy_signal(clone_flags, p); copy_sighand 会分配一个新的 sighand_struct。这里最主要的是维护信号处理函数，在 copy_sighand 里面会调用 memcpy，将信号处理函数 sighand-\u003eaction 从父进程复制到子进程。init_sigpending 和 copy_signal 用于初始化，并且复制用于维护发给这个进程的信号的数据结构。copy_signal 函数会分配一个新的 signal_struct，并进行初始化。接下来，copy_process 开始复制进程内存空间。 retval = copy_mm(clone_flags, p); 进程都有自己的内存空间，用 mm_struct 结构来表示。copy_mm 函数中调用 dup_mm，分配一个新的 mm_struct 结构，调用 memcpy 复制这个结构。dup_mmap 用于复制内存空间中内存映射的部分。前面讲系统调用的时候，我们说过，mmap 可以分配大块的内存，其实 mmap 也可以将一个文件映射到内存中，方便可以像读写内存一样读写文件，这个在内存管理那节我们讲。接下来，copy_process 开始分配 pid，设置 tid，group_leader，并且建立进程之间的亲缘关系。 INIT_LIST_HEAD(\u0026p-\u003echildren); INIT_LIST_HEAD(\u0026p-\u003esibling); ...... p-\u003epid = pid_nr(pid); if (clone_flags \u0026 CLONE_THREAD) { p-\u003eexit_signal = -1; p-\u003egroup_leader = current-\u003egroup_leader; p-\u003etgid = current-\u003etgid; } else { if (clone_flags \u0026 CLONE_PARENT) p-\u003eexit_signal = current-\u003egroup_leader-\u003eexit_signal; else p-\u003eexit_signal = (clone_flags \u0026 CSIGNAL); p-\u003egroup_leader = p; p-\u003etgid = p-\u003epid; } ...... if (clone_flags \u0026 (CLONE_PARENT|CLONE_THREAD)) { p-\u003ereal_parent = current-\u003ereal_parent; p-\u003eparent_exec_id = current-\u003eparent_exec_id; } else { p-\u003ereal_parent = current; p-\u003eparent_exec_id = current-\u003eself_exec_id; } 好了，copy_process 要结束了，上面图中的组件也初始化的差不多了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:20:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"fork 的第二件大事：唤醒新进程 _do_fork 做的第二件大事是 wake_up_new_task。新任务刚刚建立，有没有机会抢占别人，获得 CPU 呢？ void wake_up_new_task(struct task_struct *p) { struct rq_flags rf; struct rq *rq; ...... p-\u003estate = TASK_RUNNING; ...... activate_task(rq, p, ENQUEUE_NOCLOCK); p-\u003eon_rq = TASK_ON_RQ_QUEUED; trace_sched_wakeup_new(p); check_preempt_curr(rq, p, WF_FORK); ...... } 首先，我们需要将进程的状态设置为 TASK_RUNNING。activate_task 函数中会调用 enqueue_task。 static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags) { ..... p-\u003esched_class-\u003eenqueue_task(rq, p, flags); } 如果是 CFS 的调度类，则执行相应的 enqueue_task_fair。 static void enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags) { struct cfs_rq *cfs_rq; struct sched_entity *se = \u0026p-\u003ese; ...... cfs_rq = cfs_rq_of(se); enqueue_entity(cfs_rq, se, flags); ...... cfs_rq-\u003eh_nr_running++; ...... } 在 enqueue_task_fair 中取出的队列就是 cfs_rq，然后调用 enqueue_entity。在 enqueue_entity 函数里面，会调用 update_curr，更新运行的统计量，然后调用 __enqueue_entity，将 sched_entity 加入到红黑树里面，然后将 se-\u003eon_rq = 1 设置在队列上。回到 enqueue_task_fair 后，将这个队列上运行的进程数目加一。然后，wake_up_new_task 会调用 check_preempt_curr，看是否能够抢占当前进程。在 check_preempt_curr 中，会调用相应的调度类的 rq-\u003ecurr-\u003esched_class-\u003echeck_preempt_curr(rq, p, flags)。对于 CFS 调度类来讲，调用的是 check_preempt_wakeup。 static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags) { struct task_struct *curr = rq-\u003ecurr; struct sched_entity *se = \u0026curr-\u003ese, *pse = \u0026p-\u003ese; struct cfs_rq *cfs_rq = task_cfs_rq(curr); ...... if (test_tsk_need_resched(curr)) return; ...... find_matching_se(\u0026se, \u0026pse); update_curr(cfs_rq_of(se)); if (wakeup_preempt_entity(se, pse) == 1) { goto preempt; } return; preempt: resched_curr(rq); ...... } 在 check_preempt_wakeup 函数中，前面调用 task_fork_fair 的时候，设置 sysctl_sched_child_runs_first 了，已经将当前父进程的 TIF_NEED_RESCHED 设置了，则直接返回。否则，check_preempt_wakeup 还是会调用 update_curr 更新一次统计量，然后 wakeup_preempt_entity 将父进程和子进程 PK 一次，看是不是要抢占，如果要则调用 resched_curr 标记父进程为 TIF_NEED_RESCHED。如果新创建的进程应该抢占父进程，在什么时间抢占呢？别忘了 fork 是一个系统调用，从系统调用返回的时候，是抢占的一个好时机，如果父进程判断自己已经被设置为 TIF_NEED_RESCHED，就让子进程先跑，抢占自己。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:20:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 好了，fork 系统调用的过程咱们就解析完了。它包含两个重要的事件，一个是将 task_struct 结构复制一份并且初始化，另一个是试图唤醒新创建的子进程。这个过程我画了一张图，你可以对照着这张图回顾进程创建的过程。这个图的上半部分是复制 task_struct 结构，你可以对照着右面的 task_struct 结构图，看这里面的成员是如何一部分一部分地被复制的。图的下半部分是唤醒新创建的子进程，如果条件满足，就会将当前进程设置应该被调度的标识位，就等着当前进程执行 __schedule 了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:20:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"19 | 线程的创建：如何执行一个新子项目？ 上一节，我们了解了进程创建的整个过程，今天我们来看线程创建的过程。我们前面已经写过多线程编程的程序了，你应该都知道创建一个线程调用的是 pthread_create，可你知道它背后的机制吗？ ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:21:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用户态创建线程 你可能会问，咱们之前不是讲过了吗？无论是进程还是线程，在内核里面都是任务，管起来不是都一样吗？但是问题来了，如果两个完全一样，那为什么咱们前两节写的程序差别那么大？如果不一样，那怎么在内核里面加以区分呢？其实，线程不是一个完全由内核实现的机制，它是由内核态和用户态合作完成的。pthread_create 不是一个系统调用，是 Glibc 库的一个函数，所以我们还要去 Glibc 里面去找线索。果然，我们在 nptl/pthread_create.c 里面找到了这个函数。这里的参数我们应该比较熟悉了。 int __pthread_create_2_1 (pthread_t *newthread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg) { ...... } versioned_symbol (libpthread, __pthread_create_2_1, pthread_create, GLIBC_2_1); 下面我们依次来看这个函数做了些啥。首先处理的是线程的属性参数。例如前面写程序的时候，我们设置的线程栈大小。如果没有传入线程属性，就取默认值。 const struct pthread_attr *iattr = (struct pthread_attr *) attr; struct pthread_attr default_attr; if (iattr == NULL) { ...... iattr = \u0026default_attr; } 接下来，就像在内核里一样，每一个进程或者线程都有一个 task_struct 结构，在用户态也有一个用于维护线程的结构，就是这个 pthread 结构。 struct pthread *pd = NULL; 凡是涉及函数的调用，都要使用到栈。每个线程也有自己的栈。那接下来就是创建线程栈了。 int err = ALLOCATE_STACK (iattr, \u0026pd); ALLOCATE_STACK 是一个宏，我们找到它的定义之后，发现它其实就是一个函数。只是，这个函数有些复杂，所以我这里把主要的代码列一下。 # define ALLOCATE_STACK(attr, pd) allocate_stack (attr, pd, \u0026stackaddr) static int allocate_stack (const struct pthread_attr *attr, struct pthread **pdp, ALLOCATE_STACK_PARMS) { struct pthread *pd; size_t size; size_t pagesize_m1 = __getpagesize () - 1; ...... size = attr-\u003estacksize; ...... /* Allocate some anonymous memory. If possible use the cache. */ size_t guardsize; void *mem; const int prot = (PROT_READ | PROT_WRITE | ((GL(dl_stack_flags) \u0026 PF_X) ? PROT_EXEC : 0)); /* Adjust the stack size for alignment. */ size \u0026= ~__static_tls_align_m1; /* Make sure the size of the stack is enough for the guard and eventually the thread descriptor. */ guardsize = (attr-\u003eguardsize + pagesize_m1) \u0026 ~pagesize_m1; size += guardsize; pd = get_cached_stack (\u0026size, \u0026mem); if (pd == NULL) { /* If a guard page is required, avoid committing memory by first allocate with PROT_NONE and then reserve with required permission excluding the guard page. */ mem = __mmap (NULL, size, (guardsize == 0) ? prot : PROT_NONE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_STACK, -1, 0); /* Place the thread descriptor at the end of the stack. */ #if TLS_TCB_AT_TP pd = (struct pthread *) ((char *) mem + size) - 1; #elif TLS_DTV_AT_TP pd = (struct pthread *) ((((uintptr_t) mem + size - __static_tls_size) \u0026 ~__static_tls_align_m1) - TLS_PRE_TCB_SIZE); #endif /* Now mprotect the required region excluding the guard area. */ char *guard = guard_position (mem, size, guardsize, pd, pagesize_m1); setup_stack_prot (mem, size, guard, guardsize, prot); pd-\u003estackblock = mem; pd-\u003estackblock_size = size; pd-\u003eguardsize = guardsize; pd-\u003especific[0] = pd-\u003especific_1stblock; /* And add to the list of stacks in use. */ stack_list_add (\u0026pd-\u003elist, \u0026stack_used); } *pdp = pd; void *stacktop; # if TLS_TCB_AT_TP /* The stack begins before the TCB and the static TLS block. */ stacktop = ((char *) (pd + 1) - __static_tls_size); # elif TLS_DTV_AT_TP stacktop = (char *) (pd - 1); # endif *stack = stacktop; ...... } 我们来看一下，allocate_stack 主要做了以下这些事情： 如果你在线程属性里面设置过栈的大小，需要你把设置的值拿出来；为了防止栈的访问越界，在栈的末尾会有一块空间 guardsize，一旦访问到这里就错误了；其实线程栈是在进程的堆里面创建的。如果一个进程不断地创建和删除线程，我们不可能不断地去申请和清除线程栈使用的内存块，这样就需要有一个缓存。get_cached_stack 就是根据计算出来的 size 大小，看一看已经有的缓存中，有没有已经能够满足条件的；如果缓存里面没有，就需要调用 __mmap 创建一块新的，系统调用那一节我们讲过，如果要在堆里面 malloc 一块内存，比较大的话，用 __mmap；线程栈也是自顶向下生长的，还记得每个线程要有一个 pthread 结构，这个结构也是放在栈的空间里面的。在栈底的位置，其实是地址最高位；计算出 guard 内存的位置，调用 setup_stack_prot 设置这块内存的是受保护的；接下来，开始填充 pthread 这个结构里面的成员变量 stackblock、stackblock_size、guardsize、specific。这里的 specific 是用于存放 Thread Specific Data 的，也即属于线程的全局变量；将这个线程栈放到 stack_used 链表中，其实管理线程栈总共有两个链表，一个是 stack_used，也就是这个栈正被使用；另一个是 stack_cache，就是上面说的，一旦线程结束，先缓存起来，不释放，等有其他的线程创建的时候，给其他的线程用。 搞定了用户态栈的问题，其实用户态的事情基本搞定了一半。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:21:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"内核态创建任务 接下来，我们接着 pthread_create 看。其实有了用户态的栈，接着需要解决的就是用户态的程序从哪里开始运行的问题。 pd-\u003estart_routine = start_routine; pd-\u003earg = arg; pd-\u003eschedpolicy = self-\u003eschedpolicy; pd-\u003eschedparam = self-\u003eschedparam; /* Pass the descriptor to the caller. */ *newthread = (pthread_t) pd; atomic_increment (\u0026__nptl_nthreads); retval = create_thread (pd, iattr, \u0026stopped_start, STACK_VARIABLES_ARGS, \u0026thread_ran); start_routine 就是咱们给线程的函数，start_routine，start_routine 的参数 arg，以及调度策略都要赋值给 pthread。接下来 __nptl_nthreads 加一，说明又多了一个线程。真正创建线程的是调用 create_thread 函数，这个函数定义如下： static int create_thread (struct pthread *pd, const struct pthread_attr *attr, bool *stopped_start, STACK_VARIABLES_PARMS, bool *thread_ran) { const int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SYSVSEM | CLONE_SIGHAND | CLONE_THREAD | CLONE_SETTLS | CLONE_PARENT_SETTID | CLONE_CHILD_CLEARTID | 0); ARCH_CLONE (\u0026start_thread, STACK_VARIABLES_ARGS, clone_flags, pd, \u0026pd-\u003etid, tp, \u0026pd-\u003etid)； /* It's started now, so if we fail below, we'll have to cancel it and let it clean itself up. */ *thread_ran = true; } 这里面有很长的 clone_flags，这些咱们原来一直没注意，不过接下来的过程，我们要特别的关注一下这些标志位。然后就是 ARCH_CLONE，其实调用的是 __clone。看到这里，你应该就有感觉了，马上就要到系统调用了。 # define ARCH_CLONE __clone /* The userland implementation is: int clone (int (*fn)(void *arg), void *child_stack, int flags, void *arg), the kernel entry is: int clone (long flags, void *child_stack). The parameters are passed in register and on the stack from userland: rdi: fn rsi: child_stack rdx: flags rcx: arg r8d: TID field in parent r9d: thread pointer %esp+8: TID field in child The kernel expects: rax: system call number rdi: flags rsi: child_stack rdx: TID field in parent r10: TID field in child r8: thread pointer */ .text ENTRY (__clone) movq $-EINVAL,%rax ...... /* Insert the argument onto the new stack. */ subq $16,%rsi movq %rcx,8(%rsi) /* Save the function pointer. It will be popped off in the child in the ebx frobbing below. */ movq %rdi,0(%rsi) /* Do the system call. */ movq %rdx, %rdi movq %r8, %rdx movq %r9, %r8 mov 8(%rsp), %R10_LP movl $SYS_ify(clone),%eax ...... syscall ...... PSEUDO_END (__clone) 如果对于汇编不太熟悉也没关系，你可以重点看上面的注释。我们能看到最后调用了 syscall，这一点 clone 和我们原来熟悉的其他系统调用几乎是一致的。但是，也有少许不一样的地方。如果在进程的主线程里面调用其他系统调用，当前用户态的栈是指向整个进程的栈，栈顶指针也是指向进程的栈，指令指针也是指向进程的主线程的代码。此时此刻执行到这里，调用 clone 的时候，用户态的栈、栈顶指针、指令指针和其他系统调用一样，都是指向主线程的。但是对于线程来说，这些都要变。因为我们希望当 clone 这个系统调用成功的时候，除了内核里面有这个线程对应的 task_struct，当系统调用返回到用户态的时候，用户态的栈应该是线程的栈，栈顶指针应该指向线程的栈，指令指针应该指向线程将要执行的那个函数。所以这些都需要我们自己做，将线程要执行的函数的参数和指令的位置都压到栈里面，当从内核返回，从栈里弹出来的时候，就从这个函数开始，带着这些参数执行下去。接下来我们就要进入内核了。内核里面对于 clone 系统调用的定义是这样的： SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp, int __user *, parent_tidptr, int __user *, child_tidptr, unsigned long, tls) { return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls); } 看到这里，发现了熟悉的面孔 _do_fork，是不是轻松了一些？上一节我们已经沿着它的逻辑过了一遍了。这里我们重点关注几个区别。第一个是上面复杂的标志位设定，我们来看都影响了什么。对于 copy_files，原来是调用 dup_fd 复制一个 files_struct 的，现在因为 CLONE_FILES 标识位变成将原来的 files_struct 引用计数加一。 static int copy_files(unsigned long clone_flags, struct task_struct *tsk) { struct files_struct *oldf, *newf; oldf = current-\u003efiles; if (clone_flags \u0026 CLONE_FILES) { atomic_inc(\u0026oldf-\u003ecount); goto out; } newf = dup_fd(oldf, \u0026error); tsk-\u003efiles = newf; out: return error; } 对于 copy_fs，原来是调用 copy_fs_struct 复制一个 fs_struct，现在因为 CLONE_FS 标识位变成将原来的 fs_struct 的用户数加一。 static int copy_fs(unsigned long clone_flags, struct task_struct *tsk) { struct fs_struct *fs = current-\u003efs; if (clone_flags \u0026 CLONE_FS) { fs-\u003eusers++; return 0; } tsk-\u003efs = copy_fs_struct(fs); return 0; } 对于 copy_sighand，原来是创建一个新的 sighand_struct，现在因为 CLONE_SIGHAND 标识位变成将原来的 sighand_struct 引用计数加一。 static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk) { struct sighand_struct *sig; if (clone_flags \u0026 CLONE_SIGHAND) { atomic_inc(\u0026current-\u003esighand-\u003ecount); return 0; } sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL); atomic_set(\u0026sig-\u003ecount, 1); memcpy(sig-\u003eaction, current-\u003esighand-\u003eaction, sizeof(sig-\u003eaction)); return 0; } 对于 cop","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:21:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用户态执行线程 根据 __clone 的第一个参数，回到用户态也不是直接运行我们指定的那个函数，而是一个通用的 start_thread，这是所有线程在用户态的统一入口。 #define START_THREAD_DEFN \\ static int __attribute__ ((noreturn)) start_thread (void *arg) START_THREAD_DEFN { struct pthread *pd = START_THREAD_SELF; /* Run the code the user provided. */ THREAD_SETMEM (pd, result, pd-\u003estart_routine (pd-\u003earg)); /* Call destructors for the thread_local TLS variables. */ /* Run the destructor for the thread-local data. */ __nptl_deallocate_tsd (); if (__glibc_unlikely (atomic_decrement_and_test (\u0026__nptl_nthreads))) /* This was the last thread. */ exit (0); __free_tcb (pd); __exit_thread (); } 在 start_thread 入口函数中，才真正的调用用户提供的函数，在用户的函数执行完毕之后，会释放这个线程相关的数据。例如，线程本地数据 thread_local variables，线程数目也减一。如果这是最后一个线程了，就直接退出进程，另外 __free_tcb 用于释放 pthread。 void internal_function __free_tcb (struct pthread *pd) { ...... __deallocate_stack (pd); } void internal_function __deallocate_stack (struct pthread *pd) { /* Remove the thread from the list of threads with user defined stacks. */ stack_list_del (\u0026pd-\u003elist); /* Not much to do. Just free the mmap()ed memory. Note that we do not reset the 'used' flag in the 'tid' field. This is done by the kernel. If no thread has been created yet this field is still zero. */ if (__glibc_likely (! pd-\u003euser_stack)) (void) queue_stack (pd); } __free_tcb 会调用 __deallocate_stack 来释放整个线程栈，这个线程栈要从当前使用线程栈的列表 stack_used 中拿下来，放到缓存的线程栈列表 stack_cache 中。好了，整个线程的生命周期到这里就结束了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:21:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 线程的调用过程解析完毕了，我画了一个图总结一下。这个图对比了创建进程和创建线程在用户态和内核态的不同。创建进程的话，调用的系统调用是 fork，在 copy_process 函数里面，会将五大结构 files_struct、fs_struct、sighand_struct、signal_struct、mm_struct 都复制一遍，从此父进程和子进程各用各的数据结构。而创建线程的话，调用的是系统调用 clone，在 copy_process 函数里面， 五大结构仅仅是引用计数加一，也即线程共享进程的数据结构。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:21:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"20 | 内存管理（上）：为客户保密，规划进程内存空间布局 平时我们说计算机的“计算”两个字，其实说的就是两方面，第一，进程和线程对于 CPU 的使用；第二，对于内存的管理。所以从这一节开始，我们来看看内存管理的机制。我之前说把内存管理比喻为一个项目组的“封闭开发的会议室”。很显然，如果不隔离，就会不安全、就会泄密，所以我们说每个进程应该有自己的内存空间。内存空间都是独立的、相互隔离的。对于每个进程来讲，看起来应该都是独占的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:22:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"独享内存空间的原理 之前我只是简单地形容了一下。这一节，我们来深入分析一下，为啥一定要封闭开发呢？执行一个项目，要依赖于项目执行计划书里的指令。项目只要按这些指令运行就行了。但是，在运行指令的过程中，免不了要产生一些数据。这些数据要保存在一个地方，这个地方就是内存，也就是我们刚才说的“会议室”。和会议室一样，内存都被分成一块一块儿的，都编好了号。例如 3F-10，就是三楼十号会议室。内存也有这样一个地址。这个地址是实实在在的地址，通过这个地址我们就能够定位到物理内存的位置。使用这种类型的地址会不会有问题呢？我们的二进制程序，也就是项目执行计划书，都是事先写好的，可以多次运行的。如果里面有个指令是，要把用户输入的数字保存在内存中，那就会有问题。会产生什么问题呢？我举个例子你就明白了。如果我们使用那个实实在在的地址，3F-10，打开三个相同的程序，都执行到某一步。比方说，打开了三个计算器，用户在这三个程序的界面上分别输入了 10、100、1000。如果内存中的这个位置只能保存一个数，那应该保存哪个呢？这不就冲突了吗？如果不用这个实实在在的地址，那应该怎么办呢？于是，我们就想出一个办法，那就是封闭开发。 每个项目的物理地址对于进程不可见，谁也不能直接访问这个物理地址。操作系统会给进程分配一个虚拟地址。所有进程看到的这个地址都是一样的，里面的内存都是从 0 开始编号。在程序里面，指令写入的地址是虚拟地址。例如，位置为 10M 的内存区域，操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。当程序要访问虚拟地址的时候，由内核的数据结构进行转换，转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:22:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"规划虚拟地址空间 通过以上的原理，我们可以看出，操作系统的内存管理，主要分为三个方面。第一，物理内存的管理，相当于会议室管理员管理会议室。第二，虚拟地址的管理，也即在项目组的视角，会议室的虚拟地址应该如何组织。第三，虚拟地址和物理地址如何映射，也即会议室管理员如何管理映射表。 接下来，我们都会围绕虚拟地址和物理地址展开。这两个概念有点绕，很多时候你可能会犯糊涂：这个地方，我们用的是虚拟地址呢，还是物理地址呢？所以，请你在学习这一章节的时候，时刻问自己这个问题。我们还是切换到外包公司老板的角度。现在，如果让你规划一下，到底应该怎么管理会议室，你会怎么办？是不是可以先听听项目组的意见，收集一下需求。于是，你看到了项目组的项目执行计划书是这样一个程序。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e int max_length = 128; char * generate(int length){ int i; char * buffer = (char*) malloc (length+1); if (buffer == NULL) return NULL; for (i=0; i\u003clength; i++){ buffer[i]=rand()%26+'a'; } buffer[length]='\\0'; return buffer; } int main(int argc, char *argv[]) { int num; char * buffer; printf (\"Input the string length : \"); scanf (\"%d\", \u0026num); if(num \u003e max_length){ num = max_length; } buffer = generate(num); printf (\"Random string is: %s\\n\",buffer); free (buffer); return 0; } 这个程序比较简单，就是根据用户输入的整数来生成字符串，最长是 128。由于字符串的长度不是固定的，因而不能提前知道，需要动态地分配内存，使用 malloc 函数。当然用完了需要释放内存，这就要使用 free 函数。我们来总结一下，这个简单的程序在使用内存时的几种方式： 代码需要放在内存里面；全局变量，例如 max_length；常量字符串\"Input the string length : “；函数栈，例如局部变量 num 是作为参数传给 generate 函数的，这里面涉及了函数调用，局部变量，函数参数等都是保存在函数栈上面的；堆，malloc 分配的内存在堆里面；这里面涉及对 glibc 的调用，所以 glibc 的代码是以 so 文件的形式存在的，也需要放在内存里面。 这就完了吗？还没有呢，别忘了 malloc 会调用系统调用，进入内核，所以这个程序一旦运行起来，内核部分还需要分配内存： 内核的代码要在内存里面；内核中也有全局变量；每个进程都要有一个 task_struct；每个进程还有一个内核栈；在内核里面也有动态分配的内存；虚拟地址到物理地址的映射表放在哪里？ 竟然收集了这么多的需求，看来做个内存管理还是挺复杂的啊！我们现在来问一下自己，上面的这些内存里面的数据，应该用虚拟地址访问呢？还是应该用物理地址访问呢？你可能会说，这很简单嘛。用户态的用虚拟地址访问，内核态的用物理地址访问。其实不是的。你有没有想过，内核里面的代码如果都使用物理地址，就相当于公司里的项目管理部门、文档管理部门都可以直接使用实际的地址访问会议室，这对于会议室管理部门来讲，简直是一个“灾难”。因为一旦到了内核，大家对于会议室的访问都脱离了会议室管理部门的控制。所以，我们应该清楚一件事情，真正能够使用会议室的物理地址的，只有会议室管理部门，所有其他部门的行为涉及访问会议室的，都要统统使用虚拟地址，统统到会议室管理部门那里转换一道，才能进行统一的控制。我上面列举出来的，对于内存的访问，用户态的进程使用虚拟地址，这点毫无疑问，内核态的也基本都是使用虚拟地址，只有最后一项容易让人产生疑问。虚拟地址到物理地址的映射表，这个感觉起来是内存管理模块的一部分，这个是“实”是“虚”呢？这个问题先保留，我们暂不讨论，放到内存映射那一节见分晓。既然都是虚拟地址，我们就先不管映射到物理地址以后是如何布局的，反正现在至少从“虚”的角度来看，这一大片连续的内存空间都是我的了。如果是 32 位，有 2^32 = 4G 的内存空间都是我的，不管内存是不是真的有 4G。如果是 64 位，在 x86_64 下面，其实只使用了 48 位，那也挺恐怖的。48 位地址长度也就是对应了 256TB 的地址空间。我都没怎么见过 256T 的硬盘，别说是内存了。 现在，你可比世界首富房子还大。虽然是虚拟的。下面你可以尽情地去排列咱们要放的东西。请记住，现在你是站在一个进程的角度去看这个虚拟的空间，不用管其他进程。首先，这么大的虚拟空间一切二，一部分用来放内核的东西，称为内核空间，一部分用来放进程的东西，称为用户空间。用户空间在下，在低地址，我们假设就是 0 号到 29 号会议室；内核空间在上，在高地址，我们假设是 30 号到 39 号会议室。这两部分空间的分界线因为 32 位和 64 位的不同而不同，我们这里不深究。 对于普通进程来说，内核空间的那部分虽然虚拟地址在那里，但是不能访问。这就像作为普通员工，你明明知道财务办公室在这个 30 号会议室门里面，但是门上挂着“闲人免进”，你只能在自己的用户空间里面折腾。 我们从最低位开始排起，先是 Text Segment、Data Segment 和 BSS Segment。Text Segment 是存放二进制可执行代码的位置，Data Segment 存放静态常量，BSS Segment 存放未初始化的静态变量。是不是觉得这几个名字很熟悉？没错，咱们前面讲 ELF 格式的时候提到过，在二进制执行文件里面，就有这三个部分。这里就是把二进制执行文件的三个部分加载到内存里面。 接下来是堆（Heap）段。堆是往高地址增长的，是用来动态分配内存的区域，malloc 就是在这里面分配的。接下来的区域是 Memory Mapping Segment。这块地址可以用来把文件映射进内存用的，如果二进制的执行文件依赖于某个动态链接库，就是在这个区域里面将 so 文件映射到了内存中。再下面就是栈（Stack）地址段。主线程的函数调用的函数栈就是用这里的。 如果普通进程还想进一步访问内核空间，是没办法的，只能眼巴巴地看着。如果需要进行更高权限的工作，就需要调用系统调用，进入内核。一旦进入了内核，就换了一种视角。刚才是普通进程的视角，觉着整个空间是它独占的，没有其他进程存在。当然另一个进程也这样认为，因为它们互相看不到对方。这也就是说，不同进程的 0 号到 29 号会议室放的东西都不一样。但是到了内核里面，无论是从哪个进程进来的，看到的都是同一个内核空间，看到的都是同一个进程列表。虽然内核栈是各用各的，但是如果想知道的话，还是能够知道每个进程的内核栈在哪里的。所以，如果要访问一些公共的数据结构，需要进行锁保护。也就是说，不同的进程进入到内核后，进入的 30 号到 39 号会议室是同一批会议室。 内核的代码访问内核的数据结构，大部分的情况下都是使用虚拟地址的，虽然内核代码权限很大，但是能够使用的虚拟地址范围也只能在内核空间，也即内核代码访问内核数据结构。只能用 30 号到 39 号这些编号，不能用 0 到 29 号，因为这些是被进程空间占用的。而且，进程有很多个。你现在在内核，但是你不知道当前指的 0 号是哪个进程的 0 号。在内核里面也会有内核的代码，同样有 Text Segment、Data Segment 和 BSS Segment，别忘了咱们讲内核启动的时候，内核代码也是 ELF 格式的。内核的其他数据结构的分配方式就比较复杂了，这一节我们先不讲。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:22:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 好了，这一节就到这里了，我们来总结一下。这一节我们讲了为什么要独享内存空间，并且站在老板的角度，设计了虚拟地址空间应该存放的数据。通过这一节，你应该知道，一个内存管理系统至少应该做三件事情： 第一，虚拟内存空间的管理，每个进程看到的是独立的、互不干扰的虚拟地址空间；第二，物理内存的管理，物理内存地址只有内存管理模块能够使用；第三，内存映射，需要将虚拟内存和物理内存映射、关联起来。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:22:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"21 | 内存管理（下）：为客户保密，项目组独享会议室封闭开发 上一节，我们讲了虚拟空间的布局。接下来，我们需要知道，如何将其映射成为物理地址呢？你可能已经想到了，咱们前面讲 x86 CPU 的时候，讲过分段机制，咱们规划虚拟空间的时候，也是将空间分成多个段进行保存。那就直接用分段机制呗。我们来看看分段机制的原理。 分段机制下的虚拟地址由两部分组成，段选择子和段内偏移量。段选择子就保存在咱们前面讲过的段寄存器里面。段选择子里面最重要的是段号，用作段表的索引。段表里面保存的是这个段的基地址、段的界限和特权等级等。虚拟地址中的段内偏移量应该位于 0 和段界限之间。如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。例如，我们将上面的虚拟空间分成以下 4 个段，用 0～3 来编号。每个段在段表中有一个项，在物理空间中，段的排列如下图的右边所示。 如果要访问段 2 中偏移量 600 的虚拟地址，我们可以计算出物理地址为，段 2 基地址 2000 + 偏移量 600 = 2600。多好的机制啊！我们来看看 Linux 是如何使用这个机制的。在 Linux 里面，段表全称段描述符表（segment descriptors），放在全局描述符表 GDT（Global Descriptor Table）里面，会有下面的宏来初始化段描述符表里面的表项。 #define GDT_ENTRY_INIT(flags, base, limit) { { { \\ .a = ((limit) \u0026 0xffff) | (((base) \u0026 0xffff) \u003c\u003c 16), \\ .b = (((base) \u0026 0xff0000) \u003e\u003e 16) | (((flags) \u0026 0xf0ff) \u003c\u003c 8) | \\ ((limit) \u0026 0xf0000) | ((base) \u0026 0xff000000), \\ } } } 一个段表项由段基地址 base、段界限 limit，还有一些标识符组成。 DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = { #ifdef CONFIG_X86_64 [GDT_ENTRY_KERNEL32_CS] = GDT_ENTRY_INIT(0xc09b, 0, 0xfffff), [GDT_ENTRY_KERNEL_CS] = GDT_ENTRY_INIT(0xa09b, 0, 0xfffff), [GDT_ENTRY_KERNEL_DS] = GDT_ENTRY_INIT(0xc093, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER32_CS] = GDT_ENTRY_INIT(0xc0fb, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_DS] = GDT_ENTRY_INIT(0xc0f3, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_CS] = GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff), #else [GDT_ENTRY_KERNEL_CS] = GDT_ENTRY_INIT(0xc09a, 0, 0xfffff), [GDT_ENTRY_KERNEL_DS] = GDT_ENTRY_INIT(0xc092, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_CS] = GDT_ENTRY_INIT(0xc0fa, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_DS] = GDT_ENTRY_INIT(0xc0f2, 0, 0xfffff), ...... #endif } }; EXPORT_PER_CPU_SYMBOL_GPL(gdt_page); 这里面对于 64 位的和 32 位的，都定义了内核代码段、内核数据段、用户代码段和用户数据段。另外，还会定义下面四个段选择子，指向上面的段描述符表项。这四个段选择子看着是不是有点眼熟？咱们讲内核初始化的时候，启动第一个用户态的进程，就是将这四个值赋值给段寄存器。 #define __KERNEL_CS (GDT_ENTRY_KERNEL_CS*8) #define __KERNEL_DS (GDT_ENTRY_KERNEL_DS*8) #define __USER_DS (GDT_ENTRY_DEFAULT_USER_DS*8 + 3) #define __USER_CS (GDT_ENTRY_DEFAULT_USER_CS*8 + 3) 通过分析，我们发现，所有的段的起始地址都是一样的，都是 0。这算哪门子分段嘛！所以，在 Linux 操作系统中，并没有使用到全部的分段功能。那分段是不是完全没有用处呢？分段可以做权限审核，例如用户态 DPL 是 3，内核态 DPL 是 0。当用户态试图访问内核态的时候，会因为权限不足而报错。其实 Linux 倾向于另外一种从虚拟地址到物理地址的转换方式，称为分页（Paging）。对于物理内存，操作系统把它分成一块一块大小相同的页，这样更方便管理，例如有的内存页面长时间不用了，可以暂时写到硬盘上，称为换出。一旦需要的时候，再加载进来，叫做换入。这样可以扩大可用物理内存的大小，提高物理内存的利用率。这个换入和换出都是以页为单位的。页面的大小一般为 4KB。为了能够定位和访问每个页，需要有个页表，保存每个页的起始地址，再加上在页内的偏移量，组成线性地址，就能对于内存中的每个位置进行访问了。 虚拟地址分为两部分，页号和页内偏移。页号作为页表的索引，页表包含物理页每页所在物理内存的基地址。这个基地址与页内偏移的组合就形成了物理内存地址。下面的图，举了一个简单的页表的例子，虚拟内存中的页通过页表映射为了物理内存中的页。 32 位环境下，虚拟地址空间共 4GB。如果分成 4KB 一个页，那就是 1M 个页。每个页表项需要 4 个字节来存储，那么整个 4GB 空间的映射就需要 4MB 的内存来存储映射表。如果每个进程都有自己的映射表，100 个进程就需要 400MB 的内存。对于内核来讲，有点大了 。页表中所有页表项必须提前建好，并且要求是连续的。如果不连续，就没有办法通过虚拟地址里面的页号找到对应的页表项了。那怎么办呢？我们可以试着将页表再分页，4G 的空间需要 4M 的页表来存储映射。我们把这 4M 分成 1K（1024）个 4K，每个 4K 又能放在一页里面，这样 1K 个 4K 就是 1K 个页，这 1K 个页也需要一个表进行管理，我们称为页目录表，这个页目录表里面有 1K 项，每项 4 个字节，页目录表大小也是 4K。页目录有 1K 项，用 10 位就可以表示访问页目录的哪一项。这一项其实对应的是一整页的页表项，也即 4K 的页表项。每个页表项也是 4 个字节，因而一整页的页表项是 1K 个。再用 10 位就可以表示访问页表项的哪一项，页表项中的一项对应的就是一个页，是存放数据的页，这个页的大小是 4K，用 12 位可以定位这个页内的任何一个位置。这样加起来正好 32 位，也就是用前 10 位定位到页目录表中的一项。将这一项对应的页表取出来共 1k 项，再用中间 10 位定位到页表中的一项，将这一项对应的存放数据的页取出来，再用最后 12 位定位到页中的具体位置访问数据。 你可能会问，如果这样的话，映射 4GB 地址空间就需要 4MB+4KB 的内存，这样不是更大了吗？ 当然如果页是满的，当时是更大了，但是，我们往往不会为一个进程分配那么多内存。比如说，上面图中，我们假设只给这个进程分配了一个数据页。如果只使用页表，也需要完整的 1M 个页表项共 4M 的内存，但是如果使用了页目录，页目录需要 1K 个全部分配，占用内存 4K，但是里面只有一项使用了。到了页表项，只需要分配能够管理那个数据页的页表项页就可以了，也就是说，最多 4K，这样内存就节省多了。当然对于 64 位的系统，两级肯定不够了，就变成了四级目录，分别是全局页目录项 PGD（Page Global Directory）、上层页目录项 PUD（Page Upper Directory）、中间页目录项 PMD（Page Middle Directory）和页表项 PTE（Page Table Entry）。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:23:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节我们讲了分段机制、分页机制以及从虚拟地址到物理地址的映射方式。总结一下这两节，我们可以把内存管理系统精细化为下面三件事情： 第一，虚拟内存空间的管理，将虚拟内存分成大小相等的页；第二，物理内存的管理，将物理内存分成大小相等的页；第三，内存映射，将虚拟内存页和物理内存页映射起来，并且在内存紧张的时候可以换出到硬盘中。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:23:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"22 | 进程空间管理：项目组还可以自行布置会议室 上两节，我们讲了内存管理的三个方面，虚拟内存空间的管理、物理内存的管理以及内存映射。你现在对进程内存空间的整体布局应该有了一个大致的了解。今天我们就来详细看看第一个方面，进程的虚拟内存空间是如何管理的。32 位系统和 64 位系统的内存布局有的地方相似，有的地方差别比较大，接下来介绍的时候，请你注意区分。好，我们现在正式开始！ ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:24:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用户态和内核态的划分 进程的虚拟地址空间，其实就是站在项目组的角度来看内存，所以我们就从 task_struct 出发来看。这里面有一个 struct mm_struct 结构来管理内存。 struct mm_struct *mm; 在 struct mm_struct 里面，有这样一个成员变量： unsigned long task_size; /* size of task vm space */ 我们之前讲过，整个虚拟内存空间要一分为二，一部分是用户态地址空间，一部分是内核态地址空间，那这两部分的分界线在哪里呢？这就要 task_size 来定义。对于 32 位的系统，内核里面是这样定义 TASK_SIZE 的： #ifdef CONFIG_X86_32 /* * User space process size: 3GB (default). */ #define TASK_SIZE PAGE_OFFSET #define TASK_SIZE_MAX TASK_SIZE /* config PAGE_OFFSET hex default 0xC0000000 depends on X86_32 */ #else /* * User space process size. 47bits minus one guard page. */ #define TASK_SIZE_MAX ((1UL \u003c\u003c 47) - PAGE_SIZE) #define TASK_SIZE (test_thread_flag(TIF_ADDR32) ? \\ IA32_PAGE_OFFSET : TASK_SIZE_MAX) ...... 当执行一个新的进程的时候，会做以下的设置： current-\u003emm-\u003etask_size = TASK_SIZE; 对于 32 位系统，最大能够寻址 2^32=4G，其中用户态虚拟地址空间是 3G，内核态是 1G。对于 64 位系统，虚拟地址只使用了 48 位。就像代码里面写的一样，1 左移了 47 位，就相当于 48 位地址空间一半的位置，0x0000800000000000，然后减去一个页，就是 0x00007FFFFFFFF000，共 128T。同样，内核空间也是 128T。内核空间和用户空间之间隔着很大的空隙，以此来进行隔离。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:24:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用户态布局 我们先来看用户态虚拟空间的布局。之前我们讲了用户态虚拟空间里面有几类数据，例如代码、全局变量、堆、栈、内存映射区等。在 struct mm_struct 里面，有下面这些变量定义了这些区域的统计信息和位置。 unsigned long mmap_base; /* base of mmap area */ unsigned long total_vm; /* Total pages mapped */ unsigned long locked_vm; /* Pages that have PG_mlocked set */ unsigned long pinned_vm; /* Refcount permanently increased */ unsigned long data_vm; /* VM_WRITE \u0026 ~VM_SHARED \u0026 ~VM_STACK */ unsigned long exec_vm; /* VM_EXEC \u0026 ~VM_WRITE \u0026 ~VM_STACK */ unsigned long stack_vm; /* VM_STACK */ unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end; 其中，total_vm 是总共映射的页的数目。我们知道，这么大的虚拟地址空间，不可能都有真实内存对应，所以这里是映射的数目。当内存吃紧的时候，有些页可以换出到硬盘上，有的页因为比较重要，不能换出。locked_vm 就是被锁定不能换出，pinned_vm 是不能换出，也不能移动。data_vm 是存放数据的页的数目，exec_vm 是存放可执行文件的页的数目，stack_vm 是栈所占的页的数目。start_code 和 end_code 表示可执行代码的开始和结束位置，start_data 和 end_data 表示已初始化数据的开始位置和结束位置。start_brk 是堆的起始位置，brk 是堆当前的结束位置。前面咱们讲过 malloc 申请一小块内存的话，就是通过改变 brk 位置实现的。start_stack 是栈的起始位置，栈的结束位置在寄存器的栈顶指针中。arg_start 和 arg_end 是参数列表的位置， env_start 和 env_end 是环境变量的位置。它们都位于栈中最高地址的地方。mmap_base 表示虚拟地址空间中用于内存映射的起始地址。一般情况下，这个空间是从高地址到低地址增长的。前面咱们讲 malloc 申请一大块内存的时候，就是通过 mmap 在这里映射一块区域到物理内存。咱们加载动态链接库 so 文件，也是在这个区域里面，映射一块区域到 so 文件。这下所有用户态的区域的位置基本上都描述清楚了。整个布局就像下面这张图这样。虽然 32 位和 64 位的空间相差很大，但是区域的类别和布局是相似的。 除了位置信息之外，struct mm_struct 里面还专门有一个结构 vm_area_struct，来描述这些区域的属性。 struct vm_area_struct *mmap; /* list of VMAs */ struct rb_root mm_rb; 这里面一个是单链表，用于将这些区域串起来。另外还有一个红黑树。又是这个数据结构，在进程调度的时候我们用的也是红黑树。它的好处就是查找和修改都很快。这里用红黑树，就是为了快速查找一个内存区域，并在需要改变的时候，能够快速修改。 struct vm_area_struct { /* The first cache line has the info for VMA tree walking. */ unsigned long vm_start; /* Our start address within vm_mm. */ unsigned long vm_end; /* The first byte after our end address within vm_mm. */ /* linked list of VM areas per task, sorted by address */ struct vm_area_struct *vm_next, *vm_prev; struct rb_node vm_rb; struct mm_struct *vm_mm; /* The address space we belong to. */ struct list_head anon_vma_chain; /* Serialized by mmap_sem \u0026 * page_table_lock */ struct anon_vma *anon_vma; /* Serialized by page_table_lock */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; struct file * vm_file; /* File we map to (can be NULL). */ void * vm_private_data; /* was vm_pte (shared mem) */ } __randomize_layout; vm_start 和 vm_end 指定了该区域在用户空间中的起始和结束地址。vm_next 和 vm_prev 将这个区域串在链表上。vm_rb 将这个区域放在红黑树上。vm_ops 里面是对这个内存区域可以做的操作的定义。虚拟内存区域可以映射到物理内存，也可以映射到文件，映射到物理内存的时候称为匿名映射，anon_vma 中，anoy 就是 anonymous，匿名的意思，映射到文件就需要有 vm_file 指定被映射的文件。那这些 vm_area_struct 是如何和上面的内存区域关联的呢？这个事情是在 load_elf_binary 里面实现的。没错，就是它。加载内核的是它，启动第一个用户态进程 init 的是它，fork 完了以后，调用 exec 运行一个二进制程序的也是它。当 exec 运行一个二进制程序的时候，除了解析 ELF 的格式之外，另外一个重要的事情就是建立内存映射。 static int load_elf_binary(struct linux_binprm *bprm) { ...... setup_new_exec(bprm); ...... retval = setup_arg_pages(bprm, randomize_stack_top(STACK_TOP), executable_stack); ...... error = elf_map(bprm-\u003efile, load_bias + vaddr, elf_ppnt, elf_prot, elf_flags, total_size); ...... retval = set_brk(elf_bss, elf_brk, bss_prot); ...... elf_entry = load_elf_interp(\u0026loc-\u003einterp_elf_ex, interpreter, \u0026interp_map_addr, load_bias, interp_elf_phdata); ...... current-\u003emm-\u003eend_code = end_code; current-\u003emm-\u003estart_code = start_code; current-\u003emm-\u003estart_data = start_data; current-\u003emm-\u003eend_data = end_data; current-\u003emm-\u003estart_stack = bprm-\u003ep; ...... } load_elf_binary 会完成以下的事情：调用 setup_new_exec，设置内存映射区 mmap_base；调用 setup_arg_pages，设置栈的 vm_area_struct，这里面设置了 mm-\u003earg_start 是指向栈底的，current-\u003emm-\u003estart_stack 就是栈底；elf_map 会将 ELF 文件中的代码部分映射到内存中来；set_brk 设置了堆的 vm_area_struct，这里面设置了 current-\u003emm-\u003estart_brk = current-\u003emm-\u003ebrk，也即堆里面还是空的；load_elf_interp 将依赖的 so 映射到内存中的内存映射区域。 最终就形成下面这个内存映射图。 映射完毕后，什么情况下会修改呢？第一种情况是函数的调用，涉及函数栈的改变，主要是改变栈顶指针。第二种情况是通过 malloc 申请一个堆内的空间，当然底层要么执行 brk，要么执行 mmap。关于内存映射的部分，我们后面的章节讲，这里我们重点看一下 brk 是怎么做的。brk 系统调用实现的入口是 sys_brk 函数，就像下面代码定义的一样。 SYSCALL_DEFINE1(brk, unsigned long, brk) { unsigned long retval; unsigned long newbrk, oldb","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:24:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"内核态的布局 用户态虚拟空间分析完毕，接下来我们分析内核态虚拟空间。内核态的虚拟空间和某一个进程没有关系，所有进程通过系统调用进入到内核之后，看到的虚拟地址空间都是一样的。这里强调一下，千万别以为到了内核里面，咱们就会直接使用物理内存地址了，想当然地认为下面讨论的都是物理内存地址，不是的，这里讨论的还是虚拟内存地址，但是由于内核总是涉及管理物理内存，因而总是隐隐约约发生关系，所以这里必须思路清晰，分清楚物理内存地址和虚拟内存地址。在内核态，32 位和 64 位的布局差别比较大，主要是因为 32 位内核态空间太小了。我们来看 32 位的内核态的布局。 32 位的内核态虚拟地址空间一共就 1G，占绝大部分的前 896M，我们称为直接映射区。所谓的直接映射区，就是这一块空间是连续的，和物理内存是非常简单的映射关系，其实就是虚拟内存地址减去 3G，就得到物理内存的位置。在内核里面，有两个宏： __pa(vaddr) 返回与虚拟地址 vaddr 相关的物理地址；__va(paddr) 则计算出对应于物理地址 paddr 的虚拟地址。 #define __va(x) ((void *)((unsigned long)(x)+PAGE_OFFSET)) #define __pa(x) __phys_addr((unsigned long)(x)) #define __phys_addr(x) __phys_addr_nodebug(x) #define __phys_addr_nodebug(x) ((x) - PAGE_OFFSET) 但是你要注意，这里虚拟地址和物理地址发生了关联关系，在物理内存的开始的 896M 的空间，会被直接映射到 3G 至 3G+896M 的虚拟地址，这样容易给你一种感觉，这些内存访问起来和物理内存差不多，别这样想，在大部分情况下，对于这一段内存的访问，在内核中，还是会使用虚拟地址的，并且将来也会为这一段空间建设页表，对这段地址的访问也会走上一节我们讲的分页地址的流程，只不过页表里面比较简单，是直接的一一对应而已。这 896M 还需要仔细分解。在系统启动的时候，物理内存的前 1M 已经被占用了，从 1M 开始加载内核代码段，然后就是内核的全局变量、BSS 等，也是 ELF 里面涵盖的。这样内核的代码段，全局变量，BSS 也就会被映射到 3G 后的虚拟地址空间里面。具体的物理内存布局可以查看 /proc/iomem。在内核运行的过程中，如果碰到系统调用创建进程，会创建 task_struct 这样的实例，内核的进程管理代码会将实例创建在 3G 至 3G+896M 的虚拟空间中，当然也会被放在物理内存里面的前 896M 里面，相应的页表也会被创建。在内核运行的过程中，会涉及内核栈的分配，内核的进程管理的代码会将内核栈创建在 3G 至 3G+896M 的虚拟空间中，当然也就会被放在物理内存里面的前 896M 里面，相应的页表也会被创建。896M 这个值在内核中被定义为 high_memory，在此之上常称为“高端内存”。这是个很笼统的说法，到底是虚拟内存的 3G+896M 以上的是高端内存，还是物理内存 896M 以上的是高端内存呢？ 这里仍然需要辨析一下，高端内存是物理内存的概念。它仅仅是内核中的内存管理模块看待物理内存的时候的概念。前面我们也说过，在内核中，除了内存管理模块直接操作物理地址之外，内核的其他模块，仍然要操作虚拟地址，而虚拟地址是需要内存管理模块分配和映射好的。假设咱们的电脑有 2G 内存，现在如果内核的其他模块想要访问物理内存 1.5G 的地方，应该怎么办呢？如果你觉得，我有 32 位的总线，访问个 2G 还不小菜一碟，这就错了。首先，你不能使用物理地址。你需要使用内存管理模块给你分配的虚拟地址，但是虚拟地址的 0 到 3G 已经被用户态进程占用去了，你作为内核不能使用。因为你写 1.5G 的虚拟内存位置，一方面你不知道应该根据哪个进程的页表进行映射；另一方面，就算映射了也不是你真正想访问的物理内存的地方，所以你发现你作为内核，能够使用的虚拟内存地址，只剩下 1G 减去 896M 的空间了。于是，我们可以将剩下的虚拟内存地址分成下面这几个部分。 在 896M 到 VMALLOC_START 之间有 8M 的空间。VMALLOC_START 到 VMALLOC_END 之间称为内核动态映射空间，也即内核想像用户态进程一样 malloc 申请内存，在内核里面可以使用 vmalloc。假设物理内存里面，896M 到 1.5G 之间已经被用户态进程占用了，并且映射关系放在了进程的页表中，内核 vmalloc 的时候，只能从分配物理内存 1.5G 开始，就需要使用这一段的虚拟地址进行映射，映射关系放在专门给内核自己用的页表里面。PKMAP_BASE 到 FIXADDR_START 的空间称为持久内核映射。使用 alloc_pages() 函数的时候，在物理内存的高端内存得到 struct page 结构，可以调用 kmap 将其映射到这个区域。FIXADDR_START 到 FIXADDR_TOP(0xFFFF F000) 的空间，称为固定映射区域，主要用于满足特殊需求。在最后一个区域可以通过 kmap_atomic 实现临时内核映射。假设用户态的进程要映射一个文件到内存中，先要映射用户态进程空间的一段虚拟地址到物理内存，然后将文件内容写入这个物理内存供用户态进程访问。给用户态进程分配物理内存页可以通过 alloc_pages()，分配完毕后，按说将用户态进程虚拟地址和物理内存的映射关系放在用户态进程的页表中，就完事大吉了。这个时候，用户态进程可以通过用户态的虚拟地址，也即 0 至 3G 的部分，经过页表映射后访问物理内存，并不需要内核态的虚拟地址里面也划出一块来，映射到这个物理内存页。但是如果要把文件内容写入物理内存，这件事情要内核来干了，这就只好通过 kmap_atomic 做一个临时映射，写入物理内存完毕后，再 kunmap_atomic 来解映射即可。 32 位的内核态布局我们看完了，接下来我们再来看 64 位的内核布局。其实 64 位的内核布局反而简单，因为虚拟空间实在是太大了，根本不需要所谓的高端内存，因为内核是 128T，根本不可能有物理内存超过这个值。64 位的内存布局如图所示。 64 位的内核主要包含以下几个部分。从 0xffff800000000000 开始就是内核的部分，只不过一开始有 8T 的空档区域。从 __PAGE_OFFSET_BASE(0xffff880000000000) 开始的 64T 的虚拟地址空间是直接映射区域，也就是减去 PAGE_OFFSET 就是物理地址。虚拟地址和物理地址之间的映射在大部分情况下还是会通过建立页表的方式进行映射。从 VMALLOC_START（0xffffc90000000000）开始到 VMALLOC_END（0xffffe90000000000）的 32T 的空间是给 vmalloc 的。从 VMEMMAP_START（0xffffea0000000000）开始的 1T 空间用于存放物理页面的描述结构 struct page 的。从 __START_KERNEL_map（0xffffffff80000000）开始的 512M 用于存放内核代码段、全局变量、BSS 等。这里对应到物理内存开始的位置，减去 __START_KERNEL_map 就能得到物理内存的地址。这里和直接映射区有点像，但是不矛盾，因为直接映射区之前有 8T 的空当区域，早就过了内核代码在物理内存中加载的位置。到这里内核中虚拟空间的布局就介绍完了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:24:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 还记得咱们上一节咱们收集项目组需求的时候，我们知道一个进程要运行起来需要以下的内存结构。 用户态：代码段、全局变量、BSS 函数栈 堆 内存映射区 内核态：内核的代码、全局变量、BSS内核 数据结构例如 task_struct 内核栈 内核中动态分配的内存 现在这些是不是已经都有了着落？我画了一个图，总结一下进程运行状态在 32 位下对应关系。 对于 64 位的对应关系，只是稍有区别，我这里也画了一个图，方便你对比理解。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:24:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"23 | 物理内存管理（上）：会议室管理员如何分配会议室？ 前一节，我们讲了如何从项目经理的角度看内存，看到的是虚拟地址空间，这些虚拟的地址，总是要映射到物理的页面。这一节，我们来看，物理的页面是如何管理的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:25:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"物理内存的组织方式 前面咱们讲虚拟内存，涉及物理内存的映射的时候，我们总是把内存想象成它是由连续的一页一页的块组成的。我们可以从 0 开始对物理页编号，这样每个物理页都会有个页号。由于物理地址是连续的，页也是连续的，每个页大小也是一样的。因而对于任何一个地址，只要直接除一下每页的大小，很容易直接算出在哪一页。每个页有一个结构 struct page 表示，这个结构也是放在一个数组里面，这样根据页号，很容易通过下标找到相应的 struct page 结构。如果是这样，整个物理内存的布局就非常简单、易管理，这就是最经典的平坦内存模型（Flat Memory Model）。我们讲 x86 的工作模式的时候，讲过 CPU 是通过总线去访问内存的，这就是最经典的内存使用方式。 在这种模式下，CPU 也会有多个，在总线的一侧。所有的内存条组成一大片内存，在总线的另一侧，所有的 CPU 访问内存都要过总线，而且距离都是一样的，这种模式称为 SMP（Symmetric multiprocessing），即对称多处理器。当然，它也有一个显著的缺点，就是总线会成为瓶颈，因为数据都要走它。 为了提高性能和可扩展性，后来有了一种更高级的模式，NUMA（Non-uniform memory access），非一致内存访问。在这种模式下，内存不是一整块。每个 CPU 都有自己的本地内存，CPU 访问本地内存不用过总线，因而速度要快很多，每个 CPU 和内存在一起，称为一个 NUMA 节点。但是，在本地内存不足的情况下，每个 CPU 都可以去另外的 NUMA 节点申请内存，这个时候访问延时就会比较长。这样，内存被分成了多个节点，每个节点再被分成一个一个的页面。由于页需要全局唯一定位，页还是需要有全局唯一的页号的。但是由于物理内存不是连起来的了，页号也就不再连续了。于是内存模型就变成了非连续内存模型，管理起来就复杂一些。这里需要指出的是，NUMA 往往是非连续内存模型。而非连续内存模型不一定就是 NUMA，有时候一大片内存的情况下，也会有物理内存地址不连续的情况。后来内存技术牛了，可以支持热插拔了。这个时候，不连续成为常态，于是就有了稀疏内存模型。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:25:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"节点 我们主要解析当前的主流场景，NUMA 方式。我们首先要能够表示 NUMA 节点的概念，于是有了下面这个结构 typedef struct pglist_data pg_data_t，它里面有以下的成员变量： 每一个节点都有自己的 ID：node_id；node_mem_map 就是这个节点的 struct page 数组，用于描述这个节点里面的所有的页；node_start_pfn 是这个节点的起始页号；node_spanned_pages 是这个节点中包含不连续的物理内存地址的页面数；node_present_pages 是真正可用的物理页面的数目。 例如，64M 物理内存隔着一个 4M 的空洞，然后是另外的 64M 物理内存。这样换算成页面数目就是，16K 个页面隔着 1K 个页面，然后是另外 16K 个页面。这种情况下，node_spanned_pages 就是 33K 个页面，node_present_pages 就是 32K 个页面。 typedef struct pglist_data { struct zone node_zones[MAX_NR_ZONES]; struct zonelist node_zonelists[MAX_ZONELISTS]; int nr_zones; struct page *node_mem_map; unsigned long node_start_pfn; unsigned long node_present_pages; /* total number of physical pages */ unsigned long node_spanned_pages; /* total size of physical page range, including holes */ int node_id; ...... } pg_data_t; 每一个节点分成一个个区域 zone，放在数组 node_zones 里面。这个数组的大小为 MAX_NR_ZONES。我们来看区域的定义。 enum zone_type { #ifdef CONFIG_ZONE_DMA ZONE_DMA, #endif #ifdef CONFIG_ZONE_DMA32 ZONE_DMA32, #endif ZONE_NORMAL, #ifdef CONFIG_HIGHMEM ZONE_HIGHMEM, #endif ZONE_MOVABLE, __MAX_NR_ZONES }; ZONE_DMA 是指可用于作 DMA（Direct Memory Access，直接内存存取）的内存。DMA 是这样一种机制：要把外设的数据读入内存或把内存的数据传送到外设，原来都要通过 CPU 控制完成，但是这会占用 CPU，影响 CPU 处理其他事情，所以有了 DMA 模式。CPU 只需向 DMA 控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，这样就可以解放 CPU。对于 64 位系统，有两个 DMA 区域。除了上面说的 ZONE_DMA，还有 ZONE_DMA32。在这里你大概理解 DMA 的原理就可以，不必纠结，我们后面会讲 DMA 的机制。ZONE_NORMAL 是直接映射区，就是上一节讲的，从物理内存到虚拟内存的内核区域，通过加上一个常量直接映射。ZONE_HIGHMEM 是高端内存区，就是上一节讲的，对于 32 位系统来说超过 896M 的地方，对于 64 位没必要有的一段区域。ZONE_MOVABLE 是可移动区域，通过将物理内存划分为可移动分配区域和不可移动分配区域来避免内存碎片。这里你需要注意一下，我们刚才对于区域的划分，都是针对物理内存的。 nr_zones 表示当前节点的区域的数量。node_zonelists 是备用节点和它的内存区域的情况。前面讲 NUMA 的时候，我们讲了 CPU 访问内存，本节点速度最快，但是如果本节点内存不够怎么办，还是需要去其他节点进行分配。毕竟，就算在备用节点里面选择，慢了点也比没有强。既然整个内存被分成了多个节点，那 pglist_data 应该放在一个数组里面。每个节点一项，就像下面代码里面一样： struct pglist_data *node_data[MAX_NUMNODES] __read_mostly; ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:25:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"区域 到这里，我们把内存分成了节点，把节点分成了区域。接下来我们来看，一个区域里面是如何组织的。表示区域的数据结构 zone 的定义如下： struct zone { ...... struct pglist_data *zone_pgdat; struct per_cpu_pageset __percpu *pageset; unsigned long zone_start_pfn; /* * spanned_pages is the total pages spanned by the zone, including * holes, which is calculated as: * spanned_pages = zone_end_pfn - zone_start_pfn; * * present_pages is physical pages existing within the zone, which * is calculated as: * present_pages = spanned_pages - absent_pages(pages in holes); * * managed_pages is present pages managed by the buddy system, which * is calculated as (reserved_pages includes pages allocated by the * bootmem allocator): * managed_pages = present_pages - reserved_pages; * */ unsigned long managed_pages; unsigned long spanned_pages; unsigned long present_pages; const char *name; ...... /* free areas of different sizes */ struct free_area free_area[MAX_ORDER]; /* zone flags, see below */ unsigned long flags; /* Primarily protects free_area */ spinlock_t lock; ...... } ____cacheline_internodealigned_in_ 在一个 zone 里面，zone_start_pfn 表示属于这个 zone 的第一个页。如果我们仔细看代码的注释，可以看到，spanned_pages = zone_end_pfn - zone_start_pfn，也即 spanned_pages 指的是不管中间有没有物理内存空洞，反正就是最后的页号减去起始的页号。present_pages = spanned_pages - absent_pages(pages in holes)，也即 present_pages 是这个 zone 在物理内存中真实存在的所有 page 数目。managed_pages = present_pages - reserved_pages，也即 managed_pages 是这个 zone 被伙伴系统管理的所有的 page 数目，伙伴系统的工作机制我们后面会讲。per_cpu_pageset 用于区分冷热页。什么叫冷热页呢？咱们讲 x86 体系结构的时候讲过，为了让 CPU 快速访问段描述符，在 CPU 里面有段描述符缓存。CPU 访问这个缓存的速度比内存快得多。同样对于页面来讲，也是这样的。如果一个页被加载到 CPU 高速缓存里面，这就是一个热页（Hot Page），CPU 读起来速度会快很多，如果没有就是冷页（Cold Page）。由于每个 CPU 都有自己的高速缓存，因而 per_cpu_pageset 也是每个 CPU 一个。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:25:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"页 了解了区域 zone，接下来我们就到了组成物理内存的基本单位，页的数据结构 struct page。这是一个特别复杂的结构，里面有很多的 union，union 结构是在 C 语言中被用于同一块内存根据情况保存不同类型数据的一种方式。这里之所以用了 union，是因为一个物理页面使用模式有多种。第一种模式，要用就用一整页。这一整页的内存，或者直接和虚拟地址空间建立映射关系，我们把这种称为匿名页（Anonymous Page）。或者用于关联一个文件，然后再和虚拟地址空间建立映射关系，这样的文件，我们称为内存映射文件（Memory-mapped File）。如果某一页是这种使用模式，则会使用 union 中的以下变量： struct address_space *mapping 就是用于内存映射，如果是匿名页，最低位为 1；如果是映射文件，最低位为 0；pgoff_t index 是在映射区的偏移量；atomic_t _mapcount，每个进程都有自己的页表，这里指有多少个页表项指向了这个页；struct list_head lru 表示这一页应该在一个链表上，例如这个页面被换出，就在换出页的链表中；compound 相关的变量用于复合页（Compound Page），就是将物理上连续的两个或多个页看成一个独立的大页。 第二种模式，仅需分配小块内存。有时候，我们不需要一下子分配这么多的内存，例如分配一个 task_struct 结构，只需要分配小块的内存，去存储这个进程描述结构的对象。为了满足对这种小内存块的需要，Linux 系统采用了一种被称为 slab allocator 的技术，用于分配称为 slab 的一小块内存。它的基本原理是从内存管理模块申请一整块页，然后划分成多个小块的存储池，用复杂的队列来维护这些小块的状态（状态包括：被分配了 / 被放回池子 / 应该被回收）。也正是因为 slab allocator 对于队列的维护过于复杂，后来就有了一种不使用队列的分配器 slub allocator，后面我们会解析这个分配器。但是你会发现，它里面还是用了很多 slab 的字眼，因为它保留了 slab 的用户接口，可以看成 slab allocator 的另一种实现。还有一种小块内存的分配器称为 slob，非常简单，主要使用在小型的嵌入式系统。如果某一页是用于分割成一小块一小块的内存进行分配的使用模式，则会使用 union 中的以下变量： s_mem 是已经分配了正在使用的 slab 的第一个对象；freelist 是池子中的空闲对象；rcu_head 是需要释放的列表。 struct page { unsigned long flags; union { struct address_space *mapping; void *s_mem; /* slab first object */ atomic_t compound_mapcount; /* first tail page */ }; union { pgoff_t index; /* Our offset within mapping. */ void *freelist; /* sl[aou]b first free object */ }; union { unsigned counters; struct { union { atomic_t _mapcount; unsigned int active; /* SLAB */ struct { /* SLUB */ unsigned inuse:16; unsigned objects:15; unsigned frozen:1; }; int units; /* SLOB */ }; atomic_t _refcount; }; }; union { struct list_head lru; /* Pageout list */ struct dev_pagemap *pgmap; struct { /* slub per cpu partial pages */ struct page *next; /* Next partial slab */ int pages; /* Nr of partial slabs left */ int pobjects; /* Approximate # of objects */ }; struct rcu_head rcu_head; struct { unsigned long compound_head; /* If bit zero is set */ unsigned int compound_dtor; unsigned int compound_order; }; }; union { unsigned long private; struct kmem_cache *slab_cache; /* SL[AU]B: Pointer to slab */ }; ...... } ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:25:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"页的分配 好了，前面我们讲了物理内存的组织，从节点到区域到页到小块。接下来，我们来看物理内存的分配。对于要分配比较大的内存，例如到分配页级别的，可以使用伙伴系统（Buddy System）。Linux 中的内存管理的“页”大小为 4KB。把所有的空闲页分组为 11 个页块链表，每个块链表分别包含很多个大小的页块，有 1、2、4、8、16、32、64、128、256、512 和 1024 个连续页的页块。最大可以申请 1024 个连续页，对应 4MB 大小的连续内存。每个页块的第一个页的物理地址是该页块大小的整数倍。 第 i 个页块链表中，页块中页的数目为 2^i。在 struct zone 里面有以下的定义： struct free_area free_area[MAX_ORDER]; MAX_ORDER 就是指数。 #define MAX_ORDER 11 当向内核请求分配 (2^(i-1)，2^i]数目的页块时，按照 2^i 页块请求处理。如果对应的页块链表中没有空闲页块，那我们就在更大的页块链表中去找。当分配的页块中有多余的页时，伙伴系统会根据多余的页块大小插入到对应的空闲页块链表中。例如，要请求一个 128 个页的页块时，先检查 128 个页的页块链表是否有空闲块。如果没有，则查 256 个页的页块链表；如果有空闲块的话，则将 256 个页的页块分成两份，一份使用，一份插入 128 个页的页块链表中。如果还是没有，就查 512 个页的页块链表；如果有的话，就分裂为 128、128、256 三个页块，一个 128 的使用，剩余两个插入对应页块链表。上面这个过程，我们可以在分配页的函数 alloc_pages 中看到。 static inline struct page * alloc_pages(gfp_t gfp_mask, unsigned int order) { return alloc_pages_current(gfp_mask, order); } /** * alloc_pages_current - Allocate pages. * * @gfp: * %GFP_USER user allocation, * %GFP_KERNEL kernel allocation, * %GFP_HIGHMEM highmem allocation, * %GFP_FS don't call back into a file system. * %GFP_ATOMIC don't sleep. * @order: Power of two of allocation size in pages. 0 is a single page. * * Allocate a page from the kernel page pool. When not in * interrupt context and apply the current process NUMA policy. * Returns NULL when no page can be allocated. */ struct page *alloc_pages_current(gfp_t gfp, unsigned order) { struct mempolicy *pol = \u0026default_policy; struct page *page; ...... page = __alloc_pages_nodemask(gfp, order, policy_node(gfp, pol, numa_node_id()), policy_nodemask(gfp, pol)); ...... return page; } alloc_pages 会调用 alloc_pages_current，这里面的注释比较容易看懂了，gfp 表示希望在哪个区域中分配这个内存：GFP_USER 用于分配一个页映射到用户进程的虚拟地址空间，并且希望直接被内核或者硬件访问，主要用于一个用户进程希望通过内存映射的方式，访问某些硬件的缓存，例如显卡缓存；GFP_KERNEL 用于内核中分配页，主要分配 ZONE_NORMAL 区域，也即直接映射区；GFP_HIGHMEM，顾名思义就是主要分配高端区域的内存。 另一个参数 order，就是表示分配 2 的 order 次方个页。接下来调用 __alloc_pages_nodemask。这是伙伴系统的核心方法。它会调用 get_page_from_freelist。这里面的逻辑也很容易理解，就是在一个循环中先看当前节点的 zone。如果找不到空闲页，则再看备用节点的 zone。 static struct page * get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags, const struct alloc_context *ac) { ...... for_next_zone_zonelist_nodemask(zone, z, ac-\u003ezonelist, ac-\u003ehigh_zoneidx, ac-\u003enodemask) { struct page *page; ...... page = rmqueue(ac-\u003epreferred_zoneref-\u003ezone, zone, order, gfp_mask, alloc_flags, ac-\u003emigratetype); ...... } 每一个 zone，都有伙伴系统维护的各种大小的队列，就像上面伙伴系统原理里讲的那样。这里调用 rmqueue 就很好理解了，就是找到合适大小的那个队列，把页面取下来。接下来的调用链是 rmqueue-\u003e__rmqueue-\u003e__rmqueue_smallest。在这里，我们能清楚看到伙伴系统的逻辑。 static inline struct page *__rmqueue_smallest(struct zone *zone, unsigned int order, int migratetype) { unsigned int current_order; struct free_area *area; struct page *page; /* Find a page of the appropriate size in the preferred list */ for (current_order = order; current_order \u003c MAX_ORDER; ++current_order) { area = \u0026(zone-\u003efree_area[current_order]); page = list_first_entry_or_null(\u0026area-\u003efree_list[migratetype], struct page, lru); if (!page) continue; list_del(\u0026page-\u003elru); rmv_page_order(page); area-\u003enr_free--; expand(zone, page, order, current_order, area, migratetype); set_pcppage_migratetype(page, migratetype); return page; } return NULL; 从当前的 order，也即指数开始，在伙伴系统的 free_area 找 2^order 大小的页块。如果链表的第一个不为空，就找到了；如果为空，就到更大的 order 的页块链表里面去找。找到以后，除了将页块从链表中取下来，我们还要把多余部分放到其他页块链表里面。expand 就是干这个事情的。area–就是伙伴系统那个表里面的前一项，前一项里面的页块大小是当前项的页块大小除以 2，size 右移一位也就是除以 2，list_add 就是加到链表上，nr_free++ 就是计数加 1。 static inline void expand(struct zone *zone, struct page *page, int low, int high, struct free_area *area, int migratetype) { unsigned long size = 1 \u003c\u003c high; while (high \u003e low) { area--; high--; size \u003e\u003e= 1; ...... list_add(\u0026page[size].lru, \u0026area-\u003efree_list[migratetype]); area-\u003enr_free++; set_page_order(\u0026page[size], high); } } ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:25:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 对于物理内存的管理的讲解，到这里要告一段落了。这一节我们主要讲了物理内存的组织形式，就像下面图中展示的一样。如果有多个 CPU，那就有多个节点。每个节点用 struct pglist_data 表示，放在一个数组里面。每个节点分为多个区域，每个区域用 struct zone 表示，也放在一个数组里面。每个区域分为多个页。为了方便分配，空闲页放在 struct free_area 里面，使用伙伴系统进行管理和分配，每一页用 struct page 表示。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:25:6","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"24 | 物理内存管理（下）：会议室管理员如何分配会议室？ 前一节，前面我们解析了整页的分配机制。如果遇到小的对象，物理内存是如何分配的呢？这一节，我们一起来看一看。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:26:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"小内存的分配 前面我们讲过，如果遇到小的对象，会使用 slub 分配器进行分配。那我们就先来解析它的工作原理。还记得咱们创建进程的时候，会调用 dup_task_struct，它想要试图复制一个 task_struct 对象，需要先调用 alloc_task_struct_node，分配一个 task_struct 对象。从这段代码可以看出，它调用了 kmem_cache_alloc_node 函数，在 task_struct 的缓存区域 task_struct_cachep 分配了一块内存。 static struct kmem_cache *task_struct_cachep; task_struct_cachep = kmem_cache_create(\"task_struct\", arch_task_struct_size, align, SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT, NULL); static inline struct task_struct *alloc_task_struct_node(int node) { return kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node); } static inline void free_task_struct(struct task_struct *tsk) { kmem_cache_free(task_struct_cachep, tsk); } 在系统初始化的时候，task_struct_cachep 会被 kmem_cache_create 函数创建。这个函数也比较容易看懂，专门用于分配 task_struct 对象的缓存。这个缓存区的名字就叫 task_struct。缓存区中每一块的大小正好等于 task_struct 的大小，也即 arch_task_struct_size。有了这个缓存区，每次创建 task_struct 的时候，我们不用到内存里面去分配，先在缓存里面看看有没有直接可用的，这就是 kmem_cache_alloc_node 的作用。当一个进程结束，task_struct 也不用直接被销毁，而是放回到缓存中，这就是 kmem_cache_free 的作用。这样，新进程创建的时候，我们就可以直接用现成的缓存中的 task_struct 了。我们来仔细看看，缓存区 struct kmem_cache 到底是什么样子。 struct kmem_cache { struct kmem_cache_cpu __percpu *cpu_slab; /* Used for retriving partial slabs etc */ unsigned long flags; unsigned long min_partial; int size; /* The size of an object including meta data */ int object_size; /* The size of an object without meta data */ int offset; /* Free pointer offset. */ #ifdef CONFIG_SLUB_CPU_PARTIAL int cpu_partial; /* Number of per cpu partial objects to keep around */ #endif struct kmem_cache_order_objects oo; /* Allocation and freeing of slabs */ struct kmem_cache_order_objects max; struct kmem_cache_order_objects min; gfp_t allocflags; /* gfp flags to use on each alloc */ int refcount; /* Refcount for slab cache destroy */ void (*ctor)(void *); ...... const char *name; /* Name (only for display!) */ struct list_head list; /* List of slab caches */ ...... struct kmem_cache_node *node[MAX_NUMNODES]; }; 在 struct kmem_cache 里面，有个变量 struct list_head list，这个结构我们已经看到过多次了。我们可以想象一下，对于操作系统来讲，要创建和管理的缓存绝对不止 task_struct。难道 mm_struct 就不需要吗？fs_struct 就不需要吗？都需要。因此，所有的缓存最后都会放在一个链表里面，也就是 LIST_HEAD(slab_caches)。对于缓存来讲，其实就是分配了连续几页的大内存块，然后根据缓存对象的大小，切成小内存块。所以，我们这里有三个 kmem_cache_order_objects 类型的变量。这里面的 order，就是 2 的 order 次方个页面的大内存块，objects 就是能够存放的缓存对象的数量。最终，我们将大内存块切分成小内存块，样子就像下面这样。 每一项的结构都是缓存对象后面跟一个下一个空闲对象的指针，这样非常方便将所有的空闲对象链成一个链。其实，这就相当于咱们数据结构里面学的，用数组实现一个可随机插入和删除的链表。所以，这里面就有三个变量：size 是包含这个指针的大小，object_size 是纯对象的大小，offset 就是把下一个空闲对象的指针存放在这一项里的偏移量。那这些缓存对象哪些被分配了、哪些在空着，什么情况下整个大内存块都被分配完了，需要向伙伴系统申请几个页形成新的大内存块？这些信息该由谁来维护呢？接下来就是最重要的两个成员变量出场的时候了。kmem_cache_cpu 和 kmem_cache_node，它们都是每个 NUMA 节点上有一个，我们只需要看一个节点里面的情况。 在分配缓存块的时候，要分两种路径，fast path 和 slow path，也就是快速通道和普通通道。其中 kmem_cache_cpu 就是快速通道，kmem_cache_node 是普通通道。每次分配的时候，要先从 kmem_cache_cpu 进行分配。如果 kmem_cache_cpu 里面没有空闲的块，那就到 kmem_cache_node 中进行分配；如果还是没有空闲的块，才去伙伴系统分配新的页。我们来看一下，kmem_cache_cpu 里面是如何存放缓存块的。 struct kmem_cache_cpu { void **freelist; /* Pointer to next available object */ unsigned long tid; /* Globally unique transaction id */ struct page *page; /* The slab from which we are allocating */ #ifdef CONFIG_SLUB_CPU_PARTIAL struct page *partial; /* Partially allocated frozen slabs */ #endif ...... }; 在这里，page 指向大内存块的第一个页，缓存块就是从里面分配的。freelist 指向大内存块里面第一个空闲的项。按照上面说的，这一项会有指针指向下一个空闲的项，最终所有空闲的项会形成一个链表。partial 指向的也是大内存块的第一个页，之所以名字叫 partial（部分），就是因为它里面部分被分配出去了，部分是空的。这是一个备用列表，当 page 满了，就会从这里找。我们再来看 kmem_cache_node 的定义。 struct kmem_cache_node { spinlock_t list_lock; ...... #ifdef CONFIG_SLUB unsigned long nr_partial; struct list_head partial; ...... #endif }; 这里面也有一个 partial，是一个链表。这个链表里存放的是部分空闲的内存块。这是 kmem_cache_cpu 里面的 partial 的备用列表，如果那里没有，就到这里来找。下面我们就来看看这个分配过程。kmem_cache_alloc_node 会调用 slab_alloc_node。你还是先重点看这里面的注释，这里面说的就是快速通道和普通通道的概念。 /* * Inlined fastpath so that allocation functions (kmalloc, kmem_cache_alloc) * have the fastpath folded into their functions. So no function call * overhead for requests that can be satisfied on the fastpath. * * The fastpath works by first checking if the lockless freelist can b","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:26:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"页面换出 另一个物理内存管理必须要处理的事情就是，页面换出。每个进程都有自己的虚拟地址空间，无论是 32 位还是 64 位，虚拟地址空间都非常大，物理内存不可能有这么多的空间放得下。所以，一般情况下，页面只有在被使用的时候，才会放在物理内存中。如果过了一段时间不被使用，即便用户进程并没有释放它，物理内存管理也有责任做一定的干预。例如，将这些物理内存中的页面换出到硬盘上去；将空出的物理内存，交给活跃的进程去使用。什么情况下会触发页面换出呢？可以想象，最常见的情况就是，分配内存的时候，发现没有地方了，就试图回收一下。例如，咱们解析申请一个页面的时候，会调用 get_page_from_freelist，接下来的调用链为 get_page_from_freelist-\u003enode_reclaim-\u003e__node_reclaim-\u003eshrink_node，通过这个调用链可以看出，页面换出也是以内存节点为单位的。当然还有一种情况，就是作为内存管理系统应该主动去做的，而不能等真的出了事儿再做，这就是内核线程 kswapd。这个内核线程，在系统初始化的时候就被创建。这样它会进入一个无限循环，直到系统停止。在这个循环中，如果内存使用没有那么紧张，那它就可以放心睡大觉；如果内存紧张了，就需要去检查一下内存，看看是否需要换出一些内存页。 /* * The background pageout daemon, started as a kernel thread * from the init process. * * This basically trickles out pages so that we have _some_ * free memory available even if there is no other activity * that frees anything up. This is needed for things like routing * etc, where we otherwise might have all activity going on in * asynchronous contexts that cannot page things out. * * If there are applications that are active memory-allocators * (most normal use), this basically shouldn't matter. */ static int kswapd(void *p) { unsigned int alloc_order, reclaim_order; unsigned int classzone_idx = MAX_NR_ZONES - 1; pg_data_t *pgdat = (pg_data_t*)p; struct task_struct *tsk = current; for ( ; ; ) { ...... kswapd_try_to_sleep(pgdat, alloc_order, reclaim_order, classzone_idx); ...... reclaim_order = balance_pgdat(pgdat, alloc_order, classzone_idx); ...... } } 这里的调用链是 balance_pgdat-\u003ekswapd_shrink_node-\u003eshrink_node，是以内存节点为单位的，最后也是调用 shrink_node。shrink_node 会调用 shrink_node_memcg。这里面有一个循环处理页面的列表，看这个函数的注释，其实和上面我们想表达的内存换出是一样的。 /* * This is a basic per-node page freer. Used by both kswapd and direct reclaim. */ static void shrink_node_memcg(struct pglist_data *pgdat, struct mem_cgroup *memcg, struct scan_control *sc, unsigned long *lru_pages) { ...... unsigned long nr[NR_LRU_LISTS]; enum lru_list lru; ...... while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] || nr[LRU_INACTIVE_FILE]) { unsigned long nr_anon, nr_file, percentage; unsigned long nr_scanned; for_each_evictable_lru(lru) { if (nr[lru]) { nr_to_scan = min(nr[lru], SWAP_CLUSTER_MAX); nr[lru] -= nr_to_scan; nr_reclaimed += shrink_list(lru, nr_to_scan, lruvec, memcg, sc); } } ...... } ...... 这里面有个 lru 列表。从下面的定义，我们可以想象，所有的页面都被挂在 LRU 列表中。LRU 是 Least Recent Use，也就是最近最少使用。也就是说，这个列表里面会按照活跃程度进行排序，这样就容易把不怎么用的内存页拿出来做处理。内存页总共分两类，一类是匿名页，和虚拟地址空间进行关联；一类是内存映射，不但和虚拟地址空间关联，还和文件管理关联。它们每一类都有两个列表，一个是 active，一个是 inactive。顾名思义，active 就是比较活跃的，inactive 就是不怎么活跃的。这两个里面的页会变化，过一段时间，活跃的可能变为不活跃，不活跃的可能变为活跃。如果要换出内存，那就是从不活跃的列表中找出最不活跃的，换出到硬盘上。 enum lru_list { LRU_INACTIVE_ANON = LRU_BASE, LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE, LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE, LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE, LRU_UNEVICTABLE, NR_LRU_LISTS }; #define for_each_evictable_lru(lru) for (lru = 0; lru \u003c= LRU_ACTIVE_FILE; lru++) static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan, struct lruvec *lruvec, struct mem_cgroup *memcg, struct scan_control *sc) { if (is_active_lru(lru)) { if (inactive_list_is_low(lruvec, is_file_lru(lru), memcg, sc, true)) shrink_active_list(nr_to_scan, lruvec, sc, lru); return 0; } return shrink_inactive_list(nr_to_scan, lruvec, sc, lru); 从上面的代码可以看出，shrink_list 会先缩减活跃页面列表，再压缩不活跃的页面列表。对于不活跃列表的缩减，shrink_inactive_list 就需要对页面进行回收；对于匿名页来讲，需要分配 swap，将内存页写入文件系统；对于内存映射关联了文件的，我们需要将在内存中对于文件的修改写回到文件中。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:26:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 好了，对于物理内存的管理就讲到这里了，我们来总结一下。对于物理内存来讲，从下层到上层的关系及分配模式如下： 物理内存分 NUMA 节点，分别进行管理；每个 NUMA 节点分成多个内存区域；每个内存区域分成多个物理页面；伙伴系统将多个连续的页面作为一个大的内存块分配给上层；kswapd 负责物理页面的换入换出；Slub Allocator 将从伙伴系统申请的大内存块切成小块，分配给其他系统。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:26:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"25 | 用户态内存映射：如何找到正确的会议室？ 前面几节，我们既看了虚拟内存空间如何组织的，也看了物理页面如何管理的。现在我们需要一些数据结构，将二者关联起来。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:27:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"mmap 的原理 在虚拟地址空间那一节，我们知道，每一个进程都有一个列表 vm_area_struct，指向虚拟地址空间的不同的内存块，这个变量的名字叫 mmap。 struct mm_struct { struct vm_area_struct *mmap; /* list of VMAs */ ...... } struct vm_area_struct { /* * For areas with an address space and backing store, * linkage into the address_space-\u003ei_mmap interval tree. */ struct { struct rb_node rb; unsigned long rb_subtree_last; } shared; /* * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma * list, after a COW of one of the file pages. A MAP_SHARED vma * can only be in the i_mmap tree. An anonymous MAP_PRIVATE, stack * or brk vma (with NULL file) can only be in an anon_vma list. */ struct list_head anon_vma_chain; /* Serialized by mmap_sem \u0026 * page_table_lock */ struct anon_vma *anon_vma; /* Serialized by page_table_lock */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; /* Information about our backing store: */ unsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE units */ struct file * vm_file; /* File we map to (can be NULL). */ void * vm_private_data; /* was vm_pte (shared mem) */ 其实内存映射不仅仅是物理内存和虚拟内存之间的映射，还包括将文件中的内容映射到虚拟内存空间。这个时候，访问内存空间就能够访问到文件里面的数据。而仅有物理内存和虚拟内存的映射，是一种特殊情况。 前面咱们讲堆的时候讲过，如果我们要申请小块内存，就用 brk。brk 函数之前已经解析过了，这里就不多说了。如果申请一大块内存，就要用 mmap。对于堆的申请来讲，mmap 是映射内存空间到物理内存。另外，如果一个进程想映射一个文件到自己的虚拟内存空间，也要通过 mmap 系统调用。这个时候 mmap 是映射内存空间到物理内存再到文件。可见 mmap 这个系统调用是核心，我们现在来看 mmap 这个系统调用。 SYSCALL_DEFINE6(mmap, unsigned long, addr, unsigned long, len, unsigned long, prot, unsigned long, flags, unsigned long, fd, unsigned long, off) { ...... error = sys_mmap_pgoff(addr, len, prot, flags, fd, off \u003e\u003e PAGE_SHIFT); ...... } SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len, unsigned long, prot, unsigned long, flags, unsigned long, fd, unsigned long, pgoff) { struct file *file = NULL; ...... file = fget(fd); ...... retval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff); return retval; } 如果要映射到文件，fd 会传进来一个文件描述符，并且 mmap_pgoff 里面通过 fget 函数，根据文件描述符获得 struct file。struct file 表示打开的一个文件。接下来的调用链是 vm_mmap_pgoff-\u003edo_mmap_pgoff-\u003edo_mmap。这里面主要干了两件事情： 调用 get_unmapped_area 找到一个没有映射的区域；调用 mmap_region 映射这个区域。 我们先来看 get_unmapped_area 函数。 unsigned long get_unmapped_area(struct file *file, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags) { unsigned long (*get_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long); ...... get_area = current-\u003emm-\u003eget_unmapped_area; if (file) { if (file-\u003ef_op-\u003eget_unmapped_area) get_area = file-\u003ef_op-\u003eget_unmapped_area; } ...... } 这里面如果是匿名映射，则调用 mm_struct 里面的 get_unmapped_area 函数。这个函数其实是 arch_get_unmapped_area。它会调用 find_vma_prev，在表示虚拟内存区域的 vm_area_struct 红黑树上找到相应的位置。之所以叫 prev，是说这个时候虚拟内存区域还没有建立，找到前一个 vm_area_struct。如果不是匿名映射，而是映射到一个文件，这样在 Linux 里面，每个打开的文件都有一个 struct file 结构，里面有一个 file_operations，用来表示和这个文件相关的操作。如果是我们熟知的 ext4 文件系统，调用的是 thp_get_unmapped_area。如果我们仔细看这个函数，最终还是调用 mm_struct 里面的 get_unmapped_area 函数。殊途同归。 const struct file_operations ext4_file_operations = { ...... .mmap = ext4_file_mmap .get_unmapped_area = thp_get_unmapped_area, }; unsigned long __thp_get_unmapped_area(struct file *filp, unsigned long len, loff_t off, unsigned long flags, unsigned long size) { unsigned long addr; loff_t off_end = off + len; loff_t off_align = round_up(off, size); unsigned long len_pad; len_pad = len + size; ...... addr = current-\u003emm-\u003eget_unmapped_area(filp, 0, len_pad, off \u003e\u003e PAGE_SHIFT, flags); addr += (off - addr) \u0026 (size - 1); return addr; } 我们再来看 mmap_region，看它如何映射这个虚拟内存区域。 unsigned long mmap_region(struct file *file, unsigned long addr, unsigned long len, vm_flags_t vm_flags, unsigned long pgoff, struct list_head *uf) { struct mm_struct *mm = current-\u003emm; struct vm_area_struct *vma, *prev; struct rb_node **rb_link, *rb_parent; /* * Can we just expand an old mapping? */ vma = vma_merge(mm, prev, addr, addr + len, vm_flags, NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX); if (vma) goto out; /* * Determine the object being mapped and call the appropriate * specifi","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:27:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用户态缺页异常 一旦开始访问虚拟内存的某个地址，如果我们发现，并没有对应的物理页，那就触发缺页中断，调用 do_page_fault。 dotraplinkage void notrace do_page_fault(struct pt_regs *regs, unsigned long error_code) { unsigned long address = read_cr2(); /* Get the faulting address */ ...... __do_page_fault(regs, error_code, address); ...... } /* * This routine handles page faults. It determines the address, * and the problem, and then passes it off to one of the appropriate * routines. */ static noinline void __do_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long address) { struct vm_area_struct *vma; struct task_struct *tsk; struct mm_struct *mm; tsk = current; mm = tsk-\u003emm; if (unlikely(fault_in_kernel_space(address))) { if (vmalloc_fault(address) \u003e= 0) return; } ...... vma = find_vma(mm, address); ...... fault = handle_mm_fault(vma, address, flags); ...... 在 __do_page_fault 里面，先要判断缺页中断是否发生在内核。如果发生在内核则调用 vmalloc_fault，这就和咱们前面学过的虚拟内存的布局对应上了。在内核里面，vmalloc 区域需要内核页表映射到物理页。咱们这里把内核的这部分放放，接着看用户空间的部分。接下来在用户空间里面，找到你访问的那个地址所在的区域 vm_area_struct，然后调用 handle_mm_fault 来映射这个区域。 static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address, unsigned int flags) { struct vm_fault vmf = { .vma = vma, .address = address \u0026 PAGE_MASK, .flags = flags, .pgoff = linear_page_index(vma, address), .gfp_mask = __get_fault_gfp_mask(vma), }; struct mm_struct *mm = vma-\u003evm_mm; pgd_t *pgd; p4d_t *p4d; int ret; pgd = pgd_offset(mm, address); p4d = p4d_alloc(mm, pgd, address); ...... vmf.pud = pud_alloc(mm, p4d, address); ...... vmf.pmd = pmd_alloc(mm, vmf.pud, address); ...... return handle_pte_fault(\u0026vmf); } 到这里，终于看到了我们熟悉的 PGD、P4G、PUD、PMD、PTE，这就是前面讲页表的时候，讲述的四级页表的概念，因为暂且不考虑五级页表，我们暂时忽略 P4G。 pgd_t 用于全局页目录项，pud_t 用于上层页目录项，pmd_t 用于中间页目录项，pte_t 用于直接页表项。每个进程都有独立的地址空间，为了这个进程独立完成映射，每个进程都有独立的进程页表，这个页表的最顶级的 pgd 存放在 task_struct 中的 mm_struct 的 pgd 变量里面。在一个进程新创建的时候，会调用 fork，对于内存的部分会调用 copy_mm，里面调用 dup_mm。 /* * Allocate a new mm structure and copy contents from the * mm structure of the passed in task structure. */ static struct mm_struct *dup_mm(struct task_struct *tsk) { struct mm_struct *mm, *oldmm = current-\u003emm; mm = allocate_mm(); memcpy(mm, oldmm, sizeof(*mm)); if (!mm_init(mm, tsk, mm-\u003euser_ns)) goto fail_nomem; err = dup_mmap(mm, oldmm); return mm; } 在这里，除了创建一个新的 mm_struct，并且通过 memcpy 将它和父进程的弄成一模一样之外，我们还需要调用 mm_init 进行初始化。接下来，mm_init 调用 mm_alloc_pgd，分配全局页目录项，赋值给 mm_struct 的 pgd 成员变量。 static inline int mm_alloc_pgd(struct mm_struct *mm) { mm-\u003epgd = pgd_alloc(mm); return 0; } pgd_alloc 里面除了分配 PGD 之外，还做了很重要的一个事情，就是调用 pgd_ctor。 static void pgd_ctor(struct mm_struct *mm, pgd_t *pgd) { /* If the pgd points to a shared pagetable level (either the ptes in non-PAE, or shared PMD in PAE), then just copy the references from swapper_pg_dir. */ if (CONFIG_PGTABLE_LEVELS == 2 || (CONFIG_PGTABLE_LEVELS == 3 \u0026\u0026 SHARED_KERNEL_PMD) || CONFIG_PGTABLE_LEVELS \u003e= 4) { clone_pgd_range(pgd + KERNEL_PGD_BOUNDARY, swapper_pg_dir + KERNEL_PGD_BOUNDARY, KERNEL_PGD_PTRS); } ...... } pgd_ctor 干了什么事情呢？我们注意看里面的注释，它拷贝了对于 swapper_pg_dir 的引用。swapper_pg_dir 是内核页表的最顶级的全局页目录。一个进程的虚拟地址空间包含用户态和内核态两部分。为了从虚拟地址空间映射到物理页面，页表也分为用户地址空间的页表和内核页表，这就和上面遇到的 vmalloc 有关系了。在内核里面，映射靠内核页表，这里内核页表会拷贝一份到进程的页表。至于 swapper_pg_dir 是什么，怎么初始化的，怎么工作的，我们还是先放一放，放到下一节统一讨论。至此，一个进程 fork 完毕之后，有了内核页表，有了自己顶级的 pgd，但是对于用户地址空间来讲，还完全没有映射过。这需要等到这个进程在某个 CPU 上运行，并且对内存访问的那一刻了。当这个进程被调度到某个 CPU 上运行的时候，咱们在调度那一节讲过，要调用 context_switch 进行上下文切换。对于内存方面的切换会调用 switch_mm_irqs_off，这里面会调用 load_new_mm_cr3。 cr3 是 CPU 的一个寄存器，它会指向当前进程的顶级 pgd。如果 CPU 的指令要访问进程的虚拟内存，它就会自动从 cr3 里面得到 pgd 在物理内存的地址，然后根据里面的页表解析虚拟内存的地址为物理内存，从而访问真正的物理内存上的数据。这里需要注意两点。第一点，cr3 里面存放当前进程的顶级 pgd，这个是硬件的要求。cr3 里面需要存放 pgd 在物理内存的地址，不能是虚拟地址。因而 load_new_mm_cr3 里面会使用 __pa，将 mm_struct 里面的成员变量 pgd（mm_struct 里面存的都是虚拟地址）变为物理地址，才能加载到 cr3 里面去。 第二点，用户进程在运行的过程中，访问虚拟内存中的数据，会被 cr3 里面指向的页表转换为物理地址后，才在物理内存中访问数据，这个过程都是在用户态运行的，地址转换的过程无需进入内核态。只有访问虚拟内存的时候，发现没有映射到物理内存，页表也没有创建过，才触发缺页异常。进入内核调用 do_page_fault，一直调用到 __handle_mm_fault，这才有了上面解析到这个函数的时候，我们看到的代码。既然原来没有创建过页表，那只好补上这一课。于是，__handle_mm_fault 调用 pud_alloc 和 pmd_alloc，来创建相应的页目录项，最后调用 handl","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:27:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 用户态的内存映射机制，我们解析的差不多了，我们来总结一下，用户态的内存映射机制包含以下几个部分。 用户态内存映射函数 mmap，包括用它来做匿名映射和文件映射。用户态的页表结构，存储位置在 mm_struct 中。在用户态访问没有映射的内存会引发缺页异常，分配物理页表、补齐页表。如果是匿名映射则分配物理内存；如果是 swap，则将 swap 文件读入；如果是文件映射，则将文件读入。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:27:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"26 | 内核态内存映射：如何找到正确的会议室？ 前面讲用户态内存映射机制的时候，我们已经多次引申出了内核的映射机制，但是咱们都暂时放了放，这一节我们就来详细解析一下，让你彻底搞懂它。首先，你要知道，内核态的内存映射机制，主要包含以下几个部分： 内核态内存映射函数 vmalloc、kmap_atomic 是如何工作的；内核态页表是放在哪里的，如何工作的？swapper_pg_dir 是怎么回事；出现了内核态缺页异常应该怎么办？ ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:28:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"内核页表 和用户态页表不同，在系统初始化的时候，我们就要创建内核页表了。我们从内核页表的根 swapper_pg_dir 开始找线索，在 arch/x86/include/asm/pgtable_64.h 中就能找到它的定义。 extern pud_t level3_kernel_pgt[512]; extern pud_t level3_ident_pgt[512]; extern pmd_t level2_kernel_pgt[512]; extern pmd_t level2_fixmap_pgt[512]; extern pmd_t level2_ident_pgt[512]; extern pte_t level1_fixmap_pgt[512]; extern pgd_t init_top_pgt[]; #define swapper_pg_dir init_top_pgt swapper_pg_dir 指向内核最顶级的目录 pgd，同时出现的还有几个页表目录。我们可以回忆一下，64 位系统的虚拟地址空间的布局，其中 XXX_ident_pgt 对应的是直接映射区，XXX_kernel_pgt 对应的是内核代码区，XXX_fixmap_pgt 对应的是固定映射区。它们是在哪里初始化的呢？在汇编语言的文件里面的 arch\\x86\\kernel\\head_64.S。这段代码比较难看懂，你只要明白它是干什么的就行了。 __INITDATA NEXT_PAGE(init_top_pgt) .quad level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE .org init_top_pgt + PGD_PAGE_OFFSET*8, 0 .quad level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE .org init_top_pgt + PGD_START_KERNEL*8, 0 /* (2^48-(2*1024*1024*1024))/(2^39) = 511 */ .quad level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE NEXT_PAGE(level3_ident_pgt) .quad level2_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE .fill 511, 8, 0 NEXT_PAGE(level2_ident_pgt) /* Since I easily can, map the first 1G. * Don't set NX because code runs from these pages. */ PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD) NEXT_PAGE(level3_kernel_pgt) .fill L3_START_KERNEL,8,0 /* (2^48-(2*1024*1024*1024)-((2^39)*511))/(2^30) = 510 */ .quad level2_kernel_pgt - __START_KERNEL_map + _KERNPG_TABLE .quad level2_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE NEXT_PAGE(level2_kernel_pgt) /* * 512 MB kernel mapping. We spend a full page on this pagetable * anyway. * * The kernel code+data+bss must not be bigger than that. * * (NOTE: at +512MB starts the module area, see MODULES_VADDR. * If you want to increase this then increase MODULES_VADDR * too.) */ PMDS(0, __PAGE_KERNEL_LARGE_EXEC, KERNEL_IMAGE_SIZE/PMD_SIZE) NEXT_PAGE(level2_fixmap_pgt) .fill 506,8,0 .quad level1_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE /* 8MB reserved for vsyscalls + a 2MB hole = 4 + 1 entries */ .fill 5,8,0 NEXT_PAGE(level1_fixmap_pgt) .fill 51 内核页表的顶级目录 init_top_pgt，定义在 __INITDATA 里面。咱们讲过 ELF 的格式，也讲过虚拟内存空间的布局。它们都有代码段，还有一些初始化了的全局变量，放在.init 区域。这些说的就是这个区域。可以看到，页表的根其实是全局变量，这就使得我们初始化的时候，甚至内存管理还没有初始化的时候，很容易就可以定位到。接下来，定义 init_top_pgt 包含哪些项，这个汇编代码比较难懂了。你可以简单地认为，quad 是声明了一项的内容，org 是跳到了某个位置。所以，init_top_pgt 有三项，上来先有一项，指向的是 level3_ident_pgt，也即直接映射区页表的三级目录。为什么要减去 __START_KERNEL_map 呢？因为 level3_ident_pgt 是定义在内核代码里的，写代码的时候，写的都是虚拟地址，谁写代码的时候也不知道将来加载的物理地址是多少呀，对不对？因为 level3_ident_pgt 是在虚拟地址的内核代码段里的，而 __START_KERNEL_map 正是虚拟地址空间的内核代码段的起始地址，这在讲 64 位虚拟地址空间的时候都讲过了，要是想不起来就赶紧去回顾一下。这样，level3_ident_pgt 减去 __START_KERNEL_map 才是物理地址。第一项定义完了以后，接下来我们跳到 PGD_PAGE_OFFSET 的位置，再定义一项。从定义可以看出，这一项就应该是 __PAGE_OFFSET_BASE 对应的。__PAGE_OFFSET_BASE 是虚拟地址空间里面内核的起始地址。第二项也指向 level3_ident_pgt，直接映射区。 PGD_PAGE_OFFSET = pgd_index(__PAGE_OFFSET_BASE) PGD_START_KERNEL = pgd_index(__START_KERNEL_map) L3_START_KERNEL = pud_index(__START_KERNEL_map) 第二项定义完了以后，接下来跳到 PGD_START_KERNEL 的位置，再定义一项。从定义可以看出，这一项应该是 __START_KERNEL_map 对应的项，__START_KERNEL_map 是虚拟地址空间里面内核代码段的起始地址。第三项指向 level3_kernel_pgt，内核代码区。接下来的代码就很类似了，就是初始化个表项，然后指向下一级目录，最终形成下面这张图。 内核页表定义完了，一开始这里面的页表能够覆盖的内存范围比较小。例如，内核代码区 512M，直接映射区 1G。这个时候，其实只要能够映射基本的内核代码和数据结构就可以了。可以看出，里面还空着很多项，可以用于将来映射巨大的内核虚拟地址空间，等用到的时候再进行映射。如果是用户态进程页表，会有 mm_struct 指向进程顶级目录 pgd，对于内核来讲，也定义了一个 mm_struct，指向 swapper_pg_dir。 struct mm_struct init_mm = { .mm_rb = RB_ROOT, .pgd = swapper_pg_dir, .mm_users = ATOMIC_INIT(2), .mm_count = ATOMIC_INIT(1), .mmap_sem = __RWSEM_INITIALIZER(init_mm.mmap_sem), .page_table_lock = __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock), .mmlist = LIST_HEAD_INIT(init_mm.mmlist), .user_ns = \u0026init_user_ns, INIT_MM_CONTEXT(init_mm) }; 定义完了内核页表，接下来是初始化内核页表，在系统启动的时候 start_kernel 会调用 setup_arch。 void __init setup_arch(char **cmdline_p) { /* * copy kernel address range established so far and switch * to the proper swapper page table */ clone_pgd_range(swapper_pg_dir + KERNEL_PGD_BOUNDARY, initial_page_table + KERNEL_PGD_BOUNDARY, KERNEL_PGD_PTRS); load_cr3(swapper_pg_dir","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:28:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"vmalloc 和 kmap_atomic 原理 在用户态可以通过 malloc 函数分配内存，当然 malloc 在分配比较大的内存的时候，底层调用的是 mmap，当然也可以直接通过 mmap 做内存映射，在内核里面也有相应的函数。在虚拟地址空间里面，有个 vmalloc 区域，从 VMALLOC_START 开始到 VMALLOC_END，可以用于映射一段物理内存。 /** * vmalloc - allocate virtually contiguous memory * @size: allocation size * Allocate enough pages to cover @size from the page level * allocator and map them into contiguous kernel virtual space. * * For tight control over page level allocator and protection flags * use __vmalloc() instead. */ void *vmalloc(unsigned long size) { return __vmalloc_node_flags(size, NUMA_NO_NODE, GFP_KERNEL); } static void *__vmalloc_node(unsigned long size, unsigned long align, gfp_t gfp_mask, pgprot_t prot, int node, const void *caller) { return __vmalloc_node_range(size, align, VMALLOC_START, VMALLOC_END, gfp_mask, prot, 0, node, caller); } 我们再来看内核的临时映射函数 kmap_atomic 的实现。从下面的代码我们可以看出，如果是 32 位有高端地址的，就需要调用 set_pte 通过内核页表进行临时映射；如果是 64 位没有高端地址的，就调用 page_address，里面会调用 lowmem_page_address。其实低端内存的映射，会直接使用 __va 进行临时映射。 void *kmap_atomic_prot(struct page *page, pgprot_t prot) { ...... if (!PageHighMem(page)) return page_address(page); ...... vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx); set_pte(kmap_pte-idx, mk_pte(page, prot)); ...... return (void *)vaddr; } void *kmap_atomic(struct page *page) { return kmap_atomic_prot(page, kmap_prot); } static __always_inline void *lowmem_page_address(const struct page *page) { return page_to_virt(page); } #define page_to_virt(x) __va(PFN_PHYS(page_to_pfn(x) ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:28:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"内核态缺页异常 可以看出，kmap_atomic 和 vmalloc 不同。kmap_atomic 发现，没有页表的时候，就直接创建页表进行映射了。而 vmalloc 没有，它只分配了内核的虚拟地址。所以，访问它的时候，会产生缺页异常。内核态的缺页异常还是会调用 do_page_fault，但是会走到咱们上面用户态缺页异常中没有解析的那部分 vmalloc_fault。这个函数并不复杂，主要用于关联内核页表项。 /* * 32-bit: * * Handle a fault on the vmalloc or module mapping area */ static noinline int vmalloc_fault(unsigned long address) { unsigned long pgd_paddr; pmd_t *pmd_k; pte_t *pte_k; /* Make sure we are in vmalloc area: */ if (!(address \u003e= VMALLOC_START \u0026\u0026 address \u003c VMALLOC_END)) return -1; /* * Synchronize this task's top level page-table * with the 'reference' page table. * * Do _not_ use \"current\" here. We might be inside * an interrupt in the middle of a task switch.. */ pgd_paddr = read_cr3_pa(); pmd_k = vmalloc_sync_one(__va(pgd_paddr), address); if (!pmd_k) return -1; pte_k = pte_offset_kernel(pmd_k, address); if (!pte_present(*pte_k)) return -1; return 0 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:28:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 至此，内核态的内存映射也讲完了。这下，我们可以将整个内存管理的体系串起来了。物理内存根据 NUMA 架构分节点。每个节点里面再分区域。每个区域里面再分页。物理页面通过伙伴系统进行分配。分配的物理页面要变成虚拟地址让上层可以访问，kswapd 可以根据物理页面的使用情况对页面进行换入换出。对于内存的分配需求，可能来自内核态，也可能来自用户态。对于内核态，kmalloc 在分配大内存的时候，以及 vmalloc 分配不连续物理页的时候，直接使用伙伴系统，分配后转换为虚拟地址，访问的时候需要通过内核页表进行映射。对于 kmem_cache 以及 kmalloc 分配小内存，则使用 slub 分配器，将伙伴系统分配出来的大块内存切成一小块一小块进行分配。kmem_cache 和 kmalloc 的部分不会被换出，因为用这两个函数分配的内存多用于保持内核关键的数据结构。内核态中 vmalloc 分配的部分会被换出，因而当访问的时候，发现不在，就会调用 do_page_fault。对于用户态的内存分配，或者直接调用 mmap 系统调用分配，或者调用 malloc。调用 malloc 的时候，如果分配小的内存，就用 sys_brk 系统调用；如果分配大的内存，还是用 sys_mmap 系统调用。正常情况下，用户态的内存都是可以换出的，因而一旦发现内存中不存在，就会调用 do_page_fault。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:28:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"27 | 文件系统：项目成果要归档，我们就需要档案库 咱们花了这么长的时间，规划了会议室管理系统，这样多个项目执行的时候，隔离性可以得到保证。但是，会议室里面保存的资料还是暂时的，一旦项目结束，会议室会被回收，会议室里面的资料就丢失了。有一些资料我们希望项目结束也能继续保存，这就需要一个和项目运行生命周期无关的地方，可以永久保存，并且空间也要比会议室大得多。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:29:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"文件系统的功能规划 要知道，这些资料才是咱们公司的财富，是执行多个项目积累下来的，是公司竞争力的保证，需要有一个地方归档。这就需要我们有一个存放资料的档案库，在操作系统中就是文件系统。那我们应该如何组织规划文件系统这个档案库呢？对于运行的进程来说，内存就像一个纸箱子，仅仅是一个暂存数据的地方，而且空间有限。如果我们想要进程结束之后，数据依然能够保存下来，就不能只保存在内存里，而是应该保存在外部存储中。就像图书馆这种地方，不仅空间大，而且能够永久保存。我们最常用的外部存储就是硬盘，数据是以文件的形式保存在硬盘上的。为了管理这些文件，我们在规划文件系统的时候，需要考虑到以下几点。 第一点，文件系统要有严格的组织形式，使得文件能够以块为单位进行存储。这就像图书馆里，我们会设置一排排书架，然后再把书架分成一个个小格子，有的项目存放的资料非常多，一个格子放不下，就需要多个格子来存放。我们把这个区域称为存放原始资料的仓库区。 第二点，文件系统中也要有索引区，用来方便查找一个文件分成的多个块都存放在了什么位置。这就好比，图书馆的书太多了，为了方便查找，我们需要专门设置一排书架，这里面会写清楚整个档案库有哪些资料，资料在哪个架子的哪个格子上。这样找资料的时候就不用跑遍整个档案库，在这个书架上找到后，直奔目标书架就可以了。 第三点，如果文件系统中有的文件是热点文件，近期经常被读取和写入，文件系统应该有缓存层。这就相当于图书馆里面的热门图书区，这里面的书都是畅销书或者是常常被借还的图书。因为借还的次数比较多，那就没必要每次有人还了之后，还放回遥远的货架，我们可以专门开辟一个区域，放置这些借还频次高的图书。这样借还的效率就会提高。 第四点，文件应该用文件夹的形式组织起来，方便管理和查询。这就像在图书馆里面，你可以给这些资料分门别类，比如分成计算机类、文学类、历史类等等。这样你也容易管理，项目组借阅的时候只要在某个类别中去找就可以了。 在文件系统中，每个文件都有一个名字，这样我们访问一个文件，希望通过它的名字就可以找到。文件名就是一个普通的文本。当然文件名会经常冲突，不同用户取相同的名字的情况还是会经常出现的。要想把很多的文件有序地组织起来，我们就需要把它们成为目录或者文件夹。这样，一个文件夹里可以包含文件夹，也可以包含文件，这样就形成了一种树形结构。而我们可以将不同的用户放在不同的用户目录下，就可以一定程度上避免了命名的冲突问题。 如图所示，不同的用户的文件放在不同的目录下，虽然很多文件都叫“文件 1”，只要在不同的目录下，就不会有问题。有了目录结构，定位一个文件的时候，我们还会分绝对路径（Absolute Path）和相对路径（Relative Path）。所谓绝对路径，就是从根目录开始一直到当前的文件，例如“/ 根目录 / 用户 A 目录 / 目录 1/ 文件 2”就是一个绝对路径。而通过 cd 命令可以改变当前路径，例如“cd / 根目录 / 用户 A 目录”，就是将用户 A 目录设置为当前目录，而刚才那个文件的相对路径就变成了“./ 目录 1/ 文件 2”。 第五点，Linux 内核要在自己的内存里面维护一套数据结构，来保存哪些文件被哪些进程打开和使用。这就好比，图书馆里会有个图书管理系统，记录哪些书被借阅了，被谁借阅了，借阅了多久，什么时候归还。 好了，这样下来，这文件系统的几个部分，是不是就很好理解、记忆了？你不用死记硬背，只要按照一个正常的逻辑去理解，自然而然就能记住了。接下来的整个章节，我们都要围绕这五点展开解析。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:29:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"文件系统相关命令行 在 Linux 命令的那一节，我们学了一些简单的文件操作的命令，这里我们再来学几个常用的。首先是格式化，也即将一块盘使用命令组织成一定格式的文件系统的过程。咱们买个硬盘或者 U 盘，经常说要先格式化，才能放文件，说的就是这个。使用 Windows 的时候，咱们常格式化的格式为 NTFS（New Technology File System）。在 Linux 下面，常用的是 ext3 或者 ext4。当一个 Linux 系统插入了一块没有格式化的硬盘的时候，我们可以通过命令 fdisk -l，查看格式化和没有格式化的分区。 # fdisk -l Disk /dev/vda: 21.5 GB, 21474836480 bytes, 41943040 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000a4c75 Device Boot Start End Blocks Id System /dev/vda1 * 2048 41943006 20970479+ 83 Linux Disk /dev/vdc: 107.4 GB, 107374182400 bytes, 209715200 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 例如，从上面的命令的输出结果可以看出，vda 这块盘大小 21.5G，是格式化了的，有一个分区 /dev/vda1。vdc 这块盘大小 107.4G，是没有格式化的。我们可以通过命令 mkfs.ext3 或者 mkfs.ext4 进行格式化。 mkfs.ext4 /dev/vdc 执行完这个命令后，vdc 会建立一个分区，格式化为 ext4 文件系统的格式。至于这个格式是如何组织的，我们下一节仔细讲。当然，你也可以选择不将整块盘格式化为一个分区，而是格式化为多个分区。下面的这个命令行可以启动一个交互式程序。 fdisk /dev/vdc 在这个交互式程序中，你可以输入 p 来打印当前分了几个区。如果没有分过，那这个列表应该是空的。接下来，你可以输入 n 新建一个分区。它会让你选择创建主分区 primary，还是扩展分区 extended。我们一般都会选择主分区 p。接下来，它会让你输入分区号。如果原来没有分过区，应该从 1 开始。或者你直接回车，使用默认值也行。接下来，你可以一路选择默认值，直到让你指定这个分区的大小，通过 +sizeM 或者 +sizeK 的方式，默认值是整块盘都用上。你可以 输入 +5620M 分配一个 5G 的分区。这个时候再输入 p，就能看到新创建的分区了，最后输入 w，将对分区的修改写入硬盘。分区结束之后，可能会出现 vdc1, vdc2 等多个分区，这个时候你可以 mkfs.ext3 /dev/vdc1 将第一个分区格式化为 ext3，通过 mkfs.ext4 /dev/vdc2 将第二个分区格式化为 ext4.格式化后的硬盘，需要挂在到某个目录下面，才能作为普通的文件系统进行访问。 mount /dev/vdc1 /根目录/用户A目录/目录1 例如，上面这个命令就是将这个文件系统挂载到“/ 根目录 / 用户 A 目录 / 目录 1”这个目录下面。一旦挂在过去，“/ 根目录 / 用户 A 目录 / 目录 1”这个目录下面原来的文件 1 和文件 2 就都看不到了，换成了 vdc1 这个硬盘里面的文件系统的根目录。有挂载就有卸载，卸载使用 umount 命令。 umount /根目录/用户A目录/目录1 前面我们讲过，Linux 里面一切都是文件，那从哪里看出是什么文件呢？要从 ls -l 的结果的第一位标识位看出来。 表示普通文件；d 表示文件夹；c 表示字符设备文件，这在设备那一节讲解；b 表示块设备文件，这也在设备那一节讲解；s 表示套接字 socket 文件，这在网络那一节讲解；l 表示符号链接，也即软链接，就是通过名字指向另外一个文件，例如下面的代码，instance 这个文件就是指向了 /var/lib/cloud/instances 这个文件。软链接的机制我们这一章会讲解。 # ls -l lrwxrwxrwx 1 root root 61 Dec 14 19:53 instance -\u003e /var/lib/cloud/instances ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:29:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"文件系统相关系统调用 看完了命令行，我们来看一下，如何使用系统调用操作文件？我们先来看一个完整的例子。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003cunistd.h\u003e #include \u003cfcntl.h\u003e int main(int argc, char *argv[]) { int fd = -1; int ret = 1; int buffer = 1024; int num = 0; if((fd=open(\"./test\", O_RDWR|O_CREAT|O_TRUNC))==-1) { printf(\"Open Error\\n\"); exit(1); } ret = write(fd, \u0026buffer, sizeof(int)); if( ret \u003c 0) { printf(\"write Error\\n\"); exit(1); } printf(\"write %d byte(s)\\n\",ret); lseek(fd, 0L, SEEK_SET); ret= read(fd, \u0026num, sizeof(int)); if(ret==-1) { printf(\"read Error\\n\"); exit(1); } printf(\"read %d byte(s)，the number is %d\\n\", ret, num); close(fd); return 0; } 当使用系统调用 open 打开一个文件时，操作系统会创建一些数据结构来表示这个被打开的文件。下一节，我们就会看到这些。为了能够找到这些数据结构，在进程中，我们会为这个打开的文件分配一个文件描述符 fd（File Descriptor）。文件描述符，就是用来区分一个进程打开的多个文件的。它的作用域就是当前进程，出了当前进程这个文件描述符就没有意义了。open 返回的 fd 必须记录好，我们对这个文件的所有操作都要靠这个 fd，包括最后关闭文件。在 Open 函数中，有一些参数： O_CREAT 表示当文件不存在，创建一个新文件；O_RDWR 表示以读写方式打开；O_TRUNC 表示打开文件后，将文件的长度截断为 0。 接下来，write 要用于写入数据。第一个参数就是文件描述符，第二个参数表示要写入的数据存放位置，第三个参数表示希望写入的字节数，返回值表示成功写入到文件的字节数。lseek 用于重新定位读写的位置，第一个参数是文件描述符，第二个参数是重新定位的位置，第三个参数是 SEEK_SET，表示起始位置为文件头，第二个参数和第三个参数合起来表示将读写位置设置为从文件头开始 0 的位置，也即从头开始读写。read 用于读取数据，第一个参数是文件描述符，第二个参数是读取来的数据存到指向的空间，第三个参数是希望读取的字节数，返回值表示成功读取的字节数。最终，close 将关闭一个文件。对于命令行来讲，通过 ls 可以得到文件的属性，使用代码怎么办呢？我们有下面三个函数，可以返回与打开的文件描述符相关的文件状态信息。这个信息将会写到类型为 struct stat 的 buf 结构中。 int stat(const char *pathname, struct stat *statbuf); int fstat(int fd, struct stat *statbuf); int lstat(const char *pathname, struct stat *statbuf); struct stat { dev_t st_dev; /* ID of device containing file */ ino_t st_ino; /* Inode number */ mode_t st_mode; /* File type and mode */ nlink_t st_nlink; /* Number of hard links */ uid_t st_uid; /* User ID of owner */ gid_t st_gid; /* Group ID of owner */ dev_t st_rdev; /* Device ID (if special file) */ off_t st_size; /* Total size, in bytes */ blksize_t st_blksize; /* Block size for filesystem I/O */ blkcnt_t st_blocks; /* Number of 512B blocks allocated */ struct timespec st_atim; /* Time of last access */ struct timespec st_mtim; /* Time of last modification */ struct timespec st_ctim; /* Time of last status change */ }; 函数 stat 和 lstat 返回的是通过文件名查到的状态信息。这两个方法区别在于，stat 没有处理符号链接（软链接）的能力。如果一个文件是符号链接，stat 会直接返回它所指向的文件的属性，而 lstat 返回的就是这个符号链接的内容，fstat 则是通过文件描述符获取文件对应的属性。接下来我们来看，如何使用系统调用列出一个文件夹下面的文件以及文件的属性。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003cunistd.h\u003e #include \u003cfcntl.h\u003e #include \u003csys/types.h\u003e #include \u003csys/stat.h\u003e #include \u003cdirent.h\u003e int main(int argc, char *argv[]) { struct stat sb; DIR *dirp; struct dirent *direntp; char filename[128]; if ((dirp = opendir(\"/root\")) == NULL) { printf(\"Open Directory Error%s\\n\"); exit(1); } while ((direntp = readdir(dirp)) != NULL){ sprintf(filename, \"/root/%s\", direntp-\u003ed_name); if (lstat(filename, \u0026sb) == -1) { printf(\"lstat Error%s\\n\"); exit(1); } printf(\"name : %s, mode : %d, size : %d, user id : %d\\n\", direntp-\u003ed_name, sb.st_mode, sb.st_size, sb.st_uid); } closedir(dirp); return 0 } opendir 函数打开一个目录名所对应的 DIR 目录流。并返回指向 DIR 目录流的指针。流定位在 DIR 目录流的第一个条目。readdir 函数从 DIR 目录流中读取一个项目，返回的是一个指针，指向 dirent 结构体，且流的自动指向下一个目录条目。如果已经到流的最后一个条目，则返回 NULL。closedir() 关闭参数 dir 所指的目录流。到这里，你应该既会使用系统调用操作文件，也会使用系统调用操作目录了。下一节，我们开始来看内核如何实现的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:29:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们对于文件系统的主要功能有了一个总体的印象，我们通过下面这张图梳理一下。 在文件系统上，需要维护文件的严格的格式，要通过 mkfs.ext4 命令来格式化为严格的格式。每一个硬盘上保存的文件都要有一个索引，来维护这个文件上的数据块都保存在哪里。文件通过文件夹组织起来，可以方便用户使用。为了能够更快读取文件，内存里会分配一块空间作为缓存，让一些数据块放在缓存里面。在内核中，要有一整套的数据结构来表示打开的文件。在用户态，每个打开的文件都有一个文件描述符，可以通过各种文件相关的系统调用，操作这个文件描述符。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:29:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"28 | 硬盘文件系统：如何最合理地组织档案库的文档？ 上一节，我们按照图书馆的模式，规划了档案库，也即文件系统应该有的样子。这一节，我们将这个模式搬到硬盘上来看一看。 我们常见的硬盘是上面这幅图左边的样子，中间圆的部分是磁盘的盘片，右边的图是抽象出来的图。每一层里分多个磁道，每个磁道分多个扇区，每个扇区是 512 个字节。文件系统就是安装在这样的硬盘之上。这一节我们重点目前 Linux 下最主流的文件系统格式——ext 系列的文件系统的格式。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:30:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"inode 与块的存储 就像图书馆的书架都要分成大小相同的格子，硬盘也是一样的。硬盘分成相同大小的单元，我们称为块（Block）。一块的大小是扇区大小的整数倍，默认是 4K。在格式化的时候，这个值是可以设定的。一大块硬盘被分成了一个个小的块，用来存放文件的数据部分。这样一来，如果我们像存放一个文件，就不用给他分配一块连续的空间了。我们可以分散成一个个小块进行存放。这样就灵活得多，也比较容易添加、删除和插入数据。但是这也带来一个新的问题，那就是文件的数据存放得太散，找起来就比较困难。有什么办法解决呢？我们是不是可以像图书馆那样，也设立一个索引区域，用来维护“某个文件分成几块、每一块在哪里”等等这些基本信息? 另外，文件还有元数据部分，例如名字、权限等，这就需要一个结构 inode 来存放。什么是 inode 呢？inode 的“i”是 index 的意思，其实就是“索引”，类似图书馆的索引区域。既然如此，我们每个文件都会对应一个 inode；一个文件夹就是一个文件，也对应一个 inode。至于 inode 里面有哪些信息，其实我们在内核中就有定义。你可以看下面这个数据结构。 struct ext4_inode { __le16 i_mode; /* File mode */ __le16 i_uid; /* Low 16 bits of Owner Uid */ __le32 i_size_lo; /* Size in bytes */ __le32 i_atime; /* Access time */ __le32 i_ctime; /* Inode Change time */ __le32 i_mtime; /* Modification time */ __le32 i_dtime; /* Deletion Time */ __le16 i_gid; /* Low 16 bits of Group Id */ __le16 i_links_count; /* Links count */ __le32 i_blocks_lo; /* Blocks count */ __le32 i_flags; /* File flags */ ...... __le32 i_block[EXT4_N_BLOCKS];/* Pointers to blocks */ __le32 i_generation; /* File version (for NFS) */ __le32 i_file_acl_lo; /* File ACL */ __le32 i_size_high; ...... }; 从这个数据结构中，我们可以看出，inode 里面有文件的读写权限 i_mode，属于哪个用户 i_uid，哪个组 i_gid，大小是多少 i_size_io，占用多少个块 i_blocks_io。咱们讲 ls 命令行的时候，列出来的权限、用户、大小这些信息，就是从这里面取出来的。另外，这里面还有几个与文件相关的时间。i_atime 是 access time，是最近一次访问文件的时间；i_ctime 是 change time，是最近一次更改 inode 的时间；i_mtime 是 modify time，是最近一次更改文件的时间。这里你需要注意区分几个地方。首先，访问了，不代表修改了，也可能只是打开看看，就会改变 access time。其次，修改 inode，有可能修改的是用户和权限，没有修改数据部分，就会改变 change time。只有数据也修改了，才改变 modify time。我们刚才说的“某个文件分成几块、每一块在哪里”，这些在 inode 里面，应该保存在 i_block 里面。具体如何保存的呢？EXT4_N_BLOCKS 有如下的定义，计算下来一共有 15 项。 #define EXT4_NDIR_BLOCKS 12 #define EXT4_IND_BLOCK EXT4_NDIR_BLOCKS #define EXT4_DIND_BLOCK (EXT4_IND_BLOCK + 1) #define EXT4_TIND_BLOCK (EXT4_DIND_BLOCK + 1) #define EXT4_N_BLOCKS (EXT4_TIND_BLOCK + 1) 在 ext2 和 ext3 中，其中前 12 项直接保存了块的位置，也就是说，我们可以通过 i_block[0-11]，直接得到保存文件内容的块。 但是，如果一个文件比较大，12 块放不下。当我们用到 i_block[12]的时候，就不能直接放数据块的位置了，要不然 i_block 很快就会用完了。这该怎么办呢？我们需要想个办法。我们可以让 i_block[12]指向一个块，这个块里面不放数据块，而是放数据块的位置，这个块我们称为间接块。也就是说，我们在 i_block[12]里面放间接块的位置，通过 i_block[12]找到间接块后，间接块里面放数据块的位置，通过间接块可以找到数据块。如果文件再大一些，i_block[13]会指向一个块，我们可以用二次间接块。二次间接块里面存放了间接块的位置，间接块里面存放了数据块的位置，数据块里面存放的是真正的数据。如果文件再大一些，i_block[14]会指向三次间接块。原理和上面都是一样的，就像一层套一层的俄罗斯套娃，一层一层打开，才能拿到最中心的数据块。如果你稍微有点经验，现在你应该能够意识到，这里面有一个非常显著的问题，对于大文件来讲，我们要多次读取硬盘才能找到相应的块，这样访问速度就会比较慢。为了解决这个问题，ext4 做了一定的改变。它引入了一个新的概念，叫做 Extents。 我们来解释一下 Extents。比方说，一个文件大小为 128M，如果使用 4k 大小的块进行存储，需要 32k 个块。如果按照 ext2 或者 ext3 那样散着放，数量太大了。但是 Extents 可以用于存放连续的块，也就是说，我们可以把 128M 放在一个 Extents 里面。这样的话，对大文件的读写性能提高了，文件碎片也减少了。Exents 如何来存储呢？它其实会保存成一棵树。 树有一个个的节点，有叶子节点，也有分支节点。每个节点都有一个头，ext4_extent_header 可以用来描述某个节点。 struct ext4_extent_header { __le16 eh_magic; /* probably will support different formats */ __le16 eh_entries; /* number of valid entries */ __le16 eh_max; /* capacity of store in entries */ __le16 eh_depth; /* has tree real underlying blocks? */ __le32 eh_generation; /* generation of the tree */ }; 我们仔细来看里面的内容。eh_entries 表示这个节点里面有多少项。这里的项分两种，如果是叶子节点，这一项会直接指向硬盘上的连续块的地址，我们称为数据节点 ext4_extent；如果是分支节点，这一项会指向下一层的分支节点或者叶子节点，我们称为索引节点 ext4_extent_idx。这两种类型的项的大小都是 12 个 byte。 /* * This is the extent on-disk structure. * It's used at the bottom of the tree. */ struct ext4_extent { __le32 ee_block; /* first logical block extent covers */ __le16 ee_len; /* number of blocks covered by extent */ __le16 ee_start_hi; /* high 16 bits of physical block */ __le32 ee_start_lo; /* low 32 bits of physical block */ }; /* * This is index on-disk structure. * It's used at all the levels except the bottom. */ struct ext4_extent_idx { __le32 ei_block; /* index covers logical blocks from 'block' */ __le32 ei_leaf_lo; /* pointer to the physical block of the next * * level. leaf or next index could be there */ __le16 ei_leaf_hi; /* high 16 bits of physical block */ __u16 ei_unused; }; 如果文件不大，inode 里面的 i_block 中，可以放得下一个 ext4_extent_header 和 4 项 ext4_extent。所以这个时候，eh_depth 为 0，也即 inode 里面的就是叶子节点，树高度为 0。如果文件比较大，4 个 extent 放不下，就要分裂成为一棵树，eh_depth\u003e0 的节点就是索引节点，其中根节点深度最","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:30:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"inode 位图和块位图 到这里，我们知道了，硬盘上肯定有一系列的 inode 和一系列的块排列起来。接下来的问题是，如果我要保存一个数据块，或者要保存一个 inode，我应该放在硬盘上的哪个位置呢？难道需要将所有的 inode 列表和块列表扫描一遍，找个空的地方随便放吗？当然，这样效率太低了。所以在文件系统里面，我们专门弄了一个块来保存 inode 的位图。在这 4k 里面，每一位对应一个 inode。如果是 1，表示这个 inode 已经被用了；如果是 0，则表示没被用。同样，我们也弄了一个块保存 block 的位图。上海虹桥火车站的厕位智能引导系统，不知道你有没有见过？这个系统很厉害，我们要想知道哪个位置有没有被占用，不用挨个拉门，从这样一个电子版上就能看到了。 接下来，我们来看位图究竟是如何在 Linux 操作系统里面起作用的。前一节我们讲过，如果创建一个新文件，会调用 open 函数，并且参数会有 O_CREAT。这表示当文件找不到的时候，我们就需要创建一个。open 是一个系统调用，在内核里面会调用 sys_open，定义如下： SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) { if (force_o_largefile()) flags |= O_LARGEFILE; return do_sys_open(AT_FDCWD, filename, flags, mode); } 这里我们还是重点看对于 inode 的操作。其实 open 一个文件很复杂，下一节我们会详细分析整个过程。我们来看接下来的调用链：do_sys_open-\u003e do_filp_open-\u003epath_openat-\u003edo_last-\u003elookup_open。这个调用链的逻辑是，要打开一个文件，先要根据路径找到文件夹。如果发现文件夹下面没有这个文件，同时又设置了 O_CREAT，就说明我们要在这个文件夹下面创建一个文件，那我们就需要一个新的 inode。 static int lookup_open(struct nameidata *nd, struct path *path, struct file *file, const struct open_flags *op, bool got_write, int *opened) { ...... if (!dentry-\u003ed_inode \u0026\u0026 (open_flag \u0026 O_CREAT)) { ...... error = dir_inode-\u003ei_op-\u003ecreate(dir_inode, dentry, mode, open_flag \u0026 O_EXCL); ...... } ...... } 想要创建新的 inode，我们就要调用 dir_inode，也就是文件夹的 inode 的 create 函数。它的具体定义是这样的： const struct inode_operations ext4_dir_inode_operations = { .create = ext4_create, .lookup = ext4_lookup, .link = ext4_link, .unlink = ext4_unlink, .symlink = ext4_symlink, .mkdir = ext4_mkdir, .rmdir = ext4_rmdir, .mknod = ext4_mknod, .tmpfile = ext4_tmpfile, .rename = ext4_rename2, .setattr = ext4_setattr, .getattr = ext4_getattr, .listxattr = ext4_listxattr, .get_acl = ext4_get_acl, .set_acl = ext4_set_acl, .fiemap = ext4_fiemap, }; 这里面定义了，如果文件夹 inode 要做一些操作，每个操作对应应该调用哪些函数。这里 create 操作调用的是 ext4_create。接下来的调用链是这样的：ext4_create-\u003eext4_new_inode_start_handle-\u003e__ext4_new_inode。在 __ext4_new_inode 函数中，我们会创建新的 inode。 struct inode *__ext4_new_inode(handle_t *handle, struct inode *dir, umode_t mode, const struct qstr *qstr, __u32 goal, uid_t *owner, __u32 i_flags, int handle_type, unsigned int line_no, int nblocks) { ...... inode_bitmap_bh = ext4_read_inode_bitmap(sb, group); ...... ino = ext4_find_next_zero_bit((unsigned long *) inode_bitmap_bh-\u003eb_data, EXT4_INODES_PER_GROUP(sb), ino); ...... } 这里面一个重要的逻辑就是，从文件系统里面读取 inode 位图，然后找到下一个为 0 的 inode，就是空闲的 inode。对于 block 位图，在写入文件的时候，也会有这个过程，我就不展开说了。感兴趣的话，你可以自己去找代码看。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:30:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"文件系统的格式 看起来，我们现在应该能够很顺利地通过 inode 位图和 block 位图创建文件了。如果仔细计算一下，其实还是有问题的。数据块的位图是放在一个块里面的，共 4k。每位表示一个数据块，共可以表示 4∗1024∗8=215 个数据块。如果每个数据块也是按默认的 4K，最大可以表示空间为 215∗4∗1024=227 个 byte，也就是 128M。 也就是说按照上面的格式，如果采用“一个块的位图 + 一系列的块”，外加“一个块的 inode 的位图 + 一系列的 inode 的结构”，最多能够表示 128M。是不是太小了？现在很多文件都比这个大。我们先把这个结构称为一个块组。有 N 多的块组，就能够表示 N 大的文件。 对于块组，我们也需要一个数据结构来表示为 ext4_group_desc。这里面对于一个块组里的 inode 位图 bg_inode_bitmap_lo、块位图 bg_block_bitmap_lo、inode 列表 bg_inode_table_lo，都有相应的成员变量。这样一个个块组，就基本构成了我们整个文件系统的结构。因为块组有多个，块组描述符也同样组成一个列表，我们把这些称为块组描述符表。 当然，我们还需要有一个数据结构，对整个文件系统的情况进行描述，这个就是超级块ext4_super_block。这里面有整个文件系统一共有多少 inode，s_inodes_count；一共有多少块，s_blocks_count_lo，每个块组有多少 inode，s_inodes_per_group，每个块组有多少块，s_blocks_per_group 等。这些都是这类的全局信息。对于整个文件系统，别忘了咱们讲系统启动的时候说的。如果是一个启动盘，我们需要预留一块区域作为引导区，所以第一个块组的前面要留 1K，用于启动引导区。最终，整个文件系统格式就是下面这个样子。 这里面我还需要重点说一下，超级块和块组描述符表都是全局信息，而且这些数据很重要。如果这些数据丢失了，整个文件系统都打不开了，这比一个文件的一个块损坏更严重。所以，这两部分我们都需要备份，但是采取不同的策略。默认情况下，超级块和块组描述符表都有副本保存在每一个块组里面。如果开启了 sparse_super 特性，超级块和块组描述符表的副本只会保存在块组索引为 0、3、5、7 的整数幂里。除了块组 0 中存在一个超级块外，在块组 1（30=1）的第一个块中存在一个副本；在块组 3（31=3）、块组 5（51=5）、块组 7（71=7）、块组 9（32=9）、块组 25（52=25）、块组 27（33=27）的第一个 block 处也存在一个副本。对于超级块来讲，由于超级块不是很大，所以就算我们备份多了也没有太多问题。但是，对于块组描述符表来讲，如果每个块组里面都保存一份完整的块组描述符表，一方面很浪费空间；另一个方面，由于一个块组最大 128M，而块组描述符表里面有多少项，这就限制了有多少个块组，128M * 块组的总数目是整个文件系统的大小，就被限制住了。 我们的改进的思路就是引入 Meta Block Groups 特性。首先，块组描述符表不会保存所有块组的描述符了，而是将块组分成多个组，我们称为元块组（Meta Block Group）。每个元块组里面的块组描述符表仅仅包括自己的，一个元块组包含 64 个块组，这样一个元块组中的块组描述符表最多 64 项。我们假设一共有 256 个块组，原来是一个整的块组描述符表，里面有 256 项，要备份就全备份，现在分成 4 个元块组，每个元块组里面的块组描述符表就只有 64 项了，这就小多了，而且四个元块组自己备份自己的。 根据图中，每一个元块组包含 64 个块组，块组描述符表也是 64 项，备份三份，在元块组的第一个，第二个和最后一个块组的开始处。这样化整为零，我们就可以发挥出 ext4 的 48 位块寻址的优势了，在超级块 ext4_super_block 的定义中，我们可以看到块寻址分为高位和低位，均为 32 位，其中有用的是 48 位，2^48 个块是 1EB，足够用了。 struct ext4_super_block { ...... __le32 s_blocks_count_lo; /* Blocks count */ __le32 s_r_blocks_count_lo; /* Reserved blocks count */ __le32 s_free_blocks_count_lo; /* Free blocks count */ ...... __le32 s_blocks_count_hi; /* Blocks count */ __le32 s_r_blocks_count_hi; /* Reserved blocks count */ __le32 s_free_blocks_count_hi; /* Free blocks count */ ...... } ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:30:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"目录的存储格式 通过前面的描述，我们现在知道了一个普通的文件是如何存储的。有一类特殊的文件，我们会经常用到，就是目录，它是如何保存的呢？其实目录本身也是个文件，也有 inode。inode 里面也是指向一些块。和普通文件不同的是，普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。这些信息我们称为 ext4_dir_entry。从代码来看，有两个版本，在成员来讲几乎没有差别，只不过第二个版本 ext4_dir_entry_2 是将一个 16 位的 name_len，变成了一个 8 位的 name_len 和 8 位的 file_type。 struct ext4_dir_entry { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __le16 name_len; /* Name length */ char name[EXT4_NAME_LEN]; /* File name */ }; struct ext4_dir_entry_2 { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __u8 name_len; /* Name length */ __u8 file_type; char name[EXT4_NAME_LEN]; /* File name */ }; 在目录文件的块中，最简单的保存格式是列表，就是一项一项地将 ext4_dir_entry_2 列在哪里。每一项都会保存这个目录的下一级的文件的文件名和对应的 inode，通过这个 inode，就能找到真正的文件。第一项是“.”，表示当前目录，第二项是“…”，表示上一级目录，接下来就是一项一项的文件名和 inode。有时候，如果一个目录下面的文件太多的时候，我们想在这个目录下找一个文件，按照列表一个个去找，太慢了，于是我们就添加了索引的模式。如果在 inode 中设置 EXT4_INDEX_FL 标志，则目录文件的块的组织形式将发生变化，变成了下面定义的这个样子： struct dx_root { struct fake_dirent dot; char dot_name[4]; struct fake_dirent dotdot; char dotdot_name[4]; struct dx_root_info { __le32 reserved_zero; u8 hash_version; u8 info_length; /* 8 */ u8 indirect_levels; u8 unused_flags; } info; struct dx_entry entries[0]; }; 当然，首先出现的还是差不多的，第一项是“.”，表示当前目录；第二项是“…”，表示上一级目录，这两个不变。接下来就开始发生改变了。是一个 dx_root_info 的结构，其中最重要的成员变量是 indirect_levels，表示间接索引的层数。接下来我们来看索引项 dx_entry。这个也很简单，其实就是文件名的哈希值和数据块的一个映射关系。 struct dx_entry { __le32 hash; __le32 block; }; 如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。然后打开这个块，如果里面不再是索引，而是索引树的叶子节点的话，那里面还是 ext4_dir_entry_2 的列表，我们只要一项一项找文件名就行。通过索引树，我们可以将一个目录下面的 N 多的文件分散到很多的块里面，可以很快地进行查找。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:30:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"软链接和硬链接的存储格式 还有一种特殊的文件格式，硬链接（Hard Link）和软链接（Symbolic Link）。在讲操作文件的命令的时候，我们讲过软链接的概念。所谓的链接（Link），我们可以认为是文件的别名，而链接又可分为两种，硬链接与软链接。通过下面的命令可以创建。 ln [参数][源文件或目录][目标文件或目录] ln -s 创建的是软链接，不带 -s 创建的是硬链接。它们有什么区别呢？在文件系统里面是怎么保存的呢？ 如图所示，硬链接与原始文件共用一个 inode 的，但是 inode 是不跨文件系统的，每个文件系统都有自己的 inode 列表，因而硬链接是没有办法跨文件系统的。而软链接不同，软链接相当于重新创建了一个文件。这个文件也有独立的 inode，只不过打开这个文件看里面内容的时候，内容指向另外的一个文件。这就很灵活了。我们可以跨文件系统，甚至目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:30:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们描述了复杂的硬盘上的文件系统，但是对于咱们平时的应用来讲，用的最多的是两个概念，一个是 inode，一个是数据块。这里我画了一张图，来总结一下 inode 和数据块在文件系统上的关联关系。为了表示图中上半部分的那个简单的树形结构，在文件系统上的布局就像图的下半部分一样。无论是文件夹还是文件，都有一个 inode。inode 里面会指向数据块，对于文件夹的数据块，里面是一个表，是下一层的文件名和 inode 的对应关系，文件的数据块里面存放的才是真正的数据。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:30:6","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"29 | 虚拟文件系统：文件多了就需要档案管理系统 上一节，咱们的图书馆书架，也就是硬盘上的文件系统格式都搭建好了，现在我们还需要一个图书管理与借阅系统，也就是文件管理模块，不然我们怎么知道书都借给谁了呢？进程要想往文件系统里面读写数据，需要很多层的组件一起合作。具体是怎么合作的呢？我们一起来看一看。 在应用层，进程在进行文件读写操作时，可通过系统调用如 sys_open、sys_read、sys_write 等。在内核，每个进程都需要为打开的文件，维护一定的数据结构。在内核，整个系统打开的文件，也需要维护一定的数据结构。Linux 可以支持多达数十种不同的文件系统。它们的实现各不相同，因此 Linux 内核向用户空间提供了虚拟文件系统这个统一的接口，来对文件系统进行操作。它提供了常见的文件系统对象模型，例如 inode、directory entry、mount 等，以及操作这些对象的方法，例如 inode operations、directory operations、file operations 等。然后就是对接的是真正的文件系统，例如我们上节讲的 ext4 文件系统。为了读写 ext4 文件系统，要通过块设备 I/O 层，也即 BIO 层。这是文件系统层和块设备驱动的接口。为了加快块设备的读写效率，我们还有一个缓存层。最下层是块设备驱动程序。 接下来我们逐层解析。在这之前，有一点你需要注意。解析系统调用是了解内核架构最有力的一把钥匙，这里我们只要重点关注这几个最重要的系统调用就可以了： mount 系统调用用于挂载文件系统；open 系统调用用于打开或者创建文件，创建要在 flags 中设置 O_CREAT，对于读写要设置 flags 为 O_RDWR；read 系统调用用于读取文件内容；write 系统调用用于写入文件内容。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:31:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"挂载文件系统 想要操作文件系统，第一件事情就是挂载文件系统。内核是不是支持某种类型的文件系统，需要我们进行注册才能知道。例如，咱们上一节解析的 ext4 文件系统，就需要通过 register_filesystem 进行注册，传入的参数是 ext4_fs_type，表示注册的是 ext4 类型的文件系统。这里面最重要的一个成员变量就是 ext4_mount。记住它，这个我们后面还会用。 register_filesystem(\u0026ext4_fs_type); static struct file_system_type ext4_fs_type = { .owner = THIS_MODULE, .name = \"ext4\", .mount = ext4_mount, .kill_sb = kill_block_super, .fs_flags = FS_REQUIRES_DEV, }; 如果一种文件系统的类型曾经在内核注册过，这就说明允许你挂载并且使用这个文件系统。刚才我说了几个需要重点关注的系统调用，那我们就从第一个 mount 系统调用开始解析。mount 系统调用的定义如下： SYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name, char __user *, type, unsigned long, flags, void __user *, data) { ...... ret = do_mount(kernel_dev, dir_name, kernel_type, flags, options); ...... } 接下来的调用链为：do_mount-\u003edo_new_mount-\u003evfs_kern_mount。 struct vfsmount * vfs_kern_mount(struct file_system_type *type, int flags, const char *name, void *data) { ...... mnt = alloc_vfsmnt(name); ...... root = mount_fs(type, flags, name, data); ...... mnt-\u003emnt.mnt_root = root; mnt-\u003emnt.mnt_sb = root-\u003ed_sb; mnt-\u003emnt_mountpoint = mnt-\u003emnt.mnt_root; mnt-\u003emnt_parent = mnt; list_add_tail(\u0026mnt-\u003emnt_instance, \u0026root-\u003ed_sb-\u003es_mounts); return \u0026mnt-\u003emnt; } vfs_kern_mount 先是创建 struct mount 结构，每个挂载的文件系统都对应于这样一个结构。 struct mount { struct hlist_node mnt_hash; struct mount *mnt_parent; struct dentry *mnt_mountpoint; struct vfsmount mnt; union { struct rcu_head mnt_rcu; struct llist_node mnt_llist; }; struct list_head mnt_mounts; /* list of children, anchored here */ struct list_head mnt_child; /* and going through their mnt_child */ struct list_head mnt_instance; /* mount instance on sb-\u003es_mounts */ const char *mnt_devname; /* Name of device e.g. /dev/dsk/hda1 */ struct list_head mnt_list; ...... } __randomize_layout; struct vfsmount { struct dentry *mnt_root; /* root of the mounted tree */ struct super_block *mnt_sb; /* pointer to superblock */ int mnt_flags; } __randomize_layout; 其中，mnt_parent 是装载点所在的父文件系统，mnt_mountpoint 是装载点在父文件系统中的 dentry；struct dentry 表示目录，并和目录的 inode 关联；mnt_root 是当前文件系统根目录的 dentry，mnt_sb 是指向超级块的指针。接下来，我们来看调用 mount_fs 挂载文件系统。 struct dentry * mount_fs(struct file_system_type *type, int flags, const char *name, void *data) { struct dentry *root; struct super_block *sb; ...... root = type-\u003emount(type, flags, name, data); ...... sb = root-\u003ed_sb; ...... } 这里调用的是 ext4_fs_type 的 mount 函数，也就是咱们上面提到的 ext4_mount，从文件系统里面读取超级块。在文件系统的实现中，每个在硬盘上的结构，在内存中也对应相同格式的结构。当所有的数据结构都读到内存里面，内核就可以通过操作这些数据结构，来操作文件系统了。可以看出来，理解各个数据结构在这里的关系，非常重要。我这里举一个例子，来解析经过 mount 之后，刚刚那些数据结构之间的关系。我们假设根文件系统下面有一个目录 home，有另外一个文件系统 A 挂载在这个目录 home 下面。在文件系统 A 的根目录下面有另外一个文件夹 hello。由于文件系统 A 已经挂载到了目录 home 下面，所以我们就有了目录 /home/hello，然后有另外一个文件系统 B 挂载在 /home/hello 下面。在文件系统 B 的根目录下面有另外一个文件夹 world，在 world 下面有个文件夹 data。由于文件系统 B 已经挂载到了 /home/hello 下面，所以我们就有了目录 /home/hello/world/data。为了维护这些关系，操作系统创建了这一系列数据结构。具体你可以看下面的图。 文件系统是树形关系。如果所有的文件夹都是几代单传，那就变成了一条线。你注意看图中的三条斜线。第一条线是最左边的向左斜的 dentry 斜线。每一个文件和文件夹都有 dentry，用于和 inode 关联。第二条线是最右面的向右斜的 mount 斜线，因为这个例子涉及两次文件系统的挂载，再加上启动的时候挂载的根文件系统，一共三个 mount。第三条线是中间的向右斜的 file 斜线，每个打开的文件都有一个 file 结构，它里面有两个变量，一个指向相应的 mount，一个指向相应的 dentry。 我们从最上面往下看。根目录 / 对应一个 dentry，根目录是在根文件系统上的，根文件系统是系统启动的时候挂载的，因而有一个 mount 结构。这个 mount 结构的 mount point 指针和 mount root 指针都是指向根目录的 dentry。根目录对应的 file 的两个指针，一个指向根目录的 dentry，一个指向根目录的挂载结构 mount。我们再来看第二层。下一层目录 home 对应了两个 dentry，而且它们的 parent 都指向第一层的 dentry。这是为什么呢？这是因为文件系统 A 挂载到了这个目录下。这使得这个目录有两个用处。一方面，home 是根文件系统的一个挂载点；另一方面，home 是文件系统 A 的根目录。因为还有一次挂载，因而又有了一个 mount 结构。这个 mount 结构的 mount point 指针指向作为挂载点的那个 dentry。mount root 指针指向作为根目录的那个 dentry，同时 parent 指针指向第一层的 mount 结构。home 对应的 file 的两个指针，一个指向文件系统 A 根目录的 dentry，一个指向文件系统 A 的挂载结构 mount。我们再来看第三层。目录 hello 又挂载了一个文件系统 B，所以第三层的结构和第二层几乎一样。接下来是第四层。目录 world 就是一个普通的目录。只要它的 dentry 的 parent 指针指向上一层就可以了。我们来看 world 对应的 file 结构。由于挂载点不变，还是指向第三层的 mount 结构。接下来是第五层。对于文件 data，是一个普通的文件，它的 dentry 的 parent 指向第四层的 dentry。对于 data 对应的 file 结构，由于挂载点不变，还是指向第三层的 mount 结构。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:31:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"打开文件 接下来，我们从分析 Open 系统调用说起。在系统调用的那一节，我们知道，在进程里面通过 open 系统调用打开文件，最终对调用到内核的系统调用实现 sys_open。当时我们仅仅解析了系统调用的原理，没有接着分析下去，现在我们接着分析这个过程。 SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) { ...... return do_sys_open(AT_FDCWD, filename, flags, mode); } long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode) { ...... fd = get_unused_fd_flags(flags); if (fd \u003e= 0) { struct file *f = do_filp_open(dfd, tmp, \u0026op); if (IS_ERR(f)) { put_unused_fd(fd); fd = PTR_ERR(f); } else { fsnotify_open(f); fd_install(fd, f); } } putname(tmp); return fd; } 要打开一个文件，首先要通过 get_unused_fd_flags 得到一个没有用的文件描述符。如何获取这个文件描述符呢？在每一个进程的 task_struct 中，有一个指针 files，类型是 files_struct。 struct files_struct *files; files_struct 里面最重要的是一个文件描述符列表，每打开一个文件，就会在这个列表中分配一项，下标就是文件描述符。 struct files_struct { ...... struct file __rcu * fd_array[NR_OPEN_DEFAULT]; }; 对于任何一个进程，默认情况下，文件描述符 0 表示 stdin 标准输入，文件描述符 1 表示 stdout 标准输出，文件描述符 2 表示 stderr 标准错误输出。另外，再打开的文件，都会从这个列表中找一个空闲位置分配给它。文件描述符列表的每一项都是一个指向 struct file 的指针，也就是说，每打开一个文件，都会有一个 struct file 对应。do_sys_open 中调用 do_filp_open，就是创建这个 struct file 结构，然后 fd_install(fd, f) 是将文件描述符和这个结构关联起来。 struct file *do_filp_open(int dfd, struct filename *pathname, const struct open_flags *op) { ...... set_nameidata(\u0026nd, dfd, pathname); filp = path_openat(\u0026nd, op, flags | LOOKUP_RCU); ...... restore_nameidata(); return filp; } do_filp_open 里面首先初始化了 struct nameidata 这个结构。我们知道，文件都是一串的路径名称，需要逐个解析。这个结构在解析和查找路径的时候提供辅助作用。在 struct nameidata 里面有一个关键的成员变量 struct path。 struct path { struct vfsmount *mnt; struct dentry *dentry; } __randomize_layout; 其中，struct vfsmount 和文件系统的挂载有关。另一个 struct dentry，除了上面说的用于标识目录之外，还可以表示文件名，还会建立文件名及其 inode 之间的关联。接下来就调用 path_openat，主要做了以下几件事情： get_empty_filp 生成一个 struct file 结构；path_init 初始化 nameidata，准备开始节点路径查找；link_path_walk 对于路径名逐层进行节点路径查找，这里面有一个大的循环，用“/”分隔逐层处理；do_last 获取文件对应的 inode 对象，并且初始化 file 对象。 static struct file *path_openat(struct nameidata *nd, const struct open_flags *op, unsigned flags) { ...... file = get_empty_filp(); ...... s = path_init(nd, flags); ...... while (!(error = link_path_walk(s, nd)) \u0026\u0026 (error = do_last(nd, file, op, \u0026opened)) \u003e 0) { ...... } terminate_walk(nd); ...... return file; } 例如，文件“/root/hello/world/data”，link_path_walk 会解析前面的路径部分“/root/hello/world”，解析完毕的时候 nameidata 的 dentry 为路径名的最后一部分的父目录“/root/hello/world”，而 nameidata 的 filename 为路径名的最后一部分“data”。最后一部分的解析和处理，我们交给 do_last。 static int do_last(struct nameidata *nd, struct file *file, const struct open_flags *op, int *opened) { ...... error = lookup_fast(nd, \u0026path, \u0026inode, \u0026seq); ...... error = lookup_open(nd, \u0026path, file, op, got_write, opened); ...... error = vfs_open(\u0026nd-\u003epath, file, current_cred()); ...... } 在这里面，我们需要先查找文件路径最后一部分对应的 dentry。如何查找呢？Linux 为了提高目录项对象的处理效率，设计与实现了目录项高速缓存 dentry cache，简称 dcache。它主要由两个数据结构组成： 哈希表 dentry_hashtable：dcache 中的所有 dentry 对象都通过 d_hash 指针链到相应的 dentry 哈希链表中；未使用的 dentry 对象链表 s_dentry_lru：dentry 对象通过其 d_lru 指针链入 LRU 链表中。LRU 的意思是最近最少使用，我们已经好几次看到它了。只要有它，就说明长时间不使用，就应该释放了。 这两个列表之间会产生复杂的关系： 引用为 0：一个在散列表中的 dentry 变成没有人引用了，就会被加到 LRU 表中去；再次被引用：一个在 LRU 表中的 dentry 再次被引用了，则从 LRU 表中移除；分配：当 dentry 在散列表中没有找到，则从 Slub 分配器中分配一个；过期归还：当 LRU 表中最长时间没有使用的 dentry 应该释放回 Slub 分配器；文件删除：文件被删除了，相应的 dentry 应该释放回 Slub 分配器；结构复用：当需要分配一个 dentry，但是无法分配新的，就从 LRU 表中取出一个来复用。 所以，do_last() 在查找 dentry 的时候，当然先从缓存中查找，调用的是 lookup_fast。如果缓存中没有找到，就需要真的到文件系统里面去找了，lookup_open 会创建一个新的 dentry，并且调用上一级目录的 Inode 的 inode_operations 的 lookup 函数，对于 ext4 来讲，调用的是 ext4_lookup，会到咱们上一节讲的文件系统里面去找 inode。最终找到后将新生成的 dentry 赋给 path 变量。 static int lookup_open(struct nameidata *nd, struct path *path, struct file *file, const struct open_flags *op, bool got_write, int *opened) { ...... dentry = d_alloc_parallel(dir, \u0026nd-\u003elast, \u0026wq); ...... struct dentry *res = dir_inode-\u003ei_op-\u003elookup(dir_inode, dentry, nd-\u003eflags); ...... path-\u003edentry = dentry; path-\u003emnt = nd-\u003epath.mnt; } const struct inode_operations ext4_dir_inode_operations = { .create = ext4_create, .lookup = ext4_lookup, ... do_last() 的最后一步是调用 vfs_open 真正打开文件。 int vfs_open(const struct","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:31:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 对于虚拟文件系统的解析就到这里了，我们可以看出，有关文件的数据结构层次多，而且很复杂，就得到了下面这张图，这张图在这个专栏最开始的时候，已经展示过一遍，到这里，你应该能明白它们之间的关系了。 这张图十分重要，一定要掌握。因为我们后面的字符设备、块设备、管道、进程间通信、网络等等，全部都要用到这里面的知识。希望当你再次遇到它的时候，能够马上说出各个数据结构之间的关系。这里我带你简单做一个梳理，帮助你理解记忆它。 对于每一个进程，打开的文件都有一个文件描述符，在 files_struct 里面会有文件描述符数组。每个一个文件描述符是这个数组的下标，里面的内容指向一个 file 结构，表示打开的文件。这个结构里面有这个文件对应的 inode，最重要的是这个文件对应的操作 file_operation。如果操作这个文件，就看这个 file_operation 里面的定义了。对于每一个打开的文件，都有一个 dentry 对应，虽然叫作 directory entry，但是不仅仅表示文件夹，也表示文件。它最重要的作用就是指向这个文件对应的 inode。如果说 file 结构是一个文件打开以后才创建的，dentry 是放在一个 dentry cache 里面的，文件关闭了，他依然存在，因而他可以更长期地维护内存中的文件的表示和硬盘上文件的表示之间的关系。inode 结构就表示硬盘上的 inode，包括块设备号等。几乎每一种结构都有自己对应的 operation 结构，里面都是一些方法，因而当后面遇到对于某种结构进行处理的时候，如果不容易找到相应的处理函数，就先找这个 operation 结构，就清楚了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:31:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"30 | 文件缓存：常用文档应该放在触手可得的地方 上一节，我们讲了文件系统的挂载和文件的打开，并通过打开文件的过程，构建了一个文件管理的整套数据结构体系。其实到这里，我们还没有对文件进行读写，还属于对于元数据的操作。那这一节，我们就重点关注读写。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:32:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"系统调用层和虚拟文件系统层 文件系统的读写，其实就是调用系统函数 read 和 write。由于读和写的很多逻辑是相似的，这里我们一起来看一下这个过程。下面的代码就是 read 和 write 的系统调用，在内核里面的定义。 SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count) { struct fd f = fdget_pos(fd); ...... loff_t pos = file_pos_read(f.file); ret = vfs_read(f.file, buf, count, \u0026pos); ...... } SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf, size_t, count) { struct fd f = fdget_pos(fd); ...... loff_t pos = file_pos_read(f.file); ret = vfs_write(f.file, buf, count, \u0026pos); ...... } 对于 read 来讲，里面调用 vfs_read-\u003e__vfs_read。对于 write 来讲，里面调用 vfs_write-\u003e__vfs_write。下面是 __vfs_read 和 __vfs_write 的代码。 ssize_t __vfs_read(struct file *file, char __user *buf, size_t count, loff_t *pos) { if (file-\u003ef_op-\u003eread) return file-\u003ef_op-\u003eread(file, buf, count, pos); else if (file-\u003ef_op-\u003eread_iter) return new_sync_read(file, buf, count, pos); else return -EINVAL; } ssize_t __vfs_write(struct file *file, const char __user *p, size_t count, loff_t *pos) { if (file-\u003ef_op-\u003ewrite) return file-\u003ef_op-\u003ewrite(file, p, count, pos); else if (file-\u003ef_op-\u003ewrite_iter) return new_sync_write(file, p, count, pos); else return -EINVAL; } 上一节，我们讲了，每一个打开的文件，都有一个 struct file 结构。这里面有一个 struct file_operations f_op，用于定义对这个文件做的操作。__vfs_read 会调用相应文件系统的 file_operations 里面的 read 操作，__vfs_write 会调用相应文件系统 file_operations 里的 write 操作。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:32:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"ext4 文件系统层 对于 ext4 文件系统来讲，内核定义了一个 ext4_file_operations。 const struct file_operations ext4_file_operations = { ...... .read_iter = ext4_file_read_iter, .write_iter = ext4_file_write_iter, ...... } 由于 ext4 没有定义 read 和 write 函数，于是会调用 ext4_file_read_iter 和 ext4_file_write_iter。ext4_file_read_iter 会调用 generic_file_read_iter，ext4_file_write_iter 会调用 __generic_file_write_iter。 ssize_t generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter) { ...... if (iocb-\u003eki_flags \u0026 IOCB_DIRECT) { ...... struct address_space *mapping = file-\u003ef_mapping; ...... retval = mapping-\u003ea_ops-\u003edirect_IO(iocb, iter); } ...... retval = generic_file_buffered_read(iocb, iter, retval); } ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from) { ...... if (iocb-\u003eki_flags \u0026 IOCB_DIRECT) { ...... written = generic_file_direct_write(iocb, from); ...... } else { ...... written = generic_perform_write(file, from, iocb-\u003eki_pos); ...... } } generic_file_read_iter 和 __generic_file_write_iter 有相似的逻辑，就是要区分是否用缓存。缓存其实就是内存中的一块空间。因为内存比硬盘快得多，Linux 为了改进性能，有时候会选择不直接操作硬盘，而是读写都在内存中，然后批量读取或者写入硬盘。一旦能够命中内存，读写效率就会大幅度提高。因此，根据是否使用内存做缓存，我们可以把文件的 I/O 操作分为两种类型。 第一种类型是缓存 I/O。大多数文件系统的默认 I/O 操作都是缓存 I/O。对于读操作来讲，操作系统会先检查，内核的缓冲区有没有需要的数据。如果已经缓存了，那就直接从缓存中返回；否则从磁盘中读取，然后缓存在操作系统的缓存中。对于写操作来讲，操作系统会先将数据从用户空间复制到内核空间的缓存中。这时对用户程序来说，写操作就已经完成。至于什么时候再写到磁盘中由操作系统决定，除非显式地调用了 sync 同步命令。第二种类型是直接 IO，就是应用程序直接访问磁盘数据，而不经过内核缓冲区，从而减少了在内核缓存和用户程序之间数据复制。 如果在读的逻辑 generic_file_read_iter 里面，发现设置了 IOCB_DIRECT，则会调用 address_space 的 direct_IO 的函数，将数据直接读取硬盘。我们在 mmap 映射文件到内存的时候讲过 address_space，它主要用于在内存映射的时候将文件和内存页产生关联。同样，对于缓存来讲，也需要文件和内存页进行关联，这就要用到 address_space。address_space 的相关操作定义在 struct address_space_operations 结构中。对于 ext4 文件系统来讲， address_space 的操作定义在 ext4_aops，direct_IO 对应的函数是 ext4_direct_IO。 static const struct address_space_operations ext4_aops = { ...... .direct_IO = ext4_direct_IO, ...... }; 如果在写的逻辑 __generic_file_write_iter 里面，发现设置了 IOCB_DIRECT，则调用 generic_file_direct_write，里面同样会调用 address_space 的 direct_IO 的函数，将数据直接写入硬盘。ext4_direct_IO 最终会调用到 __blockdev_direct_IO-\u003edo_blockdev_direct_IO，这就跨过了缓存层，到了通用块层，最终到了文件系统的设备驱动层。由于文件系统是块设备，所以这个调用的是 blockdev 相关的函数，有关块设备驱动程序的原理我们下一章详细讲，这一节我们就讲到文件系统到块设备的分界线部分。 /* * This is a library function for use by filesystem drivers. */ static inline ssize_t do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode, struct block_device *bdev, struct iov_iter *iter, get_block_t get_block, dio_iodone_t end_io, dio_submit_t submit_io, int flags) {......} 接下来，我们重点看带缓存的部分如果进行读写。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:32:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"带缓存的写入操作 我们先来看带缓存写入的函数 generic_perform_write。 ssize_t generic_perform_write(struct file *file, struct iov_iter *i, loff_t pos) { struct address_space *mapping = file-\u003ef_mapping; const struct address_space_operations *a_ops = mapping-\u003ea_ops; do { struct page *page; unsigned long offset; /* Offset into pagecache page */ unsigned long bytes; /* Bytes to write to page */ status = a_ops-\u003ewrite_begin(file, mapping, pos, bytes, flags, \u0026page, \u0026fsdata); copied = iov_iter_copy_from_user_atomic(page, i, offset, bytes); flush_dcache_page(page); status = a_ops-\u003ewrite_end(file, mapping, pos, bytes, copied, page, fsdata); pos += copied; written += copied; balance_dirty_pages_ratelimited(mapping); } while (iov_iter_count(i)); } 这个函数里，是一个 while 循环。我们需要找出这次写入影响的所有的页，然后依次写入。对于每一个循环，主要做四件事情： 对于每一页，先调用 address_space 的 write_begin 做一些准备；调用 iov_iter_copy_from_user_atomic，将写入的内容从用户态拷贝到内核态的页中；调用 address_space 的 write_end 完成写操作；调用 balance_dirty_pages_ratelimited，看脏页是否太多，需要写回硬盘。所谓脏页，就是写入到缓存，但是还没有写入到硬盘的页面。 我们依次来看这四个步骤。 static const struct address_space_operations ext4_aops = { ...... .write_begin = ext4_write_begin, .write_end = ext4_write_end, ...... } 第一步，对于 ext4 来讲，调用的是 ext4_write_begin。 ext4 是一种日志文件系统，是为了防止突然断电的时候的数据丢失，引入了日志**（Journal）**模式。日志文件系统比非日志文件系统多了一个 Journal 区域。文件在 ext4 中分两部分存储，一部分是文件的元数据，另一部分是数据。元数据和数据的操作日志 Journal 也是分开管理的。你可以在挂载 ext4 的时候，选择 Journal 模式。这种模式在将数据写入文件系统前，必须等待元数据和数据的日志已经落盘才能发挥作用。这样性能比较差，但是最安全。 另一种模式是 order 模式。这个模式不记录数据的日志，只记录元数据的日志，但是在写元数据的日志前，必须先确保数据已经落盘。这个折中，是默认模式。还有一种模式是 writeback，不记录数据的日志，仅记录元数据的日志，并且不保证数据比元数据先落盘。这个性能最好，但是最不安全。 在 ext4_write_begin，我们能看到对于 ext4_journal_start 的调用，就是在做日志相关的工作。在 ext4_write_begin 中，还做了另外一件重要的事情，就是调用 grab_cache_page_write_begin，来得到应该写入的缓存页。 struct page *grab_cache_page_write_begin(struct address_space *mapping, pgoff_t index, unsigned flags) { struct page *page; int fgp_flags = FGP_LOCK|FGP_WRITE|FGP_CREAT; page = pagecache_get_page(mapping, index, fgp_flags, mapping_gfp_mask(mapping)); if (page) wait_for_stable_page(page); return page; } 在内核中，缓存以页为单位放在内存里面，那我们如何知道，一个文件的哪些数据已经被放到缓存中了呢？每一个打开的文件都有一个 struct file 结构，每个 struct file 结构都有一个 struct address_space 用于关联文件和内存，就是在这个结构里面，有一棵树，用于保存所有与这个文件相关的的缓存页。我们查找的时候，往往需要根据文件中的偏移量找出相应的页面，而基数树 radix tree 这种数据结构能够快速根据一个长整型查找到其相应的对象，因而这里缓存页就放在 radix 基数树里面。 struct address_space { struct inode *host; /* owner: inode, block_device */ struct radix_tree_root page_tree; /* radix tree of all pages */ spinlock_t tree_lock; /* and lock protecting it */ ...... } pagecache_get_page 就是根据 pgoff_t index 这个长整型，在这棵树里面查找缓存页，如果找不到就会创建一个缓存页。第二步，调用 iov_iter_copy_from_user_atomic。先将分配好的页面调用 kmap_atomic 映射到内核里面的一个虚拟地址，然后将用户态的数据拷贝到内核态的页面的虚拟地址中，调用 kunmap_atomic 把内核里面的映射删除。 size_t iov_iter_copy_from_user_atomic(struct page *page, struct iov_iter *i, unsigned long offset, size_t bytes) { char *kaddr = kmap_atomic(page), *p = kaddr + offset; iterate_all_kinds(i, bytes, v, copyin((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len), memcpy_from_page((p += v.bv_len) - v.bv_len, v.bv_page, v.bv_offset, v.bv_len), memcpy((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len) ) kunmap_atomic(kaddr); return bytes; } 第三步，调用 ext4_write_end 完成写入。这里面会调用 ext4_journal_stop 完成日志的写入，会调用 block_write_end-\u003e__block_commit_write-\u003emark_buffer_dirty，将修改过的缓存标记为脏页。可以看出，其实所谓的完成写入，并没有真正写入硬盘，仅仅是写入缓存后，标记为脏页。但是这里有一个问题，数据很危险，一旦宕机就没有了，所以需要一种机制，将写入的页面真正写到硬盘中，我们称为回写（Write Back）。第四步，调用 balance_dirty_pages_ratelimited，是回写脏页的一个很好的时机。 /** * balance_dirty_pages_ratelimited - balance dirty memory state * @mapping: address_space which was dirtied * * Processes which are dirtying memory should call in here once for each page * which was newly dirtied. The function will periodically check the system's * dirty state and will initiate writeback if needed. */ void balance_dirty_pages_ratelimited(struct address_space *mapping) { struct inode *inode = mapping-\u003ehost; struct backing_dev_info *bdi = inode_to_bdi(inode); struct bdi_writeback *wb = NULL; int ratelimit; ...... if (unlikely(current-\u003enr_dirtied \u003e= ratelimit)) balance_dirty_page","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:32:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"带缓存的读操作 带缓存的写分析完了，接下来，我们看带缓存的读，对应的是函数 generic_file_buffered_read。 static ssize_t generic_file_buffered_read(struct kiocb *iocb, struct iov_iter *iter, ssize_t written) { struct file *filp = iocb-\u003eki_filp; struct address_space *mapping = filp-\u003ef_mapping; struct inode *inode = mapping-\u003ehost; for (;;) { struct page *page; pgoff_t end_index; loff_t isize; page = find_get_page(mapping, index); if (!page) { if (iocb-\u003eki_flags \u0026 IOCB_NOWAIT) goto would_block; page_cache_sync_readahead(mapping, ra, filp, index, last_index - index); page = find_get_page(mapping, index); if (unlikely(page == NULL)) goto no_cached_page; } if (PageReadahead(page)) { page_cache_async_readahead(mapping, ra, filp, page, index, last_index - index); } /* * Ok, we have the page, and it's up-to-date, so * now we can copy it to user space... */ ret = copy_page_to_iter(page, offset, nr, iter); } } 读取比写入总体而言简单一些，主要涉及预读的问题。在 generic_file_buffered_read 函数中，我们需要先找到 page cache 里面是否有缓存页。如果没有找到，不但读取这一页，还要进行预读，这需要在 page_cache_sync_readahead 函数中实现。预读完了以后，再试一把查找缓存页，应该能找到了。如果第一次找缓存页就找到了，我们还是要判断，是不是应该继续预读；如果需要，就调用 page_cache_async_readahead 发起一个异步预读。最后，copy_page_to_iter 会将内容从内核缓存页拷贝到用户内存空间。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:32:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节对于读取和写入的分析就到这里了。我们发现这个过程还是很复杂的，我这里画了一张调用图，你可以看到调用过程。在系统调用层我们需要仔细学习 read 和 write。在 VFS 层调用的是 vfs_read 和 vfs_write 并且调用 file_operation。在 ext4 层调用的是 ext4_file_read_iter 和 ext4_file_write_iter。接下来就是分叉。你需要知道缓存 I/O 和直接 I/O。直接 I/O 读写的流程是一样的，调用 ext4_direct_IO，再往下就调用块设备层了。缓存 I/O 读写的流程不一样。对于读，从块设备读取到缓存中，然后从缓存中拷贝到用户态。对于写，从用户态拷贝到缓存，设置缓存页为脏，然后启动一个线程写入块设备。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:32:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"31 | 输入与输出：如何建立售前售后生态体系？ 到这一节，操作系统作为一家外包公司，里面最核心的职能部门差不多都凑齐了。我们有了项目管理部门（进程管理），有为了维护项目执行期间数据的会议室管理部门（内存管理），有项目执行完毕后归档的档案库管理部门（文件系统）。这一节，我们来规划一下这家公司的售前售后生态体系（输入输出系统）。这里你需要注意“生态”两个字，我们不仅仅是招聘一些售前和售后员工，而是应该建立一套体系让供应商，让渠道帮着我们卖，形成一个生态。计算机系统的输入和输出系统都有哪些呢？我们能举出来的，例如键盘、鼠标、显示器、网卡、硬盘、打印机、CD/DVD 等等，多种多样。这样，当然方便用户使用了，但是对于操作系统来讲，却是一件复杂的事情，因为这么多设备，形状、用法、功能都不一样，怎么才能统一管理起来呢？ ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用设备控制器屏蔽设备差异 这有点像一家公司要做 To B 的生意，发现客户多种多样，众口难调，不同的地域不一样，不同的行业不一样。如果你不懂某个地方的规矩，根本卖不出去东西；如果你不懂某个具体行业的使用场景，也无法满足客户的需求。怎么办呢？一般公司采取的策略就是建立生态，设置很多代理商，让各个地区和各个行业的代理商帮你屏蔽这些差异化。你和代理商之间只要进行简单的标准产品交付就可以了。计算机系统也是这样的，CPU 并不直接和设备打交道，它们中间有一个叫作设备控制器（Device Control Unit）的组件，例如硬盘有磁盘控制器、USB 有 USB 控制器、显示器有视频控制器等。这些控制器就像代理商一样，它们知道如何应对硬盘、鼠标、键盘、显示器的行为。 如果你是一家大公司，你的代理商往往是小公司。控制器其实有点儿像一台小电脑。它有它的芯片，类似小 CPU，执行自己的逻辑。它也有它的寄存器。这样 CPU 就可以通过写这些寄存器，对控制器下发指令，通过读这些寄存器，查看控制器对于设备的操作状态。CPU 对于寄存器的读写，可比直接控制硬件，要标准和轻松很多。这就相当于你和代理商的标准产品交付。输入输出设备我们大致可以分为两类：块设备（Block Device）和字符设备（Character Device）。 块设备将信息存储在固定大小的块中，每个块都有自己的地址。硬盘就是常见的块设备。字符设备发送或接收的是字节流。而不用考虑任何块结构，没有办法寻址。鼠标就是常见的字符设备。 由于块设备传输的数据量比较大，控制器里往往会有缓冲区。CPU 写入缓冲区的数据攒够一部分，才会发给设备。CPU 读取的数据，也需要在缓冲区攒够一部分，才拷贝到内存。这个也比较好理解，代理商我们也可以分成两种。一种是集成商模式，也就是说没有客户的时候，代理商不会在你这里采购产品，每次它遇到一个客户的时候，会带上你，共同应标。你出标准产品，地域的和行业的差异，它来搞定。这有点儿像字符设备。另外一种是代购代销模式，也就是说从你这里批量采购一批产品，然后没卖完之前，基本就不会找你了。这有点儿像块设备。CPU 如何同控制器的寄存器和数据缓冲区进行通信呢？ 每个控制寄存器被分配一个 I/O 端口，我们可以通过特殊的汇编指令（例如 in/out 类似的指令）操作这些寄存器。数据缓冲区，可内存映射 I/O，可以分配一段内存空间给它，就像读写内存一样读写数据缓冲区。如果你去看内存空间的话，有一个原来我们没有讲过的区域 ioremap，就是做这个的。 这有点儿像，如果你要给你的代理商下一个任务，或者询问订单的状态，直接打电话联系他们的负责人就可以了。如果你需要和代理商做大量的交互，共同讨论应标方案，那电话说太麻烦了，你可以把代理商拉到你们公司来，你们直接在一个会议室里面出方案。对于 CPU 来讲，这些外部设备都有自己的大脑，可以自行处理一些事情，但是有个问题是，当你给设备发了一个指令，让它读取一些数据，它读完的时候，怎么通知你呢？控制器的寄存器一般会有状态标志位，可以通过检测状态标志位，来确定输入或者输出操作是否完成。第一种方式就是轮询等待，就是一直查，一直查，直到完成。当然这种方式很不好，于是我们有了第二种方式，就是可以通过中断的方式，通知操作系统输入输出操作已经完成。 为了响应中断，我们一般会有一个硬件的中断控制器，当设备完成任务后触发中断到中断控制器，中断控制器就通知 CPU，一个中断产生了，CPU 需要停下当前手里的事情来处理中断。这就像代理商有了新客户，客户有了新需求，客户交付完毕等事件，都需要有一种机制通知你们公司，在哪里呢？当然是在办事大厅呀。如果你问，不对呀，办事大厅不是处理系统调用的么？还记得 32 位系统调用是通过 INT 产生软中断触发的么？这就统一起来了，中断有两种，一种软中断，例如代码调用 INT 指令触发，一种是硬件中断，就是硬件通过中断控制器触发的。所以将中断作为办事大厅的一项服务，没有什么问题。 有的设备需要读取或者写入大量数据。如果所有过程都让 CPU 协调的话，就需要占用 CPU 大量的时间，比方说，磁盘就是这样的。这种类型的设备需要支持 DMA 功能，也就是说，允许设备在 CPU 不参与的情况下，能够自行完成对内存的读写。实现 DMA 机制需要有个 DMA 控制器帮你的 CPU 来做协调，就像下面这个图中显示的一样。CPU 只需要对 DMA 控制器下指令，说它想读取多少数据，放在内存的某个地方就可以了，接下来 DMA 控制器会发指令给磁盘控制器，读取磁盘上的数据到指定的内存位置，传输完毕之后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了。还记得咱们讲内存的时候，有个 DMA 区域，就是这个作用。DMA 有点儿像一些比较大的代理商，不但能够帮你代购代销，而且自己有能力售前、售后和技术支持，实施部署都能自己搞定。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用驱动程序屏蔽设备控制器差异 虽然代理商机制能够帮我们屏蔽很多设备的细节，但是从上面的描述我们可以看出，由于每种设备的控制器的寄存器、缓冲区等使用模式，指令都不同，所以对于操作系统这家公司来讲，需要有个部门专门对接代理商，向其他部门屏蔽代理商的差异，类似公司的渠道管理部门。那什么才是操作系统的渠道管理部门呢？就是用来对接各个设备控制器的设备驱动程序。这里需要注意的是，设备控制器不属于操作系统的一部分，但是设备驱动程序属于操作系统的一部分。操作系统的内核代码可以像调用本地代码一样调用驱动程序的代码，而驱动程序的代码需要发出特殊的面向设备控制器的指令，才能操作设备控制器。设备驱动程序中是一些面向特殊设备控制器的代码。不同的设备不同。但是对于操作系统其它部分的代码而言，设备驱动程序应该有统一的接口。就像下面图中的一样，不同的设备驱动程序，可以以同样的方式接入操作系统，而操作系统的其它部分的代码，也可以无视不同设备的区别，以同样的接口调用设备驱动程序。接下来两节，我们会讲字符设备驱动程序和块设备驱动程序的模型，从那里我们也可以看出，所有设备驱动程序都要，按照同样的规则，实现同样的方法。 上面咱们说了，设备做完了事情要通过中断来通知操作系统。那操作系统就需要有一个地方处理这个中断，既然设备驱动程序是用来对接设备控制器的，中断处理也应该在设备驱动里面完成。然而中断的触发最终会到达 CPU，会中断操作系统当前运行的程序，所以操作系统也要有一个统一的流程来处理中断，使得不同设备的中断使用统一的流程。一般的流程是，一个设备驱动程序初始化的时候，要先注册一个该设备的中断处理函数。咱们讲进程切换的时候说过，中断返回的那一刻是进程切换的时机。不知道你还记不记得，中断的时候，触发的函数是 do_IRQ。这个函数是中断处理的统一入口。在这个函数里面，我们可以找到设备驱动程序注册的中断处理函数 Handler，然后执行它进行中断处理。 另外，对于块设备来讲，在驱动程序之上，文件系统之下，还需要一层通用设备层。比如咱们上一章讲的文件系统，里面的逻辑和磁盘设备没有什么关系，可以说是通用的逻辑。在写文件的最底层，我们看到了 BIO 字眼的函数，但是好像和设备驱动也没有什么关系。是的，因为块设备类型非常多，而 Linux 操作系统里面一切是文件。我们也不想文件系统以下，就直接对接各种各样的块设备驱动程序，这样会使得文件系统的复杂度非常高。所以，我们在中间加了一层通用块层，将与块设备相关的通用逻辑放在这一层，维护与设备无关的块的大小，然后通用块层下面对接各种各样的驱动程序。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"用文件系统接口屏蔽驱动程序的差异 上面我们从硬件设备到设备控制器，到驱动程序，到通用块层，到文件系统，层层屏蔽不同的设备的差别，最终到这里涉及对用户使用接口，也要统一。虽然我们操作设备，都是基于文件系统的接口，也要有一个统一的标准。首先要统一的是设备名称。所有设备都在 /dev/ 文件夹下面创建一个特殊的设备文件。这个设备特殊文件也有 inode，但是它不关联到硬盘或任何其他存储介质上的数据，而是建立了与某个设备驱动程序的连接。 硬盘设备这里有一点绕。假设是 /dev/sdb，这是一个设备文件。这个文件本身和硬盘上的文件系统没有任何关系。这个设备本身也不对应硬盘上的任何一个文件，/dev/sdb 其实是在一个特殊的文件系统 devtmpfs 中。但是当我们将 /dev/sdb 格式化成一个文件系统 ext4 的时候，就会将它 mount 到一个路径下面。例如在 /mnt/sdb 下面。这个时候 /dev/sdb 还是一个设备文件在特殊文件系统 devtmpfs 中，而 /mnt/sdb 下面的文件才是在 ext4 文件系统中，只不过这个设备是在 /dev/sdb 设备上的。这里我们只关心设备文件，当我们用 ls -l 在 /dev 下面执行的时候，就会有这样的结果。 # ls -l crw------- 1 root root 5, 1 Dec 14 19:53 console crw-r----- 1 root kmem 1, 1 Dec 14 19:53 mem crw-rw-rw- 1 root root 1, 3 Dec 14 19:53 null crw-r----- 1 root kmem 1, 4 Dec 14 19:53 port crw-rw-rw- 1 root root 1, 8 Dec 14 19:53 random crw--w---- 1 root tty 4, 0 Dec 14 19:53 tty0 crw--w---- 1 root tty 4, 1 Dec 14 19:53 tty1 crw-rw-rw- 1 root root 1, 9 Dec 14 19:53 urandom brw-rw---- 1 root disk 253, 0 Dec 31 19:18 vda brw-rw---- 1 root disk 253, 1 Dec 31 19:19 vda1 brw-rw---- 1 root disk 253, 16 Dec 14 19:53 vdb brw-rw---- 1 root disk 253, 32 Jan 2 11:24 vdc crw-rw-rw- 1 root root 1, 5 Dec 14 19:53 zero 对于设备文件，ls 出来的内容和我们原来讲过的稍有不同。首先是第一位字符。如果是字符设备文件，则以 c 开头，如果是块设备文件，则以 b 开头。其次是这里面的两个号，一个是主设备号，一个是次设备号。主设备号定位设备驱动程序，次设备号作为参数传给启动程序，选择相应的单元。从上面的列表我们可以看出来，mem、null、random、urandom、zero 都是用同样的主设备号 1，也就是它们使用同样的字符设备驱动，而 vda、vda1、vdb、vdc 也是同样的主设备号，也就是它们使用同样的块设备驱动。有了设备文件，我们就可以使用对于文件的操作命令和 API 来操作文件了。例如，使用 cat 命令，可以读取 /dev/random 和 /dev/urandom 的数据流，可以用 od 命令转换为十六进制后查看。 cat /dev/urandom | od -x 这里还是要明确一下，如果用文件的操作作用于 /dev/sdb 的话，会无法操作文件系统上的文件，操作的这个设备。如果 Linux 操作系统新添加了一个设备，应该做哪些事情呢？就像咱们使用 Windows 的时候，如果新添加了一种设备，首先要看这个设备有没有相应的驱动。如果没有就需要安装一个驱动，等驱动安装好了，设备就在 Windows 的设备列表中显示出来了。在 Linux 上面，如果一个新的设备从来没有加载过驱动，也需要安装驱动。Linux 的驱动程序已经被写成和操作系统有标准接口的代码，可以看成一个标准的内核模块。在 Linux 里面，安装驱动程序，其实就是加载一个内核模块。我们可以用命令 lsmod，查看有没有加载过相应的内核模块。这个列表很长，我这里列举了其中一部分。可以看到，这里面有网络和文件系统的驱动。 # lsmod Module Size Used by iptable_filter 12810 1 bridge 146976 1 br_netfilter vfat 17461 0 fat 65950 1 vfat ext4 571716 1 cirrus 24383 1 crct10dif_pclmul 14307 0 crct10dif_common 12595 1 crct10dif_pclmul 如果没有安装过相应的驱动，可以通过 insmod 安装内核模块。内核模块的后缀一般是 ko。例如，我们要加载 openvswitch 的驱动，就要通过下面的命令： insmod openvswitch.ko 一旦有了驱动，我们就可以通过命令 mknod 在 /dev 文件夹下面创建设备文件，就像下面这样： mknod filename type major minor 其中 filename 就是 /dev 下面的设备名称，type 就是 c 为字符设备，b 为块设备，major 就是主设备号，minor 就是次设备号。一旦执行了这个命令，新创建的设备文件就和上面加载过的驱动关联起来，这个时候就可以通过操作设备文件来操作驱动程序，从而操作设备。你可能会问，人家 Windows 都说插上设备后，一旦安装了驱动，就直接在设备列表中出来了，你这里怎么还要人来执行命令创建呀，能不能智能一点？当然可以，这里就要用到另一个管理设备的文件系统，也就是 /sys 路径下面的 sysfs 文件系统。它把实际连接到系统上的设备和总线组成了一个分层的文件系统。这个文件系统是当前系统上实际的设备数的真实反映。在 /sys 路径下有下列的文件夹： /sys/devices 是内核对系统中所有设备的分层次的表示；/sys/dev 目录下一个 char 文件夹，一个 block 文件夹，分别维护一个按字符设备和块设备的主次号码 (major:minor) 链接到真实的设备 (/sys/devices 下) 的符号链接文件；/sys/block 是系统中当前所有的块设备；/sys/module 有系统中所有模块的信息。 有了 sysfs 以后，我们还需要一个守护进程 udev。当一个设备新插入系统的时候，内核会检测到这个设备，并会创建一个内核对象 kobject 。 这个对象通过 sysfs 文件系统展现到用户层，同时内核还向用户空间发送一个热插拔消息。udevd 会监听这些消息，在 /dev 中创建对应的文件。 有了文件系统接口之后，我们不但可以通过文件系统的命令行操作设备，也可以通过程序，调用 read、write 函数，像读写文件一样操作设备。但是有些任务只使用读写很难完成，例如检查特定于设备的功能和属性，超出了通用文件系统的限制。所以，对于设备来讲，还有一种接口称为 ioctl，表示输入输出控制接口，是用于配置和修改特定设备属性的通用接口，这个我们后面几节会详细说。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们讲了输入与输出设备的管理，内容比较多。输入输出设备就像管理代理商一样。因为代理商复杂多变，代理商管理也同样复杂多变，需要层层屏蔽差异化的部分，给上层提供标准化的部分，最终到用户态，给用户提供了基于文件系统的统一的接口。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"32 | 字符设备（上）：如何建立直销模式？ 上一节，我们讲了输入输出设备的层次模型，还是比较复杂的，块设备尤其复杂。这一节为了让你更清晰地了解设备驱动程序的架构，我们先来讲稍微简单一点的字符设备驱动。这一节，我找了两个比较简单的字符设备驱动来解析一下。一个是输入字符设备，鼠标。代码在 drivers/input/mouse/logibm.c 这里。 /* * Logitech Bus Mouse Driver for Linux */ module_init(logibm_init); module_exit(logibm_exit); 另外一个是输出字符设备，打印机，代码 drivers/char/lp.c 这里。 /* * Generic parallel printer driver */ module_init(lp_init_module); module_exit(lp_cleanup_module); ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"内核模块 上一节，我们讲过，设备驱动程序是一个内核模块，以 ko 的文件形式存在，可以通过 insmod 加载到内核中。那我们首先来看一下，怎么样才能构建一个内核模块呢？一个内核模块应该由以下几部分组成。第一部分，头文件部分。一般的内核模块，都需要 include 下面两个头文件： #include \u003clinux/module.h\u003e #include \u003clinux/init.h\u003e 如果你去看上面两个驱动程序，都能找到这两个头文件。当然如果需要的话，我们还可以引入更多的头文件。 第二部分，定义一些函数，用于处理内核模块的主要逻辑。例如打开、关闭、读取、写入设备的函数或者响应中断的函数。例如，logibm.c 里面就定义了 logibm_open。logibm_close 就是处理打开和关闭的，定义了 logibm_interrupt 就是用来响应中断的。再如，lp.c 里面就定义了 lp_read，lp_write 就是处理读写的。 第三部分，定义一个 file_operations 结构。前面我们讲过，设备是可以通过文件系统的接口进行访问的。咱们讲文件系统的时候说过，对于某种文件系统的操作，都是放在 file_operations 里面的。例如 ext4 就定义了这么一个结构，里面都是 ext4_xxx 之类的函数。设备要想被文件系统的接口操作，也需要定义这样一个结构。例如，lp.c 里面就定义了这样一个结构。 static const struct file_operations lp_fops = { .owner = THIS_MODULE, .write = lp_write, .unlocked_ioctl = lp_ioctl, #ifdef CONFIG_COMPAT .compat_ioctl = lp_compat_ioctl, #endif .open = lp_open, .release = lp_release, #ifdef CONFIG_PARPORT_1284 .read = lp_read, #endif .llseek = noop_llseek, }; 在 logibm.c 里面，我们找不到这样的结构，是因为它属于众多输入设备的一种，而输入设备的操作被统一定义在 drivers/input/input.c 里面，logibm.c 只是定义了一些自己独有的操作。 static const struct file_operations input_devices_fileops = { .owner = THIS_MODULE, .open = input_proc_devices_open, .poll = input_proc_devices_poll, .read = seq_read, .llseek = seq_lseek, .release = seq_release, }; 第四部分，定义整个模块的初始化函数和退出函数，用于加载和卸载这个 ko 的时候调用。例如 lp.c 就定义了 lp_init_module 和 lp_cleanup_module，logibm.c 就定义了 logibm_init 和 logibm_exit。 第五部分，调用 module_init 和 module_exit，分别指向上面两个初始化函数和退出函数。就像本节最开头展示的一样。 第六部分，声明一下 lisense，调用 MODULE_LICENSE。有了这六部分，一个内核模块就基本合格了，可以工作了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:6","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"打开字符设备 字符设备可不是一个普通的内核模块，它有自己独特的行为。接下来，我们就沿着打开一个字符设备的过程，看看字符设备这个内核模块做了哪些特殊的事情。 要使用一个字符设备，我们首先要把写好的内核模块，通过 insmod 加载进内核。这个时候，先调用的就是 module_init 调用的初始化函数。例如，在 lp.c 的初始化函数 lp_init 对应的代码如下： static int __init lp_init (void) { ...... if (register_chrdev (LP_MAJOR, \"lp\", \u0026lp_fops)) { printk (KERN_ERR \"lp: unable to get major %d\\n\", LP_MAJOR); return -EIO; } ...... } int __register_chrdev(unsigned int major, unsigned int baseminor, unsigned int count, const char *name, const struct file_operations *fops) { struct char_device_struct *cd; struct cdev *cdev; int err = -ENOMEM; ...... cd = __register_chrdev_region(major, baseminor, count, name); cdev = cdev_alloc(); cdev-\u003eowner = fops-\u003eowner; cdev-\u003eops = fops; kobject_set_name(\u0026cdev-\u003ekobj, \"%s\", name); err = cdev_add(cdev, MKDEV(cd-\u003emajor, baseminor), count); cd-\u003ecdev = cdev; return major ? 0 : cd-\u003emajor; } 在字符设备驱动的内核模块加载的时候，最重要的一件事情就是，注册这个字符设备。注册的方式是调用 __register_chrdev_region，注册字符设备的主次设备号和名称，然后分配一个 struct cdev 结构，将 cdev 的 ops 成员变量指向这个模块声明的 file_operations。然后，cdev_add 会将这个字符设备添加到内核中一个叫作 struct kobj_map *cdev_map 的结构，来统一管理所有字符设备。其中，MKDEV(cd-\u003emajor, baseminor) 表示将主设备号和次设备号生成一个 dev_t 的整数，然后将这个整数 dev_t 和 cdev 关联起来。 /** * cdev_add() - add a char device to the system * @p: the cdev structure for the device * @dev: the first device number for which this device is responsible * @count: the number of consecutive minor numbers corresponding to this * device * * cdev_add() adds the device represented by @p to the system, making it * live immediately. A negative error code is returned on failure. */ int cdev_add(struct cdev *p, dev_t dev, unsigned count) { int error; p-\u003edev = dev; p-\u003ecount = count; error = kobj_map(cdev_map, dev, count, NULL, exact_match, exact_lock, p); kobject_get(p-\u003ekobj.parent); return 0; 在 logibm.c 中，我们在 logibm_init 找不到注册字符设备，这是因为 input.c 里面的初始化函数 input_init 会调用 register_chrdev_region，注册输入的字符设备，会在 logibm_init 中调用 input_register_device，将 logibm.c 这个字符设备注册到 input.c 里面去，这就相当于 input.c 对多个输入字符设备进行统一的管理。内核模块加载完毕后，接下来要通过 mknod 在 /dev 下面创建一个设备文件，只有有了这个设备文件，我们才能通过文件系统的接口，对这个设备文件进行操作。mknod 也是一个系统调用，定义如下： SYSCALL_DEFINE3(mknod, const char __user *, filename, umode_t, mode, unsigned, dev) { return sys_mknodat(AT_FDCWD, filename, mode, dev); } SYSCALL_DEFINE4(mknodat, int, dfd, const char __user *, filename, umode_t, mode, unsigned, dev) { struct dentry *dentry; struct path path; ...... dentry = user_path_create(dfd, filename, \u0026path, lookup_flags); ...... switch (mode \u0026 S_IFMT) { ...... case S_IFCHR: case S_IFBLK: error = vfs_mknod(path.dentry-\u003ed_inode,dentry,mode, new_decode_dev(dev)); break; ...... } } 我们可以在这个系统调用里看到，在文件系统上，顺着路径找到 /dev/xxx 所在的文件夹，然后为这个新创建的设备文件创建一个 dentry。这是维护文件和 inode 之间的关联关系的结构。接下来，如果是字符文件 S_IFCHR 或者设备文件 S_IFBLK，我们就调用 vfs_mknod。 int vfs_mknod(struct inode *dir, struct dentry *dentry, umode_t mode, dev_t dev) { ...... error = dir-\u003ei_op-\u003emknod(dir, dentry, mode, dev); ...... } 这里需要调用对应的文件系统的 inode_operations。应该调用哪个文件系统呢？如果我们在 linux 下面执行 mount 命令，能看到下面这一行： devtmpfs on /dev type devtmpfs (rw,nosuid,size=3989584k,nr_inodes=997396,mode=755) 也就是说，/dev 下面的文件系统的名称为 devtmpfs，我们可以在内核中找到它。 static struct dentry *dev_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data) { #ifdef CONFIG_TMPFS return mount_single(fs_type, flags, data, shmem_fill_super); #else return mount_single(fs_type, flags, data, ramfs_fill_super); #endif } static struct file_system_type dev_fs_type = { .name = \"devtmpfs\", .mount = dev_mount, .kill_sb = kill_litter_super, }; 从这里可以看出，devtmpfs 在挂载的时候，有两种模式，一种是 ramfs，一种是 shmem 都是基于内存的文件系统。这里你先不用管，基于内存的文件系统具体是怎么回事儿。 static const struct inode_operations ramfs_dir_inode_operations = { ...... .mknod = ramfs_mknod, }; static const struct inode_operations shmem_dir_inode_operations = { #ifdef CONFIG_TMPFS ...... .mknod = shmem_mknod, }; 这两个 mknod 虽然实现不同，但是都会调用到同一个函数 init_special_inode。 void init_special_inode(struct inode *inode, umode_t mode, dev_t rdev) { inode-\u003ei_mode = mode; if (S_ISCHR(mode)) { inode-\u003ei_fop = \u0026def_chr_fops; inode-\u003ei_rdev = ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:7","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"写入字符设备 当我们像打开一个文件一样打开一个字符设备之后，接下来就是对这个设备的读写。对于文件的读写咱们在文件系统那一章详细讲述过，读写的过程是类似的，所以这里我们只解析打印机驱动写入的过程。 写入一个字符设备，就是用文件系统的标准接口 write，参数文件描述符 fd，在内核里面调用的 sys_write，在 sys_write 里面根据文件描述符 fd 得到 struct file 结构。接下来再调用 vfs_write。 ssize_t __vfs_write(struct file *file, const char __user *p, size_t count, loff_t *pos) { if (file-\u003ef_op-\u003ewrite) return file-\u003ef_op-\u003ewrite(file, p, count, pos); else if (file-\u003ef_op-\u003ewrite_iter) return new_sync_write(file, p, count, pos); else return -EINVAL; } 我们可以看到，在 __vfs_write 里面，我们会调用 struct file 结构里的 file_operations 的 write 函数。上面我们打开字符设备的时候，已经将 struct file 结构里面的 file_operations 指向了设备驱动程序的 file_operations 结构，所以这里的 write 函数最终会调用到 lp_write。 static ssize_t lp_write(struct file * file, const char __user * buf, size_t count, loff_t *ppos) { unsigned int minor = iminor(file_inode(file)); struct parport *port = lp_table[minor].dev-\u003eport; char *kbuf = lp_table[minor].lp_buffer; ssize_t retv = 0; ssize_t written; size_t copy_size = count; ...... /* Need to copy the data from user-space. */ if (copy_size \u003e LP_BUFFER_SIZE) copy_size = LP_BUFFER_SIZE; ...... if (copy_from_user (kbuf, buf, copy_size)) { retv = -EFAULT; goto out_unlock; } ...... do { /* Write the data. */ written = parport_write (port, kbuf, copy_size); if (written \u003e 0) { copy_size -= written; count -= written; buf += written; retv += written; } ...... if (need_resched()) schedule (); if (count) { copy_size = count; if (copy_size \u003e LP_BUFFER_SIZE) copy_size = LP_BUFFER_SIZE; if (copy_from_user(kbuf, buf, copy_size)) { if (retv == 0) retv = -EFAULT; break; } } } while (count \u003e 0); ...... 这个设备驱动程序的写入函数的实现还是比较典型的。先是调用 copy_from_user 将数据从用户态拷贝到内核态的缓存中，然后调用 parport_write 写入外部设备。这里还有一个 schedule 函数，也即写入的过程中，给其他线程抢占 CPU 的机会。然后，如果 count 还是大于 0，也就是数据还没有写完，那我们就接着 copy_from_user，接着 parport_write，直到写完为止。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:8","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"使用 IOCTL 控制设备 对于 I/O 设备来讲，我们前面也说过，除了读写设备，还会调用 ioctl，做一些特殊的 I/O 操作。 ioctl 也是一个系统调用，它在内核里面的定义如下： SYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg) { int error; struct fd f = fdget(fd); ...... error = do_vfs_ioctl(f.file, fd, cmd, arg); fdput(f); return error; } 其中，fd 是这个设备的文件描述符，cmd 是传给这个设备的命令，arg 是命令的参数。其中，对于命令和命令的参数，使用 ioctl 系统调用的用户和驱动程序的开发人员约定好行为即可。其实 cmd 看起来是一个 int，其实他的组成比较复杂，它由几部分组成： 最低八位为 NR，是命令号；然后八位是 TYPE，是类型；然后十四位是参数的大小；最高两位是 DIR，是方向，表示写入、读出，还是读写。 由于组成比较复杂，有一些宏是专门用于组成这个 cmd 值的。 /* * Used to create numbers. */ #define _IO(type,nr) _IOC(_IOC_NONE,(type),(nr),0) #define _IOR(type,nr,size) _IOC(_IOC_READ,(type),(nr),(_IOC_TYPECHECK(size))) #define _IOW(type,nr,size) _IOC(_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size))) #define _IOWR(type,nr,size) _IOC(_IOC_READ|_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size))) /* used to decode ioctl numbers.. */ #define _IOC_DIR(nr) (((nr) \u003e\u003e _IOC_DIRSHIFT) \u0026 _IOC_DIRMASK) #define _IOC_TYPE(nr) (((nr) \u003e\u003e _IOC_TYPESHIFT) \u0026 _IOC_TYPEMASK) #define _IOC_NR(nr) (((nr) \u003e\u003e _IOC_NRSHIFT) \u0026 _IOC_NRMASK) #define _IOC_SIZE(nr) (((nr) \u003e\u003e _IOC_SIZESHIFT) \u0026 _IOC_SIZEMASK) 在用户程序中，可以通过上面的“Used to create numbers”这些宏，根据参数生成 cmd，在驱动程序中，可以通过下面的“used to decode ioctl numbers”这些宏，解析 cmd 后，执行指令。ioctl 中会调用 do_vfs_ioctl，这里面对于已经定义好的 cmd，进行相应的处理。如果不是默认定义好的 cmd，则执行默认操作。对于普通文件，调用 file_ioctl；对于其他文件调用 vfs_ioctl。 int do_vfs_ioctl(struct file *filp, unsigned int fd, unsigned int cmd, unsigned long arg) { int error = 0; int __user *argp = (int __user *)arg; struct inode *inode = file_inode(filp); switch (cmd) { ...... case FIONBIO: error = ioctl_fionbio(filp, argp); break; case FIOASYNC: error = ioctl_fioasync(fd, filp, argp); break; ...... case FICLONE: return ioctl_file_clone(filp, arg, 0, 0, 0); default: if (S_ISREG(inode-\u003ei_mode)) error = file_ioctl(filp, cmd, arg); else error = vfs_ioctl(filp, cmd, arg); break; } return error; 由于咱们这里是设备驱动程序，所以调用的是 vfs_ioctl。 /** * vfs_ioctl - call filesystem specific ioctl methods * @filp: open file to invoke ioctl method on * @cmd: ioctl command to execute * @arg: command-specific argument for ioctl * * Invokes filesystem specific -\u003eunlocked_ioctl, if one exists; otherwise * returns -ENOTTY. * * Returns 0 on success, -errno on error. */ long vfs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg) { int error = -ENOTTY; if (!filp-\u003ef_op-\u003eunlocked_ioctl) goto out; error = filp-\u003ef_op-\u003eunlocked_ioctl(filp, cmd, arg); if (error == -ENOIOCTLCMD) error = -ENOTTY; out: return error; 这里面调用的是 struct file 里 file_operations 的 unlocked_ioctl 函数。我们前面初始化设备驱动的时候，已经将 file_operations 指向设备驱动的 file_operations 了。这里调用的是设备驱动的 unlocked_ioctl。对于打印机程序来讲，调用的是 lp_ioctl。可以看出来，这里面就是 switch 语句，它会根据不同的 cmd，做不同的操作。 static long lp_ioctl(struct file *file, unsigned int cmd, unsigned long arg) { unsigned int minor; struct timeval par_timeout; int ret; minor = iminor(file_inode(file)); mutex_lock(\u0026lp_mutex); switch (cmd) { ...... default: ret = lp_do_ioctl(minor, cmd, arg, (void __user *)arg); break; } mutex_unlock(\u0026lp_mutex); return ret; } static int lp_do_ioctl(unsigned int minor, unsigned int cmd, unsigned long arg, void __user *argp) { int status; int retval = 0; switch ( cmd ) { case LPTIME: if (arg \u003e UINT_MAX / HZ) return -EINVAL; LP_TIME(minor) = arg * HZ/100; break; case LPCHAR: LP_CHAR(minor) = arg; break; case LPABORT: if (arg) LP_F(minor) |= LP_ABORT; else LP_F(minor) \u0026= ~LP_ABORT; break; case LPABORTOPEN: if (arg) LP_F(minor) |= LP_ABORTOPEN; else LP_F(minor) \u0026= ~LP_ABORTOPEN; break; case LPCAREFUL: if (arg) LP_F(minor) |= LP_CAREFUL; else LP_F(minor) \u0026= ~LP_CAREFUL; break; case LPWAIT: LP_WAIT(minor) = arg; break; case LPSETIRQ: return -EINVAL; break; case LPGETIRQ: if (copy_to_user(argp, \u0026LP_IRQ(minor), sizeof(int))) return -EFAULT; break; case LPGETSTATUS: if (mutex_lock_interruptible(\u0026lp_table[minor].port_mutex)) return -EINTR; lp_claim_parport_or_block (\u0026lp_table[minor]); status = r_str(minor); lp_release_parport (\u0026lp_table[minor]); mutex_unlock(\u0026lp_table[minor].port_mut","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:9","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节我们讲了字符设备的打开、写入和 ioctl 等最常见的操作。一个字符设备要能够工作，需要三部分配合。第一，有一个设备驱动程序的 ko 模块，里面有模块初始化函数、中断处理函数、设备操作函数。这里面封装了对于外部设备的操作。加载设备驱动程序模块的时候，模块初始化函数会被调用。在内核维护所有字符设备驱动的数据结构 cdev_map 里面注册，我们就可以很容易根据设备号，找到相应的设备驱动程序。第二，在 /dev 目录下有一个文件表示这个设备，这个文件在特殊的 devtmpfs 文件系统上，因而也有相应的 dentry 和 inode。这里的 inode 是一个特殊的 inode，里面有设备号。通过它，我们可以在 cdev_map 中找到设备驱动程序，里面还有针对字符设备文件的默认操作 def_chr_fops。第三，打开一个字符设备文件和打开一个普通的文件有类似的数据结构，有文件描述符、有 struct file、指向字符设备文件的 dentry 和 inode。字符设备文件的相关操作 file_operations 一开始指向 def_chr_fops，在调用 def_chr_fops 里面的 chrdev_open 函数的时候，修改为指向设备操作函数，从而读写一个字符设备文件就会直接变成读写外部设备了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:33:10","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"33 | 字符设备（下）：如何建立直销模式？ 上一节，我们讲了一个设备能够被打开、能够读写，主流的功能基本就完成了。我们讲输入输出设备的时候说到，如果一个设备有事情需要通知操作系统，会通过中断和设备驱动程序进行交互，今天我们就来解析中断处理机制。鼠标就是通过中断，将自己的位置和按键信息，传递给设备驱动程序。 static int logibm_open(struct input_dev *dev) { if (request_irq(logibm_irq, logibm_interrupt, 0, \"logibm\", NULL)) { printk(KERN_ERR \"logibm.c: Can't allocate irq %d\\n\", logibm_irq); return -EBUSY; } outb(LOGIBM_ENABLE_IRQ, LOGIBM_CONTROL_PORT); return 0; } static irqreturn_t logibm_interrupt(int irq, void *dev_id) { char dx, dy; unsigned char buttons; outb(LOGIBM_READ_X_LOW, LOGIBM_CONTROL_PORT); dx = (inb(LOGIBM_DATA_PORT) \u0026 0xf); outb(LOGIBM_READ_X_HIGH, LOGIBM_CONTROL_PORT); dx |= (inb(LOGIBM_DATA_PORT) \u0026 0xf) \u003c\u003c 4; outb(LOGIBM_READ_Y_LOW, LOGIBM_CONTROL_PORT); dy = (inb(LOGIBM_DATA_PORT) \u0026 0xf); outb(LOGIBM_READ_Y_HIGH, LOGIBM_CONTROL_PORT); buttons = inb(LOGIBM_DATA_PORT); dy |= (buttons \u0026 0xf) \u003c\u003c 4; buttons = ~buttons \u003e\u003e 5; input_report_rel(logibm_dev, REL_X, dx); input_report_rel(logibm_dev, REL_Y, dy); input_report_key(logibm_dev, BTN_RIGHT, buttons \u0026 1); input_report_key(logibm_dev, BTN_MIDDLE, buttons \u0026 2); input_report_key(logibm_dev, BTN_LEFT, buttons \u0026 4); input_sync(logibm_dev); outb(LOGIBM_ENABLE_IRQ, LOGIBM_CONTROL_PORT); return IRQ_HANDLED 要处理中断，需要有一个中断处理函数。定义如下： irqreturn_t (*irq_handler_t)(int irq, void * dev_id); /** * enum irqreturn * @IRQ_NONE interrupt was not from this device or was not handled * @IRQ_HANDLED interrupt was handled by this device * @IRQ_WAKE_THREAD handler requests to wake the handler thread */ enum irqreturn { IRQ_NONE = (0 \u003c\u003c 0), IRQ_HANDLED = (1 \u003c\u003c 0), IRQ_WAKE_THREAD = (1 \u003c\u003c 1), }; 其中，irq 是一个整数，是中断信号。dev_id 是一个 void * 的通用指针，主要用于区分同一个中断处理函数对于不同设备的处理。这里的返回值有三种：IRQ_NONE 表示不是我的中断，不归我管；IRQ_HANDLED 表示处理完了的中断；IRQ_WAKE_THREAD 表示有一个进程正在等待这个中断，中断处理完了，应该唤醒它。上面的例子中，logibm_interrupt 这个中断处理函数，先是获取了 x 和 y 的移动坐标，以及左中右的按键，上报上去，然后返回 IRQ_HANDLED，这表示处理完毕。其实，写一个真正生产用的中断处理程序还是很复杂的。当一个中断信号 A 触发后，正在处理的过程中，这个中断信号 A 是应该暂时关闭的，这样是为了防止再来一个中断信号 A，在当前的中断信号 A 的处理过程中插一杠子。但是，这个暂时关闭的时间应该多长呢？如果太短了，应该原子化处理完毕的没有处理完毕，又被另一个中断信号 A 中断了，很多操作就不正确了；如果太长了，一直关闭着，新的中断信号 A 进不来，系统就显得很慢。所以，很多中断处理程序将整个中断要做的事情分成两部分，称为上半部和下半部，或者成为关键处理部分和延迟处理部分。在中断处理函数中，仅仅处理关键部分，完成了就将中断信号打开，使得新的中断可以进来，需要比较长时间处理的部分，也即延迟部分，往往通过工作队列等方式慢慢处理。这个写起来可以是一本书了，推荐你好好读一读《Linux Device Drivers》这本书，这里我就不详细介绍了。有了中断处理函数，接下来要调用 request_irq 来注册这个中断处理函数。request_irq 有这样几个参数： unsigned int irq 是中断信号；irq_handler_t handler 是中断处理函数；unsigned long flags 是一些标识位；const char *name 是设备名称；void *dev 这个通用指针应该和中断处理函数的 void *dev 相对应。 static inline int __must_check request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags, const char *name, void *dev) { return request_threaded_irq(irq, handler, NULL, flags, name, dev); } 中断处理函数被注册到哪里去呢？让我们沿着 request_irq 看下去。request_irq 调用的是 request_threaded_irq。代码如下： int request_threaded_irq(unsigned int irq, irq_handler_t handler, irq_handler_t thread_fn, unsigned long irqflags, const char *devname, void *dev_id) { struct irqaction *action; struct irq_desc *desc; int retval; ...... desc = irq_to_desc(irq); ...... action = kzalloc(sizeof(struct irqaction), GFP_KERNEL); action-\u003ehandler = handler; action-\u003ethread_fn = thread_fn; action-\u003eflags = irqflags; action-\u003ename = devname; action-\u003edev_id = dev_id; ...... retval = __setup_irq(irq, desc, action); ...... } 对于每一个中断，都有一个对中断的描述结构 struct irq_desc。它有一个重要的成员变量是 struct irqaction，用于表示处理这个中断的动作。如果我们仔细看这个结构，会发现，它里面有 next 指针，也就是说，这是一个链表，对于这个中断的所有处理动作，都串在这个链表上。 struct irq_desc { ...... struct irqaction *action; /* IRQ action list */ ...... struct module *owner; const char *name; }; /** * struct irqaction - per interrupt action descriptor * @handler: interrupt handler function * @name: name of the device * @dev_id: cookie to identify the device * @percpu_dev_id: cookie to identify the device * @next: pointer to the next irqaction for shared interrupts * @irq: interrupt number * @flags: flags (see IRQF_* above) * @thread_fn: interrupt handler function for threaded interrupts * @thread: thread pointer for threaded interrupts * @secondary: pointer to secondary","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:34:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们讲了中断的整个处理过程。中断是从外部设备发起的，会形成外部中断。外部中断会到达中断控制器，中断控制器会发送中断向量 Interrupt Vector 给 CPU。对于每一个 CPU，都要求有一个 idt_table，里面存放了不同的中断向量的处理函数。中断向量表中已经填好了前 32 位，外加一位 32 位系统调用，其他的都是用于设备中断。硬件中断的处理函数是 do_IRQ 进行统一处理，在这里会让中断向量，通过 vector_irq 映射为 irq_desc。irq_desc 是一个用于描述用户注册的中断处理函数的结构，为了能够根据中断向量得到 irq_desc 结构，会把这些结构放在一个基数树里面，方便查找。irq_desc 里面有一个成员是 irqaction，指向设备驱动程序里面注册的中断处理函数。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:34:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"34 | 块设备（上）：如何建立代理商销售模式？ 上一章，我们解析了文件系统，最后讲文件系统读写的流程到达底层的时候，没有更深入地分析下去，这是因为文件系统再往下就是硬盘设备了。上两节，我们解析了字符设备的 mknod、打开和读写流程。那这一节我们就来讲块设备的 mknod、打开流程，以及文件系统和下层的硬盘设备的读写流程。块设备一般会被格式化为文件系统，但是，下面的讲述中，你可能会有一点困惑。你会看到各种各样的 dentry 和 inode。块设备涉及三种文件系统，所以你看到的这些 dentry 和 inode 可能都不是一回事儿，请注意分辨。块设备需要 mknod 吗？对于启动盘，你可能觉得，启动了就在那里了。可是如果我们要插进一块新的 USB 盘，还是要有这个操作的。 mknod 还是会创建在 /dev 路径下面，这一点和字符设备一样。/dev 路径下面是 devtmpfs 文件系统。这是块设备遇到的第一个文件系统。我们会为这个块设备文件，分配一个特殊的 inode，这一点和字符设备也是一样的。只不过字符设备走 S_ISCHR 这个分支，对应 inode 的 file_operations 是 def_chr_fops；而块设备走 S_ISBLK 这个分支，对应的 inode 的 file_operations 是 def_blk_fops。这里要注意，inode 里面的 i_rdev 被设置成了块设备的设备号 dev_t，这个我们后面会用到，你先记住有这么一回事儿。 void init_special_inode(struct inode *inode, umode_t mode, dev_t rdev) { inode-\u003ei_mode = mode; if (S_ISCHR(mode)) { inode-\u003ei_fop = \u0026def_chr_fops; inode-\u003ei_rdev = rdev; } else if (S_ISBLK(mode)) { inode-\u003ei_fop = \u0026def_blk_fops; inode-\u003ei_rdev = rdev; } else if (S_ISFIFO(mode)) inode-\u003ei_fop = \u0026pipefifo_fops; else if (S_ISSOCK(mode)) ; /* leave it no_open_fops */ } 特殊 inode 的默认 file_operations 是 def_blk_fops，就像字符设备一样，有打开、读写这个块设备文件，但是我们常规操作不会这样做。我们会将这个块设备文件 mount 到一个文件夹下面。 const struct file_operations def_blk_fops = { .open = blkdev_open, .release = blkdev_close, .llseek = block_llseek, .read_iter = blkdev_read_iter, .write_iter = blkdev_write_iter, .mmap = generic_file_mmap, .fsync = blkdev_fsync, .unlocked_ioctl = block_ioctl, .splice_read = generic_file_splice_read, .splice_write = iter_file_splice_write, .fallocate = blkdev_fallocate, }; 不过，这里我们还是简单看一下，打开这个块设备的操作 blkdev_open。它里面调用的是 blkdev_get 打开这个块设备，了解到这一点就可以了。接下来，我们要调用 mount，将这个块设备文件挂载到一个文件夹下面。如果这个块设备原来被格式化为一种文件系统的格式，例如 ext4，那我们调用的就是 ext4 相应的 mount 操作。这是块设备遇到的第二个文件系统，也是向这个块设备读写文件，需要基于的主流文件系统。咱们在文件系统那一节解析的对于文件的读写流程，都是基于这个文件系统的。还记得，咱们注册 ext4 文件系统的时候，有下面这样的结构： static struct file_system_type ext4_fs_type = { .owner = THIS_MODULE, .name = \"ext4\", .mount = ext4_mount, .kill_sb = kill_block_super, .fs_flags = FS_REQUIRES_DEV, }; 在将一个硬盘的块设备 mount 成为 ext4 的时候，我们会调用 ext4_mount-\u003emount_bdev。 static struct dentry *ext4_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data) { return mount_bdev(fs_type, flags, dev_name, data, ext4_fill_super); } struct dentry *mount_bdev(struct file_system_type *fs_type, int flags, const char *dev_name, void *data, int (*fill_super)(struct super_block *, void *, int)) { struct block_device *bdev; struct super_block *s; fmode_t mode = FMODE_READ | FMODE_EXCL; int error = 0; if (!(flags \u0026 MS_RDONLY)) mode |= FMODE_WRITE; bdev = blkdev_get_by_path(dev_name, mode, fs_type); ...... s = sget(fs_type, test_bdev_super, set_bdev_super, flags | MS_NOSEC, bdev); ...... return dget(s-\u003es_root); ...... } mount_bdev 主要做了两件大事情。第一，blkdev_get_by_path 根据 /dev/xxx 这个名字，找到相应的设备并打开它；第二，sget 根据打开的设备文件，填充 ext4 文件系统的 super_block，从而以此为基础，建立一整套咱们在文件系统那一章讲的体系。一旦这套体系建立起来以后，对于文件的读写都是通过 ext4 文件系统这个体系进行的，创建的 inode 结构也是指向 ext4 文件系统的。文件系统那一章我们只解析了这部分，由于没有到达底层，也就没有关注块设备相关的操作。这一章我们重新回过头来，一方面看 mount 的时候，对于块设备都做了哪些操作，另一方面看读写的时候，到了底层，对于块设备做了哪些操作。这里我们先来看 mount_bdev 做的第一件大事情，通过 blkdev_get_by_path，根据设备名 /dev/xxx，得到 struct block_device *bdev。 /** * blkdev_get_by_path - open a block device by name * @path: path to the block device to open * @mode: FMODE_* mask * @holder: exclusive holder identifier * * Open the blockdevice described by the device file at @path. @mode * and @holder are identical to blkdev_get(). * * On success, the returned block_device has reference count of one. */ struct block_device *blkdev_get_by_path(const char *path, fmode_t mode, void *holder) { struct block_device *bdev; int err; bdev = lookup_bdev(path); ...... err = blkdev_get(bdev, mode, holder); ...... return bdev; } blkdev_get_by_path 干了两件事情。第一个，lookup_bdev 根据设备路径 /dev/xxx 得到 block_device。第二个，打开这个设备，调用 blkdev_get。咱们上面分析过 def_blk_fops 的默认打开设备函数 blkdev_open，它也是调用 blkdev_get 的。块设备的打开往往不是直接调用设备文件的打开函数，而是调用 mount 来打开的。 /** * lookup_bdev - lookup a struct block_device by name * @pathname: special file representing the block device * * Get a reference to the bloc","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:35:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 从这一节我们可以看出，块设备比字符设备复杂多了，涉及三个文件系统，工作过程我用一张图总结了一下，下面带你总结一下。 所有的块设备被一个 map 结构管理从 dev_t 到 gendisk 的映射；所有的 block_device 表示的设备或者分区都在 bdev 文件系统的 inode 列表中；mknod 创建出来的块设备文件在 devtemfs 文件系统里面，特殊 inode 里面有块设备号；mount 一个块设备上的文件系统，调用这个文件系统的 mount 接口；通过按照 /dev/xxx 在文件系统 devtmpfs 文件系统上搜索到特殊 inode，得到块设备号；根据特殊 inode 里面的 dev_t 在 bdev 文件系统里面找到 inode；根据 bdev 文件系统上的 inode 找到对应的 block_device，根据 dev_t 在 map 中找到 gendisk，将两者关联起来；找到 block_device 后打开设备，调用和 block_device 关联的 gendisk 里面的 block_device_operations 打开设备；创建被 mount 的文件系统的 super_block。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:35:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"35 | 块设备（下）：如何建立代理商销售模式？ 在文件系统那一节，我们讲了文件的写入，到了设备驱动这一层，就没有再往下分析。上一节我们又讲了 mount 一个块设备，将 block_device 信息放到了 ext4 文件系统的 super_block 里面，有了这些基础，是时候把整个写入的故事串起来了。还记得咱们在文件系统那一节分析写入流程的时候，对于 ext4 文件系统，最后调用的是 ext4_file_write_iter，它将 I/O 的调用分成两种情况： 第一是直接 I/O。最终我们调用的是 generic_file_direct_write，这里调用的是 mapping-\u003ea_ops-\u003edirect_IO，实际调用的是 ext4_direct_IO，往设备层写入数据。第二种是缓存 I/O。最终我们会将数据从应用拷贝到内存缓存中，但是这个时候，并不执行真正的 I/O 操作。它们只将整个页或其中部分标记为脏。写操作由一个 timer 触发，那个时候，才调用 wb_workfn 往硬盘写入页面。 接下来的调用链为：wb_workfn-\u003ewb_do_writeback-\u003ewb_writeback-\u003ewriteback_sb_inodes-\u003e__writeback_single_inode-\u003edo_writepages。在 do_writepages 中，我们要调用 mapping-\u003ea_ops-\u003ewritepages，但实际调用的是 ext4_writepages，往设备层写入数据。这一节，我们就沿着这两种情况分析下去。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:36:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"直接 I/O 如何访问块设备？ 我们先来看第一种情况，直接 I/O 调用到 ext4_direct_IO。 static ssize_t ext4_direct_IO(struct kiocb *iocb, struct iov_iter *iter) { struct file *file = iocb-\u003eki_filp; struct inode *inode = file-\u003ef_mapping-\u003ehost; size_t count = iov_iter_count(iter); loff_t offset = iocb-\u003eki_pos; ssize_t ret; ...... ret = ext4_direct_IO_write(iocb, iter); ...... } static ssize_t ext4_direct_IO_write(struct kiocb *iocb, struct iov_iter *iter) { struct file *file = iocb-\u003eki_filp; struct inode *inode = file-\u003ef_mapping-\u003ehost; struct ext4_inode_info *ei = EXT4_I(inode); ssize_t ret; loff_t offset = iocb-\u003eki_pos; size_t count = iov_iter_count(iter); ...... ret = __blockdev_direct_IO(iocb, inode, inode-\u003ei_sb-\u003es_bdev, iter, get_block_func, ext4_end_io_dio, NULL, dio_flags); …… } 在 ext4_direct_IO_write 调用 __blockdev_direct_IO，有个参数你需要特别注意一下，那就是 inode-\u003ei_sb-\u003es_bdev。通过当前文件的 inode，我们可以得到 super_block。这个 super_block 中的 s_bdev，就是咱们上一节填进去的那个 block_device。__blockdev_direct_IO 会调用 do_blockdev_direct_IO，在这里面我们要准备一个 struct dio 结构和 struct dio_submit 结构，用来描述将要发生的写入请求。 static inline ssize_t do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode, struct block_device *bdev, struct iov_iter *iter, get_block_t get_block, dio_iodone_t end_io, dio_submit_t submit_io, int flags) { unsigned i_blkbits = ACCESS_ONCE(inode-\u003ei_blkbits); unsigned blkbits = i_blkbits; unsigned blocksize_mask = (1 \u003c\u003c blkbits) - 1; ssize_t retval = -EINVAL; size_t count = iov_iter_count(iter); loff_t offset = iocb-\u003eki_pos; loff_t end = offset + count; struct dio *dio; struct dio_submit sdio = { 0, }; struct buffer_head map_bh = { 0, }; ...... dio = kmem_cache_alloc(dio_cache, GFP_KERNEL); dio-\u003eflags = flags; dio-\u003ei_size = i_size_read(inode); dio-\u003einode = inode; if (iov_iter_rw(iter) == WRITE) { dio-\u003eop = REQ_OP_WRITE; dio-\u003eop_flags = REQ_SYNC | REQ_IDLE; if (iocb-\u003eki_flags \u0026 IOCB_NOWAIT) dio-\u003eop_flags |= REQ_NOWAIT; } else { dio-\u003eop = REQ_OP_READ; } sdio.blkbits = blkbits; sdio.blkfactor = i_blkbits - blkbits; sdio.block_in_file = offset \u003e\u003e blkbits; sdio.get_block = get_block; dio-\u003eend_io = end_io; sdio.submit_io = submit_io; sdio.final_block_in_bio = -1; sdio.next_block_for_io = -1; dio-\u003eiocb = iocb; dio-\u003erefcount = 1; sdio.iter = iter; sdio.final_block_in_request = (offset + iov_iter_count(iter)) \u003e\u003e blkbits; ...... sdio.pages_in_io += iov_iter_npages(iter, INT_MAX); retval = do_direct_IO(dio, \u0026sdio, \u0026map_bh); ..... } do_direct_IO 里面有两层循环，第一层循环是依次处理这次要写入的所有块。对于每一块，取出对应的内存中的页 page，在这一块中，有写入的起始地址 from 和终止地址 to，所以，第二层循环就是依次处理 from 到 to 的数据，调用 submit_page_section，提交到块设备层进行写入。 static int do_direct_IO(struct dio *dio, struct dio_submit *sdio, struct buffer_head *map_bh) { const unsigned blkbits = sdio-\u003eblkbits; const unsigned i_blkbits = blkbits + sdio-\u003eblkfactor; int ret = 0; while (sdio-\u003eblock_in_file \u003c sdio-\u003efinal_block_in_request) { struct page *page; size_t from, to; page = dio_get_page(dio, sdio); from = sdio-\u003ehead ? 0 : sdio-\u003efrom; to = (sdio-\u003ehead == sdio-\u003etail - 1) ? sdio-\u003eto : PAGE_SIZE; sdio-\u003ehead++; while (from \u003c to) { unsigned this_chunk_bytes; /* # of bytes mapped */ unsigned this_chunk_blocks; /* # of blocks */ ...... ret = submit_page_section(dio, sdio, page, from, this_chunk_bytes, sdio-\u003enext_block_for_io, map_bh); ...... sdio-\u003enext_block_for_io += this_chunk_blocks; sdio-\u003eblock_in_file += this_chunk_blocks; from += this_chunk_bytes; dio-\u003eresult += this_chunk_bytes; sdio-\u003eblocks_available -= this_chunk_blocks; if (sdio-\u003eblock_in_file == sdio-\u003efinal_block_in_request) break; ...... } } } submit_page_section 会调用 dio_bio_submit，进而调用 submit_bio 向块设备层提交数据。其中，参数 struct bio 是将数据传给块设备的通用传输对象。定义如下： /** * submit_bio - submit a bio to the block device layer for I/O * @bio: The \u0026struct bio which describes the I/O */ blk_qc_t submit_bio(struct bio *bio) { ...... return generic_make_request(bio); } ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:36:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"缓存 I/O 如何访问块设备？ 我们再来看第二种情况，缓存 I/O 调用到 ext4_writepages。这个函数比较长，我们这里只截取最重要的部分来讲解。 static int ext4_writepages(struct address_space *mapping, struct writeback_control *wbc) { ...... struct mpage_da_data mpd; struct inode *inode = mapping-\u003ehost; struct ext4_sb_info *sbi = EXT4_SB(mapping-\u003ehost-\u003ei_sb); ...... mpd.do_map = 0; mpd.io_submit.io_end = ext4_init_io_end(inode, GFP_KERNEL); ret = mpage_prepare_extent_to_map(\u0026mpd); /* Submit prepared bio */ ext4_io_submit(\u0026mpd.io_submit); ...... } 这里比较重要的一个数据结构是 struct mpage_da_data。这里面有文件的 inode、要写入的页的偏移量，还有一个重要的 struct ext4_io_submit，里面有通用传输对象 bio。 struct mpage_da_data { struct inode *inode; ...... pgoff_t first_page; /* The first page to write */ pgoff_t next_page; /* Current page to examine */ pgoff_t last_page; /* Last page to examine */ struct ext4_map_blocks map; struct ext4_io_submit io_submit; /* IO submission data */ unsigned int do_map:1; }; struct ext4_io_submit { ...... struct bio *io_bio; ext4_io_end_t *io_end; sector_t io_next_block; }; 在 ext4_writepages 中，mpage_prepare_extent_to_map 用于初始化这个 struct mpage_da_data 结构。接下来的调用链为：mpage_prepare_extent_to_map-\u003empage_process_page_bufs-\u003empage_submit_page-\u003eext4_bio_write_page-\u003eio_submit_add_bh。在 io_submit_add_bh 中，此时的 bio 还是空的，因而我们要调用 io_submit_init_bio，初始化 bio。 static int io_submit_init_bio(struct ext4_io_submit *io, struct buffer_head *bh) { struct bio *bio; bio = bio_alloc(GFP_NOIO, BIO_MAX_PAGES); if (!bio) return -ENOMEM; wbc_init_bio(io-\u003eio_wbc, bio); bio-\u003ebi_iter.bi_sector = bh-\u003eb_blocknr * (bh-\u003eb_size \u003e\u003e 9); bio-\u003ebi_bdev = bh-\u003eb_bdev; bio-\u003ebi_end_io = ext4_end_bio; bio-\u003ebi_private = ext4_get_io_end(io-\u003eio_end); io-\u003eio_bio = bio; io-\u003eio_next_block = bh-\u003eb_blocknr; return 0; } 我们再回到 ext4_writepages 中。在 bio 初始化完之后，我们要调用 ext4_io_submit，提交 I/O。在这里我们又是调用 submit_bio，向块设备层传输数据。ext4_io_submit 的实现如下： void ext4_io_submit(struct ext4_io_submit *io) { struct bio *bio = io-\u003eio_bio; if (bio) { int io_op_flags = io-\u003eio_wbc-\u003esync_mode == WB_SYNC_ALL ? REQ_SYNC : 0; io-\u003eio_bio-\u003ebi_write_hint = io-\u003eio_end-\u003einode-\u003ei_write_hint; bio_set_op_attrs(io-\u003eio_bio, REQ_OP_WRITE, io_op_flags); submit_bio(io-\u003eio_bio); } io-\u003eio_bio = NULL; } ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:36:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"如何向块设备层提交请求？ 既然不管是直接 I/O，还是缓存 I/O，最后都到了 submit_bio 里面，那我们就来重点分析一下它。submit_bio 会调用 generic_make_request。代码如下： blk_qc_t generic_make_request(struct bio *bio) { /* * bio_list_on_stack[0] contains bios submitted by the current * make_request_fn. * bio_list_on_stack[1] contains bios that were submitted before * the current make_request_fn, but that haven't been processed * yet. */ struct bio_list bio_list_on_stack[2]; blk_qc_t ret = BLK_QC_T_NONE; ...... if (current-\u003ebio_list) { bio_list_add(\u0026current-\u003ebio_list[0], bio); goto out; } bio_list_init(\u0026bio_list_on_stack[0]); current-\u003ebio_list = bio_list_on_stack; do { struct request_queue *q = bdev_get_queue(bio-\u003ebi_bdev); if (likely(blk_queue_enter(q, bio-\u003ebi_opf \u0026 REQ_NOWAIT) == 0)) { struct bio_list lower, same; /* Create a fresh bio_list for all subordinate requests */ bio_list_on_stack[1] = bio_list_on_stack[0]; bio_list_init(\u0026bio_list_on_stack[0]); ret = q-\u003emake_request_fn(q, bio); blk_queue_exit(q); /* sort new bios into those for a lower level * and those for the same level */ bio_list_init(\u0026lower); bio_list_init(\u0026same); while ((bio = bio_list_pop(\u0026bio_list_on_stack[0])) != NULL) if (q == bdev_get_queue(bio-\u003ebi_bdev)) bio_list_add(\u0026same, bio); else bio_list_add(\u0026lower, bio); /* now assemble so we handle the lowest level first */ bio_list_merge(\u0026bio_list_on_stack[0], \u0026lower); bio_list_merge(\u0026bio_list_on_stack[0], \u0026same); bio_list_merge(\u0026bio_list_on_stack[0], \u0026bio_list_on_stack[1]); } ...... bio = bio_list_pop(\u0026bio_list_on_stack[0]); } while (bio); current-\u003ebio_list = NULL; /* deactivate */ out: return ret; } 这里的逻辑有点复杂，我们先来看大的逻辑。在 do-while 中，我们先是获取一个请求队列 request_queue，然后调用这个队列的 make_request_fn 函数。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:36:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"块设备队列结构 如果再来看 struct block_device 结构和 struct gendisk 结构，我们会发现，每个块设备都有一个请求队列 struct request_queue，用于处理上层发来的请求。在每个块设备的驱动程序初始化的时候，会生成一个 request_queue。 struct request_queue { /* * Together with queue_head for cacheline sharing */ struct list_head queue_head; struct request *last_merge; struct elevator_queue *elevator; ...... request_fn_proc *request_fn; make_request_fn *make_request_fn; ...... } 在请求队列 request_queue 上，首先是有一个链表 list_head，保存请求 request。 struct request { struct list_head queuelist; ...... struct request_queue *q; ...... struct bio *bio; struct bio *biotail; ...... } 每个 request 包括一个链表的 struct bio，有指针指向一头一尾。 struct bio { struct bio *bi_next; /* request queue link */ struct block_device *bi_bdev; blk_status_t bi_status; ...... struct bvec_iter bi_iter; unsigned short bi_vcnt; /* how many bio_vec's */ unsigned short bi_max_vecs; /* max bvl_vecs we can hold */ atomic_t __bi_cnt; /* pin count */ struct bio_vec *bi_io_vec; /* the actual vec list */ ...... }; struct bio_vec { struct page *bv_page; unsigned int bv_len; unsigned int bv_offset; } 在 bio 中，bi_next 是链表中的下一项，struct bio_vec 指向一组页面。 在请求队列 request_queue 上，还有两个重要的函数，一个是 make_request_fn 函数，用于生成 request；另一个是 request_fn 函数，用于处理 request。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:36:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"块设备的初始化 我们还是以 scsi 驱动为例。在初始化设备驱动的时候，我们会调用 scsi_alloc_queue，把 request_fn 设置为 scsi_request_fn。我们还会调用 blk_init_allocated_queue-\u003eblk_queue_make_request，把 make_request_fn 设置为 blk_queue_bio。 /** * scsi_alloc_sdev - allocate and setup a scsi_Device * @starget: which target to allocate a \u0026scsi_device for * @lun: which lun * @hostdata: usually NULL and set by -\u003eslave_alloc instead * * Description: * Allocate, initialize for io, and return a pointer to a scsi_Device. * Stores the @shost, @channel, @id, and @lun in the scsi_Device, and * adds scsi_Device to the appropriate list. * * Return value: * scsi_Device pointer, or NULL on failure. **/ static struct scsi_device *scsi_alloc_sdev(struct scsi_target *starget, u64 lun, void *hostdata) { struct scsi_device *sdev; sdev = kzalloc(sizeof(*sdev) + shost-\u003etransportt-\u003edevice_size, GFP_ATOMIC); ...... sdev-\u003erequest_queue = scsi_alloc_queue(sdev); ...... } struct request_queue *scsi_alloc_queue(struct scsi_device *sdev) { struct Scsi_Host *shost = sdev-\u003ehost; struct request_queue *q; q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE); if (!q) return NULL; q-\u003ecmd_size = sizeof(struct scsi_cmnd) + shost-\u003ehostt-\u003ecmd_size; q-\u003erq_alloc_data = shost; q-\u003erequest_fn = scsi_request_fn; q-\u003einit_rq_fn = scsi_init_rq; q-\u003eexit_rq_fn = scsi_exit_rq; q-\u003einitialize_rq_fn = scsi_initialize_rq; //调用blk_queue_make_request(q, blk_queue_bio); if (blk_init_allocated_queue(q) \u003c 0) { blk_cleanup_queue(q); return NULL; } __scsi_init_queue(shost, q); ...... return q } 在 blk_init_allocated_queue 中，除了初始化 make_request_fn 函数，我们还要做一件很重要的事情，就是初始化 I/O 的电梯算法。 int blk_init_allocated_queue(struct request_queue *q) { q-\u003efq = blk_alloc_flush_queue(q, NUMA_NO_NODE, q-\u003ecmd_size); ...... blk_queue_make_request(q, blk_queue_bio); ...... /* init elevator */ if (elevator_init(q, NULL)) { ...... } ...... } 电梯算法有很多种类型，定义为 elevator_type。下面我来逐一说一下。 struct elevator_type elevator_noop Noop 调度算法是最简单的 IO 调度算法，它将 IO 请求放入到一个 FIFO 队列中，然后逐个执行这些 IO 请求。 struct elevator_type iosched_deadline Deadline 算法要保证每个 IO 请求在一定的时间内一定要被服务到，以此来避免某个请求饥饿。为了完成这个目标，算法中引入了两类队列，一类队列用来对请求按起始扇区序号进行排序，通过红黑树来组织，我们称为 sort_list，按照此队列传输性能会比较高；另一类队列对请求按它们的生成时间进行排序，由链表来组织，称为 fifo_list，并且每一个请求都有一个期限值。 struct elevator_type iosched_cfq 又看到了熟悉的 CFQ 完全公平调度算法。所有的请求会在多个队列中排序。同一个进程的请求，总是在同一队列中处理。时间片会分配到每个队列，通过轮询算法，我们保证了 I/O 带宽，以公平的方式，在不同队列之间进行共享。 elevator_init 中会根据名称来指定电梯算法，如果没有选择，那就默认使用 iosched_cfq。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:36:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"请求提交与调度 接下来，我们回到 generic_make_request 函数中。调用队列的 make_request_fn 函数，其实就是调用 blk_queue_bio。 static blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio) { struct request *req, *free; unsigned int request_count = 0; ...... switch (elv_merge(q, \u0026req, bio)) { case ELEVATOR_BACK_MERGE: if (!bio_attempt_back_merge(q, req, bio)) break; elv_bio_merged(q, req, bio); free = attempt_back_merge(q, req); if (free) __blk_put_request(q, free); else elv_merged_request(q, req, ELEVATOR_BACK_MERGE); goto out_unlock; case ELEVATOR_FRONT_MERGE: if (!bio_attempt_front_merge(q, req, bio)) break; elv_bio_merged(q, req, bio); free = attempt_front_merge(q, req); if (free) __blk_put_request(q, free); else elv_merged_request(q, req, ELEVATOR_FRONT_MERGE); goto out_unlock; default: break; } get_rq: req = get_request(q, bio-\u003ebi_opf, bio, GFP_NOIO); ...... blk_init_request_from_bio(req, bio); ...... add_acct_request(q, req, where); __blk_run_queue(q); out_unlock: ...... return BLK_QC_T_NONE; } blk_queue_bio 首先做的一件事情是调用 elv_merge 来判断，当前这个 bio 请求是否能够和目前已有的 request 合并起来，成为同一批 I/O 操作，从而提高读取和写入的性能。判断标准和 struct bio 的成员 struct bvec_iter 有关，它里面有两个变量，一个是起始磁盘簇 bi_sector，另一个是大小 bi_size。 enum elv_merge elv_merge(struct request_queue *q, struct request **req, struct bio *bio) { struct elevator_queue *e = q-\u003eelevator; struct request *__rq; ...... if (q-\u003elast_merge \u0026\u0026 elv_bio_merge_ok(q-\u003elast_merge, bio)) { enum elv_merge ret = blk_try_merge(q-\u003elast_merge, bio); if (ret != ELEVATOR_NO_MERGE) { *req = q-\u003elast_merge; return ret; } } ...... __rq = elv_rqhash_find(q, bio-\u003ebi_iter.bi_sector); if (__rq \u0026\u0026 elv_bio_merge_ok(__rq, bio)) { *req = __rq; return ELEVATOR_BACK_MERGE; } if (e-\u003euses_mq \u0026\u0026 e-\u003etype-\u003eops.mq.request_merge) return e-\u003etype-\u003eops.mq.request_merge(q, req, bio); else if (!e-\u003euses_mq \u0026\u0026 e-\u003etype-\u003eops.sq.elevator_merge_fn) return e-\u003etype-\u003eops.sq.elevator_merge_fn(q, req, bio); return ELEVATOR_NO_MERGE; } elv_merge 尝试了三次合并。第一次，它先判断和上一次合并的 request 能不能再次合并，看看能不能赶上马上要走的这部电梯。在 blk_try_merge 主要做了这样的判断：如果 blk_rq_pos(rq) + blk_rq_sectors(rq) == bio-\u003ebi_iter.bi_sector，也就是说这个 request 的起始地址加上它的大小（其实是这个 request 的结束地址），如果和 bio 的起始地址能接得上，那就把 bio 放在 request 的最后，我们称为 ELEVATOR_BACK_MERGE。如果 blk_rq_pos(rq) - bio_sectors(bio) == bio-\u003ebi_iter.bi_sector，也就是说，这个 request 的起始地址减去 bio 的大小等于 bio 的起始地址，这说明 bio 放在 request 的最前面能够接得上，那就把 bio 放在 request 的最前面，我们称为 ELEVATOR_FRONT_MERGE。否则，那就不合并，我们称为 ELEVATOR_NO_MERGE。 enum elv_merge blk_try_merge(struct request *rq, struct bio *bio) { ...... if (blk_rq_pos(rq) + blk_rq_sectors(rq) == bio-\u003ebi_iter.bi_sector) return ELEVATOR_BACK_MERGE; else if (blk_rq_pos(rq) - bio_sectors(bio) == bio-\u003ebi_iter.bi_sector) return ELEVATOR_FRONT_MERGE; return ELEVATOR_NO_MERGE; } 第二次，如果和上一个合并过的 request 无法合并，那我们就调用 elv_rqhash_find。然后按照 bio 的起始地址查找 request，看有没有能够合并的。如果有的话，因为是按照起始地址找的，应该接在人家的后面，所以是 ELEVATOR_BACK_MERGE。第三次，调用 elevator_merge_fn 试图合并。对于 iosched_cfq，调用的是 cfq_merge。在这里面，cfq_find_rq_fmerge 会调用 elv_rb_find 函数，里面的参数是 bio 的结束地址。我们还是要看，能不能找到可以合并的。如果有的话，因为是按照结束地址找的，应该接在人家前面，所以是 ELEVATOR_FRONT_MERGE。 static enum elv_merge cfq_merge(struct request_queue *q, struct request **req, struct bio *bio) { struct cfq_data *cfqd = q-\u003eelevator-\u003eelevator_data; struct request *__rq; __rq = cfq_find_rq_fmerge(cfqd, bio); if (__rq \u0026\u0026 elv_bio_merge_ok(__rq, bio)) { *req = __rq; return ELEVATOR_FRONT_MERGE; } return ELEVATOR_NO_MERGE; } static struct request * cfq_find_rq_fmerge(struct cfq_data *cfqd, struct bio *bio) { struct task_struct *tsk = current; struct cfq_io_cq *cic; struct cfq_queue *cfqq; cic = cfq_cic_lookup(cfqd, tsk-\u003eio_context); if (!cic) return NULL; cfqq = cic_to_cfqq(cic, op_is_sync(bio-\u003ebi_opf)); if (cfqq) return elv_rb_find(\u0026cfqq-\u003esort_list, bio_end_sector(bio)); return NUL } 等从 elv_merge 返回 blk_queue_bio 的时候，我们就知道，应该做哪种类型的合并，接着就要进行真的合并。如果没有办法合并，那就调用 get_request，创建一个新的 request，调用 blk_init_request_from_bio，将 bio 放到新的 request 里面，然后调用 add_acct_request，把新的 request 加到 request_queue 队列中。至此，我们解析完了 generic_make_request 中最重要的两大逻辑：获取一个请求队列 request_queue 和调用这个队列的 ma","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:36:6","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"请求的处理 设备驱动程序往设备里面写，调用的是请求队列 request_queue 的另外一个函数 request_fn。对于 scsi 设备来讲，调用的是 scsi_request_fn。 static void scsi_request_fn(struct request_queue *q) __releases(q-\u003equeue_lock) __acquires(q-\u003equeue_lock) { struct scsi_device *sdev = q-\u003equeuedata; struct Scsi_Host *shost; struct scsi_cmnd *cmd; struct request *req; /* * To start with, we keep looping until the queue is empty, or until * the host is no longer able to accept any more requests. */ shost = sdev-\u003ehost; for (;;) { int rtn; /* * get next queueable request. We do this early to make sure * that the request is fully prepared even if we cannot * accept it. */ req = blk_peek_request(q); ...... /* * Remove the request from the request list. */ if (!(blk_queue_tagged(q) \u0026\u0026 !blk_queue_start_tag(q, req))) blk_start_request(req); ..... cmd = req-\u003especial; ...... /* * Dispatch the command to the low-level driver. */ cmd-\u003escsi_done = scsi_done; rtn = scsi_dispatch_cmd(cmd); ...... } return; ...... } 在这里面是一个 for 无限循环，从 request_queue 中读取 request，然后封装更加底层的指令，给设备控制器下指令，实施真正的 I/O 操作。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:36:7","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节我们讲了如何将块设备 I/O 请求送达到外部设备。对于块设备的 I/O 操作分为两种，一种是直接 I/O，另一种是缓存 I/O。无论是哪种 I/O，最终都会调用 submit_bio 提交块设备 I/O 请求。对于每一种块设备，都有一个 gendisk 表示这个设备，它有一个请求队列，这个队列是一系列的 request 对象。每个 request 对象里面包含多个 BIO 对象，指向 page cache。所谓的写入块设备，I/O 就是将 page cache 里面的数据写入硬盘。对于请求队列来讲，还有两个函数，一个函数叫 make_request_fn 函数，用于将请求放入队列。submit_bio 会调用 generic_make_request，然后调用这个函数。另一个函数往往在设备驱动程序里实现，我们叫 request_fn 函数，它用于从队列里面取出请求来，写入外部设备。 至此，整个写入文件的过程才算完全结束。这真是个复杂的过程，涉及系统调用、内存管理、文件系统和输入输出。这足以说明，操作系统真的是一个非常复杂的体系，环环相扣，需要分层次层层展开来学习。到这里，专栏已经过半了，你应该能发现，很多我之前说“后面会细讲”的东西，现在正在一点一点解释清楚，而文中越来越多出现“前面我们讲过”的字眼，你是否当时学习前面知识的时候，没有在意，导致学习后面的知识产生困惑了呢？没关系，及时倒回去复习，再回过头去看，当初学过的很多知识会变得清晰很多。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:36:8","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"36 | 进程间通信：遇到大项目需要项目组之间的合作才行 前面咱们接项目的时候，主要强调项目之间的隔离性。这是因为，我们刚开始接的都是小项目。随着我们接的项目越来越多，就难免遇到大项目，这就需要多个项目组进行合作才能完成。两个项目组应该通过什么样的方式进行沟通与合作呢？作为老板，你应该如何设计整个流程呢？ ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:37:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"管道模型 好在有这么多成熟的项目管理流程可以参考。最最传统的模型就是软件开发的瀑布模型（Waterfall Model）。所谓的瀑布模型，其实就是将整个软件开发过程分成多个阶段，往往是上一个阶段完全做完，才将输出结果交给下一个阶段。就像下面这张图展示的一样。 这种模型类似进程间通信的管道模型。还记得咱们最初学 Linux 命令的时候，有下面这样一行命令： ps -ef | grep 关键字 | awk '{print $2}' | xargs kill -9 这里面的竖线“|”就是一个管道。它会将前一个命令的输出，作为后一个命令的输入。从管道的这个名称可以看出来，管道是一种单向传输数据的机制，它其实是一段缓存，里面的数据只能从一端写入，从另一端读出。如果想互相通信，我们需要创建两个管道才行。管道分为两种类型，“|” 表示的管道称为匿名管道，意思就是这个类型的管道没有名字，用完了就销毁了。就像上面那个命令里面的一样，竖线代表的管道随着命令的执行自动创建、自动销毁。用户甚至都不知道自己在用管道这种技术，就已经解决了问题。所以这也是面试题里面经常会问的，到时候千万别说这是竖线，而要回答背后的机制，管道。另外一种类型是命名管道。这个类型的管道需要通过 mkfifo 命令显式地创建。 mkfifo hello hello 就是这个管道的名称。管道以文件的形式存在，这也符合 Linux 里面一切皆文件的原则。这个时候，我们 ls 一下，可以看到，这个文件的类型是 p，就是 pipe 的意思。 # ls -l prw-r--r-- 1 root root 0 May 21 23:29 hello 接下来，我们可以往管道里面写入东西。例如，写入一个字符串。 # echo \"hello world\" \u003e hello 这个时候，管道里面的内容没有被读出，这个命令就是停在这里的，这说明当一个项目组要把它的输出交接给另一个项目组做输入，当没有交接完毕的时候，前一个项目组是不能撒手不管的。这个时候，我们就需要重新连接一个终端。在终端中，用下面的命令读取管道里面的内容： # cat \u003c hello hello world 一方面，我们能够看到，管道里面的内容被读取出来，打印到了终端上；另一方面，echo 那个命令正常退出了，也即交接完毕，前一个项目组就完成了使命，可以解散了。我们可以看出，瀑布模型的开发流程效率比较低下，因为团队之间无法频繁地沟通。而且，管道的使用模式，也不适合进程间频繁地交换数据。于是，我们还得想其他的办法，例如我们是不是可以借鉴传统外企的沟通方式——邮件。邮件有一定的格式，例如抬头，正文，附件等，发送邮件可以建立收件人列表，所有在这个列表中的人，都可以反复地在此邮件基础上回复，达到频繁沟通的目的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:37:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"消息队列模型 这种模型类似进程间通信的消息队列模型。和管道将信息一股脑儿地从一个进程，倒给另一个进程不同，消息队列有点儿像邮件，发送数据时，会分成一个一个独立的数据单元，也就是消息体，每个消息体都是固定大小的存储块，在字节流上不连续。这个消息结构的定义我写在下面了。这里面的类型 type 和正文 text 没有强制规定，只要消息的发送方和接收方约定好即可。 struct msg_buffer { long mtype; char mtext[1024]; }; 接下来，我们需要创建一个消息队列，使用 msgget 函数。这个函数需要有一个参数 key，这是消息队列的唯一标识，应该是唯一的。如何保持唯一性呢？这个还是和文件关联。我们可以指定一个文件，ftok 会根据这个文件的 inode，生成一个近乎唯一的 key。只要在这个消息队列的生命周期内，这个文件不要被删除就可以了。只要不删除，无论什么时刻，再调用 ftok，也会得到同样的 key。这种 key 的使用方式在这一章会经常遇到，这是因为它们都属于 System V IPC 进程间通信机制体系中。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003csys/msg.h\u003e int main() { int messagequeueid; key_t key; if((key = ftok(\"/root/messagequeue/messagequeuekey\", 1024)) \u003c 0) { perror(\"ftok error\"); exit(1); } printf(\"Message Queue key: %d.\\n\", key); if ((messagequeueid = msgget(key, IPC_CREAT|0777)) == -1) { perror(\"msgget error\"); exit(1); } printf(\"Message queue id: %d.\\n\", messagequeueid); } 在运行上面这个程序之前，我们先使用命令 touch messagequeuekey，创建一个文件，然后多次执行的结果就会像下面这样： # ./a.out Message Queue key: 92536. Message queue id: 32768. System V IPC 体系有一个统一的命令行工具：ipcmk，ipcs 和 ipcrm 用于创建、查看和删除 IPC 对象。例如，ipcs -q 就能看到上面我们创建的消息队列对象。 # ipcs -q ------ Message Queues -------- key msqid owner perms used-bytes messages 0x00016978 32768 root 777 0 0 接下来，我们来看如何发送信息。发送消息主要调用 msgsnd 函数。第一个参数是 message queue 的 id，第二个参数是消息的结构体，第三个参数是消息的长度，最后一个参数是 flag。这里 IPC_NOWAIT 表示发送的时候不阻塞，直接返回。下面的这段程序，getopt_long、do-while 循环以及 switch，是用来解析命令行参数的。命令行参数的格式定义在 long_options 里面。每一项的第一个成员“id”“type”“message”是参数选项的全称，第二个成员都为 1，表示参数选项后面要跟参数，最后一个成员’i’‘t’‘m’是参数选项的简称。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003csys/msg.h\u003e #include \u003cgetopt.h\u003e #include \u003cstring.h\u003e struct msg_buffer { long mtype; char mtext[1024]; }; int main(int argc, char *argv[]) { int next_option; const char* const short_options = \"i:t:m:\"; const struct option long_options[] = { { \"id\", 1, NULL, 'i'}, { \"type\", 1, NULL, 't'}, { \"message\", 1, NULL, 'm'}, { NULL, 0, NULL, 0 } }; int messagequeueid = -1; struct msg_buffer buffer; buffer.mtype = -1; int len = -1; char * message = NULL; do { next_option = getopt_long (argc, argv, short_options, long_options, NULL); switch (next_option) { case 'i': messagequeueid = atoi(optarg); break; case 't': buffer.mtype = atol(optarg); break; case 'm': message = optarg; len = strlen(message) + 1; if (len \u003e 1024) { perror(\"message too long.\"); exit(1); } memcpy(buffer.mtext, message, len); break; default: break; } }while(next_option != -1); if(messagequeueid != -1 \u0026\u0026 buffer.mtype != -1 \u0026\u0026 len != -1 \u0026\u0026 message != NULL){ if(msgsnd(messagequeueid, \u0026buffer, len, IPC_NOWAIT) == -1){ perror(\"fail to send message.\"); exit(1); } } else { perror(\"arguments error\"); } return 0; } 接下来，我们可以编译并运行这个发送程序。 gcc -o send sendmessage.c ./send -i 32768 -t 123 -m \"hello world\" 接下来，我们再来看如何收消息。收消息主要调用 msgrcv 函数，第一个参数是 message queue 的 id，第二个参数是消息的结构体，第三个参数是可接受的最大长度，第四个参数是消息类型, 最后一个参数是 flag，这里 IPC_NOWAIT 表示接收的时候不阻塞，直接返回。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003csys/msg.h\u003e #include \u003cgetopt.h\u003e #include \u003cstring.h\u003e struct msg_buffer { long mtype; char mtext[1024]; }; int main(int argc, char *argv[]) { int next_option; const char* const short_options = \"i:t:\"; const struct option long_options[] = { { \"id\", 1, NULL, 'i'}, { \"type\", 1, NULL, 't'}, { NULL, 0, NULL, 0 } }; int messagequeueid = -1; struct msg_buffer buffer; long type = -1; do { next_option = getopt_long (argc, argv, short_options, long_options, NULL); switch (next_option) { case 'i': messagequeueid = atoi(optarg); break; case 't': type = atol(optarg); break; default: break; } }while(next_option != -1); if(messagequeueid != -1 \u0026\u0026 type != -1){ if(msgrcv(messagequeueid, \u0026buffer, 1024, type, IPC_NOWAIT) == -1){ perror(\"fail to recv message.\"); exit(1); } printf(\"received message type : %d, text: %s.\", buffer.mtype, buffer.mtext); } else { perror(\"arguments error\"); } return 0; } 接下来，我们可以编译并运行这个发送程序。可以看到，如果有消息，可以正确地读到消息；如果没有，则返回没有消息。 # ./recv -i 32768 -t 123 received message type : 123, text: hello world. # ./recv -i 32768 -t 123 fail to recv message.: No message of desired typ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:37:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"共享内存模型 但是有时候，项目组之间的沟通需要特别紧密，而且要分享一些比较大的数据。如果使用邮件，就发现，一方面邮件的来去不及时；另外一方面，附件大小也有限制，所以，这个时候，我们经常采取的方式就是，把两个项目组在需要合作的期间，拉到一个会议室进行合作开发，这样大家可以直接交流文档呀，架构图呀，直接在白板上画或者直接扔给对方，就可以直接看到。可以看出来，共享会议室这种模型，类似进程间通信的共享内存模型。前面咱们讲内存管理的时候，知道每个进程都有自己独立的虚拟内存空间，不同的进程的虚拟内存空间映射到不同的物理内存中去。这个进程访问 A 地址和另一个进程访问 A 地址，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。但是，咱们是不是可以变通一下，拿出一块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去。共享内存也是 System V IPC 进程间通信机制体系中的，所以从它使用流程可以看到熟悉的面孔。我们可以创建一个共享内存，调用 shmget。在这个体系中，创建一个 IPC 对象都是 xxxget，这里面第一个参数是 key，和 msgget 里面的 key 一样，都是唯一定位一个共享内存对象，也可以通过关联文件的方式实现唯一性。第二个参数是共享内存的大小。第三个参数如果是 IPC_CREAT，同样表示创建一个新的。 int shmget(key_t key, size_t size, int flag); 创建完毕之后，我们可以通过 ipcs 命令查看这个共享内存。 #ipcs ­­--shmems ------ Shared Memory Segments ------ ­­­­­­­­ key shmid owner perms bytes nattch status 0x00000000 19398656 marc 600 1048576 2 dest 接下来，如果一个进程想要访问这一段共享内存，需要将这个内存加载到自己的虚拟地址空间的某个位置，通过 shmat 函数，就是 attach 的意思。其中 addr 就是要指定 attach 到这个地方。但是这个地址的设定难度比较大，除非对于内存布局非常熟悉，否则可能会 attach 到一个非法地址。所以，通常的做法是将 addr 设为 NULL，让内核选一个合适的地址。返回值就是真正被 attach 的地方。 void *shmat(int shm_id, const void *addr, int flag); 如果共享内存使用完毕，可以通过 shmdt 解除绑定，然后通过 shmctl，将 cmd 设置为 IPC_RMID，从而删除这个共享内存对象。 int shmdt(void *addr); int shmctl(int shm_id, int cmd, struct shmid_ds *buf); ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:37:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"信号量 这里你是不是有一个疑问，如果两个进程 attach 同一个共享内存，大家都往里面写东西，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。所以，这里就需要一种保护机制，使得同一个共享的资源，同时只能被一个进程访问。在 System V IPC 进程间通信机制体系中，早就想好了应对办法，就是信号量（Semaphore）。因此，信号量和共享内存往往要配合使用。信号量其实是一个计数器，主要用于实现进程间的互斥与同步，而不是用于存储进程间通信数据。我们可以将信号量初始化为一个数值，来代表某种资源的总体数量。对于信号量来讲，会定义两种原子操作，一个是 P 操作，我们称为申请资源操作。这个操作会申请将信号量的数值减去 N，表示这些数量被他申请使用了，其他人不能用了。另一个是 V 操作，我们称为归还资源操作，这个操作会申请将信号量加上 M，表示这些数量已经还给信号量了，其他人可以使用了。 例如，你有 100 元钱，就可以将信号量设置为 100。其中 A 向你借 80 元，就会调用 P 操作，申请减去 80。如果同时 B 向你借 50 元，但是 B 的 P 操作比 A 晚，那就没有办法，只好等待 A 归还钱的时候，B 的 P 操作才能成功。之后，A 调用 V 操作，申请加上 30 元，也就是还给你 30 元，这个时候信号量有 50 元了，这时候 B 的 P 操作才能成功，才能借走这 50 元。所谓原子操作（Atomic Operation），就是任何一块钱，都只能通过 P 操作借给一个人，不能同时借给两个人。也就是说，当 A 的 P 操作（借 80）和 B 的 P 操作（借 50），几乎同时到达的时候，不能因为大家都看到账户里有 100 就都成功，必须分个先来后到。如果想创建一个信号量，我们可以通过 semget 函数。看，又是 xxxget，第一个参数 key 也是类似的，第二个参数 num_sems 不是指资源的数量，而是表示可以创建多少个信号量，形成一组信号量，也就是说，如果你有多种资源需要管理，可以创建一个信号量组。 int semget(key_t key, int num_sems, int sem_flags); 接下来，我们需要初始化信号量的总的资源数量。通过 semctl 函数，第一个参数 semid 是这个信号量组的 id，第二个参数 semnum 才是在这个信号量组中某个信号量的 id，第三个参数是命令，如果是初始化，则用 SETVAL，第四个参数是一个 union。如果初始化，应该用里面的 val 设置资源总量。 int semctl(int semid, int semnum, int cmd, union semun args); union semun { int val; struct semid_ds *buf; unsigned short int *array; struct seminfo *__buf; }; 无论是 P 操作还是 V 操作，我们统一用 semop 函数。第一个参数还是信号量组的 id，一次可以操作多个信号量。第三个参数 numops 就是有多少个操作，第二个参数将这些操作放在一个数组中。数组的每一项是一个 struct sembuf，里面的第一个成员是这个操作的对象是哪个信号量。第二个成员就是要对这个信号量做多少改变。如果 sem_op \u003c 0，就请求 sem_op 的绝对值的资源。如果相应的资源数可以满足请求，则将该信号量的值减去 sem_op 的绝对值，函数成功返回。当相应的资源数不能满足请求时，就要看 sem_flg 了。如果把 sem_flg 设置为 IPC_NOWAIT，也就是没有资源也不等待，则 semop 函数出错返回 EAGAIN。如果 sem_flg 没有指定 IPC_NOWAIT，则进程挂起，直到当相应的资源数可以满足请求。若 sem_op \u003e 0，表示进程归还相应的资源数，将 sem_op 的值加到信号量的值上。如果有进程正在休眠等待此信号量，则唤醒它们。 int semop(int semid, struct sembuf semoparray[], size_t numops); struct sembuf { short sem_num; // 信号量组中对应的序号，0～sem_nums-1 short sem_op; // 信号量值在一次操作中的改变量 short sem_flg; // IPC_NOWAIT, SEM_UNDO } 信号量和共享内存都比较复杂，两者还要结合起来用，就更加复杂，它们内核的机制就更加复杂。这一节我们先不讲，放到本章的最后一节重点讲解。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:37:4","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"信号 上面讲的进程间通信的方式，都是常规状态下的工作模式，对应到咱们平时的工作交接，收发邮件、联合开发等，其实还有一种异常情况下的工作模式。例如出现线上系统故障，这个时候，什么流程都来不及了，不可能发邮件，也来不及开会，所有的架构师、开发、运维都要被通知紧急出动。所以，7 乘 24 小时不间断执行的系统都需要有告警系统，一旦出事情，就要通知到人，哪怕是半夜，也要电话叫起来，处理故障。对应到操作系统中，就是信号。信号没有特别复杂的数据结构，就是用一个代号一样的数字。Linux 提供了几十种信号，分别代表不同的意义。信号之间依靠它们的值来区分。这就像咱们看警匪片，对于紧急的行动，都是说，“1 号作战任务”开始执行，警察就开始行动了。情况紧急，不能啰里啰嗦了。信号可以在任何时候发送给某一进程，进程需要为这个信号配置信号处理函数。当某个信号发生的时候，就默认执行这个函数就可以了。这就相当于咱们运维一个系统应急手册，当遇到什么情况，做什么事情，都事先准备好，出了事情照着做就可以了。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:37:5","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节，我们整体讲解了一下进程间通信的各种模式。你现在还能记住多少？ 类似瀑布开发模式的管道 类似邮件模式的消息队列 类似会议室联合开发的共享内存加信号量 类似应急预案的信号 当你自己使用的时候，可以根据不同的通信需要，选择不同的模式。 管道，请你记住这是命令行中常用的模式，面试问到的话，不要忘了。消息队列其实很少使用，因为有太多的用户级别的消息队列，功能更强大。共享内存加信号量是常用的模式。这个需要牢记，常见到一些知名的以 C 语言开发的开源软件都会用到它。信号更加常用，机制也比较复杂。我们后面会有单独的一节来解析。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:37:6","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"37 | 信号（上）：项目组A完成了，如何及时通知项目组B？ 上一节最后，我们讲了信号的机制。在某些紧急情况下，我们需要给进程发送一个信号，紧急处理一些事情。这种方式有点儿像咱们运维一个线上系统，为了应对一些突发事件，往往需要制定应急预案。就像下面的列表中一样。一旦发生了突发事件，马上能够找到负责人，根据处理步骤进行紧急响应，并且在限定的事件内搞定。 我们现在就按照应急预案的设计思路，来看一看 Linux 信号系统的机制。首先，第一件要做的事情就是，整个团队要想一下，线上到底能够产生哪些异常情况，越全越好。于是，我们就有了上面这个很长很长的列表。在 Linux 操作系统中，为了响应各种各样的事件，也是定义了非常多的信号。我们可以通过 kill -l 命令，查看所有的信号。 # kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX 这些信号都是什么作用呢？我们可以通过 man 7 signal 命令查看，里面会有一个列表。 Signal Value Action Comment ────────────────────────────────────────────────────────────────────── SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard SIGQUIT 3 Core Quit from keyboard SIGILL 4 Core Illegal Instruction SIGABRT 6 Core Abort signal from abort(3) SIGFPE 8 Core Floating point exception SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers SIGALRM 14 Term Timer signal from alarm(2) SIGTERM 15 Term Termination signal SIGUSR1 30,10,16 Term User-defined signal 1 SIGUSR2 31,12,17 Term User-defined signal 2 …… 就像应急预案里面给出的一样，每个信号都有一个唯一的 ID，还有遇到这个信号的时候的默认操作。一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。 1.执行默认操作。Linux 对每种信号都规定了默认操作，例如，上面列表中的 Term，就是终止进程的意思。Core 的意思是 Core Dump，也即终止进程后，通过 Core Dump 将当前进程的运行状态保存在文件里面，方便程序员事后进行分析问题在哪里。2.捕捉信号。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。3.忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 SIGKILL 和 SEGSTOP，它们用于在任何时候中断或结束某一进程。 接下来，我们来看一下信号处理最常见的流程。这个过程主要是分成两步，第一步是注册信号处理函数。第二步是发送信号。这一节我们主要看第一步。如果我们不想让某个信号执行默认操作，一种方法就是对特定的信号注册相应的信号处理函数，设置信号处理方式的是 signal 函数。 typedef void (*sighandler_t)(int); sighandler_t signal(int signum, sighandler_t handler); 这其实就是定义一个方法，并且将这个方法和某个信号关联起来。当这个进程遇到这个信号的时候，就执行这个方法。如果我们在 Linux 下面执行 man signal 的话，会发现 Linux 不建议我们直接用这个方法，而是改用 sigaction。定义如下： int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact); 这两者的区别在哪里呢？其实它还是将信号和一个动作进行关联，只不过这个动作由一个结构 struct sigaction 表示了。 struct sigaction { __sighandler_t sa_handler; unsigned long sa_flags; __sigrestore_t sa_restorer; sigset_t sa_mask; /* mask last for extensibility */ }; 和 signal 类似的是，这里面还是有 __sighandler_t。但是，其他成员变量可以让你更加细致地控制信号处理的行为。而 signal 函数没有给你机会设置这些。这里需要注意的是，signal 不是系统调用，而是 glibc 封装的一个函数。这样就像 man signal 里面写的一样，不同的实现方式，设置的参数会不同，会导致行为的不同。例如，我们在 glibc 里面会看到了这样一个实现： # define signal __sysv_signal __sighandler_t __sysv_signal (int sig, __sighandler_t handler) { struct sigaction act, oact; ...... act.sa_handler = handler; __sigemptyset (\u0026act.sa_mask); act.sa_flags = SA_ONESHOT | SA_NOMASK | SA_INTERRUPT; act.sa_flags \u0026= ~SA_RESTART; if (__sigaction (sig, \u0026act, \u0026oact) \u003c 0) return SIG_ERR; return oact.sa_handler; } weak_alias (__sysv_signal, sysv_signal) 在这里面，sa_flags 进行了默认的设置。SA_ONESHOT 是什么意思呢？意思就是，这里设置的信号处理函数，仅仅起作用一次。用完了一次后，就设置回默认行为。这其实并不是我们想看到的。毕竟我们一旦安装了一个信号处理函数，肯定希望它一直起作用，直到我显式地关闭它。另外一个设置就是 SA_NOMASK。我们通过 __sigemptyset，将 sa_mask 设置为空。这样的设置表示在这个信号处理函数执行过程中，如果再有其他信号，哪怕相同的信号到来的时候，这个信号处理函数会被中断。如果一个信号处理函数真的被其他信号中断，其实问题也不大，因为当处理完了其他的信号处理函数后，还会回来接着处理这个信号处理函数的，但是对于相同的信号就有点尴尬了，这就需要这个信号处理函数写得比较有技巧了。 例如，对于这个信号的处理过程中，要操作某个数据结构，因为是相同的信号，很可能操作的是同一个实例，这样的话，同步、死锁这些都要想好。其实一般的思路应该是，当某一个信号的信号处理函数运行的时候，我们暂时屏蔽这个信号。后面我们还会仔细分析屏蔽这个动作，屏蔽并不意味着信号一定丢失，而是暂存，这样能够做到信号处理函数对于相同的信号，处理完","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:38:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节讲了如何通过 API 注册一个信号处理函数，整个过程如下图所示。 在用户程序里面，有两个函数可以调用，一个是 signal，一个是 sigaction，推荐使用 sigaction。用户程序调用的是 Glibc 里面的函数，signal 调用的是 __sysv_signal，里面默认设置了一些参数，使得 signal 的功能受到了限制，sigaction 调用的是 __sigaction，参数用户可以任意设定。无论是 __sysv_signal 还是 __sigaction，调用的都是统一的一个系统调用 rt_sigaction。在内核中，rt_sigaction 调用的是 do_sigaction 设置信号处理函数。在每一个进程的 task_struct 里面，都有一个 sighand 指向 struct sighand_struct，里面是一个数组，下标是信号，里面的内容是信号处理函数。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:38:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"38 | 信号（下）：项目组A完成了，如何及时通知项目组B？ 信号处理最常见的流程主要是两步，第一步是注册信号处理函数，第二步是发送信号和处理信号。上一节，我们讲了注册信号处理函数，那一般什么情况下会产生信号呢？我们这一节就来看一看。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:39:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"信号的发送 有时候，我们在终端输入某些组合键的时候，会给进程发送信号，例如，Ctrl+C 产生 SIGINT 信号，Ctrl+Z 产生 SIGTSTP 信号。有的时候，硬件异常也会产生信号。比如，执行了除以 0 的指令，CPU 就会产生异常，然后把 SIGFPE 信号发送给进程。再如，进程访问了非法内存，内存管理模块就会产生异常，然后把信号 SIGSEGV 发送给进程。这里同样是硬件产生的，对于中断和信号还是要加以区别。咱们前面讲过，中断要注册中断处理函数，但是中断处理函数是在内核驱动里面的，信号也要注册信号处理函数，信号处理函数是在用户态进程里面的。对于硬件触发的，无论是中断，还是信号，肯定是先到内核的，然后内核对于中断和信号处理方式不同。一个是完全在内核里面处理完毕，一个是将信号放在对应的进程 task_struct 里信号相关的数据结构里面，然后等待进程在用户态去处理。当然有些严重的信号，内核会把进程干掉。但是，这也能看出来，中断和信号的严重程度不一样，信号影响的往往是某一个进程，处理慢了，甚至错了，也不过这个进程被干掉，而中断影响的是整个系统。一旦中断处理中有了 bug，可能整个 Linux 都挂了。有时候，内核在某些情况下，也会给进程发送信号。例如，向读端已关闭的管道写数据时产生 SIGPIPE 信号，当子进程退出时，我们要给父进程发送 SIG_CHLD 信号等。 最直接的发送信号的方法就是，通过命令 kill 来发送信号了。例如，我们都知道的 kill -9 pid 可以发送信号给一个进程，杀死它。另外，我们还可以通过 kill 或者 sigqueue 系统调用，发送信号给某个进程，也可以通过 tkill 或者 tgkill 发送信号给某个线程。虽然方式多种多样，但是最终都是调用了 do_send_sig_info 函数，将信号放在相应的 task_struct 的信号数据结构中。 kill-\u003ekill_something_info-\u003ekill_pid_info-\u003egroup_send_sig_info-\u003edo_send_sig_info tkill-\u003edo_tkill-\u003edo_send_specific-\u003edo_send_sig_info tgkill-\u003edo_tkill-\u003edo_send_specific-\u003edo_send_sig_info rt_sigqueueinfo-\u003edo_rt_sigqueueinfo-\u003ekill_proc_info-\u003ekill_pid_info-\u003egroup_send_sig_info-\u003edo_send_sig_info do_send_sig_info 会调用 send_signal，进而调用 __send_signal。 SYSCALL_DEFINE2(kill, pid_t, pid, int, sig) { struct siginfo info; info.si_signo = sig; info.si_errno = 0; info.si_code = SI_USER; info.si_pid = task_tgid_vnr(current); info.si_uid = from_kuid_munged(current_user_ns(), current_uid()); return kill_something_info(sig, \u0026info, pid); } static int __send_signal(int sig, struct siginfo *info, struct task_struct *t, int group, int from_ancestor_ns) { struct sigpending *pending; struct sigqueue *q; int override_rlimit; int ret = 0, result; ...... pending = group ? \u0026t-\u003esignal-\u003eshared_pending : \u0026t-\u003epending; ...... if (legacy_queue(pending, sig)) goto ret; if (sig \u003c SIGRTMIN) override_rlimit = (is_si_special(info) || info-\u003esi_code \u003e= 0); else override_rlimit = 0; q = __sigqueue_alloc(sig, t, GFP_ATOMIC | __GFP_NOTRACK_FALSE_POSITIVE, override_rlimit); if (q) { list_add_tail(\u0026q-\u003elist, \u0026pending-\u003elist); switch ((unsigned long) info) { case (unsigned long) SEND_SIG_NOINFO: q-\u003einfo.si_signo = sig; q-\u003einfo.si_errno = 0; q-\u003einfo.si_code = SI_USER; q-\u003einfo.si_pid = task_tgid_nr_ns(current, task_active_pid_ns(t)); q-\u003einfo.si_uid = from_kuid_munged(current_user_ns(), current_uid()); break; case (unsigned long) SEND_SIG_PRIV: q-\u003einfo.si_signo = sig; q-\u003einfo.si_errno = 0; q-\u003einfo.si_code = SI_KERNEL; q-\u003einfo.si_pid = 0; q-\u003einfo.si_uid = 0; break; default: copy_siginfo(\u0026q-\u003einfo, info); if (from_ancestor_ns) q-\u003einfo.si_pid = 0; break; } userns_fixup_signal_uid(\u0026q-\u003einfo, t); } ...... out_set: signalfd_notify(t, sig); sigaddset(\u0026pending-\u003esignal, sig); complete_signal(sig, t, group); ret: return ret; } 在这里，我们看到，在学习进程数据结构中 task_struct 里面的 sigpending。在上面的代码里面，我们先是要决定应该用哪个 sigpending。这就要看我们发送的信号，是给进程的还是线程的。如果是 kill 发送的，也就是发送给整个进程的，就应该发送给 t-\u003esignal-\u003eshared_pending。这里面是整个进程所有线程共享的信号；如果是 tkill 发送的，也就是发给某个线程的，就应该发给 t-\u003epending。这里面是这个线程的 task_struct 独享的。struct sigpending 里面有两个成员，一个是一个集合 sigset_t，表示都收到了哪些信号，还有一个链表，也表示收到了哪些信号。它的结构如下： struct sigpending { struct list_head list; sigset_t signal; }; 如果都表示收到了信号，这两者有什么区别呢？我们接着往下看 __send_signal 里面的代码。接下来，我们要调用 legacy_queue。如果满足条件，那就直接退出。那 legacy_queue 里面判断的是什么条件呢？我们来看它的代码。 static inline int legacy_queue(struct sigpending *signals, int sig) { return (sig \u003c SIGRTMIN) \u0026\u0026 sigismember(\u0026signals-\u003esignal, sig); } #define SIGRTMIN 32 #define SIGRTMAX _NSIG #define _NSIG 64 当信号小于 SIGRTMIN，也即 32 的时候，如果我们发现这个信号已经在集合里面了，就直接退出了。这样会造成什么现象呢？就是信号的丢失。例如，我们发送给进程 100 个 SIGUSR1（对应的信号为 10），那最终能够被我们的信号处理函数处理的信号有多少呢？这就不好说了，比如总共 5 个 SIGUSR1，分别是 A、B、C、D、E。如果这五个信号来得太密。A 来了，但是信号处理函数还没来得及处理，B、C、D、E 就都来了。根据上面的逻辑，因为 A 已经将 SIGUSR1 放在 sigset_t 集合中了，因而后面四个都要丢失。 如果是另一种情况，A 来了已经被信号处理函数处理了，内核在调用信号处理函数之前，我们会将集合中的标志位清除，这个时候 B 再来，B 还是会进入集合，还是会被处理，也就不会丢。这样信号能够处理多少，和信号处理函数什么时候被调用，信号多大频率被发送，都有关系，而且从后面的分析，我们可以知道，信号处理函数的调用时间也是不确定的。看小于 32 的信号如此不靠谱，我们就称它为不可靠信号。 如果大于 32 的信号是什么情况呢？我们接着看。接下来，__sigqueue_alloc 会分配一个 struct sigqueue 对象，然后通过 list_add_tail 挂在 struct sigpending 里面的链表上。这样就靠谱多了是不是","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:39:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"信号的处理 好了，信号已经发送到位了，什么时候真正处理它呢？就是在从系统调用或者中断返回的时候，咱们讲调度的时候讲过，无论是从系统调用返回还是从中断返回，都会调用 exit_to_usermode_loop，只不过我们上次主要关注了 _TIF_NEED_RESCHED 这个标识位，这次我们重点关注 _TIF_SIGPENDING 标识位。 static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags) { while (true) { ...... if (cached_flags \u0026 _TIF_NEED_RESCHED) schedule(); ...... /* deal with pending signal delivery */ if (cached_flags \u0026 _TIF_SIGPENDING) do_signal(regs); ...... if (!(cached_flags \u0026 EXIT_TO_USERMODE_LOOP_FLAGS)) break; } } 如果在前一个环节中，已经设置了 _TIF_SIGPENDING，我们就调用 do_signal 进行处理。 void do_signal(struct pt_regs *regs) { struct ksignal ksig; if (get_signal(\u0026ksig)) { /* Whee! Actually deliver the signal. */ handle_signal(\u0026ksig, regs); return; } /* Did we come from a system call? */ if (syscall_get_nr(current, regs) \u003e= 0) { /* Restart the system call - no handlers present */ switch (syscall_get_error(current, regs)) { case -ERESTARTNOHAND: case -ERESTARTSYS: case -ERESTARTNOINTR: regs-\u003eax = regs-\u003eorig_ax; regs-\u003eip -= 2; break; case -ERESTART_RESTARTBLOCK: regs-\u003eax = get_nr_restart_syscall(regs); regs-\u003eip -= 2; break; } } restore_saved_sigmask(); } do_signal 会调用 handle_signal。按说，信号处理就是调用用户提供的信号处理函数，但是这事儿没有看起来这么简单，因为信号处理函数是在用户态的。咱们又要来回忆系统调用的过程了。这个进程当时在用户态执行到某一行 Line A，调用了一个系统调用，在进入内核的那一刻，在内核 pt_regs 里面保存了用户态执行到了 Line A。现在我们从系统调用返回用户态了，按说应该从 pt_regs 拿出 Line A，然后接着 Line A 执行下去，但是为了响应信号，我们不能回到用户态的时候返回 Line A 了，而是应该返回信号处理函数的起始地址。 static void handle_signal(struct ksignal *ksig, struct pt_regs *regs) { bool stepping, failed; ...... /* Are we from a system call? */ if (syscall_get_nr(current, regs) \u003e= 0) { /* If so, check system call restarting.. */ switch (syscall_get_error(current, regs)) { case -ERESTART_RESTARTBLOCK: case -ERESTARTNOHAND: regs-\u003eax = -EINTR; break; case -ERESTARTSYS: if (!(ksig-\u003eka.sa.sa_flags \u0026 SA_RESTART)) { regs-\u003eax = -EINTR; break; } /* fallthrough */ case -ERESTARTNOINTR: regs-\u003eax = regs-\u003eorig_ax; regs-\u003eip -= 2; break; } } ...... failed = (setup_rt_frame(ksig, regs) \u003c 0); ...... signal_setup_done(failed, ksig, stepping); } 这个时候，我们就需要干预和自己来定制 pt_regs 了。这个时候，我们要看，是否从系统调用中返回。如果是从系统调用返回的话，还要区分我们是从系统调用中正常返回，还是在一个非运行状态的系统调用中，因为会被信号中断而返回。我们这里解析一个最复杂的场景。还记得咱们解析进程调度的时候，我们举的一个例子，就是从一个 tap 网卡中读取数据。当时我们主要关注 schedule 那一行，也即如果当发现没有数据的时候，就调用 schedule，自己进入等待状态，然后将 CPU 让给其他进程。具体的代码如下： static ssize_t tap_do_read(struct tap_queue *q, struct iov_iter *to, int noblock, struct sk_buff *skb) { ...... while (1) { if (!noblock) prepare_to_wait(sk_sleep(\u0026q-\u003esk), \u0026wait, TASK_INTERRUPTIBLE); /* Read frames from the queue */ skb = skb_array_consume(\u0026q-\u003eskb_array); if (skb) break; if (noblock) { ret = -EAGAIN; break; } if (signal_pending(current)) { ret = -ERESTARTSYS; break; } /* Nothing to read, let's sleep */ schedule(); } ...... } 这里我们关注和信号相关的部分。这其实是一个信号中断系统调用的典型逻辑。首先，我们把当前进程或者线程的状态设置为 TASK_INTERRUPTIBLE，这样才能使这个系统调用可以被中断。其次，可以被中断的系统调用往往是比较慢的调用，并且会因为数据不就绪而通过 schedule 让出 CPU 进入等待状态。在发送信号的时候，我们除了设置这个进程和线程的 _TIF_SIGPENDING 标识位之外，还试图唤醒这个进程或者线程，也就是将它从等待状态中设置为 TASK_RUNNING。当这个进程或者线程再次运行的时候，我们根据进程调度第一定律，从 schedule 函数中返回，然后再次进入 while 循环。由于这个进程或者线程是由信号唤醒的，而不是因为数据来了而唤醒的，因而是读不到数据的，但是在 signal_pending 函数中，我们检测到了 _TIF_SIGPENDING 标识位，这说明系统调用没有真的做完，于是返回一个错误 ERESTARTSYS，然后带着这个错误从系统调用返回。然后，我们到了 exit_to_usermode_loop-\u003edo_signal-\u003ehandle_signal。在这里面，当发现出现错误 ERESTARTSYS 的时候，我们就知道这是从一个没有调用完的系统调用返回的，设置系统调用错误码 EINTR。接下来，我们就开始折腾 pt_regs 了，主要通过调用 setup_rt_frame-\u003e__setup_rt_frame。 static int __setup_rt_frame(int sig, struct ksignal *ksig, sigset_t *set, struct pt_regs *regs) { struct rt_sigframe __user *frame; void __user *fp = NULL; int err = 0; frame = get_sigframe(\u0026ksig-\u003eka, regs, sizeof(struct rt_sigframe), \u0026fp); ...... put_user_try { ...... /* Set up to return from userspace. If provided, use a stub already in userspace. */ /* x86-64 should always use SA_RESTORER. */ if (ksig-\u003eka.sa.sa_flags \u0026 SA_RESTORER) { put_user_ex(ksig-\u003eka.sa.sa_restorer, \u0026frame-\u003epretcode); } } put_user_catch(err); err |= setup_sigcontext(\u0026frame-\u003euc.uc_mcontext, fp, regs, set-\u003esig[0]); err |= __copy_to_user(\u0026frame-\u003euc.uc_sigmask, set, s","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:39:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 信号的发送与处理是一个复杂的过程，这里来总结一下。 假设我们有一个进程 A，main 函数里面调用系统调用进入内核。按照系统调用的原理，会将用户态栈的信息保存在 pt_regs 里面，也即记住原来用户态是运行到了 line A 的地方。在内核中执行系统调用读取数据。当发现没有什么数据可读取的时候，只好进入睡眠状态，并且调用 schedule 让出 CPU，这是进程调度第一定律。将进程状态设置为 TASK_INTERRUPTIBLE，可中断的睡眠状态，也即如果有信号来的话，是可以唤醒它的。其他的进程或者 shell 发送一个信号，有四个函数可以调用 kill、tkill、tgkill、rt_sigqueueinfo。四个发送信号的函数，在内核中最终都是调用 do_send_sig_info。do_send_sig_info 调用 send_signal 给进程 A 发送一个信号，其实就是找到进程 A 的 task_struct，或者加入信号集合，为不可靠信号，或者加入信号链表，为可靠信号。do_send_sig_info 调用 signal_wake_up 唤醒进程 A。进程 A 重新进入运行状态 TASK_RUNNING，根据进程调度第一定律，一定会接着 schedule 运行。进程 A 被唤醒后，检查是否有信号到来，如果没有，重新循环到一开始，尝试再次读取数据，如果还是没有数据，再次进入 TASK_INTERRUPTIBLE，即可中断的睡眠状态。当发现有信号到来的时候，就返回当前正在执行的系统调用，并返回一个错误表示系统调用被中断了。系统调用返回的时候，会调用 exit_to_usermode_loop。这是一个处理信号的时机。调用 do_signal 开始处理信号。根据信号，得到信号处理函数 sa_handler，然后修改 pt_regs 中的用户态栈的信息，让 pt_regs 指向 sa_handler。同时修改用户态的栈，插入一个栈帧 sa_restorer，里面保存了原来的指向 line A 的 pt_regs，并且设置让 sa_handler 运行完毕后，跳到 sa_restorer 运行。返回用户态，由于 pt_regs 已经设置为 sa_handler，则返回用户态执行 sa_handler。sa_handler 执行完毕后，信号处理函数就执行完了，接着根据第 15 步对于用户态栈帧的修改，会跳到 sa_restorer 运行。sa_restorer 会调用系统调用 rt_sigreturn 再次进入内核。在内核中，rt_sigreturn 恢复原来的 pt_regs，重新指向 line A。从 rt_sigreturn 返回用户态，还是调用 exit_to_usermode_loop。这次因为 pt_regs 已经指向 line A 了，于是就到了进程 A 中，接着系统调用之后运行，当然这个系统调用返回的是它被中断了，没有执行完的错误。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:39:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"39 | 管道：项目组A完成了，如何交接给项目组B？ 在这一章的第一节里，我们大致讲了管道的使用方式以及相应的命令行。这一节，我们就具体来看一下管道是如何实现的。我们先来看，我们常用的匿名管道（Anonymous Pipes），也即将多个命令串起来的竖线，背后的原理到底是什么。上次我们说，它是基于管道的，那管道如何创建呢？管道的创建，需要通过下面这个系统调用。 int pipe(int fd[2]) 在这里，我们创建了一个管道 pipe，返回了两个文件描述符，这表示管道的两端，一个是管道的读取端描述符 fd[0]，另一个是管道的写入端描述符 fd[1]。 我们来看在内核里面是如何实现的。 SYSCALL_DEFINE1(pipe, int __user *, fildes) { return sys_pipe2(fildes, 0); } SYSCALL_DEFINE2(pipe2, int __user *, fildes, int, flags) { struct file *files[2]; int fd[2]; int error; error = __do_pipe_flags(fd, files, flags); if (!error) { if (unlikely(copy_to_user(fildes, fd, sizeof(fd)))) { ...... error = -EFAULT; } else { fd_install(fd[0], files[0]); fd_install(fd[1], files[1]); } } return error; } 在内核中，主要的逻辑在 pipe2 系统调用中。这里面要创建一个数组 files，用来存放管道的两端的打开文件，另一个数组 fd 存放管道的两端的文件描述符。如果调用 __do_pipe_flags 没有错误，那就调用 fd_install，将两个 fd 和两个 struct file 关联起来。这一点和打开一个文件的过程很像了。我们来看 __do_pipe_flags。这里面调用了 create_pipe_files，然后生成了两个 fd。从这里可以看出，fd[0]是用于读的，fd[1]是用于写的。 static int __do_pipe_flags(int *fd, struct file **files, int flags) { int error; int fdw, fdr; ...... error = create_pipe_files(files, flags); ...... error = get_unused_fd_flags(flags); ...... fdr = error; error = get_unused_fd_flags(flags); ...... fdw = error; fd[0] = fdr; fd[1] = fdw; return 0; ...... } 创建一个管道，大部分的逻辑其实都是在 create_pipe_files 函数里面实现的。这一章第一节的时候，我们说过，命名管道是创建在文件系统上的。从这里我们可以看出，匿名管道，也是创建在文件系统上的，只不过是一种特殊的文件系统，创建一个特殊的文件，对应一个特殊的 inode，就是这里面的 get_pipe_inode。 int create_pipe_files(struct file **res, int flags) { int err; struct inode *inode = get_pipe_inode(); struct file *f; struct path path; ...... path.dentry = d_alloc_pseudo(pipe_mnt-\u003emnt_sb, \u0026empty_name); ...... path.mnt = mntget(pipe_mnt); d_instantiate(path.dentry, inode); f = alloc_file(\u0026path, FMODE_WRITE, \u0026pipefifo_fops); ...... f-\u003ef_flags = O_WRONLY | (flags \u0026 (O_NONBLOCK | O_DIRECT)); f-\u003eprivate_data = inode-\u003ei_pipe; res[0] = alloc_file(\u0026path, FMODE_READ, \u0026pipefifo_fops); ...... path_get(\u0026path); res[0]-\u003eprivate_data = inode-\u003ei_pipe; res[0]-\u003ef_flags = O_RDONLY | (flags \u0026 O_NONBLOCK); res[1] = f; return 0; ...... } 从 get_pipe_inode 的实现，我们可以看出，匿名管道来自一个特殊的文件系统 pipefs。这个文件系统被挂载后，我们就得到了 struct vfsmount *pipe_mnt。然后挂载的文件系统的 superblock 就变成了：pipe_mnt-\u003emnt_sb。如果你对文件系统的操作还不熟悉，要返回去复习一下文件系统那一章啊。 static struct file_system_type pipe_fs_type = { .name = \"pipefs\", .mount = pipefs_mount, .kill_sb = kill_anon_super, }; static int __init init_pipe_fs(void) { int err = register_filesystem(\u0026pipe_fs_type); if (!err) { pipe_mnt = kern_mount(\u0026pipe_fs_type); } ...... } static struct inode * get_pipe_inode(void) { struct inode *inode = new_inode_pseudo(pipe_mnt-\u003emnt_sb); struct pipe_inode_info *pipe; ...... inode-\u003ei_ino = get_next_ino(); pipe = alloc_pipe_info(); ...... inode-\u003ei_pipe = pipe; pipe-\u003efiles = 2; pipe-\u003ereaders = pipe-\u003ewriters = 1; inode-\u003ei_fop = \u0026pipefifo_fops; inode-\u003ei_state = I_DIRTY; inode-\u003ei_mode = S_IFIFO | S_IRUSR | S_IWUSR; inode-\u003ei_uid = current_fsuid(); inode-\u003ei_gid = current_fsgid(); inode-\u003ei_atime = inode-\u003ei_mtime = inode-\u003ei_ctime = current_time(inode); return inode; ...... } 我们从 new_inode_pseudo 函数创建一个 inode。这里面开始填写 Inode 的成员，这里和文件系统的很像。这里值得注意的是 struct pipe_inode_info，这个结构里面有个成员是 struct pipe_buffer *bufs。我们可以知道，所谓的匿名管道，其实就是内核里面的一串缓存。另外一个需要注意的是 pipefifo_fops，将来我们对于文件描述符的操作，在内核里面都是对应这里面的操作。 const struct file_operations pipefifo_fops = { .open = fifo_open, .llseek = no_llseek, .read_iter = pipe_read, .write_iter = pipe_write, .poll = pipe_poll, .unlocked_ioctl = pipe_ioctl, .release = pipe_release, .fasync = pipe_fasync, }; 我们回到 create_pipe_files 函数，创建完了 inode，还需创建一个 dentry 和他对应。dentry 和 inode 对应好了，我们就要开始创建 struct file 对象了。先创建用于写入的，对应的操作为 pipefifo_fops；再创建读取的，对应的操作也为 pipefifo_fops。然后把 private_data 设置为 pipe_inode_info。这样从 struct file 这个层级上，就能直接操作底层的读写操作。至此，一个匿名管道就创建成功了。如果对于 fd[1]写入，调用的是 pipe_write，向 pipe_buffer 里面写入数据；如果对于 fd[0]的读入，调用的是 pipe_read，也就是从 pipe_buffer 里面读取数据。但是这个时候，两个文件描述符都是在一个进程里面的，并没有起到进程间通信的作用，怎么样才能使得管道是跨两个进程的呢？还记得创建进程调用的 fork 吗？在这里面，创建的子进程会复制父进程的 struct files_struct，在这里面 fd 的数组会复制一份，但是 fd 指向的 struct file 对于同一个文件还是只","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:40:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 无论是匿名管道，还是命名管道，在内核都是一个文件。只要是文件就要有一个 inode。这里我们又用到了特殊 inode、字符设备、块设备，其实都是这种特殊的 inode。在这种特殊的 inode 里面，file_operations 指向管道特殊的 pipefifo_fops，这个 inode 对应内存里面的缓存。当我们用文件的 open 函数打开这个管道设备文件的时候，会调用 pipefifo_fops 里面的方法创建 struct file 结构，他的 inode 指向特殊的 inode，也对应内存里面的缓存，file_operations 也指向管道特殊的 pipefifo_fops。写入一个 pipe 就是从 struct file 结构找到缓存写入，读取一个 pipe 就是从 struct file 结构找到缓存读出。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:40:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"40 | IPC（上）：不同项目组之间抢资源，如何协调？ 我们前面讲了，如果项目组之间需要紧密合作，那就需要共享内存，这样就像把两个项目组放在一个会议室一起沟通，会非常高效。这一节，我们就来详细讲讲这个进程之间共享内存的机制。有了这个机制，两个进程可以像访问自己内存中的变量一样，访问共享内存的变量。但是同时问题也来了，当两个进程共享内存了，就会存在同时读写的问题，就需要对于共享的内存进行保护，就需要信号量这样的同步协调机制。这些也都是我们这节需要探讨的问题。下面我们就一一来看。共享内存和信号量也是 System V 系列的进程间通信机制，所以很多地方和我们讲过的消息队列有点儿像。为了将共享内存和信号量结合起来使用，我这里定义了一个 share.h 头文件，里面放了一些共享内存和信号量在每个进程都需要的函数。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003csys/ipc.h\u003e #include \u003csys/shm.h\u003e #include \u003csys/types.h\u003e #include \u003csys/sem.h\u003e #include \u003cstring.h\u003e #define MAX_NUM 128 struct shm_data { int data[MAX_NUM]; int datalength; }; union semun { int val; struct semid_ds *buf; unsigned short int *array; struct seminfo *__buf; }; int get_shmid(){ int shmid; key_t key; if((key = ftok(\"/root/sharememory/sharememorykey\", 1024)) \u003c 0){ perror(\"ftok error\"); return -1; } shmid = shmget(key, sizeof(struct shm_data), IPC_CREAT|0777); return shmid; } int get_semaphoreid(){ int semid; key_t key; if((key = ftok(\"/root/sharememory/semaphorekey\", 1024)) \u003c 0){ perror(\"ftok error\"); return -1; } semid = semget(key, 1, IPC_CREAT|0777); return semid; } int semaphore_init (int semid) { union semun argument; unsigned short values[1]; values[0] = 1; argument.array = values; return semctl (semid, 0, SETALL, argument); } int semaphore_p (int semid) { struct sembuf operations[1]; operations[0].sem_num = 0; operations[0].sem_op = -1; operations[0].sem_flg = SEM_UNDO; return semop (semid, operations, 1); } int semaphore_v (int semid) { struct sembuf operations[1]; operations[0].sem_num = 0; operations[0].sem_op = 1; operations[0].sem_flg = SEM_UNDO; return semop (semid, operations, 1); } ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:41:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"共享内存 我们先来看里面对于共享内存的操作。首先，创建之前，我们要有一个 key 来唯一标识这个共享内存。这个 key 可以根据文件系统上的一个文件的 inode 随机生成。然后，我们需要创建一个共享内存，就像创建一个消息队列差不多，都是使用 xxxget 来创建。其中，创建共享内存使用的是下面这个函数： int shmget(key_t key, size_t size, int shmflag); 其中，key 就是前面生成的那个 key，shmflag 如果为 IPC_CREAT，就表示新创建，还可以指定读写权限 0777。对于共享内存，需要指定一个大小 size，这个一般要申请多大呢？一个最佳实践是，我们将多个进程需要共享的数据放在一个 struct 里面，然后这里的 size 就应该是这个 struct 的大小。这样每一个进程得到这块内存后，只要强制将类型转换为这个 struct 类型，就能够访问里面的共享数据了。在这里，我们定义了一个 struct shm_data 结构。这里面有两个成员，一个是一个整型的数组，一个是数组中元素的个数。生成了共享内存以后，接下来就是将这个共享内存映射到进程的虚拟地址空间中。我们使用下面这个函数来进行操作。 void *shmat(int shm_id, const void *addr, int shmflg); 这里面的 shm_id，就是上面创建的共享内存的 id，addr 就是指定映射在某个地方。如果不指定，则内核会自动选择一个地址，作为返回值返回。得到了返回地址以后，我们需要将指针强制类型转换为 struct shm_data 结构，就可以使用这个指针设置 data 和 datalength 了。当共享内存使用完毕，我们可以通过 shmdt 解除它到虚拟内存的映射。 int shmdt(const void *shmaddr)； ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:41:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"信号量 看完了共享内存，接下来我们再来看信号量。信号量以集合的形式存在的。首先，创建之前，我们同样需要有一个 key，来唯一标识这个信号量集合。这个 key 同样可以根据文件系统上的一个文件的 inode 随机生成。然后，我们需要创建一个信号量集合，同样也是使用 xxxget 来创建，其中创建信号量集合使用的是下面这个函数。 int semget(key_t key, int nsems, int semflg); 这里面的 key，就是前面生成的那个 key，shmflag 如果为 IPC_CREAT，就表示新创建，还可以指定读写权限 0777。这里，nsems 表示这个信号量集合里面有几个信号量，最简单的情况下，我们设置为 1。信号量往往代表某种资源的数量，如果用信号量做互斥，那往往将信号量设置为 1。这就是上面代码中 semaphore_init 函数的作用，这里面调用 semctl 函数，将这个信号量集合的中的第 0 个信号量，也即唯一的这个信号量设置为 1。对于信号量，往往要定义两种操作，P 操作和 V 操作。对应上面代码中 semaphore_p 函数和 semaphore_v 函数，semaphore_p 会调用 semop 函数将信号量的值减一，表示申请占用一个资源，当发现当前没有资源的时候，进入等待。semaphore_v 会调用 semop 函数将信号量的值加一，表示释放一个资源，释放之后，就允许等待中的其他进程占用这个资源。我们可以用这个信号量，来保护共享内存中的 struct shm_data，使得同时只有一个进程可以操作这个结构。你是否记得咱们讲线程同步机制的时候，构建了一个老板分配活的场景。这里我们同样构建一个场景，分为 producer.c 和 consumer.c，其中 producer 也即生产者，负责往 struct shm_data 塞入数据，而 consumer.c 负责处理 struct shm_data 中的数据。下面我们来看 producer.c 的代码。 #include \"share.h\" int main() { void *shm = NULL; struct shm_data *shared = NULL; int shmid = get_shmid(); int semid = get_semaphoreid(); int i; shm = shmat(shmid, (void*)0, 0); if(shm == (void*)-1){ exit(0); } shared = (struct shm_data*)shm; memset(shared, 0, sizeof(struct shm_data)); semaphore_init(semid); while(1){ semaphore_p(semid); if(shared-\u003edatalength \u003e 0){ semaphore_v(semid); sleep(1); } else { printf(\"how many integers to caculate : \"); scanf(\"%d\",\u0026shared-\u003edatalength); if(shared-\u003edatalength \u003e MAX_NUM){ perror(\"too many integers.\"); shared-\u003edatalength = 0; semaphore_v(semid); exit(1); } for(i=0;i\u003cshared-\u003edatalength;i++){ printf(\"Input the %d integer : \", i); scanf(\"%d\",\u0026shared-\u003edata[i]); } semaphore_v(semid); } } } 在这里面，get_shmid 创建了共享内存，get_semaphoreid 创建了信号量集合，然后 shmat 将共享内存映射到了虚拟地址空间的 shm 指针指向的位置，然后通过强制类型转换，shared 的指针指向放在共享内存里面的 struct shm_data 结构，然后初始化为 0。semaphore_init 将信号量进行了初始化。接着，producer 进入了一个无限循环。在这个循环里面，我们先通过 semaphore_p 申请访问共享内存的权利，如果发现 datalength 大于零，说明共享内存里面的数据没有被处理过，于是 semaphore_v 释放权利，先睡一会儿，睡醒了再看。如果发现 datalength 等于 0，说明共享内存里面的数据被处理完了，于是开始往里面放数据。让用户输入多少个数，然后每个数是什么，都放在 struct shm_data 结构中，然后 semaphore_v 释放权利，等待其他的进程将这些数拿去处理。我们再来看 consumer 的代码。 #include \"share.h\" int main() { void *shm = NULL; struct shm_data *shared = NULL; int shmid = get_shmid(); int semid = get_semaphoreid(); int i; shm = shmat(shmid, (void*)0, 0); if(shm == (void*)-1){ exit(0); } shared = (struct shm_data*)shm; while(1){ semaphore_p(semid); if(shared-\u003edatalength \u003e 0){ int sum = 0; for(i=0;i\u003cshared-\u003edatalength-1;i++){ printf(\"%d+\",shared-\u003edata[i]); sum += shared-\u003edata[i]; } printf(\"%d\",shared-\u003edata[shared-\u003edatalength-1]); sum += shared-\u003edata[shared-\u003edatalength-1]; printf(\"=%d\\n\",sum); memset(shared, 0, sizeof(struct shm_data)); semaphore_v(semid); } else { semaphore_v(semid); printf(\"no tasks, waiting.\\n\"); sleep(1); } } } 在这里面，get_shmid 获得 producer 创建的共享内存，get_semaphoreid 获得 producer 创建的信号量集合，然后 shmat 将共享内存映射到了虚拟地址空间的 shm 指针指向的位置，然后通过强制类型转换，shared 的指针指向放在共享内存里面的 struct shm_data 结构。接着，consumer 进入了一个无限循环，在这个循环里面，我们先通过 semaphore_p 申请访问共享内存的权利，如果发现 datalength 等于 0，就说明没什么活干，需要等待。如果发现 datalength 大于 0，就说明有活干，于是将 datalength 个整型数字从 data 数组中取出来求和。最后将 struct shm_data 清空为 0，表示任务处理完毕，通过 semaphore_v 释放权利。通过程序创建的共享内存和信号量集合，我们可以通过命令 ipcs 查看。当然，我们也可以通过 ipcrm 进行删除。 # ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x00016988 32768 root 777 516 0 ------ Semaphore Arrays -------- key semid owner perms nsems 0x00016989 32768 root 777 1 下面我们来运行一下 producer 和 consumer，可以得到下面的结果： # ./producer how many integers to caculate : 2 Input the 0 integer : 3 Input the 1 integer : 4 how many integers to caculate : 4 Input the 0 integer : 3 Input the 1 integer : 4 Input the 2 integer : 5 Input the 3 integer : 6 how many integers to caculate : 7 Input the 0 integer : 9 Input the 1 integer : 8 Input the 2 integer : 7 Input the 3 integer : 6 Input the 4 integer : 5 Input the 5 integer : 4 Input the 6 integer : 3 # ./consumer 3+4=7 3+4+5+6=18 9+8+7+6+5+4+3=42 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:41:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 这一节的内容差不多了，我们来总结一下。共享内存和信号量的配合机制，如下图所示：无论是共享内存还是信号量，创建与初始化都遵循同样流程，通过 ftok 得到 key，通过 xxxget 创建对象并生成 id；生产者和消费者都通过 shmat 将共享内存映射到各自的内存空间，在不同的进程里面映射的位置不同；为了访问共享内存，需要信号量进行保护，信号量需要通过 semctl 初始化为某个值；接下来生产者和消费者要通过 semop(-1) 来竞争信号量，如果生产者抢到信号量则写入，然后通过 semop(+1) 释放信号量，如果消费者抢到信号量则读出，然后通过 semop(+1) 释放信号量；共享内存使用完毕，可以通过 shmdt 来解除映射。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:41:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"41 | IPC（中）：不同项目组之间抢资源，如何协调？ 了解了如何使用共享内存和信号量集合之后，今天我们来解析一下，内核里面都做了什么。不知道你有没有注意到，咱们讲消息队列、共享内存、信号量的机制的时候，我们其实能够从中看到一些统一的规律：它们在使用之前都要生成 key，然后通过 key 得到唯一的 id，并且都是通过 xxxget 函数。在内核里面，这三种进程间通信机制是使用统一的机制管理起来的，都叫 ipcxxx。为了维护这三种进程间通信进制，在内核里面，我们声明了一个有三项的数组。我们通过这段代码，来具体看一看。 struct ipc_namespace { ...... struct ipc_ids ids[3]; ...... } #define IPC_SEM_IDS 0 #define IPC_MSG_IDS 1 #define IPC_SHM_IDS 2 #define sem_ids(ns) ((ns)-\u003eids[IPC_SEM_IDS]) #define msg_ids(ns) ((ns)-\u003eids[IPC_MSG_IDS]) #define shm_ids(ns) ((ns)-\u003eids[IPC_SHM_IDS]) 根据代码中的定义，第 0 项用于信号量，第 1 项用于消息队列，第 2 项用于共享内存，分别可以通过 sem_ids、msg_ids、shm_ids 来访问。这段代码里面有 ns，全称叫 namespace。可能不容易理解，你现在可以将它认为是将一台 Linux 服务器逻辑的隔离为多台 Linux 服务器的机制，它背后的原理是一个相当大的话题，我们需要在容器那一章详细讲述。现在，你就可以简单的认为没有 namespace，整个 Linux 在一个 namespace 下面，那这些 ids 也是整个 Linux 只有一份。接下来，我们再来看 struct ipc_ids 里面保存了什么。首先，in_use 表示当前有多少个 ipc；其次，seq 和 next_id 用于一起生成 ipc 唯一的 id，因为信号量，共享内存，消息队列，它们三个的 id 也不能重复；ipcs_idr 是一棵基数树，我们又碰到它了，一旦涉及从一个整数查找一个对象，它都是最好的选择。 struct ipc_ids { int in_use; unsigned short seq; struct rw_semaphore rwsem; struct idr ipcs_idr; int next_id; }; struct idr { struct radix_tree_root idr_rt; unsigned int idr_next; }; 也就是说，对于 sem_ids、msg_ids、shm_ids 各有一棵基数树。那这棵树里面究竟存放了什么，能够统一管理这三类 ipc 对象呢？通过下面这个函数 ipc_obtain_object_idr，我们可以看出端倪。这个函数根据 id，在基数树里面找出来的是 struct kern_ipc_perm。 struct kern_ipc_perm *ipc_obtain_object_idr(struct ipc_ids *ids, int id) { struct kern_ipc_perm *out; int lid = ipcid_to_idx(id); out = idr_find(\u0026ids-\u003eipcs_idr, lid); return out; } 如果我们看用于表示信号量、消息队列、共享内存的结构，就会发现，这三个结构的第一项都是 struct kern_ipc_perm。 struct sem_array { struct kern_ipc_perm sem_perm; /* permissions .. see ipc.h */ time_t sem_ctime; /* create/last semctl() time */ struct list_head pending_alter; /* pending operations */ /* that alter the array */ struct list_head pending_const; /* pending complex operations */ /* that do not alter semvals */ struct list_head list_id; /* undo requests on this array */ int sem_nsems; /* no. of semaphores in array */ int complex_count; /* pending complex operations */ unsigned int use_global_lock;/* \u003e0: global lock required */ struct sem sems[]; } __randomize_layout; struct msg_queue { struct kern_ipc_perm q_perm; time_t q_stime; /* last msgsnd time */ time_t q_rtime; /* last msgrcv time */ time_t q_ctime; /* last change time */ unsigned long q_cbytes; /* current number of bytes on queue */ unsigned long q_qnum; /* number of messages in queue */ unsigned long q_qbytes; /* max number of bytes on queue */ pid_t q_lspid; /* pid of last msgsnd */ pid_t q_lrpid; /* last receive pid */ struct list_head q_messages; struct list_head q_receivers; struct list_head q_senders; } __randomize_layout; struct shmid_kernel /* private to the kernel */ { struct kern_ipc_perm shm_perm; struct file *shm_file; unsigned long shm_nattch; unsigned long shm_segsz; time_t shm_atim; time_t shm_dtim; time_t shm_ctim; pid_t shm_cprid; pid_t shm_lprid; struct user_struct *mlock_user; /* The task created the shm object. NULL if the task is dead. */ struct task_struct *shm_creator; struct list_head shm_clist; /* list by creator */ } __randomize_layout; 也就是说，我们完全可以通过 struct kern_ipc_perm 的指针，通过进行强制类型转换后，得到整个结构。做这件事情的函数如下： static inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int id) { struct kern_ipc_perm *ipcp = ipc_obtain_object_idr(\u0026sem_ids(ns), id); return container_of(ipcp, struct sem_array, sem_perm); } static inline struct msg_queue *msq_obtain_object(struct ipc_namespace *ns, int id) { struct kern_ipc_perm *ipcp = ipc_obtain_object_idr(\u0026msg_ids(ns), id); return container_of(ipcp, struct msg_queue, q_perm); } static inline struct shmid_kernel *shm_obtain_object(struct ipc_namespace *ns, int id) { struct kern_ipc_perm *ipcp = ipc_obtain_object_idr(\u0026shm_ids(ns), id); return container_of(ipcp, struct shmid_kernel, shm_perm); } 通过这种机制，我们就可以将信号量、消息队列、共享内存抽象为 ipc 类型进行统一处理。你有没有觉得，这有点儿面向对象编程中抽象类和实现类的意思？没错，如果你试图去了解 C++ 中类的实现机制，其实也是这么干的。 有了抽象类，接下来我们来看共享内存和信号量的具体实现。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:42:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"如何创建共享内存？ 首先，我们来看创建共享内存的的系统调用。 SYSCALL_DEFINE3(shmget, key_t, key, size_t, size, int, shmflg) { struct ipc_namespace *ns; static const struct ipc_ops shm_ops = { .getnew = newseg, .associate = shm_security, .more_checks = shm_more_checks, }; struct ipc_params shm_params; ns = current-\u003ensproxy-\u003eipc_ns; shm_params.key = key; shm_params.flg = shmflg; shm_params.u.size = size; return ipcget(ns, \u0026shm_ids(ns), \u0026shm_ops, \u0026shm_params); } 这里面调用了抽象的 ipcget、参数分别为共享内存对应的 shm_ids、对应的操作 shm_ops 以及对应的参数 shm_params。如果 key 设置为 IPC_PRIVATE 则永远创建新的，如果不是的话，就会调用 ipcget_public。ipcget 的具体代码如下： int ipcget(struct ipc_namespace *ns, struct ipc_ids *ids, const struct ipc_ops *ops, struct ipc_params *params) { if (params-\u003ekey == IPC_PRIVATE) return ipcget_new(ns, ids, ops, params); else return ipcget_public(ns, ids, ops, params); } static int ipcget_public(struct ipc_namespace *ns, struct ipc_ids *ids, const struct ipc_ops *ops, struct ipc_params *params) { struct kern_ipc_perm *ipcp; int flg = params-\u003eflg; int err; ipcp = ipc_findkey(ids, params-\u003ekey); if (ipcp == NULL) { if (!(flg \u0026 IPC_CREAT)) err = -ENOENT; else err = ops-\u003egetnew(ns, params); } else { if (flg \u0026 IPC_CREAT \u0026\u0026 flg \u0026 IPC_EXCL) err = -EEXIST; else { err = 0; if (ops-\u003emore_checks) err = ops-\u003emore_checks(ipcp, params); ...... } } return err; } 在 ipcget_public 中，我们会按照 key，去查找 struct kern_ipc_perm。如果没有找到，那就看是否设置了 IPC_CREAT；如果设置了，就创建一个新的。如果找到了，就将对应的 id 返回。我们这里重点看，如何按照参数 shm_ops，创建新的共享内存，会调用 newseg。 static int newseg(struct ipc_namespace *ns, struct ipc_params *params) { key_t key = params-\u003ekey; int shmflg = params-\u003eflg; size_t size = params-\u003eu.size; int error; struct shmid_kernel *shp; size_t numpages = (size + PAGE_SIZE - 1) \u003e\u003e PAGE_SHIFT; struct file *file; char name[13]; vm_flags_t acctflag = 0; ...... shp = kvmalloc(sizeof(*shp), GFP_KERNEL); ...... shp-\u003eshm_perm.key = key; shp-\u003eshm_perm.mode = (shmflg \u0026 S_IRWXUGO); shp-\u003emlock_user = NULL; shp-\u003eshm_perm.security = NULL; ...... file = shmem_kernel_file_setup(name, size, acctflag); ...... shp-\u003eshm_cprid = task_tgid_vnr(current); shp-\u003eshm_lprid = 0; shp-\u003eshm_atim = shp-\u003eshm_dtim = 0; shp-\u003eshm_ctim = get_seconds(); shp-\u003eshm_segsz = size; shp-\u003eshm_nattch = 0; shp-\u003eshm_file = file; shp-\u003eshm_creator = current; error = ipc_addid(\u0026shm_ids(ns), \u0026shp-\u003eshm_perm, ns-\u003eshm_ctlmni); ...... list_add(\u0026shp-\u003eshm_clist, \u0026current-\u003esysvshm.shm_clist); ...... file_inode(file)-\u003ei_ino = shp-\u003eshm_perm.id; ns-\u003eshm_tot += numpages; error = shp-\u003eshm_perm.id; ...... return error; } newseg 函数的第一步，通过 kvmalloc 在直接映射区分配一个 struct shmid_kernel 结构。这个结构就是用来描述共享内存的。这个结构最开始就是上面说的 struct kern_ipc_perm 结构。接下来就是填充这个 struct shmid_kernel 结构，例如 key、权限等。newseg 函数的第二步，共享内存需要和文件进行关联。** 为什么要做这个呢？我们在讲内存映射的时候讲过，虚拟地址空间可以和物理内存关联，但是物理内存是某个进程独享的。虚拟地址空间也可以映射到一个文件，文件是可以跨进程共享的。咱们这里的共享内存需要跨进程共享，也应该借鉴文件映射的思路。只不过不应该映射一个硬盘上的文件，而是映射到一个内存文件系统上的文件。mm/shmem.c 里面就定义了这样一个基于内存的文件系统。这里你一定要注意区分 shmem 和 shm 的区别，前者是一个文件系统，后者是进程通信机制。在系统初始化的时候，shmem_init 注册了 shmem 文件系统 shmem_fs_type，并且挂在到了 shm_mnt 下面。 int __init shmem_init(void) { int error; error = shmem_init_inodecache(); error = register_filesystem(\u0026shmem_fs_type); shm_mnt = kern_mount(\u0026shmem_fs_type); ...... return 0; } static struct file_system_type shmem_fs_type = { .owner = THIS_MODULE, .name = \"tmpfs\", .mount = shmem_mount, .kill_sb = kill_litter_super, .fs_flags = FS_USERNS_MOUNT, }; 接下来，newseg 函数会调用 shmem_kernel_file_setup，其实就是在 shmem 文件系统里面创建一个文件。 /** * shmem_kernel_file_setup - get an unlinked file living in tmpfs which must be kernel internal. * @name: name for dentry (to be seen in /proc/\u003cpid\u003e/maps * @size: size to be set for the file * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size */ struct file *shmem_kernel_file_setup(const char *name, loff_t size, unsigned long flags) { return __shmem_file_setup(name, size, flags, S_PRIVATE); } static struct file *__shmem_file_setup(const char *name, loff_t size, unsigned long flags, unsigned int i_flags) { struct file *res; struct inode *inode; struct path path; struct super_block *sb; ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:42:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"如何将共享内存映射到虚拟地址空间？ 从上面的代码解析中，我们知道，共享内存的数据结构 struct shmid_kernel，是通过它的成员 struct file *shm_file，来管理内存文件系统 shmem 上的内存文件的。无论这个共享内存是否被映射，shm_file 都是存在的。接下来，我们要将共享内存映射到虚拟地址空间中。调用的是 shmat，对应的系统调用如下： SYSCALL_DEFINE3(shmat, int, shmid, char __user *, shmaddr, int, shmflg) { unsigned long ret; long err; err = do_shmat(shmid, shmaddr, shmflg, \u0026ret, SHMLBA); force_successful_syscall_return(); return (long)ret; } long do_shmat(int shmid, char __user *shmaddr, int shmflg, ulong *raddr, unsigned long shmlba) { struct shmid_kernel *shp; unsigned long addr = (unsigned long)shmaddr; unsigned long size; struct file *file; int err; unsigned long flags = MAP_SHARED; unsigned long prot; int acc_mode; struct ipc_namespace *ns; struct shm_file_data *sfd; struct path path; fmode_t f_mode; unsigned long populate = 0; ...... prot = PROT_READ | PROT_WRITE; acc_mode = S_IRUGO | S_IWUGO; f_mode = FMODE_READ | FMODE_WRITE; ...... ns = current-\u003ensproxy-\u003eipc_ns; shp = shm_obtain_object_check(ns, shmid); ...... path = shp-\u003eshm_file-\u003ef_path; path_get(\u0026path); shp-\u003eshm_nattch++; size = i_size_read(d_inode(path.dentry)); ...... sfd = kzalloc(sizeof(*sfd), GFP_KERNEL); ...... file = alloc_file(\u0026path, f_mode, is_file_hugepages(shp-\u003eshm_file) ? \u0026shm_file_operations_huge : \u0026shm_file_operations); ...... file-\u003eprivate_data = sfd; file-\u003ef_mapping = shp-\u003eshm_file-\u003ef_mapping; sfd-\u003eid = shp-\u003eshm_perm.id; sfd-\u003ens = get_ipc_ns(ns); sfd-\u003efile = shp-\u003eshm_file; sfd-\u003evm_ops = NULL; ...... addr = do_mmap_pgoff(file, addr, size, prot, flags, 0, \u0026populate, NULL); *raddr = addr; err = 0; ...... return err; } 在这个函数里面，shm_obtain_object_check 会通过共享内存的 id，在基数树中找到对应的 struct shmid_kernel 结构，通过它找到 shmem 上的内存文件。接下来，我们要分配一个 struct shm_file_data，来表示这个内存文件。将 shmem 中指向内存文件的 shm_file 赋值给 struct shm_file_data 中的 file 成员。然后，我们创建了一个 struct file，指向的也是 shmem 中的内存文件。为什么要再创建一个呢？这两个的功能不同，shmem 中 shm_file 用于管理内存文件，是一个中立的，独立于任何一个进程的角色。而新创建的 struct file 是专门用于做内存映射的，就像咱们在讲内存映射那一节讲过的，一个硬盘上的文件要映射到虚拟地址空间中的时候，需要在 vm_area_struct 里面有一个 struct file *vm_file 指向硬盘上的文件，现在变成内存文件了，但是这个结构还是不能少。新创建的 struct file 的 private_data，指向 struct shm_file_data，这样内存映射那部分的数据结构，就能够通过它来访问内存文件了。新创建的 struct file 的 file_operations 也发生了变化，变成了 shm_file_operations。 static const struct file_operations shm_file_operations = { .mmap = shm_mmap, .fsync = shm_fsync, .release = shm_release, .get_unmapped_area = shm_get_unmapped_area, .llseek = noop_llseek, .fallocate = shm_fallocate, }; 接下来，do_mmap_pgoff 函数我们遇到过，原来映射硬盘上的文件的时候，也是调用它。这里我们不再详细解析了。它会分配一个 vm_area_struct 指向虚拟地址空间中没有分配的区域，它的 vm_file 指向这个内存文件，然后它会调用 shm_file_operations 的 mmap 函数，也即 shm_mmap 进行映射。 static int shm_mmap(struct file *file, struct vm_area_struct *vma) { struct shm_file_data *sfd = shm_file_data(file); int ret; ret = __shm_open(vma); ret = call_mmap(sfd-\u003efile, vma); sfd-\u003evm_ops = vma-\u003evm_ops; vma-\u003evm_ops = \u0026shm_vm_ops; return 0; } shm_mmap 中调用了 shm_file_data 中的 file 的 mmap 函数，这次调用的是 shmem_file_operations 的 mmap，也即 shmem_mmap。 static int shmem_mmap(struct file *file, struct vm_area_struct *vma) { file_accessed(file); vma-\u003evm_ops = \u0026shmem_vm_ops; return 0; } 这里面，vm_area_struct 的 vm_ops 指向 shmem_vm_ops。等从 call_mmap 中返回之后，shm_file_data 的 vm_ops 指向了 shmem_vm_ops，而 vm_area_struct 的 vm_ops 改为指向 shm_vm_ops。我们来看一下，shm_vm_ops 和 shmem_vm_ops 的定义。 static const struct vm_operations_struct shm_vm_ops = { .open = shm_open, /* callback for a new vm-area open */ .close = shm_close, /* callback for when the vm-area is released */ .fault = shm_fault, }; static const struct vm_operations_struct shmem_vm_ops = { .fault = shmem_fault, .map_pages = filemap_map_pages, }; 它们里面最关键的就是 fault 函数，也即访问虚拟内存的时候，访问不到应该怎么办。当访问不到的时候，先调用 vm_area_struct 的 vm_ops，也即 shm_vm_ops 的 fault 函数 shm_fault。然后它会转而调用 shm_file_data 的 vm_ops，也即 shmem_vm_ops 的 fault 函数 shmem_fault。 static int shm_fault(struct vm_fault *vmf) { struct file *file = vmf-\u003evma-\u003evm_file; struct shm_file_data *sfd = shm_file_data(file); return sfd-\u003evm_ops-\u003efault(vmf); } 虽然基于内存的文件系统，已经为这个内存文件分配了 inode，但是内存也却是一点儿都没分配，只有在发生缺页异常的时候才进行分配。 static int shmem_fault(","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:42:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 我们来总结一下共享内存的创建和映射过程。 调用 shmget 创建共享内存。先通过 ipc_findkey 在基数树中查找 key 对应的共享内存对象 shmid_kernel 是否已经被创建过，如果已经被创建，就会被查询出来，例如 producer 创建过，在 consumer 中就会查询出来。如果共享内存没有被创建过，则调用 shm_ops 的 newseg 方法，创建一个共享内存对象 shmid_kernel。例如，在 producer 中就会新建。在 shmem 文件系统里面创建一个文件，共享内存对象 shmid_kernel 指向这个文件，这个文件用 struct file 表示，我们姑且称它为 file1。调用 shmat，将共享内存映射到虚拟地址空间。shm_obtain_object_check 先从基数树里面找到 shmid_kernel 对象。创建用于内存映射到文件的 file 和 shm_file_data，这里的 struct file 我们姑且称为 file2。关联内存区域 vm_area_struct 和用于内存映射到文件的 file，也即 file2，调用 file2 的 mmap 函数。file2 的 mmap 函数 shm_mmap，会调用 file1 的 mmap 函数 shmem_mmap，设置 shm_file_data 和 vm_area_struct 的 vm_ops。内存映射完毕之后，其实并没有真的分配物理内存，当访问内存的时候，会触发缺页异常 do_page_fault。vm_area_struct 的 vm_ops 的 shm_fault 会调用 shm_file_data 的 vm_ops 的 shmem_fault。在 page cache 中找一个空闲页，或者创建一个空闲页。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:42:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"42 | IPC（下）：不同项目组之间抢资源，如何协调？ IPC 这块的内容比较多，为了让你能够更好地理解，我分成了三节来讲。前面我们解析完了共享内存的内核机制后，今天我们来看最后一部分，信号量的内核机制。首先，我们需要创建一个信号量，调用的是系统调用 semget。代码如下： SYSCALL_DEFINE3(semget, key_t, key, int, nsems, int, semflg) { struct ipc_namespace *ns; static const struct ipc_ops sem_ops = { .getnew = newary, .associate = sem_security, .more_checks = sem_more_checks, }; struct ipc_params sem_params; ns = current-\u003ensproxy-\u003eipc_ns; sem_params.key = key; sem_params.flg = semflg; sem_params.u.nsems = nsems; return ipcget(ns, \u0026sem_ids(ns), \u0026sem_ops, \u0026sem_params); } 我们解析过了共享内存，再看信号量，就顺畅很多了。这里同样调用了抽象的 ipcget，参数分别为信号量对应的 sem_ids、对应的操作 sem_ops 以及对应的参数 sem_params。ipcget 的代码我们已经解析过了。如果 key 设置为 IPC_PRIVATE 则永远创建新的；如果不是的话，就会调用 ipcget_public。在 ipcget_public 中，我们能会按照 key，去查找 struct kern_ipc_perm。如果没有找到，那就看看是否设置了 IPC_CREAT。如果设置了，就创建一个新的。如果找到了，就将对应的 id 返回。我们这里重点看，如何按照参数 sem_ops，创建新的信号量会调用 newary。 static int newary(struct ipc_namespace *ns, struct ipc_params *params) { int retval; struct sem_array *sma; key_t key = params-\u003ekey; int nsems = params-\u003eu.nsems; int semflg = params-\u003eflg; int i; ...... sma = sem_alloc(nsems); ...... sma-\u003esem_perm.mode = (semflg \u0026 S_IRWXUGO); sma-\u003esem_perm.key = key; sma-\u003esem_perm.security = NULL; ...... for (i = 0; i \u003c nsems; i++) { INIT_LIST_HEAD(\u0026sma-\u003esems[i].pending_alter); INIT_LIST_HEAD(\u0026sma-\u003esems[i].pending_const); spin_lock_init(\u0026sma-\u003esems[i].lock); } sma-\u003ecomplex_count = 0; sma-\u003euse_global_lock = USE_GLOBAL_LOCK_HYSTERESIS; INIT_LIST_HEAD(\u0026sma-\u003epending_alter); INIT_LIST_HEAD(\u0026sma-\u003epending_const); INIT_LIST_HEAD(\u0026sma-\u003elist_id); sma-\u003esem_nsems = nsems; sma-\u003esem_ctime = get_seconds(); retval = ipc_addid(\u0026sem_ids(ns), \u0026sma-\u003esem_perm, ns-\u003esc_semmni); ...... ns-\u003eused_sems += nsems; ...... return sma-\u003esem_perm.id; } newary 函数的第一步，通过 kvmalloc 在直接映射区分配一个 struct sem_array 结构。这个结构是用来描述信号量的，这个结构最开始就是上面说的 struct kern_ipc_perm 结构。接下来就是填充这个 struct sem_array 结构，例如 key、权限等。struct sem_array 里有多个信号量，放在 struct sem sems[]数组里面，在 struct sem 里面有当前的信号量的数值 semval。 struct sem { int semval; /* current value */ /* * PID of the process that last modified the semaphore. For * Linux, specifically these are: * - semop * - semctl, via SETVAL and SETALL. * - at task exit when performing undo adjustments (see exit_sem). */ int sempid; spinlock_t lock; /* spinlock for fine-grained semtimedop */ struct list_head pending_alter; /* pending single-sop operations that alter the semaphore */ struct list_head pending_const; /* pending single-sop operations that do not alter the semaphore*/ time_t sem_otime; /* candidate for sem_otime */ } ____cacheline_aligned_in_smp; struct sem_array 和 struct sem 各有一个链表 struct list_head pending_alter，分别表示对于整个信号量数组的修改和对于某个信号量的修改。newary 函数的第二步，就是初始化这些链表。newary 函数的第三步，通过 ipc_addid 将新创建的 struct sem_array 结构，挂到 sem_ids 里面的基数树上，并返回相应的 id。信号量创建的过程到此结束，接下来我们来看，如何通过 semctl 对信号量数组进行初始化。 SYSCALL_DEFINE4(semctl, int, semid, int, semnum, int, cmd, unsigned long, arg) { int version; struct ipc_namespace *ns; void __user *p = (void __user *)arg; ns = current-\u003ensproxy-\u003eipc_ns; switch (cmd) { case IPC_INFO: case SEM_INFO: case IPC_STAT: case SEM_STAT: return semctl_nolock(ns, semid, cmd, version, p); case GETALL: case GETVAL: case GETPID: case GETNCNT: case GETZCNT: case SETALL: return semctl_main(ns, semid, semnum, cmd, p); case SETVAL: return semctl_setval(ns, semid, semnum, arg); case IPC_RMID: case IPC_SET: return semctl_down(ns, semid, cmd, version, p); default: return -EINVAL; } } 这里我们重点看，SETALL 操作调用的 semctl_main 函数，以及 SETVAL 操作调用的 semctl_setval 函数。对于 SETALL 操作来讲，传进来的参数为 union semun 里面的 unsigned short *array，会设置整个信号量集合。 static int semctl_main(struct ipc_namespace *ns, int semid, int semnum, int cmd, void __user *p) { struct sem_array *sma; struct sem *curr; int err, nsems; ushort fast_sem_io[SEMMSL_FAST]; ushort *sem_io = fast_sem_io; DEFINE_WAKE_Q(wake_q); sma = sem_obtain_object_check(ns, semid); nsems = sma-\u003esem_nsems; ...... switch (cmd) { ...... case SETALL: { int i; struct sem_undo *un; ...... if (copy_from_user(sem_io, p, nsems*sizeof(ushort))) {","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:43:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 信号量的机制也很复杂，我们对着下面这个图总结一下。 调用 semget 创建信号量集合。ipc_findkey 会在基数树中，根据 key 查找信号量集合 sem_array 对象。如果已经被创建，就会被查询出来。例如 producer 被创建过，在 consumer 中就会查询出来。如果信号量集合没有被创建过，则调用 sem_ops 的 newary 方法，创建一个信号量集合对象 sem_array。例如，在 producer 中就会新建。调用 semctl(SETALL) 初始化信号量。sem_obtain_object_check 先从基数树里面找到 sem_array 对象。根据用户指定的信号量数组，初始化信号量集合，也即初始化 sem_array 对象的 struct sem sems[]成员。调用 semop 操作信号量。创建信号量操作结构 sem_queue，放入队列。创建 undo 结构，放入链表。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:43:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"43 预习 | Socket通信之网络协议基本原理 上一节我们讲的进程间通信，其实是通过内核的数据结构完成的，主要用于在一台 Linux 上两个进程之间的通信。但是，一旦超出一台机器的范畴，我们就需要一种跨机器的通信机制。一台机器将自己想要表达的内容，按照某种约定好的格式发送出去，当另外一台机器收到这些信息后，也能够按照约定好的格式解析出来，从而准确、可靠地获得发送方想要表达的内容。这种约定好的格式就是网络协议（Networking Protocol）。我们将要讲的 Socket 通信以及相关的系统调用、内核机制，都是基于网络协议的，如果不了解网络协议的机制，解析 Socket 的过程中，你就会迷失方向，因此这一节，我们有必要做一个预习，先来大致讲一下网络协议的基本原理。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:44:0","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"网络为什么要分层？ 我们这里先构建一个相对简单的场景，之后几节内容，我们都要基于这个场景进行讲解。我们假设这里就涉及三台机器。Linux 服务器 A 和 Linux 服务器 B 处于不同的网段，通过中间的 Linux 服务器作为路由器进行转发。 说到网络协议，我们还需要简要介绍一下两种网络协议模型，一种是 OSI 的标准七层模型，一种是业界标准的 TCP/IP 模型。它们的对应关系如下图所示： 为什么网络要分层呢？因为网络环境过于复杂，不是一个能够集中控制的体系。全球数以亿记的服务器和设备各有各的体系，但是都可以通过同一套网络协议栈通过切分成多个层次和组合，来满足不同服务器和设备的通信需求。我们这里简单介绍一下网络协议的几个层次。 我们从哪一个层次开始呢？从第三层，网络层开始，因为这一层有我们熟悉的 IP 地址。也因此，这一层我们也叫 IP 层。我们通常看到的 IP 地址都是这个样子的：192.168.1.100/24。斜杠前面是 IP 地址，这个地址被点分隔为四个部分，每个部分 8 位，总共是 32 位。斜线后面 24 的意思是，32 位中，前 24 位是网络号，后 8 位是主机号。为什么要这样分呢？我们可以想象，虽然全世界组成一张大的互联网，美国的网站你也能够访问的，但是这个网络不是一整个的。你们小区有一个网络，你们公司也有一个网络，联通、移动、电信运营商也各有各的网络，所以一个大网络是被分成个小的网络。那如何区分这些网络呢？这就是网络号的概念。一个网络里面会有多个设备，这些设备的网络号一样，主机号不一样。不信你可以观察一下你家里的手机、电视、电脑。连接到网络上的每一个设备都至少有一个 IP 地址，用于定位这个设备。无论是近在咫尺的你旁边同学的电脑，还是远在天边的电商网站，都可以通过 IP 地址进行定位。因此，IP 地址类似互联网上的邮寄地址，是有全局定位功能的。 就算你要访问美国的一个地址，也可以从你身边的网络出发，通过不断的打听道儿，经过多个网络，最终到达目的地址，和快递员送包裹的过程差不多。打听道儿的协议也在第三层，称为路由协议（Routing protocol），将网络包从一个网络转发给另一个网络的设备称为路由器。路由器和路由协议十分复杂，我们这里就不详细讲解了，感兴趣可以去看我写的另一个专栏“趣谈网络协议”里的相关文章。 总而言之，第三层干的事情，就是网络包从一个起始的 IP 地址，沿着路由协议指的道儿，经过多个网络，通过多次路由器转发，到达目标 IP 地址。从第三层，我们往下看，第二层是数据链路层。有时候我们简称为二层或者 MAC 层。所谓 MAC，就是每个网卡都有的唯一的硬件地址（不绝对唯一，相对大概率唯一即可，类比UUID）。这虽然也是一个地址，但是这个地址是没有全局定位功能的。 就像给你送外卖的小哥，不可能根据手机尾号找到你家，但是手机尾号有本地定位功能的，只不过这个定位主要靠“吼”。外卖小哥到了你的楼层就开始大喊：“尾号 xxxx 的，你外卖到了！”MAC 地址的定位功能局限在一个网络里面，也即同一个网络号下的 IP 地址之间，可以通过 MAC 进行定位和通信。从 IP 地址获取 MAC 地址要通过 ARP 协议，是通过在本地发送广播包，也就是“吼”，获得的 MAC 地址。由于同一个网络内的机器数量有限，通过 MAC 地址的好处就是简单。匹配上 MAC 地址就接收，匹配不上就不接收，没有什么所谓路由协议这样复杂的协议。当然坏处就是，MAC 地址的作用范围不能出本地网络，所以一旦跨网络通信，虽然 IP 地址保持不变，但是 MAC 地址每经过一个路由器就要换一次。我们看前面的图。服务器 A 发送网络包给服务器 B，原 IP 地址始终是 192.168.1.100，目标 IP 地址始终是 192.168.2.100，但是在网络 1 里面，原 MAC 地址是 MAC1，目标 MAC 地址是路由器的 MAC2，路由器转发之后，原 MAC 地址是路由器的 MAC3，目标 MAC 地址是 MAC4。所以第二层干的事情，就是网络包在本地网络中的服务器之间定位及通信的机制。 我们再往下看，第一层，物理层，这一层就是物理设备。例如连着电脑的网线，我们能连上的 WiFi，这一层我们不打算进行分析。从第三层往上看，第四层是传输层，这里面有两个著名的协议 TCP 和 UDP。尤其是 TCP，更是广泛使用，在 IP 层的代码逻辑中，仅仅负责数据从一个 IP 地址发送给另一个 IP 地址，丢包、乱序、重传、拥塞，这些 IP 层都不管。处理这些问题的代码逻辑写在了传输层的 TCP 协议里面。我们常称，TCP 是可靠传输协议，也是难为它了。因为从第一层到第三层都不可靠，网络包说丢就丢，是 TCP 这一层通过各种编号、重传等机制，让本来不可靠的网络对于更上层来讲，变得“看起来”可靠。哪有什么应用层岁月静好，只不过 TCP 层帮你负重前行。传输层再往上就是应用层，例如咱们在浏览器里面输入的 HTTP，Java 服务端写的 Servlet，都是这一层的。二层到四层都是在 Linux 内核里面处理的，应用层例如浏览器、Nginx、Tomcat 都是用户态的。内核里面对于网络包的处理是不区分应用的。从四层再往上，就需要区分网络包发给哪个应用。在传输层的 TCP 和 UDP 协议里面，都有端口的概念，不同的应用监听不同的端口。例如，服务端 Nginx 监听 80、Tomcat 监听 8080；再如客户端浏览器监听一个随机端口，FTP 客户端监听另外一个随机端口。 应用层和内核互通的机制，就是通过 Socket 系统调用。所以经常有人会问，Socket 属于哪一层，其实它哪一层都不属于，它属于操作系统的概念，而非网络协议分层的概念。只不过操作系统选择对于网络协议的实现模式是，二到四层的处理代码在内核里面，七层的处理代码让应用自己去做，两者需要跨内核态和用户态通信，就需要一个系统调用完成这个衔接，这就是 Socket。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:44:1","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"发送数据包 网络分完层之后，对于数据包的发送，就是层层封装的过程。就像下面的图中展示的一样，在 Linux 服务器 B 上部署的服务端 Nginx 和 Tomcat，都是通过 Socket 监听 80 和 8080 端口。这个时候，内核的数据结构就知道了。如果遇到发送到这两个端口的，就发送给这两个进程。在 Linux 服务器 A 上的客户端，打开一个 Firefox 连接 Ngnix。也是通过 Socket，客户端会被分配一个随机端口 12345。同理，打开一个 Chrome 连接 Tomcat，同样通过 Socket 分配随机端口 12346。 在客户端浏览器，我们将请求封装为 HTTP 协议，通过 Socket 发送到内核。内核的网络协议栈里面，在 TCP 层创建用于维护连接、序列号、重传、拥塞控制的数据结构，将 HTTP 包加上 TCP 头，发送给 IP 层，IP 层加上 IP 头，发送给 MAC 层，MAC 层加上 MAC 头，从硬件网卡发出去。网络包会先到达网络 1 的交换机。我们常称交换机为二层设备，这是因为，交换机只会处理到第二层，然后它会将网络包的 MAC 头拿下来，发现目标 MAC 是在自己右面的网口，于是就从这个网口发出去。网络包会到达中间的 Linux 路由器，它左面的网卡会收到网络包，发现 MAC 地址匹配，就交给 IP 层，在 IP 层根据 IP 头中的信息，在路由表中查找。下一跳在哪里，应该从哪个网口发出去？在这个例子中，最终会从右面的网口发出去。我们常把路由器称为三层设备，因为它只会处理到第三层。从路由器右面的网口发出去的包会到网络 2 的交换机，还是会经历一次二层的处理，转发到交换机右面的网口。最终网络包会被转发到 Linux 服务器 B，它发现 MAC 地址匹配，就将 MAC 头取下来，交给上一层。IP 层发现 IP 地址匹配，将 IP 头取下来，交给上一层。TCP 层会根据 TCP 头中的序列号等信息，发现它是一个正确的网络包，就会将网络包缓存起来，等待应用层的读取。 应用层通过 Socket 监听某个端口，因而读取的时候，内核会根据 TCP 头中的端口号，将网络包发给相应的应用。HTTP 层的头和正文，是应用层来解析的。通过解析，应用层知道了客户端的请求，例如购买一个商品，还是请求一个网页。当应用层处理完 HTTP 的请求，会将结果仍然封装为 HTTP 的网络包，通过 Socket 接口，发送给内核。内核会经过层层封装，从物理网口发送出去，经过网络 2 的交换机，Linux 路由器到达网络 1，经过网络 1 的交换机，到达 Linux 服务器 A。在 Linux 服务器 A 上，经过层层解封装，通过 socket 接口，根据客户端的随机端口号，发送给客户端的应用程序，浏览器。于是浏览器就能够显示出一个绚丽多彩的页面了。即便在如此简单的一个环境中，网络包的发送过程，竟然如此的复杂。不过这一章后面，我们还是会层层剖析每一层做的事情。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:44:2","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["Advanced learning"],"content":"总结时刻 网络协议是一个大话题，如果你想了解网络协议的方方面面，欢迎你订阅我写的另一个专栏“趣谈网络协议”。这个专栏重点解析在这个网络通信过程中，发送端和接收端的操作系统都做了哪些事情，对于中间通路上的复杂的网络通信逻辑没有做深入解析。如果只是为了掌握这一章的内容，这一节我们讲的网络协议的七个层次，你不必每一层的每一个协议都很清楚，只要记住 TCP/UDP-\u003eIPv4-\u003eARP 这一条链就可以了，因为后面我们的分析都是重点分析这条链。另外，前面那个简单的拓扑图中，网络包的封装、转发、解封装的过程，建议你多看几遍，了熟于心，因为接下来，我们就能从代码层面，看到这个过程。到时候，对应起来，你就比较容易理解。了解了 Socket 的基本原理，下一篇文章，我们就来看一看在 Linux 操作系统里面 Socket 系统调用的接口是什么样的。 ","date":"2022-08-05 22:05:00","objectID":"/interesting_talk_linux/:44:3","tags":["linux"],"title":"Interesting talk about Linux operating system","uri":"/interesting_talk_linux/"},{"categories":["School courses"],"content":"深入浅出分布式技术原理 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:0:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"开篇词｜掌握好学习路径，分布式系统原来如此简单 陈现麟 2022-01-24 从我的经验来看，初学者想要高效且系统性地掌握分布式系统，这本身就是一个悖论。不管你有多努力，要知道理论与实践之间是有一道巨大的鸿沟的。你很可能会被场景、时间、自身积累与理解程度等原因限制住，导致学习事倍功半，实践也效率低下。而对于这个主题来说，论文、源码等资料的学习仅仅是入门，我们还需要结合业务场景、工作经历和实践经验，再加上思考与时间去慢慢沉淀才可以。 这里也附上一份参考资料，是我之前参加的一个知乎圆桌会议，里面记录了我对分布式系统的理解，希望能够帮到你。 概述篇学习一个知识应该先理解这个知识的来龙去脉，所以在一开始的概述篇中，我们先来讨论分布式系统产生的过程：它为什么会产生，产生后解决了什么问题，又带来了哪些新问题，遇到哪些方面的挑战。通过“概述篇”的学习，你可以比较好地抓到分布式系统的脉络和关键点，有了很强的学习目标和路径，就不会迷失在各种系统和框架实现的细节中了。 分布式计算篇 分布式计算是你日常工作中接触最多的分布式技术，这部分看起来像是微服务相关的知识，但是我们不是从微服务的角度来讲解的。因为分布式系统有各种各样的实现形式，而微服务只是其中的一种具体的实现形式，所以，我们会从单机系统演进到分布式系统后，引入哪些新问题的角度，在技术原理层面一个一个讨论并解决这些问题。你在学习之后，可以在各种系统和场景中理解和运用它，并且知道在系统设计层面应该如何取舍。由于这一部分的内容你平时都有接触，所以学习起来的难度相对会比较低，共鸣会比较强烈。 分布式存储篇 这一部分你可以理解为是分布式技术篇中的进阶篇，我们对计算进行分布式扩展后，再一起来讨论存储的分布式扩展。这里我们从简单到复杂，一起讨论数据分片、数据复制、分布式事务和一致性等相关的知识。掌握之后，再做架构设计时，你会发现思维的深度和广度都得到了提升。 总结篇 待上面三个模块的内容学习完成后，你已经对分布式系统的重要原理有了系统性地理解，这个时候，我们再一起来看分布式系统的发展历程和未来趋势。我们从分布式计算的角度，一起讨论分布式系统是怎么从单机系统演进到 Service Mesh 的；还会从分布式存储的角度，一起讨论分布式系统是怎么从单机系统的 ACID 演进到 NewSQL 的。最终，交给你一张继续深入学习的路线图。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:1:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"概述 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:2:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"01｜导读：以前因后果为脉络，串起网状知识体系 首先，在 CAP 及其相关理论与权衡方面，需要了解 ACID 、 BASE 和 CAP 理论这三个主题。我推荐你阅读一篇文章以及文章后面相关的参考文献，读完后你就能很好地理解 CAP 理论中的取舍了，这是英文版本：https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed/ ，这是中文版本：https://www.infoq.cn/article/cap-twelve-years-later-how-the-rules-have-changed/ 。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:3:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"02｜新的挑战：分布式系统是银弹吗？我看未必！ 从本质上来说，单体系统是以单进程的形式运行在一个计算机节点上，而分布式系统是以多进程的形式运行在多个计算机节点上，二者的本质差别就导致了分布式系统面临着四个方面的新问题，分别是：故障处理、异步网络、时钟同步和共识协同。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:4:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"03｜CAP 理论：分布式场景下我们真的只能三选二吗？ ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:5:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 到这里，我们知道了在分布式系统场景下，CAP 理论的相关知识，我们一起来总结一下这节课的主要内容：首先，我们一起讨论了什么是 CAP 理论，它是指分布式系统中，在满足分区容错的前提下，没有算法能同时满足数据一致性和服务可用性，只能在数据一致性和服务可用性之间二选一。然后，我们讨论了 CAP 理论产生的影响，可以说 CAP 理论的出现，让人们接受了 BASE 理论，并且推动了 NoSQL 运动的发展，开启了它的黄金十年。最后，我们探讨了现在人们对于 CAP 理论的新理解，对于 CAP 理论，我们不会简单地三选二或者二选一。对于 AP 模型的系统，我们会努力去提升数据一致性的级别，而对于 CP 模型的系统，我们会努力去提升系统可用性的级别。同时，由于系统分区的情况非常少见，我们可以在网络不出现分区的时候，将 A 和 C 都选择上；在网络出现分区的时候，再选择放弃部分的可用性，或者降低数据一致性的级别，通过推迟 CAP 选择来提高系统的可用性和数据一致性。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:5:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"time for computer 在计算机系统内部，主要有两种时钟：墙上时钟和单调时钟，它们都可以衡量时间，但却有本质的区别。在这节课中，我将带你了解两种时钟的相关知识，其中的墙上时钟是本节课的重点部分，然后我们再一起探讨如何对两种时钟进行管理。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:6:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"墙上时钟 学习墙上时钟的相关知识，我们要先从墙上时钟的同步入手，了解时间同步出现误差的原因以及现有的解决方案，之后再分析闰秒出现的原因，以及闰秒的处理方式，最后我们会根据处理方式中的“跳跃式调整”的处理逻辑，来分析 2012 年一个 Linux 服务器宕机的案例。墙上时钟又叫钟表时间，顾名思义，和我们平时使用的钟表的时间一样，表示形式为日期与时间。在 Linux 系统中，墙上时钟的表示形式为 UTC 时间，记录的是自公元 1970 年 1 月 1 日 0 时 0 分 0 秒以来的秒数和毫秒数（不含闰秒）。Linux 系统需要处理闰秒的逻辑就是因为 Linux 系统使用 UTC 时间，但是系统中记录的 UTC 时间是不含闰秒的。 墙上时钟的同步 根据墙上时钟的定义，我们可以发现，墙上时钟的标准是在计算机外部定义的，所以确保墙上时钟的准确性就变成了一个问题。计算机内部的计时器为石英钟，但是它不够精确，随着机器的温度波动，会存在过快或者过慢的问题，所以依靠计算机自身，来维持墙上时钟的准确性是不可能的，这就是计算机系统内的时间需要与外部时间进行同步的原因。 目前普遍采取的一种方式为：计算机与 NTP 时间服务器定期通过网络同步。很明显，这个方式受限于网络时延的影响，一般来说，至少会有 35 毫秒的偏差，最大的时候可能会超过 1 秒。在一些对时间精度要求很高的系统中，通过 NTP 进行同步是远远不够的，这时我们可以通过 GPS 接收机，接收标准的墙上时钟，然后在机房内部通过精确时间协议（ PTP ）进行同步。 PTP 是一种高精度时间同步协议，可以达到亚微秒级精度，有资料说可达到 30 纳秒左右的偏差精度，但是它需要网络的节点（交换机）支持 PTP 协议，才能实现纳秒量级的同步。在时间同步这个问题上， Google 的做法更酷，通过 GPS 接收机，接收标准的墙上时钟，然后通过机房内部去部署原子钟，使得它的精度可以达到每 2000 万年才误差 1 秒，用这种方式来防止 GPS 接收机的故障。接着，再把这些时间协调装置连接到特定数量的主服务器，最后再由主服务器，向整个谷歌网络中运行的其他计算机传输时间读数，即 TrueTime API 。 Google 正是基于上面的时间精度保证，在此基础上实现了第一个可扩展的、全球分布式的数据库 Spanner。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:6:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"闰秒出现的原因 从上述的讨论中，我们可以知道计算机的墙上时钟通过同步机制，确保时间的误差会保持在一个范围以内。虽然它保证了时间精度，但是因为 Linux 系统中，墙上时钟的表示形式为 UTC 时间，而 UTC 时间是不含闰秒的，所以如何处理闰秒就成为了一个重要的问题，那么我们先来想想闰秒出现的原因。 因为地球自转速率变慢，所以目前的两种时间计量系统：世界时和原子时，它们之间发生了误差，这就是闰秒出现的根本原因，下面我们就从世界时和原子时这两方面，具体来分析一下。 世界时（ UT1 ）以地球自转运动来计量时间，它定义地球自转一周为一天，绕太阳公转一周为一年，这对人们的日常生活非常重要。但是，因为地球自转速率正在变慢，世界时的秒长就会有微小的变化，每天会长千分之几秒，也就是说，后一天的 24 小时会比前一天的 24 小时要长千分之几秒，所以用世界时来度量时间，会出现均匀性非常不好的问题。原子时取微观世界的铯原子中，两个超精细能级间的跃迁辐射频率来度量时间，精确度非常高，每天快慢不超过千万分之一秒。所以，原子时的均匀性非常好，是度量时间的理想尺度。可是，原子时与地球空间位置无关，由于地球自转速率正在变慢，如果在某地区使用原子时，从今天开始计时，那么原子时到了明天凌晨 0：00 的时候，地球还需要等千分之几秒才自转完一周。这样一天一天地累积，就会出现原子时到了凌晨 0：00 这个时候，太阳还在地球正上空的情况，这显然是不符合常识的。所以，为了统一原子时与世界时之间的差距，协调世界时（ UTC ）就产生了。从 1972 年 1 月 1 日 0 时起，协调世界时秒长采用原子时秒长，时刻与世界时的时刻之差保持在正负 0.9 秒之内，必要时用阶跃 1 整秒的方式来调整。 这个 1 整秒的调整，叫做闰秒，如果增加 1 秒就是正闰秒，减少 1 秒就是负闰秒。 UTC 从 1972 年 1 月起正式成为国际标准时间，它是原子时和世界时这两种时间尺度的结合。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:6:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"闰秒的处理 因为 Linux 系统记录着，自公元 1970 年 1 月 1 日 0 时 0 分 0 秒以来的秒数和毫秒数，但是不含闰秒这种情况，导致了在 Linux 系统中每分钟有 60 秒，每天有 86400 秒是系统定义死的。所以 Linux 系统需要额外的逻辑来处理闰秒。目前处理闰秒的方式主要有两种，一种是在 Linux 系统上进行跳跃式调整，另一种是在 NTP 服务上进行渐进式调整的 Slew 模型，下面我们具体讲一讲这两种处理逻辑。 跳跃式调整 首先是在 Linux 系统上进行跳跃式调整，当 UTC 时间插入一个正闰秒后，Linux 系统需要跳过 1 秒，即这一秒时间过去后，在 Linux 的时间管理程序中不应该去计时，因为闰秒的这一秒钟在 Linux 系统中不能被表示。但是，当 UTC 时间插入一个负闰秒后，Linux 系统就需要插入 1 秒，即 Linux 的时间管理程序中要增加 1 秒钟的计时。虽然并没有过去 1 秒钟的时间，但是闰秒的这一秒钟在 Linux 系统中是不存在的。目前 Linux 系统就是采用这种方式来处理闰秒的，所以在 2012 年 6 月 30 日, UTC 时间插入一个正闰秒的时候，Linux 系统会启动相应的逻辑来处理这个插入的正闰秒，这样就使某些版本的闰秒处理逻辑，触发了一个死锁的 bug，造成了大规模的 Linux 服务器内核死锁而宕机的情况。 Slew 模式 NTP 服务的 Slew 模式并不使用跳跃式修改时间，而是渐进式地调整。比如，当 UTC 时间需要插入一个正闰秒时， NTP 服务就会每秒调整一定的 ms 来缓慢修正时间。这样 Linux 系统从 NTP 服务同步时间的时候，就不会感知闰秒的存在了，内核也就不需要启动闰秒相关的逻辑了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:6:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"单调时钟 关于墙上时钟，我们主要讨论的是如何进行时间同步，确保时间精度的问题，而计算机系统中的第二种时钟，单调时钟是一个相对时钟，不需要与外部的时钟进行同步，较墙上时钟要简单很多，所以这里我们就简单地分析一下。单调时钟总是保证时间是向前的，不会出现墙上时钟的回拨问题，所以它非常适合用来测量持续时间段，比如在一个时间点读取单调时钟的值，完成某项工作后再次获得单调时钟的值，时钟值之差就是两次检测之间的时间间隔。到这里，我们可以看出，墙上时钟是绝对时钟，不同计算机节点上的墙上时间可以进行比较，但是它是有误差的，导致比较的结果不可信；而单调时钟是相对时钟，它的绝对值没有任何意义，有可能是计算机自启动以后经历的纳秒数等，所以，比较不同计算机节点上的单调时钟的值是没有意义的。这两点正是分布式系统面临时钟的问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:6:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"时间的管理 前面我们讨论了墙上时钟和单调时钟，你一定很好奇操作系统内部是如何处理时间的，这里你可以先思考两个问题，我们带着问题再具体讨论。第一个问题是：计算机系统是没有时间概念的机器，那么它怎么来计算与管理时间呢？另一个问题是：计算机系统可以提供微秒甚至纳秒，那么它怎么处理这么高精度的时间呢？首先，时间的概念对于计算机来说有些模糊，计算机必须在硬件的帮助下才能计算和管理时间。前面说的石英钟就是用来做计算机的系统定时器的，系统定时器以某种固定的频率自行触发时钟中断。由于时钟中断的频率是编程预定的，所以内核知道连续两次时钟中断的间隔时间，这个间隔时间就叫做节拍。通过时钟中断，内核周期性地更新系统的墙上时钟和单调时钟，从而计算和管理好时间。其次，目前系统定时器的中断频率为 1000 HZ，那么计算机能处理的时间精度为 1 ms。然而很多时候需要更加精确的时间，比如 1 微秒，计算机是怎么来解决这个问题的呢？其实解决的方式非常简单，在每一次计算机启动的时候，计算机都会计算一次 BogoMIPS 的值，这个值的意义是，处理器在给定的时间内执行指令数，通过 BogoMIPS 值，计算机就可以得到非常小的精度了。比如在 1 秒内，计算机执行了 N 条指令，那么计算机的精度就可以达到 N 分之一秒。很明显 N 是一个非常大的数目，因此计算机可以得到非常精确的时间了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:6:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 在这节课中，我们从计算机系统内部的两种时钟出发，深入地讨论了时间相关的话题。在墙上时钟这部分，我们讨论了计算机系统时间同步的方式，分析了闰秒产生的原因，以及 Linux 系统应对闰秒的办法，然后概览性地讲了 Linux 系统是通过时钟中断进行时间的计算与管理的，最后分析了 Linux 系统可以提高时间精度的方法。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:6:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"分布式计算篇 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:7:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"04｜注册发现： AP 系统和 CP 系统哪个更合适？ 在前面的“概述篇”里，我们介绍了分布式技术的来龙去脉，以及在构建一个分布式系统的时候，我们会面临的相关挑战。从这节课开始，我们将一起进入到“分布式技术篇”的学习当中。在这个专栏里，我们会聚焦日常工作中接触最频繁的分布式在线业务技术。学完这部分内容，相信你会对分布式计算技术心中有数，同时不会迷失于实现的细节中。当然，分布式计算是个非常大的技术体系，包括 MapReduce 之类的分布式批处理技术，Flink 之类的分布式流计算技术和 Istio 之类的分布式在线业务技术。但是万变不离其宗，我们掌握了分布式计算技术中稳定不变的知识、原理和解决问题的思路，再研究这些技术的时候也会一通百通。如果直接讨论技术知识和原理，可能会让你觉得非常枯燥和抽象。通过具体的场景案例来讨论技术是非常好的方式，所以我给你虚构了后面这个场景。假设你是极客时间的一个研发工程师，负责极客时间 App 的后端开发工作。目前极客时间采用的是单体架构，服务端所有的功能、模块都耦合在一个服务里。由于现在用户数据和流量都在快速增长，经常会因为一次小的发布，导致全站都不可用，所以在白天的时候，你都不敢发布服务。等到时间一长，凌晨流量低峰时的运维慢慢变成常态，你经常收到机器 CPU、内存的报警，但是每一次都很难知道是什么业务功能导致的，只能直接升级机器配置。慢慢的，你发现工作中的问题和挑战越来越多，但是不知道怎么处理。你是不是也在面临这样的困境呢？我要告诉你的是别担心，在接下来的课程中，我们将会通过分布式技术来一一解决这些问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:8:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么需要服务注册发现 其实极客时间服务器采用的单体架构，在业务早期的快速迭代中，发挥了非常重要的作用。但随着用户数量和流量的快速上涨，这个单体架构就遇到了成本、效率和稳定性的问题。 单体服务面临的问题 首先是成本方面。我们在做所有的事情时都会考虑投入产出比（ROI），所以成本是我们必须考虑的一个问题。对于单体服务在服务器硬件方面的成本，我们需要特别注意异构工作负载和不同保障级别这两个方面的问题。 我们先来看异构工作负载方面的问题。单体服务会包含多种多样的功能模块，有一些是 IO 密集型的模块，比如主要对数据库进行 CRUD 的功能模块；另一些则是计算密集型的模块，比如图片、音频和视频转码相关的功能模块。如果能将 IO 密集型和 CPU 密集型的模块拆分成不同的服务，分开部署到更合适的硬件上，将可以节省大量的机器成本。比如 IO 密集型的模块，我们可以部署在 CPU 性能相对较低的机器上。另一个问题是不同的保障级别。不同业务等级的保障级别也是不一样的：对于账号模块等核心模块，必须确保资源充足；但是对于非核心模块，保障的资源可以相对少一些。而对于一个单体服务来说，是没有办法对不同的模块实施不同的保障级别的。 其次，研发效率是我们能够高效、舒心工作的基本保障，所以必须要注意单体服务模式导致的串行的编译、测试和发布，以及研发团队只能选择单一的研发语言和生态（一般在进程内跨语言都会有限制）这两个限制。串行的编译、测试和发布很好理解：多个研发团队会同时开发不同的功能，由于是单体服务，这些功能只能一起编译、测试和发布，非常浪费时间。如果还要进行灰度发布，那么效率将会更低。 另外还有单一的语言和生态限制。要知道，不同的业务需求可能会对应不同的编程语言和生态。如果是单体服务，则很难按业务需求来选择编程语言和相关的生态，这会大大影响研发效率。最后，我们来讨论一下单体服务引发的稳定性问题。 一来局部风险会放大到全局，因为整个单体服务会包括非常多的功能，一个局部非核心功能的崩溃、死锁等各种异常情况，都会影响所有的业务。这样的风险非常大，而且我们没有办法将故障隔离开。二来业务迭代周期差异大，一般来说，越底层核心的功能，需求就越稳定，因为它的迭代周期会比较长，比如 4 周迭代一次；而越上层的业务功能，需求变更就越频繁，因为它的迭代周期会比较短，比如 1 周迭代一次。由于单体服务不能分开发布，所以在业务功能迭代的时候，底层核心功能也必须频繁地发布，这对于稳定性来说是一个考验。经过仔细分析，我们会发现上面三个方面的本质问题，都是因为我们的业务是一个单体应用，不能按资源类型进行分别扩容，不能按功能或者服务进行小范围的部署，也不能按业务的需求来选择更适合的研发语言和生态等，所以我们决定按资源和业务等维度对单体服务进行拆分。 服务注册发现的业务场景 这个时候，我们会遇到一个新的问题：之前所有的功能都在一个服务里面，不同模块和功能之间直接通过本地函数进行调用，拆分为多个服务后，怎么调用其他服务的函数呢？你肯定能很快想到，通过 REST API 或者 RPC 来进行跨服务的调用。的确，这是个非常好的办法，但是通过 REST API 或者 RPC 都需要知道被调用服务的 IP 和 Port。所以，我们还需要解决一个问题：如果服务 A 需要调用服务 B，那么服务 A 怎么获取被调用服务 B 的 IP 和 Port 呢？这个其实就是服务注册发现的业务场景。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:8:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"服务注册发现的关键问题是什么 我们先一起来讨论一下可以尝试哪些可行的方式。首先，最容易想到的方式是配置 IP 和 Port 列表，即直接在服务 A 的配置文件中配置服务 B 的 IP 和 Port，如果服务 B 有多个实例，那么就配置一个列表。 这样的确解决了问题，但是如果服务 C、D、E 等非常多的服务，都需要调用服务 B，那么这些服务都需要维护服务 B 的 IP 和 Port 列表。每一次当服务 B 增加、删除一个实例，或者一个实例的 IP 和 Port 发生改变时，所有调用服务 B 的服务都需要更新配置，这是一个非常繁杂并且容易出错的工作，那么怎么避免这个问题呢？ 其实，我们可以将配置 IP 和 Port 列表的方式修改为配置域名和 Port，即在服务 A 的配置文件中不再配置服务 B 的 IP 和 Port 列表，而是配置服务 B 的域名和 Port。这样可以通过域名解析获得所有服务 B 的 IP 列表，让所有的服务 B 都监听同一个 Port。 当服务 B 的实例有变更，不论有多少个服务调用服务 B，只需要修改服务 B 的域名解析就行了，这样就解决了配置分散到各个调用服务，导致配置一致性的问题。但是如果服务 B 的某个实例出现了崩溃、网络不通等情况时，服务 A 在对服务 B 的域名做 DNS 解析时，会因为我们不能实时感知服务实例的状态变更，依然获得该实例的 IP，从而导致访问错误。这里我们举一个租房中介的例子来说明一下。假设每一个要租 A 小区房子的人，都需要亲自去 A 小区获得租房的信息，同样，如果还想租 B 小区的房子，也需要亲自去 B 小区获得租房的信息，这是一个非常麻烦的事情。而更麻烦的是，一个小区的租房信息有变化了，之前获得信息的人都不会立刻知道，非常影响我们的租房效率和成功率。这个时候，租房中介出现了，他每天去各个小区收集租房信息，我们需要租房的时候，直接联系中介就可以获得相关小区的租房信息，并且，中介会记录谁关心哪一个小区的租房信息。如果一个小区的租房信息有变化，中介会主动通知给关心这个小区的人，这样就让租房这件事情变得非常高效了。这里的租房中介，其实就是承担租房信息的注册和发现的功能。所以，经过前面的讨论，我们可以得出服务注册发现需要解决的两个关键问题： 统一的中介存储：调用方在唯一的地方获得被调用服务的所有实例的信息。状态更新与通知：服务实例的信息能够及时更新并且通知到服务调用方。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:8:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"怎么实现服务注册发现 接下来我们就一起来讨论“统一的中介存储”和“状态更新与通知”这两个关键问题的解决办法。 如何选择适合的中介存储 “中介存储”这个问题，其实是我们在解决服务注册发现的时候，引入的一个中间层。“计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决”，这一经典论断又一次被验证了。 我们需要找一个外部存储来做解决问题的中间层，但是基于服务注册发现的场景，我认为这个存储需要有以下几个特点： 可用性要求非常高：因为服务注册发现是整个分布式系统的基石，如果它出现问题，整个分布式系统将不可用。性能要求中等：只要设计得当，整体的性能要求还是可控的，不过需要注意的是性能要求会随分布式系统的实例数量变多而提高。数据容量要求低：因为主要是存储实例的 IP 和 Port 等元数据，单个实例存储的数据量非常小。API 友好程度：是否能很好支持服务注册发现场景的“发布 / 订阅”模式，将被调用服务实例的 IP 和 Port 信息同步给调用方。 基于上面对所需求存储系统特点的分析，我们一起来对常见的存储系统做一个系统性的比较： 通过上面的分析，我们可以看到，这些存储系统几乎都能用来作为服务发现的中介存储系统，但是基于整体考虑，MySQL 和 Redis 在高可用性和 API 友好程度上不满足要求，所以更合适的存储系统为 etcd、ZooKeeper 和 Eureka。如果你希望在系统出现网络分区的时候，调用方一定不能获取过期的被调用服务实例信息，那么就选择 etcd 和 ZooKeeper，但是在被分区的部分网络中，可能出现因为不能获取被调用服务实例信息，而导致请求失败的情况。 如果你认为获取过期的实例信息，可能比完全不能获取被调用服务的实例信息要好，那么就选择 Eureka。毕竟大部分情况下，信息并没有过期，因为被调用服务的实例配置还没有发生变更，并且就算获得的信息过期了，也只是导致一次请求失败。 怎么做服务状态的更新与通知 对于“状态更新与通知”这个问题，我们可以将其分解为两个问题解决： 首先是服务的状态更新，即服务注册：如上图中的 1，服务的每一个实例每隔一段时间，比如 30 秒，主动向中介存储上报一次自己的 IP 和 Port 信息，同时告诉中介存储这一信息的有效期，比如 90 秒。这样如果实例一直存活，那么每隔 30 秒，它都会将自己的状态信息更新到中介存储。如果实例崩溃或者被 Kill 了，那么 90 秒后，中介存储就会自动将该实例的信息清除，避免了实例信息的不一致。所以这里的数据同步是最终一致性的。然后是服务的状态通知，即服务发现：如上图中的 2，服务的调用方通过中介存储监听被调用服务的状态变更信息。这里可以采用“发布 / 订阅”模式，也可以采用轮询模式，比如每 30 秒去中介存储获取一次。所以这里的数据同步也是最终一致性的。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:8:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"选择 AP 还是 CP 根据上面的讨论，从服务注册发现的场景来说，我认为 Eureka 之类的 AP 系统更符合要求。因为服务发现是整个分布式系统的基石，所以可用性是最关键的设计目标。并且上面介绍的服务，在同步自己的状态到中介存储，以及调用方通过中介存储区获得服务的状态，这两个过程中的数据同步都是最终一致性的。既然服务注册发现系统整体是一个 AP 系统，那么将中介存储设计为 CP 系统，去放弃部分的可用性是不值得的。到这里，服务注册发现的基本原理就介绍完了。当我们去研究各种各样服务发现的实现方式时，就会发现其实它们都是在解决“如何选择适合的中介存储”和“怎么做服务状态的更新与通知”的问题。当然由于服务发现是非常基础和重要的功能，所以其中的各种实现都是在高性能、高可用性的基础上解决上面的两个问题，做着各自的优化与权衡。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:8:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 到这里，我们一起完整地讨论了分布式系统中，一个非常关键的组件“服务注册发现”。我们一起来总结一下这节课的主要内容。 首先，我们一起讨论了为什么会对单体服务进行拆分，主要有成本、效率和稳定性三个维度的原因。然后，在将单体服务拆分后，之前很方便的本地函数调用变成了跨实例或者跨机器的远程调用。这个时候，调用方需要知道被调用方的 IP 和 Port 等信息。接着，我们发现 IP 和 Port 信息列表手动配置存在配置分散，无法统一管理的问题，在调用方变多之后，将会变得无法维护。域名 和 Port 信息的手动配置需要解决了配置统一管理的问题，但是如果实例出现突发的异常情况，将无法通知到调用方，导致故障。最后，我们讨论了通过中介存储做服务发现的方式，其中最关键的是对于中介存储的选择问题。而且在服务发现的场景里面，高可用性是最应该去考虑的设计指标，所以选择 AP 系统做中介存储是一个不错的选择。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:8:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"05｜负载均衡：从状态的角度重新思考负载均衡 通过学习“注册发现”的内容，你已经明白了分布式系统为什么需要注册发现组件，也知道了在实现注册发现时要注意的两个关键点，并且理解了从 CAP 理论的角度来说，注册发现是一个 AP 模型。如果我们想把极客时间这个单体服务，改造成一个分布式系统，那么这些内容都将为我们打下一个良好的基础。同时，极客时间为了实现系统的高可用和高性能，它所有的服务都会部署多个实例，那么这就会导致在极客时间的后端系统，调用方通过注册发现组件，去获得被调用服务实例的网络地址时，获取到包含多个服务实例的网络地址列表。这时你将面临一个新的问题，那就是调用方应该将请求，发向被调用服务的哪一个服务实例呢？ 在本节课里，我们就一起来解决分布式系统中，多个被调用服务实例的选择问题，即负载均衡策略。我们会先从负载均衡在架构设计中需要考虑的关键点出发，根据负载均衡策略是否关心请求中携带的信息，即请求是否有状态，将负载均衡策略分为无状态的负载均衡、半状态的负载均衡和全状态的负载均衡，从状态的角度来重新思考。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:9:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"负载均衡的关键点 每一个被调用服务（后面简称为后端服务）都会有多个实例，那么服务的调用方应该将请求，发向被调用服务的哪一个服务实例，这就是负载均衡的业务场景。关于如何解决这个问题，我们可以换一个角度，站在被调用服务实例（后面简称为后端实例）的角度理解负载均衡。对于后端实例组来说，负载均衡就是一个调度器，它将发送给被调用服务的每一个请求，按一定的策略分配给后端实例组中的一个实例，确保能高效、正确地提供服务。 根据上面的讨论，我们可以得出，负载均衡需要达到的目的是“确保能高效、正确地提供服务”，同时从这个目的中，我们还可以分析出负载均衡的两个关键点。首先，我们结合“高效地提供服务”这个目的来分析。如何高效地提供服务，我认为可以理解为后端实例组多个实例的资源运行效率问题。负载均衡需要考虑到各个实例性能差异的情况，让每一个实例都能充分发挥它的能力，不要出现一些实例负载比较高，而另一些实例的负载却非常低的情况，这样会造成资源浪费。所以，我们从中可以得出，负载均衡的第一个关键点是公平性，即负载均衡需要关注被调用服务实例组之间的公平性，不要出现旱的旱死，涝的涝死的情况。 接着，我们来讨论一下“正确地提供服务”这个目的。如何正确地提供服务，我认为这是后端服务对外表现出的整体结果。负载均衡需要确保外部对后端服务的请求，一定能被路由到可以提供正确服务的实例上。如果后端实例是有状态的，比如需要利用本地缓存和存储来处理请求的，我们就需要考虑每个请求携带的状态，然后依据状态信息，将请求正确路由到后端的实例上。 从这里我们可以得出，负载均衡的第二个关键点是正确性，即对于有状态的服务来说，负载均衡需要关心请求的状态，将请求调度到能处理它的后端实例上，不要出现不能处理和错误处理的情况。 我们已经讨论出了负载均衡的两个关键点：公平性和正确性。所以，在后面讨论负载均衡各种不同的策略时，我们将采用公平性和正确性这两个维度，来评价每一种负载均衡策略的具体情况。为了更好地实现负载均衡的公平性和正确性，针对各种不同的业务场景，出现了多种不同的策略。在这些不同的业务场景中，我认为对负载均衡策略的设计，影响最大的因素是后端实例是否存在状态，后端实例有状态，负载均衡就需要关心请求的状态。 如果一个有状态的请求，被路由到错误的后端实例上，将会导致请求无法处理或者获得错误的结果。比如一个查询用户年龄的请求，如果负载均衡策略将该请求，错误地路由到一个没有存储该用户年龄数据的实例上，那么这个实例就只能返回 not found 。对于有状态的请求，如果路由错误，就会影响负载均衡的正确性。因此我们会在下文中，依据负载均衡是否关心请求的状态，将负载均衡策略分为无状态的负载均衡、半状态的负责均衡和全状态的负载均衡，结合负载均衡的两个关键点一一进行分析。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:9:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"无状态的负载均衡 无状态的负载均衡是我们日常工作中接触最多的负载均衡模型，它指的是参与负载均衡的后端实例是无状态的，所有的后端实例都是对等的，一个请求不论发向哪一个实例，都会得到相同的并且正确的处理结果，所以无状态的负载均衡策略不需要关心请求的状态。到这里，你可能会有一个疑问，这些无状态实例难道不能处理像存储数据这样的状态吗？如果需要处理状态应该怎么办呢？这是一个很好的问题，答案也非常简单。 实例将这些状态信息的处理都交给一个中心存储来负责，比如 MySQL 数据库和 Redis 缓存等，实例不在本地机器的磁盘或者内存中，存储任何状态信息。这是一个非常好的设计原则，让专业的中心存储来处理状态信息，大大简化了系统的设计。下面我们以轮询和权重轮询来举例，先讲一讲它们的负载均衡策略，再结合公平性和正确性这两个关键点，评价无状态的负载均衡策略的具体情况。 轮询 轮询的负载均衡策略非常简单，只需要将请求按顺序分配给多个实例，不用再做其他的处理。例如，轮询策略会将第一个请求分配给第一个实例，然后将下一个请求分配给第二个实例，这样依次分配下去，分配完一轮之后，再回到开头分配给第一个实例，再依次分配。轮询在路由时，不利用请求的状态信息，属于无状态的负载均衡策略，所以它不能用于有状态实例的负载均衡器，否则正确性会出现问题。在公平性方面，因为轮询策略只是按顺序分配请求，所以适用于请求的工作负载和实例的处理能力差异都较小的情况。 权重轮询 权重轮询的负载均衡策略是将每一个后端实例分配一个权重，分配请求的数量和实例的权重成正比轮询。例如有两个实例 A，B，假设我们设置 A 的权重为 20，B 的权重为 80，那么负载均衡会将 20% 的请求数量分配给 A，80 % 的请求数量分配给 B。权重轮询在路由时，不利用请求的状态信息，属于无状态的负载均衡策略，所以它也不能用于有状态实例的负载均衡器，否则正确性会出现问题。在公平性方面，因为权重策略会按实例的权重比例来分配请求数，所以，我们可以利用它解决实例的处理能力差异的问题，认为它的公平性比轮询策略要好。 无状态的负载均衡策略除了上面的两种外，还有 FAIR 、随机、权重随机和最少链接数等策略，你可以从两个关键点出发对这些负载均衡策略进行分析。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:9:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"半状态的负载均衡 半状态的负载均衡指的是，虽然负载均衡策略利用请求的状态信息进行路由，但是仅仅进行简单的规则处理，比如 Hash 运算加求模来路由请求，它不保证路由的正确性，这个正确性由后端实例来保证。另外，一些实例会在内存中缓存一些状态数据，用于提升系统的性能，如果一个请求被路由到错误的实例中，该实例可以立即通过中心存储，读取出所需要的数据，然后在内存中重建并缓存正确的处理请求，不会导致请求出现错误。而对于路由错误，后端实例不能恢复状态数据的场景，后端节点需要适应路由策略来保证数据的正确性，例如基于 Hash 策略路由的 MySQL 集群，如果集群的数目发生变更，我们需要通过数据迁移来保证路由的正确性。 所以，我们可以看出，半状态的负载均衡将请求按一定的策略进行路由，后端实例可以利用路由规则来进行优化。假设后端实例在进程里面缓存用户的信息，如果我们能将同一个用户的多个请求，都路由到同一个实例上，相对于轮询策略，单个实例不需要缓存全部的用户信息，可以大大减少缓存的内存容量。 为了评价半状态的负载均衡策略的具体情况，我们以 Hash 和一致性 Hash 来举例。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:9:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"Hash Hash 负载均衡策略是指将请求的状态信息，按一定的 Hash 算法固定分配到一个实例上，例如，按请求的来源 IP 地址或者用户的 ID，将同一个来源 IP 地址或者用户 ID 的请求固定到一个实例上。我们来举个例子，如果有两个实例，我们想将相同用户 ID 的请求，固定分配到一个实例上面，那么按如下的方法来计算： $i =MD5(ID)%2$ 这里要说明一下，公式中的 2 为实例的数量，除了 MD5 外，我们还可以使用不同的 Hash 算法。我们将实例从 0 开始编号，上面公式的计算结果 i 为负载均衡将要分配实例的编号。 从这个计算公式中，我们可以看出 Hash 负载均衡策略，在机器实例数量发生变化的时候，几乎所有请求的分配实例都会发送变化。如果后端实例依赖 Hash 负载均衡策略来保证正确性，那么当实例数发生变化的时候，正确性将会出现问题。对于 Hash 策略是如何保证正确性的具体内容，在后面“数据分片”的课程中，我们将会继续讨论。公平性方面，在不考虑 Hash 算法均匀性的情况下，Hash 策略会按 Hash 值按模等分，它和轮询策略类似，不能解决请求的工作负载和实例的处理能力差异的问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:9:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"一致性 Hash Hash 的负载均衡策略中，最大的一个问题是基于机器数量求模，如果机器数量发生变化，请求和实例的分配关系机会将全部变化，这会影响它的正确性，而一致性 Hash 就可以用来解决这个问题，你可以结合下图来理解： 假设我们定义 Hash 环的空间大小为 $2^32$，那么我们先将 0 ~ $2^32$ 均匀地分配到上图的 Hash 环上，将所有的实例按其唯一标识（例如名字的字符串 “ Node A ”）计算在环上的位置： $iNode=hash( Node ID) %2^32$ 然后，对于每一个请求，我们也按上面的方法计算其在环上的位置： $iRequest=hash(Request ID) %2^32$ 最后，按请求在环上的位置沿环顺时针“行走”，遇到的第一个服务器节点，就是该请求负载均衡分配的节点。这里要注意的是，“键 5 ”沿环顺时针“行走”到环的结尾，如果还没有找到服务器节点，将从环的开头继续找，直到找到 Node A 。你可以看到，一致性 Hash 和 Hash 策略最大的区别在于，一致性 Hash 是对固定值 232 求模，不会随着机器数量的变化而变化，所以对于同一个 Request ID ， iRequest 是始终稳定不变的，这样就解决了 Hash 的策略在实例数量发送变化后，几乎所有的分配关系都会发生变化的问题。如果一致性 Hash 的机器数量发生变化后，会出现什么问题呢？其实就是发生变化的实例节点逆时针方向的一些请求的路由实例会发生改变，例如 Node A 下线了，那么“键 5 ”将被路由到 Node B ，如果在“键 5 ”和 Node B 之间新增了一个节点，那么“键 5 ”将路由到新增的节点。那么关于一致性 Hash 策略如何保证正确性的问题，我们也是在后面的“数据分片”课程中详细讨论。到这里，你是不是觉得一致性 Hash 能在后端实例数量变化的时候，依然保持比较好的正确性，已经很完善了呢？ 其实还有一个问题，那就是公平性，这里有两点需要我们注意。首先，如果后端实例数非常少，公平性将会出现问题，假设上图中只有 Node B 和 Node C ，那么 Node B 将要承担 70% 以上的请求；其次，如果各个节点的性能差异比较大，这样的情况我们会希望能按权重来进行分配。关于一致性 Hash 策略公平性的问题，一致性 Hash 是通过增加虚拟节点的方法来解决的，在 Hash 环中路由到虚拟实例的请求，会被路由到它的真实实例上，比如下图中“键 1”和“键 3”的请求将路由到 Node A。 对于实例数过少导致的公平性问题，一致性 Hash 策略让每一个实例都生成多个虚拟实例，使分配更加均衡；对于实例之间性能差异的问题，一致性 Hash 策略通过让实例生成虚拟实例的数量，与该实例的权重成正比的策略来解决。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:9:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"全状态的负载均衡 全状态的负载均衡是指，负载均衡策略不仅利用请求的状态信息进行路由，并且在后端实例有状态的情况下，依然会保证路由的正确性。那它是怎么做到的呢？下面我们就来讨论一下全状态负载均衡的实现。全状态的负载均衡一般以路由服务的形式存在，在路由服务里面，都会存储后端实例 ID 和状态信息的索引，在进行请求路由的时候，路由服务从请求的状态信息中获得索引的标识，通过查询索引获得后端实例的 ID，然后再进行路由。 如果你了解过“数据分片”机制，你就会发现它和全状态的负载均衡非常类似，其实它们就是一个事情，只是我们讨论的角度不同。如果我们从请求调度的角度来讨论，这就是一个全状态服务的负载均衡问题，如果我们从后端实例数据分布的角度来讨论，这就是一个数据分片的问题。那么关于全状态的负载均衡策略，我们将放到后面的“数据分片”课程中进行讨论，这里就先不再赘述了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:9:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 到这里，我们一起讨论了分布式系统场景下的负载均衡问题，一起来总结一下这节课的主要内容：首先，我们通过对负载均衡业务场景的讨论，确定了评价负载均衡策略的关键点：公平性和正确性，以后当我们碰到负载均衡策略选型的时候，可以通过公平性和正确性来进行讨论。然后，我们讨论了后端为无状态实例，常用的无状态的负载均衡策略：轮询、权重和 FAIR 等，学完这部分，你可以为无状态实例来选择合适的负载均衡策略。接着，我们讨论了后端实例有状态，但是正确性不需要由负载均衡策略来保证的半状态负载均衡策略，常用的半状态的负载均衡策略有：Hash 和一致性 Hash 等，这里我们就知道了，怎么利用负载均衡策略的特点，优化后端服务的性能。最后，我们讨论了全状态的负载均衡策略，其实全状态的负载均衡和数据分片是同一件事情，只是我们讨论的角度不一样而已，你会发现负载均衡和数据分片之间是有非常多的交集的。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:9:7","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"06｜配置中心：如何确保配置的强一致性呢？ 通过学习“负载均衡”的内容，你知道了怎么评价一个负载均衡策略，以及针对不同的业务场景，应该怎么选择合适的负载均衡策略。现在，你已经能够顺利地解决分布式系统中，服务实例的选择问题，恭喜你又前进了一大步。但是，随着极客时间分布式架构的逐渐演进，之前的单体服务慢慢被拆分为越来越多的服务，虽然拆分后的架构对公司研发的成本、效率和稳定性方面有着非常大的改进，可是你在系统运维的时候，特别是管理系统配置的时候，却发现效率越来越低了，并且还经常会出现因为配置问题导致的故障。可能你很快就能想到这个问题产生的原因，因为在目前的分布式架构迭代过程中，极客时间的后端系统由之前单体架构的一个服务，被拆分成了多个服务，并且服务的数量还在继续增加。我们管理 1 个服务的配置是很轻松的，但是用管理 1 个服务配置的方法，来管理 10 个、20 个甚至更多的服务配置，效率一定是非常低的，并且也避免不了出错。虽然能想到原因，但是真正处理时，却不知道怎么做，你是不是也有这样的疑问呢？不要担心，在这节课中，我将和你一起来讨论在分布式系统中，我们应该怎么管理服务配置信息？我们从分布式场景下，手动管理配置的问题出发，思考为什么需要配置中心，然后进一步讨论配置中心需要具备的功能，接着从存储系统的选择，配置信息的同步这两个方面，来结合业务场景实际讨论，解决如何实现配置中心的问题，最后再探讨一下，需要配置同时生效的场景下，如何确保配置信息的强一致性。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:10:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么需要配置中心 在思考配置问题之前，我们先讨论一下单体服务架构是怎么管理配置的，如果直接使用单体服务的方式来处理分布式系统的配置，将会出现什么样的问题，从而引出解决配置管理问题的高效方法——配置中心。单体服务架构的场景下，一般是将配置信息视为代码的一部分，工程师会像编辑代码一样，编辑好配置，然后通过发布系统，将配置发布到服务程序所在的机器上，接下来，程序会通过加载本地存储上的配置文件，使配置生效。在单体架构下，这个配置即代码的方法能够很好地运行，但是在分布式架构下则会出现以下几个问题。 首先，这种方法缺乏整体的配置管理平台，会使配置管理的效率变得很低。单体服务的架构只有一个服务，不需要用全局视角来管理配置，但是在分布式系统中，如果将配置信息视为代码的一部分，会导致不同服务的配置文件，出现在不同的代码仓库中。当我们需要检索和查看多个服务的配置时，需要在一个个代码仓库中查找，效率会非常低。 其次，这种方法会导致实例之间的配置出现不一致的情况。其实在单体架构下，也会有这个问题，不过整个单体系统只有一个服务，通过人工来保证实例之间配置一致是比较简单的。但是在分布式系统中，随着服务的增加，想要靠人工来保障是不可能的。因为配置是随着程序一起发布的，每一个实例都会加载本地机器上存储的配置信息，如果配置文件有人为修改或其他故障时，会因为多实例之间的配置信息不相同，出现实例之间的行为不一致性的情况，进而出现各种奇怪的问题。 最后，配置即代码的方法会使配置修改的操作，变得非常冗余和低效，这个问题同样存在于单体架构中。由于每一次的配置修改，都需要走一次完整的代码发布流程，所以工程师都需要从服务的代码仓库中找到配置文件，在对配置文件进行修改后，提交修改到代码仓库，然后通过发布系统进行发布，最后程序会通过重启或者热更新的方式加载配置。其中，只有修改配置文件和发布配置文件这两个操作是必须的，其他的流程都和配置修改无关。 结合上面的分析你会发现，配置即代码的配置管理方式有非常多的问题，那么我们能不能直接手动管理配置呢？其实从操作上来说是可以的，比如你登录到每一台机器上手动修改，然后再让程序加载，重新配置文件即可。但是这样一来，每一次服务的配置修改，都需要修改该服务的所有实例的配置，效率又低又容易出错。如果你的操作稍微有一点失误，就会导致同一个服务中，多个实例的配置信息直接不一致了。而且这样的操作，还会导致配置文件的修改没有历史记录，如果出现了当前配置文件错误的问题，需要回滚到上一个版本的时候，就麻烦了。那么，到底怎么能更高效、更准确地解决分布式系统的配置管理问题呢？一般来说，在分布式系统中，如果一个问题的影响半径超出单一服务的范围，就可以考虑通过引入一个中间层的方法来解决，即“计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决”这个经典论断。它会经常出现在我们的课程中，帮助你培养解决问题的高效思路。 **所以，在解决分布式系统的配置管理问题时，我们也来引入一个中间层，把这个中间层称之为配置中心。**引入配置中心这个高效的解决方法之后，我们可以进一步地讨论一个理想的配置中心应该是什么样子的，这样你就知道在建设配置中心时，我们需要注意什么样的关键点。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:10:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"配置中心需要具备哪些功能 在解决问题之前，应该先定义好问题。所以我们在讨论配置中心的具体实现之前，先来定义一下，什么是配置中心，具体来说就是配置中心应该要具备哪些功能？我们可以结合上文中，配置即代码的方法在分布式系统中面临的三个问题，推导出在分布式系统的架构下，一个理想的配置中心应该具备哪些特点。 首先，这个配置中心，能够统一管理分布式系统所有服务的配置信息。那么研发工程师就可以在配置中心上，便捷地全局搜索和查看每一个服务的配置信息，而不是看到所有服务的配置信息都散落在不同的地方。更进一步来说，配置中心需要能统一存储和管理整个分布式系统的所有配置文件。其次，配置中心里，同一个服务实例之间的配置应该保持一致。也就是说，配置中心需要保证一个服务所有的实例，都加载同一份配置文件，而不是每一个实例维护一个配置文件的副本。这就需要配置中心统一去管理，服务当前版本的配置，并且服务的实例通过网络去配置中心，获得当前的配置信息，确保 Single Source of Truth ( SSOT )。最后，这个配置中心应该能高效地修改配置。研发工程师只需要关心，并且高效地完成配置的修改、发布和回滚操作，而其他的就不需要研发工程师手动来操作了，比如配置文件的版本管理等，这些都由配置中心来自动完成。 经过前面的讨论，我们结合这节课开头提到的配置中心的业务场景，可以总结出配置中心需要解决的两个关键问题： 统一的配置存储：一个带版本管理的存储系统，按服务的维度，存储和管理整个分布式系统的配置信息，这样可以很方便地对服务的配置信息，进行搜索、查询和修改。配置信息的同步：所有的实例，本地都不存储配置信息，实例能够从配置中心获得服务的配置信息，在配置修改后，能够及时将最新的配置，同步给服务的每一个实例。 那么到这里，你会发现配置中心和服务的注册发现机制是非常类似的，唯一不同的地方是服务注册发现所存储的服务实例的 IP 和 Port 等信息，是服务实例自己注册的，并且会设置过期时间，随着实例上线时主动写入，下线后会因为过期而被删除。但是配置中心的配置信息是研发工程师主动写入的，并且不会设置过期时间。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:10:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何实现配置中心 我们确定了“统一的配置存储”和“配置的更新与同步”这两个关键问题，并且还发现了配置中心与服务的注册发现机制之间的相似性，掌握了这些信息，我们接下来就可以思考，如何实现配置中心的解决方法了。关于如何实现配置中心，我们首先结合“统一的配置存储”这个关键点来分析，可以从“如何选择合适的存储系统”的角度来思考解决方法；然后再从“如何做配置信息的同步”的角度，讨论“配置的更新与同步”这个关键点。 如何选择合适的存储系统 与服务注册发现类似，实现配置中心也需要找一个外部存储，来做配置中心的统一存储。通过对配置中心的场景分析，我认为配置中心对存储系统的要求主要为以下几点： 可用性要求非常高：因为配置中心和服务注册发现一样，是整个分布式系统的基石，如果配置中心出现问题，整个分布式系统都将出现非常严重的问题。性能要求中等：只要设计得当，整体的性能要求还是可控的，不过需要注意的是，性能要求会随分布式系统的实例数量变多而提高。数据容量要求低：配置中心是用来存储服务的配置信息的，一般来说，服务的配置信息都非常小，如果出现非常大的配置，一般也不当成配置来处理，可以将它放到外部存储上，在配置中配置下载的链接。API 友好程度：是否能很好地支持配置中心场景的“发布 / 订阅”模式，将服务的配置信息及时同步给服务的实例。 基于上面对所需求存储系统特点的分析，我们一起来对常见的存储系统做一个系统性的比较，由于注册发现和配置中心类似，所以我们使用第 4 节课“注册发现”中的这张图片，从配置中心的角度进一步分析： 通过上面的分析，我们可以看到，MySQL 和 Redis 在高可用性和 API 友好程度上不满足要求，而 etcd、ZooKeeper 和 Eureka 这三个存储系统中，更适合的是 Eureka。下面我们来讨论一下，为什么 Eureka 这样的 AP 系统要比 etcd 和 ZooKeeper 这样的 CP 系统更合适。 如果我们选择 etcd 和 ZooKeeper，那么出现网络分区的时候，在网络分区的少数派节点一侧，配置中心将不能提供服务，这一侧的服务实例也就不能通过配置中心获取配置，这时如果有实例的重启等操作，就一定会发生故障。 如果选择 Eureka，那么配置中心这个整体，依然可以正常提供服务，唯一的问题是，如果这时有配置的更新，那么同一个服务中不同实例的配置可能会不一致，但是这个问题并不是最关键的，主要原因有两个。 首先，即使配置中心内部是强一致性的，但是配置中心和服务实例之间是通过网络同步配置的，而网络的时延是不确定的，这会导致配置信息同步到实例的时间有先有后，不能同时到达，使得配置中心和同一服务多实例之间的配置，同步退化到最终一致性。其次，配置修改的频率是非常低的，而且因为是人工操作，所以在出现网络分区的时候，如果我们不去修改配置，那么 Eureka 上多个副本的数据就是一致的。 如何做配置信息的同步 讨论完“如何选择合适的存储系统”，我们接着讨论配置中心的另一个关键点“如何做配置信息的同步”，对于这个问题，我们可以将其分解为两个问题解决，具体操作如下图： 首先，实例刚启动的时候，主动去配置中心获取完整的配置信息，即首次同步：如上图中的 1，服务的每一个实例启动后，通过服务的唯一标识，去配置中心获取服务的所有配置，然后加载配置，完成实例的启动流程。然后，在实例的运行过程中，如果服务的配置有修改，配置中心需要及时同步到实例，即变更同步：如上图中的 2 和 3，服务的配置信息有变更后，配置中心监听到服务的配置修改了，需要及时通知到服务的所有实例。这里可以采用“发布 / 订阅”的模式，也可以采用轮询模式，比如每 30 秒去配置中心查询一下，配置是否有变更。这里的数据同步是最终一致性的。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:10:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何确保配置的强一致性 通过上面的讨论，我们知道了怎么来实现一个配置中心，并且知道了配置中心和服务实例之间的配置同步是最终一致性的。这时候你可能会有一个疑问，有没有一些业务场景必须要求，同一服务的多个实例之间的配置信息同时生效呢？如果有的话，应该怎么来保证呢？所以，我们最后来讨论一下，在需要配置同时生效的场景下，如何确保配置信息的强一致性。确实有这样的场景，我们通过一个例子来分析一下。因为这部分只讨论配置强一致性的问题，所以这个数据迁移的例子，不会涉及整个数据迁移的完整流程。假设有一个分布式存储系统，如下图所示，我们现在需要通过配置信息，发送数据迁移指令，将数据集 2 从存储节点 1 迁移到节点 2 上。 在这个例子中，如果 Proxy 实例之间，对数据迁移的配置信息没有同时生效，将会导致什么样的异常情况呢？从上图可以看出，在进行数据迁移前，Proxy 对数据集 2 的读写请求，都会路由到存储系统 1 上。我们通过配置中心，配置好数据迁移的配置后，如果 Proxy 1 已经加载了数据迁移的配置，Proxy 2 还没有接收到数据迁移的配置，那么在处理数据集 2 的请求时，就会出现 Proxy 1 读写存储节点 2，Proxy 2 读写存储节点 1 的情况，导致数据不一致的问题，反过来也是一样的。那么我们应该怎么来解决这个问题呢？其实这是一个共识问题，需要所有的 Proxy 实例对数据迁移的配置达成共识后，才能进行迁移。而配置中心和多实例的配置同步，是通过网络来完成的，不是一个强一致性的模型，所以，我们不能简单依赖配置中心的配置同步来解决。 我们可以使用这样的解决思路，配置信息不能按上面讨论的方式直接通过网络进行同步，而需要通过类似两阶段提交的方式来解决这个问题。这里我们主要讨论处理这个问题的思路，不展开故障处理的情况，有了这个思路，后面你就可以处理多节点数据一致性和共识相关的问题了。 首先，从配置中心的存储节点中选择一个实例作为协调者 A。在投票阶段，协调者 A 向所有的 Proxy 节点发送 Prepare 消息，即数据迁移的配置信息，Proxy 节点在收到数据迁移配置后，确认自己当前的状态是否可以执行数据迁移工作。如果可以，那么就阻塞当前节点所有的读写操作，进入 Prepare 状态，并回复协调者 A 同意执行数据迁移，否则回复不同意执行数据迁移。那么这里要注意一点，为了数据的一致性，我们放弃了一定的可用性，Prepare 状态下的 Proxy 节点相当于被锁住，不能进行读写操作了。在执行阶段，协调者 A 收集所有的 Proxy 节点的反馈，如果所有的 Proxy 都同意执行数据迁移，那么协调者 A 向所有的 Proxy 节点发送 Commit 消息，Proxy 节点收到 Commit 消息后，就应用数据迁移的配置信息，按最新的配置信息，接受读写请求，进行数据迁移。上文的例子，就是对于数据集 2 的读写请求，都路由到节点 2 来处理，否则就发送 Rollback 消息，Proxy 节点收到后，回滚状态，重新接受读写请求。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:10:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 到这里，我们一起完整地讨论了分布式系统中，一个非常关键的组件“配置中心”，我们一起来总结一下这节课的主要内容。首先，我们一起讨论了为什么需要配置中心，主要有统一配置管理、同一个服务实例之间的配置一致性和配置修改效率这三个方面的原因。然后，我们分析了一个理想的配置中心，应该具备什么功能，从中总结出配置中心的两个关键点：统一的配置存储和配置信息的同步。接着，讨论了对于配置中心的业务场景来说，选择一个 AP 模型的存储系统是最优的方案，并且知道了应该如何做配置信息的同步。最后，我们通过配置信息需要强一致性的例子，介绍了一个类似两阶段提交的方式，来实现强一致性的配置发布。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:10:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"07｜分布式锁：所有的分布式锁都是错误的？ 通过学习“配置中心”的内容，你已经理解了在分布式系统中，为什么需要配置中心，以及怎么去实现一个设计良好的配置中心，现在，你终于不用再为管理极客时间后端各种服务的配置而烦恼了，这是一件值得高兴的事情。但是，在极客时间后端系统快速迭代的过程中，你发现了一个服务中的代码逻辑问题：在有些场景下，你并不想让所有的实例都一起运行，只需要一个实例运行就够了，比如在用户生日当天，给用户发的祝福短信等类似定时通知的情况。目前同一个服务的所有实例都是对等的，只能每一个实例都运行。如果将这个服务运行的实例修改为一个，虽然能解决刚才讨论的问题，但是这个实例就变成了一个单点，会面临性能瓶颈和单点故障的风险。 这真是一个两难的问题，我们应该如何解决呢？其实，这个问题的本质在于，我们希望同一个服务的多个实例，按照一定的逻辑来进行协同，比如刚才讨论的定时任务的逻辑。那么多个实例在同一时刻只能有一个实例运行，它就是一个典型的分布式锁的场景。 所以，在本节课中，我们将从“为什么需要分布式锁”，“怎么实现分布式锁”和“分布式锁的挑战”这三个层次依次递进，和你一起来讨论分布式锁相关的内容，解决你的困惑。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:11:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么需要分布式锁 在探讨分布式锁之前，我们先来了解一下锁的定义：锁是操作系统的基于原语，它是用于并发控制的，能够确保在多 CPU 、多个线程的环境中，某一个时间点上，只能有一个线程进入临界区代码，从而保证临界区中操作数据的一致性。在我们日常的研发工作中，经常会在进程内部缓存一些状态信息，通过锁可以很方便地控制、修改这些内部状态信息的临界区代码，确保不会出现多个线程同时修改临界区的资源这种情况，防止异常问题的发生。所以，锁是我们研发工作中一个非常重要的工具。其实，我们将锁的定义推广到分布式系统的场景中，也是依然成立的。只不过锁控制的对象从一个进程内部的多个线程，变成了分布式场景下的多个进程，同时，临界区的资源也从进程内多个线程共享的资源，变成了分布式系统内部共享的中心存储上的资源。但是，锁的定义在本质上没有任何的改变，只有持有锁的线程或进程才能执行临界区的代码。 这句话如何理解呢？我们来看看这个例子。在进程内部，多个线程同时修改一个变量，可能会出现多个线程每个都写一部分，导致变量写入冲突的情况发生。那么在分布式系统中，如果多个进程，同时往一个中心存储的同一个位置写入一个文件，同样也会出现文件写入冲突的情况。所以，锁的定义在本质上没有任何的改变。另外，我们从课程开头提到的定时任务代码的例子里，可以知道在同一时间内，临界区只能由一个进程来执行，而只有持有锁的线程或进程才能执行临界区的代码。所以我们可以这样理解，分布式锁是一个跨进程的锁，是一个更高维度的锁。我们在进程内部碰到的临界区问题，在分布式系统中依然存在，我们需要通过分布式锁，来解决分布式系统中的多进程的临界区问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:11:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"怎么实现分布式锁 我认为锁可以分为三个不同的层次，除了我们上面讨论过的，进程内部的锁和跨进程、跨机器之间的分布式锁外，还有介于它们之间的，同一台机器上的多进程之间的锁。进程内的锁，是操作系统直接提供的，它本质上是内存中的一个整数，用不同的数值表示不同的状态，比如用 0 表示空闲状态。加锁时，判断锁是否空闲，如果空闲，修改为加锁态 1，并且返回成功，如果已经是加锁状态，则返回失败，而解锁时，则将锁状态修改为空闲状态 0。整个加锁或者解锁的过程，操作系统保证它的原子性。 对于同一台机器上的多进程之间，我们可以直接通过操作系统的锁来实现，只不过由于协调的是多个进程，需要将锁存放在所有进程都可以访问的共享内存中，所有进程通过共享内存中的锁来进行加锁和解锁。到这里，你应该明白了，对于跨进程、跨机器之间的分布式锁的实现也是同样的思路，通过一个状态来表示加锁和解锁，只不过要让所有需要锁的服务，都能访问到状态存放的位置。在分布式系统中，一个非常自然的方案就是，将锁的状态信息存放在一个存储服务，即锁服务中，其他的服务再通过网络去访问锁服务来修改状态信息，最后进行加锁和解锁。上面讨论的就是分布式锁最核心的原理，不过从分布式锁的场景出发，如果我们想实现一把完备的分布式锁，需要满足以下几个特性，接下来我们就一起来讨论具体怎么实现。 第一个特性就是互斥，即保证不同节点、不同线程的互斥访问，这部分知识我们在上面已经讨论过，就不再赘述了。第二个特性是超时机制，即超时设置，防止死锁，分布式锁才有这个特性。在概述篇的第二节课“新的挑战”中，我们讨论过部分失败和异步网络的问题，而这个问题在分布式锁的场景下就会出现。因为锁服务和请求锁的服务分散在不同的机器上面，它们之间是通过网络来通信的，所以我们需要用超时机制，来避免获得锁的节点故障或者网络异常，导致它持有的锁不能归还，出现死锁的情况。 同时，我们还要考虑，持有锁的节点需要处理的临界区代码非常耗时这种问题，我们可以通过另一个线程或者协程不断延长超时时间，避免出现锁操作还没有处理完，锁就被释放，之后其他的节点再获得锁，导致锁的互斥失败这种情况。对于超时机制，我们可以在每一次成功获得锁的时候，为锁设置一个超时时间，获得锁的节点与锁服务保持心跳，锁服务每一次收到心跳，就延长锁的超时时间，这样就可以解决上面的两个问题了。 第三个特性是完备的锁接口，即阻塞接口 Lock 和非阻塞接口 tryLock。通过阻塞 Lock 接口获取锁，如果当前锁已经被其他节点获得了，锁服务将获取锁的请求挂起，直到获得锁为止，才响应获取锁的请求；通过 tryLock 接口获取锁，如果当前锁已经被其他节点获得了，锁服务直接返回失败，不会挂起当前锁的请求。 第四个特性是可重入性，即一个节点的一个线程已经获取了锁，那么该节点持有锁的这个线程可以再次成功获取锁。我们只需在锁服务处理加锁请求的时候，记录好当前获取锁的节点 + 线程组合的唯一标识，然后在后续的加锁请求时，如果当前请求的节点 + 线程的唯一标识和当前持有锁的相同，那么就直接返回加锁成功，如果不相同，则按正常加锁流程处理。 最后是公平性，即对于 Lock 接口获取锁失败被阻塞等待的加锁请求，在锁被释放后，如果按先来后到的顺序，将锁颁发给等待时间最长的一个加锁请求，那么就是公平锁，否则就是非公平锁。锁的公平性的实现也非常简单，对于被阻塞的加锁请求，我们只要先记录好它们的顺序，在锁被释放后，按顺序颁发就可以了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:11:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"分布式锁的挑战 通过上面的学习，你已经学会了分布式锁的基本原理，不过在分布式系统中，由于部分失败和异步网络的问题，分布式锁会面临正确性、高可用和高性能这三点的权衡问题的挑战。所以，我们接下来讨论一下分布式锁的挑战问题，这样你在以后的工作中，就可以依据业务场景来实现合适的分布式锁了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:11:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"分布式锁的正确性 首先，我们一起来讨论分布式锁的正确性问题。我们在使用分布式锁的情况下，是否有办法做到，不论出现怎样的异常情况，都能保证分布式锁互斥语义的正确性呢？那么这里，我们将从进程内的锁如何保证互斥语义的正确性出发，分析在分布式锁的场景中，部分失败和异步网络同时存在的情况下，是否能确保分布式锁互斥语义正确性的问题。对于进程内的锁，如果一个线程持有锁，只要它不释放，就只有它能操作临界区的资源。同时，因为进程内锁的场景中，不会出现部分失败的情况，所以在它崩溃时，虽然没有去做解锁操作，但是整个进程都会崩溃，不会出现死锁的情况。这里要说明一下，我们讨论出现死锁的情况，不包括业务逻辑层面出现死锁，因为这个与锁本身的正确性没有关系。我们讨论的是与业务逻辑无关的原因，导致的死锁问题，这个是锁自身的问题，需要锁自己来解决。 另一个方面，进程内锁的解锁操作是进程内部的函数调用，这个过程是同步的。不论是硬件或者其他方面的原因，只要发起解锁操作就一定会成功，如果出现失败的情况，整个进程或者机器都会挂掉。所以，因为整体失败和同步通信这两点，我们可以保证进程内的锁有绝对的正确性。 接下来，我们再来用同样的思路，讨论一下同一台机器上多进程锁的正确性问题。在这个情况下，由于锁是存放在多进程的共享内存中，所以进程和锁之间的通信，依然是同步的函数调用，不会出现解锁后信息丢失，导致死锁的情况。但是，因为是多个进程来使用锁，所以会出现一个进程获取锁后崩溃，导致死锁的情况，这个就是部分失败导致的。不过，在单机情况下，我们可以非常方便地通过操作系统提供的机制，来正确判断一个进程是否存活，比如，父进程在获得进程挂掉的信号后，可以去查看当前挂掉的进程是否持有锁，如果持有就进行释放，这可以当作是进程崩溃后清理工作的一部分。 讨论完进程内的锁和同一台机器上多进程锁的正确性问题后，我们还需要考虑到，在分布式锁的场景中，部分失败和异步网络这两个问题是同时存在的。如果一个进程获得了锁，但是这个进程与锁服务之间的网络出现了问题，导致无法通信，那么这个情况下，如果锁服务让它一直持有锁，就会导致死锁的发生。一般在这种情况下，锁服务在进程加锁成功后，会设置一个超时时间，如果进程持有锁超时后，将锁再颁发给其他的进程，就会导致一把锁被两个进程持有的情况出现，使锁的互斥语义被破坏。那么出现这个问题的根本原因是超时后，锁的服务自动释放锁的操作，它是建立在这样一个假设之上的： 锁的超时时间 » 获取锁的时延 + 执行临界区代码的时间 + 各种进程的暂停（比如 GC） 对于这个假设，我们暂且认为“执行临界区代码的时间 + 各种进程的暂停”是非常小的，而“获取锁的时延”在一个异步网络环境中是不确定的，它的时间从非常小，到很大，再到因为网络隔离变得无穷大都是有可能的，所以这个假设不成立。如果你计划让客户端在“获取锁的时延”上加心跳和超时机制，这是一个聪明的想法，但是这可能会导致锁服务给客户端颁发了锁，但是因为响应超时，客户端以为自己没有获取锁的情况发生。这样一来，依然会在一定程度上，影响锁的互斥语义的正确性，并且会在某些场景下，影响系统的可用性。对于这些问题，如果我们获得锁是为了写一个共享存储，那么有一种方案可以解决上面的问题，那就是在获得锁的时候，锁服务生成一个全局递增的版本号，在写数据的时候，需要带上版本号。共享存储在写入数据的时候，会检查版本号，如果版本号回退了，就说明当前锁的互斥语义出现了问题，那么就拒绝当前请求的写入，如果版本号相同或者增加了，就写入数据和当前操作的版本号。 但是这个方案其实只是将问题转移了，如果一个存储系统能通过版本号，来检测写入冲突，那么它已经支持多版本并发控制（MVCC）了，这本身是乐观锁的实现原理。那么我们相当于是用共享存储自身的乐观锁，来解决分布式锁在异常情况下，互斥语义失败的问题，这就和我们设计分布式锁的初衷背道而驰了。所以，我认为对于在共享存储中写入数据等等，完全不能容忍分布式锁互斥语义失败的情况，不应该借助分布式锁从外部来实现，而是应该在共享存储内部来解决。比如，在数据库的实现中，隔离性就是专门来解决这个问题的。分布式锁的设计，应该多关注高可用与性能，以及怎么提高正确性，而不是追求绝对的正确性。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:11:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"分布式锁的权衡 接下来，我们一起来讨论关于分布式锁的高可用、高性能与正确性之间的权衡问题。关于正确性的问题，我们从上面的讨论中，明白了在分布式锁的场景下，没有办法保证 100% 的正确性，所以，我们要避免通过外部分布式锁，来保证需要 100% 正确性的场景，将分布式锁定位为，可以容忍非常小概率互斥语义失效场景下的锁服务。一般来说，一个分布式锁服务，它的正确性要求越高，性能可能就会越低。 对于高可用的问题，我认为它是在设计分布式锁时，需要考虑的关键因素。我们必须提供非常高的 SLA ，因为分布式锁是一个非常底层的服务组件，是整个分布式系统的基石之一，所以一般来说，越底层、越基础的组件，依赖它的功能和服务就会越多，那么它的影响面就会越大。如果它出现了故障，必然会导致整个分布式系统大面积出现故障。 对于高性能的问题，这是一个由业务场景来决定的因素，我们需要通过业务场景，来决定提供什么性能的分布式锁服务。一般来说，我们可以在成本可接受的范围内，提供性能最好的分布式锁服务。如果我们提供的分布式锁服务的性能不佳，一定要在文档甚至接口的名字中体现出来，否则如果被误用的话，可能会导致分布式锁服务故障，系统将出现非常大的事故。 基于以上三点权衡，我们就可以根据业务情况，来实现或者选择自己的分布式锁服务了。其中关于分布式锁服务的存储的选择问题，因为对于主流存储系统的选择与对比，已经在第 4 讲“注册发现”和第 6 讲“配置中心”中讨论过，所以这里就不再赘述了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:11:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 到这里，我们一起讨论了分布式系统场景下的分布式锁的相关问题，接下来我们一起来总结一下这节课的主要内容：首先，我们讨论了单进程内和单节点内进程的临界区问题，并且这个问题在分布式系统中依然存在，那么对于分布式场景下的临界区问题，我们需要用分布式锁服务来解决。其次，我们一起讨论了，怎么实现分布式锁服务的互斥、超时机制、完备的锁接口、可重入和公平性等特性，基于这些知识和原理，我们就可以很轻松地实现自己的分布式锁服务了。最后，我们一起探讨了在分布式场景下的正确性问题，发现分布式场景下，锁服务没有办法保证 100% 的正确性，并且，我们认为可用性是设计分布式锁服务非常关键的一个目标。这样，我们就可以依据不同的业务场景，来设计和权衡我们的分布式锁服务了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:11:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"08｜重试幂等：让程序 Exactly-once 很难吗？ 通过学习“分布式锁”的内容，你已经了解了如何实现一个分布式锁服务，并且知道了在分布式锁的场景下，我们应该如何在正确性、高可用和高性能之间做取舍。那么对于分布式场景下，实例或服务之间的协调问题，我们就心中有数了，你可以根据业务场景，做出最合适的选择，我们又一起往前走了一大步。但是，在极客时间的开发过程中，你又面临了一个新的问题。在通过 RPC 远程调用极客时间的课程购买接口的过程中，你可能是这样处理 RPC 的响应结果的，先是将“请求超时”的响应结果解释为“课程购买失败”，返回给用户，可是这会影响到用户的正常购买，导致一部分用户放弃。后来，为了尽可能让用户购买成功，你对“请求超时”响应的请求进行了重试，发现用户的购买成功率确实提高了，但是却有少量的用户反馈说，他只点击了 1 次购买，页面却出现了 2 笔支付成功的订单。 这确实是一个两难的问题，要么让一部分用户放弃购买，要么让少量的用户重复购买，难道没有一个好的办法吗？这里我们可以先来分析一下这个问题的根本原因，在请求的响应结果为“请求超时”的时候，我们不知道这个请求是否已经被远端的服务执行了，进一步来说就是请求的消息，是否精确一次发送到远端服务的问题，即 Exactly-once。 所以在这节课中，我们将从“为什么不能保证 Exactly-once”、“如何保证 Exactly-once ”和“Exactly-once 的挑战”这三个方面，一起来讨论如何让程序 Exactly-once。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:12:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么不能保证 Exactly-once 在单机系统中，模块之间的通信都是进程内的本地函数调用，在这个整体失败和同步通信的模型中，要么进程整体崩溃，要么调用完成，不会存在其他的情况，但是在分布式系统中，程序不能保证 Exactly-once 的原因主要有以下两个： 第一个是网络方面的原因。在分布式系统中，服务和服务之间都是通过网络来进行通信的，而这个网络是一个异步网络。在这个网络中，经过中间的路由器等网络设备的时候，会出现排队等待或者因为缓冲区溢出，导致消息被丢弃的情况，那么将一个消息从一个节点发送到另一个节点的时延是没有上界的，有可能非常快，比如 1 ms，也有可能是 1 分钟，甚至无穷大，这个时候就是出现消息丢失的情况了。在服务间进行远程调用的时候，如果迟迟没有收到响应结果，为了系统整体的可用性，我们不能无限等待下去，只能通过超时机制来快速获得一个结果。其实这样做是将无界时延的异步网络模型，通过超时机制转化成了有界时延，这个方式大大减轻了我们在写程序时的心智负担。但是，计算机的世界里没有银弹，我们在收到响应为“请求超时”的时候，无法判断是请求发送的过程中延迟了，远端服务没有收到请求；还是远端服务收到请求并且正确处理了，却在响应发送的过程中延迟了。 第二个原因是远端服务发生了故障。如果远端服务在收到请求之前发生了故障，我们会收到“网络地址不可达”的错误，对于这个错误，我们能明确判断请求没有被远端服务执行过。但是，如果远端服务是在收到请求之后发生了故障，导致无法响应而引起“请求超时”，我们无法判断请求是否被远端服务执行过，或者被部分执行过。通过上文提到的两个原因，我们可以知道，当请求方收到“请求超时”的时候，我们无法判断远端服务是否处理过这个请求。这个时候就出现了本课开头的问题：如果我们认为这是一个临时的故障，对请求进行重试，那么可能会出现多次执行的情况，即 At-least-once，如果不进行重试，就可能会出现一次都没有执行的情况，即 At-most-once。关于这个问题，在之前的课程“新的挑战”中“本地调用与远程调用”这部分，也有过深入的讨论，你可以参照着一起来看。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:12:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何保证 Exactly-once 通过上面的分析，我们知道了导致消息传递，不能保证 Exactly-once 的原因主要有两个，一个是网络出现丢包或者分区等故障，另一个是远端服务发生了故障。因为这两点在分布式系统中是永远存在的，所以我们必须去直面这两个问题，通过上层的容错机制来解决它们。一般来说，在分布式系统中，实现消息的 Exactly-once 传递，主要有三种方式：一种是至少一次消息传递加消息幂等性，一种是分布式快照加状态回滚，还有一种是整体重做，下面我们来一一介绍。 至少一次消息传递加消息幂等性 至少一次消息传递加消息幂等性的思路特别简单，我们可以结合本课开始提到的场景来分析，如果调用方在课程购买的 RPC 接口返回网络层错误，比如请求超时以及网络地址不可达等，对于这样的情况，调用方就进行重试，直到响应结果为成功或业务错误等非网络层错误。当然，这里的请求超时也有可能是远端服务的执行时间太长导致的，为了简化讨论中的语言描述，后面我们统一归类为网络错误。但是，我们同样要考虑到，重试会让用户对当前的课程重复购买，对于这个情况，我们可以在远端服务对课程购买接口的实现上，对请求进行去重，确保远端服务对同一个购买请求处理一次和多次的结果是完全相同的，对于这样的接口，我们称之为幂等的。 其实这个去重的思路也非常简单，你可以结合下图理解。我们只需要对用户发起的每一次课程购买的请求，生成一个唯一的 ID ，然后在课程购买的 RPC 请求中带上这个唯一的 ID ，在首次调用和重试的时候，这个唯一的 ID 都保持不变。接着，课程购买服务在接收到请求后，先查询当前的 ID 是否已经处理过，如果是已经处理过的请求，就直接返回结果，不重复执行购买相关的逻辑了。 分布式快照加状态回滚 分布式快照加状态回滚指的是，在整个分布式系统运行的过程中，定期对整个系统的状态做快照，在系统运行时，不论系统的哪个地方出现故障，就将整个系统回滚到上一个快照状态，然后再重放上一个快照状态之后的情况，直到所有的消息都被正常处理，你可以结合下图理解具体操作： 可是很明显，分布式快照加状态回滚的方式并不适合在线业务的情况。首先，要对在线业务的所有状态做快照是非常难的一件事情，因为在线业务的状态一般都是在数据库中，如果要对整个系统的数据库都定期做快照，这将消耗非常大的资源。其次，在通过快照进行状态回滚的时候，整个系统不能处理当前的业务请求，当前的业务请求需要进行排队等待，等系统通过快照将状态回滚完，并且重放了上一个快照状态之后的所有请求，才能开始正常处理当前业务。这个过程可能很长，这对于在线业务系统是不能接受的。最后，如果出现任何一个小的问题或者故障，就要对整个分布式系统进行状态回滚，这也是不能接受的。所以，分布式快照加状态回滚的方式，一般不会应用于在线业务架构中，它的主要应用场景是例如 Flink 之类的流式计算。因为在流式计算中，系统状态的存储也是系统设计的一部分，我们可以在系统设计的时候，就考虑支持快照和回滚功能。并且，在流式计算中，消息来源一般都是 Kafka 之类的消息系统，这样对消息进行重放就非常方便了。 整体重做 整体重做的 Exactly-once 的方式，可以看成是分布式快照加状态回滚的一种特殊情况。在执行任务的过程中，如果系统出现故障，就将整个任务的状态删除，然后再进行重做。整体重做的方案，一般的使用场景为批处理任务的情况，比如 MapReduce 之类的批处理计算引擎。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:12:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"Exactly-once 的挑战 因为这个专栏主要讨论的是在线业务架构的分布式系统，所以接下来，我们只讨论分布式在线业务架构系统中，对于解决 Exactly-once 问题，常用的“最少一次消息传递加消息幂等性机制”面临的挑战。 重试面临的挑战 通过“最少一次消息传递加消息幂等性机制”来确保消息的 Exactly-once，我们首先要采用重试策略，来确保消息最少传递一次，但是在执行重试策略的过程中，我们要避免重试导致的系统雪崩的问题。在系统快要接近性能瓶颈的时候，某些节点可能会因为负载过高而响应超时，如果这个时候再无限制地重试，就会进一步放大系统的请求量，将一个局部节点的性能问题，放大到整个系统，造成雪崩效应。 一般情况下，重试策略都会有两个限制，第一个是限制重试的次数，比如，如果重试 3 次都失败了，就直接返回请求失败，不再继续重试；第二个是控制重试的间隔，一般采取指数退避的策略，比如重试 3 次，第一次请求失败后，等待 1 秒再进行重试，如果再次失败，就等待 3 秒再进行重试，仍然失败的话，就等待 9 秒后再进行重试。 幂等面临的挑战 对于请求的幂等问题，首先，我们要讨论能否通过对操作进行改写，将一个非幂等操作变成一个幂等操作，然后，我们再讨论如何将一个非幂等操作变成一个幂等操作，最后，我们讨论在有外部系统的情况下，如何保证请求的幂等性。 操作的幂等性讨论 对于请求的幂等处理，如果请求本身就是幂等的，比如请求只是查询数据，没有任何的状态修改，或者是像更新头像这样简单的重置操作，那么我们可以什么都不用做。这里我们要注意一个情况，假设有一个请求是为用户的余额增加 5 元，如果采用下面的 SQL 进行处理，我们都知道它不是幂等的： UPDATE table SET balance = balance + 5 WHERE UID = 用户 ID ； 但是，如果我们将上面的 SQL 改写为下面的三个操作，你可以思考一下，这个时候我们的请求是否为幂等的呢？ 在数据库中查询用户的余额：SELECT balance FROM table WHERE UID = 用户 ID ； 在内存中计算用户的余额：balance = balance + 5 ，假设计算结果为 10 。 更新用户的余额到数据库：UPDATE table SET balance = 10 WHERE UID = 用户 ID ； 在上面的操作中，虽然对数据库的两个操作都是幂等的，但是整体的操作却不是幂等的，因为第 2 步的操作不是幂等的，上面的改写只是将这个计算操作，从数据库中迁移到内存中，并不会改变这个请求的幂等性。 如何确保操作的幂等性 如果是一个非幂等操作的请求，我们如何将其变成一个幂等的请求呢？一个常用的方法就是我们在上面课程购买的例子中介绍的，在请求中增加唯一 ID ，然后在处理请求时，通过 ID 进行去重，确保对相同 ID 的请求只处理一次。这里要特别注意的是，将请求处理结果写入数据库的操作，以及标记请求已处理的操作，也就是将请求唯一的 ID 写入数据库，它们都必须在同一个事务中，让事务来保证这两个操作的原子性。 否则，如果在写入处理结果后，请求唯一的 ID 写入数据库之前，服务发生崩溃的话，重试的时候就会使请求被执行多次；如果在请求唯一的 ID 写入数据库后，写入处理结果之前，服务发生崩溃，那么后面的重试请求都将因为去重而丢弃，导致请求一次都没有执行。 外部系统的幂等性保障 另外，还有一种情况，如果我们请求的操作会影响外部系统的状态，比如在一个请求中，我们需要给用户发送一条 IM 消息，因为发送 IM 消息是由外部的 IM 服务来提供的，我们可以通过下面两种方案，来保证请求操作整体的幂等性： 第一个方案，由 IM 服务提供幂等的消息发送接口。在这种情况下，我们采用全局唯一的 ID 作为请求的 ID，这样当前请求在调用 IM 消息发送接口时，我们只需要传入当前请求的唯一 ID 作为消息发送的 ID 即可，由 IM 服务内部根据消息发送 ID 来进行去重操作，确保 IM 消息发送的幂等性。第二个方案，IM 服务提供 2PC 的消息发送接口，然后我们在当前请求的内部通过 2PC 的机制，确保该请求的内部状态修改逻辑， IM 消息的发送和请求唯一的 ID 写入数据库，这三个操作整体是一个原子操作。 到这里可以看出，如果我们请求的操作会影响到外部系统的状态，要保证请求的幂等性是需要依赖外部系统的支持才能实现的。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:12:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课，我们一起讨论了分布式系统场景下的重试和幂等的相关问题，接下来一起来总结一下这节课的主要内容：首先，我们讨论了在分布式场景下，由于不可靠的网络和随时都有可能出现的故障，导致在单体服务上非常容易保证的 Exactly-once ，在分布式系统中却非常困难。其次，我们一起讨论了保证 Exactly-once 的三种方式：至少一次消息传递加消息幂等性、分布式快照加状态回滚和整体重做。这样，以后你再碰到需要 Exactly-once 的业务场景，就可以依据业务场景来进行选择了。最后，我们一起讨论了在分布式系统中，确保 Exactly-once 面临的挑战：第一是重试的时候需要限制重试的间隔和次数，确保系统不会受到局部故障的影响，导致整体雪崩；第二是保障接口的幂等性，特别是对于涉及外部系统的情况下，如何保障接口整体的幂等性。通过这些讨论，以后对于 Exactly-once 你就心中有数了。 在 IM 系统中，我们如何实现幂等的消息发送接口？ ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:12:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"09 | 雪崩（一）：熔断，让故障自适应地恢复 通过学习重试幂等的内容，让我们在网络故障和部分失败的分布式系统中，也有办法确保程序之间的调用实现 Exactly-once 的效果，这样当系统出现临时故障的时候，用户依然能正常购买，我们的系统又健壮了一点。在日常运维极客时间服务端系统的过程中，你一定碰到过大规模故障的情况，可是事后复盘时，却发现故障的起因，大多都是一些局部的小问题引起的，比如因为一个接口响应时间变长，使相关实例的协程或线程数暴涨，让系统的负载进一步增加，最终导致实例所有接口的响应时间都变长，这就从一个接口的局部故障演变成了一个全局的故障。在一个分布式系统中，局部故障是不可避免的，但是如果不能将局部故障控制好，导致其演变成一个全局的系统故障，这对我们来说是不可以接受的，那么我们应该如何解决这个问题呢？其实这就是分布式系统中的雪崩场景问题，那么从这节课开始，我们将用四节课的时间来解决，如何让一个分布式系统避免发生雪崩的问题。这一节课，我们先讨论雪崩现象出现的原因，然后再分析如何通过熔断机制来避免雪崩，最后一起总结熔断机制应该注意的关键点。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:13:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么会出现雪崩 雪崩是由于局部故障被正反馈循环，从而导致的不断放大的连锁故障，正如我们上文的例子所说，雪崩通常是由于整个系统中，一个很小的部分出现故障，进而导致系统其他部分也出现故障而引发的。但是，一个正常运行的服务为什么会发生雪崩呢？我认为在实际工作中，出现雪崩一般会经历以下三个阶段，如下图。 首先，服务的处理能力开始出现过载。服务过载是指服务器只能处理一定 QPS 的请求，当发往该服务器的 QPS 超出后，由于资源不够等原因，会出现超时、内存增加等各种异常情况，使服务的请求处理能力进一步降低，过载情况更加严重。服务处理能力出现过载有多种原因，比如服务可能由于 Bug 导致性能下降，或者由于崩溃导致过载，也有可能就是突发的流量超过了服务的设计目标，或者是机器宕机导致可提供服务的实例数量减少等原因。 然后，服务由于资源耗尽而不可用。当服务严重过载后，会出现大量请求的积压，这会导致服务消耗更多的内存、 CPU 、线程和文件描述符等资源，待这些资源被消耗尽后，服务将出现严重超时和崩溃等异常情况，最终对外表现为不可用。当服务的某一个实例崩溃后，负载均衡器会将请求发送给其他的实例，导致其他的实例也出现过载的情况，从而造成整个服务过载的故障。最后，由于服务内部出现严重的过载，导致响应严重超时，服务的调用方同样会出现大量请求的积压使资源耗尽，这样正反馈循环就形成了，故障沿着调用链路逆向传播，导致整个系统出现雪崩。 通过上面的讨论，我们可以看出雪崩的根本原因是系统过载，如果在系统过载的情况下，不进行任何控制，异常情况就会急剧扩散，导致雪崩情况出现。所以，想要避免系统雪崩，要么通过快速减少系统负载，即熔断、降级、限流等快速失败和降级机制；要么通过快速增加系统的服务能力来避免雪崩的发生，即弹性扩容机制。在本节课中，我们先来讨论如何通过熔断来避免系统发生雪崩。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:13:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"利用熔断机制避免雪崩 其实对于熔断机制，我们并不陌生。在日常生活中，电路保险丝的熔断就是我们最常见的熔断机制，它指的是在电路系统中，当电路超过负荷运行时，保险丝会自动断开，从而保证电路中的电器不受损害。那么我们就借鉴这个原理来讨论熔断机制。当服务之间发起调用的时候，如果被调用方返回的指定错误码的比例超过一定的阈值，那么后续的请求将不会真正发起，而是由调用方直接返回错误。我们知道电路在工作的时候，有两种工作状态，分别是通路和开路，计算机的熔断机制则略有不同，在熔断机制的模式下，服务调用方需要为每一个调用对象，可以是服务、实例和接口，维护一个状态机，在这个状态机中有三种状态。 熔断机制下，状态机的三种状态 首先，是闭合状态 ( Closed )。在这种状态下，我们需要一个计数器来记录调用失败的次数和总的请求次数，如果在一个时间窗口内，请求的特定错误码的比例达到预设的阈值，就切换到断开状态。其次，是断开状态 ( Open )。在该状态下，发起请求时会立即返回错误，也可以返回一个降级的结果，我们会在后面的课程“降级”中再详细讨论。在断开状态下，会启动一个超时计时器，当计时器超时后，状态切换到半打开状态。最后，是半打开状态 ( Half-Open )。在该状态下，允许应用程序将一定数量的请求发往被调用服务，如果这些调用正常，那么就可以认为被调用服务已经恢复正常，此时熔断器切换到闭合状态，同时需要重置计数。如果这部分仍有调用失败的情况，我们就认为被调用方仍然没有恢复，熔断器会切换到断开状态，然后重置计数器。所以半打开状态能够有效防止正在恢复中的服务，被突然出现的大量请求再次打垮的情况。 通过上文对熔断机制的讨论，我们将服务由于过载原因导致的错误比例，作为熔断器断开的阈值，当被调用服务出现过载的时候，熔断器通过错误比例感知到被调用服务过载后，就立即将调用请求返回错误，这样可以减少被调用服务的请求数量，也可以减少调用服务由于等待请求响应而积压的请求，完美切断了正反馈循环，确保了雪崩不会发生。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:13:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"熔断机制的关键点 到这里，我们已经明白了什么是熔断机制，以及如何利用熔断机制来避免雪崩，但是在熔断机制的具体实现上，还会面临熔断的粒度选择和过载判断等关键的问题，所以接下来我们一起从“粒度控制”、“错误类型”、“存活与过载的区别”、“重试和熔断的关系”和“熔断机制的适应范围”这五个角度来讨论熔断机制的关键点。 粒度控制 对于熔断的粒度控制问题，进一步来说，就是我们想将监控资源过载的粒度控制在一个什么样的范围内，这个范围可以由服务、实例和接口这三个维度的组合来得到，具体见下表。 结合我的工作经验，在实现熔断机制的时候，更建议你选择“实例的接口”这个熔断粒度，主要有以下三个原因。 首先，熔断的敏感度高。假设有一个服务部署了 10 个实例，并且这 10 个实例都是均匀接受请求流量的。在这种情况下，只有一个实例的一个接口负载过高时，即使它的每一次请求都超时，但由于其他实例的这个接口都是正常的，所以基于“接口”粒度统计到的请求错误率不会超过 10 %，而基于“服务”和“实例”粒度的熔断器统计到的错误率将更低。如果熔断器的阈值大于 10 %，那么将不能识别到这个实例接口过载的情况，只有等这个接口的过载慢慢被放大，才能被基于“服务”、“实例”和“接口”粒度的熔断器感知到，但是这个结果明显不是我们期待的。 其次，熔断的误伤范围小。当同一服务的不同实例，所分配的资源不相同时，“实例的接口”粒度的熔断机制，能够正确识别有问题实例的接口进行熔断，而不是将这个服务所有实例的这个接口进行熔断，更不是对实例和服务进行熔断，这样就提升了系统的可用性水平。 最后，虽然实现粒度越细的熔断机制，需要维护更多的熔断状态机，导致更多的资源消耗，但是设计优良的熔断机制所消耗的资源是非常少的，“实例的接口”粒度的熔断机制所消耗的资源，完全在系统可以承受的范围之内。 错误类型 由于熔断机制是用来消除系统过载的，所以，我们需要识别出与系统过载相关的错误，来进行熔断处理，一般来说，主要有下面两个错误类型。第一，系统被动对外表现出来的过载错误，一般来说，如果一个接口过载了，那么它的响应时间就会变长，熔断器捕获到的错误类型就是“响应超时”之类的超时错误。第二，系统主动对外表现出来的过载错误，对于这种情况，一般是请求的流量触发了限流等机制返回的错误码，这个是我们在程序开发过程中主动设计的。另外，我们要记住，熔断机制一定不要关心应用层的错误，比如余额不足之类的错误，因为这一类型的错误和系统的过载没有关系。 过载与存活的区别 熔断机制关心的是服务是否过载，而判断一个服务是否过载，最好的方式是依据请求在队列中的平均等待时间来计算服务的负载。之所以不选择请求的平均处理时间，是为了去除下游服务调用的影响，有时处理时间的增加并不代表当前的服务过载了，而是代表请求依赖的下游服务过载了，并且请求的处理时间增加到一定程度，当前服务的资源也会逐渐耗尽，最终反映在等待时间的增加上。但是在熔断场景中，我们对服务的过载判断进行了简化，直接通过服务接口请求的结果来进行判断。我们执行这个接口的逻辑，如果请求发生错误，并且错误为超时或者限流等错误的比例超过一定的阈值时，我们可以认为该接口是过载的，然后进行熔断。而存活一般是指机器或者服务是否存活，对于机器是否存活，一般是通过定期 ping 机器的 IP ，如果超过一定时间不能 ping 通，则认为该机器不存活了。对于服务是否存活，一般是由服务来提供一个专门用于探活的、逻辑非常简单的接口，之后定期请求这个接口，如果超过一定时间不能请求成功，则认为该服务不存活了。当然，服务严重过载会导致服务的存活性出现问题，不过总体来说，过载更关心服务当前的状态好不好，而存活只关心服务是否能活着，这是一个更低的要求。 熔断与重试的关系 熔断和重试都会对服务之间的调用请求进行额外的处理，但不同的是，重试是指在一个请求失败后，如果我们认为这次请求失败是因为系统的临时错误导致的，那么为了提高系统的可用性，我们会重新发起请求。而熔断则认为当前系统的这一个接口已经出现过载的情况，为了确保系统不会出现雪崩，而对当前接口的请求进行快速失败，直接返回失败，而不是真正地发起请求，以此来减少系统当前的过载情况。所以，我们可以认为熔断和重试是两个层面的操作，它们之间是相互独立的，不需要相互干扰。我们在需要重试的业务场景中进行重试操作，来提高系统的可用性，而熔断一般会内置到系统的框架中，并且默认开启，作为系统稳定性的最后一道保险丝，来确保系统不会因为过载而雪崩。至于因为熔断被迫进行快速失败的这个请求，它是首次的还是重试的请求，我们并不关心。 熔断机制的适应范围 通过前面的讨论，我们知道了熔断机制是用来解决过载问题的，所以只要是过载问题的场景，我们都可以考虑利用熔断机制来解决，不论是分布式系统中服务之间的调用，还是服务与数据库之间等其他场景的调用。 比如伴鱼开源的数据库中间件 Weir（项目地址：https://github.com/tidb-incubator/weir），它就实现了 SQL 粒度的熔断机制，在后端数据库过载的情况下，通过熔断机制来快速减少数据库的请求压力，确保数据库的稳定性。同时，一般来说，如果系统出现熔断，都是出现了一定的故障，所以熔断机制状态的变化都是系统非常关键的状态信息，可以通过报警之类的形式通知相关的负责人，来一起观察系统的状态，在必要的时候可以人工介入。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:13:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 到这里，我们一起讨论了分布式系统中为什么会出现雪崩，以及如何通过熔断机制来避免系统出现雪崩，我们一起来总结一下这节课的主要内容。首先，我们知道了因为局部故障被正反馈循环导致不断放大，会使系统出现雪崩，这就是为什么一些非常大的故障，其根本原因都是非常小的问题。在了解了什么是熔断机制，并且如何利用熔断机制来避免系统出现雪崩后，你就能自己实现一个熔断器，来避免你负责的系统雪崩了。最后，通过了解熔断机制的 5 个关键点，我们正确理解了熔断机制和实现熔断机器的核心问题，从此就能彻底掌握熔断机制了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:13:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"10 | 雪崩（二）：限流，抛弃超过设计容量的请求 通过上一节课的学习，我们了解了因为局部故障的正反馈循环而导致的雪崩，可以通过熔断来阻断，这样我们就为极客时间的后端系统，加上了熔断这一根保险丝，再也不用担心小故障被放大成一个全局的故障了，这让极客时间的后端系统，在稳定性上又向前跨进了一大步。但是有的时候，我们明明知道一个服务的最高处理能力为 10 w QPS ，并且我们也知道这一次活动，这个服务的请求会超过 10 w QPS 。这个时候，如果只有熔断机制，我们就需要等待服务由于过载出现故障后触发熔断，然后再恢复正常，那么系统就是在被动地应对服务请求过载的问题。其实这是一个典型的限流场景，那么，我们应该如何优雅地处理这个问题呢？在这节课中，我们将一起讨论，保障分布式系统稳定性的另一个方法——限流，从限流的原因入手，分析如何实现限流，再一起讨论限流机制要注意的关键问题，从这三个方面来分析，如何通过限流机制主动处理服务流程过载。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:14:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么需要限流 限流和熔断是经常一起出现的两个概念，都是用来解决服务过载问题的，那么在有了熔断机制后，为什么还需要限流呢？我认为主要有以下几个方面的原因。 首先，熔断的处理方式不够优雅。回到课程开始的例子，虽然在服务过载的时候，熔断可以避免雪崩的发生，但是熔断机制是被动感知故障，然后再进行处理的，它需要先让过载发生，等系统出现故障后，才会介入处理，让系统恢复到正常。这样的处理方式会让系统产生不必要的抖动，如果是处理意料之外的过载问题，我们是可以接受的。但是，在明知道服务的服务能力的情况下，依然让故障发生，然后在事后进行被动处理，这个处理思路就不够优雅了。 其次，熔断机制是最后底线。虽然熔断可以解决雪崩问题，但是它应该作为系统稳定性保障的最后一道防线，我们没有必要时刻把它亮出来。正确使用熔断的思路应该是，在其他方法用尽之后，如果过载问题依旧存在，这时熔断才会被动触发。所以，我们的系统虽然有熔断机制，保障雪崩不会出现，但是当熔断出现的时候，依然代表着我们的系统已经失控了。我们需要更主动地解决问题，防患于未然，而限流就可以达到这个目的。 再次，在快速失败的时候，需要能考虑调用方的重要程度。熔断是调用方依据响应结果自适应来触发的，在被调用方出现过载的时候，所有的调用方都将受到影响。但是很多时候，不同调用方的重要程度是不一样的，比如同样是查询用户信息的接口，在用户详情页面调用这个接口的重要程度，会高于评论列表页面，如果查询用户信息的接口出现过载了，我们要优先保障用户详情页面的调用是正常的。 最后，在多租户的情况下，不能让一个租户的问题影响到其他的租户，我们需要对每一个租户分配一定的配额，谁超过了就对谁进行限流，保证租户之间的隔离性。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:14:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何实现限流 通过上面的讨论，我们了解到限流机制是熔断等其他机制无法替代的，是必须的，那么我们该如何实现限流机制呢？这里我们先介绍一下常见的限流算法，然后讨论单节点限流机制需要注意的问题，最后再讨论分布式场景下限流机制的权衡。 限流算法 限流算法是限流机制的基础和核心，并且后续关于限流机制的讨论，都会涉及相关的限流算法，所以我们先介绍最常用的四个限流算法：固定窗口、滑动窗口、漏桶和令牌桶算法，把它们两两结合来进行分析。 固定窗口和滑动窗口 固定窗口就是定义一个“固定”的统计周期，比如 10 秒、30 秒或者 1 分钟，然后在每个周期里，统计当前周期中被接收到的请求数量，经过计数器累加后，如果超过设定的阈值就触发限流，直到进入下一个周期后，计数器清零，流量接收再恢复正常状态，如下图所示。 假设我们现在设置的是 2 秒内不能超过 100 次请求，但是因为流量的进入往往都不是均匀的，所以固定窗口会出现以下两个问题。 第一，抗抖动性差。由于流量突增使请求超过预期，导致流量可能在一个统计周期的前 10 ms 内就达到了 100 次，给服务的处理能力造成一定压力，同时后面的 1990 ms 将会触发限流。这个问题虽然可以通过减小统计周期来改善，但是因为统计周期变小，每个周期的阈值也会变小，一个小的流量抖动就会导致限流的发生，所以系统的抗抖动能力就变得更差了。 第二，如果上一个统计周期的流量集中在最后 10 ms ，而现在这个统计周期的流量集中在前 10 ms ，**那么这 20 ms 的时间内会出现 200 次调用，这就超过了我们预期的 2 秒内不能超过 100 次请求的目的了。**这时候，我们就需要使用“滑动窗口”算法来改善这个问题了。 其实，滑动窗口就是固定窗口的优化，它对固定窗口做了进一步切分，将统计周期的粒度切分得更细，比如 1 分钟的固定窗口，切分为 60 个 1 秒的滑动窗口，然后统计的时间范围随着时间的推移同步后移，如下图所示。 但是这里要注意一个问题，如果滑动窗口的统计窗口切分得过细，会增加系统性能和资源损耗的压力。同时，滑动窗口和固定窗口一样面临抗抖动性差的问题，“漏桶”算法可以进一步改进它们的问题。 漏桶和令牌桶 我们可以在图中看到，“漏桶”就像一个漏斗，进来的水量就像访问流量一样，而出去的水量就像是我们的系统处理请求一样。当访问流量过大时，这个漏斗中就会积水，如果水太多了就会溢出。 相对于滑动窗口和固定窗口来说，漏桶有两个改进点，第一，增加了一个桶来缓存请求，在流量突增的时候，可以先缓存起来，直到超过桶的容量才触发限流；第二，对出口的流量上限做了限制，使上游流量的抖动不会扩散到下游服务。这两个改进大大提高了系统的抗抖动能力，使漏桶有了流量整形的能力。但是，漏桶提供流量整形能力有一定的代价，超过漏桶流出速率的请求，需要先在漏桶中排队等待，其中流出速率是漏桶限流的防线，一般会设置得相对保守，可是这样就无法完全利用系统的性能，就增加了请求的排队时间。 那么从资源利用率的角度来讲，有没有更好的限流方式呢？我们可以继续看下面介绍的“令牌桶”算法。如图，我们可以看到，令牌桶算法的核心是固定“进口”速率，限流器在一个一定容量的桶内，按照一定的速率放入 Token ，然后在处理程序去处理请求的时候，需要拿到 Token 才能处理；如果拿不到，就进行限流。因此，当大量的流量进入时，只要令牌的生成速度大于等于请求被处理的速度，那么此时系统处理能力就是极限的。 根据漏桶和令牌桶的特点，我们可以看出，这两种算法都有一个“恒定”的速率和“可变”的速率。令牌桶以“恒定”的速率生产令牌，但是请求获取令牌的速率是“可变”的，桶里只要有令牌就直接发，令牌没了就触发限流；而漏桶只要桶非空，就以“恒定”的速率处理请求，但是请求流入桶的速率是“可变”的，只要桶还有容量，就可以流入，桶满了就触发限流。 这里我们也需要注意到，“令牌桶”算法相对于“漏桶”，虽然提高了系统的资源利用率，但是却放弃了一定的流量整形能力，也就是当请求流量突增的时候，上游流量的抖动可能会扩散到下游服务。 所以，计算机的世界没有银弹，一个方案总是有得必有失，一般来说折中的方案可能是使用最广泛的，这就是没有完美的架构，只有完美的 trade-off 的原因。 单节点限流 由于只有一个节点，不需要和其他的节点共享限流的状态信息，所以单节点限流的实现是比较简单的，我们可以基于内存来实现限流算法，让需要限流的请求先经历一遍限流算法，由限流算法来决定是正常执行，还是触发限流，这里需要注意两个问题。 首先，限流机制作用的位置是客户端还是服务端，即选择客户端限流还是服务端限流。一般来说，熔断机制作用的位置是客户端，限流机制作用的位置更多是服务端，因为熔断更强调自适应，让作用点分散在客户端是没有问题的，而限流机制则更强调控制，它的作用点在服务端的控制能力会更强。 但是，将作用点放置在服务端，会给服务端带来性能压力。如果将作用点放置在客户端，这就是一个天然的分布式模式，每一个调用方的客户端执行自己的限流逻辑，这部分我们会在下面的分布式限流中继续讨论。而将作用点放置在服务端时，服务端要执行所有请求的限流逻辑，就需要更多的内存来缓存请求，以及更多的 CPU 来执行限流逻辑。我们可以考虑的一个策略是，在客户端实现限流策略的底线，比如，一个客户端对一个接口的调用不能超过 10000 并发，这是一个正常情况下完全不会达到的阈值，如果超过就进行客户端限流，避免客户端的异常流量对服务端造成压力。同时，因为这是一个非常粗粒度的阈值，设置好默认值后，几乎不会去修改，所以就缓解了客户端限流带来的阈值管理问题，之后就可以在服务端实现更精细和复杂的限流机制了。 其次，如果触发限流后，我们应该直接抛弃请求还是阻塞等待，即否决式限流和阻塞式限流。一般来说，如果我们可以控制流量产生的速率，那么阻塞式限流就是一个更好的选择，因为它既可以实现限流的目的，又不会抛弃请求；如果我们不能控制流量产生的速率，那么阻塞式限流将会因为请求积压，出现大量系统资源占用的情况，很容易引发雪崩，这时否决式限流将是更好的选择。 所以，对于在线业务的服务端场景来说，服务之间相互调用的请求流量主要是用户行为产生的，不论是客户端限流还是服务端限流，限流的作用点都处于流量的接收方，因为接收方不能控制流量产生的速率，所以超出阈值后通常直接丢弃，进行否决式限流。而对于像消费 MQ 消息或者发送 Push 时，为了避免打挂所依赖的下游服务，我们可以通过对 MQ 消费或者发送 Push 的行为进行限速，来控制流量产生的速率，在这种情况下，如果超出阈值了，我们一般选择阻塞等待，进行阻塞式限流。 分布式限流 讨论完单节点限流后，我们还需要重点关注分布式限流，即为了系统高可用，每一个服务都会运行多个实例，所以我们在对某一服务进行限流的时候，就需要协调该服务的多个实例，统一进行限流。因为上文中对于单节点限流讨论的问题，在分布式限流场景同样适用，这里就不再赘述了。下面我们主要来讨论，在实现分布式场景下，如何来协同多个节点进行统一的限流。 首先，最容易想到的一个方案是进行集中式限流。单节点限流是在进程内的内存中实现限流器的，而对于分布式限流来说，我们可以借助一个外部存储来实现限流器，比如 Redis 。在分布式限流的场景下，我们一般选择令牌桶算法，但是这个方法的缺点是，每一次请求都需要先访问外部的限流器获取令牌，这将带来三个问题。 第一，限流器会成为系统的性能瓶颈，如果在系统的 QPS 非常高的情况下，限流器的压力是非常大的。虽然我们可以将请求，通过 Hash 策略扩展到多个限流器实例上，但是这也增加了系统的复杂性。复杂性是系统架构最大的敌人，我们一定要保持敏感。第二，限流器的故障将会影响所有接入限流器的服务。不过，我们可以在限流器故障的情况下，进行降级处理，例如，如果服务访问限流器获取令牌出现了错误时，可以降级为直接进行调用，而不是抛弃请求。第三，增加了调用的时延。每一次调用前，都需要先通过网络访问一次限流器，这是一个毫秒级别的时延。 其次，另一个方案是将分布式限流进行本地化处理。限流器在获得一个服务限额的总阈值后，将这个总阈值按一定的策略分配给服务的实例，每一个实例依据分配的阈值进行单节点限流。这里要注意的是，如果服务实例的性能不一样，在负载均衡层面，我们会考虑性能差异进行流量分配。在限流层面，我们也需要考虑这个问题，性能不同的实例，限流的阈值也不一样，性能好的节点，限流的阈值会更高。 但是，这个方式也有一个问题，该模式的分配比例模型，是依据统计意义来进行分配的，而现实中，具体到一个限流策略上，它的精确性可能会出现问题。比如有两个实例的服务，对一个用户限流为 10 QPS ，假设这两个实例的性能相同，每个实例限流的阈值为 5 QPS ，但是如果这个用户的流量，都被路由到其中的一个实例上，这就会导致该用户的流量，在 5 QPS 的时候就触发了限流，和我们的设计预期不一致了。 最后，我们来讨论一个折中的方案，这个方案建立在集中式限流的基础上，为了解决每次请求都需要，通过网络访问限流器获取令牌的问题，客户端只有在令牌数不足时，才会通过限流器获取令牌，并且一次获取一批令牌。这个方案的令牌是由集中式限流器来生成的，但是具体限流是在本地化处理的，所以在限流的性能和精确性之间，就有了一个比较好的平衡。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:14:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"限流机制的关键问题 了解完限流的实现原理之后，我们就知道如何去实现一个限流器了，但是，在限流器实际落地的过程中，我们需要去配置限流的阈值，同时还要确保系统，不会因为触发了不必要的限流而导致故障，所以我们还需要思考下面两个关键问题。 如何确定限流的阈值 当我们对服务进行限流的时候，首先要面临的第一个问题是，确定服务触发限流的阈值。 一个最简单的方案是，根据经验设置一个比较保守，并且满足系统负载要求的阈值，在之后的使用中慢慢进行调整。但是这个方案会出现一个问题，我们预测的限流阈值不够准确，甚至会出现比较大的偏差，对于限流的阈值来说，不论过高还是过低都会出现问题，阈值过高则限流不会起作用，阈值过低则无法发挥出服务的性能。另外，我们可以通过压力测试来决定限流的阈值。但是，压测的环境很难和线上环境保持一致，特别是在涉及缓存和存储的情况下，并且单个接口的压力测试不能反映出，正常运行情况下系统的状态。虽然全链路压测可以通过流量回放，一定程度上模拟线上真实流量的比例，但是它也只是用历史的流量比例来预测未来，并且这个工作量是非常大的。同时，我们的系统在一个持续的迭代过程中，系统的性能可能会随着迭代而发生变化，所以限流的阈值设置好之后，还需要付出一定的维护成本。 限流可能会引入脆弱性 我们引入限流，本来是为了提高系统的稳定性，达到“反脆弱”的目的，但是，如果我们在分布式系统的复杂拓扑调用中，遍布限流功能，那么以后对每个服务的扩容，新功能的上线，以及调用拓扑结构的变更，就都有可能会导致局部服务流量的骤增，从而引发限流使业务有损。所以，限流可能会引入脆弱性，这是一个很值得讨论的问题。 限流机制的“反脆弱”也有可能会导致“脆弱”的出现，它的本质原因是，在限流的阈值设置后，我们很难适应调用拓扑、机器性能等等的变化，但是，在熔断的阈值里是可以自适应这些变化的，也就没有这个问题了。所以，当我们决定对系统进行大规模限流设置时，需要谨慎地审视系统的限流能力和成熟度，判断它们是否能支撑起如此大规模的应用。最后，通过这两个讨论，我认为使用限流机制比较好的一个方式是，在系统的核心链路和核心服务上，默认启用限流机制，比如，像网关这样的流量入口和账号这样的核心服务，不论是限流阈值的设定，还是脆弱性的判断，我们都可以通过减少限流引入的范围，来简化使用限流的复杂度；而对于其他的位置和服务，则默认不启用限流机制，在出现故障的时候，通过手动设置阈值再启用，把它作为处理系统故障的一个手段。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:14:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 到这里，我们一起讨论了需要限流机制的原因，限流机制的算法、实现原理以及关键问题，下面一起来总结一下这节课的主要内容。通过了解有了熔断之后，还需要限流机制的原因，你在后续的工作中，如果碰到这样的限流场景，就可以引入限流机制了。另外，在掌握了限流的算法、单节点限流和分布式限流的技术原理之后，你就可以为你现在的系统实现一个限流器了。 最后，我们从限流机制的关键问题：限流阈值的设置和引入的脆弱性中，得出在核心链路和核心服务上，默认启用限流机制，在其他位置上，手动启用限流机制，把它作为处理系统故障的一个手段。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:14:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"11｜雪崩（三）：降级，无奈的丢车保帅之举 通过学习限流的内容，我们掌握了限流机制的应用场景、实现原理和关键问题，这样我们就可以为极客时间后端的分布式系统，在关键路径和核心服务上，去引入限流机制，进一步提高系统的稳定性。但是，在系统因为过载而出现故障的时候，虽然熔断机制可以确保系统不会雪崩，限流可以确保，被保护的服务不会因为过载而出现故障，可是这时候，系统的可用性或多或少都会受到一定的影响，并且这个影响不会区分核心业务和非核心业务。那么你的脑海里一定会出现一个想法，是否可以在故障出现的时候，通过减少或停掉非核心业务，来降低系统的负载，让核心业务不会受到，或者少受到影响呢？其实是可以的，这就是一个典型的降级场景问题。在这节课中，我们将一起讨论保障分布式系统稳定性的第三个方法——降级，分析如何通过降级机制，来保障系统的核心服务稳定运行。这节课我依然会按照需要降级的原因，如何实现降级，以及降级机制应该注意的关键问题这一条思路来为你讲解。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:15:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么需要降级 为什么有了熔断和限流之后，我们依然需要降级机制呢？在分布式系统中，熔断、限流和降级是保障系统稳定性的三板斧，缺一不可，并且在保障系统的稳定性方面，降级有着熔断和限流所没有的优点，因此它们之间相互配合和补充，能够最大限度地保障系统的稳定性水平。 首先，降级机制能从全局角度对资源进行调配，通过牺牲非核心服务来保障核心服务的稳定性。比如，在当前极客时间的后端系统出现了过载问题的时候，或者我们预计到由于运营活动会出现突发流量的时候，我们有账号、支付和评论三个服务，停掉任意一个服务都可以让系统正常运行，那么相对于账号和支付这两个非常核心的服务，毫无疑问，我们会选择停掉评论服务来丢车保帅，降低系统故障对外的影响，这其实就是降级的核心思路。你可能会想到，通过限流机制也可以出现降级的效果，比如，直接将评论服务的请求 QPS 限制为 0，但是本质上来说，限流和降级机制的思维方式还是不一样的。限流一般是通过对请求流量控制，来保证被限流服务的正常运行，而降级却恰恰相反，它是通过牺牲被降级的接口或者服务，来保障其他的接口和服务正常运行的。 其次，降级可以提高系统的用户体验性和可用性。在分布式系统中，如果接口的正常调用出现非业务层错误后，在某些情况下，我们可以不用直接返回错误，而是执行这个接口的“ B 计划”进行降级。虽然降级后的执行结果没有正常调用那么完美，但是和直接返回调用错误相比，这对系统的用户体验和可用性来说，却是一个不小的提升。在这个场景下，降级可以和熔断、限流机制配合使用，在系统触发熔断和限流的时候，我们可以不直接返回错误，而是执行预先准备好的降级结果。降级需要提前设计，并且降级的逻辑也要消耗系统资源，所以一般来说，对于核心的接口或服务，我们可以通过缓存或者其他的方法来提供一些，一致性等方面较差，但是业务可以接受的返回结果；而对于非核心的接口和服务，我们可以考虑通过友好的提示等低成本的方式，来提升用户的体验。这里一定要注意，降级在和熔断、限流机制配合使用时，一定要评估降级逻辑的性能，千万不能因为降级逻辑，再次导致系统雪崩。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:15:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何实现降级 通过上面的讨论，我们了解到在故障出现的时候，降级机制可以从全局角度，提高系统资源使用的效率，进一步提升系统的稳定性和用户体验，而且这一点是熔断和限流机制都无法替代的。那么我们该如何实现降级机制呢？下面我们根据降级操作是否由人工触发，将降级机制分为手动降级和自动降级，来一一介绍。 手动降级 手动降级是指在分布式系统中提前设置好降级开关，然后通过类似配置中心的集中式降级平台，来管理降级开关的配置信息，在系统需要降级的时候，通过降级平台手动启动降级开关，对系统进行降级处理。手动降级由人工操作，有可控性强的优点，但是一般来说，一个分布式系统中，会有成百上千的服务和成千上万的实例，如果在出现故障的时候，一个接口、一个服务地去手动启动降级开关是非常低效的。对于这个问题，有一个可行的方案是，通过对降级分级，利用服务的等级信息和业务信息进行批量降级，具体的思路如下。首先，将系统中的所有服务，按照对业务的重要程度进行分级，这里，我分享一个服务定级的标准，具体定义见下表。这个标准从高到低按重要程度分为 P0 ~ P3 这 4 个级别，你可以作为参考，依据自己的业务形态进行调整。 然后，根据服务的等级信息、业务信息和调用链路的依赖关系，对非核心服务建立分级降级机制。这里以服务为粒度进行分级，实际工作中，如果有需要也可以以接口为粒度进行分级。假设 P0 为核心业务，其他的为非核心业务，我们可以简单地将降级分为以下 3 个级别。 一级降级：会对 P1、P2、P3 的服务同时进行降级。二级降级：会对 P2、P3 的服务同时进行降级。三级降级：会对 P3 的服务同时进行降级。 这样在需要降级的时候，我们就可以根据系统当时的情况，按接口、服务和降级级别进行手动降级。当然在实际操作中，你还可以综合业务场景来设置降级级别，并且根据业务需要来设置更多的降级级别。这里要注意，不论是服务分级还是降级分级，都是需要谨慎对待的一件事情，如果出错将会导致人为的故障发生。 自动降级 自动降级是指在分布式系统中，当系统的某些指标或者接口调用出现错误时，直接启动降级逻辑，但是因为自动降级不能通过开关来控制，所以需要认真评估。一般来说，系统关键链路上的“ B 计划”可以进行自动降级，否则业务将无法正常提供服务。这里我们来看一个鉴权接口自动降级的例子。假设我们在网关中调用鉴权服务进行鉴权，每一个调用鉴权服务的鉴权接口，需要执行如下的两个校验逻辑，不论哪一个失败，都会导致鉴权失败。 校验 Token 是否合法。2. 校验 UID 是否被管理员封禁。 在这个情况下，我们可以将 Token 设计为可以自校验的，在鉴权服务出现故障的时候，则启动降级逻辑，直接在网关中校验 Token 是否合法，如果合法就返回鉴权成功。因为在大多数业务场景中，Token 被管理员封禁是小概率事件，所以相对于所有用户都不能正常鉴权的情况，我们认为个别被管理员封禁的用户也可以鉴权成功，是完全可以接受的。其实，我们可以将自动降级理解为手动降级的特殊情况，即降级开关为启用的手动降级。所以，还有一个思路就是，不提供自动降级，在需要自动降级的场景下，通过降级开关为启用的手动降级来实现，这样还可以进一步提高降级的灵活性。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:15:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"降级机制的关键问题 学习完降级的实现原理后，我们就知道了如何在自己的系统中引入降级机制了。但是一般来说，我们使用降级都是在系统已经出现过载的场景下，这时我们需要考虑，降级的配置信息是否能正常下发。并且，降级通常会与熔断和限流一起出现，我们应该如何处理它们三者之间的关系。基于这两点，在降级机制实际使用的过程中，我们还需要思考下面两个关键问题。 配置信息下发的问题 对于熔断和限流来说，其阈值相关的配置信息在系统正常运行的时候，就已经下发到实例上了，所以在系统出现故障的时候，这些配置信息会直接生效。但是对于降级机制来说，如果采用了手动降级的机制，并且默认设置为关闭，在系统出现故障的时候，我们需要通过降级平台下发配置来启动降级。但是在系统出现故障的时候，有可能会出现降级配置无法正常下发的情况，这时我们将不能启动降级策略。我们可以考虑，由服务直接暴露出修改降级配置的 HTTP 接口，在必要的时候，可以手动通过 HTTP 接口，来启动服务的降级逻辑。 熔断、限流和降级之间的关系 在分布式系统中，熔断、限流和降级是保障系统稳定性的三板斧，经常一起出现，很容易导致混淆，所以，下面我们就对熔断、限流和降级机制之间的关系进行比较和总结：首先，因为熔断机制是系统稳定性保障的最后一道防线，并且它是自适应的，所以我们应该在系统全局默认启用；其次，限流是用来保障被限流服务稳定性的，所以我们建议，一般在系统的核心链路和核心服务上，默认启用限流机制；最后，降级是通过牺牲被降级的接口或者服务，来保障其他的接口和服务正常运行的，所以我们可以通过降级直接停用非核心服务，然后对于核心接口和服务，在必要的时候，可以提供一个“ B 计划”。其实，从整个系统的角度来看，不论是熔断还是限流，一旦触发了规则，都是通过抛弃一些请求，来保障系统的稳定性的，所以，如果更广泛地定义降级的话，可以说熔断和限流都是降级的一种特殊情况。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:15:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 我们掌握了需要降级机制的原因，以及实现原理和关键问题，一起来总结一下这节课的主要内容。通过讨论有了熔断和限流机制之后，依然需要降级机制的原因，我们了解了限流的作用和应用场景，在后续的工作中碰到相关的问题时，可以引入降级机制。另外，我们一起分析了如何实现降级机制，从操作的角度来讲，降级分为手动降级和自动降级，掌握了这些知识和原理后，你就能为你现在的系统实现一个降级机制了。我们还一起探讨了限流机制的关键问题：配置信息下发的问题，以及熔断、限流和降级机制之间的关系，这样一来，你不仅能实现一个健壮的降级机制，并且还能更好地理解熔断、限流和降级三者之间的关系。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:15:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"12｜雪崩（四）：扩容，没有用钱解决不了的问题 在降级的学习中，我们掌握了降级机制的应用场景，手动降级和自动降级的实现原理，以及降级机制值得注意的一些关键问题，这样我们就可以引入分级降级策略，来快速降低系统的负载，确保核心服务的可用性了。现在我们已经学习完了分布式系统稳定性的三板斧：熔断、限流和降级，以后对维护后端系统的稳定性就更有信心了。虽然熔断、限流和降级，很大程度上保障了系统的稳定性，但是从结果来看，它们都是通过放弃一定的用户体验和可用性，来确保系统在过载情况下依然正常运行的，这是一种通过有损节流，来降级系统负载的思路，那么有没有一种无损的方式，可以保障系统在过载下依然正常运行呢？其实，这个问题就引出了一个典型的扩容场景，在这节课中，我们将一起讨论保障分布式系统稳定性的最后一个方法——扩容，了解需要扩容的原因，讨论如何实现扩容，最后再一起分析扩容机制与云原生的关系。这里要说明一点，因为缩容是扩容的逆向操作，所涉及的思路，原理和扩容一致，所以在课程中就不分开说明了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:16:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么需要扩容 在“雪崩”系列的前三课中，我们分别介绍了解决分布式系统稳定性的三板斧：熔断、限流和降级，它们从系统底线的保障、核心服务的保障和非核心服务的牺牲这三个角度，全方位地保障着分布式系统的正常运行。但是，正如课程开始提到的，这些方法本质上都是对系统进行降级，通过有损的方式来保障系统不会雪崩。究其根本原因，熔断、限流和降级都是一种静态思维模式，当系统过载了，就通过各种方式来放弃一部分请求，降低系统负载，从而让系统恢复正常。我们在降级这节课中也提到过，从更广义上来讲，熔断和限流都是降级的一种特殊情况，都在做丢车保帅的事情。而扩容则是一种动态的思维模式，当系统过载了，就增加资源让系统重新恢复正常，而不是对系统进行降级处理，所以扩容是一种无损的系统过载恢复手段。 但是，扩容也会带来问题，我们需要用更多的资源来应对系统过载，也就是需要花费更多的钱。这是一个投入产出比（ ROI ）的问题，是通过有损降级恢复系统，导致用户的体验和可用性，以及用户口碑、品牌等方面的损失，与扩容资源投入的价值之间的比较。不过对于我们来说，这也不是一个二选一的问题，正常的情况下两个方式都会需要，我们在有损降级和扩容之间，找到适合自己的平衡点即可。 一般对于一个公司来说，在不同的阶段，对于平衡点的选择会有不同的倾向，早期公司会更倾向于使用有损降级的方向，而成熟公司会更倾向于使用扩容的方向，这其实就是由系统稳定性保障的 ROI 来决定的。那么，在拥有扩容机制之后，我们的雪崩处理策略也会发生变化，不论是像运营活动等计划内的流量突增场景，还是计划外的系统过载问题，我们都会先投入一定的资源对系统进行扩容，来应对系统的过载问题。如果扩容后，系统依然处于过载状态，那么就通过熔断、限流和降级等有损机制，对系统的稳定性进行兜底。而对于扩容应该投入多少资源，每个公司根据自己的情况来设置这个平衡点。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:16:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何实现扩容 通过上面的讨论，我们知道除了熔断、限流和降级之类的有损策略外，还可以通过扩容这样的无损策略，将系统恢复到正常的情况，这对于用户规模大、品牌价值强的公司来说，无疑多了一个非常好的选择。在对系统进行扩容的时候，首先我们需要评估出需要扩容的服务，以及需要扩容到什么样的容量，然后才能进行扩容。一般来说，对于运营活动之类的计划内的扩容，我们通过历史数据和经验来评估，而对于线上计划外的系统过载触发的扩容，我们就需要通过监控，来捕捉系统的过载服务和程度，然后才能进行扩容操作。一般来说，动态扩容的流程如下图。 那么接下来，我们先介绍如何通过自适应的方式，来判断服务是否出现过载问题，然后从自动扩容的角度讨论如何实现扩容机制。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:16:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"过载判断 过载判断是一件复杂的事情，如果我们打算通过基准测量，来确定服务过载指标，这将是一件无法持续的事情。因为服务会持续迭代，服务运行的硬件随时都有可能发生变化，这就会导致一种情况，即付出了巨大的工作量，但测量出的过载指标可能还是无法匹配线上运行，那么最终就会让过载判断出现错误，人为引入了故障。所以，我们需要寻找可以自适应的过载判断标准。 对于这个问题，有一个可行的方案，是我们在熔断这节课中介绍的，我们可以依据请求在队列中的平均等待时间来计算服务的负载。比如，一个服务在 1 分钟之内的平均等待时间超过 3 秒，我们就认为该服务进入过载状态。这里的“ 1 分钟之内的平均等待时间超过 3 秒”是一个自适应的指标，不论服务是否进行优化和迭代，以及服务运行在什么样的硬件上，我们通过这个指标来进行判断都是成立的。 但是，这个方式需要入侵到每一个服务的实现逻辑中，所有的服务都需要在实现时，暴露出接口请求的排队时间。如果有一个服务没有暴露，我们将无法捕捉到这个服务的过载状态，从而导致故障的发生。并且有些服务的实现，不会对接口请求进行排队，在这样的情况下，我们也就无法通过排队时间，来判断服务的过载情况了。 所以在熔断场景下，我们对服务的过载判断进行了简化，直接对服务接口请求的结果来进行判断，如果请求发生了过载原因导致的错误，并且超过一定的阈值时，我们就可以认为该接口是过载的。 经过上面的讨论，可能你会觉得对服务的过载判断还是比较难的，其实从本质上来说，过载判断是非常简单的，我们只需要知道服务的满载指标，接近或者超过这个指标就是过载。但是依据这个思路确定服务过载指标时，会有 2 个问题。 初始满载指标测量的工作量大：服务非常多，并且还会快速增长，需要持续测量每一个服务的满载指标。服务的满载指标是会变的：服务持续迭代，并且会运行在不相同的硬件上，导致满载指标是不稳定的。 而上述的 2 个问题，对于物理机器和 K8S 上的 Pod 这样的节点来说，都是非常容易解决的。 初始满载指标是硬件指标，不需要测量，可以直接从操作系统中准确获取，比如 CPU 32 核，内存 64G 等。一般为了避免出现过载情况，我们会相对保守，将满载指标按硬件指标的百分比来设置，比如 60% 之类的。满载指标是硬件的指标，是不会变的。 所以，另一个判断服务过载的方案是，将服务和节点一一绑定，一个节点上只运行一个服务，如果节点的系统指标过载，则说明该服务出现了过载，需要扩容。在一台物理机器上只运行一个服务，资源浪费会比较严重，而 K8S 上的 Pod 则是一个非常好的方案。当然，在某些业务场景下，我们认为服务每秒的 QPS 之类的指标，是决定系统过载最好的指标，我们也可以使用这个指标，来判断服务是否需要扩容。只不过我们要记住这个指标不是自适应的，在服务及其部署节点的性能发生变化后，我们需要再次评估好指标的阈值。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:16:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"自动扩容 判断出系统过载的服务以及过载的程度之后，对系统进行扩容就是一个自动化部署的事情了。自动扩容分为两个层面，一个是容器的层面，另一个是机器节点的层面。首先，对于容器层面的扩容有两个维度，一个是水平扩容，即通过增加服务的实例数量对系统进行扩容；另一个是垂直扩容，即通过升级服务部署节点的资源对系统进行扩容。在 K8S 中，Horizontal Pod Autoscaler（ HPA ）对应水平扩展，Vertical Pod Autoscaler（ VPA ）对应垂直扩展，具体的策略如下图。 一般来说，水平扩容不受单机硬件的限制，我们可以优先考虑，但是对于有状态服务，在水平扩容的时候，会涉及数据迁移。如果这个有状态服务，对数据的自动迁移原生支持不好的话，会给系统增加复杂度，这时垂直扩容是一个不错的选择。 其次，当我们进行容器层面的扩容后，整个集群的资源也会发生变化，如果集群的资源不足或者比较空闲，这时就需要进行机器节点层面的扩缩容了。对于节点层面的自动缩放涉及 Cluster Autoscaler（ CA ），它会在以下情况中自动调整集群的大小。 由于集群中的容量不足，任何 Pod 都无法运行并进入挂起状态，在这种情况下，CA 将向上扩展集群的容量。集群中的节点在一段时间内未得到充分利用，并且节点上的 Pod 是可以迁移的，在这种情况下，CA 将缩小集群容量。 CA 进行例行检查来确定是否有任何 Pod ，因为等待额外资源处于待定状态；或者集群节点是否未得到充分利用，如果需要更多资源，就会相应地调整 Cluster 节点的数量。 CA 通过与云提供商交互，来请求其他节点或关闭空闲节点，并确保按比例放大或者缩小的集群，保持在用户设置的限制范围内。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:16:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"扩容机制与云原生的关系 我认为自动扩容和缩容是云原生时代软件的标志之一，即利用云的能力来实现软件能力的弹性变化。你会发现，在云原生时代之前，所有的系统都部署在自己运维的 IDC 机房中，由于机房的成本是一次性投入的，不能按需使用，所以当时的扩容是一件笨重和昂贵的事情。 当时我们需要有计划地做容量预计，然后购买机器，再进行扩容。如果计划中，有流量巨大的运营活动，就需要提前进行扩容处理，并且在运营活动过去之后，流量降下来了，也没有办法进行缩容，这就会导致我们要为系统的峰值付费，是巨大的成本浪费。所以，如果那时系统出现了计划外的过载问题，熔断、限流和降级是更常用的方案。虽然一般来说， IDC 机房中会准备一定的备用机器，但是这些资源还没有弹性利用的机制，需要人工介入，效率非常低。 而现在则完全不一样了，K8S 与公有云结合，通过 Cluster Autoscaler（ CA ）请求增加节点或关闭空闲节点，可以为我们提供按需付费的弹性资源，这样一来，不论是在成本还是效率方面都有了非常大的改进，扩容和缩容将会变成一个自生而来的事情。所以，我认为系统能否利用公有云或私有云进行弹性扩容，是云原生系统的核心标志。并且在以后，扩容将是解决系统过载问题最常用的方法。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:16:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 在这节课中，我们先从故障恢复手段，对系统的用户体验性和可用性影响的角度，讨论了在有了三板斧之后，需要扩容机制的原因。通过这个讨论，你知道了扩容的作用和应用场景，在后续的工作中碰到相关的问题时，可以引入扩容机制。另外在如何实现扩容机制的讨论中，我们知道了如何判断一个服务是否过载，以及自动扩容的两个方式：水平扩容和垂直扩容，掌握了这些知识和原理后，你就能为你现在的系统引入一个扩容机制了。最后，我们一起探讨了扩容机制与云原生之间的关系，并且了解了云原生系统的核心标志是，能否利用公有云或私有云进行弹性扩容，在以后，扩容将是解决系统过载问题最常用的方法。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:16:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"13｜可观测性（一）：如何监控一个复杂的分布式系统？ 通过学习“雪崩”系列的内容，我们掌握了构建一个稳定的分布式系统所需的四大方法：熔断、限流、降级和扩容，再也不用担心由于一个局部的小问题，导致整个系统出现重大的故障了。在“雪崩”系列课程中，我们曾经提到需要基于系统内部的运行状态，来进行相应的降级和扩容操作，特别是在扩容机制中，需要通过服务过载的信息来进行相应的扩容，可是我们应该如何来获得系统内部的运行状态呢？其实这就是分布式系统中的可观测性问题，那么从这节课开始，我们将用 2 节课的时间来讨论，如何通过分布式系统的可观测性，来解决系统监控与告警的问题。在这一节课中，我们先讨论需要监控的原因，然后分析监控与可观测性之间的关系，接着介绍搭建一个可观测性系统涉及的开源组件，最后，重点讨论对于一个大规模的分布式系统，设计监控系统应该遵循的经验和原则。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:17:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么需要监控 如果一辆汽车没有仪表盘，我们就不知道汽车当前的速度，只能凭着感觉开，很容易出现超速甚至意外，另外由于不知道当前还有多少汽油或者电量，一不小心就会因为能源耗尽抛锚在路上。监控之于分布式系统，更甚于仪表盘之于汽车，因为分布式系统的内部更加复杂，更容易出现意外的情况。那么对于“为什么需要监控”的这个问题，我们就从监控有哪些作用的角度来回答。 第一，从规则角度，监控信息是扩容、缩容和报警等规则的数据来源。只有通过监控了解了系统的状态信息，才能基于状态信息设置一定的规则，当规则满足后，就触发扩容、缩容和报警等相关处理。第二，从全局角度，基于监控信息，我们才能构建监控大盘。监控大盘能让我们快速地了解当前系统的情况，并且能回答当前系统表现的一些基本问题。第三，从长期角度，通过监控信息，可以分析系统的长期趋势。比如从系统当前磁盘的使用情况和增长速率，我们可以推测出什么时候需要进行扩容。第四，从实时角度，在系统出现变更的时候，可以通过监控系统，迅速了解最新的变更是否异常。比如缓存命中率是否下降，请求时延是否变长。第五，从调试角度，当系统出现报警信息的时候，通过监控系统能帮我们快速定位问题。在前面的“雪崩”系列中，虽然我们已经知道如何保障系统的稳定性了，但是既然故障出现了，就一定要定位到根本原因，然后彻底去解决。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:17:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"监控和可观测性之间的关系 在 2018 年以前， IT 领域一直使用监控这个术语，来表示通过采集系统、服务和网络的内部信息，诊断和预测系统的行为。到了 2018 年， CNCF Landscape 率先出现了 Observability 的概念，将可观测性（ Observability ）从控制论（ Cybernetics ）中引入到 IT 领域。在控制论中，可观测性是指系统可以由其外部输出，来推断其内部状态的程度，系统的可观察性越强，我们对系统的可控制性就越强。自此以后，“可观测性”逐渐取代了“监控”，成为云原生技术领域最热门的话题之一。那么，在 IT 领域中，为什么会用“可观测性”逐渐取代“监控”呢？我们先来感性认识一下，一般来说，监控主要告诉我们以下的信息，这些信息主要表现为结果。 CPU 超过 80% 。系统负载超过 200% 。机器宕机了，服务崩溃了。 而一个可观测系统，除了告诉我们这些结果信息之外，还需要能回答出导致这些结果的原因。 性能问题是由什么原因导致的？瓶颈在哪里？请求执行过程都需要经过哪些服务？请求失败的原因是什么？每个服务如何处理请求？服务之间的依赖关系是什么样的？ 从上面的对比中，我们可以初步了解到监控和可观测性之间的区别，接下来，我们再进一步从基本概念的角度，来讨论监控和可观测性概念的差别。 如下图，在 IT 建设中，我们将“可观测性”能力划分为 5 个层级，其中告警（ Alerting ）与应用概览（ Overview ）都属于传统监控的概念范畴，因为触发告警的往往是明显的症状与表象。但在云原生时代，架构与应用部署方式的变化是非常频繁的，不告警并非意味着一切正常，因此，通过获取系统内部的信息，来主动发现（ Preactive ）问题就显得非常重要了。 可观测性通过排错、剖析与依赖分析，这三个部分来主动发现故障，具体如下。排错（ Debugging ），即运用数据和信息去诊断故障出现的原因。剖析（ Profiling ），即运用数据和信息进行性能分析。依赖分析（ Dependency Analysis ），即运用数据信息追踪系统模块的依赖关系，进行关联分析。 并且，这三部分的逻辑关系是：首先，无论是否发生告警，运用主动发现能力，都能对系统运行情况进行诊断，通过指标呈现系统运行的实时状态；其次，一旦发现异常，逐层下钻，进行性能分析，调取详细信息，建立深入洞察；最后，调取模块与模块间的交互状态，通过链路追踪构建“上帝视角”。因此，主动发现能力的目的，并不仅仅是为了告警与排障，而是通过获取最全面的数据与信息，构建对系统、应用架构最深入的认知，而这种认知可以帮助我们提前预测与防范故障的发生。 通过上面的讨论，我们可以看出可观测性是监控的扩展和进化，监控是建立在可观测性收集的数据之上的，我们可以结合下图，从三个维度进行理解。 首先，监控到可观测性是从黑盒往白盒方向的进化。监控更注重结果，即当前出现了什么问题，或者将要出现什么问题；而可观测性也同等关注问题出现的原因，即通过内部状态的展示来回答这个问题。其次，监控到可观测性是从资源往服务方向的进化。监控的使用主体主要是运维，监控的对象主要为系统资源相关，而在云原生时代，分布式系统越来越复杂，仅仅通过系统层面监控是远远不够的，所以可观测性使用的主体引入了研发。通过研发的加入，增加了服务内部状态和服务之间调用关系的暴露，大大增强了我们对系统的控制力。最后，监控到可观测性是处理方式从被动监控到主动分析的进化。在监控为黑盒的情况下，我们完全不了解系统内部的情况，只能等待监控信息触发报警后，再进行处理；而当监控为白盒的情况下，研发人员可以方便地了解系统内部运行的情况，并且进行分析，主动发现问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:17:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"可观测性系统的开源组件 搭建一个可观测性平台，主要通过对日志（ Logs ）、链路（ Traces ）与指标（ Metrics ）这三类数据进行采集、计算和展示，它们的具体信息如下。 日志信息（ Logs ），即记录处理的离散事件。它展现的是应用运行而产生的信息，或者程序在执行任务过程中产生的信息，可以详细解释系统的运行状态。虽然日志数据很丰富，但是不做进一步处理，就会变得难以查询和分析。目前，我们主要通过 ELK 来处理。追踪链路（ Traces ），处理请求范围内的信息，可以绑定到系统中，单个事务对象的生命周期的任何数据。在很大程度上， Traces 可以帮助人们了解请求的生命周期中，系统的哪些组件减慢了响应等。目前，我们主要通过分布式调用链跟踪系统 Jaeger 来处理。指标信息（ Metrics ），它作为可聚合性数据，通常为一段时间内可度量的数据指标，透过它，我们可以观察系统的状态与趋势。目前，我们主要通过 Prometheus 进行采集和存储，通过 Grafana 进行展示来解决。 基于上面的开源组件，我们可以很方便地搭建一个可观测性平台，来提升极客时间后端分布式系统的可观测性。同时，我们也可以看出，当前对于可观测性的三类数据 Logs 、 Traces 和 Metrics 的处理系统是割裂的，这会导致它们相互之间的数据不通、标准不统一、组件繁杂。所以 CNCF 社区推出了 OpenTelemetry 项目，旨在统一 Logs、 Traces 和 Metrics 三种数据，实现可观测性大一统，这是一个非常有雄心的计划，目前正在推进中，我们敬请期待。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:17:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"监控系统的设计经验 在可观测性的五个层次中， Overview 和 Alerting 这两个部分和业务结合得非常紧密，完全是依照业务场景来定制的。并且 Overview 是我们使用可观测性系统主要的入口， Alerting 是可观测性系统将故障通知到工程师的通道，在可观测性系统中，它们是和工程师日常工作非常紧密的两个部分。因此，一个设计良好的 Overview 和 Alerting 非常影响可观测性系统整体的使用效率和体验，那么接下来，我就将我在工作过程中，对设计 Overview 总结的一些经验分享给你， Alerting 将会在下一课中介绍。 分层设计，每一层都有自己的 Overview 在公司中，每一个层级的工程师，所关心的目标是不一样的，所以， Overview 应该分层设计，你关心的目标是什么，你的 Overview 就应该展示什么。一般来说，层级越高，关心的事情越偏业务，不要将各层的关注点混合在一起。 首先，对于整个研发部门来说， Overview 展示的是，能够实时体现公司业务状态最核心的指标，例如 Amazon 和 eBay 会跟踪销售量， Google 和 Facebook 会跟踪广告曝光次数等与收入直接相关的实时指标；而 Netflix 由于是订阅制，销售数据不实时反映业务的情况，则通过反映用户满意度的指标——播放按钮的点击率来替代，即视频每秒开始播放数，简称为 SPS（ Starts Per Sencond ）。其次，对于运维研发、 DBA 、基础服务之类的各级研发团队，同样需要有自己的 Overview ，总体原则依然是，团队的目标是什么， Overview 就应该展示什么，并且整个团队的人都需要关心这个 Overview。最后，最低一层的 Overview 对象不是工程师，而是为每一个服务、机器节点建立 Overview 。因为相对于部门组织来说，工程师的数量大，并且变化比较频繁， Overview 的维护成本非常大。并且，个人查看自己负责的服务和机器的 Overview 需求，可以通过团队、服务和机器节点的 Overview 来解决。 通过对 Overview 分层设计，保证了每一层的专注力都能聚焦在核心指标上，如果上层指标出现异常，我们可以查看下一层的指标，进一步诊断问题，并且结合 Logs 和 Traces ，直到找到问题发生的原因。 少就是多， Overview 不要超过一个页面 在分层设计中，我们已经知道对 Overview 进行分层设计的方法了，这里我们主要来讨论 Overview 内容的设计经验。除了上文提到的“ Overview 应该分层设计，你关心的目标是什么，你的 Overview 就应该展示什么”之外，还有一个非常重要的原则是，一个 Overview 不应该超过一个页面。首先，在使用上，我们经常要持续观察系统的情况，如果一个 Overview 有多个页面，我们就需要不停地切换，这非常影响我们的专注力，效率非常低。其次，在内容上， Overview 的信息应该越精简越好，精简到不能再精简为止，如果超过了一个页面，就说明信噪比比较低，聚合和精简做得不彻底。最后，在日常维护上，Overview 不超过一个页面，相当于规定了指标的最大数量，也就避免了我们只增加指标，从来不去清理的问题。 四个黄金指标 Google SRE 团队在介绍它的监控系统时，明确说明监控系统的四个黄金指标是延迟、流量、错误和饱和度，如果系统只能监控四个指标，那么就应该监控这四个指标，具体介绍如下。延迟是指服务处理某个请求所需要的时间。我们在计算延迟的时候，应该区分成功的请求和失败的请求，比如，某一个接口的一个实例触发了熔断，被熔断的请求时延非常低，如果将熔断的请求时延和正常请求一起统计的话，就会产生误导性的结论。流量是用来度量系统负载的。对于 API 接口来说是请求的 QPS ；对于音视频流媒体来说，是并发数或者网络 I/O 速率。错误是指请求失败的速率，包含显示错误，比如 HTTP 500 以及隐式错误，比如 HTTP 200 回复中包含的错误。饱和度描述的是，系统当前的负载占满载的百分比，一般来说，以整个系统最受限的资源的指标来表示，比如，对于 Redis 这样的内存系统，内存就是它的饱和度指标，假设当前机器使用内存为 16 G ，机器总内存为 32 G ，那么当前系统的饱和度为 50%。 选择合适的度量方法和采样频率 分位值是指把所有的数值从小到大排序，取前 n% 位置的值，即为该分位的值。它更加能描述数据的分布情况，比如，请求时延 60% 的分位置为 3 s ，说明有 60% 的请求时延小于或者等于 3 s ，40% 的请求时延大于或者等于 3 s ，依据这个信息，我们就能判断出，当前接口时延大于 3 s 的比例。由于平均值容易受到少数极值的影响，所以，当请求时延的平均值为 3 s 时，我们不能判断出接口时延的真实情况。比如， 100 个请求的时延为 10 ms，有 1 个请求的时延为 102 s 的情况下，虽然平均时延很大，但是 99% 的请求时延都很快，在 10 ms 以内。一般情况下，我们经常会通过计算多个分位值，来进一步了解数据的分布情况，比如，请求的时延情况，我们经常会通过 80% 、90% 、99% 、99.9% 多个分位值来度量。而对于采样频率，我们需要对系统的不同部分，选择不同的频率。比如 CPU 使用率的变化是非常大的，我们可以选择 1 s 采集一次，而对于变化非常缓慢的磁盘容量， 1 分钟一次的频率可能都高了。并且，我们可以通过降低历史数据的采样频率，来降低存储空间，提高访问速度。比如，对于 CPU 使用率，超过一个月的数据，可以从 1 s 一个采样点，聚合为 1 分钟内只保留 3 个数据：最大值、最小值和平均值。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:17:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课，我们先从规则、全局、长期、实时和调试的角度，分析了需要监控的原因，这样你就明白了监控的重要性。通过讨论两个很容易混淆的概念，即监控和可观测性之间的关系，我们将多个维度进行对比，得出监控是可观测性的一部分，可观测性是监控的扩展和进化这个结论。为了了解开源社区主流的实现方式，我们介绍了可观测系统需要收集的关键数据：日志（ Logs ）、链路（ Trace ）与指标（ Metrics ），这样你就知道如何搭建一个可观测性平台了。最后，我们讨论了监控系统的设计经验，依据这些设计经验，你在设计一个监控系统的时候，就能游刃有余了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:17:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"14｜可观测性（二）：如何设计一个高效的告警系统？ 通过上节课的学习，我们掌握了在可观测性体系中，监控的位置和重要性，以及设计一个监控系统的基本原则，这样我们就可以为极客时间搭建一个可观测体系，并且设计一个简洁有效的监控系统了。但是，只有监控还是不够的，因为我们不能一直盯着监控系统，所以需要通过一些规则，自动从监控的信息中发现问题，实时通知给负责的工程师，让工程师实时接入来处理。那么解决这个问题的有效方法就是告警，你作为工程师，应该收到过各种各样的告警信息，并且及时解决了很多线上问题。但是，你也一定收到过很多无效的报警信息，这些信息浪费了我们的精力；有时线上故障真的发生了，反而会出现收不到告警信息的情况，导致我们错过了最佳的修复时间。其实这是因为告警系统的设计不够高效，那么在本节课中，我们将一起来解决这个问题。我们会先讨论一个告警系统的评价指标，然后基于我的亲身经历，来讨论如何进行告警的治理，最后再来总结告警系统的设计经验。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:18:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"告警系统的评价指 标告警系统的作用是把线上已经出现，或即将出现的故障及时通知给我们，所以一个理想的告警系统应该是不多报，不漏报，报对人。即所有的通知都是有效的，是需要立即处理的；所有的故障或即将出现的故障，都有告警通知；所有通知的接受对象，应该是处理这个问题的最佳人选。 从上面的讨论中，我们可以得出如下的三个指标，来评价一个告警系统。信噪比：指有效告警通知数和无效告警通知数的比例，信噪比越高越好，是用来评估“多报”问题的。覆盖率：指被告警系统通知的故障占全部线上故障的比例，同样，覆盖率也是越高越好，是用来评估“漏报”问题的。转交率：指被转交的告警通知数占全部告警通知数的比例，转交率越低越好，是用来评估“对比人”问题的。 综上，依据这三个指标，我们能够评估一个告警系统的设计是否高效。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:18:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"告警治理案例 关于如何告警治理，在我的工作中，有过一个非常有意思的经历，我在这里分享给你。我们的告警系统会自动分析每个服务实例的日志，通过日志等级和数量自动生成告警通知，例如下表所示的规则。 案例背景 因为公司业务发展非常快，工程师的数量快速增长，所以业务需求也快速地迭代，我们都无暇对报警信息进行及时清理和处理。记得那时候，一天当中告警系统发出的告警通知有几千条，我们已经进入了恶性循环当中，告警通知越多，越不关注告警以及处理报警信息，导致告警通知变得更多了。其实我们很清楚，这是一个非常不好的问题，整个告警系统的信噪比太低了，告警系统已经形同虚设，很容易因为漏过告警通知，而错过问题处理的最佳时间，导致更大的故障出现。于是，我们决定采取一些行动来解决这个问题。 首先，我们通过开会来强调这个问题，希望提高所有人的重视度，让工程师们及时处理告警通知，并且清理不需要的告警通知，提升系统的信噪比，使整件事情进入一个正循环。在会后一段时间内情况有好转，但是后来又慢慢恢复到了之前的状态，其中一个主要原因是研发的工作比较忙碌。虽然所有人都明白清理好报警的长期受益，但是这个事情需要持续去优化才能显示出效果，我们坚持做几天问题不大，可是持续坚持就非常难了。这个现象在减肥、跑步、健身等场景里太司空见惯了，做一件事情很简单，但是长期坚持做一件简单的事情却非常难。 然后，我们开始思考既然开会不行，那么就通过统计数据，做一个服务的告警排名，并且每天都公开发布，让所有人了解自己负责服务的情况，基于排名开始竞争起来。 Leader 们也通过为告警少的服务点赞，推动告警多的服务做清理。刚刚开始实行时，效果很不错，但是不久后，所有人就很难坚持下去了，工程师们对告警排名麻木了，效果越来越差。在上面的两个方法依然不见收效后，我们仔细分析了出现这个问题的原因。 通知机制太弱：当时的告警通知都发在一个群里面，然后 @ 相关的负责人，人们很容易忽略掉。其实当时也可以通过规则来设置电话报警，但是这个需要你主动来配置。推动清理的粒度太大：不论是开会强调，还是告警排名，推动的频率都是以天来计算的，可是经过一天的时间，已经积累了很多告警，处理的压力增加，动力就会差很多。 解决方法 进一步找出问题后，我们对告警通知采取了下面的处理机制。基于服务等级（服务等级在第 11 讲雪崩（三）中有详细介绍）对所有的告警通知，启用电话报警规则，并且严格执行，具体如下表所示。 我们这样设计的原因，主要建立在这两个认识上： 第一，相信工程师，并且让工程师自己负责起来。这里主要指的是，相信工程师对它负责的服务发出的告警通知都是有效的，并且他是有能力和义务来做好这一点的，所以，所有的告警通知都需要经过严格的电话告警规则来认真对待。 第二，通过分级机制来提高处理效率，避免频繁骚扰。虽然告警通知都是有效的，但是故障越大，告警通知就一定会越多。那么当频率非常低的时候，大概率是偶发性的问题，我们可以让告警信息进入工单，后续再处理，不需要立即打断工程师的工作。同时，利用服务等级信息来唯一确定电话告警的阈值，在工程师的工作效率和故障处理的实时性之间，找到一个平衡点。 在方案评估会议时，工程师们纷纷表示，如果电话报警太多，能不能自己来调整服务的电话报警阈值，我们给出的回复是不行的，具体的解释如下。 自定义阈值很容易出现阈值设置不合理的情况，导致覆盖率降低；同时自定义报警阈值会让工程师们，对于报警通知重要程度的理解不一致，增加沟通的成本。如果一定要调整阈值，只能通过调整服务等级的形式来实现。但是如果调低服务等级，该服务在运维层面的资源保障也要跟着降低，我们主要通过这个机制来进行制衡，通过降低服务等级来提高报警阈值的问题。电话报警多，说明需要清理服务告警的时候到了。 后来，这个告警方案实行的第一周，所有人确实都比较辛苦，报警电话很多，经常需要打断工作进行处理。但是 2 周后，电话报警就非常少了，我们开始进入了一个正常的状态，并且这个机制是持续、实时生效的，这样服务的告警问题就彻底解决了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:18:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"告警系统的设计经验 讨论完上面的告警系统治理案例，接下来我会结合工作中对告警系统的设计，分享如下的设计经验。首先，对于告警系统，“相信工程师，并且责任到人”和“利用服务等级信息，来建立告警规则”，这两点在我们的案例中讨论得比较多，就不再赘述了。不过，这里要特别强调一点，服务的等级信息，是我们对分布式系统，或服务运维、治理中最重要的元数据，是其他系统可以依靠的、非常关键的一个分级依据。 其次，为了避免告警通知的单点问题，如果服务的负责人没有及时处理，我们就可以依照组织架构逐级上升。比较推荐的一个告警信息的处理流程是，负责人在收到告警的电话通知后，在告警信息的通知群里面，点击“正在处理”，这样该类型的告警会自动抑制一小段时间，避免告警信息的过度骚扰。再次，告警规则应该简单易懂，工程师看到告警信息，就能知道触发的原因，告警规则的可解释性对于告警的处理非常重要。这一点 Google 也说明过，他们当前还没有使用基于 AI 的告警规则，就是为了确保告警规则的可解释性。最后，比故障更严重的问题是告警缺失。告警缺失会使我们错过处理故障的最佳时机，导致故障被放大。总之，作为一个工程师，如果让用户来告诉你系统出现故障了，是一个非常羞愧的事情，所以我们一定要比用户先知道。 我们都明白，故障是无法 100% 避免的，但是告警却可以保证不缺失。因为告警是多层网状覆盖的，其中一个地方的故障，往往会导致多个层面出现报警信息，除非所有层面都告警缺失了，才会出现问题。比如，一台机器突然崩溃了，虽然我们的系统可以自愈，但是在应用层面，当时正在调用这台机器上服务的调用方，会由于调用失败而告警；在机器层面，监控机器存活性的程序会告警。所以，在故障复盘中，如果故障发生时，告警缺失了，这就是一个必须严肃讨论的问题，我们需要思考，出现告警缺失的原因，以及还有没有类似的报警缺失。 对于告警缺失的问题，我们还需要注意一点，就是告警系统自身的问题，比如告警系统出现故障了怎么办，它的告警通知由谁来发送。我们可以考虑做一个独立的程序，来监控告警系统，并且，这个程序有独立发送告警信息的通道，通过这个独立的程序与告警系统，相互监控和告警来解决这个问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:18:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 在这节课中，我们总结出了告警系统的三个评价指标：信噪比、覆盖率和转交率，这样你就有了评估一个告警系统是否高效的依据，可以利用它来评估你现在使用的告警系统。接着，我们通过一个告警治理的真实案例，了解了告警治理的难点，以及如何一步步分析、权衡，最后解决这个问题，你可以把这个案例作为对照，去思考你当前告警治理的情况。最后，依据告警系统的设计经验，你在设计一个监控系统时，就能游刃有余，少走很多弯路了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:18:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"15｜故障（一）：预案管理竟然能让被动故障自动恢复？ 通过学习“监控”与“告警”这两节课的内容，你已经学会如何利用 Metric、Trace 和 Log 搭建一个可观测系统，去监控极客时间这样的分布式系统。并且知道了在系统出现故障时，职责明确的告警机制，可以在第一时间通知到相关的工程师。但是，我们现在还不能掉以轻心，因为极客时间是 7 * 24 小时无间断为用户提供服务的，能掌控和发现故障还不够，如果故障出现了，我们还必须能快速恢复故障。所以本课我们一起来讨论另外一个非常重要的问题：如果系统发生故障了，我们应该怎么来快速恢复故障？故障恢复是一个非常复杂的问题，这里我们首先要讨论的是：怎么理解故障，以及它的评估标准是什么？只有定义好问题，并且确定好标准，我们才能明确解决问题的方向。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:19:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何理解故障及其评估标准 对于如何理解故障和故障评估标准的问题，我认为可以从两个方面去理解和思考。首先，评价故障的标准一定不是有或没有。虽然我们不希望有故障发生，但这却是所有的工程师必须面对的问题。同时，我们不能出现故障就处理，没有故障我们就什么也不做，我们要积极地应对故障。在系统设计的时候，应该充分考虑到故障的存在，并且做好充分的预案，才能在故障发生时，将系统的影响降到最低。既然我们必须面对故障，那么应该如何评估一个故障的水平呢？我认为有两个指标非常重要。 平均出现故障的频率：指平均多少时间出现一次故障，这个频率越低越好。平均故障恢复的时间：指出现故障后，系统在多长时间恢复到正常状态，这个时间越短越好，并且，我认为这是一个更关键的指标。 我们可以这样来思考上面的两个指标，假设我们的系统可用性 SLA 是 99.99%，那么只要全年故障时间不超过 52 分钟，就是符合要求的。但是，对于这个 52 分钟的故障时间，却有不同的情况，如下表所示，如果是在线用户数等其他情况一样的前提下，1 个持续 52 分钟的故障和 52 个持续 1 分钟的故障，还有 3120 个持续 1 秒的故障，你认为哪一种情况对用户的影响最小呢？ 聪明的你一定会选择 3120 个持续 1 秒的故障，因为这样的故障只会导致用户的某几个请求失败，用户自己或者系统内部自动重试一下就好了，对体验的影响非常小。并且，这种情况用户一般是能接受的，因为在进出电梯等情况下，也会经常出现偶发的网络错误。如果是 52 个持续 1 分钟的故障，那么用户会明确感知到系统出现故障。而如果是 1 次 52 分钟的故障，那么故障期间，所有的用户都不能使用系统提供的服务，其中的损失和影响是无法估计的。当然，这里并不是说平均出现故障频率这个指标不重要，如果频繁出现短暂的故障也会影响到用户的体验。 所以，我们通过上面的讨论，就得到了两个关键的结论：一方面我们应该在事前做好故障避免，降低平均出现故障的频率；另一方面，如果出现故障了，降低平均故障恢复的时间是非常关键的指标。 到这里，我们就知道了出现故障后，快速恢复是非常关键的指标。那么，具体应该如何快速恢复故障呢？对于这个问题，根据我长期在系统稳定性建设方面的经验，我认为可以采用分治法。根据故障是由于宕机等被动原因，还是由于系统迭代过程中，人为引入等主动原因导致的，将故障分为被动故障和主动故障，然后我们再一一来讨论如何快速恢复。由于故障恢复涉及的内容比较多，所以我们将用两节课的时间来讨论这个问题，这节课我将先和你一起讨论如何快速恢复被动故障。通过分析被动故障的来源，我们可以从中归纳出被动故障的特点，推导出处理被动故障的方法——预案调度，然后再通过一个案例，来讨论预案调度具体的实施方法，最后梳理被动故障的预案，和你分享我的经验。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:19:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"被动故障的分析与思考 被动故障的来源 我们先一起来分析一下都有哪些原因会引发被动故障。依据我们对被动故障的定义，从用户的 App 发起请求到系统提供服务的过程中，被动故障的来源主要出现在以下四个地方。DNS 解析问题：用户本地网络的 DNS 服务不能将我们的域名正确解析到 IP 地址。网络连通性问题：用户已经解析到正确的 IP 地址，但是从用户网络到我们服务器的 IP 地址之间的网络慢或者不通。系统内部的硬件设施故障：比如机器突然宕机，内部网线中断等。系统依赖的各种第三方服务：比如 CDN 服务、短信网关、语音识别等第三方服务故障。 当我们结合平时工作中的案例去分析上述问题时，会发现被动故障有一些特点，首先是它每一次出现的原因都各不相同。比如 DNS 解析的问题，有可能是用户本地的 DNS 服务配置错误，也有可能是 DNS 服务器网络的问题，还有可能是 DNS 服务器的问题，并且这些原因都不受我们的控制，这也是被动故障的一个特点。 处理被动故障的思维方式 对于各不相同以及完全不受我们控制的原因，应该如何处理呢？其实这个问题在计算机领域有非常明确的答案：计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决。由于出故障的地方不受我们控制，并且每一次故障的原因可能都不相同，通过 case by case 的方式，来一一来解决是不可能的。但是也正是因为不受我们的控制，所以这些出问题的地方都有相对标准的服务和方案，不论是服务还是硬件，不会随着业务的变化而快速变化，并且故障来源的数量是非常有限的。那我们就可以从细节中跳出来，从更高层次的角度来思考这些问题，对于每一个故障来源，我们都可以准备多个预案，然后通过增加一个中间层来进行自动调度，对外屏蔽这些问题，从而达到快速恢复故障的目的，我们将这个方法称之为预案调度。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:19:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"通过预案调度来恢复故障 接下来，我们一起来讨论在实际工作中，如何通过预案调度来处理被动故障。就拿我亲身经历的一个真实故事来说吧，当时我们就是用了预案调度，解决了第三方服务 CDN 的下载故障，以后你处理类似问题的时候，也可以借鉴其中的思路。公司在刚开拓东南亚业务的时候，因为那边的基础设施比国内要差不少，所以用户经常出现图片加载失败的情况，非常影响他们的体验。 当时，研发工程师们将国内、国外所有的 CDN 供应商都换了一遍，但每一家 CDN 供应商都或多或少出现过问题，比如 CDN 供应商 A 在越南的服务质量不错，但是在泰国很不好；CDN 供应商 B 则是在泰国的服务质量不错，但是到了马来西亚却很不好。并且工程师们还没有任何的办法来优化 CDN 的问题，因为这些都是第三方 CDN 供应商自身的问题。那时候，我们也走了一些弯路，花了不少的时间去定位问题，通过客户端的日志分析出用户 CDN 图片加载失败的时间点、 URL 和当时域名解析的 IP 地址，然后推送给 CDN 供应商来优化。这样做有一定的效果，但是需要注意两个问题，一是，我们只能在故障出现后去解决问题，这时用户已经被影响了，体验很不好；二是，导致故障的问题没有办法收敛。因为网络本身是动态的， CDN 供应商将一个接入点的网络质量优化好，等到下次外部网络环境发生变化，就又会出现新的问题。结合上述思考，最后我们采用如下的方法解决了问题：对每一张图片都提供两个以上供应商的 CDN 链接返回给客户端，并且根据之前的网络访问数据，统计其网络质量，按照质量从优到低排序；客户端则依据返回的 CDN 列表，从优到低下载图片，直到下载成功为止。 现在我们来总结一下，通过预案调度解决被动故障的思路。首先，使用“对每一张图片都提供两个以上供应商的 CDN 链接，并且按质量从优到低排序”的方法，其实就是为每一张图片的 CDN 服务准备了多个下载预案，并且这些预案是有优先级的。然后，使用“客户端依据返回的 CDN 列表，从优到低下载图片，直到下载成功为止”的方法，就是在客户端实现了一个，通过 CDN 链接下载图片的预案调度层，它依据当时的网络情况，择优选择 CDN 供应商，来提供图片的下载服务。就这样，这个问题就彻底解决了，一个核心思考点是：虽然每一家 CDN 供应商都有下载失败的情况，但是一张图片几乎不会出现，所有 CDN 供应商都下载失败的情况。 通过上面的例子，我们将预案调度的思路总结为下图。之前，我们的业务系统直接调用标准服务，在增加调度层之后，业务系统直接调用调度层，不需要关心具体的标准服务（即预案）。所以我们可以通过增加调度层的方式，来屏蔽各种预案之间的差异，并且可以在不同的预案之间进行自动最优的调度。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:19:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"被动故障的预案梳理 通过上面的学习，你已经掌握了通过预案调度来解决被动故障的思路，这个时候，你一定想知道，有哪些场景适合通过预案调度，来快速恢复被动故障呢？我从被动故障来源的维度，给你总结了解决每一类被动故障的预案列表，你可以先通读，理解一下预案列表的设计思路，在有需要的时候，可以直接查询这个列表，具体的预案细节如下。第一，对于 DNS 解析问题，因为是解析服务失败导致的，所以我们可以通过提供不同的域名解决服务来解决问题，依据可靠性从高到低依次为。 第二，网络连通性的问题与网络链路有关，我们可以通过提供不同接入的网络链路来解决问题，依据实现成本从低到高依次为。 第三，对于系统内部的硬件设施故障的问题，因为设备是我们可以控制的，所以处理方式比较简单，直接通过冗余设备的方式来解决。 第四，对于系统依赖的各种第三方服务，我们可以通过提高服务供应商的质量和数量来解决问题，依据实现成本从低到高依次为。 并且，对于“DNS 解析预案”和“网络连通性预案”这前两个预案来说，它们都是内置在客户端的，我认为比较好的方案是，将这两个层面实现一个统一的调度层，这一个调度层不仅用来快速恢复故障，还可以通过 App 端对网络性能数据的统计，实时提供当前网络性能最好的接入服务和 IP 地址。对于“系统内部的硬件设施预案”，它的调度层可能不需要我们再次实现，因为系统的高可用已经覆盖了这个部分。比如机器宕机会导致上面跑的服务实例都挂掉，系统的服务注册发现模块会实时摘除这些实例的 IP 和 Port 信息，并且通知给相关的调用方。对于“第三方服务的预案”，它的调度层可以实现为，通过实时的数据统计，来为我们的系统选择质量最好的第三方服务并且使用其服务。通过上面被动故障的预案梳理，我们掌握了不同故障的预案都有哪些，这个不一定全面，如果你有更好的方案，也可以继续补充，这节课更重要的目标是讨论处理故障的思维方式。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:19:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课，我们先讨论了应该如何理解故障，以及故障的评估标准：平均出现故障的频率和平均故障恢复时间。在 SLA 一定的情况下，平均故障恢复时间越短，对用户体验的影响就越小，所以快速恢复故障是一个非常关键的目标。接着，我们分析了被动故障的来源都有哪些，我们发现这些故障完全不受我们的控制，可能每一个 case 的原因都不相同，我们很难通过穷举来消除故障，因此我们决定对可能出现故障的地方增加多个预案，通过增加一个中间层来进行调度，对外屏蔽这些问题，从而达到快速恢复故障的目的。我们还详细讨论了被动故障各个来源的预案，在调度层通过对各个预案的实时数据统计，不仅能提供可用性非常高的服务，还可以为系统提供最优质的服务，这个是提供高质量服务非常关键的优化点。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:19:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"16｜故障（二）：变更管理，解决主动故障的高效思维方式 通过上一节课的学习，你已经理解了系统故障的评估标准，并且明白了在 SLA 一定的情况下，平均故障恢复的时间越短，对用户体验的影响就越小，所以快速恢复故障是一个非常关键的目标。接着，我们采用分治法，将故障分为被动故障和主动故障，讨论了如何通过预案调度快速恢复被动故障的策略。相信你已经对被动故障如何处理心中有数了，但是，我们对于故障恢复的处理还远远没有结束。根据极客时间以往的故障报告进行分析，我们会发现很多故障都是在系统迭代过程中，人为引入的主动故障，比如发布新版本服务引入的 Bug 和崩溃等。所以，在这节课中，我们就继续来学习，如何处理由于主动原因导致的系统故障。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:20:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"主动故障的分析与思考 首先，我们一起来思考一下，主动故障是否也可以通过预案调度的形式来快速恢复呢？答案一定是不可以的。我们来回忆一下被动故障的特点，虽然出现被动故障的地方，不受我们的控制，但是它有相对标准的服务和方案，不会随着业务的迭代而快速变化，所以处理被动故障时，我们准备多预案的成本是可控的。 而主动故障是工程师们在业务迭代过程中，人为引入的故障，如配置错误、代码 Bug 等，它来自于我们的业务系统，我们不可能为了做预案，同时组织两个不同的研发团队，分别开发同一个业务系统，这个多预案的成本实在是太高了。 如果预案调度的形式不可用，那么我们应该如何快速恢复主动故障呢？当我们去分析主动故障时，会发现每一次发生主动故障的原因都不太相同，比如需求理解错误、逻辑考虑不全面这些不可穷举的问题。但是我们可以通过分析问题的根源确认一点，主动故障主要是工程师们在业务迭代过程中引入的，也就说明如果业务系统没有发生迭代变化，就不会发生主动故障。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:20:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"主动故障的来源与处理方法 通过对根本原因的分析，下面我们就可以从工程师的哪些行为，会导致系统发生变化的角度去思考了，这样问题就能很快收敛了。你可以想一想，在日常工作中，我们碰到的主动故障来源是什么，是否几乎包含在下面几类中。 程序发布变更：指服务器、App 和 Web 等发布了新版本的程序和服务。实例数目变更：指服务器新增实例和下线实例。配置发布变更：指发布了新版本的配置。运营策略变更：指举办了导致用户流量增长的运营活动，比如购买了新的推广广告等。 虽然发生主动故障的具体原因各不相同，但是它的来源却只有这几个。所以对于这种情况，我认为快速恢复主动故障，可以从变更入手：出现主动故障的时候，如果我们没有足够的信息，去判断当前的故障出自什么原因，我们就应该第一时间定位故障可能存在的范围，比如某一个服务或者数据库，然后我们就去看这个服务或者数据库最近是否有相关的变更，依据变更信息来确定故障恢复方案。 但是，在突发线上故障这种高压力、争分夺秒的情况下，我们应该如何准确、快速获得主动故障相关的服务和数据库的最新变更呢？如果只是在故障现场去询问工程师显然是不行的，可能会出现询问的人不全、回答的信息缺失或者不正确等问题，这会对故障的快速恢复造成非常严重的影响，甚至还可能出现更大的故障。这一类信息的收集、展示和查询需求是非常适合用管理系统来解决的，所以，一个自动化的变更管理系统是非常有必要的。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:20:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"变更信息的管理 上文中的讨论，让我们明确了变更管理系统可以收集，整个分布式系统所有的变更信息，为工程师提供变更信息展示和查询服务。它的实现相对比较简单，我们只需要在发布系统、配置中心和运营中心等，可能导致系统变更的运营和运维系统中，将每一次变更的信息丢入消息队列，变更管理系统就会消费消息队列的信息，然后做好展示和查询。具体的架构设计图如下。 变更管理系统有两个值得我们关注的地方。首先，变更信息最少要包括 4 个“什么”的内容：什么人在什么时间和什么地点，做了什么事情。如果还需要其他的信息，可以自行增加。其次，变更信息最少要包含，时间维度的视图和支持按服务或系统维度的查询。因为一般来说，故障能提供给我们最关键的信息就是这两个：发生故障的时间和位置，所以我们需要通过这两个信息来定位相关的变更信息。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:20:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"稳定版本的概念 有了变更管理系统后，我们就能基于变更信息快速处理故障了吗？其实，还有一个问题会影响我们对故障的判断和处理效率。例如，你负责的某一个服务，在今天白天的低峰期有 10 个变更，系统一直正常，但是到了晚上的高峰期突然出现了故障，这个时候，你应该如何定位，是哪一个或哪几个变更导致的故障呢？如果要通过回滚来恢复故障，那么你应该回滚到哪一个变更呢？你可以按时间倒序一个一个变更回滚，然后观察系统是否正常，但是这样非常低效。首先，每一次回滚都需要重新发布，其次，有一些系统故障就算已经回滚到正常版本了，它的恢复也是需要一些时间的，并且我们不能确定这个时间的长短，一般无状态的服务恢复时间会很快，有状态的服务则慢得多。所以，在每一次回滚后，你都需要等待一段时间，来确定是否恢复到正常版本了，有时，甚至需要回滚很多个版本，才能让系统恢复到稳定版本。那么你一定在想，有没有更高效的故障处理方式呢？ 更高效的处理方式是有的，在我的实践经验中，一个比较好的方法就是引入稳定版本的概念，出现故障的时候，如果定位到了引起故障的服务，首先回滚到上一个版本，因为最后一次变更导致故障出现的概率是非常大的，如果系统还没有恢复，就可以直接回滚到这个服务的稳定版本了。对于稳定版本的定义，我们可以先基于公司的业务流量情况，定义出公司业务的高峰时段，然后将经历整个完整高峰时段的变更，标记为稳定版本，这个功能可以设计为变更管理系统的一部分。例如，公司的业务高峰期是 19 点 - 22 点，那么所有在 19 点前发布，并且持续到 22 点，依然在提供服务的变更就是稳定版本，变更管理系统通过分析每一个变更的上线时间和下线时间，自动标记变更是否为稳定版本。 这里要注意一个关键点，一定要关注新的变更是否持续了整个高峰期，否则很有可能会出现在高峰期的时候，故障被回滚的变更版本，依然标记为稳定版本的情况。例如，在一个业务高峰期为 19 点 - 22 点的系统中，如果有一个变更是 16 点发布的，到了 20 点出现了故障，因为这个变更版本没有持续运行到 22 点，没有经历一个完整的高峰期，那么它就不能被标记为稳定版本。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:20:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"故障恢复流程 有了变更管理系统和稳定版本这两个工具，再结合可观测性的监控系统，整个故障恢复的流程就变得简单和高效了，如下图所示。 通过可观测性的信息快速确定导致故障的服务。回滚到上一个版本，观察故障是否恢复，如果恢复，结束流程，否则执行 3。回滚到最新的稳定版本，观察故障是否恢复，如果恢复，结束流程，否则执行 4。确认之前故障定位的服务是否准确，如果不准确，重新定位，然后执行 2；如果准确，则需要考虑该服务是否被其他的因素影响了，比如网络、机器等，这个需要具体问题具体分析。 这里还要特别强调一点，一般来说，我们的服务和系统等变更都是要求可回滚的，即向前兼容。当然，我们也可以容忍回滚的时候，新功能失效，但是老功能和数据不能因为回滚出现问题，这样在发布出现故障的时候，我们才能够通过回滚快速恢复。其中，有一些变更设计成可回滚的成本非常高，那么对于这一类变更，如果选择不向前兼容的设计，那么上线前就要经过更严格的评估和测试，确保不会出现问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:20:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课，我们先讨论了通过预案调度来快速恢复主动故障的办法是不可行的，因为我们不能对同一个业务开发两套系统，它的研发和协调成本实在太高了。然后，我们通过分析主动故障的来源，将主动故障分为四个原因：程序发布变更、实例数目变更、配置发布变更和运营策略变更，这样主动故障的问题就收敛了。最后，我们一起探讨了如何设计一个变更管理系统，如何来定义一个变更的版本为稳定版本，并且分析了基于变更管理和稳定版本，如何快速恢复主动故障的流程。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:20:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"期中测试 问题回顾首先，我们来回顾一下 IM 系统的业务和架构方面的需求。业务上的需求：支持单聊。100 个人以内的群聊。峰值同时在线 1000 w。峰值发送消息 10 w QPS。架构上的需求：开发模式简单，新功能支持快速迭代。长连接支持就近接入和负载均衡。分层设计。在功能迭代上线的时候，不要影响到用户已经建立好的长连接。发送消息的接口是幂等的。 问题解析 基于这些业务和架构上的需求，我完成了一个架构设计，具体见下图。接下来，我们就基于这个设计图来回答期中测试的问题。这里要特别说明一点，如果你的架构设计和我的不一样，也不一定就是错了。我们在做架构设计的时候，都是在不断地做 trade-off，很多方案没有绝对的对与错，只有深入理解业务，才能做出更适合业务场景的架构设计。 IM 系统一般都会涉及基于 TCP 的长连接通道和基于 HTTP(S) 的短连接通道，你认为长连接通道和短连接通道的职责分别是什么？长连接在客户端和服务器端都需要维护状态，并且消息是异步收发的，我们对长连接的设计应该尽量简单，而短连接可以理解为无状态的，并且请求是同步处理的，方便去完成一些复杂的功能，所以我认为一个比较好的职责划分方式是： 长连接作为信令通道，用于服务端主动给客户端发送信令通知，例如有新消息之类的主动通知，信令通知的数据结构做通用设计，在扩展的时候，新增信令类型的枚举即可。短连接作为业务通道，用于实现业务功能，客户端通过短连接请求服务器的 API ，来完成业务功能，例如长连接通道发送有新消息的信令后，客户端通过短连接请求获得消息之类的接口，得到新消息的内容和顺序。 长连接的就近接入和负载均衡应该怎么来做？（可以考虑通过设计一个路由服务来解决。）设计一个路由服务，客户端在建立长连接之前，先请求路由服务，路由服务通过客户端的 IP 或者 GPS 等位置信息，在充分考虑就近接入和负载均衡的基础上，给客户端返回最合适的接入点。 整个 IM 系统应该怎么分层？每一层的职责是什么？（可以考虑从长连接接入、Push 和 IM 等方面来进行分层。）这个 IM 系统可以分为 3 层：接入层、Push 层 和 IM 层，它们具体的职责为： 接入层：外部路由服务：负责长连接服务的发现、负载均衡和连通性保障。长连接服务：负责长连接高效高质量的鉴权、接入和数据发送，它与业务无关，长连接 ID 为全局唯一 ID 即可，不要包含任何业务信息。网关服务：负责接入短连接请求，以及鉴权相关网关职责功能。 Push 层：Push 服务：负责服务器对客户端的信令推送，由于信令一般都是通过用户 ID 来发送的，所以在 Push 层需要做一个绑定操作，将用户 ID 和长连接 ID 进行绑定，在发送推送的时候，通过用户 ID 找到长连接 ID，然后再发送推送的信令数据。内部路由服务：存储长连接与长连接服务的对应关系，提供通过长连接 ID 查询长连接服务实例的接口。 IM 层：负责 IM 层的业务逻辑，主要的业务功能都通过短连接的 API 对外提供，如果服务器需要主动通知客户端，则通过 Push 层来发送信令。 在系统设计中，如何让功能在迭代上线的时候，不要影响到用户已经建立好的长连接呢？在上面的分层中，我们接入层长连接服务的设计与业务无关，并且信令的数据结构易扩展，这样可以保证业务功能迭代上线时，只需要发布 IM 层的服务，而长连接服务几乎不需要迭代升级，这也就保证了在功能上线时，不会影响到用户已经建立好的长连接。 对于业务需求，IM 系统的消息扩散模式，采用读扩散还是写扩散？为什么？因为业务需求为单聊和 100 人以内的群聊，所以我们可以采用写扩散的模式，为每一个用户建立一个“收件箱”，该用户在每一次收到消息后，我们都向用户的收件箱写入一条数据，这样用户在获取新消息的时候，只需要拉取收件箱的数据即可。而对于微博这样的关注模式，一个明星用户可能有 1000 w 的粉丝，如果采用写扩散，那么一个明星用户发布一条微博，就会导致 1000 w 次写“收件箱”，所以这种情况下，我们一般采用读扩散，用户拉取微博消息列表的时候，即用户读微博信息的时候，根据关注用户发布的微博列表来生成微博消息列表。其实，很多的场景为了满足业务要求，会通过写扩散和读扩散的混合模式来进行消息的扩散，如果一条消息的接收者非常多，则采用读扩散，否则采用写扩散。 如何保障消息的发送接口是幂等的？客户端在发送消息时，生成唯一 ID，唯一 ID 的生成逻辑可以按以下的方式生成： 唯一 ID = Hash（UID + DID + 时间戳 + 自增计数） 其中，UID 为用户 ID，DID 为设备 ID，自增计数为同一个时间戳下发送的消息数。然后我们可以依据第 8 讲重试幂等中的“至少一次消息传递加消息幂等性”的方式来处理。 如果要对 IM 系统进行限流，你认为应该在哪几个地方来实施？为什么？我认为可以在下面三个地方进行限流： 接入层，保证接入点不会出现过载的情况，所以我们可以对路由服务进行限流。超过阈值，则返回客户端当前不能建立新的连接，让客户端等待一段时间后再重试，这个时间依据当时的情况而定。Push 层，整个 Push 层的关键是信令通道的正常，所以对推送信令的 QPS 进行限流，并且由于很多对 IM 层接口的访问，都是收到信令后的动作，比如收到有新消息的信令，就会执行获得消息相关的接口，那么减少信令数也可以减少对 IM 层接口的访问。网关服务或者 IM 服务，整个 IM 层的关键是消息发送和获取消息相关的接口，所以我们可以对消息发送和获取消息相关的接口进行限流，确保这些接口和服务的正常性。 如何提高长连接和短连接通道的连接成功率？关于这个问题，你可以具体查看第 15 讲“被动故障的预案梳理”，其中关于 DNS 解析问题和网络连通性问题的预案，就能解答你的疑惑。 整个 IM 系统要满足业务需求的指标，大约需要多少机器资源？是怎么计算的？这部分我们一起按照业务的需求，来估算所需要的机器数量，注意，这里只是估算，不是绝对准确的数据。 接入层：如果我们决定通过一个接入实例承载 100 w 长连接，通过 10 个接入实例承担 1000 w 的长连接，那么一个实例运行的配置为：内存：一个 TCP 连接在接入服务运行的系统上，最大消耗的内存为 8 k 左右，那么为了支撑 100 w 长连接需要 8 G 内存，还需要为业务层内存保留 8 G 内存，另外，期望支持 100 w 长连接的时候，系统的内存使用率为 50%，所以，最终内存为 32 G。CPU：因为接入层为 IO 密集型服务，所以 CPU 和内存的配比为 1：1，CPU 为 32 核。 Push 层：峰值发送消息 10 w QPS，考虑到群消息会放大消息数量，我们预估放大 1 倍为 20 w QPS。假设经过我们的测试， 32 核 32 G 的机器可以支撑 5 w QPS 的信令推送，并且预留一定的空间，那么最终需要 5 台 32 核 32 G 的机器。 IM 层：峰值发送消息为 10 w QPS，对应消息发送 API 的 QPS 为 10 w，如上讨论，信令推送的 QPS 为 20 w QPS，对应消息拉取 API 的 QPS 为 20 w QPS，IM 层其它的 API 接口预估 10 w QPS，可以得出 IM 层 API 接口总计 40 w QPS。假设经过我们的测试， 32 核 32 G 的机器，可以支撑 5 w QPS 的 IM 层 API 接口的调用，并且预留一定的空间，最终需要 10 台 32 核 32 G 的机器。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:21:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"分布式存储篇 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:22:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"17｜分片（一）：如何选择最适合的水平分片方式？ 结束了“分布式计算篇”的系列学习，我们掌握了如何解决分布式系统中，无状态节点或服务之间内部的协调问题，利用这些知识和技术原理，你就可以轻松地构建、运维一个大规模无状态的分布式系统了，恭喜你，取得了一个值得庆祝的学习成果。接下来我们乘胜追击，继续了解有状态分布式系统的相关知识和技术原理。在“分布式存储篇”，我们先解决单机存储和性能瓶颈的“分片”，再解决数据高可用的“复制”，然后讨论如何在已经“分片”和“复制”的数据集上实现 ACID 事务，最后从实践回归到理论，讨论分布式系统最核心、最重要的两个问题：一致性和共识，进一步提高你对分布式技术的理解和认识。 从这节课开始，我们将用两节课的时间来讨论，如何通过“分片”技术，突破单机存储和性能瓶颈，让分布式系统的计算和存储能力可以线性扩展。本节课，我们先梳理常用的分片策略，然后讨论水平分片的算法，并对其优缺点进行比较，最后从理论到实践，分析这些分片策略在实际工作中的应用场景。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:23:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"分片策略讨论与梳理 在 2000 年左右，由于互联网的快速发展，用户数据爆炸性增长，如何存储和管理这些海量的用户数据成为了一道难题，当时摆在工程师面前主要有两条道路。第一条是垂直扩容，即 PC 机扛不住换小型机，小型机扛不住换大型机，大型机扛不住换超级计算机，通过不断提高机器的配置来应对数据的增长。但是，这条道路会受到材料的物理极限、制造的工艺水平和使用成本的限制，不是一条可持续的道路。另一条是水平扩容，即通过将数据进行分片，分散到不同的 PC 机上，每一个 PC 节点负责一部分数据的存储和计算，来应对数据和成本的增长。这一条道路是由 Google 在 2000 年代的三篇论文 GFS 、MapReduce 和 BigTable 开启的，并且成为了解决数据激增问题的事实标准。那么对数据进行分片的策略，主要有三种：水平分片、垂直分片和混合分片，具体如下图所示。 我们从图中可以看到，水平分片和垂直分片是通过数据切分的操作方向来区分的，而混合分片是它们的组合体。为了帮助你更好地理解，本节课我们将详细讨论水平分片的知识、原理和应用，下一节课我们再讨论垂直分片和混合分片的内容。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:23:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"水平分片策略介绍 结合水平分片的原理，你是不是也联想到了负载均衡，其实我们在第 5 讲负载均衡中也讨论过这个问题。对于有状态服务，水平分片和负载均衡是解决单机存储与性能瓶颈问题中，相辅相成的两件事情，从流量角度来看，是负载均衡，从数据存储角度来看，是水平分片。 水平分片算法有两个最关键的因素，一是，如何对数据进行划分，即数据划分，二是，分片是否支持动态分裂与合并，即数据平衡。所以接下来，我们将从这两个维度来讨论水平分片策略。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:23:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"数据划分 数据划分要解决的问题是，将整个数据空间划分为多个分片空间，它主要有两种方式，基于模运算划分和基于范围划分。基于模运算的划分，在“负载均衡”篇中 Hash 负载均衡策略的部分充分介绍过了，这里不再重复。下面我们重点介绍基于范围的划分，如下图所示，它分为基于关键词划分和基于关键词的 Hash 值划分两种方式。 这两种划分方式都是给每一个分片，分配一个确定的数据范围，在这个数据范围内的所有数据，都属于这个分片。基于关键词划分和基于关键词的 Hash 值划分，二者唯一的区别在于，前者是直接利用关键词进行划分，而后者是利用关键词的 Hash 值进行划分。虽然只有这一个区别，但是却会深刻地影响数据的分布规律，所以我们接下来将重点讨论。 基于关键词划分的好处是，分片后数据的分布依然保留了关键词的顺序，我们可以方便地进行区间查询。假如我们在设计一个中国的公民数据库，将地址信息作为分片的关键词进行划分。如果我们需要查询“北京市海淀区”的所有公民，将查询区间设置为 [北京市海淀区 , 北京市海淀区] 即可，因为所有“北京市海淀区”的公民信息是连续存储在一起的。 但是基于关键词划分也会带来问题，即数据分布不均匀和访问的热度不均匀。比如在上文公民数据库的例子中，如果我们按省级行政单位进行划分，每一个省一个分片的时候，你会发现存储西藏数据的分片只有 300 多万条数据，而广东分片则有 1 亿 2000 多万条数据，这就会导致数据分布不均匀。 而数据分布不均匀也会导致访问的热度不均匀，比如，在对数据的访问频率相差不大的情况下，访问广东分片的热度要远远高于西藏分片的热度。并且，如果基于自增 ID 或者时间等关键词对数据进行分片的时候，即使数据是均匀分布的，对于一般的业务场景来说，往往新产生数据的访问热度，也是远远大于历史数据的，这也会导致访问的热度不均匀。为了解决基于关键词划分带来的问题，我们可以对它的分布规律再进行一些调整。比如，可以对广东分片的数据进一步分片，分为“广东广州”、“广东中山”等多个分片，西藏分片可以与周边的分片合并为一个分片。而对于基于自增 ID 或者时间戳等原因，导致的访问冷热不均匀的关键词，则避免作为数据划分的关键词。到这里，你会发现基于关键词划分，很明显会使数据分布和关键词自身的分布保持一致。在我们不了解数据分布的情况下，选择哪一个字段作为关键词是一个难题，有没有一种好方法来解决呢？其实基于关键词的 Hash 值划分就可以解决这些问题，它通过对关键词进行 Hash 运算，然后基于计算后的 Hash 值范围对数据进行划分，一个好的 Hash 算法可以处理数据倾斜并让它均匀分布。这里我们可以理解为通过 Hash 运算，去除了关键词数据分布的业务属性，从而解决了数据分布和访问的热度不均匀的问题。 但是这里依旧没有银弹，基于关键词的 Hash 值划分，带来了数据分布和访问热度更均匀的优点，但同时，它也失去了基于关键词的顺序性，不能方便地通过关键词进行区间查询了。并且，在极端情况下，如果一个关键词的访问热度非常大，那么基于关键词的 Hash 值划分也完全不起作用了。 这里要特别说明一点，我们可以将一致性 Hash 算法理解为基于关键词的 Hash 值划分的一种实现。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:23:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"数据平衡 根据数据分片是否支持动态的分裂与合并，我们可以将水平分片的数据平衡方式分为静态分片和动态分片。其中静态分片是指在系统设计之初，数据分片的数目和区间就预估好了，数据划分后不能再变化，而动态分片则可以在运行时，根据分片的负载和容量做调整。 对于静态分片，由于分片区间在运行时不能再调整，所以数据划分时一定要谨慎考虑。如果我们对数据的分布有足够的了解，并且数据的分布是比较稳定的，就可以采用基于关键词的方式，通过选择合适的关键词对数据进行划分。例如上文中提到的中国公民数据库的例子中，对于中国各省市的人口分布，因为我们有统计数据支撑，并且人口分布的数据非常稳定，所以就可以基于地址信息，并且结合数据的分布进行划分了。 在我们对数据的分布不了解，或者数据的分布不稳定的情况下，如果要采用静态分片的话，比较稳妥的方式是，采用基于关键词的 Hash 值的方式对数据进行划分，通过 Hash 算法解决数据分布和访问的热度不均匀的问题。 而对于动态分片，因为在运行时，分片区间是可以进行分裂和合并的，所以我们不用担心不了解数据分布，而导致分片区间划分不合理的情况，也不用担心在分片区间划分后，数据的分布发生变化，使分片区间不合适的问题。总而言之，动态分片与基于关键词的划分，往往是一个比较好的组合方式，它避免了基于关键词划分的问题，还保留了数据基于关键词有序的优点。 但是，在基于关键词的划分中，基于自增 ID 或者时间戳等原因，导致的访问冷热不均匀的问题，即使是在动态分片中也不能很好地解决，因为数据的热点往往集中在最新的一个分片区间上。而基于关键词的 Hash 值划分的方式，则可以很方便地将最新的热点数据分布到多个分片上，很好地解决这个问题。另外，动态分片存在冷启动的问题。当一个基于动态分片的存储系统启动时，通常是从一个分片开始，当数据量不断增长后，再动态进行分裂。在第一次进行分裂前，所有的读写请求都由第一个分片来进行处理，而其他的节点则都属于空闲状态。关于这个问题，一个比较好的解决方式是，动态分片在冷启动时，预分裂为多个分片来缓解。 这里还要特别强调一点，像 Codis、Redis Cluster 这样，预先分配固定数据量 slot ，slot 不能合并和分裂，但是可以通过将 slot 迁移到新增的节点上，进行水平扩容。比如预先分配 1024 个 slot，在 3 副本的情况下，刚开始运行的时候，可能是 3 个节点，每个节点上分布全部的 1024 个 slot 。在数据量增大的情况下，可以增加新的节点，将一部分 slot 迁移到新的节点上，实现水平扩展。 在课程中，由于预先分配 slot 后，就不能再进行合并和分裂了，所以我们将预先分配固定数据量 slot 归类为静态平衡方案。它能提供有限的水平扩容能力，最大程度是一个节点运行一个 slot ，但是当一个 slot 出现非常极端的数据热度和访问热度时，不能再进行分裂和水平扩容。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:23:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"水平分片策略分析 了解了水平分片的两个维度，数据划分策略和数据平衡策略后，我们将常见的数据划分策略和动态、静态的数据平衡策略交叉组合，一一来讨论它们的优缺点和应用场景，具体如下表。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:23:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 在这节课中，我们先讨论了数据分片的原因，了解了数据分片策略有三个类型，水平分片、垂直分片和混合分片，这样你就对整个数据分片有了一个全局的了解。接着，我们介绍了水平分片策略的两个关键维度：数据划分和数据平衡，通过对这两个维度的讨论和分析，你可以基于业务特点，清晰地选择适合你的水平分片策略。最后，我们对水平分片策略的所有算法和应用场景进行了全面的总结和对比，你可以进一步地理解水平分片策略了。同时，它也是一个非常有价值的结论，一个非常方便的知识库，在有需要的时候，你可以直接查看。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:23:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"18｜分片（二）：垂直分片和混合分片的 trade-off 通过了解水平分片策略中，关于数据划分和数据平衡的原理和知识，我们可以基于极客时间的业务场景，选择合适的数据划分和数据平衡的方式，组合出最佳的水平分片策略。而在一些数据分析的场景中，一行数据往往有非常多的字段，我们在计算时，却只需要一列或者几列的数据。这时基于水平分片策略，虽然能解决数据容量的问题，但是却没有充分利用数据分析场景的业务特点进行优化。那么是否有针对这个场景设计的数据分片方式呢？ 答案是肯定的，数据的垂直分片与混合分片，比起水平分片来说，能更好地满足数据分析场景。所以在本节课中，我们将一起来讨论数据分片的另外两种方式：垂直分片与混合分片，思考一下垂直分片与混合分片，是如何利用数据分析场景的业务特点，来做数据存储优化的。我们会先讨论垂直分片策略的应用场景和技术原理，接着分析混合分片策略是如何结合垂直分片与混合分片，在读写和水平扩展之间达到最优平衡的，最后再对讨论垂直分片时，引入的两种存储方式：行式存储和列式存储，进行对比和总结。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:24:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"垂直分片策略 垂直分片策略和水平分片策略都是对数据进行分片，但是它们的思路却截然不同。水平分片策略将整个数据集的条数作为划分的对象，每一个分片负责处理一定的数据条数。而垂直分片策略则是将数据 Schema 的字段集个数作为划分的对象，每一个分片负责处理一个或几个字段的全部数据，具体如下图所示。 从上文的描述中不难看出，对于数据的水平扩展能力，垂直分片策略是很有限的。因为数据 Schema 的字段个数是非常有限的，常见的字段个数从几个到几百个不等，即使一个字段一个分片，在字段数少的数据集上，水平扩展能力也是非常差的。关于这个问题，可以将垂直分片与水平分片策略组合起来解决，我们会在下一部分的“混合分片策略”中讨论。 这里你会发现一个很有意思的地方，如果垂直分片策略的处理方式为一个字段一个分片，那么垂直分片策略就等价于列式存储了，所以列式存储是垂直分片策略的一种特殊情况，也是最常见的情况。接下来，我们就以列式存储为例，从它应用最广泛的大数据分析场景，来讨论垂直分片策略的特点，当然这些特点在垂直分片策略中依然生效。 我们先来解释下大数据分析场景，它是指从用户的行为数据中获得新的洞见，来改进我们的产品和运营方式。大数据分析场景的数据处理一般有以下的特点： 宽表存储，按列读取：数据往往以宽表的形式存储，一个表上百列，但是一次分析往往只关心一列或者几列。读多写少：一次写入，多次读取，几乎不更新。数据量大：大数据会存储全站的所有数据，包括日志和数据库内的数据，并且会持续增加。查询无规律，不能索引覆盖：在分析场景中，我们会通过各种维度和组合，来统计和分析数据，所以这些查询方式是无规律的，不可能全部通过索引来覆盖。 由于大数据场景存储和计算的数据非常大，所以存储成本和计算性能是非常核心的设计指标，现在我们就来分析一下，列式存储是如何利用数据分析场景的特点，来达到低成本、高性能的。 第一，对于宽表存储，按列读取的场景，如果采用行式存储，当我们只需要读取一列数据的时候，可以按行顺序读取整个宽表所有列的数据，但是这会导致读取的数据量放大上百倍；或者我们可以跳着只读取所需列的数据，这样读取的数据量不会放大，但是读取数据的方式就从顺序读取变成随机读取了，这会增加非常多的寻址操作。并且，因为不能充分预读，在很大程度上，会降低磁盘的读性能。特别是对于机械磁盘来说，随机读取导致的寻址操作是毫秒级别的时延。 第二，读多写少的场景，会减少列式存储对写性能的影响。一般来说，数据写入存储系统是以行的形式写入的，而列式存储会导致一行数据的写入操作，按字段拆分为多个写入操作，使写入放大。不过，这个问题可以进行一定的优化，并且由于分析场景的数据写入模式是读多写少，所以不会影响整个系统性能的设计目标。 第三，因为数据量大，并且会持续增加的特点，要求存储系统能进行非常高效的压缩，降低存储数据的容量。那么我们先来分析下，列式存储是如何利用业务特性，进行数据压缩和提升性能的。 首先，在列式存储中相邻的数据类型是一致性的，并且通常会出现前缀一样，甚至完全相同的数据的特点，比如在用户的地址信息中，同一个地方的用户，省市县都是完全相同的，这非常适合使用 RLE 压缩、前缀压缩和字典压缩等压缩算法去压缩。 这里我们介绍一下字典压缩算法，其他的算法也是类似的思路，就不再一一介绍了。字典压缩算法的思路是，在数据重复度比较高的情况下，对数据采用字典重新编码，来减少数据的大小，具体见下图。 其次，虽然列式存储通过数据压缩大大提高了存储效率，节省了存储成本，但是与原始数据的存储相比，在写入和读取数据时，需要进行压缩和解压的操作，这需要消耗 CPU 来进行计算，所以，数据压缩其实是利用 CPU 资源来换取 IO 资源。 不过，在数据分析场景中，这是一个非常值得的选择，因为压缩算法在减少数据大小的同时，也减少了磁盘的寻道时间，提高了 I/O 性能，因为减少了数据的传输时间，并且提高了缓冲区的命中率，导致这些环节中得到的收益，能轻易地补偿压缩数据带来的额外 CPU 开销。 第四，如果熟悉数据库索引设计，你应该知道，数据库虽然有 Hash 索引或位图索引，但是最常见的索引模型是，将被索引的一个或多个关键词作为 Key ，按一定规则进行排序，Value 为行数据主键的指针，然后我们可以通过二分查询或 B+ Tree 进行查询，查到索引的关键词后，通过主键的指针找到行数据。 而对于大数据场景来说，经常需要读取一列或者几列中的大量数据、全表数据，那么列式存储通过按列顺序存储、按需读取和高效压缩，可以使按列读取的性能大大提高。其中，主键所在的列是有序的，其他列的读取性能也非常不错，可以理解为数据即索引，所以一般来说，列式存储系统对二级索引依赖不大，列式存储可以方便地应对查询无规律，不能预先建立索引的情况。 到这里，我们会发现，架构设计总是依赖业务场景的特点来做取舍，所以我们说，没有完美的架构，只有完美的 trade-off，列式存储其实是牺牲了按行写入的性能，去换取按列读取性能的 trade-off。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:24:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"混合分片策略 在上文中，我们分析出了数据 Schema 的字段个数是非常有限的，特别是在字段数少的数据集上，完全依赖垂直分片策略，解决数据的水平扩展是不现实的，所以我们可以将垂直分片策略和水平分片策略结合起来解决这个问题。根据这两种策略的组合顺序，可以将它们分为垂直水平分片策略和水平垂直分片策略。前者先进行垂直分片，再进行水平分片，而后者先进行水平分片，然后再进行垂直分片，具体方法如下图所示。 我们可以从图中看出，垂直水平分片策略就是垂直分片策略的水平扩容版本。对于水平分片策略，我们通常会选择主键进行水平分片，这样主键的列在整个存储系统中是有序的，垂直水平分片策略的数据分布特性和优缺点，与垂直分片策略完全相同，这里就不再重复讨论了。 而水平垂直分片策略更像是，水平分片策略和垂直分片策略的结合体，它对于整个数据集来说，一般是主键先利用基于关键词划分的水平分片策略，将数据集成不同的分片，然后对一个分片内的数据再进行垂直分片。 这样带来的好处是在一个水平分片内，依然按列式存储来存储数据，所以它有列式存储按列读取数据，高效和压缩比高的优点。在按行写入和读取多列的时候，都在一个数据分片上，大大地减少了网络 IO ，要知道在大规模的数据处理系统中，网络 IO 有可能是整个系统的瓶颈，同时，也能将一些分布式事务变成本地事务，提升系统的处理效率。总体来说，水平垂直分片策略不仅保留了列式存储的优点，而且将多列操作控制在一个数据分片上，减少了网络 IO 和分布式事务，是混合分片策略常见的方式，Google 的 Dremel 数据库就采用了这种分片策略。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:24:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"行列存储比较 在介绍水平分片、垂直分片和混合分片这三种分片策略的过程中，我们引入了行式存储和列式存储的讨论，并且我们发现这是存储引擎非常关键的一个选择，所以，最后我们来总结和分析一下，它们的优缺点以及应用场景，具体如下表。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:24:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课，我们讨论了垂直分片策略在按列读取时的 IO 优势、数据压缩方面的存储优势和数据自动索引的查询优势，当然这些优势都是付出了代价的。这样你就深入了解了垂直分片策略的特点，并且掌握了在架构设计时，如何根据业务场景进行取舍。接着，我们解决了垂直分片模式水平扩容差的问题，了解了混合分片策略的两种模式：垂直水平分片策略和水平垂直分片策略，其中垂直水平分片策略是垂直分片策略的水平扩容版本，而水平垂直分片策略，是水平分片策略和垂直分片策略的结合版本。然后，我们对行式存储和列式存储，进行了全面的对比和总结，这样你就进一步地理解了行式存储和列式存储的优缺点以及适用场景。同时，它也是一个非常有价值的结论，一个非常方便的知识库，在有需要的时候，你也可以直接查看。通过对各种策略不同优缺点的讨论和对比，希望你能明白，架构设计总是依赖业务场景的特点来做取舍，没有完美的架构，只有完美的 trade-off。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:24:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"19｜复制（一）：主从复制从副本的数据可以读吗？ 通过学习“分片”的内容，我们使用分片技术，让数据按一定的策略分布到多台机器上，解决了极客时间用户量快速增长，导致存储或处理的用户数据量超过单台机器限制的问题。但是，我们还不能高兴得太早，如果现在有一台提供数据服务的机器，由于宕机、网络不通等原因不可用了，那这一台机器上的所有数据分片就都不能被访问，这对于极客时间要求 7 * 24 小时提供服务的系统来说是不能接受的，而这就是我们工作中经常会涉及的高可用问题的场景。那么从这节课开始，我们将一起花三节课的时间来解决这个问题。这一节课，我们先讨论让存储服务高可用的思路，接着讨论具体的解决方法，即数据复制的三种方案，最后学习第一种主从复制的基本原理，另外两种数据复制的方案在后面的两节课程中我们再来讨论。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:25:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何让存储服务高可用 通过分析上文提到的场景，我们可以迅速锁定这是存储服务高可用的问题，解决高可用问题通常有两个思路： 第一种思路是避免故障出现。我们通过深入细节，一个一个去消除可能导致故障的原因，从而避免故障的出现，比如停电会导致宕机，那么我们就增加备用电源。但是这样，我们会遇到两个无法确定的问题： 我们可能无法穷尽所有的可能性，如果一个意料之外的问题出现了，故障就会发生。我们虽然知道某些故障的原因，但是无法控制，比如机房会因为海啸、台风等自然不可抗力原因而宕机，在一定成本范围内，我们没有好的办法来防范。 而第二种思路则恰恰相反，接受故障随时可能出现的事实，通过冗余的方法让系统在故障发生时，也能够正常提供服务。这也正是第 15 讲“被动故障恢复”中，通过多预案冗余来解决问题的思路。其实，只要我们能够接受提供的冗余机器和人力成本，那么冗余就是值得优先考虑的方案。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:25:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"数据复制的三种方案 通过上述的思考和讨论，本课开头提出的极客时间存储服务的高可用问题，就变成了下面两个子问题： 我们有没有办法让提供数据服务的机器永远可用？2. 我们将每一份数据都复制到多台机器上，让它们都能提供相关的数据服务，这个成本我们能接受吗？ 对于目前的计算机系统来说，第 1 个子问题的答案显然是否定的。 而第 2 个子问题的解决其实也非常困难，虽然我们能接受数据复制的成本，但是这个成本真的非常大。首先，数据复制导致的相关硬件成本是成倍增长的。其次，由于数据是持续变化的，导致复制操作不能一次完成，我们必须持续将这些变化复制到它所有的节点上，这就给分布式存储带来一个非常大的麻烦：在数据复制的过程中，由于节点可用性和网络中断等各种原因，可能会导致不同节点的数据不完全相同，这就是数据的一致性问题。 数据一致性问题是伴随整个分布式存储发展的技术与理论，它是分布式存储的核心问题，在后面“一致性与共识”的课程中会有深入的介绍。虽然数据复制的成本和复杂度非常大，但是为了让存储服务高可用，我们也别无选择。下面我们先介绍一下数据复制的一些基本概念。对于一个数据集来说，每个保持完整数据集的节点我们称为副本。如果一个副本接受外部客户端的写请求，并且这个副本在新数据写入本地存储后，通过复制日志和更改流将新数据发送给所有的副本，那么我们就将接受写请求的副本称为主副本，其他的副本称为从副本，从副本可以接受读请求。 于是，基于是否有主副本，有一个还是多个主副本，我们可以将数据复制的方案分为以下三种：主从复制：整个系统中只有一个主副本，其他的都为从副本。多主复制：系统中存在多个主副本，客户端将写请求发送给其中的一个主副本，该主副本负责将数据变更发送到其他所有的主副本。无主复制：系统中不存在主副本，每一个副本都能接受客户端的写请求，接受写请求的副本不会将数据变更同步到其他的副本。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:25:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"主从复制的工作流程 这节课我们先来了解主从复制，主从复制就像是一个主人带一堆的仆从，主人能接收外面的信息，仆从不能接触外面的信息，主人在接收到外面的信息后，按照一定的策略将外面的信息完整分享给仆从们。 主从复制是我们工作中最常见、最容易理解的复制方案，比如我们接触最频繁的缓存系统 Redis、关系数据库 MySQL 和 PostgreSQL、非关系数据库 MongoDB 和消息队列 Kafka 都内置支持主从复制，它的工作流程如下图。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:25:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"关键选择：同步复制 OR 异步复制 主从复制的工作流程，主要就是将变更数据从主副本复制到从副本，但是这里有一个非常关键的选择：主副本在接受外部客户端的写请求，将新数据写入本地存储后，是同步等待从副本也将新数据写入本地存储后，才回复客户端写入成功，即同步复制；还是立即向客户端回复写入成功，即异步复制呢？具体的过程如下图。 我们可以从图中看到，主副本在处理写请求的时候，会等待配置同步复制的从副本 1 确认成功后，才返回客户端。而对于配置异步复制的从副本 2，主副本不会等待它的确认，就直接向客户端返回写入成功了。这里我们来举一个例子，比如我在极客时间上更新了头像后，如果你立即去查看我的头像，那么可能会出现以下三种情况： 读主副本：你从主副本去读取我的头像，因为头像就是从这个副本写入的，所以你查询到的是我刚刚更新的头像。读同步复制的从副本：你从从副本 1 去读取我的头像，因为头像的写入请求会同步等待这个从副本复制完成，所以你查询到的是我刚刚更新的头像。读异步复制的从副本：你从从副本 2 去读取我的头像，因为头像的写入请求和这个从副本的复制操作是异步的，写入请求成功不能保证该副本数据复制完成，所以你有可能查询到我刚刚更新的头像，也有可能是上一次更新的头像。 了解完同步复制和异步复制的工作流程后，你可能觉得它们之间差别也不太大，只是在主副本处理客户端写请求时，是否等待从副本同步完成后再返回客户端。但是，这两种不同的复制方案却对系统可用性的设计有着非常大的影响。如果我们选择同步复制，那么在数据更新的时候，主副本都需要等待从副本写入成功。正常情况下这个时间非常短，在 1 秒钟内就可以完成，但是由于主、从副本分别运行在不同的机器上，可能出现网络延迟、中断和机器故障的情况。因此数据从主副本复制到从副本的时延就变得不可预测，可能数秒或者数十秒，甚至写入失败。 例如，一个用户在更新头像时，如果一个从副本突然宕机，那主副本就会迟迟收不到这个从副本同步完成的通知。由于是同步复制，系统就不能向用户返回更新成功的提示，待等待超时后，只能提示用户头像更新失败，这样会非常影响系统可用性的设计。 但是它也有优点，由于所有的数据都是同步从主副本复制到从副本的，所以主、从副本都有最新的数据版本，它们都能对外提供读服务，并且数据都是正确的，即系统的数据是强一致性的。 如果我们选择异步复制，在数据更新的时候，主副本写入成功就会返回成功，不会同步等待从副本是否写入成功，数据变更后通过异步的方式进行复制。由于数据的更新操作不依赖从副本，所以不受网络和从副本机器故障的影响，写入性能和系统可用性会大大提高。但是由于主、从副本间的数据变更不是同步复制的，所以从副本上的数据可能不是最新的版本，那么就会有两个问题。 首先，当主副本突然故障时，主副本上写入成功，但是还没有复制到从副本的变更就会丢失，这种情况在数据正确性要求高的场景里是不可以接受的。比如你在极客时间的 App 中充值了 100 元，充值请求就会将余额增加 100 元的变更写入主副本。在数据更新还未同步到所有从副本的时候，主副本突然宕机了，这个时候，我们会将其中的一个从副本切换为主副本，以便正常对外提供服务。但是由于主、从副本间的数据变更不是同步复制的，现在所有的副本都没有收到余额增加 100 元的数据变更，那么你就会发现刚刚的 100 元已经花出去了，而余额中并没有增加 100 元，这个时候你一定会找客服投诉的。 其次，我们可能会通过从副本读到老版本的数据，在正确性要求高的场景下，就不能通过从副本来提供读服务了。在异步复制的场景中，如果要通过从副本读取数据，要么我们能接受旧版本的数据，要么我们在读数据的时候给定一个版本号，要求读取小于或者等于这个版本号最新的数据。然后处理读请求的从副本，通过等待或者主动向主副本同步数据的方式，确保本地数据的版本超过读请求的版本号后，再按要求返回数据。从上面的讨论中，可以看到 CAP 理论的权衡，同步复制模式选择了 C ，而放弃了 A ，是 CP 模型；而异步复制模式选择了 A ，而放弃了 C ，是 AP 模型。为了让你更好地理解，我总结了同步复制和异步复制的优缺点，具体见下表： 通过这些讨论，你会发现同步复制和异步复制的优缺点都非常明显，所以我们很自然会想到混合的复制方式，比如有一个主副本，一个同步复制的从副本，其他都是异步复制的从副本。这样如果主副本故障，由于有一个同步复制的从副本，所以不会出现数据丢失的严重问题，并且这个从副本也能提供数据完全一致的读服务。另外其他从副本可能会读到旧版本数据，但是由于只有一个同步复制的从副本，对系统的写性能和可用性的影响也相对较少。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:25:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"主从复制的粒度更小一点 我们上面讨论的主从复制模型，是基于每一个副本都有全量的数据集的，如果我们将这个主从复制的粒度变小一点，比如可以指定每一个副本最大为 128 M，对于全量数据集按 128 M 拆分成多个副本，在每一个主从复制副本集内部做同步复制，这其实就是水平分片和主从复制的组合方式，也是当前分布式存储系统中非常流行的数据复制方案。比如，我们有 4 台存储机器，每台机器可以存储 3 * 128 M 数据，当前我们的数据集合总量为 4 * 128 M，那么，我们可以将这个数据集拆分为 4 个分片，每个 128 M，然后将这些分片和分片的副本分布到这 4 台机器上，具体的方法见下图。 这样将主从复制的粒度变小一点的方法，可以带来一些显著优点：首先，系统中的每一台机器都可以负责一部分主副本，提升了系统的写入性能和可用性。其次，可以让主从复制的副本数量不再和机器数量强绑定。在前面讨论的每一个副本都有全量数据集的方案中，每增加一台机器，都会导致副本集的数目增加 1 个，给系统带来了更多数据副本的性能开销。但是，当主从复制的副本数量不再和机器数量强绑定，比如指定副本数量为 3 个，那么我们需要同步的从副本数量就是 2 个，不论集群的机器的数量如何增加，副本的数量都不会改变，这样我们就可以通过增加机器，来提升整个系统整体的读写性能。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:25:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 在分布式系统中，为了实现数据的高可用性，我们只能通过数据复制将数据保存多个副本。那么基于是否有主副本，有一个还是多个主副本，我们可以将数据复制的方案分为以下三种：主从复制、多主复制、无主复制。接下来，我们重点讨论了主从复制的工作流程和主从复制的一个关键选择：同步复制 OR 异步复制，讨论了它们相关的优缺点，以及它们对系统设计的影响。并且，为了在系统的写性能和可用性之间取得更好地平衡，我们进一步讨论了同步、异步复制的混合使用方式。最后，我们通过将主从复制的粒度变小一点的方法，得到了当前分布式存储系统中非常流行的数据复制方案。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:25:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"20｜复制（二）：多主复制的多主副本同时修改了怎么办？ 通过上节课的学习，我们掌握了主从复制中，同步复制和异步复制的原理与知识，这样我们就可以根据业务场景，为极客时间后端的缓存系统 Redis 、关系数据库 MySQL 和 PostgreSQL 选择合适的数据复制方式，确保存储系统的高可用了。但是，随着极客时间业务的快速发展，我们对产品的可用性和用户体验会提出更高的要求，那么在异地建立多个数据中心就是一个不错的思路，它可以让系统容忍地区性的灾害，并且用户也可以就近接入数据中心来优化网络时延。不过，如果我们在多个数据中心之间，依然通过主从复制来同步数据，那么所有的写请求都需要经过主副本所在的数据中心，容灾能力和网络时延的问题并没有彻底改善，这个问题如何解决呢？其实通过多主复制的方式进行数据复制，就可以避免主从复制，不能发挥多数据中心优势的问题了，所以本节课，我们将通过多主复制的技术原理解决这个问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:26:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"为什么需要多主复制 我们都知道，数据复制是指将同一份数据复制到多个机器上，来避免机器故障时数据丢失的问题，它主要是用于保障数据高可用的。可一旦我们有了多个数据副本，为了提供更好的容灾能力，数据的多个副本应该分布得足够远，分布在多个机房或者多个城市中。接下来，我们很自然就会想到，既然数据已经分布在多个机房或者城市中了，那么是否允许用户直接读写离自己最近的数据中心的数据呢？在主从复制的情况下，将多个从副本分别部署到不同的数据中心上，对于读请求来说，如果是对一致性要求不高，或者主从之间是同步复制的情况，用户可以就近读取离自己最近的数据中心副本的数据；但是对于写请求来说，由于必须通过主副本写入，就导致所有的写请求必须经过主副本所在的数据中心写入。而多主复制和无主复制，允许多个副本写入，就可以避免上面的问题了，那么在本节课中，我们主要讨论多主复制，下节课再介绍无主复制。 其实除了上面讨论的，在多数据中心提供就近读写的应用场景之外，多主复制还有在线文档和在线日历之类的客户端本地修改场景。在这个场景中，每一个可以本地修改的客户端，都可以视为一个主副本，它们与远端服务器进行异步复制变更信息，只不过这个异步复制在离线的场景下，可能是几分钟、几天甚至更长。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:26:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何实现多主复制 基于主从复制模式，我们来介绍一下多主复制模式。它是指在一个数据系统中，存在多个主从复制单元，每一个主从复制单元都可以处理读写请求，一个主从复制单元的主副本处理了写请求后，需要复制到其他的主从复制单元的主副本，具体的流程见下图。 在实现多主复制的时有几个值得注意的地方，首先，每一个主从复制单元内部是一个常规的主从复制模式，这里的主副本、从副本之间的复制可以是同步的，也可以是异步的，具体的讨论可以查看第 19 讲“主从复制”。其次，多个主从复制单元之间，每一个主副本都会将自己的修改复制到其他的主副本，主副本之间的复制可以是同步的，也可以是异步的。 如果主副本之间的复制是同步的，那么一个主副本的写入，需要等待复制到其他的主副本成功后，才能返回给用户，这样当写入出现冲突时，可以返回失败或由用户来解决冲突。但是，它却失去了多主复制最重要的一个优点，即多个主副本都可以独立处理写入，这就导致整个模式退化为主从复制的形式。所以一般来说，多主复制的主副本之间，大多采用异步模式，我们本课中讨论的多主复制也都是异步模式。 如果主副本之间的复制是异步的，那么一个主副本待自己写入成功后，就立即返回给用户，然后再异步地将修改复制给其他的主副本。这时也会出现一个问题，如果多个主副本同时成功修改一个数据，当主副本之间复制这个数据的修改时，会出现冲突，我们就不知道以哪一个主副本的写入结果为准了。所以接下来，我们就一起讨论对于异步模式的多主复制，如何解决多个主副本写入冲突的问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:26:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"冲突解决 写入冲突是由于多个主副本同时接受写入，并且主副本之间异步复制导致的，那么依据这个定义，我们可以推导出写入冲突的两种主要形式。首先是由于更新导致的冲突，多个主副本同时更新了一个数据，导致这个数据的版本是非线性的，出现了分叉，具体见下图。 其次，由于新增导致的冲突，多个主副本同时新增了一个含有唯一性约束的数据，导致数据的唯一性约束被破坏。例如，在酒店预订业务中，一个时段内一个房间只能预订给一个用户，如果多个用户在多个主副本上，同时发起预订操作，就可能出现同一个时段内，一个房间被多个用户预定成功的情况。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:26:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"避免冲突 基于冲突的定义，我们应该怎么解决呢？有一个很自然的思路是，既然冲突是多个主副本同时修改了一个数据，或者破坏了数据的唯一性约束导致的，那么我们就对数据进行分片，让不同的主数据负责不同的数据分片，具体分片策略可以查看“分片”系列课程。这个方式确实可以在一定程度上避免冲突，但是会出现两个问题。 首先，一个修改操作可能会修改多个分片数据，这样我们就没有办法通过分片来隔离修改了。比如，我们将修改用户余额的操作进行水平分片， ID 为 0-10 的用户在主副本 1 写入， ID 为 11-20 的用户在主副本 2 写入。当 ID 6 的用户给 ID 16 的用户转账时，如果在主副本 1 上执行，那么同一时间， ID 16 的用户在主副本 2 上也有修改时，就会出现写入冲突。 其次，由于就近接入和故障等原因，我们会将出现故障的主副本流量切换到其他的主副本，这时也会出现写入冲突的情况。我们继续按刚才的例子分析，ID 为 0-10 的用户在主副本 1 写入，ID 为 11-20 的用户在主副本 2 写入。 假设 ID 8 的用户在主副本 1 写入成功，但是数据的变更还没有同步到主副本 2 ，这时如果 ID 8 的用户到主副本 1 的网络出现问题，我们会立即将 ID 为 0-10 的用户的写入流量切换到主副本 2 ，那么在主副本 2 上，再对 ID 8 的数据进行修改就会导致冲突发生。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:26:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"写时解决冲突 对于异步模式的多主复制，写入冲突是不可避免的，那么我们可以考虑，在数据写入一个主副本后，在主副本间进行复制时，检测是否有冲突，如果有冲突，就立即解决，这种方式称为写时解决冲突。它有两种实现方式，预定义解决冲突和自定义解决冲突，下面我们来一一讨论。 预定义解决冲突，是指由存储系统预先定义好规则，在冲突发生时依据预先定义好的规则，自动来解决冲突，它的规则主要有以下几种。 一是，从操作维度来处理，最后写入获胜。也就是为每一个写操作分配一个时间戳，如果发生冲突，只保留时间戳最大的版本数据，其他的修改都丢弃，但是这个方法会导致修改丢失。二是，从副本维度来处理，最高优先级写入获胜。也就是为每一个副本都排好优先级，如果发生冲突，只保留优先级最高的副本修改数据，其他的修改都丢弃。例如，为每一个副本分配一个 ID ， ID 越大的副本，修改的优先级就越高，在发生冲突时，只保留 ID 最大的副本数据。同样，这个方法也会导致修改丢失。三是，从数据结构和算法的维度来处理，通过研究一些可以自动解决冲突的数据结构来解决问题。比如 Google Doc 利用“操作转换”（Operational transformation）作为协作、编辑的冲突解决算法，但是目前这种方式还不太成熟，所以应用的范围比较少。 第二种实现方式是自定义解决冲突，它是由业务系统来定义冲突的解决方式，如果发生冲突了，存储系统就依据业务系统定义的方式执行。 自定义冲突解决的处理逻辑是，在主副本之间复制变更日志时，如果检测到冲突，就调用用户自定义的冲突处理程序来进行处理。由于主副本之间的数据复制是异步的，所以一般都是后台执行，不会提示用户。一般来说，正确解决冲突是需要理解业务的，因此由业务来定义解决冲突的逻辑是非常合理的，所以大多数支持多主复制的存储系统，都会以用户自定义的逻辑，来提供解决冲突的入口。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:26:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"读时解决冲突 读时解决冲突的思路和写时解决冲突的思路正好相反，即在写入数据时，如果检测到冲突，不用立即进行处理，只需要将所有冲突的写入版本都记录下来。当下一次读取数据时，会将所有的数据版本都返回给业务层，在业务层解决冲突，那么读时解决冲突的方式有下面两种。 第一种方式是由用户来解决冲突。毕竟用户才是最知道如何处理冲突的人，业务层将冲突提示给用户，让用户来解决。另一个方式是自定义解决冲突。业务层先依据业务情况，自定义好解决冲突的处理程序，当检测到冲突时，直接调用处理程序来解决，你会发现它和写时解决冲突的第二种实现方式一样，只不过一个在写入时解决冲突，一个在读取时解决冲突。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:26:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"多主复制的关键问题 多主复制虽然有多个主副本独立写入的优点，但是在一致性方面，多主复制的存储系统却面临着三个关键问题。首先，正确解决冲突的难度非常大。从上文讨论的复杂情况中不难看出，解决冲突是一件非常难的事情，如果解决不当，就会出现修改丢失或错误的问题。 其次，异步模式的多主复制会存在数据一致性的问题。为了获得多个主副本可以独立写入的优点，多主副本之间，通常是通过异步的方式来复制数据的，这就会出现读取到陈旧版本数据的问题，影响整个系统的一致性。这里要特别说明一点，在多副本之间进行数据复制，如果你期望数据强一致性，那么目前最好的方案是 Paxos 和 Raft 之类的分布式一致性算法。最后是多个主副本之间的复制拓扑结构问题。一般来说，多主复制的主副本之间的复制拓扑结构主要有三种：环形拓扑、星形拓扑以及全部至全部拓扑，具体见下图： 我们从图中可以看出，采用环形拓扑和星形拓扑结构时，如果一个主副本出现故障，可能会导致其他的主副本，也不能正常复制变更，甚至整个复制拓扑都会出现中断的情况。这时我们必须修复好出问题的主副本节点，或者重新调整复制的拓扑结构，才能恢复到正常状态。一般来说，这个过程需要人工参与且不能自愈，这会进一步延迟系统的恢复时间，使系统的可用性降低，同时降低系统的一致性。在采用全部至全部拓扑结构时，虽然一个主副本的故障，不会影响其他主副本之间的数据复制，但是却会出现一个问题，那就是由于副本之间的网络时延各不相同，会使数据复制出现乱序，更新相互覆盖，变更丢失等错误情况，也会影响系统的一致性。 总而言之，虽然异步模式的多主复制有多个主副本可以独立写入的优点，但是也会在一定程度上降低系统的一致性，所以我们在使用时，需要评估业务特点，对一致性要求容忍度高的业务，可以使用多主复制，而对于一致性要求高的业务，则需要慎重考虑。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:26:7","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 这节课中，我们先讨论了多主复制的优点，即在多数据中心的场景下，每个数据中心的主副本可以单独写入，提高了系统的写入性能，并且用户可以实现就近读写，降低了系统的延迟。如果你的业务要实施多数据中心部署，也可以考虑是否采用多主复制的模式。接着，我们讨论了如何实现多主复制，这里要注意一个关键点，如果要发挥多主复制的优点，就需要采用异步模式的多主复制，但是异步模式的多主复制还会有写入冲突的情况。关于如何解决冲突，我们讨论了避免冲突、写时解决冲突和读时解决冲突的思路，当你在实施多主复制的时候，也可以通过这些方法，解决多主复制的写入冲突问题。最后，因为异步模式的多主复制会在一定程度上，降低系统的一致性，所以我们在使用时，需要评估业务特点，对于一致性要求高的业务，需要慎重考虑。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:26:8","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"21｜复制（三）：最早的数据复制方式竟然是无主复制？ 通过上节课，我们掌握了在部署多数据中心的时候，可以用多主复制的方式，让用户直接读写离自己最近的数据中心的数据，减少用户与数据中心之间的网络延迟，提升用户体验。当我们的业务面向全球的用户时，这个优点将会变得尤为重要，比如一个北京的用户，访问北京的数据中心，网络时延为毫秒级别，但是当他访问美国的数据中心时，网络时延就是百毫秒级别了，这是影响用户体验的关键点。所以，当极客时间启动全球业务的时候，多主复制是一个可以考虑的方案。但不论是主从复制还是多主复制，所有的写入操作都必须依赖主节点，如果主节点出现故障，则必须再选举出一个新的主节点后，才能继续提供写服务，否则就将大大影响系统的可用性。那么是否有办法可以让单节点故障时，系统的可用性完全不受到影响呢？我们可以这样思考一下，既然系统的可用性是由主节点的故障导致的，那么我们是否能去掉主节点和从节点的角色，也就是让系统中所有节点的角色都是对等的，这样是否可以解决问题呢？ 其实这就是无主复制的数据复制方式，它确实可以解决由主节点故障，导致的系统可用性问题。虽然无主复制是“复制”系列课程的最后一节，但其实它才是最早出现的数据复制方式。无主复制又称为去中心复制，只不过在关系数据库出现并且主导后，由于要确保各副本写入顺序的一致性，主从复制开始流行起来，无主复制被大家慢慢遗忘了。本节课中，我们将按无主复制的实现方案，面临的问题，以及如何解决这个思路来学习，最后对主从复制、多主复制和无主复制，这三种数据复制的方式进行比较和总结。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:27:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何实现无主复制 无主复制顾名思义，即集群中没有主节点和从节点之分，所有节点的角色都是对等的，每个节点负责存储和处理一定范围的数据，并且由于高可用的要求，每一份数据都需要在多个节点上存储，那么一种常见的处理方式就如下图所示。 从图中可以看到，每一份数据按顺序存储多个副本，每一个节点都会负责多个范围数据的存储，节点 B 存储 Key Range (H，B) 的数据，节点 C 存储 Key Range (I，C) 的数据，节点 E 存储 Key Range (A，E) 的数据。这里要特别注意，主从复制和无主复制有一个非常大的区别，主从复制先写主节点，然后由主节点将数据变更同步到所有的从副本，从副本数据的变更顺序由主节点的写入顺序决定；但是，无主复制是由客户端或代理程序，直接负责将数据写入多个存储节点，这些存储节点之间是不会直接进行数据同步的。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:27:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"数据读写 从刚才的讨论中可以看出，无主复制写入数据时，为了数据的高可用，会向多个节点写入多份数据，那么它是等所有的节点都写入成功，客户端才返回成功呢？还是有一个节点写入成功，客户端就返回成功呢？同样地，读取数据也存在这个问题，每一份数据都有多个副本，那么它是等所有的节点都读取成功，客户端才返回成功呢？还是有一个节点读取成功，客户端就返回成功呢？这里我们举个例子来讨论一下，假设现在有 3 个副本，如果数据成功写入 1 个副本，那么要确保读请求一定能读取到最新写入的数据，就需要成功读取 3 个副本的数据；如果数据成功写入 2 个副本，则需要成功读取 2 个副本的数据；如果数据成功写入 3 个副本，那么成功读取 1 个副本的数据即可。这样就可以得出一个结论，如果要确保读取到最新的数据，读取的副本和写入的副本之间的交集不能为空，只要存在交集，就必定有一个写入的最新副本被读取到，那么我们就可以按如下的方式来定义这个问题。 假设对于每一份数据，我们保存 n 个副本，客户端写入成功的副本数为 w ，读取成功的副本数为 r ，那么只需要满足仲裁条件 w + r \u003e n 成立，读副本和写副本之间的交集就一定不为空，即一定能读取到最新的写入。 我们将满足仲裁条件 w + r \u003e n 的 w 和 r 称之为法定票数写和读，这就是 Quorum 机制，你也一定能发现它其实就是抽屉原理的应用。那么对于 w、r 和 n 的值，通常是可以配置的，一个常见的配置选择为，设置 n 为奇数（通常为 3 或 5 ），w = r = (n + 1)/2 向上取整。这个配置的读写比较均衡，比如 n = 5，那么 w = r = 3，读和写都保证 3 个副本成功即可，能容忍 2 个节点故障。 在实际的读多写少的业务场景中，我们假设 n = 5 ，如果想要读性能最高，可以设置 w = n = 5 ，r = 1 ，在读取的时候，只需等待一个节点读取成功即可。但是在写入的时候，需要所有的副本都写入成功，因此它不能容忍节点故障，如果有一个节点不可用，将会导致写入失败。如果 w = 4 ，r = 2 ，那么读性能依然比较高，并且能容忍一个节点不可用，这就是读性能、写性能和可用性之间的权衡。反之也是同样的思路，对于写多读少的业务场景，我们假设 n = 5 ，如果想要写性能最高，那么可以设置 r = n = 5 ，w = 1 ，在写入的时候，只需等待一个节点读取成功即可。但是在读取的时候，需要所有的副本都读取成功，因此它不能容忍节点故障，如果有一个节点不可用，将会导致读取失败。如果 r = 4 ，w = 2 ，那么读性能依然比较高，并且能容忍一个节点不可用。现在我们可以看出，Quorum 机制通过参数的调整，能够非常方便地适应业务的特点，在读性能、写性能和可用性之间达到平衡。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:27:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"数据修复 我们知道一个复制模型，应该确保数据最终都能成功复制到所有的副本上，主从复制和多主复制是通过主节点接受数据写入，并且由主节点负责将数据副本，成功复制到所有的从副本来保证的。但是在上文“数据读写”的讨论中，我们了解了当 w \u003c n 时，并不能保证数据成功写入所有的副本中，那么无主复制的这个问题应该如何解决呢？一般来说，有如下的两种方式来实现数据的修复。首先，是读修复。当客户端并行读取多个副本时，如果检测到某一副本上的数据是过期的，那么在读取数据成功后，就异步将新值写入到所有过期的副本上，进行数据修复，具体如下图所示。 其次，是反熵过程。由存储系统启动后台进程，不断去查找副本之间数据的差异，将数据从新的副本上复制到旧的副本上。这里要注意，反熵过程在同步数据的时候，不能保证以数据写入的顺序复制到其他的副本，这和主从复制有着非常大的差异，同时由于数据同步是后台异步复制的，会有明显的同步滞后。总体来看，读修复对于读取频繁的数据，修复会非常及时，但它只有在数据被读取时才会发生，那么如果系统只支持读修复，不支持反熵过程的话，有一些很少访问的数据，在还没有发生读修复时，会因为副本节点的不可用而更新丢失，影响系统的持久性。所以，将读修复和反熵过程结合是一种更全面的策略。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:27:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"一个关键的选择 到这里，我们已经掌握了如何实现一个无主复制的数据系统，不过在这个系统中，还有一个非常关键的选择，如果系统的某些节点发生故障，导致读或写的时候，无法等到系统配置的 w 和 r 个客户端成功返回，我们应该如何处理呢？这里我们可以依据 2 个方案来思考。 当读写无法到达 Quorum 要求的 w 或 r 时，直接返回失败，并且明确地将错误返回给客户端。 在读写的时候，依然是等待 w 和 r 个客户端成功返回，只不过有一些节点不在事先指定的 n 个节点的集合内。比如本课第一幅图中的 Key K，它指定的存储副本集合应该是 B、C、D 和 E，假设 D 出现故障了，那么它的存储集合可以临时修改为 B、C、E 和 F 你会发现第一个方案，即当系统的故障已经导致仲裁条件不成立时，就返回失败，并且明确地将错误返回给客户端的选择，是一致性和可用性之间的权衡，是为了数据的一致性而放弃了系统的可用性。对于第二个方案，在数据读写时，当我们在规定的 n 个节点的集合内，无法达到 w 或 r 时，就按照一定的规则再读写一定的节点。这些法定集合之外的数据读写的节点，可以设置一些简单的规则，比如对于一致性 Hash 环来说，可以将读写顺延到下一个节点，作为临时节点进行读写。当故障恢复时，临时节点需要将这些接收到的数据，全部复制到原来的节点上，即进行数据的回传。通过这个方式，我们可以确保在数据读写时，系统只需要有任意 w 或 r 个节点可用，就能读写成功，这将大大提升系统的可用性。但是这也说明，即使系统的读写能满足仲裁条件 w + r \u003e n ，我们依然无法保证，一定能读取到最新的值，因为新值写入的节点并不包含在这 n 个节点之中。 那么这个方案叫 Sloppy Quorum ，相比于传统的 Quorum ，它为了系统的可用性而牺牲了数据的一致性。目前，几乎所有无主复制的存储系统都支持 Sloppy Quorum，但是它在 Cassandra 中是默认关闭的，而在 Riak 中则是默认启用的，所以我们在使用时，可以根据业务情况进行选择。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:27:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"三种数据复制模式总结 目前，我们已经学习了三种数据复制模式：主从复制、多主复制和无主复制，因为在我们进行存储系统设计时，数据复制是一个非常关键的选择，所以我们再来总结和分析一下，它们的优缺点以及应用场景，具体如下表。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:27:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 无主复制由于写入不依赖主节点，所以在主节点故障时，不会出现不可用的情况。但是，也是由于写入不依赖主节点，可能导致副本之间的写入顺序不相同，会影响数据的一致性。在实现无主复制时，有两个关键问题：数据读写和数据修复。数据读写是通过仲裁条件 w + r \u003e n 来保证的，如果满足 w + r \u003e n ，那么读副本和写副本之间就一定有交集，即一定能读取到最新的写入。而数据修复是通过读修复和反熵过程实现的，这两个方法在数据的持久性和一致性方面存在一定的问题，如果对数据有强一致性的要求，就要谨慎采用无主复制。然后，我们了解了 Sloppy Quorum ，它相比于传统的 Quorum ，为了系统的可用性而牺牲了数据的一致性，这里我们可以进一步得出，无主复制是一个可用性优先的复制模型。最后，我们对比了“复制”系列中，三种数据复制模型的优缺点和应用场景，你可以通过这些对比，更加深刻地理解数据复制，并且依据业务场景做出最佳的选择。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:27:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"22｜事务（一）：一致性，事务的集大成者 通过学习“数据复制”系列的内容，我们使用数据复制，将同一份数据按一定的策略复制到多台机器上，解决了存储服务由于宕机等故障，不能为用户提供服务和数据丢失的问题，恭喜你又攻克了一个难关。但是，由于极客时间用户量增多，每一天课程购买的订单数都在急剧增加，你开始接到用户这样的投诉，他在购买课程时出现了错误，课程没有购买成功，但是余额却被扣了。同时，财务的同事也开始向你反馈，他们在算账时，发现收入和支出的数目对不上。在遇到这样的问题时，你是不是一时抓不到头绪呢？其实这些都是我们工作中经常碰到的事务场景问题。那么从这节课开始，我们将一起花四节课的时间来解决分布式场景下的事务问题。这一节课，我们先通过分析业务场景来讨论事务是什么，以及它可以解决的问题，然后学习它的四个特性：一致性、原子性、隔离性和持久性，最后再一起来讨论如何实现事务的一致性 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:28:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"事务是什么 在本课开头，我们列举了工作中经常碰到的两个事务场景问题，那么我们先来了解一下事务是什么？事务可以看成是一个或者多个操作的组合操作，并且它对这个组合操作提供一个保证，如果这个组合操作之前的数据是一致的（即正确的），那么操作之后的数据也应该是一致的。不论这个组合操作执行的过程中，是否发生系统故障，还是在这个组合操作执行的过程中，是否与其他事务一起执行。 为了让你更好地理解事务的定义，我们结合开头提到的两个具体的事务场景问题来讨论一下。第一个问题，用户在极客时间购买课程时出现了错误，课程没有购买成功，但是余额却被扣了。这里我们先分析一下，用户在购买课程时，在我们的服务器程序里，需要实现三个操作： 余额检查：确认用户的余额是否大于课程的价格，如果余额足够，则可以购买，否则不可以购买。余额扣费：从用户的余额中扣除购买课程的金额。权益发放：给用户发放购买课程的学习权益。 正常情况下，上述三个操作完成后，用户的余额被扣除，也获得了学习权益。但是如果在这三个操作执行的过程中，出现了发放学习权益的服务崩溃或者课程下线等情况，那么这三个操作就无法全部执行成功。此时，用户的余额已经被扣除，但是在 App 上却收到了购买失败的提示，用户一定是不认可的。 我们仔细分析就会发现，这个问题的根本原因是余额扣费和权益发放不是一个整体操作，出现了部分执行成功的情况。这里可以结合事务来思考，如果将课程购买通过一个事务来执行的话，这个事务就会包括余额检查、余额扣费和权益发放 3 个操作，并且它对这个组合操作提供了一个保证，保证不论出现什么故障，这 3 个操作要么都执行成功，要么都不执行。所以当我们结合事务来思考，如下图所示，这个问题就迎刃而解了。 第二个问题，财务的同事反馈他们在算账时，发现收入和支出的数目对不上。这里我们还是拿用户购买课程来分析，假设用户余额为 100 元，课程价格为 60 元，此时因为余额大于课程价格，所以余额检查是通过的。如果在这时，用户同时还在买另一个价格为 80 元的课程，因为上笔订单没有付款，用户的余额依旧为 100 元，大于课程价格 80 元，所以这笔订单的余额检查也是通过的。接下来，当这两个课程都购买成功时，就相当于用户用 100 元的余额，分别购买了 60 元和 80 元的两个课程，所以最后用户的余额就变成了负 40 元，这个结果显然是不符合财务同事预期的。那么它的根本原因就是用户的两个购买操作并发执行。我们也结合事务来思考一下，如下图，如果这两个课程购买的操作通过事务来执行的话，事务会对这个组合操作提供一个保证，保证它和一个一个串行执行的操作一样，不会出现由于并发执行而导致数据不正确的问题。 通过分析具体的事务场景问题，我们会发现事务为日常的研发工作，提供了一个非常优雅的抽象，让我们可以将一组操作过程中的内部状态处理等细节交给事务来处理，而我们只需要去关心这一组操作是否成功就可以了，这大大简化了研发的工作负担。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:28:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"事务的四个特性 事务是一个非常实用的工具，它为我们的研发提供了非常友好的保证，但是，你心里一定会有一个问题，它是通过什么具体方法来实现的呢？事务主要是通过提供以下四个特性来实现的： 一致性（C）：一个事务能够正确地将数据从一个一致性的状态，变换到另一个一致性的状态。原子性（A）：一个事务所有的操作，要么全部执行，要么就一个都不执行，即 all-or nothing。它可以让事务在出现故障等原因，导致不能全部执行成功时，将已经执行的部分操作，回滚到事务前的状态。隔离性（I）：如果多个事务并发执行，那么执行结果和一个一个串行执行是一样的。它可以使事务在执行时，不会受到其他事务的影响。不过在实践中，由于考虑到性能的问题，一般都使用较弱一点的保证，我们在后续的课程中会专门讨论。持久性（D）：如果一个事务已经提交，不论什么原因，它产生的结果都是永久存在的，它保证了事务的结果不会丢失。 从上面的分析中，我们了解了事务是如何通过四个特性来达成它的目标的。在四个特性中，一致性是对事务执行最终结果正确性的保证，它需要依赖事务的其他特性来协助完成，我们可以将它看成是事务操作的一个概览。所以，本节课我们会先来讨论事务的一致性。并且这里要特别说明一下，事务的四个特性不是孤立的，它们之间是相互联系的。在学习事务一致性时，我们需要思考事务原子性、隔离性、持久性与一致性之间的联系。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:28:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"一致性是怎么实现的 上文提到了一致性的定义，即一个事务能够正确地将数据从一个一致性的状态，变换到另一个一致性的状态。也就是在事务执行的过程中，不能出现任何不一致的问题，如果一个事务执行前的数据是正确的，那么执行后的数据也必须是正确的，所以，事务的一致性其实就是正确性。 事务的一致性需要保证一个事务在执行时，不论出现停电、宕机等任何问题，最终的执行结果都是正确的，这是一个非常高的要求，接下来我们就来分析一下，在高要求下事务是如何实现一致性的？首先，我们从数据复制的角度来看，为了保障系统的高可用，每一份数据都复制了多个副本，事务执行后，这多个副本的数据需要完全一致，即数据的多副本必须通过强一致性的策略进行复制。这个问题我们在“数据复制”的课程中已经有过讨论，并且在“事务（四）”的课程中会继续讨论，这里就不再赘述了。然后，我们可以从事务的原子性、隔离性和持久性方面，来讨论事务的一致性是如何实现的。在事务的执行过程中，不能因为系统故障等原因，出现部分操作执行成功的情况，比如我们前面提到的课程购买例子中，余额扣费成功，但是权益却发放失败的情况，这个部分需要事务的原子性来保证。 同时，也不能出现因为事务并发导致执行后状态错误的情况，比如两个课程购买的事务并发执行的例子中，当时余额检查都成功，但是到了后面扣费时，由于用户余额不足出现了负数。为了让事务在执行时，不会受到其他事务的影响，事务的隔离性也需要注意。另外，在事务执行的过程中，也要考虑到因为数据丢失，导致执行后的结果错误的情况，这个部分需要事务的持久性来保证。虽然有了底层存储多副本数据强一致性的支持，以及事务三个特性的保驾护航，但我们还是要考虑事务执行的最终结果，是否满足数据库层以及业务层的约束规则，所以最后我们要做好约束检测。这里又分为如下两个层面来讨论。 第一个是数据库层面，数据库内部需要基于一些约束规则，来检测数据是否违反了一致性的约束，比如外键约束和唯一性约束等。另一个是应用层的业务逻辑，它需要结合业务场景做一些约束检测，这样做是为了保障数据的一致性，比如用户课程购买的场景，从用户账号扣掉的钱，应该和收款方的数目是相等的。如果应用层的处理逻辑出现 Bug，导致用户账号扣掉的钱比收款方的多，这样的一致性问题在数据库的事务层面是无法约束检测的，它需要应用层的业务逻辑来保证。 所以，通过上面的分析，我们可以了解到，事务一致性的实现需要多维度来保证，比如底层存储的多副本数据强一致性，事务原子性、隔离性和持久性的一起协作，以及数据库层和应用层的约束检测等各方面来保障，它不单单是事务层面的一致性问题。这也是事务的一致性和其他三个特性不一样的地方，事务的原子性、隔离性和持久性这三个特性可以通过各自的实现机制来保障，而一致性则是应用层通过运用事务的原子性、隔离性和持久性的特性，加上数据库层的约束检测，并且在应用层开发中做好相关的约束检测才能达成，所以，我们说一致性是事务的集大成者。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:28:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 这节课中，我们讨论了事务的概念，事务是一个或多个操作的组合操作，并且它对这个组合操作提供一个保证，如果这个组合操作之前的数据是一致的，那么操作之后的数据也应该是一致的。然后，我们通过分析极客时间 App 出现的课程购买问题，引出了事务的具体业务场景。如果我们期望多个操作同时成功或者失败，并且期望多组操作之间不能相互影响，就需要通过一个事务来执行。而且事务为我们日常的研发工作，提供了一个非常优雅的抽象，大大简化了研发的工作负担。在学习了事务的四个特性，一致性、原子性、隔离性和持久性后，我们了解到事务的四个特性之间是相互联系和影响的。最后，我们探讨了事务的一致性是如何实现的，它是通过底层存储的多副本数据强一致性，事务的原子性、隔离性和持久性一起协作，以及数据库层和应用层的约束检测等各方面来保障的，不单单是事务层面的一致性问题，这也正说明了事务的四个特性有着直接的联系与影响。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:28:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"23｜事务（二）：原子性，对应用层提供的完美抽象 通过上节课的学习，我们理解了事务的一致性的定义，并且知道了事务一致性的实现，是通过底层存储的多副本数据强一致性，事务的原子性、隔离性和持久性一起协作，以及数据库层和应用层的约束检测等各方面来保障的，那么本节课，我们就继续来讨论事务中，另一个非常重要的特性：原子性。我们从原子性的定义出发，一起分析在分布式系统中，原子性的实现方法，最后再对原子性的关键问题进行讨论。当我们对事务的原子性进行讨论和学习后，你就能明白原子性是一个非常完美的抽象，因为它对应用程序，屏蔽了分布式系统中部分失败的问题，这可以大大减少我们在编程时的心智负担。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:29:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"原子性的定义 一般来说，我们在计算机领域第一次接触“原子”这一概念，都来源于操作系统的“原子操作”。在操作系统中，原子操作的定义是指，不可被中断的一个或者一系列操作，它包含了两个层面的意思。首先，是整体的不可分割性。一个原子操作的所有操作，要么全部执行，要么就一个都不执行，即 all-or-nothing 。其次，是可串行化的隔离性，即线程安全。原子操作是在单核 CPU 时代定义的，由于原子操作是不可中断的，那么系统在执行原子操作的过程中，唯一的 CPU 就被占用了，这就确保了原子操作的临界区，不会出现竞争的情况。原子操作自带了线程安全的保证，即最严格的隔离级别的可串行化，所以我们在编程的时候，就不需要对原子操作加锁，来保护它的临界区了。 但是，我们上节课提到了事务中原子性的定义，一个事务所有的操作，要么全部执行，要么就一个都不执行，即 all-or-nothing 。它可以让事务在执行的过程中，当遇到故障等原因，不能全部执行成功的时候，将已经执行的操作，回滚到事务前的状态。你会发现事务中对原子性的定义，只保留了原子操作的不可分割性，并没有关注可串行化的隔离性。其实这也很好理解，主要是基于性能的考虑，如果事务的原子性同时定义了不可分割性和可串行化的隔离性，那么对数据库性能的影响将会非常大，因为数据库需要频繁地操作，相对于内存来说非常慢的磁盘，而可串行化地去操作磁盘，在很多业务场景下的性能是我们不可以接受的。 因此，在事务的定义中，就将原子操作的不可分割性和隔离性，分别定义出了两个特性，即原子性和隔离性。其中隔离性为了在性能和正确性之间权衡，定义了多种隔离级别，我们可以依据自己的业务情况进行选择，具体的隔离性讨论，我们将在下一节课进行。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:29:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"怎么实现原子性 通过上面的讨论，我们知道了事务的原子性只关注整体的不可分割性，一个事务所有的操作，要么全部执行，要么就一个都不执行。那么我们应该如何实现事务的原子性呢？从不可分割性的角度来思考，实现一个事务需要解决两个维度上的操作分割： 第一个维度是单节点事务，即单节点上操作的不可分割性。在单节点上，一个事务在执行的过程中出现崩溃等问题，它的一部分操作已经执行完成，而另一部分操作则无法继续执行，这时就会出现整个事务操作无法继续完成，仅仅部分操作完成的情况。第二个维度是分布式事务，即多节点之间的操作不可分割性。在多节点上，一个事务操作需要在多个节点上运行，如果某些节点检测到违反约束、冲突、网络故障或者崩溃等问题，事务将无法继续执行，而其他节点的事务却已经顺利完成了，这时就会出现部分节点操作完成的情况。 下面我们就从单节点事务和分布式事务的维度，来一一讨论事务原子性的实现。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:29:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"单节点事务 对于单节点上运行的事务（单节点事务）来说，在执行过程中，不需要与其他的节点交互，也就不会出现部分节点失败导致的操作分割，我们只需要考虑当前节点整体失败导致的操作分割即可。对于单节点事务，一般是在存储引擎上，通过 Undo Log 、 Redo Log 和 Commit 记录来实现，具体流程如下图。 我们从图中不难看出，对于单节点事务来说，一个非常关键的顺序就是在磁盘上持久化数据的顺序：先写入 Undo Log 和 Redo Log ，然后再写入 Commit 记录。其中事务的提交或中止由 Commit 记录来决定，如果在写入 Commit 记录之前发生崩溃，那么事务就需要中止，通过 Undo Log 回滚已执行的操作；如果事务已经写入了 Commit 记录，就表明事务已经安全提交，后面发生了崩溃的话，就要等待系统重启后，通过 Redo Log 恢复事务，然后再提交。接下来，我们通过举例来简单描述下这个过程，注意这里简化了 Undo Log 和 Redo Log 的格式。假设一个事务操作 A、B 两个数据，他们的初值分别为 1 和 2 ，事务的操作内容为将 A 修改为 3 ，B 修改为 4 ，那么事务的执行流程如下图。 通过这些讨论，我们可以看出，** Redo Log 保证了事务的持久性， Undo Log 保证了事务的原子性，而写入 Commit 记录了事务的提交点**，它来决定事务是否应该安全提交。通过提交点，我们就可以将事务中多个操作的提交，绑定在一个提交点上，实现事务的原子提交。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:29:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"分布式事务 对于多节点上运行的事务（分布式事务）来说，除了当前节点整体失败导致的操作分割之外，还存在部分节点失败导致的操作分割。我们知道当前节点整体失败导致的操作分割，可以按单节点事务来处理，而对于部分节点失败导致的操作分割，一个常见的思路是通过两阶段提交（ 2PC ）来解决，实现 2PC 的思路如下图所示。 选择一个协调者，这个协调者可以是分布式事务的参与节点，也可以是一个单独的进程。 阶段 1 协调者发送事务请求（Prepare）到所有的参与节点，并询问它们是否可以提交。如果所有的参与节点都回复“是”，那么接下来协调者在阶段 2 发出提交（Commit）请求。如果任何的参与节点都回复“否”，那么接下来协调者在阶段 2 发出放弃（Rollback）请求。 阶段 2 依据阶段 1 返回的结果，决定事务最终是提交（Commit）还是放弃（Rollback）。 关于 2PC ，在实现的时候，要特别注意 2 个关键点。 一是，两个关键承诺。第一个承诺在阶段 1 ，当事务的参与节点回复“是”的时候，对于当前事务，这个参与节点一定是能够安全提交的，它不仅要保障事务在提交时，不会出现冲突和约束违规之类的问题，还要保障即使出现系统崩溃、电源故障和磁盘空间不足等系统问题时，事务依然能够正常提交成功。 第二个承诺在阶段 2 ，当协调者基于参与者的投票，做出提交或者中止的决定后，这个决定是不可以撤销的。对于协调者来说，如果协调者通知参与者失败，那么协调者必须一直重试，直到所有的参与节点都通知成功为止；而对于参与者来说，不论协调者通知的结果是提交还是中止，参与者都必须严格执行，不能反悔。即使出现了故障，在故障恢复后，还是必须要执行，直到成功为止。 第二个关键点是 2PC 的提交点。当协调者通过参与者的投票，做出提交或者中止事务的决定后，需要先将决定写入事务日志，然后再通知事务的参与者。如果协调者在事务执行过程中崩溃了，那么等到协调者恢复后，在事务日志中如果没有发现未解决的事务，就中止事务；反之，就会继续执行事务。 所以，协调者将阶段 1 的决定写入事务日志就是 2PC 中事务的提交点，通过这个提交点，将多个节点的事务操作绑定在一个提交点上，然后像单节点事务一样，利用这个提交点来保证事务的原子性。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:29:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"2PC 面临的问题 通过上面的讨论，我们知道了 2PC 可以解决分布式事务的原子性问题，但是要正确使用 2PC，还需要了解以下几个方面的问题。 第一，2PC 是一个阻塞式协议。当 2PC 的一个参与者，在阶段 1 做出了“是”的回复后，参与者将不能单方面放弃，它必须等待协调者的决定，也就意味着参与者所有占用的资源都不能释放。如果协调者出现故障，不能将决定通知给参与者，那么这个参与者只能无限等待，直到协调者恢复后，成功收到协调者的决定为止。因为 2PC 有阻塞问题，所以后来又提出了 3PC 协议，它在 2PC 的两个阶段之间插入了一个阶段，增加了一个相互协商的过程，并且还引入了超时机制来防止阻塞。虽然 3PC 能解决 2PC 由于协调者崩溃而无限等待的问题，但是它却有着超高的延迟，并且在网络分区时，还可能会出现不一致的问题，这些原因导致它在实际应用中的效果并不好，所以目前普遍使用的依然是 2PC 。 第二，2PC 是一个逆可用性协议。如果在阶段 1 ，任何一个参与者发生故障，使准备请求失败或者超时，协调者都将中止操作；如果在阶段 2 ，协调者发生故障，也会导致参与者只能等待，无法完成操作。你是否感觉很奇怪，同样是共识算法，Raft 和 Paxos 等共识算法都能容忍少数节点失败的情况，那为什么 2PC 则完全不能容忍节点的失败呢？其实，这个差异的出现是因为 2PC 是一个原子提交协议，为了 all-or-nothing ，在操作过程中就需要与所有的节点达成共识；而 Raft 和 Paxos 则只需要与大部分节点达成一致，确保共识成立即可，它可以容忍少数节点不可用，当故障恢复的时候，之前不可用的节点可以向其他正常的节点同步之前达成的共识。 第三，虽然 2PC 能保证事务的原子性，即一个事务所有的操作，要么都成功，要么都失败，但是它并不能保证多个节点的事务操作会同时提交。如果没有同时提交，即一部分节点已经提交成功，而另一部分节点还没有提交的时候，就将使事务的可见性出现问题，这部分知识，我们将在课程“事务（三）”中继续讨论。总而言之，虽然 2PC 在性能、可用性和可见性方面都存在问题，但是目前分布式事务中，使用最广泛的还是 2PC 。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:29:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 在这节课中，我们先讨论了原子性的定义，了解了事务的原子性，以及操作系统的原子操作是两个不同的概念，事务的原子性只要求 all-or-nothing ，而操作系统的原子操作除了要求 all-or-nothing 之外，还需要可串行化的隔离级别。然后，我们从单节点事务和分布式事务的角度，讨论了如何实现事务的原子性。对于单节点事务来说，我们将事务的多个操作绑定到，事务提交信息写入的一个提交点上，如果提交信息写入成功，那么事务提交，否则事务回滚。而对于分布式事务来说，它在单节点事务的基础上，进一步地要求事务的多个参与者做出两个关键承诺，第一个承诺在阶段 1 ，当事务的参与节点回复“是”的时候，该参与者是一定可以提交的；第二个承诺在阶段 2 ，当协调者基于参与者的投票，做出提交或者中止的决定后，这个决定是不可以撤销的。最后，我们讨论了 2PC 在性能、可用性和可见性方面有着一些问题，但是 2PC 依然是当前分布式事务场景中，使用最多的原子提交协议。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:29:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"24｜事务（三）：隔离性，正确与性能之间权衡的艺术 通过上节课的学习，我们掌握了通过 2PC 实现分布式事务原子性的技术原理，并且也明白了 2PC 在可用性等方面存在的问题，这些知识能够帮助我们在极客时间的架构选型中，做出正确的选择。同时，我们还讨论了事务原子性的定义，区分出了事务的原子性并不等价于操作系统里面的原子操作，事务的原子性只定义了操作的不可分割性，而不关心多个事务是否由于并发相互竞争而出现错误，那么在本节课中，我们就一起来讨论事务并发执行的问题，即事务的隔离性。我们先一起来讨论隔离性的级别和各个隔离级别可能出现的异常情况，然后分析在业务代码中，如何避免异常情况的出现，最后通过讨论隔离性的实现方式，让你进一步理解隔离级别。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:30:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"什么是隔离性 隔离性定义的是，如果多个事务并发执行时，事务之间不应该出现相互影响的情况，它其实就是数据库的并发控制。可能你对隔离性还有点陌生，其实在编程的过程中，隔离性是我们经常会碰到的一个概念，下面我们就具体讨论一下。在应用程序的开发中，我们通常会利用锁进行并发控制，确保临界区的资源不会出现多个线程同时进行读写的情况，这其实就对应了事务的最高隔离级别：可串行化，它能保证多个并发事务的执行结果和一个一个串行执行是一样的。现在你就会发现，隔离级别是我们日常开发中经常碰到的一个概念，那么你肯定会有一个疑问，为什么应用程序中可以提供可串行化的隔离级别，而数据库却不能呢？ 其实根本原因就是应用程序对临界区大多是内存操作，而数据库要保证持久性（即 ACID 中的 Durability），需要把临界区的数据持久化到磁盘，可是磁盘操作比内存操作要慢好几个数量级，一次随机访问内存、 SSD 磁盘和 SATA 磁盘，对应的操作时间分别为几十纳秒、几十微秒和几十毫秒，这会导致临界区持有的时间变长，对临界区资源的竞争将会变得异常激烈，数据库的性能则会大大降低。 所以，数据库的研究者就对事务定义了隔离级别这个概念，也就是在高性能与正确性之间提供了一个缓冲地带，相当于明确地告诉使用者，我们提供了正确性差一点但是性能好一点的模式，以及正确性好一点但是性能差一点的模式，使用者可以按照自己的业务场景来选择。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:30:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"隔离级别与异常情况 通过对隔离性定义的讨论，我们知道了隔离性是高性能与正确性之间的一个权衡，那么它都提供了哪些权衡呢？首先，这个权衡是由隔离级别（Isolation Level）来定义的， SQL-92 标准定义了 4 种事务的隔离级别：读未提交（Read Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable Read）和串行化（Serializable），在后面的发展过程中，又增加了快照隔离级别（Snapshot Isolation）。由于我们在讨论事务隔离级别的时候，经常通过是否避免某一些异常情况来定义，所以在具体讨论每一个隔离级别之前，我们先来看看事务并发时可能会出现的异常情况，具体有以下几种。 其一，脏写（Dirty Write），即有两个事务 T1 和 T2 ， T1 更改了 x ，在 T1 提交之前， T2 随之也更改了 x ，这就是脏写，这时因为 T1 还没有提交，所以 T2 更改的就是 T1 的中间状态。假如现在 T2 提交了， T1 就要回滚，如果回滚到 T1 开始前的状态，已经提交的 T2 对 x 的操作就丢失了；假如不回滚到 T1 开始前的状态，已经 Roll Back 的 T1 的影响就还存在于数据库中。能够允许这种现象的数据库基本是不可用的，因为它已经不能完成事务的 Roll Back 了。 其二，脏读（Dirty Read），即有两个事务 T1 和 T2 ， T1 更改了 x ，将 x 从 0 修改为 5 ，在 T1 提交之前， T2 对 x 进行了读取操作，读到 T1 的中间状态 x = 5 ，这就是脏读。假设最终 T1 Roll Back 了，而 T2 却根据 T1 的中间状态 x = 5 做了一些操作，那么最终就会出现不一致的结果。 其三，不可重复读（Nonrepeatable read）/ 读倾斜（Read Skew），即有两个事务 T1 和 T2 ， T1 先读了 x = 0 ，然后 T2 更改了 x = 5 ，接着提交成功，这时如果 T1 再次读取 x = 5 ，就是不可重复读。不可重复读会出现在一个事务内，两次读同一个数据而结果不一样的情况。 其四，丢失更新（Loss of Update），即有两个事务 T1 和 T2 ， T1 先读 x = 0 ，然后 T2 读 x = 0 ，接着 T1 将 x 加 3 后提交， T2 将 x 加 4 后提交，这时 x 的值最终为 4 ， T1 的更新丢失了，如果 T1 和 T2 是串行的话，最终结果为 7 。 其五，幻读（Phantom Read），即有两个事务 T1 和 T2 ， T1 根据条件 1 从表中查询满足条件的行，随后 T2 往这个表中插入满足条件 1 的行或者更新不满足条件 1 的行，使其满足条件 1 后提交，这时如果 T1 再次通过条件 1 查询，则会出现在一个事务内，两次按同一条件查询的结果却不一样的情况。 其六，写倾斜（Write Skew），即假如 x ， y 需要满足约束 x + y \u003e= 0 ，初始时 x = -3 ， y = 5 ，事务 T1 先读 x 和 y ，然后事务 T2 读 x 和 y ，接着事务 T2 将 y 更新为 3 后提交，事务 T1 将 x 改为 -5 后提交，最终 x = -5 ， y = 3 不满足约束 x + y \u003e= 0 。 讨论完这些异常情况后，我们再通过一个表格来看看，事务的隔离级别与这些异常情况的关系。 我们从表格中可以看到，在隔离级别的一致性强度上，读未提交 \u003c 读已提交 \u003c 可重复读 \u003c\u003e 快照 \u003c 串行化，可重复度和快照隔离级别之间是不可以比较的。 这里要特别注意，由于 SQL 标准对隔离级别的定义还存在不够精确的地方，并且标准的定义有时还与实现有关系，而各个数据库对隔离级别的具体实现又各不相同，所以上面的表格只是对常见的隔离级别异常情况的定义，你可以把它当成一个通用的标准参考。当你使用某一个数据库时，需要读一下它的文档，确定好它的每一个隔离级别具体的异常情况。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:30:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何避免异常情况 现在我们已经知道了每一个隔离级别可能会出现的异常情况，如果当前数据库使用了某一个隔离级别，我们也知道它有哪些异常情况，是否有办法来避免呢？其实这是一个非常好的问题，不过有些异常情况只能通过提升隔离级别来避免，那么接下来，我们就针对每一种异常情况来一一讨论一下。 其一，对于脏写，几乎所有的数据库都可以防止异常的出现，并且我们可以理解为出现脏写的数据库是不可用的，所以这里就不讨论脏写的情况了。其二，对于脏读，提供“读已提交”隔离级别及以上的数据库，都可以防止异常的出现，如果业务中不能接受脏读，那么隔离级别最少在“读已提交”隔离级别或者以上。其三，对于不可重复读或读倾斜，“可重复读”隔离级别及以上的数据库都可以防止问题的出现，如果业务中不能接受不可重复读和读倾斜，那么隔离级别最少在“可重复读”隔离级别或者以上。其四，对于丢失更新，如果数据库的隔离级别不能达到“可重复读”隔离级别或者以上，那么我们可以考虑以下的几种方法来避免。 首先，如果数据库提供了原子写操作，那么一定要避免在应用层代码中，进行“读－修改－写”操作，应该直接通过数据库的原子操作执行，避免更新丢失的问题。例如关系数据库中的 udpate table set value ＝ value ＋ 1 where key ＝ ＊ ，MongoDB 中的 $set、$unset 等操作。 数据库的原子操作一般通过独占锁来实现，相当于可串行化的隔离级别，所以不会有问题。不过，在使用 ORM 框架时，很容易在应用层代码中完成“读－修改－写”的操作，导致无法使用数据库的原子操作。 其次，如果数据库不支持原子操作，或者在某些场景中，原子操作不能处理时，可以通过对查询结果显式加锁来解决。对于 MySQL 来说，就是 select for update ，通过 for update 告诉数据库，查询出来的数据行过一会是需要更新的，需要加锁防止其他的事务，对同一块数据也进行读取加更新操作，从而导致更新丢失的情况。 最后，我们还可以通过原子比较和设置来实现，例如 update table set value ＝ newvalue where id ＝ ＊ and value ＝ oldvalue 。但是这个方式有一个问题，如果 where 条件的判断是基于某一个旧快照来执行的，那么 where 的判断就是没有意义的，所以要是采用原子比较和设置避免更新丢失的话，一定要确认数据库比较－设置操作的安全运行条件。 我们把第五点和第六点合在一起讨论，对于幻读和写倾斜，如果数据库的隔离级别不能达到可串行化的隔离级别，我们就可以考虑通过显式加锁来避免幻读和写倾斜。通过对事务利用 select for update 显式加锁，可以确保事务以可串行化的隔离级别运行，所以这个方案是可以避免幻读和写倾斜的，但不是在所有的情况下都适用。比如 select for update 中，如果在 select 时不能查询到数据，那么这时的数据库将无法对数据进行加锁。 例如，在订阅会议室时，多个事务先通过 select for update 查询会议室某一时段的订阅记录，当该会议室在这个时间点还没有被订阅时，就都查询不到订阅记录，select for update 也就无法进行显式加锁。如果后面多个事务都会订阅成功，就会导致一个会议室，在某一时段只能订阅一次的约束被破坏。 所以，显式加锁对于写倾斜不能适用的情况就是，如果在 select 阶段没有查询到临界区的数据，就会导致无法加锁。这种情况下，我们可以人为引入用于加锁的数据，然后通过显式加锁来避免写倾斜的问题。比如，在订阅会议室时，我们为所有会议室的所有时间都创建好数据，每一个“时间－会议室”一条数据，这个数据没有其他的意义，只是在 select for update 时，数据库可以 select 查询到数据来进行加锁操作。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:30:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何来实现隔离性 到这里，我们已经讨论完事务的隔离级别，每一个隔离级别可能遇到的异常情况，以及避免这些异常情况的具体技术方案，最后我们一起来讨论一下，事务的隔离性是如何实现的。 既然事务的隔离性是用来确保多个事务并发执行时的正确性的，那么我们就可以依据应用程序开发中经常使用的并发控制策略，来思考事务的隔离性如何实现，这样就可以轻松得出如下的几个方法。 首先，最容易想到的是通过锁来实现事务的隔离性。对于锁的方案，最简单的策略是整个数据库只有一把互斥锁，持有锁的事务可以执行，其他的事务只能等待。但是这个策略有很明显的问题，那就是锁的粒度太粗，会导致整个数据库的并发度变为 1 。 不过，我们可以进行优化，为事务所操作的每一块数据都分配一把锁，通过降低锁的粒度来增加事务的并发度。同时，相对于互斥锁来说，读写锁是一个更好的选择，通过读写锁，多个事务对同一块数据的读写和写写操作会相互阻塞，但却能允许多个读操作并发进行。这样我们就得到了一个事务的并发模型，但是一个事务通常由多个操作组成，那么一个事务在持有锁修改某一个数据后，不能立即释放锁。如果立即释放锁，在其他的事务读到这个修改或者基于这个修改进行写入操作，当前事务却因为后续操作出现问题而回滚的时候，就会出现脏读或脏写的情况。 对于这个问题有一个解决方法，即事务对于它持有的锁，在当前的数据操作完成后，不能立即释放，需要等事务结束（提交或者回滚）完成后，才能释放锁。这个加锁的方式就是两阶段锁（2PL）：第一阶段（当事务正在执行时）获取锁，第二阶段（在事务结束时）释放所有的锁。 那么现在是否就得到了可串行化的隔离性呢？其实还不是的，我们现在还没有解决幻读和写倾斜的问题，幻读指的是其他的事务改变了当前事务的查询结果，在幻读的情况下，可能会导致写倾斜，比如前面提到的例子，当订阅会议室的事务进行 select 操作时，由于会议室还没有被订阅，所以数据库没有办法对订阅记录加锁，这样多个事务同时操作，就会导致一个会议室，在同一个时间内出现多个订阅记录的异常情况。关于这个问题，我们可以通过谓词锁（Predicate Lock）来解决。它类似于前面描述的读写 / 互斥锁，但是它的加锁对象不属于特定的对象（例如表中的一行），它属于所有符合某些搜索条件的对象，如果对符合下面 SELECT 条件的对象加锁。 sql SELECT * FROM bookings WHERE room_id = 888 AND start_time \u003c ‘2022-02-02 13:00’ AND end_time \u003e ‘2022-02-02 12:00’; 这样就可以避免一个会议室，在同一个时间内被订阅多次的情况了。同时，间隙锁（Next-Key Locking）也可以解决这个问题，它是关于谓词锁的简化以及性能更好的一个实现。 其次，我们可以通过多版本并发控制（MVCC , Multi-Version Concurrency Control）实现隔离性。数据库为每一个写操作创建了一个新的版本，同时给每一个对象保留了多个不同的提交版本，读操作读取历史提交的版本，这样对同一个数据来说，只有写写事务会发生冲突，读读事务和读写事务是不会发生冲突的。对于写写冲突的问题，可以通过加锁的方式来解决，不过对于 MVCC 来说，相对于悲观锁，乐观锁是一个更常见的选择。 另外，通过 MVCC 来实现隔离性，由于读操作都是读取旧版本的数据，所以数据库需要知道哪些读取结果可能已经改变了，然后中止事务，不然就会导致写倾斜的问题出现。这需要数据库能够检测出异常情况，然后中止事务，而实现这个异常检测机制的 MVCC ，我们称为可序列化快照隔离（SSI , Serializable Snapshot Isolation），这是一个比较新的研究方向，目前还处于快速发展中。 最后是一个最简单的方式，通过避免并发的情况出现，在单个线程上按顺序一次只执行一个事务。这个方式避免了并发的出现，但是也失去了并发带来的多机多核的计算能力提升，目前在一些基于内存的数据库上使用过，比如 Redis ，同时它也在研发和发展中。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:30:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课中，我们先掌握了有哪些隔离级别，以及每一个隔离级别可能出现的异常情况，这样在业务开发的过程中，我们对程序可能出现的异常情况就心中有数了。其次，我们一起学习了如何避免异常情况的出现，在以后的业务选型过程中，我们不仅知道如何来选择数据库的隔离级别，也知道了当数据库的隔离级别不能调整时，如何通过应用开发手段来避免一些异常情况。最后，我们讨论了如何实现数据库的隔离级别，这个过程能帮我们更深刻地理解隔离性的知识和原理。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:30:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"25｜事务（四）：持久性，吃一碗粉就付一碗粉的钱 通过上节课的学习，我们掌握了通过 MVCC 和 2PL 实现分布式事务隔离性的技术原理，并且也明白了隔离级别是事务在正确性和性能之间的一个权衡，以后在极客时间的业务研发中，我们就可以根据业务特点，对事情的隔离级别做出正确的选择了。虽然事务在有了一致性、原子性和隔离性的保障后，已经可以很好地保障业务在各种使用场景下的正确性了，但是如果机器突然断电或者崩溃，导致已经提交成功的事务数据丢失了，最终也就功亏一篑了。所以在这节课中，我们将一起来讨论如何确保机器在突然断电、崩溃等异常情况下，不会将已经成功的事务数据丢失掉的问题，即事务的持久性。我们首先会通过持久性的定义分析出它面临的挑战，然后再一起讨论一下如何通过非易失性存储来保障事务的持久性，最后一起讨论在分布式系统中，如何通过数据复制来进一步提高事务的持久性。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:31:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"持久性的挑战 持久性在事务中的定义是，如果一个事务已经提交，不论什么原因，它产生的结果都是永久存在的，这保证了事务的结果不会丢失。通过持久性的定义，我们会发现要保证事务的持久性，一个显而易见的思路就是，将事务的结果立即写入到非易失性存储设备中，比如 SSD 硬盘和 SATA 硬盘等，并且写入的副本数越多，持久性就越高。 但是理想很完美，现实却很骨感，将数据写入到硬盘中其实是非常消耗性能的。如果将每一个事务所有的操作结果，都实时写入到持久化的存储设备中，这样的数据库几乎就是不可用的，更不用说多副本的写入了，那么我们如何来解决存储设备的写入性能和事务持久性之间的矛盾呢？ ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:31:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何通过非易失性存储保障持久性 关于这个问题，我们需要从磁盘设备的特性开始说起。对于 SATA 硬盘来说，可以将它简单理解为一个有很多同心圆的圆盘，在写入数据的时候，会经历以下几个步骤： 寻道，找到数据所在的同心圆，这个时间是毫秒级别的；寻址，找到数据所在的同心圆的位置，这个时间也是毫秒级别的；开始读写数据，每秒可以读写的数据量为 100M 级别的数据，这个是非常快的。 我们可以从上面看出，如果没有寻道和寻址这两个步骤， SATA 硬盘的性能其实是非常不错的。那么如何避免寻道和寻址呢？如果第一次寻道和寻址后，持续对数据进行大量的读写，即顺序读写，是可以忽略寻道和寻址的时间消耗的。而对应顺序读写的是随机读写，它每一次读写的数据量很小，并且数据位置不相邻，都需要先寻道、寻址，然后才能进行数据读写，所以随机读写的性能是非常差的。 对于 SSD 硬盘，寻址的情况则大大改观，不需要像 SATA 硬盘一样机械地寻道、寻址，它可以通过电路直接获得读写的地址。但是，SSD 硬盘与传统的 SATA 硬盘有一点不同，即它不能够覆盖写，所以对于已经存在数据的 SSD 磁盘来说，一次数据的写入需要分为 2 个步骤： 擦除 SSD 上已有的数据；写入新的数据。 但是对于 SSD 来说，一般每次写入的最小单位为 Page ，一个 Page 的大小为 4KB，而每次擦除的大小单位为 Block ， Block 通常由 64 或 128 个 Page 组成。由此看出， SSD 的写入与擦除的单位大小不匹配，那么如果仅仅是要修改一个 Page 的数据，在单个 Block 之中没有了空余的 Page 时，需要先读取 Block 的内容，然后擦除一个 Block 的数据，再将 Block 的内容和修改的内容进行合并，写入一个 Block 的数据。而这就会导致原本只需要写入 4KB 的数据，最终却写入了 64 倍甚至是 128 倍的数据，出现写放大的问题。 从上面的讨论中，我们发现对于 SSD 磁盘来说，写放大是无法避免的，相比于顺序写入，随机写入会大大加剧写放大的问题。 总而言之，不论是 SATA 硬盘还是 SSD 硬盘，从硬盘自身的特点来说，顺序读写的性能都要远远高于随机读写。另外从系统的角度来看，顺序读写在预读和缓存命中率等方面也要大大优于随机读写。 现在，我们就可以回答存储设备的写入性能和事务持久性之间矛盾的问题了：由于事务的持久性是必须的，如果一个事务已经提交，不论什么原因，它产生的结果都是永久存在的，所以对于单节点来说，我们可以先在内存中将事务的操作完成，然后将处理的结果顺序写入日志文件中，这就避免了事务操作结果随机写入存储的性能问题了。然后我们再提交事务，这样一来，哪怕事务提交后，机器立即崩溃了，在机器故障恢复后，系统依然能通过日志文件，恢复已经提交的事务。所以，通过顺序写入日志的形式，避免了非易失性存储设备随机写入性能差的问题，达到了事务提交时，所有事务操作结果都写入存储设备的目的。在这个时候，即使系统崩溃，事务的持久性也是有保障的。我们把这种通过顺序写入日志的形式，称之为重做日志（RedoLog）或预写日志（Write Ahead Log）。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:31:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"如何通过数据冗余保障持久性 通过 Redo Log 或 WAL，是否可以完美地解决事务持久性的问题呢？其实还是不够的。虽然 Redo Log 能保证系统在崩溃、重启等问题出现时的持久性，但是当存储设备出现了故障，比如数据都不可读的时候，还是会出现即使事务已经提交成功，但是事务结果却丢失的情况。那么这个问题应该如何处理呢？ 有一种思路是，通过磁盘阵列，从磁盘内部通过冗余数据来解决。比如 RAID 1 ，我们将多块硬盘组成一个磁盘阵列，磁盘阵列中每块磁盘都有一个或多个是副本磁盘。事务的每一次写入都同时写入所有的副本硬盘，这样只要不是所有的副本磁盘同时出现故障的情况，我们就都可以正常从磁盘上读到数据，不会影响事务的持久性。还有一种磁盘阵列的方式是 RAID 5 ，它是通过冗余校验数据的方法来保障持久性。 磁盘阵列的方法确实可以解决事务的持久性问题，但是由于磁盘阵列上多块硬盘的地理位置通常都是在一起的，这样如果出现地震、火灾和洪水等自然灾害时，可能会导致整个磁盘阵列上的硬盘都不可用，那么事务的持久性就不能被保证了。 而另外一个思路是通过增加副本，通过网络复制数据来解决。其实这个问题在“复制”系列课程中已经详细讨论过了，但是对于事务的场景来说，由于数据的复制必须是线性一致性的，所以我们只能采用同步的主从复制，但是这个方式在性能和可用性方面都存在问题。 性能问题：一次写入必须等所有的节点都写入成功，整体的写入性能取决于最慢的节点的写入性能，并且网络的不确定性会加剧性能问题。可用性问题：对于同步复制来说，如果一个节点出现故障，就会导致写入失败，非常影响系统整体的可用性。 对于事务场景，如果我们不采用同步的主从复制，是否有其他的办法来解决呢？ 其实我们可以通过 Raft 或者 Paxos 之类的共识算法来解决。对于数据复制到多个副本来说，其实就是多个副本对写入的结果达成共识，利用 Raft 或者 Paxos 之类的共识算法进行数据的复制，可以实现线性一致性，同时共识算法可以避免同步主从复制在性能、可用性问题和磁盘阵列多副本地理位置相近的问题。 性能问题：一次写入只需要等大多数的节点都写入成功即可，整体的写入性能取决于最快的大多数节点的写入性能。可用性问题：只要出现故障的节点数不超过大多数，系统就会写入成功，它能容忍少数节点的故障。地理位置相近的问题：数据通过网络复制，可以将副本分布到不同的数据中心、城市或者大洲，进一步提高事务的持久性。 对于共识算法，我们会在后面的课程“一致性与共识”中详细讨论，在这节课里，你只要知道对于存储系统内部的多节点数据副本，一般都通过共识算法来解决即可。到这里，我们就学习完了“事务”序列的课程，这里从事务的四个特性的角度，总结一下： 一致性（C）：是指事务的正确性，需要底层数据的线性一致性，事务层的原子性、隔离性、持久性，以及数据库层面的约束检测和应用层的约束检测来保证。原子性（A）：是指事务操作的不可分割性，一般通过原子提交协议 2PC 或 3PC 来保障。隔离性（I）：是指事务操作在并发控制，一般数据库都提供弱隔离性，是数据库在性能和正确性之间的衡权，一般通过 MVCC 或者 2PL 来实现。持久性（D）：是指已提交的事务结果不可丢失，一般在单机上通过非易失性存储来保障，在分布式场景下，通过数据冗余来保障。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:31:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课，我们知道了事务在实现持久性的过程中，会面临性能和可用性这两个方面的挑战。首先，要保障事务在系统宕机情况下的持久性，必须保证事务的操作结果能够立即保存到硬盘之类的非易失性存储中，但是不论是 SATA 硬盘还是 SSD 硬盘，对于这一类随机读写操作都会面临严重的性能问题，目前我们主要是通过重做日志（RedoLog）或预写日志（Write Ahead Log），将随机读写转化为顺序读写来提高事务的性能。其次，要保障事务在磁盘故障情况下的持久性，必须将数据复制到多块磁盘上，这节课我们介绍了两种思路：**一是通过磁盘阵列，从磁盘内部复制数据来解决；另一种是通过外部的数据复制来解决。**其中，磁盘阵列的多块硬盘的地理位置通常都是在一起的，地震、火灾和洪水等自然灾害，可能会导致整个磁盘阵列同时毁坏；而外部的数据复制方法需要保证数据的强一致性，它会面临性能和可用性的问题，这里我们主要通过 Raft 或者 Paxos 之类的共识算法来解决。最后，我们可以看到，事务的持久性会让事务的结果被持久保存下来，不会出现因为数据丢失、毁坏而导致不认账的情况，这也就是本节课标题提到的“吃一碗粉就付一碗粉的钱”的真正含义。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:31:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"26｜一致性与共识（一）：数据一致性都有哪些级别？ 通过学习“事务”序列的内容，我们从事务的四个特性 ACID 的角度讨论了相关的知识与技术原理，这样在以后的工作中，事务对我们来说就不再是一个陌生和难懂的概念，而是越发清晰了。我们能清楚地知道事务能提供哪些保障，我们的代码逻辑可能会出现什么样的异常情况，以及怎么避免这些异常情况的出现。恭喜你在学习分布式的道路上又前进了一大步！不过，在前面课程的学习中，我们经常会碰到两个概念：多副本数据的一致性和多节点的共识，比如在分布式锁、事务的原子性等场景中。其实在分布式系统中，一致性和共识是两个绕不过的话题，现在各种各样的分布式系统都是建立在一致性和共识之上的，**可以说没有一致性和共识，就没有可用的分布式系统。**既然一致性和共识对于分布式系统来说这么关键，那么我们一定要好好掌握。可是，通过前面课程中对一致性和共识场景的讨论，你现在虽然对二者有了很多的感性认识，知道在什么场景下会遇到一致性和共识方面的问题，也知道一些具体的解决方案，但是如果要你具体介绍一致性和共识的话，心里是不是不太有底呢？所以，从这节课开始，我们将一起花三节课的时间来解决这个问题。这一节课，我们先介绍一致性问题的来源，然后我们从一致性模型从强到弱的角度，来介绍几种经典的一致性的模型，并且一起讨论和对比各个一致性模型之间的差异。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:32:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"一致性问题的来源 虽然数据一致性是分布式系统的基石，但是其实最早研究一致性的场景并不是分布式系统，而是多路处理器。不过我们可以将多路处理器理解为单机计算机系统内部的分布式场景，它有多个执行单元，每一个执行单元都有自己的存储（缓存），一个执行单元修改了自己存储中的一个数据后，这个数据在其他执行单元里面的副本就面临数据一致的问题。当时间走到 1990 年代时，由于互联网公司的快速发展，单机系统在计算和存储方面都面临瓶颈，分布式是一个必然的选择，但是这也进一步放大了数据一致性面临的问题。对于数据的一致性，最理想的模型当然是表现得和一份数据完全一样，修改没有延迟，即所有的数据修改后立即被同步，但是这在现实世界中，数据的传播是需要时间的，所以理想的一致性模型是不存在的。 不过从应用层的角度来看，我们并不需要理想的一致性模型，只需要一致性模型能满足业务场景的需求就足够了，比如在一些统计点赞数的场景中，是能容忍一定的误差的，而评论之类的场景中，可能只要有因果关系的操作顺序一致就可以了。同时由于一致性要求越高，实现的难度和性能消耗就越大，所以我们可以通过评估业务场景来降低数据一致性的要求，这样人们就定义了不同的一致性模型来满足不同的需求。是不是发现了这里的思考逻辑和事务的隔离级别一样了？都是正确性和性能之前的衡权。 讨论完了一致性问题的来源后，接下来我们从客户端读写操作的维度来讨论一致性模型。由于一致性模型的定义大多是基于数学语言来定义的，理解起来有一定的难度，所以在课程中，我们尽量用简单的语言来讨论。接下来，我们将讨论四个经典且常见的一致性模型：线性一致性、顺序一致性、因果一致性和最终一致性。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:32:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"线性一致性 线性一致性模型（Linearizability）是 Herlihy 和 Wing 等于 1987 年在论文 “Axioms for Concurrent Objects” 中提出的，线性一致性也被称为原子一致性（Atomic Consistency）、强一致性（Strong Consistency）、立即一致性（Immediate Consistency）和外部一致性（External Consistency）。线性一致性是非常重要的一个一致性模型，在分布性锁、Leader 选举、唯一性约束等很多场景都可以看到它的身影。对于线性一致性的描述，我们可以从读写操作的维度来描述。 对于写操作来说，任意两个写操作 x1 和 x2： 如果写 x1 操作和写 x2 操作有重叠，那么可能 x1 覆盖 x2，也可能 x2 覆盖 x1；如果写 x1 操作在写 x2 开始前完成，那么 x2 一定覆盖 x1。 对于读操作来说： 写操作完成后，所有的客户端都能立即观察到；对于多个客户端来说，必须读取到一样的顺序。 我们可以看到，线性一致性保证了所有的读取都可以读到最新写入的值，即一旦新的值被写入或读取，所有后续的读都会看到写入的值，直到它被再次覆盖。在线性一致性模型中不论是数据的覆盖顺序还是读取顺序，都是按时间线从旧值向新值移动，而不会出现旧值反转的情况。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:32:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"顺序一致性 顺序一致性模型（Sequential Consistency）是 Leslie Lamport 在 1979 年发表的论文 “How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Program” 中提出的，在论文中具体的定义如下： A multiprocessor is said to be sequentially consistent if the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program. 如果任何执行的结果与所有处理器的操作都以某种顺序执行的结果相同，并且每个单独的处理器的操作按照其程序顺序出现在该序列中，则称多处理器是顺序一致的。 对于顺序一致性，论文中的定义虽然严谨，但是理解起来也是有难度的，它需要掌握一些前置的定义，比如 “program order”。不过在这里，我们依然可以用简单的语言来描述。对于写操作来说，任意两个写操作 x1 和 x2： 如果写 x1 操作和写 x2 操作有重叠，那么可能 x1 覆盖 x2，也可能 x2 覆盖 x1；当写 x1 操作在写 x2 开始前完成，如果两个写操作没有因果关系，当写 x1 操作在写 x2 开始前完成，那么有可能 x1 覆盖 x2，也有可能 x2 覆盖 x1；如果两个写操作有因果关系，即同一台机器节点先写 x1，或者先看到 x1 然后再写 x2，则所有节点必须用 x2 覆盖 x1。 对于读操作来说： 如果写操作 x2 覆盖 x1 完成，那么如果一个客户端到 x2 后，它就无法读取到 x1 了，但是这个时候，其他的客户端还可以观察到 x1；对于多个客户端来说，必须观察到一样的顺序。 相对于线性一致性来说，顺序一致性在一致性方面有两点放松： 对于写操作，对没有因果关系的非并发写入操作，不要求严格按时间排序；对于读操作，只要求所有的客户端观察到的顺序一致性，不要求写入后，所有的客户端都必须读取新值。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:32:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"因果一致性 因果一致性模型（Causal Consistency）是 Mustaque Ahamad, Gil Neiger, James E. Burns, Prince Kohli, Phillip W. Hutto 在 1991 年发表的论文 “Causal memory: definitions, implementation, and programming” 中提出的一种一致性强度低于顺序一致性的模型。在这里，我们依然从读写操作的维度来进行描述。 对于写操作来说，任意两个写操作 x1 和 x2：如果两个写操作没有因果关系，那么写 x1 操作在写 x2 开始前完成，有的节点是 x1 覆盖 x2，有的节点则 x2 可能覆盖 x1；如果两个写操作有因果关系，即同一台机器节点先写 x1，或者先看到 x1 然后再写 x2，则所有节点必须用 x2 覆盖 x1。 对于读操作来说：如果写操作 x2 覆盖 x1 完成，那么如果一个客户端到 x2 后，它就无法读取到 x1 了，但是这个时候，其他的客户端还可以观察到 x1。 相对于顺序一致性来说，因果一致性在一致性方面有两点放松：对于写操作，对没有因果关系的非并发写入操作，不仅不要求按时间排序，还不再要求节点之间的写入顺序一致了；对于读操作，由于对非并发写入顺序不再要求一致性，所以自然也无法要求多个客户端必须观察到一样的顺序。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:32:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"最终一致性 最终一致性模型（Eventual Consistency）是 Amazon 的 CTO Werner Vogels 在 2009 年发表的一篇论文 “Eventual Consistency” 里提出的，它是 Amazon 基于 Dynamo 等系统的实战经验所总结的一种很务实的实现，它不同于前面几种由大学计算机科学的教授提出的一致性模型，所以也没有非常学院派清晰的定义，但是我们依然可以从读写操作的维度来描述它。 对于同一台机器的两个写操作 x1 和 x2 来说：如果写 x1 操作在写 x2 开始前完成，那么所有节点在最终某时间点后，都会用 x2 覆盖 x1。 对于读操作来说：在数据达到最终一致性的过程中，客户端的多次观察可以看到的结果是 x1 和 x2 中的任意值；在数据达到最终一致性的过程后，所有客户端都将只能观察到 x2。 我们可以看出来，“最终”是一个模糊的、不确定的概念，它是没有明确上限的，Vogels 提出这个不一致的时间窗口可能是由通信延迟、负载和复制次数造成的，但是最终所有进程的观点都一致，这个不一致的时间窗口可能是几秒也可能是几天。所以，最终一致性是一个一致性非常低的模型，但是它能非常高性能地实现，在一些业务量非常大，但是对一致性要求不高的场景，是非常推荐使用的。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:32:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 到这里，我们已经讨论完了几种最经典也最常见的一致性模型，现在我们来对这节课的内容做一个总结。首先，我们讨论了一致性问题最早出现在多路处理器的场景，现在在分布式系统中广泛出现。同时，我们还得出了一个结论：对一致性模型进行分级是正确性和性能之间的一个权衡。接着，我们从一致性模型强弱的维度，讨论了四个经典一致性模型的定义与差异，这里我们再从其他的维度描述一下，让你对一致性模型有一个更立体的理解。 第一，现在可以实现的一致性级别最强的是线性一致性，它是指所有进程看到的事件历史一致有序，并符合时间先后顺序, 单个进程遵守 program order，并且有 total order。第二，是顺序一致性，它是指所有进程看到的事件历史一致有序，但不需要符合时间先后顺序, 单个进程遵守 program order，也有 total order。第三，是因果一致性，它是指所有进程看到的因果事件历史一致有序，单个进程遵守 program order，不对没有因果关系的并发排序。第四，是最终一致性，它是指所有进程互相看到的写无序，但最终一致。不对跨进程的消息排序。在课程“复制（三）：最早的数据复制方式竟然是无主复制？”中讨论的 Quorum 机制就是最终一致性。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:32:6","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"27｜一致性与共识（二）：它们是鸡生蛋还是蛋生鸡？ 通过上节课的学习，我们了解了一致性模型的发展历史，同时还掌握了各个一致性模型之间的强弱差异，这样在极客时间后端技术的选型和演进过程中，你就能够做出最适合业务场景的选择了，这对于我们搭建分布式系统是非常关键的一个权衡。其实一致性和共识是两个如影随形的概念，我们在讨论一致性的时候，总是会提到共识，同时我们在研究共识的时候，一致性也是不能绕过的话题。那么，你一定会很好奇它们之间的关系是什么？一致性和共识是像鸡生蛋和蛋生鸡这种非常紧密的关系呢？还是其他的比较弱的关系呢？在这节课中，我们主要来讨论一致性与共识之间的关系，一方面解开你的疑问，另一方面通过探讨它们之间的关系，让你能够进一步理解一致性和共识。我们先一起来了解共识问题的场景与定义，然后分析达成共识所面临的挑战，最后再来探讨一致性和共识的关系。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:33:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"共识问题的定义 在分布式系统中，共识（Consensus）问题是最重要，也是最基本的问题之一，简单来说它就是多个节点（进程）对某一个事情达成一致的结果。在分布式系统中，我们经常碰到这样的场景，比如在主从复制的模型中，需要在多个节点选举出 Leader 节点。由于有且只能有一个 Leader 节点，所以多个节点必须就哪一个节点是 Leader 这个决定达成一致。那么共识算法经常用于像选举 Leader 、分布式锁服务这样，有且只有一个能胜出的场景。在讨论共识问题的时候，我们通常会做这样的形式化定义：一个或多个节点可以提议（Propose）某些值，而共识算法决定（Decide）采用其中某一个节点提议的某个值。比如在 Leader 选举的例子中，每一个节点都可以提议自己为 Leader 节点，而共识算法会让所有的节点对某一个节点为 Leader 达成一致。所以，通过上面的讨论，我们可以得出共识算法必须满足的四个条件，具体如下。 一致同意（Uniform Agreement）：所有协议的节点必须接受相同的决议。诚实性（Integrity）：所有节点不能反悔，即对一项提议，一个节点不能做两次决定。合法性（Validity）：如果决定了值 v ，则 v 一定是由某个节点所提议的。可终止性（Termination）：如果节点不崩溃，则一定可以达成决议。 其中，一致同意和诚实性定义了共识的核心思想：所有人都决定了相同的结果，并且一旦决定了，就不能再改变。合法性主要是为了排除没有意义的解决方案。例如无论节点提议了什么值，都可以让所有节点始终以某一个固定值（如 nil）达成共识的算法，这个算法满足一致同意和诚实性，但是由于达成共识的值是固定的，不是由某一个节点提出的，所以不满足合法性。可终止性确保了，共识算法在部分节点故障的情况下，其他的节点也能达成一致，可终止性让共识算法能够容错。如果共识算法不需要容错是很容易实现的，比如将某一个节点指定为共识算法的“独裁者”，其他的节点必须同意该节点做出的所有决定。不过这个算法的问题是如果“独裁者”节点出现故障，系统就将无法达成共识了。其实 2PC 协议就是不满足可终止性的共识协议。在 2PC 中，协调者节点就是“独裁者”节点，它在第一阶段通过收集参与者节点 Prepare 的响应做出决定，但是当协调者故障时，参与者就无法决定提交还是中止了。 到这里，你是否觉得共识问题非常简单呢？其实不然，共识问题是一个非常难的问题，如果处理不好共识，很有可能会出现各种问题或故障，比如在分布式锁服务 Leader 选举的场景中，如果出现两个 Leader，那么整个分布式锁服务就进入了脑裂的状态，锁的互斥性将会被破坏，使业务上出现不可预期的情况。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:33:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"达成共识的挑战 我们已经知道共识处理不好，可能会出现各种问题或故障，那么接下来，我们就从共识理论出发，分析达成共识面临的挑战，提前发现问题，解决问题。 第一个挑战是，在异步网络模型中，如果一个节点出现崩溃，那么共识就将无法达成，这就是大名鼎鼎的 “ FLP 不可能”。但是在分布式系统中，节点的故障是我们必须要面对的问题，如果以 Leader 选举的场景来讨论，需要达成共识的一个主要场景就是， Leader 节点崩溃了，需要重新选择一个新的 Leader ，选择新的 Leader 需要达成共识，但是因为 “ FLP 不可能”，所以共识不能在节点崩溃的时候达成。 这样看来问题就无解了，但是在实际应用中，我们是可以通过 Raft 或者 Paxos 之类的共识算法来解决这一类问题的，这是否和 “FLP 不可能” 冲突了呢？其实出现这个问题的根本原因是，在异步网络模型的定义中，网络中消息的传递延迟和节点的处理延迟是无上限的，所以对于消息是不能使用任何时钟或超时的，这样就导致在节点出现崩溃的时候，我们无法判断是否有节点崩溃，只能无限等待下去，使共识算法不能满足“可终止性”；但是在真实的环境中，我们可以允许共识算法使用超时或其他方法，来识别可疑的崩溃节点（即使有时怀疑是错误的），这样就避免了无限等待，使达成共识成为一个可行的事情。 第二个挑战与我们对分布式系统的故障模型定义有关。一般来说，在分布式系统中，我们对故障模型的定义是“崩溃 - 恢复失败”（Crash-Recovery Failure）模型。简单来说就是，在一个节点很长时间没有返回消息时，我们不能确定它是因为崩溃，还是因为网络或者计算速度过慢等原因导致的。其中网络或者计算速度过慢等原因，都是可以恢复的，这个模型和我们现在的分布式模型是最匹配的。 而像 Raft 和 Paxos 之类的共识算法，我们可以在“崩溃 - 恢复失败”（Crash-Recovery Failure）模型上，通过超时来识别可疑的崩溃节点，这就解决了一个问题：一个或多个节点可以提议（Propose）某些值，而共识算法决定（Decide）采用其中某一个节点提议的某个值。除此之外，还有“拜占庭失败”（Byzantine Failure）和“崩溃 - 停止失败”（Crash-Stop Failure）等模型。其中，“拜占庭失败”（Byzantine Failure）模型在“崩溃 - 恢复失败”（Crash-Recovery Failure）模型上，增加了节点会主动伪造和发布虚假消息的情况，由于这个情况在内网的分布式环境中几乎不会出现，并且要解决它的代价非常高，所以一般的共识算法，不会考虑解决“拜占庭失败”（Byzantine Failure） 模型下的共识问题。但是，在公网的分布式环境中，是需要解决这个问题的，例如比特币是通过“工作量证明”这样的算法，利用经济学原理，让节点造假的成本高于收益，来避免节点发布虚假消息的。而“崩溃 - 停止失败” （Crash-Stop Failure）模型在“崩溃 - 恢复失败”（Crash-Recovery Failure）模型上，去掉了节点崩溃后的不确定性，如果一个节点很长时间没有返回消息，那么它就是崩溃了，不会再回复什么消息，即崩溃后就立即停止。但是，在实际的分布式场景中，由于网络或者计算太慢而故障的节点，待恢复后，很久之前响应的消息是会正常出现的。所以，如果共识算法只能处理“崩溃 - 停止失败”（Crash-Stop Failure）模型，就不能适应我们实际的网络环境了。接下来，我们总结一下课程中提到的三种故障模型，如下表所示。 最后，还要特别强调一点，我们应该尽量选择像 ZooKeeper 和 etcd 这样，开源并且经过了广泛应用而被验证的程序，来为我们的应用提供共识能力，而不是自己再依据 Raft 或 Paxos 算法实现一个共识算法。因为相对于实现一个共识算法，证明共识算法实现的正确性是一个更难的问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:33:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"一致性和共识的关系 通过学习共识问题的定义和挑战，我们对共识问题有了一定的了解，接下来，我们将一致性和共识结合，讨论一下它们之间的关系，这里的一致性我们定义为一致性最强的线性一致性。在本专栏第 19 讲“主从复制”的课程中，我们讨论过主从复制：主节点承接所有的写入操作，然后以相同的顺序将它们应用到从节点，从而使主、从副本节点的数据保持最终一致性。如果在主节点或同步副本的从节点上读取数据，那么就是线性一致性的。当然如果数据库的读为快照读，由于不能读到最新版本的数据，这个情况下就不是线性一致性的。到这里，你是否觉得线性一致性非常容易实现，而且和共识算法也没有什么关系呢？其实不然，在主从复制的模型中，如果主节点不出现故障，那么一切都非常美好，但是如果主节点发生崩溃了，应该怎么办呢？ 首先，最简单的办法是等待主节点修复，如果主节点无法快速修复或者无法修复，那么系统的高可用就名存实亡了。对于等待主节点恢复的方式，我们可以理解为系统对之前达成主节点的共识是不可改变的。其次，人工切换主节点，这个方案是可行的，不过它的时间不确定，或长或短。如果出故障的时候，找不到合适的人来操作，就会严重影响系统的可用性。对于这个方式，我们可以理解为，系统对于主节点的共识是由操作人员来提供的，这是一个来自“上帝”视角的共识。最后，让程序自动切换主节点，这就需要其余正常运行的节点，来选择一个新的主节点，这样就回到了 Leader 选举的场景，分布式系统中的共识问题就出现了。这个方式是通过共识算法，让系统对一个新 Leader 节点达成共识，避免多个 Leader 节点出现，导致脑裂的情况发生。 到这里，我们就明白了，线性一致性是数据存储系统对外表现的一种形式，即好像只有一个数据副本，但是在实现数据一致性，实现容错的时候，我们需要共识算法的帮助。当然，这里要特别注意，我们通过共识算法，除了可以实现线性一致性，也可以实现顺序一致性等其他的数据一致性，共识算法是用来满足线性一致性的容错性的。同时，不使用共识算法，我们也可以实现数据的线性一致性，比如 ABD 和 SCD broadcast 之类的非共识算法，也可以实现线性一致性。总而言之，我们通过共识算法，可以实现高可用的线性一致性，以及其他的一致性存储系统，在这种情况下，共识算法是手段，一致性是目的，先有共识算法，后有高可用的线性一致性系统。同时，不通过共识算法，我们也可以用其他的方法，来实现线性一致性等其他的一致性，在这种情况下，共识和一致性就没有关系了。不过，目前通过共识算法，来实现高可用的线性一致性模型，是一个最常见的选择。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:33:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课中，我们通过 Leader 选举的业务场景，讨论了共识问题的定义，并且得出了一个共识算法需要满足四个要求：一致同意、诚实性、合法性和可终止性。现在，你不仅可以识别出业务场景中的共识问题，还能深刻理解这些场景需要引入共识的原因。接着，我们一起分析了达成共识所面临的挑战，其中让人震惊的是“ FLP 不可能”原理竟然证明了，在异步网络中，如果一个节点出现故障，共识就不可能达成。不过这种理论上的不可能，我们可以在现实中通过超时等机制解决。同时，我们还讨论了分布式系统中的几种故障模型，这让我们可以更好地理解分布式理论的研究对象，以及现实的分布式系统所面临的问题。最后，我们讨论了一致性和共识的关系，得出了具体结论：通过共识算法，我们可以实现高可用的线性一致性，但是共识算法不是线性一致性的必要条件。到这里，你一定对一致性和共识有了清晰的认识。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:33:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"28｜一致性与共识（三）：共识与事务之间道不明的关系 通过上节课的学习，我们知道了共识问题的使用场景、定义和经典的算法，并且从共识的角度深入探讨了一致性和共识的关系，这让我们对一致性和共识的理解更进了一步。你应该还记得，在课程第 23 讲“原子性”中提到过，当我们在实现事务的原子性时，采用的是 2PC 或 3PC 这样的共识协议；同时，在课程第 25 讲“持久性”中我们也讲过，通过线性一致性算法来复制数据，可以提高事务的持久性。另外，最显而易见的就是，事务的 ACID 中，C 就是一致性。那么，你一定在想，在分布式事务中，共识与事务之间是什么关系呢？是不是像共识和线性一致性一样，共识是方法和手段，事务的一致性是目的呢？在这节课中，我们就一起来讨论一下共识与事务之间的关系。我们先从事务的特性 ACID 的维度，一一来分析事务与共识的关系，然后以它们的关系为基础，探讨事务的本质问题，让你深入理解事务与共识、一致性之间的联系，从根本上理解分布式事务，为以后的工作打下一个坚实的基础。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:34:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"事务与共识的关系 通过课程第 22 讲“一致性”的学习，我们知道了事务的最终目的是实现一致性，即确保事务正确地将数据从一个一致性的状态，变换到另一个一致性的状态。为了达成这个目标，除了需要应用层的逻辑保证外，在事务层面还需要通过原子性、隔离性和持久性这三个特性一起协作。很有意思的一件事情是，在分布式事务中，事务这三个特性都与共识有一定的关系，下面我们来一一讨论一下。首先，对于原子性来说，在分布式系统中，需要通过 2PC 或 3PC 之类的原子提交协议来实现。以 2PC 为例，协调者在第一阶段通过接收所有参与者对 Prepare 请求的响应，才能最终确定当前的事务是提交还是中止，而这就是典型的共识场景：所有的参与者都同意，就提交事务；如果有参与者不同意，就中止事务。所以，我们认为 2PC 或 3PC 之类的原子提交协议是共识协议。 另外，还要特别注意一点，我们在上节课讨论过， 2PC 不是一个完备的共识算法，它满足共识算法的一致同意、诚实性以及合法性，但是在协调者出现故障的时候，并不能满足共识算法的可终止性。其次，对于隔离性来说，我们一般通过 2PL 或 MVCC 的方式来实现，可是它们能正确实现隔离性的前提条件，建立在底层数据为单副本的基础之上。但是在分布式系统中，为了系统的高可用，底层存储的数据是多副本，为了对事务操作表现出单副本的状态，数据的复制协议必须是线性一致性的，而线性一致性的数据复制协议，通常都是通过共识算法来实现的。学到这里，你会发现特别有意思，我们从事务的隔离性深层次去探索，就会触碰到共识这个话题。 最后，对于持久性来说，我们在课程第 25 讲中讨论过，在分布式系统中，为了进一步提高事务的持久性，我们会对数据进行复制，通过冗余来提高持久性。虽然数据复制可以不需要共识，但是就像上一段的讨论那样，为了保障事务的隔离性，数据的复制必须是线性一致性的。所以我们可以得出，事务为了持久性而引入了数据复制，但是为了保障隔离性，只能选择线性一致性的数据复制算法，而一旦涉及线性一致性，就说明我们又回到共识了。通过上面的讨论，你是否会感觉到在分布式系统中，当我们为了实现一个确定性正确的程序，一步一步深挖下去，就一定会碰到共识问题呢？其实这一点很好理解，比如在现实生活中，多人合作完成一件事情，如果人们的意见不能达成一致，是很难将事情正确完成的。想要使他们的意见达成一致，就是共识问题了，人们通过沟通来达成共识，计算机节点之间通过交换信息来达成共识，本质上都是一样的。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:34:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"事务的本质是什么 在本专栏中，我们特别用四节课的时间做了一个“事务”系列课程，主要有两方面原因。一方面为了说明在分布式系统中，事务占有非常重要的位置，另一方面是为了让你学习到与分布式事务相关的技术原理。但是，这些知识都是从外向内来解释事务是什么，会让我们感觉到分布式事务涉及的技术原理非常繁多，但是正因为有了这些知识的铺垫，现在我们就可以从更深的维度去探讨事务，让分布式事务变得更加简单和清晰了。那么接下来，我们就来探讨一个问题，事务的本质是什么？ 首先，我们简单回忆一下事务的隔离级别：读未提交 (Read Uncommitted)、读已提交 (Read Committed)、可重复读 (Repeatable Read)、快照隔离级别 (Snapshot Isolation) 和串行化 (Serializable) ，从隔离级别的名称和异常情况中，我们都不难发现，隔离级别都是从读异常情况的角度来定义的（其中，脏写和写倾斜也可以看成是，由于脏读和幻读导致的写异常），那么这是为什么呢？其实这是由于事务面对的数据存储，是单副本数据或线性一致的多副本，单个写操作完成后，读操作都是可以立即读取到的，所以在单个写操作的层面，事务是不会出现异常情况的。但是，**由于事务一般都涉及对多个数据对象的读写操作，为了避免并发事务的相互影响，事务需要将还未提交的写操作结果，与其他并发事务进行隔离处理，**那么如何实现隔离呢？ 既然写操作已经实际发生了，那就只能通过读操作进行隔离了，即将一个事务时间内多个离散的写操作，通过对读操作在并发事务之间隔离的方式，使事务的多个操作对外表现为一个原子操作一样。接着，我们再来梳理一下数据一致性的模型。从课程第 26 讲“数据一致性都有哪些级别”的定义与讨论中，我们不难看出线性一致性、顺序一致性、因果一致性和最终一致性，这四种线性一致性模型讨论的都是，对单个数据对象操作时，单节点或多节点的多个写操作的顺序，以及复制时延的问题。在数据一致性的模型中，读异常都是由于对单个数据对象的写操作，在多个副本之间的不同原子同步导致的。 到这里，我们会发现事务和数据一致性是非常类似的，它们本质上都是期望它的一个完整操作是原子操作，研究的本质问题都是数据的一致性问题。只不过事务对一个完整操作的定义是，一个事务内，对一个或多个数据对象的一个或多个读写操作，它需要解决的是对多个数据对象操作的一致性问题；而数据一致性对一个完整操作的定义是，在多个数据副本上对一个数据对象的写操作，它要解决的是单个数据操作，复制到多个副本上的一致性问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:34:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"“一致性与共识”系列小结 到这里，“一致性与共识”系列课程就结束了，为了让你对这部分知识有一个整体的把握，以及充分的理解，接下来我们分别从一致性、共识以及它们之间的关系出发，做一个小结。首先，数据的一致性模型定义了，一个数据对象在多个节点上有多个副本时，对外部读写表现出来的现象。数据一致性模型从强至弱分别为：线性一致性、顺序一致性、因果一致性和最终一致性。其中线性一致性是我们目前可以实现的一致性最强的模型，对于线性一致性的数据复制模型，我们可以认为它和操作单副本是一样的结果，基于它搭建的数据系统一般都是 CP 系统。 而一致性级别最弱的最终一致性，它只能确保数据最终会一致，并不能明确这个时间有多长。最终一致性牺牲了数据一定程度上的正确性，换取了高性能和高可用，在高并发的互联网场景中经常被使用，基于它搭建的数据系统一般都是 AP 系统。其次，共识是指多个节点（进程）对某一个事情达成一致的结果，一个完备的共识算法需要满足四个要求：一致同意、诚实性、合法性和可终止性。共识算法主要用于解决 Leader 选举和分布式锁服务等分布式场景中，最底层、最基础的问题，所以基于 Leader 的线性一致性算法，通常都需要依赖共识算法来实现选举。最后，通过讨论共识与分布式事务之间的关系，我们发现在事务的原子性、隔离性和持久性的实现中，都可以看到共识的身影，并且当我们对事务与数据的一致性进行比较后，发现事务是多个数据操作的一致性问题，而数据一致性则可以理解为，对多个副本的单个数据对象的事务问题。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:34:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课中，我们先讨论了事务与共识的关系，发现它们之间有着非常密切的关系，世界的尽头在哪里我不知道，但是我可以明确地告诉你，分布式的尽头就是共识。然后，我们通过分析分布式事务，并且与数据一致性做对比，发现事务可以理解为对多个数据操作的一致性问题，这样我们对分布式事务的理解就又多了一个维度。其实深入理解事务，是学习好分布式存储的基石，也会为你以后的工作打下一个坚实的基础。最后，我们对“一致性与共识”系列课程进行了总结和梳理，相信你对于这些知识已经有了非常深入和系统的理解，恭喜你，在学习分布式系统的道路上，跨过了“一致性与共识”这一道坎。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:34:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结篇 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:35:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"29｜分布式计算技术的发展史：从单进程服务到 Service Mesh ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:36:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"史前期 在分布式系统的史前期，最简单的形式是单进程系统：整个系统只有一个进程，并且运行在一个节点上。单进程系统是非常符合我们直觉的分布式系统的史前期形式，除此之外，还有一种情况也可以归类为分布式系统的史前期，下面我们接着来讨论一下。对于服务端系统来说，高可用和高性能是无法回避的两个要求，而要达到这两个要求，最简单的方式就是通过多副本来实现：将单进程的程序复制到多台机器上，然后通过负载均衡将流量分发至多台机器上。但是在这个系统中，多个副本的进程之间是不需要任何通信的，彼此之间也不会感知对方的存在，并且在架构层面，你会发现单进程系统和简单复制的多副本系统，它们都是单体架构，差异只在部署方式上。从严格定义来说，我们可以将这个系统称为集群，但它不是分布式系统。所以在本课中，我们将单体架构的系统定义为史前期，史前期的时间大约从有计算机程序开始，到 1990 年代之前。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:36:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"探索期与萌芽期 在史前期，人们通过对单体架构的系统进行集群化部署，解决了业务对高可用和高性能的需求，但是在互联网公司快速发展的过程中，单体架构逐渐在成本和效率方面，暴露出了很多的问题，这部分内容，我们在第 4 讲课程“注册发现”中详细讨论过，这里就不再重复了。于是，在 1990 年代左右，人们开始探索一种新的架构——分布式业务系统架构，来解决这个问题。在探索的过程中，许许多多的科学家和工程师都贡献了自己的聪明才智，在 1990 年代为分布式业务系统打下了坚实的理论基础，特别是 1996 年 Gartner 公司提出了 SOA 的概念。基于上述讨论，我认为分布式业务系统架构的探索期为 1990 年度，在这一时期，主要是对 SOA 架构进行探索。到了 2002 年， Gartner 公司正式推出了 SOA 概念，从此单体架构快速向 SOA 架构迁移，所以在课程中，我们将 2000 年代定义为分布式业务系统架构的萌芽期。相比于史前期的单体架构来说，探索与萌芽期的 SOA 架构有如下的特点： 单体架构所有的逻辑都在一个进程中，而 SOA 架构要求面向服务对业务的逻辑进行拆分。被拆分的多个服务，需要通过 ESB 进行通信。 从此，单体架构慢慢退出了历史的舞台，面向服务进行拆分则变成了一个理所当然的常识。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:36:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"爆发期 SOA 架构推广后，越来越多的人和公司开始使用 SOA 架构，在使用的过程中，服务的粒度慢慢变得更细，并且慢慢倾向于让不同的服务之间直接通信，而不需要借助 ESB 这样中心化的组件。终于在 2014 年， Martin Fowler 和 James Lewis 在 SOA 的基础上，提出了微服务的架构。它相比于 SOA 架构来说，具体的差异如下。 服务的粒度拆分得更细，更加强调一个服务只做一个事情，并且做到最好。去中心化，服务之间的通信不走 ESB 这样中心化的组件，而是由服务之间直接通信。更加强调复用性，服务化和组件化更加彻底。微服务架构更强调数据是服务私有的，其他服务不能直接访问服务的私有数据，只能通过服务提供的接口来获取。 通过上面的描述，我们可以看到微服务架构比 SOA 架构更加复杂，一个微服务中少则几百个服务，多则上千或更多的服务，所以通过人工运维一个微服务是低效并且不可能的，从此服务治理就开始变成了微服务的标配。关于服务治理相关的技术原理，在本专栏的 “分布式计算”中有非常详细的讨论，这里就不再重复了。基于上述讨论，我认为分布式业务系统架构的爆发期为 2010 - 2015 年度，在这一时期， SOA 架构逐步被微服务架构所取代，分布式业务系统的架构开始进入微服务时代。 2016 年，开源 Spring Cloud 就是微服务架构的一个经典实现。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:36:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"云原生期 微服务架构由于在工程上的成本和效率方面，能满足互联网公司快速迭代的需求，所以很快便风靡起来。但是，从架构上来看，微服务的框架层（比如服务注册发现、熔断降级和负载均衡等）是以 SDK 的形式集成在服务代码中的，而框架层的 SDK 和服务的业务代码，在公司中通常都是由两个团队来开发和维护的：框架层的 SDK 由基础架构团队来开发和维护，业务代码由业务研发团队来开发和维护。 而服务的发布权限在服务的 Owner 业务研发手中，这就使基础架构团队和业务研发团队在程序发布的时候耦合了，基础架构团队想上线新的功能，只能去和业务研发团队沟通，可是业务研发团队的目标在业务上，就导致两个需要紧密协作的团队，出现目标不一致的情况，这是非常影响工作效率的。所以，在 2016 年 Buoyant 公司提出了 Service Mesh 架构，它在微服务的基础上，做了下面的架构设计优化。 不再基于机器进行架构设计，而是直接在云原生基础设施 K8S 的基础上进行架构，这样能直接利用云的弹性能力。将微服务的框架层拆分出来，以一个 Sidecar 的形式，独立部署在服务运行的节点上，通过这个方式来解耦基础架构团队和业务研发团队。最终目标是将微服务的框架层所做的服务治理相关的功能，都抽象到 Sidecar 上，通过 Sidecar 建立云原生时代的 Service Mesh 。 另外，我们将 Service Mesh 定位为云原生时代的 TCP / IP 协议，为了帮助你更好地理解，下面我们就对 TCP / IP 和 Service Mesh 进行一个比较。 首先，是路由能力层面。 TCP / IP 协议在发送数据的时候，通过路由协议，利用网络唯一标识 IP 地址找到所属的计算机节点，而 Service Mesh 通过服务注册发现机制，利用服务的唯一标识来找到服务实例的 IP 列表。 其次，是控制能力层面。 TCP / IP 协议通过慢启动、拥塞控制等一系列的手段，确保网络能够正常运行，而 Service Mesh 通过服务治理中的熔断、降级和限流等机制，确保整个分布式系统正常运行。 通过上面对 TCP / IP 和 Service Mesh 的比较，你会发现它们虽然做的事情不一样，工作的层次不相同，但是工作原理是一样的：TCP / IP 负责将数据包通过网络发送给指定 IP 的主机， Service Mesh 负责将请求通过网络发送给指定的服务，并且它们都会进行流量控制，关心整个网络运行的效率。 对于分布式业务系统架构的云原生期，我认为是从 2015 年 - 至今。在这一时期， Service Mesh 从概念刚刚出现，发展到许多的公司都开始在生产环境中使用，并且出现了许多优秀的开源框架，比如 2016 年 Buoyant 的 Linkerd 和 Lyft 的 Envoy ，2017 年由 IBM 、 Google 和 Lyft 共同推出的 Istio 。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:36:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课中，我们讨论了分布式业务系统的演进历史，现在我们一起来总结一下。首先，是 1990 年代以前的史前期，这个时期主要的架构形式是单体架构，为了高可用和高性能，部署形式为集群部署。然后，是 1990 年代的探索期，为了解决单体架构在研发效率和成本方面的不足，人们开始对分布式系统进行探索，其中的标志性事件是 1996 年 Gartner 公司提出了 SOA 的概念。之后是 2000 年代的萌芽期，在这一时期，SOA 架构正式推出并且在工业界广泛实践。接着，是 2010 年代 - 2015 年代的爆发期，在这一时期， SOA 架构已经深入人心，同时在 2014 年，基于 SOA 架构进化的微服务架构，被 Martin Fowler 和 James Lewis 提出并推广。最后，是 2015 年代到现在的云原生期，在云原生期，人们希望微服务架构中的服务治理变成像 TCP / IP 协议一样的网络基础设施，其中的标志性事件是 2016 年 Buoyant 公司提出了 Service Mesh 架构。到这里，你会发现从历史的发展脉络中，我们可以看到未来的方向，你自然也就明白为什么 Service Mesh 是分布式业务系统中代表未来的架构了。同时，通过增加分布式业务系统中，时间维度的学习后，你对于单体架构、 SOA 、微服务和 Service Mesh 一定也有了更深刻的认识，也就知道如何选择适合公司业务特点的架构了。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:36:5","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"30｜分布式存储技术的发展史：从 ACID 到 NewSQL 通过上节课的学习，我们明白了分布式在线业务系统是如何一步步从单体架构、SOA、微服务到 Service Mesh 的，这对于帮助我们理解 Service Mesh 为什么被设计为现在这个样子，并且为什么 Service Mesh 是一种更好的架构，给出了一个清晰的结论。接下来，我们开始讨论本专栏另一个重点对象——分布式存储系统的演进历史。通过对这段历史的讨论和研究，从时间和历史的维度上，帮助你建立网状、立体的知识体系。这一节课，我们主要讨论分布式存储系统中，分布式在线数据库的演进历史：从 ACID 到 NewSQL。与分布式业务系统的演进历史一样，我们也将分布式在线数据库的演进历史，梳理为史前期、探索期、萌芽期、爆发期和云原生期，这 5 个阶段来讨论和总结。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:37:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"史前期与探索期 在 1990 年以前，互联网还没有被广泛使用，能连网的用户和设备非常有限，存储的数据量还在单机的承受范围之内。同时，由于 1970 年发布的 SQL 有表达能力强、面向集合和声明式等优良设计，所以在数据库中被广泛地使用，使得当时的在线数据库主要为单机的关系数据库，其中最著名的是 1979 年甲骨文发布的 Oracle 和 1983 年 IBM 发布的 DB2。所以，我们可以认为 1990 年以前为分布式在线数据库的史前期。单机数据库非常大的一个优点是提供了一个非常完美的抽象，即 ACID 事务，让业务层可以专心去处理业务逻辑。关于事务，我们在第 22- 25 讲“事务”系列课程中非常详细地讨论过，这里就不再重复了。但是，随着互联网的快速发展，用户量快速增长，单机数据库在存储容量和并发性能方面面临非常大的挑战，于是人们开始探索新的解决方案。一种方案是从业务层面来解决单机数据库的问题，具体有如下两个操作方式。一是，将业务垂直拆分为不同的逻辑单元，然后将不同逻辑单元上的数据库表，按数据容量和并发量等规则拆分到不同的数据库实例上。这种方法虽然可以大大扩展单机数据库的容量和性能，但是如果单个数据库表的数据非常大，那么分库就无法解决了。二是，为了解决单表数据量非常大的问题，在业务上从一个表中，选择某一个字段为分片键，将其水平拆分为多个子表，这样每一个子表负责原表的一部分数据存储和读写。这种方法可以从更小的粒度对数据库进行扩容，但是对于非分片键的查询等操作是非常麻烦的。对于这种方案，多个数据库实例之间并不需要相互感知，分库分表都是由业务来进行处理，所以只能称为单机数据库的集群模式，不能称之为分布式数据库。另一种方案是从数据库层面解决单机数据库面临的问题，通过将数据库扩展为一个分布式数据库提升存储容量和性能，并且对业务来说，它依然和使用单机数据库一样使用分布式数据库。正常来说，分布式数据库的解决方案需要提供和单机数据库一样的 ACID 事务，但是在分布式数据库中，数据被分片存储到数据库的多个节点上，事务操作不能在一个节点上完成，需要支持跨节点的分布式事务。而这对于当时的计算机工程与理论水平来说，是一个非常大的挑战，于是在 1990 年代，工业界和学术界都进行了深入探索和实践，其中最有影响力的成果如下。 首先是 1990 年，著名的分布式理论科学家 Leslie Lamport 提出了 Paxos 算法。Paxos 是一个可以容错的共识算法，后来为分布式存储技术的发展提供了底层的共识基础，但是，在当时这是一颗被遗弃的明珠，并没有受到人们的重视。接着是 1997 年的 BASE 理论和 2000 年的 CAP 理论，这两个理论直接将分布式数据库的发展推向了另一条道路：NoSQL ，在线数据库为了水平扩展能力而放弃了 ACID 事务。关于 BASE 理论和 CAP 理论，在课程第 3 讲“CAP 理论”中有详细的讨论，这里就不再重复了。我们可以看出，1990 年代是分布式数据库的理论探索期，在这期间，Leslie Lamport 提出了一个可以容错的共识算法 Paxos 算法，这个算法是后来 NewSQL 的理论基础。而同在这一时期的 BASE 理论和 CAP 理论，则提供了另外的一个新选择，放弃 ACID 事务，选择了 NoSQL。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:37:1","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"萌芽期 通过 1990 年理论上的探索后，BASE 理论和 CAP 理论深入人心，当时人们通过它们确定了分布式数据库的理论边界，于是放弃 ACID 事务的 NoSQL 数据库一时大放光彩。所以在 2000 年代这一时期，出现了非常多而且优秀的 NoSQL 数据库，下面我们来介绍几个著名的数据库。首先是 2006 年，Google 发表了论文 “Bigtable: A Distributed Storage System for Structured Data” ，在这篇论文中，Google 对外分享了公司内部的分布式存储系统 Bigtable 的实现原理。Bigtable 在设计上有一个妥协，即只支持单行事务，不支持跨行事务。接着在 2007 年，AWS 发表了论文“Dynamo: Amazon’s Highly Available Key-value Store”。这篇论文中，AWS 陈述了它们发现自己的很多业务场景，比如购入车场景，对数据库的关系型能力需求并不频繁，大约 70% 的操作都是键 - 值类操作，即仅使用一个主键，返回一个单行数据；大约 20% 的操作会返回一组行数据，但是也仍然位于单个表上，所以 AWS 重新设计了一个 Key-value 数据库 Dynamo。同时由于 AWS 业务规模巨大，对系统的可扩展性和可用性有非常高的要求，所以 AWS 特别在论文中指出“可靠性是我们最重要的需求之一，因为即使是最微小的故障也会造成巨大的经济损失，而且会降低客户对我们的信任。” 基于上面的设计目标，Dynamo 采用了无主复制的数据复制策略，并且通过 Quorum 机制让业务根据自身的特点在读性能、写性能和可用性之间达成平衡。关于无主复制和 Quorum 机制，在课程第 21 讲“无主复制”中有详细的讨论，这里就不再重复了。总而言之，Dynamo 整体是一个非常优秀的技术方案，2008 年 Facebook 推出的 Cassandra 是 Dynamo 的一个开源实现。后来在 2009 年，10gen 公司（后改名为 MongoDB Inc ）推出了 MongoDB，它是一个文档型的数据库，简单来说是一个 Schemaless，数据即文档的数据库。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:37:2","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"爆发期与云原生期 在 2010 年代，由于互联网公司数据的快速增长，人们接受了 BASE 理论和 CAP 理论，在数据库的架构设计方面，为了水平扩展能力和高可用性，放弃了数据的一致性，也就是在 CAP 里面选择了 AP 模型。可是我们在课程第 28 讲“共识与事务”中讨论过，要实现事务，底层多副本数据的复制模型必须是线性一致性的，所以 NoSQL 选择了 AP 模型，也就相当于放弃了事务。在课程第 22 讲“事务的一致性”中我们已经详细讨论过，对于业务逻辑来说，事务提供了原子性、隔离性、持久性和一致性，这是一个非常好的抽象，所以工程师们非常希望自己使用的数据库是支持事务的。如果不支持事务的话，工程师为了保障业务逻辑的正确性，需要自己在业务逻辑层实现事务本身应该提供的保障。比如，由于 Google 的 Bigtable 数据库只支持单行事务，不支持跨行事务，而业务中的跨行事务是很正常的逻辑，所以在 Google 里面使用 Bigtable 的工程师们，就只能在 Bigtable 之上构建自己的事务，这个过程是非常浪费时间并且很容易出现错误的。 而且在这一时期，人们对于分布式理论的认识、存储硬件和工程经验方面都有了长足的发展。在理论层面，Google 在 2006 年发布了分布式锁 Chubby 的论文 “The Chubby Lock Service for Loosely-Coupled Distributed Systems”，在论文中可以看到，人们已经充分认识到共识算法 Paxos 对于构建分布式系统的重要性了。在硬件层面，SSD 磁盘已经普及，随机读写能力几乎高出 SATA 磁盘 3 个数量级。在工程经验层面，工程师们在 2000 年代就构建了大量的 NoSQL 系统，已经积累了关于构建一个分布式存储系统的丰富经验。在这样的背景下，Google 于 2012 年发布了 Spanner 的论文 “Spanner: Google’s Globally-Distributed Database”，于 2013 年发布了 F1 的论文“F1: A Distributed SQL Database That Scales”，这两篇论文介绍了一个 Google 内部开发的支持外部一致性（External Consistency）的全球分布式关系数据库，直接宣告数据库行业进入了 NewSQL 时代。 Google 发布论文，开源界进行跟进是最近多年的一个规律，于是在 2015 年，开源界陆续推出了 NewSQL 数据库 CockroachDB 和 TiDB，所以我们认为 2010 年 - 2015 年为分布式数据库 NewSQL 的爆发时期。从 2015 年开始，DBaaS （ DB as a Service ）的趋势越来越明显，据 AWS 的数据显示，用户在 2019 年迁移到 AWS 云上数据库的数量，超过了 2015 年到 2018 年的总数。DBaaS 也给分布式数据库提出了新的要求：分布式数据库需要能利用云的弹性等能力，来动态扩展自己的服务能力。从此，开启了分布式数据库的云原生时代。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:37:3","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"总结 本节课中，我们讨论了分布式在线数据库的演进历史，现在一起来总结一下。首先，是 1990 年以前的史前期，这个时期主要的架构形式是支持 ACID 事务的单机数据库。然后，在 1990 年代的探索期，由于互联网的快速发展，单机数据库在存储容量和性能方面都面临非常大的挑战，所以人们开始探索新的解决方案：分库分表的集群方案和分布式数据库。接着，是 2000 年代的萌芽期，人们接受了 BASE 理论和 CAP 理论，为了应对互联网的海量数据，人们为了水平扩展能力而放弃了 ACID 事务，这一时期出现了大量的 NoSQL 数据库。由于业务层对事务的需求非常强烈，并且人们在工程能力和理论水平方面都在不断进步，所以在 2010 年 - 2015 年，分布式支持 ACID 事务的 NewSQL 数据库诞生，从此进入了 NewSQL 时代。最后，是 2015 年 - 至今的云原生时代，这个时期最鲜明的特点是 DBaaS 和分布式数据库，能够利用云的弹性能力进行动态扩展。到这里，我们可以看到，技术的发展是曲折前进的，在 1990 年以前的单机关系数据库就支持了 ACID 事务，但是到了 2000 年代，由于当时理论和工程水平的原因，许多 NoSQL 数据库为了水平扩展能力，而放弃了 ACID 事务，这就是一个非常大的权衡。后来随着技术的发展，在 NewSQL 时代，既支持水平扩展，又支持 ACID 事务的分布式数据库终于出现了。所以，在技术的发展过程中，没有完美的架构，只有完美的 trade-off，取舍永远是最关键的因素。 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:37:4","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"分布式数据库30讲 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:38:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"基础 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:39:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"开发 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:40:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"实践 ","date":"2022-07-20 16:24:55","objectID":"/ds_geek/:41:0","tags":["distributed system"],"title":"DS_geek","uri":"/ds_geek/"},{"categories":["School courses"],"content":"编译原理实战课 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:0:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"开篇词 | 在真实世界的编译器中游历 宫文学 2020-06-01 你好，我是宫文学，一名技术创业者，现在是北京物演科技的 CEO，很高兴在这里跟你见面。我在 IT 领域里已经工作有 20 多年了。这其中，我个人比较感兴趣的，也是倾注时间精力最多的，是做基础平台类的软件，比如国内最早一批的 BPM 平台、BI 平台，以及低代码 / 无代码开发平台（那时还没有这个名字）等。这些软件之所以会被称为平台，很重要的原因就是拥有很强的定制能力，比如流程定制、界面定制、业务逻辑定制，等等。而这些定制能力，依托的就是编译技术。 在前几年，我参与了一些与微服务有关的项目。我发现，前些年大家普遍关注那些技术问题，比如有状态的服务（Stateful Service）的横向扩展问题，在云原生、Serverless、FaaS 等新技术满天飞的时代，不但没能被很好地解决，反而更恶化了。究其原因就是，状态管理还是被简单地交给数据库，而云计算的场景使得数据库的压力更大了，数据库原来在性能和扩展能力上的短板，就更加显著了。 而比较好的解决思路之一，就是大胆采用新的计算范式，发明新的计算机语言，所以我也有意想自己动手搞一下。 我从去年开始做设计，已经鼓捣了一阵了，采用了一些很前卫的理念，比如云原生的并发调度、基于 Actor 的数据管理等。总的目标，是要让开发云原生的、有状态的应用，像开发一个简单的单机应用一样容易。那我们就最好能把云架构和状态管理的细节给抽象掉，从而极大地降低成本、减少错误。而为编程提供更高的抽象层次，从来就是编译技术的职责。 Serverless 和 FaaS 已经把无状态服务的架构细节透明掉了。但针对有状态的服务，目前还没有答案。对我而言，这是个有趣的课题。 在我比较熟悉的企业应用领域，ERP 的鼻祖 SAP、SaaS 的鼻祖 SalesForce，都用自己的语言开发应用，很可惜国内的企业软件厂商还没有做到这一点。而在云计算时代，设计这样一门语言绕不过去的一个问题，就是解决有状态服务的云化问题。我希望能为解决这个问题提供一个新工具。当然，这个工具必须是开源的。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:1:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"为什么要解析真实编译器？ 第一，研究这些语言的编译机制，能直接提高我们的技术水平。一方面，深入了解自己使用的语言的编译器，会有助于你吃透这门语言的核心特性，更好地运用它，从而让自己向着专家级别的工程师进军。举个例子，国内某互联网公司的员工，就曾经向 Oracle 公司提交了 HotSpot 的高质量补丁，因为他们在工作中发现了 JVM 编译器的一些不足。那么，你是不是也有可能把一门语言吃得这么透呢？另一方面，IT 技术的进化速度是很快的，作为技术人，我们需要迅速跟上技术更迭的速度。而这些现代语言的编译器，往往就是整合了最前沿的技术。比如，Java 的 JIT 编译器和 JavaScript 的 V8 编译器，它们都不约而同地采用了“Sea of Nodes”的 IR 来做优化，这是为什么呢？这种 IR 有什么优势呢？这些问题我们都需要迅速弄清楚。 第二，阅读语言编译器的源码，是高效学习编译原理的重要路径。传统上，我们学习编译原理，总是要先学一大堆的理论和算法，理解起来非常困难，让人望而生畏。这个方法本身没有错，因为我们学习任何知识，都要掌握其中的原理。不过，这样可能离实现一款实用的编译器还有相当的距离。那么根据我的经验，学习编译原理的一个有效途径，就是阅读真实世界中编译器的源代码，跟踪它的执行过程，弄懂它的运行机制。因为只要你会写程序，就能读懂代码。既然能读懂代码，那为什么不直接去阅读编译器的源代码呢？在开源的时代，源代码就是一个巨大的知识宝库。面对这个宝库，我们为什么不进去尽情搜刮呢？想带走多少就带走多少，没人拦着。 当然，你可能会犯嘀咕：编译器的代码一般都比较难吧？以我的水平，能看懂吗？是会有这个问题。当我们面对一大堆代码的时候，很容易迷路，抓不住其中的重点和核心逻辑。不过没关系，有我呢。在本课程中，我会给你带路，并把地图准备好，带你走完这次探险之旅。而当你确实把握了编译器的脉络以后，你对自己的技术自信心会提升一大截。这些计算机语言，就被你摘掉了神秘的面纱。俗话说“读万卷书，行万里路”。如果说了解编译原理的基础理论和算法是读书的过程，那么探索真实世界里的编译器是什么样子，就是行路的过程了。根据我的体会，当你真正了解了身边的语言的编译器是怎样编写的之后，那些抽象的理论就会变得生动和具体，你也就会在编译技术领域里往前跨出一大步了。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:1:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"我们可以解析哪些语言的编译器？ 那你可能要问了，在本课程中，我都选择了哪些语言的编译器呢？选择这些编译器的原因又是什么呢？ 这次，我要带你解析的编译器还真不少，包括了 Java 编译器（javac）、Java 的 JIT 编译器（Graal）、Python 编译器（CPython）、JavaScript 编译器（V8）、Julia 语言的编译器、Go 语言的编译器（gc），以及 MySQL 的编译器，并且在讲并行的时候，还涉及了 Erlang 的编译器。 我选择剖析这些语言的编译器，有三方面的原因：第一，它们足够有代表性，是你在平时很可能会用到的。这些语言中，除了 Julia 比较小众外，都比较流行。而且，虽然 Julia 没那么有名，但它使用的 LLVM 工具很重要。因为 LLVM 为 Swift、Rust、C++、C 等多种语言提供了优化和后端的支持，所以 Julia 也不缺乏代表性。第二，它们采用了各种不同的编译技术。这些编译器，有的是编译静态类型的语言，有的是动态类型的语言；有的是即时编译（JIT），有的是提前编译（AOT）；有高级语言，也有 DSL（SQL）；解释执行的话，有的是用栈机（Stack Machine），有的是用寄存器机，等等。不同的语言特性，就导致了编译器采用的技术会存在各种差异，从而更加有利于你开阔视野。第三，通过研究多种编译器，你可以多次迭代对编译器的认知过程，并通过分析对比，发现这些编译器之间的异同点，探究其中的原因，激发出更多的思考，从而得到更全面的、更深入的认知。 看到这里，你可能会有所疑虑：有些语言我没用过，不怎么了解，怎么办？其实没关系。因为现代的高级语言，其实相似度很高。一方面，对于不熟悉的语言，虽然你不能熟练地用它们来做项目，但是写一些基本的、试验性的程序，研究它的实现机制，是没有什么问题的。另一方面，学习编译原理的人会练就一项基本功，那就是更容易掌握一门语言的本质。特别是我这一季的课程，就是要帮你成为钻到了铁扇公主肚子里的孙悟空。研究某一种语言的编译器，当然有助于你通过“捷径”去深入地理解它。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:1:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"我是如何规划课程模块的？ 这门课程的目标，是要让你对现代语言的编译器的结构、所采用的算法以及设计上的权衡，都获得比较真切的认识。其最终结果是，如果要你使用编译技术来完成一个项目，你会心里非常有数，知道应该在什么地方使用什么技术。因为你不仅懂得原理，更有很多实际编译器的设计和实现的思路作为你的决策依据。为了达到本课程的目标，我仔细规划了课程的内容，将其划分为预备知识篇、真实编译器解析篇和现代语言设计篇三部分。 在预备知识篇，我会简明扼要地帮你重温一下编译原理的知识体系，让你对这些关键概念的理解变得更清晰。磨刀不误砍柴工，你学完预备知识篇后，再去看各种语言编译器的源代码和相关文档时，至少不会被各种名词、术语搞晕，也能更好地建立具体实现跟原理之间的关联，能互相印证它们。在真实编译器解析篇，我会带你研究语言编译器的源代码，跟踪它们的运行过程，分析编译过程的每一步是如何实现的，并对有特点的编译技术点加以分析和点评。这样，我们在研究了 Java、Java JIT、Python、JavaScript、Julia、Go、MySQL 这 7 个编译器以后，就相当于把编译原理印证了 7 遍。在现代语言设计篇，我会带你分析和总结前面已经研究过的编译器，进一步提升你对相关编译技术的认知高度。学完这一模块以后，你对于如何设计编译器的前端、中端、后端、运行时，都会有比较全面的了解，知道如何在不同的技术路线之间做取舍。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:1:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"01 | 编译的全过程都悄悄做了哪些事情？ 编译，其实就是把源代码变成目标代码的过程。如果源代码编译后要在操作系统上运行，那目标代码就是汇编代码，我们再通过汇编和链接的过程形成可执行文件，然后通过加载器加载到操作系统里执行。如果编译后是在解释器里执行，那目标代码就可以不是汇编代码，而是一种解释器可以理解的中间形式的代码即可。我举一个很简单的例子。这里有一段 C 语言的程序，我们一起来看看它的编译过程。 int foo(int a){ int b = a + 3; return b; } 这段源代码，如果把它编译成汇编代码，大致是下面这个样子： .section __TEXT,__text,regular,pure_instructions .globl _foo ## -- Begin function foo _foo: ## @foo pushq %rbp movq %rsp, %rbp movl %edi, -4(%rbp) movl -4(%rbp), %eax addl $3, %eax movl %eax, -8(%rbp) movl -8(%rbp), %eax popq %rbp retq 你可以看出，源代码和目标代码之间的差异还是很大的。那么，我们怎么实现这个翻译呢？其实，编译和把英语翻译成汉语的大逻辑是一样的。前提是你要懂这两门语言，这样你看到一篇英语文章，在脑子里理解以后，就可以把它翻译成汉语。编译器也是一样，你首先需要让编译器理解源代码的意思，然后再把它翻译成另一种语言。表面上看，好像从英语到汉语，一下子就能翻译过去。但实际上，大脑一瞬间做了很多个步骤的处理，包括识别一个个单词，理解语法结构，然后弄明白它的意思。同样，编译器翻译源代码，也需要经过多个处理步骤，如下图所示。 我来解释一下各个步骤。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:2:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"词法分析（Lexical Analysis） 首先，编译器要读入源代码。在编译之前，源代码只是一长串字符而已，这显然不利于编译器理解程序的含义。所以，编译的第一步，就是要像读文章一样，先把里面的单词和标点符号识别出来。程序里面的单词叫做 Token，它可以分成关键字、标识符、字面量、操作符号等多个种类。把字符串转换为 Token 的这个过程，就叫做词法分析。 图 2：把字符串转换为 Token（注意：其中的空白字符，代表空格、tab、回车和换行符，EOF 是文件结束符） ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:2:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"语法分析（Syntactic Analysis） 识别出 Token 以后，离编译器明白源代码的含义仍然有很长一段距离。下一步，我们需要让编译器像理解自然语言一样，理解它的语法结构。这就是第二步，语法分析。上语文课的时候，老师都会让你给一个句子划分语法结构。比如说：“我喜欢又聪明又勇敢的你”，它的语法结构可以表示成下面这样的树状结构。 图 3：把一个句子变成语法树 那么在编译器里，语法分析阶段也会把 Token 串，转换成一个体现语法规则的、树状的数据结构，这个数据结构叫做抽象语法树（AST，Abstract Syntax Tree）。我们前面的示例程序转换为 AST 以后，大概是下面这个样子： 图 4：foo 函数对应的语法树 这样的一棵 AST 反映了示例程序的语法结构。比如说，我们知道一个函数的定义包括了返回值类型、函数名称、0 到多个参数和函数体等。这棵抽象语法树的顶部就是一个函数节点，它包含了四个子节点，刚好反映了函数的语法。再进一步，函数体里面还可以包含多个语句，如变量声明语句、返回语句，它们构成了函数体的子节点。然后，每个语句又可以进一步分解，直到叶子节点，就不可再分解了。而叶子节点，就是词法分析阶段生成的 Token（图中带边框的节点）。对这棵 AST 做深度优先的遍历，你就能依次得到原来的 Token。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:2:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"语义分析（Semantic Analysis） 生成 AST 以后，程序的语法结构就很清晰了，编译工作往前迈进了一大步。但这棵树到底代表了什么意思，我们目前仍然不能完全确定。比如说，表达式“a+3”在计算机程序里的完整含义是：“获取变量 a 的值，把它跟字面量 3 的值相加，得到最终结果。”但我们目前只得到了这么一棵树，完全没有上面这么丰富的含义。 图 5：a+3 对应的 AST 这就好比西方的儿童，很小的时候就能够给大人读报纸。因为他们懂得发音规则，能念出单词来（词法分析），也基本理解语法结构（他们不见得懂主谓宾这样的术语，但是凭经验已经知道句子有不同的组成部分），可以读得抑扬顿挫（语法分析），但是他们不懂报纸里说的是什么，也就是不懂语义。这就是编译器解读源代码的下一步工作，语义分析。 那么，怎样理解源代码的语义呢？实际上，语言的设计者在定义类似“a+3”中加号这个操作符的时候，是给它规定了一些语义的，就是要把加号两边的数字相加。你在阅读某门语言的标准时，也会看到其中有很多篇幅是在做语义规定。在 ECMAScript（也就是 JavaScript）标准 2020 版中，Semantic 这个词出现了 657 次。下图是其中加法操作的语义规则，它对于如何计算左节点、右节点的值，如何进行类型转换等，都有规定。 图 6：ECMAScript 标准中加法操作的语义规则 所以，我们可以在每个 AST 节点上附加一些语义规则，让它能反映语言设计者的本意。add 节点：把两个子节点的值相加，作为自己的值；变量节点（在等号右边的话）：取出变量的值；数字字面量节点：返回这个字面量代表的值。 这样的话，如果你深度遍历 AST，并执行每个节点附带的语义规则，就可以得到 a+3 的值。这意味着，我们正确地理解了这个表达式的含义。运用相同的方法，我们也就能够理解一个句子的含义、一个函数的含义，乃至整段源代码的含义。这也就是说，AST 加上这些语义规则，就能完整地反映源代码的含义。这个时候，你就可以做很多事情了。比如，你可以深度优先地遍历 AST，并且一边遍历，一边执行语法规则。那么这个遍历过程，就是解释执行代码的过程。你相当于写了一个基于 AST 的解释器。不过在此之前，编译器还要做点语义分析工作。那么这里的语义分析是要解决什么问题呢？给你举个例子，如果我把示例程序稍微变换一下，加一个全局变量的声明，这个全局变量也叫 a。那你觉得“a+3”中的变量 a 指的是哪个变量？ int a = 10; //全局变量 int foo(int a){ //参数里有另一个变量a int b = a + 3; //这里的a指的是哪一个？ return b; } 我们知道，编译程序要根据 C 语言在作用域方面的语义规则，识别出“a+3”中的 a，所以这里指的其实是函数参数中的 a，而不是全局变量的 a。这样的话，我们在计算“a+3”的时候才能取到正确的值。而把“a+3”中的 a，跟正确的变量定义关联的过程，就叫做引用消解（Resolve）。这个时候，变量 a 的语义才算是清晰了。变量有点像自然语言里的代词，比如说，“我喜欢又聪明又勇敢的你”中的“我”和“你”，指的是谁呢？如果这句话前面有两句话，“我是春娇，你是志明”，那这句话的意思就比较清楚了，是“春娇喜欢又聪明又勇敢的志明”。引用消解需要在上下文中查找某个标识符的定义与引用的关系，所以我们现在可以回答前面的问题了，语义分析的重要特点，就是做上下文相关的分析。 在语义分析阶段，编译器还会识别出数据的类型。比如，在计算“a+3”的时候，我们必须知道 a 和 3 的类型是什么。因为即使同样是加法运算，对于整型和浮点型数据，其计算方法也是不一样的。语义分析获得的一些信息（引用消解信息、类型信息等），会附加到 AST 上。这样的 AST 叫做带有标注信息的 AST（Annotated AST/Decorated AST），用于更全面地反映源代码的含义。 图 7：带有标注信息的 AST 好了，前面我所说的，都是如何让编译器更好地理解程序的语义。不过在语义分析阶段，编译器还要做很多语义方面的检查工作。在自然语言里，我们可以很容易写出一个句子，它在语法上是正确的，但语义上是错误的。比如，“小猫喝水”这句话，它在语法和语义上都是对的；而“水喝小猫”这句话，语法是对的，语义上则是不对的。计算机程序也会存在很多类似的语义错误的情况。比如说，对于“int b = a+3”的这个语句，语义规则要求，等号右边的表达式必须返回一个整型的数据（或者能够自动转换成整型的数据），否则就跟变量 b 的类型不兼容。如果右边的表达式“a+3”的计算结果是浮点型的，就违背了语义规则，就要报错。总结起来，在语义分析阶段，编译器会做语义理解和语义检查这两方面的工作。词法分析、语法分析和语义分析，统称编译器的前端，它完成的是对源代码的理解工作。 做完语义分析以后，接下来编译器要做什么呢？本质上，编译器这时可以直接生成目标代码，因为编译器已经完全理解了程序的含义，并把它表示成了带有语义信息的 AST、符号表等数据结构。生成目标代码的工作，叫做后端工作。做这项工作有一个前提，就是编译器需要懂得目标语言，也就是懂得目标语言的词法、语法和语义，这样才能保证翻译的准确性。这是显而易见的，只懂英语，不懂汉语，是不可能做英译汉的。通常来说，目标代码指的是汇编代码，它是汇编器（Assembler）所能理解的语言，跟机器码有直接的对应关系。汇编器能够将汇编代码转换成机器码。熟练掌握汇编代码对于初学者来说会有一定的难度。但更麻烦的是，对于不同架构的 CPU，还需要生成不同的汇编代码，这使得我们的工作量更大。所以，我们通常要在这个时候增加一个环节：先翻译成中间代码（Intermediate Representation，IR）。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:2:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"中间代码（Intermediate Representation） 中间代码（IR），是处于源代码和目标代码之间的一种表示形式。我们倾向于使用 IR 有两个原因。第一个原因，是很多解释型的语言，可以直接执行 IR，比如 Python 和 Java。这样的话，编译器生成 IR 以后就完成任务了，没有必要生成最终的汇编代码。第二个原因更加重要。我们生成代码的时候，需要做大量的优化工作。而很多优化工作没有必要基于汇编代码来做，而是可以基于 IR，用统一的算法来完成。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:2:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"优化（Optimization） 那为什么需要做优化工作呢？这里又有两大类的原因。第一个原因，是源语言和目标语言有差异。源语言的设计目的是方便人类表达和理解，而目标语言是为了让机器理解。在源语言里很复杂的一件事情，到了目标语言里，有可能很简单地就表达出来了。比如“I want to hold your hand and with you I will grow old.” 这句话挺长的吧？用了 13 个单词，但它实际上是诗经里的“执子之手，与子偕老”对应的英文。这样看来，还是中国文言文承载信息的效率更高。同样的情况在编程语言里也有。以 Java 为例，我们经常为某个类定义属性，然后再定义获取或修改这些属性的方法： Class Person{ private String name; public String getName(){ return name; } public void setName(String newName){ this.name = newName } } 如果你在程序里用“person.getName()”来获取 Person 的 name 字段，会是一个开销很大的操作，因为它涉及函数调用。在汇编代码里，实现一次函数调用会做下面这一大堆事情： #调用者的代码 保存寄存器1 #保存现有寄存器的值到内存 保存寄存器2 ... 保存寄存器n 把返回地址入栈 把person对象的地址写入寄存器，作为参数 跳转到getName函数的入口 #_getName 程序 在person对象的地址基础上，添加一个偏移量，得到name字段的地址 从该地址获取值，放到一个用于保存返回值的寄存器 跳转到返回地 你看了这段伪代码，就会发现，简单的一个 getName() 方法，开销真的很大。保存和恢复寄存器的值、保存和读取返回地址，等等，这些操作会涉及好几次读写内存的操作，要花费大量的时钟周期。但这个逻辑其实是可以简化的。怎样简化呢？就是跳过方法的调用。我们直接根据对象的地址计算出 name 属性的地址，然后直接从内存取值就行。这样优化之后，性能会提高好多倍。这种优化方法就叫做内联（inlining），也就是把原来程序中的函数调用去掉，把函数内的逻辑直接嵌入函数调用者的代码中。在 Java 语言里，这种属性读写的代码非常多。所以，Java 的 JIT 编译器（把字节码编译成本地代码）很重要的工作就是实现内联优化，这会让整体系统的性能提高很大的一个百分比！总结起来，我们在把源代码翻译成目标代码的过程中，没有必要“直译”，而是可以“意译”。这样我们完成相同的工作，对资源的消耗会更少。 第二个需要优化工作的原因，是程序员写的代码不是最优的，而编译器会帮你做纠正。比如下面这段代码中的 bar() 函数，里面就有多个地方可以优化。甚至，整个对 bar() 函数的调用，也可以省略，因为 bar() 的值一定是 101。这些优化工作都可以在编译期间完成。 int bar(){ int a = 10*10; //这里在编译时可以直接计算出100这个值，这叫做“常数折叠” int b = 20; //这个变量没有用到，可以在代码中删除，这叫做“死代码删除” if (a\u003e0){ //因为a一定大于0，所以判断条件和else语句都可以去掉 return a+1; //这里可以在编译器就计算出是101 } else{ return a-1; } } int a = bar(); //这里可以直接换成 a=101 综上所述，在生成目标代码之前，需要做的优化工作可以有很多，这通常也是编译器在运行时，花费时间最长的一个部分。 图 8：多个前端和多个后端，可以采用统一的 IR 而采用中间代码来编写优化算法的好处，是可以把大部分的优化算法，写成与具体 CPU 架构无关的形式，从而大大降低编译器适配不同 CPU 的工作量。并且，如果采用像 LLVM 这样的工具，我们还可以让多种语言的前端生成相同的中间代码，这样就可以复用中端和后端的程序了。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:2:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"生成目标代码 编译器最后一个阶段的工作，是生成高效率的目标代码，也就是汇编代码。这个阶段，编译器也有几个重要的工作。第一，是要选择合适的指令，生成性能最高的代码。第二，是要优化寄存器的分配，让频繁访问的变量（比如循环变量）放到寄存器里，因为访问寄存器要比访问内存快 100 倍左右。第三，是在不改变运行结果的情况下，对指令做重新排序，从而充分运用 CPU 内部的多个功能部件的并行计算能力。目标代码生成以后，整个编译过程就完成了。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:2:6","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 本讲我从头到尾概要地讲解了编译的过程，希望你能了解每一个阶段存在的原因（Why），以及要完成的主要任务（What）。编译是一个比较复杂的过程，但如果我们能够分而治之，那么每一步的挑战就会降低很多。这样最后针对每个子任务，我们就都能找到解决的办法。我希望这一讲能帮你在大脑里建立起一个概要的地图。在后面几讲中，我会对编译过程的各个环节展开讨论，让你有越来越清晰的理解。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:2:7","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"02 | 词法分析：用两种方式构造有限自动机 上一讲，我带你把整个编译过程走了一遍。这样，你就知道了编译过程的整体步骤，每一步是做什么的，以及为什么要这么做。进一步地，你就可以研究一下每个环节具体是如何实现的、有哪些难点、有哪些理论和算法。通过这个过程，你不仅可以了解每个环节的原理，还能熟悉一些专有词汇。这样一来，你在读编译原理领域的相关资料时，就会更加顺畅了。不过，编译过程中涉及的算法和原理有些枯燥，所以我会用尽量通俗、直观的方式来给你解读，让你更容易接受。本讲，我主要跟你讨论一下词法分析（Lexical Analysis）这个环节。通过这节课，你可以掌握词法分析这个阶段是如何把字符串识别成一个个 Token 的。进而，你还会学到如何实现一个正则表达式工具，从而实现任意的词法解析。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:3:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"词法分析的原理 首先，我们来了解一下词法分析的原理。通过上一讲，你已经很熟悉词法分析的任务了：输入的是字符串，输出的是 Token 串。所以，词法分析器在英文中一般叫做 Tokenizer。 图 1：把字符串转换为 Token（注意：其中的空白字符，代表空格、tab、回车和换行符，EOF 是文件结束符） 但具体如何实现呢？这里要有一个计算模型，叫做有限自动机（Finite-state Automaton，FSA），或者叫做有限状态自动机（Finite-state Machine，FSM）。有限自动机这个名字，听上去可能比较陌生。但大多数程序员，肯定都接触过另一个词：状态机。假设你要做一个电商系统，那么订单状态的迁移，就是一个状态机。 图 2：状态机的例子（订单的状态和迁移过程） 有限自动机就是这样的状态机，它的状态数量是有限的。当它收到一个新字符的时候，会导致状态的迁移。比如说，下面的这个状态机能够区分标识符和数字字面量。 图 3：一个能够识别标识符和数字字面量的有限自动机 在这样一个状态机里，我用单线圆圈表示临时状态，双线圆圈表示接受状态。接受状态就是一个合格的 Token，比如图 3 中的状态 1（数字字面量）和状态 2（标识符）。当这两个状态遇到空白字符的时候，就可以记下一个 Token，并回到初始态（状态 0），开始识别其他 Token。可以看出，词法分析的过程，其实就是对一个字符串进行模式匹配的过程。说起字符串的模式匹配，你能想到什么工具吗？对的，正则表达式工具。 大多数语言，以及一些操作系统的命令，都带有正则表达式工具，来帮助你匹配合适的字符串。比如下面的这个 Linux 命令，可以用来匹配所有包含“sa”“sb” … “sh”字符串的进程。 ps -ef | grep 's[a-h]' 在这个命令里，“s[a-h]”是用来描述匹配规则的，我们把它叫做一个正则表达式。同样地，正则表达式也可以用来描述词法规则。这种描述方法，我们叫做正则文法（Regular Grammar）。比如，数字字面量和标识符的正则文法描述是这样的： IntLiteral : [0-9]+; //至少有一个数字 Id : [A-Za-z][A-Za-z0-9]*; //以字母开头，后面可以是字符或数字 与普通的正则表达式工具不同的是，词法分析器要用到很多个词法规则，每个词法规则都采用“Token 类型: 正则表达式”这样一种格式，用于匹配一种 Token。然而，当我们采用了多条词法规则的时候，有可能会出现词法规则冲突的情况。比如说，int 关键字其实也是符合标识符的词法规则的。 Int : int; //int关键字 For : for; //for关键字 Id : [A-Za-z][A-Za-z0-9]*; //以字母开头，后面可以是字符或数字 所以，词法规则里面要有优先级，比如排在前面的词法规则优先级更高。这样的话，我们就能够设计出区分 int 关键字和标识符的有限自动机了，可以画成下面的样子。其中，状态 1、2 和 3 都是标识符，而状态 4 则是 int 关键字。 图 4：一个能够识别 int 关键字和标识符的有限自动机 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:3:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"从正则表达式生成有限自动机 现在，你已经了解了如何构造有限自动机，以及如何处理词法规则的冲突。基本上，你就可以按照上面的思路来手写词法分析器了。但你可能觉得，这样手写词法分析器的步骤太繁琐了，我们能否只写出词法规则，就自动生成相对应的有限自动机呢？当然是可以的，实际上，正则表达式工具就是这么做的。此外，词法分析器生成工具 lex（及 GNU 版本的 flex）也能够基于规则自动生成词法分析器。它的具体实现思路是这样的：把一个正则表达式翻译成 NFA，然后把 NFA 转换成 DFA。对不起，我这里又引入了两个新的术语：NFA 和 DFA。先说说 DFA，它是“Deterministic Finite Automaton”的缩写，即确定的有限自动机。它的特点是：该状态机在任何一个状态，基于输入的字符，都能做一个确定的状态转换。前面例子中的有限自动机，都属于 DFA。再说说 NFA，它是“Nondeterministic Finite Automaton”的缩写，即不确定的有限自动机。它的特点是：该状态机中存在某些状态，针对某些输入，不能做一个确定的转换。 这又细分成两种情况：对于一个输入，它有两个状态可以转换。存在ε转换的情况，也就是没有任何字符输入的情况下，NFA 也可以从一个状态迁移到另一个状态。 比如，“a[a-zA-Z0-9]*bc”这个正则表达式，对字符串的要求是以 a 开头，以 bc 结尾，a 和 bc 之间可以有任意多个字母或数字。可以看到，在图 5 中，状态 1 的节点输入 b 时，这个状态是有两条路径可以选择的：一条是迁移到状态 2，另一条是仍然保持在状态 1。所以，这个有限自动机是一个 NFA。 图 5：一个 NFA 的例子，识别“a[a-zA-Z0-9]*bc”的自动机 这个 NFA 还有引入ε转换的画法，如图 6 所示，它跟图 5 的画法是等价的。实际上，图 6 表示的 NFA 可以用我们下面马上要讲到的算法，通过正则表达式自动生成出来。 图 6：另一个 NFA 的例子，同样能识别“a[a-zA-Z0-9]*bc”，其中有ε转换 需要注意的是，无论是 NFA 还是 DFA，都等价于正则表达式。也就是说，所有的正则表达式都能转换成 NFA 或 DFA；而所有的 NFA 或 DFA，也都能转换成正则表达式。 理解了 NFA 和 DFA 以后，接下来我再大致说一下算法。首先，一个正则表达式可以机械地翻译成一个 NFA。它的翻译方法如下： 识别字符 i 的 NFA。当接受字符 i 的时候，引发一个转换，状态图的边上标注 i。其中，第一个状态（i，initial）是初始状态，第二个状态 (f，final) 是接受状态。 图 7：识别 i 的 NFA 转换“s|t”这样的正则表达式。它的意思是，或者 s，或者 t，二者选一。s 和 t 本身是两个子表达式，我们可以增加两个新的状态：开始状态和接受状态。然后，用ε转换分别连接代表 s 和 t 的子图。它的含义也比较直观，要么走上面这条路径，那就是 s，要么走下面这条路径，那就是 t： 转换“st”这样的正则表达式。s 之后接着出现 t，转换规则是把 s 的开始状态变成 st 整体的开始状态，把 t 的结束状态变成 st 整体的结束状态，并且把 s 的结束状态和 t 的开始状态合二为一。这样就把两个子图衔接了起来，走完 s 接着走 t。 图 9：识别 st 的 NFA 对于“?”“”和“+”这样的符号，它们的意思是可以重复 0 次、0 到多次、1 到多次，转换时要增加额外的状态和边。以“s”为例，我们可以做下面的转换： 图 10：识别 s* 的 NFA 你能看出，它可以从 i 直接到 f，也就是对 s 匹配 0 次，也可以在 s 的起止节点上循环多次。如果是“s+”，那就没有办法跳过 s，s 至少要经过一次： 图 11：识别 s+ 的 NFA 通过这样的转换，所有的正则表达式，都可以转换为一个 NFA。基于 NFA，你仍然可以实现一个词法分析器，只不过算法会跟基于 DFA 的不同：当某个状态存在一条以上的转换路径的时候，你要先尝试其中的一条；如果匹配不上，再退回来，尝试其他路径。这种试探不成功再退回来的过程，叫做回溯（Backtracking）。小提示：下一讲的递归下降算法里，也会出现回溯现象，你可以对照着理解。 基于 NFA，你也可以写一个正则表达式工具。实际上，我在示例程序中已经写了一个简单的正则表达式工具，使用了Regex.java中的 regexToNFA 方法。如下所示，我用了一个测试用的正则表达式，它能识别 int 关键字、标识符和数字字面量。在示例程序中，这个正则表达式首先被表示为一个内部的树状数据结构，然后可以转换成 NFA。 int | [a-zA-Z][a-zA-Z0-9]* | [0-9]* 示例程序也会将生成的 NFA 打印输出，下面的输出结果中列出了所有的状态，以及每个状态到其他状态的转换，比如“0 ε -\u003e 2”的意思是从状态 0 通过 ε 转换，到达状态 2 ： NFA states: 0 ε -\u003e 2 ε -\u003e 8 ε -\u003e 14 2 i -\u003e 3 3 n -\u003e 5 5 t -\u003e 7 7 ε -\u003e 1 1 (end) acceptable 8 [a-z]|[A-Z] -\u003e 9 9 ε -\u003e 10 ε -\u003e 13 10 [0-9]|[a-z]|[A-Z] -\u003e 11 11 ε -\u003e 10 ε -\u003e 13 13 ε -\u003e 1 14 [0-9] -\u003e 15 15 ε -\u003e 14 ε -\u003e 1 我用图片来直观展示一下输出结果，分为上、中、下三条路径，你能清晰地看出解析 int 关键字、标识符和数字字面量的过程： 图 12：由算法自动生成的 NFA 那么生成 NFA 之后，我们要如何利用它，来识别某个字符串是否符合这个 NFA 代表的正则表达式呢？还是以图 12 为例，当我们解析“intA”这个字符串时，首先选择最上面的路径进行匹配，匹配完 int 这三个字符以后，来到状态 7，若后面没有其他字符，就可以到达接受状态 1，返回匹配成功的信息。 可实际上，int 后面是有 A 的，所以第一条路径匹配失败。失败之后不能直接返回“匹配失败”的结果，因为还有其他路径，所以我们要回溯到状态 0，去尝试第二条路径，在第二条路径中，我们尝试成功了。运行 Regex.java 中的 matchWithNFA() 方法，你可以用 NFA 来做正则表达式的匹配。其中，在匹配“intA”时，你会看到它的回溯过程： NFA matching: 'intA' trying state : 0, index =0 trying state : 2, index =0 //先走第一条路径，即int关键字这个路径 trying state : 3, index =1 trying state : 5, index =2 trying state : 7, index =3 trying state : 1, index =3 //到了末尾，发现还有字符'A'没有匹配上 trying state : 8, index =0 //回溯，尝试第二条路径，即标识符 trying state : 9, index =1 trying state : 10, index =1 //在10和11这里循环多次 trying state : 11, index =2 trying state : 10, index =2 trying state : 11, index =3 trying state : 10, index =3 true 从中你可以看到用 NFA 算法的特点：因为存在多条可能的路径，所以需要试探和回溯，在比较极端的情况下，回溯次数会非常多，性能会变得非常差。特别是当处理类似“s*”这样的语句时，因为 s 可以重复 0 到无穷次，所以在匹配字符串时，可能需要尝试很多次。NFA 的运行可能导致大量的回溯，那么能否将 NFA 转换成 DFA，让字符串的匹配过程更简单呢？如果能的话，那整个过程都可以自动化，从正则表达式到 NFA，再从 NFA 到 DFA。方法是有的，这个算法就是子集构造法。不过我这里就不展开介绍了，如果你想继续深入学习的话，可以去看看本讲最后给出的参考资料。总之，只要有了准确的正则表达式，是可以根据算法自动生成对字符串进行匹配的程序的，这就是正则表达式工具的基本原理，也是有些工具（比如 ANTLR 和 flex）能够自动给你生成一个词法分析器的原理。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:3:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 本讲涵盖了词法分析所涉及的主要知识点。词法分析跟你日常使用的正则表达式关系很密切，你可以用正则表达式来表示词法规则。在实际的编译器中，词法分析器一般都是手写的，依据的基本原理就是构造有限自动机。不过有一些地方也会用手工编码的方式做一些优化（如 javac 编译器），有些编译器会做用一些特别的技巧来提升解析速度（如 JavaScript 的 V8 编译器），你在后面的课程中会看到。基于正则表达式构造 NFA，再去进行模式匹配，是一个很好的算法思路，它不仅仅可以用于做词法分析，其实还可以用于解决其他问题（比如做语法分析），值得你去做举一反三的思考。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:3:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"03 | 语法分析：两个基本功和两种算法思路 通过第 1 讲的学习，现在你已经清楚了语法分析阶段的任务：依据语法规则，把 Token 串转化成 AST。今天，我就带你来掌握语法分析阶段的核心知识点，也就是两个基本功和两种算法思路。理解了这些重要的知识点，对于语法分析，你就不是外行了。 两个基本功：第一，必须能够阅读和书写语法规则，也就是掌握上下文无关文法；第二，必须要掌握递归下降算法。两种算法思路：一种是自顶向下的语法分析，另一种则是自底向上的语法分析。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:4:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"上下文无关文法（Context-Free Grammar） 在开始语法分析之前，我们要解决的第一个问题，就是如何表达语法规则。在上一讲中，你已经了解了，我们可以用正则表达式来表达词法规则，语法规则其实也差不多。我还是以下面这个示例程序为例，里面用到了变量声明语句、加法表达式，我们看看语法规则应该怎么写： int a = 2; int b = a + 3; return b; 第一种写法是下面这个样子，它看起来跟上一讲的词法规则差不多，都是左边是规则名称，右边是正则表达式。 start：blockStmts ; //起始 block : '{' blockStmts '}' ; //语句块 blockStmts : stmt* ; //语句块中的语句 stmt = varDecl | expStmt | returnStmt | block; //语句 varDecl : type Id varInitializer？ ';' ; //变量声明 type : Int | Long ; //类型 varInitializer : '=' exp ; //变量初始化 expStmt : exp ';' ; //表达式语句 returnStmt : Return exp ';' ; //return语句 exp : add ; //表达式 add : add '+' mul | mul; //加法表达式 mul : mul '*' pri | pri; //乘法表达式 pri : IntLiteral | Id | '(' exp ')' ; //基础表达式 在语法规则里，我们把冒号左边的叫做非终结符（Non-terminal），又叫变元（Variable）。非终结符可以按照右边的正则表达式来逐步展开，直到最后都变成标识符、字面量、运算符这些不可再展开的符号，也就是终结符（Terminal）。终结符其实也是词法分析过程中形成的 Token。 提示： 在本课程，非终结符以小写字母开头，终结符则以大写字母开头，或者是一个原始的字符串格式。 在谈论语法分析的时候，我们可以把 Token 和终结符这两个术语互换使用。 像这样左边是非终结符，右边是正则表达式的书写语法规则的方式，就叫做扩展巴科斯范式（EBNF）。你在 ANTLR 这样的语法分析器生成工具中，经常会看到这种格式的语法规则。 对于 EBNF 的严格定义，你可以去参考Wikipedia上的解释。 在教科书中，我们还经常采用另一种写法，就是产生式（Production Rule），又叫做替换规则（Substitution Rule）。产生式的左边是非终结符（变元），它可以用右边的部分替代，中间通常会用箭头连接。 为了避免跟 EBNF 中的“*”号、“+”号等冲突，在本节课中，凡是采用 EBNF 格式，就给字符串格式的终结符加引号，左右两边用“::=”或冒号分隔开；凡是采用产生式，字符串就不加引号，并且采用“-\u003e”分隔产生式的左右两侧。 add -\u003e add + mul add -\u003e mul mul -\u003e mul * pri mul -\u003e pri 也有个偷懒的写法，就是把同一个变元的多个产生式写在一起，用竖线分隔（但这时候，如果产生式里面原本就要用到“|”终结符，那么就要加引号来进行区分）。但也就仅此为止了，不会再引入“*”和“+”等符号，否则就成了 EBNF 了。 add -\u003e add + mul | mul mul -\u003e mul * pri | pri 产生式不用“ * ”和“+”来表示重复，而是用迭代，并引入“ε”（空字符串）。所以“blockStmts : stmt*”可以写成下面这个样子： blockStmts -\u003e stmt blockStmts | ε 总结起来，语法规则是由 4 个部分组成的：一个有穷的非终结符（或变元）的集合；一个有穷的终结符的集合；一个有穷的产生式集合；一个起始非终结符（变元）。 那么符合这四个特点的文法规则，就叫做上下文无关文法（Context-Free Grammar，CFG）。你可能会问，上下文无关文法和词法分析中用到的正则文法是否有一定的关系？是的，正则文法是上下文无关文法的一个子集。其实，正则文法也可以写成产生式的格式。比如，数字字面量（正则表达式为“[0-9]+”）可以写成： IntLiteral -\u003e Digit IntLiteral1 IntLiteral1 -\u003e Digit IntLiteral1 IntLiteral1 -\u003e ε Digit -\u003e [0-9] 但是，在上下文无关文法里，产生式的右边可以放置任意的终结符和非终结符，而正则文法只是其中的一个子集，叫做线性文法（Linear Grammar）。它的特点是产生式的右边部分最多只有一个非终结符，比如 X-\u003eaYb，其中 a 和 b 是终结符。 图 1：正则文法是上下文无关文法的子集 你可以试一下，把上一讲用到的正则表达式“a[a-zA-Z0-9]*bc”写成产生式的格式，它就符合线性文法的特点。 S0 -\u003e aS1bc S1 -\u003e [a-zA-Z0-9]S1 S1 -\u003e ε 但对于常见的语法规则来说，正则文法是不够的。比如，你最常用的算术表达式的规则，就没法用正则文法表示，因为有的产生式需要包含两个非终结符（如“add + mul”）。你可以试试看，能把“2+3”“2+3*5”“2+3+4+5”等各种可能的算术表达式，用一个正则表达式写出来吗？实际是不可能的。 add -\u003e add + mul add -\u003e mul mul -\u003e mul * pri mul -\u003e pri 好，现在你已经了解了上下文无关文法，以及它与正则文法的区别。可是，为什么它会叫“上下文无关文法”这样一个奇怪的名字呢？难道还有上下文相关的文法吗？答案的确是有的。举个例子来说，在高级语言里，本地变量必须先声明，才能在后面使用。这种制约关系就是上下文相关的。不过，在语法分析阶段，我们一般不管上下文之间的依赖关系，这样能使得语法分析的任务更简单。而对于上下文相关的情况，则放到语义分析阶段再去处理。好了，现在你已经知道，用上下文无关文法可以描述程序的语法结构。学习编译原理，阅读和书写语法规则是一项基本功。针对高级语言中的各种语句，你要都能够手写出它们的语法规则来才可以。接下来，我们就要依据语法规则，编写语法分析程序，把 Token 串转化成 AST。语法分析的算法有很多，但有一个算法也是你必须掌握的一项基本功，这就是递归下降算法。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:4:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"递归下降算法（Recursive Descent Parsing） 递归下降算法其实很简单，它的基本思路就是按照语法规则去匹配 Token 串。比如说，变量声明语句的规则如下： varDecl : types Id varInitializer？ ';' ; //变量声明 varInitializer : '=' exp ; //变量初始化 exp : add ; //表达式 add : add '+' mul | mul; //加法表达式 mul : mul '*' pri | pri; //乘法表达式 pri : IntLiteral | Id | '(' exp ')' ; //基础表达式 如果写成产生式格式，是下面这样： varDecl -\u003e types Id varInitializer ';' varInitializer -\u003e '=' exp varInitializer -\u003e ε exp -\u003e add add -\u003e add + mul add -\u003e mul mul -\u003e mul * pri mul -\u003e pri pri -\u003e IntLiteral pri -\u003e Id pri -\u003e ( exp ) 而基于这个规则做解析的算法如下： 匹配一个数据类型(types) 匹配一个标识符(Id)，作为变量名称 匹配初始化部分(varInitializer)，而这会导致下降一层，使用一个新的语法规则： 匹配一个等号 匹配一个表达式(在这个步骤会导致多层下降：exp-\u003eadd-\u003emul-\u003epri-\u003eIntLiteral) 创建一个varInitializer对应的AST节点并返回 如果没有成功地匹配初始化部分，则回溯，匹配ε，也就是没有初始化部分。 匹配一个分号 创建一个varDecl对应的AST节点并返回 用上述算法解析“int a = 2”，就会生成下面的 AST： 图 2：“int a = 2”对应的 AST 那么总结起来，递归下降算法的特点是：对于一个非终结符，要从左到右依次匹配其产生式中的每个项，包括非终结符和终结符。在匹配产生式右边的非终结符时，要下降一层，继续匹配该非终结符的产生式。如果一个语法规则有多个可选的产生式，那么只要有一个产生式匹配成功就行。如果一个产生式匹配不成功，那就回退回来，尝试另一个产生式。这种回退过程，叫做回溯（Backtracking）。 所以说，递归下降算法是非常容易理解的。它能非常有效地处理很多语法规则，但是它也有两个缺点。第一个缺点，就是著名的左递归（Left Recursion）问题。比如，在匹配算术表达式时，产生式的第一项就是一个非终结符 add，那么按照算法，要下降一层，继续匹配 add。这个过程会一直持续下去，无限递归下去。 add -\u003e add + mul 所以，递归下降算法是无法处理左递归问题的。那么有什么解决办法吗？你可能会说，把产生式改成右递归不就可以了吗？也就是 add 这个递归项在右边： add -\u003e mul + add 这样确实可以避免左递归问题，但它同时也会导致结合性的问题。举个例子来说，我们按照上面的语法规则来解析“2+3+4”这个表达式，会形成如下所示的 AST。 图 3：结合性错误的 AST 它会先计算“3+4”，而不是先计算“2+3”。这破坏了加法的结合性规则，加法运算本来应该是左结合的。其实有一个标准的方法，能避免左递归问题。我们可以改写原来的语法规则，也就是引入add'，把左递归变成右递归： add -\u003e mul add' add' -\u003e + mul add' | ε 接下来，我们用刚刚改写的规则再次解析一下 “2+3+4”这个表达式，会得到下图中的 AST： 图 4：基于改写后的文法所生成的 AST 你能看出，这种改写方法虽然能够避免左递归问题，但由于add’的规则是右递归的，采用标准的递归下降算法，仍然会出现运算符结合性的错误。那么针对这点，我们有没有解决办法呢？有的，方法就是把递归调用转化成循环。这里利用了很多同学都知道的一个原理，即递归调用可以转化为循环。其实我把上面的规则换成用 EBNF 方式来表达就很清楚了。在 EBNF 格式里，允许用“*”号和“+”号表示重复： add ： mul ('+' mul)* ； 所以说，对于('+‘mul)*这部分，我们其实可以写成一个循环。而在循环里，我们可以根据结合性的要求，手工生成正确的 AST。它的伪代码如下： 左子节点 = 匹配一个mul while(下一个Token是+){ 消化掉+ 右子节点 = 匹配一个mul 用左、右子节点创建一个add节点 左子节点 = 该add节点 } 采用上面的算法，就可以创建正确的 AST，如下图所示： 图 5：结合性正确的 AST 递归下降算法的第二个缺点，就是当产生式匹配失败的时候，必须要“回溯”，这就可能导致浪费。 这个时候，我们有个针对性的解决办法，就是预读后续的一个 Token，判断该选择哪个产生式。以 stmt 变元为例，考虑它的三个产生式，分别是变量声明语句、表达式语句和 return 语句。那么在递归下降算法中，我们可以在这里预读一个 Token，看看能否根据这个 Token 来选择某个产生式。经过仔细观察，你发现如果预读的 Token 是 Int 或 Long，就选择变量声明语句；如果是 IntLiteral、Id 或左括号，就选择表达式语句；而如果是 Return，则肯定是选择 return 语句。因为这三个语句开头的 Token 是不重叠的，所以你可以很明确地做出选择。如果我们手写递归下降算法，可以用肉眼识别出每次应该基于哪个 Token，选择用哪个产生式。但是，对于一些比较复杂的语法规则，我们要去看好几层规则，这样比较辛苦。那么能否有一个算法，来自动计算出选择不同产生式的依据呢？当然是有的，这就是 LL 算法家族。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:4:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"LL 算法：计算 First 和 Follow 集合 LL 算法的要点，就是计算 First 和 Follow 集合。First 集合是每个产生式开头可能会出现的 Token 的集合。就像 stmt 有三个产生式，它的 First 集合如下表所示。 而 stmt 的 First 集合，就是三个产生式的 First 集合的并集，也是 Int Long IntLiteral Id ( Return。总体来说，针对非终结符 x，它的 First 集合的计算规则是这样的： 如果产生式以终结符开头，那么把这个终结符加入 First(x)；如果产生式以非终结符 y 开头，那么把 First(y) 加入 First(x);如果 First(y) 包含ε，那要把下一个项的 First 集合也加入进来，以此类推；如果 x 有多个产生式，那么 First(x) 是每个产生式的并集。 在计算 First 集合的时候，具体可以采用“不动点法”。相关细节这里就不展开了，你可以参考示例程序FirstFollowSet类的 CalcFirstSets() 方法，运行示例程序能打印各个非终结符的 First 集合。 不过，这样是不是就万事大吉了呢？其实还有一种特殊情况我们需要考虑，那就是对于某个非终结符，它自身会产生ε的情况。比如说，示例文法中的 blockStmts，它是可能产生ε的，也就是块中一个语句都没有。 block : '{' blockStmts '}' ; //语句块 blockStmts : stmt* ; //语句块中的语句 stmt = varDecl | expStmt | returnStmt; //语句 语法解析器在这个时候预读的下一个 Token 是什么呢？是右花括号。这证明 blockStmts 产生了ε，所以才读到了后续跟着的花括号。对于某个非终结符后面可能跟着的 Token 的集合，我们叫做 Follow 集合。如果预读到的 Token 在 Follow 中，那么我们就可以判断当前正在匹配的这个非终结符，产生了ε。 Follow 的算法也比较简单，以非终结符 x 为例： 扫描语法规则，看看 x 后面都可能跟着哪些符号；对于后面跟着的终结符，都加到 Follow(x) 集合中去；如果后面是非终结符 y，就把 First(y) 加 Follow(x) 集合中去；最后，如果 First(y) 中包含ε，就继续往后找；如果 x 可能出现在程序结尾，那么要把程序的终结符 $ 加入到 Follow(x) 中去。 这样在计算了 First 和 Follow 集合之后，你就可以通过预读一个 Token，来完全确定采用哪个产生式。这种算法，就叫做 LL(1) 算法。LL(1) 中的第一个 L，是 Left-to-right 的缩写，代表从左向右处理 Token 串。第二个 L，是 Leftmost 的缩写，意思是最左推导。最左推导是什么呢？就是它总是先把产生式中最左侧的非终结符展开完毕以后，再去展开下一个。这也就相当于对 AST 从左子节点开始的深度优先遍历。LL(1) 中的 1，指的是预读一个 Token。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:4:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"LR 算法：移进和规约 前面讲的递归下降和 LL 算法，都是自顶向下的算法。还有一类算法，是自底向上的，其中的代表就是 LR 算法。自顶向下的算法，是从根节点逐层往下分解，形成最后的 AST；而 LR 算法的原理呢，则是从底下先拼凑出 AST 的一些局部拼图，并逐步组装成一棵完整的 AST。所以，其中的关键之处在于如何“拼凑”。假设我们采用下面的上下文无关文法，来推演一个实例，具体语法规则如下所示： start-\u003eadd add-\u003eadd+mul add-\u003emul mul-\u003emul*pri mul-\u003epri pri-\u003eInt pri-\u003e(add) 如果用于解析“2+3*5”，最终会形成下面的 AST： 图 6：2+3*5 对应的 AST 那算法是怎么从底部凑出这棵 AST 来的呢？LR 算法和 LL 算法一样，也是从左到右地消化掉 Token。在第 1 步，它会取出“2”这个 Token，放到一个栈里，这个栈是用来组装 AST 的工作区。同时，它还会预读下一个 Token，也就是“+”号，用来帮助算法做判断。在下面的示意图里，我画了一条橙色竖线，竖线的左边是栈，右边是预读到的一个 Token。在做语法解析的过程中，竖线会不断地往右移动，把 Token 放到栈里，这个过程叫做“移进”（Shift）。 图 7：第 1 步，移进一个 Token 注意，我在图 7 中还用虚线框推测了 AST 的其他部分。也就是说，如果第一个 Token 遇到的是整型字面量，而后面跟着一个 + 号，那么这两个 Token 就决定了它们必然是这棵推测出来的 AST 的一部分。而图中右边就是它的推导过程，其中的每个步骤，都使用了一个产生式加了一个点（如“.add”）。这个点，就相当于图中左边的橙色竖线。所以你就可以根据这棵假想的 AST，也就是依据假想的推导过程，给它反推回去。把 Int 还原为 pri。这个还原过程，就叫做“规约”（Reduce）。工作区里的元素也随之更新成 pri。 图 8：第 2 步，Int 规约为 pri 按照这样的思路，不断地移进和规约，这棵 AST 中推测出来的节点会不断地被证实。而随着读入的 Token 越来越多，这棵 AST 也会长得越来越高，整棵树变得更大。下图是推导过程中间的一个步骤。 图 9：移进和规约过程中的一个步骤 最后，整个 AST 构造完毕，而工作区里也就只剩了一个 Start 节点。 图 10：最后一步，add 规约为 start 通过上面的介绍，你应该已经建立了对 LR 算法的直觉认识。如果要把这个推导过程写成严密的算法，你可以参考《编译原理之美》的第 18 讲。从示例中，你应该已经看出来了，相对于 LL 算法，LR 算法的优点是能够处理左递归文法。但它也有缺点，比如不利于输出全面的编译错误信息。因为在没有解析完毕之前，算法并不知道最后的 AST 是什么样子，所以也不清楚当前的语法错误在整体 AST 中的位置。最后我再提一下 LR 的意思，来帮你更完整地理解 LR 算法。L 还是代表从左到右读入 Token，而 R 是最右推导（Rightmost）的意思。我把“2+3*5”最右推导的过程写在了下面，而如果你从最后一行往前一步步地看，它恰好就是规约的过程。 如果你见到 LR(k)，那它的意思就是会预读 k 个 Token，我们在示例中采用的是 LR(1)。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:4:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 今天花了一讲的时间，把语法分析的要点给你讲解了一下。对于上下文无关的文法，你要知道产生式、非终结符、终结符、EBNF 这几个基本概念，能够熟练阅读各种语言的语法规则，这是一个基本功。递归下降算法是另一项基本功，所以也一定要掌握。你要注意，递归下降是深度优先的，只有最左边的子树都生成完了，才会往右生成它的兄弟节点。有的同学会在没有把左侧的非终结符匹配完毕的情况下，就开始匹配右边的项，从而不自觉地采用了宽度优先的思路，这是我发现很多同学会容易陷入的一个思维误区。对于 LL 算法和 LR 算法，我只做了简单的讲解，目的是为了帮助你建立直观的理解。我们在后面的课程中，还会遇到使用它们的实际例子，到时你可以与这一讲的内容相互印证。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:4:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"04 | 语义分析：让程序符合语义规则 对计算机程序语义的研究，是一个专门的学科。要想很简单地把它讲清楚，着实不是太容易的事情。但我们可以退而求其次，只要能直观地去理解什么是语义就可以了。语义，就是程序要表达的意思。因为计算机最终是用来做计算的，那么理解程序表达的意思，就是要知道让计算机去执行什么计算动作，这样才好翻译成目标代码。那具体来说，语义分析要做什么工作呢？我们在第 1 讲中说过，每门计算机语言的标准中，都会定义很多语义规则，比如对加法运算要执行哪些操作。而在语义分析阶段，就是去检查程序是否符合这些语义规则，并为后续的编译工作收集一些语义信息，比如类型信息。再具体一点，这些语义规则可以分为两大类。 第一类规则与上下文有关。因为我们说了，语法分析只能处理与上下文无关的工作。而与上下文有关的工作呢，自然就放到了语义分析阶段。第二类规则与类型有关。在计算机语言中，类型是语义的重要载体。所以，语义分析阶段要处理与类型有关的工作。比如，声明新类型、类型检查、类型推断等。在做类型分析的时候，我们会用到一个工具，就是属性计算，也是需要你了解和掌握的。 补充：某些与类型有关的处理工作，还必须到运行期才能去做。比如，在多态的情况，调用一个方法时，到底要采用哪个子类的实现，只有在运行时才会知道。这叫做动态绑定。 在语义分析过程中，会使用两个数据结构。一个还是 AST，但我们会把语义分析时获得的一些信息标注在 AST 上，形成带有标注的 AST。另一个是符号表，用来记录程序中声明的各种标识符，并用于后续各个编译阶段。那今天这一讲，我就会带你看看如何完成与上下文有关的分析、与类型有关的处理，并带你认识符号表和属性计算。首先，我们来学习如何处理与上下文有关的工作。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:5:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"上下文相关的分析 那什么是与上下文有关的工作呢？在解析一个程序时，会有非常多的分析工作要结合上下文来进行。接下来，我就以控制流检查、闭包分析和引用消解这三个场景和你具体分析下。 场景 1：控制流检查像 return、break 和 continue 等语句，都与程序的控制流有关，它们必须符合控制流方面的规则。在 Java 这样的语言中，语义规则会规定：如果返回值不是 void，那么在退出函数体之前，一定要执行一个 return 语句，那么就要检查所有的控制流分支，是否都以 return 语句结尾。 场景 2：闭包分析很多语言都支持闭包。而要正确地使用闭包，就必须在编译期知道哪些变量是自由变量。这里的自由变量是指在本函数外面定义的变量，但被这个函数中的代码所使用。这样，在运行期，编译器就会用特殊的内存管理机制来管理这些变量。所以，对闭包的分析，也是上下文敏感的。 场景 3：引用消解我们重点说一下引用消解，以及相关的作用域问题。引用消解（Reference Resolution），有时也被称作名称消解（Name Resolution）或者标签消解（Label Resolution）。对变量名称、常量名称、函数名称、类型名称、包名称等的消解，都属于引用消解。因此，引用消解是一种非常重要的上下文相关的语义规则，我来重点讲解下。 在高级语言里，我们会做变量、函数（或方法）和类型的声明，然后在其他地方使用它们。这个时候，我们要找到定义和使用之间的正确引用关系。我们来看一个例子。在语法分析阶段，对于“int b = a + 3”这样一条语句，无论 a 是否提前声明过，在语法上都是正确的。而在实际的计算机语言中，如果引用某个变量，这个变量就必须是已经声明过的。同时，当前这行代码，要处于变量 a 的作用域中才行。 图 1：变量引用的消解 对于变量来说，为了找到正确的引用，就需要用到作用域（Scope）这个概念。在编译技术里面，作用域这个词，有两个稍微有所差异的使用场景。作用域的第一个使用场景，指的是变量、函数等标识符可以起作用的范围。下图列出了三个变量的作用域，每个变量声明完毕以后，它的下一句就可以引用它。 图 2：变量的作用域 作用域的第二个使用场景，是词法作用域（Lexical Scope），也就是程序中的不同文本区域。比如，一个语句块、参数列表、类定义的主体、函数（方法）的主体、模块主体、整个程序等。到这里，咱们来总结下这两个使用场景。标识符和词法的作用域的差异在于：一个本地变量（标识符）的作用域，虽然属于某个词法作用域（如某个函数体），但其作用范围只是在变量声明之后的语句。而类的成员变量（标识符）的作用域，跟词法作用域是一致的，也就是整个类的范围，跟声明的位置无关。如果这个成员变量不是私有的，它的作用域还会覆盖到子类。那具体到不同的编程语言，它们的作用域规则是不同的。比如，C 语言里允许你在一个 if 语句块里定义一个变量，覆盖外部的变量，而 Java 语言就不允许这样。所以，在给 Java 做语义分析时，我们要检查出这种错误。 void foo(){ int a = 2; if (...){ int a = 3; //在C语言里允许，在Java里不允许 ... } } 在做引用消解的时候，为了更好地查找变量、类型等定义信息，编译器会使用一个辅助的数据结构：符号表。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:5:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"符号表（Symbol Table） 在写程序的时候，我们会定义很多标识符，比如常量名称、变量名称、函数名称、类名称，等等。在编译器里，我们又把这些标识符叫做符号（Symbol）。用来保存这些符号的数据结构，就叫做符号表。比如，对于变量 a 来说，符号表中的基本信息可以包括： 名称：a分类：变量类型：int作用域：foo 函数体其他必要的信息。 符号表的具体实现，每个编译器可能都不同。比如，它可能是一张线性的表格，也可能是按照作用域形成的一种有层次的表格。以下面这个程序为例，它包含了两个函数，每个函数里面都定义了多个变量： void foo(){ int a； int b； if (a\u003e0){ int c; int d; } else{ int e; int f; } } void bar(){ int g; { int h; int i; } } 它的符号表可能是下面这样的，分成了多个层次，每个层次对应了一个作用域。在全局作用域，符号表里包含 foo 和 bar 两个函数。在 foo 函数体里，有两个变量 a 和 b，还有两个内部块，每个块里各有两个变量。 图 3：一种层次化的符号表 那针对引用消解，其实就是从符号表里查找被引用的符号的定义，如下图所示： 图 4：利用符号表帮助做引用消解 更进一步地，符号表除了用于引用消解外，还可以辅助完成语义分析的其他工作。比如，在做类型检查的时候，我们可以从符号表里查找某个符号的类型，从而检查类型是否兼容。其实，不仅仅是在语义分析阶段会用到符号表，其他的编译阶段也会用到。比如，早在词法分析阶段，你就可以为符号表建立条目；在生成 IR、做优化和生成目标代码的时候，都会用到符号表里的信息。 图 5：编译过程中的每个阶段，都可能会使用符号表 有的编译器，在前期做语法分析的时候，如果不依赖符号表的话，它是不可能完整地做语法分析的。甚至，除了编译阶段，在链接阶段，我们也要用到符号表。比如，在 foo.c 中定义了一个函数 foo()，并编译成目标文件 foo.o，在 bar.c 中使用了这个 foo() 函数。那么在链接的时候，链接器需要找到 foo() 函数的地址。为了满足这个场景，你必须在目标文件中找到 foo 符号的相关信息。同样的道理，在 Java 的字节码文件里也需要保存符号信息，以便在加载后我们可以定位其中的类、方法和成员变量。好了，以上就是语义分析的第一项重要工作上下文相关的分析，以及涉及的数据结构符号表的重点内容了。我们再来考察一下语义分析中第二项重要的工作：类型分析和处理。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:5:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"类型分析和处理 语义分析阶段的一个重要工作就是做类型检查，现代语言还普遍增加了类型推断的能力。那什么是类型呢？通常来说，在计算机语言里，类型是数据的一个属性，它的作用是来告诉编译器或解释器，程序可以如何使用这些数据。比如说，对于整型数据，它可能占 32 或者 64 位存储，我们可以对它做加减乘除操作。而对于字符串，它可能占很多个字节，并且通过一定的编码规则来表示字符。字符串可以做连接、查找、获取子字符串等操作，但不能像整数一样做算术运算。 一门语言的类型系统是包含了与类型有关的各种规则的一个逻辑系统。类型系统包含了一系列规则，规定了如何把类型用于变量、表达式和函数等程序元素，以及如何创建自定义类型，等等。比如，如果你定义了某个类有哪些方法，那你就只能通过调用这些方法来使用这个类，没有别的方法。这些强制规定减少了程序出错的可能性。所以在语义分析阶段，一个重要的工作就是做类型检查。那么，类型检查是怎样实现的呢？我们要如何做类型检查呢？关于类型检查，编译器一般会采用属性计算的方法，来计算出每个 AST 节点的类型属性，然后检查它们是否匹配。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:5:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"属性计算 以“int b = a+3”为例，它的 AST 如下图所示。编译器会计算出 b 节点所需的类型和 init 节点的实际类型，比较它们是否一致（或者可以自动转换）。 图 6：“int b = a+3”对应的 AST 我们首先要计算等号右边“a+3”的类型。其中，3 是个整型字面量，我们可以据此把它的类型标注为整型；a 是一个变量，它的类型可以从符号表中查到，也是整型。那么“a+3”是什么类型呢？根据加法的语义，两个整型数据相加，结果仍然是整型，因此“a+3”这个表达式整体是整型的。因为 init 只有一个子节点（add），所以 init 的类型也一样是整型。在刚才这段推理中，我们实际上是依据“a+3”的 AST，从下级节点的类型计算出上级节点的类型。 那么，我们能否以同样的方法计算 b 节点的类型呢？答案是不可以。因为 b 根本没有子节点。但声明变量 b 的时候，有个 int 关键字，所以在 AST 中，b 有一个兄弟节点，就是 int 关键字。根据变量声明的语义，b 的类型就是 int，因此它的类型是从 AST 的兄弟节点中获得的。你看，同样是计算 AST 节点的类型，等号右边和左边的计算方法是不一样的。 实际上，我们刚才用的分析方法，就是属性计算。其中，有些属性是通过子节点计算出来的，这叫做 S 属性（Synthesized Attribute，综合出来的属性），比如等号右边的类型。而另一些属性，则要根据父节点或者兄弟节点计算而来，这种属性叫做 I 属性（Inherited Attribute，继承到的属性），比如等号左边的 b 变量的类型。计算出来的属性，我们可以标注在 AST 上，这就形成我第 1 讲曾经提过的带有标注信息的 AST，（Annotated Tree），也有人称之为 Decorated Tree，或者 Attributed Tree。虽然叫法有很多，但都是一个意思，都是向 AST 中添加了语义信息。 图 7：带有标注信息的 AST 属性计算的方法，就是基于语法规则，来定义一些属性计算的规则，在遍历 AST 的时候执行这些规则，我们就可以计算出属性值。这种基于语法规则定义的计算规则，被叫做属性文法（Attribute Grammar）。补充：基于属性计算的方法可以做类型检查，那其实也可以做类型推断。有些现代语言在声明一个变量的时候，可以不明确指定的类型，那么它的类型就可以通过变量声明语句的右边部分推断出来。你可能会问，属性计算的方法，除了计算类型，还可以计算什么属性呢？ 根据不同语言的语义，可能有不同的属性需要计算。其实，value（值）也可以看做是一个属性，你可以给每个节点定义一个“value”属性。对表达式求值，也就是对 value 做属性计算，比如，“a + 3”的值，我们就可以自下而上地计算出来。这样看起来，value 是一个 S 属性。针对 value 这个属性的属性文法，你可以参考下面这个例子，在做语法解析（或先解析成 AST，再遍历 AST）的时候，执行方括号中的规则，我们就可以计算出 AST 的值了。 add1 → add2 + mul [ add1.value = add2.value + mul.value ] add → mul [ add.value = mul.value ] mul1 → mul2 * primary [ mul1.value = mul2.value * primary.value ] mul → primary [ mul.value = primary.value ] primary → ( add ) [ primary.value = add.value ] primary → integer [ primary.value = strToInt(integer.str) ] 这种在语法规则上附加一系列动作，在解析语法的时候执行这些动作的方式，是一种编译方法，在龙书里有一个专门的名字，叫做语法制导的翻译（Syntax Directed Translation，SDT）。使用语法制导的翻译可以做很多事情，包括做属性计算、填充符号表，以及生成 IR。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:5:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 在实际的编译器中，语义分析相关的代码量往往要比词法分析和语法分析的代码量大。因为一门语言有很多语义规则，所以要做的语义分析和检查工作也很多。并且，因为每门语言之间的差别主要都体现在语义上，所以每门语言在语义处理方面的工作差异也比较大。比如，一门语言支持闭包，另一门语言不支持；有的语言支持泛型，另一门语言不支持；一门语言的面向对象特性是基于继承实现的，而另一门语言则是基于组合实现的，等等。不过，这没啥关系。我们主要抓住它们的共性就好了。这些共性，就是我们本讲的内容： 做好上下文相关的分析，比如对各种引用的消解、控制流的检查、闭包的分析等；做好与类型有关的分析和处理，包括类型检查、类型推断等；掌握属性计算这个工具，用于计算类型、值等属性；最后，把获得的语义信息保存到符号表和 AST 里。 我把本讲的知识点也整理成了脑图，供你参考： ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:5:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"05 | 运行时机制：程序如何运行，你有发言权 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:6:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"06 | 中间代码：不是只有一副面孔 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:7:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"07 | 代码优化：跟编译器做朋友，让你的代码飞起来 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:8:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"08 | 代码生成：如何实现机器相关的优化？ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:9:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"知识地图 | 一起来复习编译技术核心概念与算法 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:10:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"真实编译器解析篇 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:11:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"24 | Go语言编译器：把它当作教科书吧 宫文学 2020-07-27 你好，我是宫文学。今天这一讲，我来带你研究一下 Go 语言自带的编译器，它可以被简称为 gc。我之所以要来带你研究 Go 语言的编译器，一方面是因为 Go 现在确实非常流行，很多云端服务都用 Go 开发，Docker 项目更是巩固了 Go 语言的地位；另一方面，我希望你能把它当成编译原理的教学参考书来使用。这是因为： Go 语言的编译器完全用 Go 语言本身来实现，它完全实现了从前端到后端的所有工作，而不像 Java 要分成多个编译器来实现不同的功能模块，不像 Python 缺少了后端，也不像 Julia 用了太多的语言。所以你研究它所采用的编译技术会更方便。Go 编译器里基本上使用的都是经典的算法：经典的递归下降算法、经典的 SSA 格式的 IR 和 CFG、经典的优化算法、经典的 Lower 和代码生成，因此你可以通过一个编译器就把这些算法都贯穿起来。除了编译器，你还可以学习到一门语言的其他构成部分的实现思路，包括运行时（垃圾收集器、并发调度机制等）、标准库和工具链，甚至连链接器都是用 Go 语言自己实现的，从而对实现一门语言所需要做的工作有更完整的认识。最后，Go 语言的实现继承了从 Unix 系统以来形成的一些良好的设计哲学，因为 Go 语言的核心设计者都是为 Unix 的发展，做出过重要贡献的极客。因此了解了 Go 语言编译器的实现机制，会提高你的软件设计品味。 扩展：每种语言都有它的个性，而这个个性跟语言设计者的背景密切相关。Go 语言的核心设计者，是 Unix 领域的极客，包括 Unix 的创始人和 C 语言的共同发明人之一，Ken Tompson。Rob Pike 也是 Unix 的核心作者。Go 语言的作者们显然希望新的语言体现出他们的设计哲学和口味。比如，致力于像 Unix 那样的简洁和优雅，并且致力于让 Go 再次成为一款经典作品。 所以，在已经研究了多个高级语言的编译器之后，我们可以拿 Go 语言的编译器，把整个编译过程再重新梳理和印证一遍。好了，现在就开始我们今天探索的旅途吧。首先，我们来看看 Go 语言编译器的前端。 重要提示：照例，你要下载 Go 语言的源代码，本讲采用的是 1.14.2 版本。并且，你最好使用一个 IDE，便于跟踪调试编译器的执行过程。Go 的源代码中附带的介绍编译器的文档，写得很好、很清晰，你可以参考一下。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:12:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"词法分析和语法分析 Go 的编译器的词法分析和语法分析功能的实现，是在 cmd/compile/internal/syntax 目录下。词法分析器是 scanner.go。其实大部分编程语言的词法分析器的算法，都已经很标准了，我们在Java 编译器里就曾经分析过。甚至它们处理标识符和关键字的方式也都一致，都是先作为标识符识别出来，然后再查表挑出关键字来。Go 的词法分析器并没有像 V8 那样在不遗余力地压榨性能，它跟你平常编码的方式是很一致的，非常容易阅读。 语法分析器是 parser.go。它是一个标准的手写的递归下降算法。在解析二元表达式的时候，Go 的语法分析器也是采用了运算符优先级算法，这个已经是我们第 N 次见到这个算法了，所以你一定要掌握！不过，每个编译器的实现都不大一样，而 Go 的实现方式相当的简洁，你可以去自己看一下，或者用调试器来跟踪一下它的执行过程。 图 1：用 IDE 工具 Goland 跟踪调试编译过程 Go 的 AST 的节点，是在 nodes.go 中定义的，它异常简洁，可以说简洁得让你惊讶。你可以欣赏一下。 Go 的语法分析器还有一个很有特色的地方，就是对错误的处理。它在处理编译错误时，有一个原则，就是不要遇到一个错误就停止编译，而是要尽可能跳过当前这个出错的地方，继续往下编译，这样可以一次多报几个语法错误。 parser.go 的处理方式是，当语法分析器在处理某个产生式的时候，如果发现了错误，那就记录下这个错误，并且往下跳过一些 Token，直到找到一个 Token 是属于这个产生式的 Follow 集合的。这个时候编译器就认为找到了这个产生式的结尾。这样分析器就可以跳过这个语法单元，继续处理下面的语法单元。 比如，在解析函数声明语句时，如果 Go 的语法分析器没有找到函数名称，就报错“expecting name or (”，然后往后找到“{”或者“;”，这样就跳过了函数名称的声明部分，继续去编译后面的函数体部分。 在 cmd/compile/internal/syntax 目录下，还有词法分析器和语法分析器的测试程序，你可以去运行测试一下。 最后，如果你还想对 Go 语言的语法分析有更加深入地了解，我建议你去阅读一下Go 语言的规范，它里面对于每个语法单元，都有 EBNF 格式的语法规则定义，比如对语句的定义。你通过看代码、看语言规范，积累语法规则的第一手经验，以后再看到一段程序，你的脑子里就能反映出它的语法规则，并且能随手画出 AST 了，这是你学习编译原理需要建立的硬功夫。比如说，这里我节选了一段 Go 语言的规范中针对语句的部分语法规则。 Statement = Declaration | LabeledStmt | SimpleStmt | GoStmt | ReturnStmt | BreakStmt | ContinueStmt | GotoStmt | FallthroughStmt | Block | IfStmt | SwitchStmt | SelectStmt | ForStmt | DeferStmt . SimpleStmt = EmptyStmt | ExpressionStmt | SendStmt | IncDecStmt | Assignment | ShortVarDecl . 好，在了解了 Go 语言编译器的语法分析工作以后，接下来，我们再来看看它的语义分析阶段。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:12:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"语义分析（类型检查和 AST 变换） 语义分析的程序，是在 cmd/compile/internal/gc 目录下（注意，gc 的意思是 Go Compiler，不是垃圾收集的意思）。在入口代码 main.go 中，你能看到整个编译过程的主干步骤。语义分析的主要程序是在typecheck.go中。这里你要注意，不要被“typecheck”的名称所误导，它其实不仅是做类型检查，还做了名称消解（Name Resolution）和类型推导。 你已经知道，名称消解算法的特点，是分阶段完成。举个例子，在给表达式“a=b”中的变量 b 做引用消解之前，编译器必须先处理完 b 的定义，比如“var b Person”，这样才知道符号 b 指的是一个 Person 对象。 另外，在前面学习Java 编译器的时候，你已经知道，对方法体中的本地变量的消解，必须放在最后，才能保证变量的使用总是引用到在它前面的变量声明。Go 的编译器也是采用了相同的实现思路，你可以借此再回顾一下这个知识点，加深认识。 在语义分析阶段，Go 的编译器还做了一些 AST 变换的工作。其中就有内联优化和逃逸分析这两项工作。在我们之前解析的编译器当中，这两项工作都是基于专门做优化的 IR（比如 Sea of Nodes）来做的，而在 Go 的编译器里，却可以基于 AST 来做这两项优化。你看，是不是真实世界中的编译器，才能让你如此开阔眼界？ 你可以用“-m”参数来编译程序，它会打印出与内联和逃逸方面有关的优化。你可以带上多个“-m”参数，打印出嵌套层次更深的算法步骤的决策。 go build -gcflags '-m -m' hello.go 好了，现在我们借 gc 编译器，又复习了一遍语义分析中的一些关键知识点：名称消解算法要分阶段，在语义分析阶段会对 AST 做一些变换。我们继续来看 gc 编译器下一步的处理工作。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:12:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"生成 SSA 格式的 IR gc 编译器在做完语义分析以后，下一步就是生成 IR 了。并且，gc 的 IR 也是 SSA 格式的。你可以通过 gc，来进一步了解如何生成和处理 SSA 格式的 IR。好，首先，我们来看看 Go 语言的 IR 是什么样子的。针对下面的示例代码 foo.go，我们来看下它对应的 SSA 格式的 IR： package main func Foo(a int) int { var b int if (a \u003e 10) { b = a } else { b = 10 } return b } 在命令行中输入下面的命令，让 gc 打印出为 foo 函数生成的 IR。在当前目录下，你能看到一个 ssa.html 文件，你可以在浏览器中打开它。 goSSAFUNC=Foo go build -gcflags '-S' foo.go 在这个文件当中，你能看到编译器对 IR 做了多步的处理，也能看到每次处理后所生成的 IR。gc 的 IR 是基于控制流图（CFG）的。一个函数会被分成多个基本块，基本块中包含了一行行的指令。点击某个变量，你能看到它的定义和使用情况（def-use 链，图中显示成绿色）。你还能看到，图中灰色的变量，根据定义和使用关系，会发现它们没有被使用，所以是死代码，可以删除。 图 2：foo 示例程序各个处理阶段的 IR 针对第一个阶段（Start 阶段），我来给你解释一下每行指令的含义（可参考 genericOps.go），帮助你了解 Go 语言的 IR 的设计特点。 你可以参考代码库中介绍 SSA 的文档，里面介绍了 Go 的 SSA 的几个主要概念。 下面我来给你解读一下。 首先是值（Value）。Value 是 SSA 的最主要构造单元，它可以定义一次、使用多次。在定义一个 Value 的时候，需要一个标识符（ID）作为名称、产生该 Value 的操作码（Op）、一个类型（Type，就是代码中 \u003c\u003e 里面的值），以及一些参数。 操作码有两类。一类是机器无关的，其定义在genericOps.go中；一类是机器相关的，它是面向特定的 CPU 架构的，其定义在 XXXOps.go 中。比如，AMD64Ops.go中是针对 AMD64 架构 CPU 的操作码信息。 在做 Lower 处理时，编译器会把机器无关的操作码转换为机器相关的操作码，有利于后序生成目标代码。机器无关的优化和机器相关的优化，分别作用在采用这两类不同操作码的 IR 上。 Value 的类型信息，通常就是 Go 语言中的类型。但有几个类型是只会在 SSA 中用到的特殊类型，就像上面语句中的，即内存 (TypeMem) 类型；以及 TypeFlags，也就是 CPU 的标志位类型。 这里我要特别讲一下内存类型。内存类型代表的是全局的内存状态。如果一个操作码带有一个内存类型的参数，那就意味着该操作码依赖该内存状态。如果一个操作码的类型是内存类型，则意味着它会影响内存状态。SSA 的介绍文档中有一个例子，能帮助你理解内存类型的用法。在这个例子中，程序首先会向地址 a 写入 3 这个值。这个时候，内存状态就修改了（从 v1 到了 v10）。接着，把地址 a 的值写入地址 b，内存状态又发生了一次修改。在 IR 中，第二行代码依赖第一行代码的内存状态（v10），因此就导致这行代码只能出现在定义了 v10 之后。 // *a = 3 //向a地址写入3 // *b = *a //向b地址写入a的值 v10 = Store \u003cmem\u003e {int} v6 v8 v1 v14 = Store \u003cmem\u003e {int} v7 v8 v10 这里你需要注意，对内存的读和写（各种 IR 一般都是使用 Load 和 Store 这两个词汇）是一类比较特殊的指令。其他的 Value，我们都可以认为它们是在寄存器中的，是计算过程中的临时变量，所以它们在代码中的顺序只受数据流中依赖关系的制约。而一旦中间有读写内存的操作，那么代码顺序就会受到一定的限制。 我们可以跟在Graal 编译器中学到的知识印证一下。当你读写一个 Java 对象的属性的时候，也会涉及内存的读写，这些操作对应的 IR 节点，在顺序上也是受到限制的，我们把它们叫做固定节点。 此外，Value 结构中还包含了两个辅助信息字段：AuxInt 和 Aux。AuxInt 是一个整型值，比如，在使用 Const64 指令中，AuxInt 保存了常量的值；而 Aux 则可能是个复杂的结构体，用来保存每个操作码的个性化的信息。 在 IR 中你还能看到基本块（Block），这是第二个重要的数据结构。Go 编译器中的基本块有三种：简单（Plain）基本块，它只有一个后继基本块；退出（Exit）基本块，它的最后一个指令是一个返回指令；还有 if 基本块，它有一个控制值，并且它会根据该值是 true 还是 false，跳转到不同的基本块。 第三个数据结构是函数（Func）。函数是由多个基本块构成的。它必须有一个入口基本块（Entry Block），但可以有 0 到多个退出基本块，就像一个 Go 函数允许包含多个 Return 语句一样。 现在，你已经知道了 Go 的 IR 的关键概念和相关的数据结构了。Go 的 IR 在运行时就是保存在 Value、Block、Func 等内存结构中，就像 AST 一样。它不像 LLVM 的 bitcode 还有文本格式、二进制格式，可以保存在文件中。 那么接下来，编译器就可以基于 IR，来做优化了。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:12:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"基于 SSA 格式的 IR 做优化 SSA 格式的 IR 对编译器做优化很有帮助。 以死代码删除为例，Value 结构中有一个 Uses 字段，记录了它的使用数。如果它出现在另一个 Value 的操作码的参数里，或者是某个基本块的控制变量，那么使用数就会加 1；而如果 Uses 字段的值是 0，那就证明这行代码没什么用，是死代码，可以删掉。 而你应该记得，在第 7 讲中曾提到过，我们需要对一个函数的所有基本块都扫描一遍甚至多遍，才能知道某个变量的活跃性，从而决定是否可以删除掉它。那相比起来，采用 SSA 格式，可以说简单太多了。 基于这样的 IR 来做优化，就是对 IR 做很多遍（Pass）的处理。在cmd/compile/internal/ssa/compile.go的代码里，列出了所有这些 Pass，有将近 50 个。你能看到每个处理步骤执行的是哪个优化函数，你还可以在 ssa.html 中，看到每个 Pass 之后，IR 都被做了哪些修改。 图 3：compiler.go 中的 Pass 这些处理算法都是在cmd/compile/internal/ssa目录下。比如cse.go里面是消除公共子表达式的算法，而nilcheck.go是被用来消除冗余的 nil 检查代码。 有些算法还带了测试程序（如cse_test.go，nilcheck_test.go）。你可以去阅读一下，看看测试程序是如何构造测试数据的，并且你还可以通过 Debugger 来跟踪测试程序的执行过程，从而理解相关优化算法是如何实现的，这是一个很有效的学习方式。 另外，gc 还有一些比较简单的优化算法，它们是基于一些规则，对 IR 做一些重写（rewrite）。Go 的编译器使用了自己的一种 DSL，来描述这些重写规则：针对机器无关的操作码的重写规则，是在generic.rules文件中；而针对机器有关的操作码的重写规则是在 XXX.rules 中，比如AMD64.rules。 我们来看几个例子：在 generic.rules 中，有这样一个机器无关的优化规则，它是把 x*1 的运算优化为 x。 图 4：把 x*1 的运算优化为 x 的规则 在 AMD64.rules 中，有一个机器相关的优化规则，这个规则是把 MUL 指令转换为 LEA 指令，LEA 指令比 MUL 指令消耗的时钟周期更少。 (MUL(Q|L)const [ 3] x) -\u003e (LEA(Q|L)2 x x) generic.rules 中的规则会被rulegen.go解析，并生成 Go 代码rewritegeneric.go。而 AMD64.rules 中的规则，被解析后会生成rewriteAMD64.go。其中，Lower 的过程，也就是把机器无关的操作码转换为机器相关的操作码，它也是用这种重写规则实现的。 通过 gc 这种基于规则做指令转换的方法，你应该产生一个感悟，也就是在写软件的时候，我们经常要设计自己的 DSL，让自己的软件更具灵活性。比如，gc 要增加一个新的优化功能，只需要增加一条规则就行了。我们还可以再拿 Graal 编译器印证一下。你还记得，Graal 在生成 LIR 的时候，要进行指令的选择，那些选择规则是用注解来生成的，而那些注解规则，也是一种 DSL。 好了，谈完了优化，我们继续往下看。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:12:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"生成机器码 最后，编译器就可以调用 gc/ssa.go 中的genssa方法，来生成汇编码了。在 ssa.html 的最右边一栏，就是调用 genssa 方法以后生成的汇编代码（采用的是 Go 编译器特有的格式，其中有些指令，如 PCDATA 和 FUNCDATA 是用来与垃圾收集器配合的）。 你可能会问，编译器在生成机器码之前，不是还要做指令选择、寄存器分配、指令排序吗？那我们看看 gc 是如何完成这几项任务的。 寄存器分配（regalloc.go）作为一个 Pass，已经在生成机器码之前执行了。它采用的是线性扫描算法（Linear Scan Register Allocator）。 指令选择会分为两部分的工作。一部分工作，是在优化算法中已经做了一些指令选择，我们前面提到的重写规则，就蕴含了根据 IR 的模式，来生成合适的指令的规则；另一部分工作，则放到了汇编器当中。 这就是 Go 的编译器与众不同的地方。原来，gc 生成的汇编代码，是一种“伪汇编”，它是一种半抽象的汇编代码。在生成特定 CPU 的机器码的时候，它还会做一些转换，这个地方可以完成另一些指令选择的工作。 至于指令排序，我没看到过在 gc 编译器中的实现。我请教了谷歌的一位研究员，他给我的信息是：像 AMD64 这样的 CPU，已经能够很好地支持乱序执行了，所以指令重排序给 gc 编译器的优化工作，带来的好处很有限。 而 gc 目前没有做指令排序，还有一个原因就是，指令重排序算法的实现代价比较高，而 gc 的一个重要设计目标，就是要求编译速度要快。 扩展：Go 语言的另外两个编译器，gccgo 和 GoLLVM 都具备指令重排序功能。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:12:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 这一讲，我给你介绍了 gc 编译器的主要特点。之所以能压缩在一讲里面，是因为你已经见识了好几款编译器，渐渐地可以触类旁通、举一反三了。 在 gc 里面，你能看到很多可以借鉴的成熟实践：语法分析：递归下降算法，加上针对二元表达式的运算符优先级算法；语义分析：分阶段的名称消解算法，以及对 AST 的转换；优化：采用了 SSA 格式的 IR、控制流图（CFG）、多个 Pass 的优化框架，以及通过 DSL 支持的优化规则。 所以在这一讲的开头，我还建议你把 Go 语言的编译器作为你学习编译原理的“教学参考书”，建议你在图形化的 IDE 界面里，来跟踪调试每一个功能，这样你就能很方便去观察它的算法执行过程。本讲的思维导图如下： ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:12:6","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"参考资料 Introduction to the Go compiler 官方文档，介绍了 gc 的主要结构。Introduction to the Go compiler’s SSA backend 官方文档，介绍了 gc 的 SSA。Go compiler internals: adding a new statement to Go - Part 1、Part2。在这两篇博客里，作者做了一个实验：如果往 Go 里面增加一条新的语法规则，需要做哪些事情。你能贯穿性地了解一个编译器的方法。Go compiler: SSA optimization rules description language这篇博客，详细介绍了 gc 编译器的 SSA 优化规则描述语言的细节。A Primer on Go Assembly和A Quick Guide to Go’s Assembler 。gc 编译器采用的汇编语言是它自己的一种格式，是“伪汇编”。这两篇文章中有 Go 汇编的细节。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:12:7","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"25 | MySQL编译器（一）：解析一条SQL语句的执行过程 数据库系统能够接受 SQL 语句，并返回数据查询的结果，或者对数据库中的数据进行修改，可以说几乎每个程序员都使用过它。而 MySQL 又是目前使用最广泛的数据库。所以，解析一下 MySQL 编译并执行 SQL 语句的过程，一方面能帮助你加深对数据库领域的编译技术的理解；另一方面，由于 SQL 是一种最成功的 DSL（特定领域语言），所以理解了 MySQL 编译器的内部运作机制，也能加深你对所有使用数据操作类 DSL 的理解，比如文档数据库的查询语言。另外，解读 SQL 与它的运行时的关系，也有助于你在自己的领域成功地使用 DSL 技术。 那么，数据库系统是如何使用编译技术的呢？接下来，我就会花两讲的时间，带你进入到 MySQL 的内部，做一次全面的探秘。今天这一讲，我先带你了解一下如何跟踪 MySQL 的运行，了解它处理一个 SQL 语句的过程，以及 MySQL 在词法分析和语法分析方面的实现机制。好，让我们开始吧！ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:13:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"编译并调试 MySQL 按照惯例，你要下载MySQL 的源代码。我下载的是 8.0 版本的分支。源代码里的主要目录及其作用如下，我们需要分析的代码基本都在 sql 目录下，它包含了编译器和服务端的核心组件。 图 1：MySQL 的源代码包含的主要目录 MySQL 的源代码主要是.cc 结尾的，也就是说，MySQL 主要是用 C++ 编写的。另外，也有少量几个代码文件是用 C 语言编写的。为了跟踪 MySQL 的执行过程，你要用 Debug 模式编译 MySQL，具体步骤可以参考这篇开发者文档。如果你用单线程编译，大约需要 1 个小时。编译好以后，先初始化出一个数据库来： ./mysqld --initialize --user=mysql 这个过程会为 root@localhost 用户，生成一个缺省的密码。接着，运行 MySQL 服务器： ./mysqld \u0026 之后，通过客户端连接数据库服务器，这时我们就可以执行 SQL 了： ./mysql -uroot -p #连接mysql server 最后，我们把 GDB 调试工具附加到 mysqld 进程上，就可以对它进行调试了。 gdb -p `pidof mysqld` #pidof是一个工具，用于获取进程的id，你可以安装一下 提示：这一讲中，我是采用了一个 CentOS 8 的虚拟机来编译和调试 MySQL。我也试过在 macOS 下编译，并用 LLDB 进行调试，也一样方便。 注意，你在调试程序的时候，有两个设置断点的好地方：dispatch_command：在 sql/sql_parse.cc 文件里。在接受客户端请求的时候（比如一个 SQL 语句），会在这里集中处理。my_message_sql：在 sql/mysqld.cc 文件里。当系统需要输出错误信息的时候，会在这里集中处理。 这个时候，我们在 MySQL 的客户端输入一个查询命令，就可以从雇员表里查询姓和名了。在这个例子中，我采用的数据库是 MySQL 的一个示例数据库 employees，你可以根据它的文档来生成示例数据库。 mysql\u003e select first_name, last_name from employees; #从mysql库的user表中查询信息 这个命令被 mysqld 接收到以后，就会触发断点，并停止执行。这个时候，客户端也会老老实实地停在那里，等候从服务端传回数据。即使你在后端跟踪代码的过程会花很长的时间，客户端也不会超时，一直在安静地等待。给我的感觉就是，MySQL 对于调试程序还是很友好的。在 GDB 中输入 bt 命令，会打印出调用栈，这样你就能了解一个 SQL 语句，在 MySQL 中执行的完整过程。为了方便你理解和复习，这里我整理成了一个表格： 我也把 MySQL 执行 SQL 语句时的一些重要程序入口记录了下来，这也需要你重点关注。它反映了执行 SQL 过程中的一些重要的处理阶段，包括语法分析、处理上下文、引用消解、优化和执行。你在这些地方都可以设置断点。 好了，现在你就已经做好准备，能够分析 MySQL 的内部实现机制了。不过，由于 MySQL 执行的是 SQL 语言，它跟我们前面分析的高级语言有所不同。所以，我们先稍微回顾一下 SQL 语言的特点。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:13:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"SQL 语言：数据库领域的 DSL SQL 是结构化查询语言（Structural Query Language）的英文缩写。举个例子，这是一个很简单的 SQL 语句： select emp_no, first_name, last_name from employees; 其实在大部分情况下，SQL 都是这样一个一个来做语句执行的。这些语句又分为 DML（数据操纵语言）和 DDL（数据定义语言）两类。前者是对数据的查询、修改和删除等操作，而后者是用来定义数据库和表的结构（又叫模式）。我们平常最多使用的是 DML。而 DML 中，执行起来最复杂的是 select 语句。所以，在本讲，我都是用 select 语句来给你举例子。那么，SQL 跟我们前面分析的高级语言相比有什么不同呢？ 第一个特点：SQL 是声明式（Declarative）的。这是什么意思呢？其实就是说，SQL 语句能够表达它的计算逻辑，但它不需要描述控制流。高级语言一般都有控制流，也就是详细规定了实现一个功能的流程：先调用什么功能，再调用什么功能，比如 if 语句、循环语句等等。这种方式叫做命令式（imperative）编程。更深入一点，声明式编程说的是“要什么”，它不关心实现的过程；而命令式编程强调的是“如何做”。前者更接近人类社会的领域问题，而后者更接近计算机实现。 第二个特点：SQL 是一种特定领域语言（DSL，Domain Specific Language），专门针对关系数据库这个领域的。SQL 中的各个元素能够映射成关系代数中的操作术语，比如选择、投影、连接、笛卡尔积、交集、并集等操作。它采用的是表、字段、连接等要素，而不需要使用常见的高级语言的变量、类、函数等要素。 所以，SQL 就给其他 DSL 的设计提供了一个很好的参考：采用声明式，更加贴近领域需求。比如，你可以设计一个报表的 DSL，这个 DSL 只需要描述报表的特征，而不需要描述其实现过程。采用特定领域的模型、术语，甚至是数学理论。比如，针对人工智能领域，你完全就可以用张量计算（力学概念）的术语来定义 DSL。 好了，现在我们分析了 SQL 的特点，从而也让你了解了 DSL 的一些共性特点。那么接下来，顺着 MySQL 运行的脉络，我们先来了解一下 MySQL 是如何做词法分析和语法分析的。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:13:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"词法和语法分析 词法分析的代码是在 sql/sql_lex.cc 中，入口是 MYSQLlex() 函数。在 sql/lex.h 中，有一个 symbols[]数组，它定义了各类关键字、操作符。 MySQL 的词法分析器也是手写的，这给算法提供了一定的灵活性。比如，SQL 语句中，Token 的解析是跟当前使用的字符集有关的。使用不同的字符集，词法分析器所占用的字节数是不一样的，判断合法字符的依据也是不同的。而字符集信息，取决于当前的系统的配置。词法分析器可以根据这些配置信息，正确地解析标识符和字符串。 MySQL 的语法分析器是用 bison 工具生成的，bison 是一个语法分析器生成工具，它是 GNU 版本的 yacc。bison 支持的语法分析算法是 LALR 算法，而 LALR 是 LR 算法家族中的一员，它能够支持大部分常见的语法规则。bison 的规则文件是 sql/sql_yacc.yy，经过编译后会生成 sql/sql_yacc.cc 文件。 sql_yacc.yy 中，用你熟悉的 EBNF 格式定义了 MySQL 的语法规则。我节选了与 select 语句有关的规则，如下所示，从中你可以体会一下，SQL 语句的语法是怎样被一层一层定义出来的： select_stmt: query_expression | ... | select_stmt_with_into ; query_expression: query_expression_body opt_order_clause opt_limit_clause | with_clause query_expression_body opt_order_clause opt_limit_clause | ... ; query_expression_body: query_primary | query_expression_body UNION_SYM union_option query_primary | ... ; query_primary: query_specification | table_value_constructor | explicit_table ; query_specification: ... | SELECT_SYM /*select关键字*/ select_options /*distinct等选项*/ select_item_list /*select项列表*/ opt_from_clause /*可选：from子句*/ opt_where_clause /*可选：where子句*/ opt_group_clause /*可选：group子句*/ opt_having_clause /*可选：having子句*/ opt_window_clause /*可选：window子句*/ ; ... 其中，query_expression 就是一个最基础的 select 语句，它包含了 SELECT 关键字、字段列表、from 子句、where 子句等。你可以看一下 select_options、opt_from_clause 和其他几个以 opt 开头的规则，它们都是 SQL 语句的组成部分。opt 是可选的意思，也就是它的产生式可能产生ε。 opt_from_clause: /* Empty. */ | from_clause ; 另外，你还可以看一下表达式部分的语法。在 MySQL 编译器当中，对于二元运算，你可以大胆地写成左递归的文法。因为它的语法分析的算法用的是 LALR，这个算法能够自动处理左递归。一般研究表达式的时候，我们总是会关注编译器是如何处理结合性和优先级的。那么，bison 是如何处理的呢？原来，bison 里面有专门的规则，可以规定运算符的优先级和结合性。在 sql_yacc.yy 中，你会看到如下所示的规则片段： 你可以看一下 bit_expr 的产生式，它其实完全把加减乘数等运算符并列就行了。 bit_expr : ... | bit_expr '+' bit_expr %prec '+' | bit_expr '-' bit_expr %prec '-' | bit_expr '*' bit_expr %prec '*' | bit_expr '/' bit_expr %prec '/' ... | simple_expr 如果你只是用到加减乘除的运算，那就可以不用在产生式的后面加 %prec 这个标记。但由于加减乘除这几个还可以用在其他地方，比如“-a”可以用来表示把 a 取负值；减号可以用在一元表达式当中，这会比用在二元表达式中有更高的优先级。也就是说，为了区分同一个 Token 在不同上下文中的优先级，我们可以用 %prec，来说明该优先级是上下文依赖的。 好了，在了解了词法分析器和语法分析器以后，我们接着来跟踪一下 MySQL 的执行，看看编译器所生成的解析树和 AST 是什么样子的。在 sql_class.cc 的 sql_parser() 方法中，编译器执行完解析程序之后，会返回解析树的根节点 root，在 GDB 中通过 p 命令，可以逐步打印出整个解析树。你会看到，它的根节点是一个 PT_select_stmt 指针（见图 3）。解析树的节点是在语法规则中规定的，这是一些 C++ 的代码，它们会嵌入到语法规则中去。 下面展示的这个语法规则就表明，编译器在解析完 query_expression 规则以后，要创建一个 PT_query_expression 的节点，其构造函数的参数分别是三个子规则所形成的节点。对于 query_expression_body 和 query_primary 这两个规则，它们会直接把子节点返回，因为它们都只有一个子节点。这样就会简化解析树，让它更像一棵 AST。关于 AST 和解析树（也叫 CST）的区别，我在解析 Python 的编译器中讲过了，你可以回忆一下。 query_expression: query_expression_body opt_order_clause opt_limit_clause { $$ = NEW_PTN PT_query_expression($1, $2, $3); /*创建节点*/ } | ... query_expression_body: query_primary { $$ = $1; /*直接返回query_primary的节点*/ } | ... query_primary: query_specification { $$= $1; /*直接返回query_specification的节点*/ } | ... 最后，对于“select first_name, last_name from employees”这样一个简单的 SQL 语句，它所形成的解析树如下： 图 3：示例 SQL 解析后生成的解析树 而对于“select 2 + 3”这样一个做表达式计算的 SQL 语句，所形成的解析树如下。你会看到，它跟普通的高级语言的表达式的 AST 很相似： 图 4：“select 2 + 3”对应的解析树 图 4 中的 PT_query_expression 等类，就是解析树的节点，它们都是 Parse_tree_node 的子类（PT 是 Parse Tree 的缩写）。这些类主要定义在 sql/parse_tree_nodes.h 和 parse_tree_items.h 文件中。其中，Item 代表了与“值”有关的节点，它的子类能够用于表示字段、常量和表达式等。你可以通过 Item 的 val_int()、val_str() 等方法获取它的值。 图 5：解析树的树节点（部分） 由于 SQL 是一个个单独的语句，所以 select、insert、update 等语句，它们都各自有不同的根节点，都是 Parse_tree_root 的子类。 好了，现在你就已经了解了 SQL 的解析过程和它所生成的 AST 了。前面我说过，MySQL 采用的是 LALR 算法，因此我们可以借助 MySQL 编译器，来加深一下对 LR 算法家族的理解。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:13:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"重温 LR 算法 你在阅读 yacc.yy 文件的时候，在注释里，你会发现如何跟踪语法分析器的执行过程的一些信息。你可以用下面的命令，带上“-debug”参数，来启动 MySQL 服务器： mysqld --debug=\"d,parser_debug\" 然后，你可以通过客户端执行一个简单的 SQL 语句：“select 2+3*5”。在终端，会输出语法分析的过程。这里我截取了一部分界面，通过这些输出信息，你能看出 LR 算法执行过程中的移进、规约过程，以及工作区内和预读的信息。 我来给你简单地复现一下这个解析过程。第 1 步，编译器处于状态 0，并且预读了一个 select 关键字。你已经知道，LR 算法是基于一个 DFA 的。在这里的输出信息中，你能看到某些状态的编号达到了一千多，所以这个 DFA 还是比较大的。第 2 步，把 select 关键字移进工作区，并进入状态 42。这个时候，编译器已经知道后面跟着的一定是一个 select 语句了，也就是会使用下面的语法规则： query_specification: ... | SELECT_SYM /*select关键字*/ select_options /*distinct等选项*/ select_item_list /*select项列表*/ opt_from_clause /*可选：from子句*/ opt_where_clause /*可选：where子句*/ opt_group_clause /*可选：group子句*/ opt_having_clause /*可选：having子句*/ opt_window_clause /*可选：window子句*/ ; 为了给你一个直观的印象，这里我画了 DFA 的局部示意图（做了一定的简化），如下所示。你可以看到，在状态 42，点符号位于“select”关键字之后、select_options 之前。select_options 代表了“distinct”这样的一些关键字，但也有可能为空。 图 7：移进 select 后的 DFA 第 3 步，因为预读到的 Token 是一个数字（NUM），这说明 select_options 产生式一定生成了一个ε，因为 NUM 是在 select_options 的 Follow 集合中。这就是 LALR 算法的特点，它不仅会依据预读的信息来做判断，还要依据 Follow 集合中的元素。所以编译器做了一个规约，也就是让 select_options 为空。也就是，编译器依据“select_options-\u003eε”做了一次规约，并进入了新的状态 920。注意，状态 42 和 920 从 DFA 的角度来看，它们是同一个大状态。而 DFA 中包含了多个小状态，分别代表了不同的规约情况。 图 8：基于“select_options-\u003eε”规约后的 DFA 你还需要注意，这个时候，老的状态都被压到了栈里，所以栈里会有 0 和 42 两个状态。栈里的这些状态，其实记录了推导的过程，让我们知道下一步要怎样继续去做推导。 图 9：做完前 3 步之后，栈里的情况 第 4 步，移进 NUM。这时又进入一个新状态 720。 图 10：移进 NUM 后的 DFA 而旧的状态也会入栈，记录下推导路径： 图 11：移进 NUM 后栈的状态 第 5~8 步，依次依据 NUM_literal-\u003eNUM、literal-\u003eNUM_literal、simple_expr-\u003eliteral、bit_expr-\u003esimple_expr 这四条产生式做规约。这时候，编译器预读的 Token 是 + 号，所以你会看到，图中的红点停在 + 号前。 图 12：第 8 步之后的 DFA 第 9~10 步，移进 + 号和 NUM。这个时候，状态又重新回到了 720。这跟第 4 步进入的状态是一样的。 图 13：第 10 步之后的 DFA 而栈里的目前有 5 个状态，记录了完整的推导路径。 图 14：第 10 步之后栈的状态 到这里，其实你就已经了解了 LR 算法做移进和规约的思路了。不过你还可以继续往下研究。由于栈里保留了完整的推导路径，因此 MySQL 编译器最后会依次规约回来，把栈里的元素清空，并且形成一棵完整的 AST。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:13:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 这一讲，我带你初步探索了 MySQL 编译 SQL 语句的过程。你需要记住几个关键点： 掌握如何用 GDB 来跟踪 MySQL 的执行的方法。你要特别注意的是，我给你梳理的那些关键的程序入口，它是你理解 MySQL 运行过程的地图。SQL 语言是面向关系数据库的一种 DSL，它是声明式的，并采用了领域特定的模型和术语，可以为你设计自己的 DSL 提供启发。MySQL 的语法分析器是采用 bison 工具生成的。这至少说明，语法分析器生成工具是很有用的，连正式的数据库系统都在使用它，所以你也可以大胆地使用它，来提高你的工作效率。我在最后的参考资料中给出了 bison 的手册，希望你能自己阅读一下，做一些简单的练习，掌握 bison 这个工具。最后，你一定要知道 LR 算法的运行原理，知其所以然，这也会更加有助于你理解和用好工具。 我依然把本讲的内容给你整理成了一张知识地图，供你参考和复习回顾： ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:13:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"参考资料 MySQL 的内行手册（MySQL Internals Manual）能提供一些重要的信息。但我发现文档内容经常跟源代码的版本不同步，比如介绍源代码的目录结构的信息就过时了，你要注意这点。bison 的手册。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:13:6","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"26 | MySQL编译器（二）：编译技术如何帮你提升数据库性能？ 通过上一讲的学习，你已经了解了 MySQL 编译器是怎么做词法和语法分析的了。那么在做完语法分析以后，MySQL 编译器又继续做了哪些处理，才能成功地执行这个 SQL 语句呢？所以今天，我就带你来探索一下 MySQL 的实现机制，我会把重点放在 SQL 的语义分析和优化机制上。当你学完以后，你就能真正理解以下这些问题了：高级语言的编译器具有语义分析功能，那么 MySQL 编译器也会做语义分析吗？它有没有引用消解问题？有没有作用域？有没有类型检查？MySQL 有没有类似高级语言的那种优化功能呢？好，让我们开始今天的探究吧。不过，在讨论 MySQL 的编译过程之前，我想先带你了解一下 MySQL 会用到的一些重要的数据结构，因为你在解读代码的过程中经常会见到它们。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:14:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"认识 MySQL 编译器的一些重要的数据结构 第一组数据结构，是下图中的几个重要的类或结构体，包括线程、保存编译上下文信息的 LEX，以及保存编译结果 SELECT_LEX_UNIT 和 SELECT_LEX。 图 1：MySQL 编译器的中的几个重要的类和结构体 首先是 THD，也就是线程对象。对于每一个客户端的连接，MySQL 编译器都会启动一个线程来处理它的查询请求。THD 中的一个重要数据成员是 LEX 对象。你可以把 LEX 对象想象成是编译 SQL 语句的工作区，保存了 SQL 语句编译过程中的上下文信息，编译器会把编译的成果放在这里，而编译过程中所需要的信息也是从这里查找。在把 SQL 语句解析完毕以后，编译器会形成一些结构化的对象来表示一个查询。其中 SELECT_LEX_UNIT 结构体，就代表了一个查询表达式（Query Expression）。一个查询表达式可能包含了多个查询块，比如使用 UNION 的情况。而 SELECT_LEX 则代表一个基本的查询块（Query Block），它里面的信息包括了所有的列和表达式、查询用到的表、where 条件等。在 SELECT_LEX 中会保存查询块中涉及的表、字段和表达式等，它们也都有对应的数据结构。 第二组需要了解的数据结构，是表示表、字段等信息的对象。Table_ident 对象保存了表的信息，包括数据库名、表名和所在的查询语句（SELECT_LEX_UNIT 对象）。 图 2：Table_indent 对象，代表一个表 而字段和表达式等表示一个值的对象，用 Item 及其子类来表示。SQL 语句中的每个字段、每个计算字段，最后都对应一个 Item。where 条件，其实也是用一个 Item 就能表示。具体包括：字段（Item_field）。各种常数，包括数字、字符和 null 等（Item_basic_constant）。能够产生出值的运算（Item_result_field），包括算术表达式（Item_num_op）、存储过程（Item_func_sp）、子查询（Item_subselect）等。在语法分析过程中产生的 Item（Parse_tree_item）。它们是一些占位符，因为在语法分析阶段，不容易一下子创建出真正的 Item，这些 Parse_tree_item 需要在上下文分析阶段，被替换成真正的 Item。 图 3：Item 及其子类 好了，上面这些就是 MySQL 会用到的最核心的一些数据结构了。接下来的编译工作，就会生成和处理上述的数据结构。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:14:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"上下文分析 我们先来看一下 MySQL 编译器的上下文分析工作。你已经知道，语法分析仅仅完成的是上下文无关的分析，还有很多的工作，需要基于上下文来做处理。这些工作，就属于语义分析。 MySQL 编译器中，每个 AST 节点，都会有一个 contextualize() 方法。从这个方法的名称来看，你就能知道它是做上下文处理的（contextualize，置于上下文中）。对一个 Select 语句来说，编译器会调用其根节点 PT_select_stmt 的 contextualize() 方法，从而深度遍历整个 AST，并调用每个节点的 contextualize() 方法。 那么，MySQL 编译器的上下文处理，都完成了什么工作呢？首先，是检查数据库名、表名和字段名是否符合格式要求（在 table.cc 中实现）。 比如，MySQL 会规定表名、字段名等名称不能超过 64 个字符，字段名不能包含 ASCII 值为 255 的字符，等等。这些规则在词法分析阶段是不检查的，要留在语义分析阶段检查。 然后，创建并填充 SELECT_LEX_UNIT 和 SELECT_LEX 对象。 前面我提到了，SELECT_LEX_UNIT 和 SELECT_LEX 中，保存了查询表达式和查询块所需的所有信息，依据这些信息，MySQL 就可以执行实际的数据库查询操作。那么，在 contextualize 的过程中，编译器就会生成上述对象，并填充它们的成员信息。比如，对于查询中用到的表，在语法分析阶段就会生成 Table_ident 对象。但其中的数据库名称可能是缺失的，那么在上下文的分析处理当中，就会被编译器设置成当前连接所采用的默认数据库。这个信息可以从线程对象（THD）中获得，因为每个线程对应了一个数据库连接，而每个数据库连接是针对一个具体的数据库的。 好了，经过上下文分析的编译阶段以后，我们就拥有了可以执行查询的 SELECT_LEX_UNIT 和 SELECT_LEX 对象。可是，你可能会注意到一个问题：为什么在语义分析阶段，MySQL 没有做引用的消解呢？不要着急，接下来我就给你揭晓这个答案。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:14:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"MySQL 是如何做引用消解的？ 我们在 SQL 语句中，会用到数据库名、表名、列名、表的别名、列的别名等信息，编译器肯定也需要检查它们是不是正确的。这就是引用消解（或名称消解）的过程。一般编译器是在语义分析阶段来做这项工作的，而 MySQL 是在执行 SQL 命令的时候才做引用消解。引用消解的入口是在 SQL 命令的的 prepare() 方法中，它会去检查表名、列名都对不对。通过 GDB 调试工具，我们可以跟踪编译器做引用消解的过程。你可以在 my_message_sql() 函数处设个断点，然后写个 SQL 语句，故意使用错误的表名或者列名，来看看 MySQL 是在什么地方检查出这些错误的。比如说，你可以执行“select * from fake_table”，其中的 fake_table 这个表，在数据库中其实并不存在。下面是打印出的调用栈。你会注意到，MySQL 在准备执行 SQL 语句的过程中，会试图去打开 fake_table 表，这个时候编译器就会发现这个表不存在。 你还可以再试一下“select fake_column from departments”这个语句，也一样会查出，fake_column 并不是 departments 表中的一列。 那么，MySQL 是如何知道哪些表和字段合法，哪些不合法的呢？原来，它是通过查表的定义，也就是数据库模式信息，或者可以称为数据字典、元数据。MySQL 在一个专门的库中，保存了所有的模式信息，包括库、表、字段、存储过程等定义。你可以跟高级语言做一下类比。高级语言，比如说 Java 也会定义一些类型，类型中包含了成员变量。那么，MySQL 中的表，就相当于高级语言的类型；而表的字段（或列）就相当于高级语言的类型中的成员变量。所以，在这个方面，MySQL 和高级语言做引用消解的思路其实是一样的。 但是，高级语言在做引用消解的时候有作用域的概念，那么 MySQL 有没有类似的概念呢？有的。举个例子，假设一个 SQL 语句带了子查询，那么子查询中既可以引用本查询块中的表和字段，也可以引用父查询中的表和字段。这个时候就存在了两个作用域，比如下面这个查询语句： select dept_name from departments where dept_no in (select dept_no from dept_emp where dept_name != 'Sales' #引用了上一级作用域中的字段 group by dept_no having count(*)\u003e 20000) 其中的 dept_name 字段是 dept_emp 表中所没有的，它其实是上一级作用域中 departments 表中的字段。提示：这个 SQL 当然写得很不优化，只是用来表现作用域的概念。好。既然要用到作用域，那么 MySQL 的作用域是怎么表示的呢？这就要用到 Name_resolution_context 对象。这个对象保存了当前作用域中的表，编译器可以在这些表里查找字段；它还保存了对外层上下文的引用（outer_context），这样 MySQL 就可以查找上一级作用域中的表和字段。 图 4：MySQL 用来表示作用域的对象 好了，现在你就对 MySQL 如何做引用消解非常了解了。我们知道，对于高级语言的编译器来说，接下来它还会做一些优化工作。那么，MySQL 是如何做优化的呢？它跟高级语言编译器的优化工作相比，又有什么区别呢？ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:14:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"MySQL 编译器的优化功能 MySQL 编译器的优化功能主要都在 sql_optimizer.cc 中。就像高级语言一样，MySQL 编译器会支持一些常见的优化。我来举几个例子。 第一个例子是常数传播优化（const propagation）。假设有一个表 foo，包含了 x 和 y 两列，那么 SQL 语句：“select * from foo where x = 12 and y=x”，会被优化成“select * from foo where x = 12 and y = 12”。你可以在 propagate_cond_constants() 函数上加个断点，查看常数传播优化是如何实现的。 第二个例子是死代码消除。比如，对于 SQL 语句：“select * from foo where x=2 and y=3 and x\u003c y”，编译器会把它优化为“select * from foo where x=2 and y=3”，把“x\u003c y”去掉了，这是因为 x 肯定是小于 y 的。该功能的实现是在 remove_eq_conds() 中。 第三个例子是常数折叠。这个优化工作我们应该很熟悉了，主要是对各种条件表达式做折叠，从而降低计算量。其实现是在 sql_const_folding.cc 中。 你要注意的是，上述的优化主要是针对条件表达式。因为 MySQL 在执行过程中，对于每一行数据，可能都需要执行一遍条件表达式，所以上述优化的效果会被放大很多倍，这就好比针对循环体的优化，是一个道理。 不过，MySQL 还有一种特殊的优化，是对查询计划的优化。比如说，我们要连接 employees、dept_emp 和 departments 三张表做查询，数据库会怎么做呢？最笨的办法，是针对第一张表的每条记录，依次扫描第二张表和第三张表的所有记录。这样的话，需要扫描多少行记录呢？是三张表的记录数相乘。基于我们的示例数据库的情况，这个数字是 8954 亿。上述计算其实是做了一个笛卡尔积，这会导致处理量的迅速上升。而在数据库系统中，显然不需要用这么笨的方法。 你可以用 explain 语句，让 MySQL 输出执行计划，下面我们来看看 MySQL 具体是怎么做的： explain select employees.emp_no, first_name, departments.dept_no dept_name from employees, dept_emp, departments where employees.emp_no = dept_emp.emp_no and dept_emp.dept_no = departments.dept_no; 这是 MySQL 输出的执行计划： 从输出的执行计划里，你能看出，MySQL 实际的执行步骤分为了 3 步：第 1 步，通过索引，遍历 departments 表；第 2 步，通过引用关系（ref），找到 dept_emp 表中，dept_no 跟第 1 步的 dept_no 相等的记录，平均每个部门在 dept_emp 表中能查到 3.7 万行记录；第 3 步，基于第 2 步的结果，通过等值引用（eq_ref）关系，在 employees 表中找到相应的记录，每次找到的记录只有 1 行。这个查找可以通过 employees 表的主键进行。 根据这个执行计划来完成上述的操作，编译器只需要处理大约 63 万行的数据。因为通过索引查数据相比直接扫描表来说，处理每条记录花费的时间比较长，所以我们假设前者花费的时间是后者的 3 倍，那么就相当于扫描了 63*3=189 万行表数据，这仍然仅仅相当于做笛卡尔积的 47 万分之一。我在一台虚拟机上运行该 SQL 花费的时间是 5 秒，而如果使用未经优化的方法，则需要花费 27 天！ 通过上面的例子，你就能直观地理解做查询优化的好处了。MySQL 会通过一个 JOIN 对象，来为一个查询块（SELECT_LEX）做查询优化，你可以阅读 JOIN 的方法，来看看查询优化的具体实现。关于查询优化的具体算法，你需要去学习一下数据库的相关课程，我在本讲末尾也推荐了一点参考资料，所以我这里就不展开了。 从编译原理的角度来看，我们可以把查询计划中的每一步，看做是一条指令。MySQL 的引擎，就相当于能够执行这些指令的一台虚拟机。如果再做进一步了解，你就会发现，MySQL 的执行引擎和存储引擎是分开的。存储引擎提供了一些基础的方法（比如通过索引，或者扫描表）来获取表数据，而做连接、计算等功能，是在 MySQL 的执行引擎中完成的。好了，现在你就已经大致知道了，一条 SQL 语句从解析到执行的完整过程。但我们知道，普通的高级语言在做完优化后，生成机器码，这样性能更高。那么，是否可以把 SQL 语句编译成机器码，从而获得更高的性能呢？ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:14:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"能否把 SQL 语句编译成机器码？ MySQL 编译器在执行 SQL 语句的过程中，除了查找数据、做表间连接等数据密集型的操作以外，其实还有一些地方是需要计算的。比如： where 条件：对每一行扫描到的数据都需要执行一次。计算列：有的列是需要计算出来的。聚合函数：像 sum、max、min 等函数，也是要对每一行数据做一次计算。 在研究 MySQL 的过程中，你会发现上述计算都是解释执行的。MySQL 做解释执行的方式，基本上就是深度遍历 AST。比如，你可以对代表 where 条件的 Item 求值，它会去调用它的下级节点做递归的计算。这种计算过程和其他解释执行的语言差不多，都是要在运行时判断数据的类型，进行必要的类型转换，最后执行特定的运算。因为很多的判断都要在运行时去做，所以程序运行的性能比较低。 另外，由于 MySQL 采用的是解释执行机制，所以它在语义分析阶段，其实也没有做类型检查。在编译过程中，不同类型的数据在运算的时候，会自动进行类型转换。比如，执行“select'2’ + 3”，MySQL 会输出 5，这里就把字符串'2’转换成了整数。 那么，我们能否把这些计算功能编译成本地代码呢？因为我们在编译期就知道每个字段的数据类型了，所以编译器其实是可以根据这些类型信息，生成优化的代码，从而提升 SQL 的执行效率。这种思路理论上是可行的。不过，目前我还没有看到 MySQL 在这方面的工作，而是发现了另一个数据库系统 PostgreSQL，做了这方面的优化。PostgreSQL 的团队发现，如果解释执行下面的语句，表达式计算所用的时间，占到了处理一行记录所需时间的 56%。而基于 LLVM 实现 JIT 以后（编译成机器码执行），所用的时间只占到总执行时间的 6%，这就使得 SQL 执行的整体性能整整提高了一倍。 select count(*) from table_name where (x + y) \u003e 100 中国用户对 MySQL 的用量这么大，如果能做到上述的优化，那么仅仅因此而减少的碳排放，就是一个很大的成绩！所以，你如果有兴趣的话，其实可以在这方面尝试一下！ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:14:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 这一讲我们分析了 MySQL 做语义分析、优化和执行的原理，并探讨了一下能否把 SQL 编译成本地代码的问题。你要记住以下这些要点：MySQL 也会做上下文分析，并生成能够代表 SQL 语句的内部数据结构；MySQL 做引用消解要基于数据库模式信息，并且也支持作用域；MySQL 会采用常数传播等优化方法，来优化查询条件，并且要通过查询优化算法，形成高效的执行计划；把 SQL 语句编译成机器码，会进一步提升数据库的性能，并降低能耗。 我把相应的知识点总结成了思维导图，供你参考： 总结这两讲对 MySQL 所采用的编译技术介绍，你会发现这样几个很有意思的地方：第一，编译技术确实在数据库系统中扮演了很重要的作用。第二，数据库编译 SQL 语句的过程与高级语言有很大的相似性，它们都包含了词法分析、语法分析、语义分析和优化等处理。你对编译技术的了解，能够指导你更快地看懂 MySQL 的运行机制。另外，如果你也要设计类似的系统级软件，这就是一个很好的借鉴。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:14:6","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"现代语言设计篇 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:15:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"27 | 课前导读：学习现代语言设计的正确姿势 对于一门完整的语言来说，编译器只是其中的一部分。它通常还有两个重要的组成部分：一个是运行时，包括内存管理、并发机制、解释器等模块；还有一个是标准库，包含了一些标准的功能，如算术计算、字符串处理、文件读写，等等。 C 语言是 Unix 系统的脚本，COBOL 是大型机的脚本，SQL 是数据库系统的脚本，JavaScript、Java 和 C# 都是浏览器的脚本，Swift 和 Objective-C 是苹果系统的脚本，Kotlin 是 Android 的脚本。让一门语言成为某个流行的技术系统的脚本，为这个生态提供编程支持，就是一种定位很清晰的需求。 作为对比，Go 语言的设计主要是用来编写服务端程序的，那么它的关键特性也是与这个定位相适应。 并发：服务端的软件最重要的一项能力就是支持大量并发任务。Go 在语言设计上把并发能力作为第一等级的语言要素。垃圾收集：由于垃圾收集会造成整个应用程序停下来，所以 Go 语言特别重视降低由于垃圾收集而产生的停顿。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:16:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"32 | 运行时（二）：垃圾收集与语言的特性有关吗？ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"垃圾收集算法概述 垃圾收集主要有标记 - 清除（Mark and Sweep）、标记 - 整理（Mark and Compact）、停止 - 拷贝（Stop and Copy）、引用计数、分代收集、增量收集和并发收集等不同的算法，在这里我简要地和你介绍一下。首先，我们先学习一下什么是内存垃圾。内存垃圾，其实就是一些保存在堆里的、已经无法从程序里访问的对象。我们看一个具体的例子。 在堆中申请一块内存时（比如 Java 中的对象实例），我们会用一个变量指向这块内存。但是，如果给变量赋予一个新的地址，或者当栈桢弹出时，该栈桢的变量全部失效，这时，变量所指向的内存就没用了（如图中的灰色块）。 图 1：A 是内存垃圾 另外，如果 A 对象有一个成员变量指向 C 对象，那么 A 不可达，C 也会不可达，也就失效了。但 D 对象除了被 A 引用，还被 B 引用，仍然是可达的。 图 2：A 和 C 是内存垃圾 那么，所有不可达的内存就是垃圾。所以，垃圾收集的重点就是找到并清除这些垃圾。接下来，我们就看看不同的算法是怎么完成这个任务的。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"标记 - 清除 标记 - 清除算法，是从 GC 根节点出发，顺着对象的引用关系，依次标记可达的对象。这里说的 GC 根节点，包括全局变量、常量、栈里的本地变量、寄存器里的本地变量等。从它们出发，就可以找到所有有用的对象。那么剩下的对象，就是内存垃圾，可以清除掉。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"标记 - 整理 采用标记 - 清除算法，运行时间长了以后，会形成内存碎片。这样在申请内存的时候，可能会失败。 图 3：内存碎片导致内存申请失败 为了避免内存碎片，你可以采用变化后的算法，也就是标记 - 整理算法：在做完标记以后，做一下内存的整理，让存活的对象都移动到一边，消除掉内存碎片。 图 4：内存整理以后，可以更有效地利用内存 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"停止 - 拷贝 停止和拷贝算法把内存分为新旧空间两部分。你需要保持一个堆指针，指向自由空间开始的位置。申请内存时，把堆指针往右移动就行了。 图 5：在旧空间中申请内存 当旧空间内存不够了以后，就会触发垃圾收集。在收集时，会把可达的对象拷贝到新空间，然后把新旧空间互换。 图 6：新旧空间互换 停止 - 拷贝算法，在分配内存的时候，不需要去查找一块合适的空闲内存；在垃圾收集完毕以后，也不需要做内存整理，因此速度是最快的。但它也有缺点，就是总有一半内存是闲置的。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"引用计数 引用计数方法，是在对象里保存该对象被引用的数量。一旦这个引用数为零，那么就可以作为垃圾被收集走。有时候，我们会把引用计数叫做自动引用计数（ARC），并把它作为跟垃圾收集（GC）相对立的一个概念。所以，如果你读到相关的文章，它把 ARC 和 GC 做了对比，也不要吃惊。引用计数实现起来简单，并且可以边运行边做垃圾收集，不需要为了垃圾收集而专门停下程序。可是，它也有缺陷，就是不能处理循环引用（Reference Cycles）的情况。在下图中，四个对象循环引用，但没有 GC 根指向它们。它们已经是垃圾，但计数却都为 1。 图 7：循环引用 另外，由于在程序引用一个对象的前后，都要修改引用计数，并且还有多线程竞争的可能性，所以引用计数法的性能开销比较大。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"分代收集 在程序中，新创建的对象往往会很快死去，比如，你在一个方法中，使用临时变量指向一些新创建的对象，这些对象大多数在退出方法时，就没用了。这些数据叫做新生代。而如果一个对象被扫描多次，发现它还没有成为垃圾，那就会标记它为比较老的时代。这些对象可能 Java 里的静态数据成员，或者调用栈里比较靠近根部的方法所引用的，不会很快成为垃圾。对于新生代对象，可以更频繁地回收。而对于老一代的对象，则回收频率可以低一些。并且，对于不同世代的对象，还可以用不同的回收方法。比如，新生代比较适合复制式收集算法，因为大部分对象会被收集掉，剩下来的不多；而老一代的对象生存周期比较长，拷贝的话代价太大，比较适合标记 - 清除算法，或者标记 - 整理算法。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:6","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"增量收集和并发收集 垃圾收集算法在运行时，通常会把程序停下。因为在垃圾收集的过程中，如果程序继续运行，可能会出错。这种停下整个程序的现象，被形象地称作“停下整个世界（STW）”。可是让程序停下来，会导致系统卡顿，用户的体验感会很不好。一些对实时性要求比较高的系统，根本不可能忍受这种停顿。所以，在自动内存管理领域的一个研究的重点，就是如何缩短这种停顿时间。增量收集和并发收集算法，就是在这方面的有益探索： 增量收集可以每次只完成部分收集工作，没必要一次把活干完，从而减少停顿。并发收集就是在不影响程序执行的情况下，并发地执行垃圾收集工作。 好了，理解了垃圾收集算法的核心原理以后，我们就可以继续去探索各门语言是怎么运用这些算法的了。首先，我们从 Python 的垃圾收集算法学起。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:7","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"Python 与引用计数算法 Python 语言选择的是引用计数的算法。除此之外，Swift 语言和方舟编译器，采用的也是引用计数，所以值得我们重视。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:8","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"Python 的内存管理和垃圾收集机制 首先我们来复习一下 Python 内存管理的特征。在 Python 里，每个数据都是对象，而这些对象又都是在堆上申请的。对比一下，在 C 和 Java 这样的语言里，很多计算可以用本地变量实现，而本地变量是在栈上申请的。这样，你用到一个整数的时候，只占用 4 个字节，而不像 Python 那样有一个对象头的固定开销。栈的优势还包括：不会产生内存碎片，数据的局部性又好，申请和释放的速度又超快。而在堆里申请太多的小对象，则会走向完全的反面：太多次系统调用，性能开销大；产生内存碎片；数据的局部性也比较差。 所以说，Python 的内存管理方案，就决定了它的内存占用大、性能低。这是 Python 内存管理的短板。而为了稍微改善一下这个短板，Python 采用了一套基于区域（Region-based）的内存管理方法，能够让小块的内存管理更高效。简单地说，就是 Python 每次都申请一大块内存，这一大块内存叫做 Arena。当需要较小的内存的时候，直接从 Arena 里划拨就好了，不用一次次地去操作系统申请。当用垃圾回收算法回收内存时，也不一定马上归还给操作系统，而是归还到 Arena 里，然后被循环使用。这个策略能在一定程度上提高内存申请的效率，并且减少内存碎片化。 接下来，我们就看看 Python 是如何做垃圾回收的。回忆一下，在第 19 讲分析 Python 的运行时机制时，其中提到了一些垃圾回收的线索。Python 里每个对象都是一个 PyObject，每个 PyObject 都有一个 ob_refcnt 字段用于记录被引用的数量。 在解释器执行字节码的时候，会根据不同的指令自动增加或者减少 ob_refcnt 的值。当一个 PyObject 对象的 ob_refcnt 的值为 0 的时候，意味着没有任何一个变量引用它，可以立即释放掉，回收对象所占用的内存。 现在你已经知道，采用引用计数方法，需要解决循环引用的问题。那 Python 是如何实现的呢？ Python 在 gc 模块里提供了一个循环检测算法。接下来我们通过一个示例，来看看这个算法的原理。在这个例子中，有一个变量指向对象 A。你能用肉眼看出，对象 A、B、C 不是垃圾，而 D 和 E 是垃圾。 图 8：把容器对象加入待扫描列表 在循环检测算法里，gc 使用了两个列表。一个列表保存所有待扫描的对象，另一个列表保存可能的垃圾对象。注意，这个算法只检测容器对象，比如列表、用户自定义的类的实例等。而像整数对象这样的，就不用检测了，因为它们不可能持有对其他对象的引用，也就不会造成循环引用。在这个算法里，我们首先让一个 gc_ref 变量等于对象的引用数。接着，算法假装去掉对象之间的引用。比如，去掉从 A 到 B 的引用，这使得 B 对象的 gc_ref 值变为了 0。在遍历完整个列表以后，除了 A 对象以外，其他对象的 gc_ref 都变成了 0。 图 9：扫描列表，修改 gc_ref 的值 gc_ref 等于零的对象，有的可能是垃圾对象，比如 D 和 E；但也有些可能不是，比如 B 和 C。那要怎么区分呢？我们先把这些对象都挪到另一个列表中，怀疑它们可能是垃圾。 图 10：认为 gc_ref 为 0 的对象可能是垃圾 这个时候，待扫描对象区只剩下了对象 A。它的 gc_ref 是大于零的，也就是从 gc 根是可到达的，因此肯定不是垃圾对象。那么顺着这个对象所直接引用和间接引用到的对象，也都不是垃圾。而剩下的对象，都是从 gc 根不可到达的，也就是真正的内存垃圾。 图 11：去除其中可达的对象，剩下的是真正的垃圾 另外，基于循环检测的垃圾回收算法是定期执行的，这会跟 Java 等语言的垃圾收集器一样，导致系统的停顿。所以，它也会像 Java 等语言的垃圾收集器一样，采用分代收集的策略来减少垃圾收集的工作量，以及由于垃圾收集导致的停顿。好了，以上就是 Python 的垃圾收集算法。我们前面提过，除了 Python 以外，Swift 和方舟编译器也使用了引用计数算法。另外，还有些分代的垃圾收集器，在处理老一代的对象时，也会采用引用计数的方法，这样就可以在引用计数为零的时候收回内存，而不需要一遍遍地扫描。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:9","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"编译器如何配合引用计数算法？ 对于 Python 来说，引用计数的增加和减少，是由运行时来负责的，编译器并不需要做额外的工作。它只需要生成字节码就行了。而对于 Python 的解释器来说，在把一个对象赋值给一个变量的时候，要把对象的引用数加 1；而当该变量超出作用域的时候，要把对象的引用数减 1。不过，对于编译成机器码的语言来说，就要由编译器介入了。它要负责生成相应的指令，来做引用数的增减。 不过，这只是高度简化的描述。实际实现时，还要解决很多细致的问题。比如，在多线程的环境下，对引用数的改变，必须要用到锁，防止超过一个线程同时修改引用数。这种频繁地对锁的使用，会导致性能的降低。这时候，我们之前学过的一些优化算法就可以派上用场了。比如，编译器可以做一下逃逸分析，对于没有逃逸或者只是参数逃逸的对象，就可以不使用锁，因为这些对象不可能被多个线程访问。这样就可以提高程序的性能。除了通过逃逸分析优化对锁的使用，编译器还可以进一步优化。比如，在一段程序中，一个对象指针被调用者通过参数传递给某个函数使用。在函数调用期间，由于调用者是引用着这个对象的，所以这个对象不会成为垃圾。而这个时候，就可以省略掉进入和退出函数时给对象引用数做增减的代码。还有不少类似上面的情况，需要编译器配合垃圾收集机制，生成高效的、正确的代码。你在研究 Swift 和方舟编译器时，可以多关注一下它们对引用计数做了哪些优化。 接下来，我们再看看其他语言是怎么做垃圾收集的。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:10","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"其他语言是怎么做垃圾收集的？ 除了 Python 以外，我们在第二个模块研究的其他几门语言，包括 Java、JavaScript（V8）和 Julia，都没有采用引用计数算法（除了在分代算法中针对老一代的对象），它们基本都采用了分代收集的策略。针对新生代，通常是采用标记 - 清除或者停止拷贝算法。 它们不采用引用计数的原因，其实我们可以先猜测一下，那就是因为引用计数的缺点。比如增减引用计数所导致的计算量比较多，在多线程的情况下要用到锁，就更是如此；再比如会导致内存碎片化、局部性差等。 而采用像停止 - 拷贝这样的算法，在总的计算开销上会比引用计数的方法低。Java 和 Go 语言主要是用于服务端程序开发的。尽量减少内存收集带来的性能损耗，当然是语言的设计者重点考虑的问题。 再进一步看，采用像停止 - 拷贝这样的算法，其实是用空间换时间，以更大的内存消耗换来性能的提升。如果你的程序需要 100M 内存，那么虚拟机需要给它准备 200M 内存，因为有一半空间是空着的。这其实也是为什么 Android 手机比 iPhone 更加消耗内存的原因之一。 在为 iPhone 开发程序的时候，无论是采用 Objective C 还是 Swift，都是采用引用计数的技术。并且，程序员还负责采用弱引用等技术，来避免循环引用，从而进一步消除了在运行时进行循环引用检测的开销。 通过上面的分析，我们能发现移动端应用和服务端应用有不同的特点，因此也会导致采用不同的垃圾收集算法。那么方舟编译器采用引用计数的方法，来编译原来的 Android 应用，是否也是借鉴了 iPhone 的经验呢？我没有去求证过，所以不得而知。但我们可以根据自己的知识去做一些合理的猜测。 好，回过头来，我们继续分析一下用 Java 和 Go 语言来写服务端程序对垃圾收集的需求。对于服务器端程序来说，垃圾收集导致的停顿，是一个令程序员们头痛的问题。有时候，一次垃圾收集会让整个程序停顿一段非常可观的时间（比如上百毫秒，甚至达到秒级），这对于实时性要求较高或并发量较大的系统来说，就会引起很大的问题。也因此，一些很关键的系统很长时间内无法采用 Java 和 Go 语言编写。 所以，Java 和 Go 语言一直在致力于减少由于垃圾收集而产生的停顿。最新的垃圾收集器，已经使得垃圾收集导致的停顿降低到了几毫秒内。 在这里，你需要理解的要点，是为什么在垃圾收集时，要停下整个程序？又有什么办法可以减少停顿的时间？ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:11","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"为什么在垃圾收集时，要停下整个程序？ 其实，对于引用计数算法来说，是不需要停下整个程序的，每个对象的内存在计数为零的时候就可以收回。 而采用标记 - 清除算法时，你就必须要停下程序：首先做标记，然后做清除。在做标记的时候，你必须从所有的 GC 根出发，去找到所有存活的对象，剩下的才是垃圾。所以，看上去，这是一项完整的工作，程序要一直停顿到这项完整的工作做完。 让事情更棘手的是，你不仅要停下当前的线程，扫描栈里的所有 GC 根，你还要停下其他的线程，因为其他线程栈里的对象，也可能引用了相同的对象。最后的结果，就是你停下了整个世界。 当然也有例外，就是如果别的线程正在运行的代码，没有可能改变对象之间的引用关系，比如仅仅是在做一个耗费时间的数学计算，那么是不用停下来的。你可以参考 Julia 的gc 程序中的一段注释，来理解什么样的代码必须停下来。 更麻烦的是，不仅仅在扫描阶段你需要停下整个世界，如果垃圾收集算法需要做内存的整理或拷贝，那么这个时候仍然要停下程序。而且，程序必须停在一些叫做安全点（SafePoint）的地方。 在这些地方，修改对象的地址不会破坏程序数据的一致性。比如说，假设代码里有一段逻辑，是访问对象的某个成员变量，而这个成员变量的地址是根据对象的地址加上一个偏移量计算出来的。那么如果你修改了对象的地址，而这段代码仍然去访问原来的地址，那就出错了。而当代码停留在安全点上，就不会有这种不一致。 安全点是编译器插入到代码中一个片段。在查看Graal 生成的汇编代码时，我们曾经看到过这样的指令片段。 好了，到目前为止，你了解了为什么要停下整个世界，以及要停在哪里才合适。那么我们继续研究，如何能减少停顿时间。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:12","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"如何能减少停顿时间？ 第一招，分代收集可以减少垃圾收集的工作量，不用每次都去扫描所有的对象，因此也会减少停顿时间。像 Java、Julia 和 V8 的垃圾收集器都是分代的。第二招，可以尝试增量收集。你可能会问了，怎样才能实现增量呀？不是说必须扫描所有的 GC 根，才能确认一个对象是垃圾吗？ 其实是有方法可以实现增量收集的，比如三色标记（Tri-color Marking）法。这种方法的原理，是用三种颜色来表示不同的内存对象的处理阶段： 白色，表示算法还没有访问的对象。灰色，表示这个节点已经被访问过，但子节点还没有被访问过。黑色，表示这个节点已经被访问过，子节点也已经被访问过了（当然，没有子节点也是“子节点被访问过了”）。 我们用一个例子来了解一下这个算法的原理。这个例子中有 8 个对象。你可以看出，其中三个对象是内存垃圾。在垃圾收集的时候，一开始所有对象都是白色的。 然后，扫描所有 GC 根所引用的对象，把这些对象加入到一个工作区，并标记为灰色。在例子中，我们把 A 和 F 放入了灰色区域。 如果这个对象的所有子节点都被访问过之后，就把它标记为黑色。在例子中，A 和 F 已经被标记为黑色，而 B、C、D 被标记为灰色。 继续上面的过程，B、C、D 也被标记为黑色。这个时候，灰色区域已经没有对象了。那么剩下的白色对象 E、G 和 H 就能确定是垃圾了。 回收掉 E、G 和 H 以后，就可以进入下一次循环。重新开始做增量收集。 从上面的原理还可以看出这个算法的特点：黑色对象永远不能指向白色对象，顶多指向灰色对象。我们只要始终保证这一条，就可以去做增量式的收集。 具体来说，垃圾收集器可以做了一段标记工作后，就让程序再运行一段。如果在程序运行期间，一个黑色对象被修改了，比如往一个黑色对象 a 里新存储了一个指针 b，那么把 a 涂成灰色，或者把 b 涂成灰色，就可以了。等所有的灰色节点变为黑色以后，就可以做垃圾清理了。 总结起来，三色标记法中，黑色的节点是已经处理完毕的，灰色的节点是正在处理的。如果灰色节点都处理完，剩下的白色节点就是垃圾。而如果在两次处理的间隙，有黑色对象又被改了，那么要重新处理。 那在增量收集的过程中，需要编译器做什么配合？肯定是需要的，编译器需要往生成的目标代码中插入读屏障（Read Barrier）和写屏障（Write Barrier）的代码。也就是在程序读写对象的时候，要执行一些逻辑，保证三色的正确性。 好了，你已经理解了增量标识的原理，知道了它可以减少程序的整体停顿时间。那么，能否再进一步减少停顿时间呢？ 这就涉及到第三招：并发收集。我们再仔细看上面的增量式收集算法：既然垃圾收集程序和主程序可以交替执行，那么是否可以一边运行主程序，一边用另一个或多个线程来做垃圾收集呢？ 这是可以的。实际上，除了少量的时候需要停下整个程序（比如一开头处理所有的 GC 根），其他时候是可以并发的，这样就进一步减少了总的停顿时间。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:13","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 今天这一讲，我带你了解运行时中的一个重要组成部分：垃圾收集器。采用什么样的垃圾收集算法，是实现一门语言时要着重考虑的点。垃圾收集算法包含的内容有很多，我们这一讲并没有展开所有的内容，而是聚焦在介绍常用的几种算法（比如引用计数、分代收集、增量收集等）的原理，以及几种典型语言的编译器是如何跟选定的垃圾收集算法配合的。比如，在生成目标代码的时候，生成安全点、写屏障和读屏障的代码，修改引用数的代码，以及能够减少垃圾收集工作的一些优化工作。我把今天的知识点做成了思维导图，供你参考： ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:17:14","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"33 | 并发中的编译技术（一）：如何从语言层面支持线程？ 现代的编程语言，开始越来越多地采用并发计算的模式。这也对语言的设计和编译技术提出了要求，需要能够更方便地利用计算机的多核处理能力。并发计算需求的增长跟两个趋势有关：一是，CPU 在制程上的挑战越来越大，逼近物理极限，主频提升也越来越慢，计算能力的提升主要靠核数的增加，比如现在的手机，核数越来越多，动不动就 8 核、12 核，用于服务器的 CPU 核数则更多；二是，现代应用对并发处理的需求越来越高，云计算、人工智能、大数据和 5G 都会吃掉大量的计算量。因此，在现代语言中，友好的并发处理能力是一项重要特性，也就需要编译技术进行相应的配合。现代计算机语言采用了多种并发技术，包括线程、协程、Actor 模式等。我会用三讲来带你了解它们，从而理解编译技术要如何与这些并发计算模式相配合。 这一讲，我们重点探讨线程模式，它是现代计算机语言中支持并发的基础模式。它也是讨论协程和 Actor 等其他话题的基础。不过在此之前，我们需要先了解一下并发计算的一点底层机制：并行与并发、进程和线程。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:18:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"并发的底层机制：并行与并发、进程与线程 我们先来学习一下硬件层面对并行计算的支持。假设你的计算机有两颗 CPU，每颗 CPU 有两个内核，那么在同一时间，至少可以有 4 个程序同时运行。后来 CPU 厂商又发明了超线程（Hyper Threading）技术，让一个内核可以同时执行两个线程，增加对 CPU 内部功能单元的利用率，这有点像我们之前讲过的流水线技术(8讲)。这样一来，在操作系统里就可以虚拟出 8 个内核（或者叫做操作系统线程），在同一时间可以有 8 个程序同时运行。这种真正的同时运行，我们叫做并行（parallelism）。 可是仅仅 8 路并行，也不够用呀。如果你去查看一下自己电脑里的进程数，会发现运行着几十个进程，而线程数就更多了。所以，操作系统会用分时技术，让一个程序执行一段时间，停下来，再让另一个程序运行。由于时间片切得很短，对于每一个程序来说，感觉上似乎一直在运行。这种“同时”能处理多个任务，但实际上并不一定是真正同时执行的，就叫做并发（Concurrency）。 实际上，哪怕我们的计算机只有一个内核，我们也可以实现多个任务的并发执行。这通常是由操作系统的一个调度程序（Scheduler）来实现的。但是有一点，操作系统在调度多个任务的时候，是有一定开销的： 一开始是以进程为单位来做调度，开销比较大。在切换进程的时候，要保存当前进程的上下文，加载下一个进程的上下文，也会有一定的开销。由于进程是一个比较大的单位，其上下文的信息也比较多，包括用户级上下文（程序代码、静态数据、用户堆栈等）、寄存器上下文（各种寄存器的值）和系统级上下文（操作系统中与该进程有关的信息，包括进程控制块、内存管理信息、内核栈等）。 相比于进程，线程技术就要轻量级一些。在一个进程内部，可以有多个线程，每个线程都共享进程的资源，包括内存资源（代码、静态数据、堆）、操作系统资源（如文件描述符、网络连接等）和安全属性（用户 ID 等），但拥有自己的栈和寄存器资源。这样一来，线程的上下文包含的信息比较少，所以切换起来开销就比较小，可以把宝贵的 CPU 时间用于执行用户的任务。 总结起来，线程是操作系统做并发调度的基本单位，并且可以跟同一个进程内的其他线程共享内存等资源。操作系统会让一个线程运行一段时间，然后把它停下来，把它所使用的寄存器保存起来，接着让另一个线程运行，这就是线程调度原理。你要在大脑里记下这个场景，这样对理解后面所探讨的所有并发技术都很有帮助。 图 2：进程的共享资源和线程私有的资源 我们通常把进程作为资源分配的基本单元，而把线程作为并发执行的基本单元。不过，有的时候，用进程作为并发的单元也是比较好的，比如谷歌浏览器每打开一个 Tab 页，就新启动一个进程。这是因为，浏览器中多个进程之间不需要有互动。并且，由于各个进程所使用的资源是独立的，所以一个进程崩溃也不会影响到另一个。 而如果采用线程模型的话，由于它比较轻量级，消耗的资源比较少，所以你可以在一个操作系统上启动几千个线程，这样就能执行更多的并发任务。所以，在一般的网络编程模型中，我们可以针对每个网络连接，都启动一条线程来处理该网络连接上的请求。在第二个模块中我们分析过的MySQL就是这样做的。你每次跟 MySQL 建立连接，它就会启动一条线程来响应你的查询请求。 采用线程模型的话，程序就可以在不同线程之间共享数据。比如，在数据库系统中，如果一个客户端提交了一条 SQL，那么这个 SQL 的编译结果可以被缓存起来。如果另一个用户恰好也执行了同一个 SQL，那么就可以不用再编译一遍，因为两条线程可以访问共享的内存。 但是共享内存也会带来一些问题。当多个线程访问同样的数据的时候，会出现数据处理的错误。如果使用并发程序会造成错误，那当然不是我们所希望的。所以，我们就要采用一定的技术去消除这些错误。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:18:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"java的并发机制 …… ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:18:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"34 | 并发中的编译技术（二）：如何从语言层面支持协程？ 上一讲我们提到了线程模式是当前计算机语言支持并发的主要方式。不过，在有些情况下，线程模式并不能满足要求。当需要运行大量并发任务的时候，线程消耗的内存、线程上下文切换的开销都太大。这就限制了程序所能支持的并发任务的数量。在这个背景下，一个很“古老”的技术重新焕发了青春，这就是协程（Coroutine）。它能以非常低的代价、友好的编程方式支持大量的并发任务。像 Go、Python、Kotlin、C# 等语言都提供了对协程的支持。今天这一讲，我们就来探究一下如何在计算机语言中支持协程的奇妙功能，它与编译技术又是怎样衔接的。首先，我们来认识一下协程。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:19:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"协程（Coroutine）的特点与使用场景 我说协程“古老”，是因为这个概念是在 1958 年被马尔文 · 康威（Melvin Conway）提出来、在 20 世纪 60 年代又被高德纳（Donald Ervin Knuth）总结为两种子过程（Subroutine）的模式之一。一种是我们常见的函数调用的方式，而另一种就是协程。在当时，计算机的性能很低，完全没有现代的多核计算机。而采用协程就能够在这样低的配置上实现并发计算，可见它是多么的轻量级。有的时候，协程又可能被称作绿色线程、纤程等，所采用的技术也各有不同。但总的来说，它们都有一些共同点。 首先，协程占用的资源非常少。你可以在自己的笔记本电脑上随意启动几十万个协程，而如果你启动的是几十万个线程，那结果就是不可想象的。比如，在 JVM 中，缺省会为每个线程分配 1MB 的内存，用于线程栈等。这样的话，几千个线程就要消耗掉几个 GB 的内存，而几十万个线程理论上需要消耗几百 GB 的内存，这还没算程序在堆中需要申请的内存。当然，由于底层操作系统和 Java 应用服务器的限制，你也无法启动这么多线程。 其次，协程是用户自己的程序所控制的并发。也就是说，协程模式，一般是程序交出运行权，之后又被另外的程序唤起继续执行，整个过程完全是由用户程序自己控制的。而线程模式就完全不同了，它是由操作系统中的调度器（Scheduler）来控制的。 我们看个 Python 的例子： py def running_avg(): total = 0.0 count = 0 avg = 0 while True: num = yield avg total += num count += 1 avg = total/count #生成协程，不会有任何输出 ra = running_avg() #运行到yield next(ra) print(ra.send(2)) print(ra.send(3)) print(ra.send(4)) print(ra.send(7)) print(ra.send(9)) print(ra.send(11)) #关掉协程 ra.close 可以看到，使用协程跟我们平常使用函数几乎没啥差别，对编程人员很友好。实际上，它可以认为是跟函数并列的一种子程序形式。和函数的区别是，函数调用时，调用者跟被调用者之间像是一种上下级的关系；而在协程中，调用者跟被调用者更像是互相协作的关系，比如一个是生产者，一个是消费者。这也是“协程”这个名字直观反映出来的含义。我们用一张图来对比下函数和协程中的调用关系。 细想一下，编程的时候，这种需要子程序之间互相协作的场景有很多，我们一起看两种比较常见的场景。 第一种比较典型的场景，就是生产者和消费者模式。如果你用过 Unix 管道或者消息队列编程的话，会非常熟悉这种模式。但那是在多个进程之间的协作。如果用协程的话，在一个进程内部就能实现这种协作，非常轻量级。就拿编译器前端来说，词法分析器（Tokenizer）和语法分析器（Parser）就可以是这样的协作关系。也就是说，为了更好的性能，没有必要一次把词法分析完毕，而是语法分析器消费一个，就让词法分析器生产一个。因此，这个过程就没有必要做成两个线程了，否则就太重量级了。这种场景，我们可以叫做生成器（Generator）场景：主程序调用生成器，给自己提供数据。 特别适合使用协程的第二种场景是 IO 密集型的应用。比如做一个网络爬虫同时执行很多下载任务，或者做一个服务器同时响应很多客户端的请求，这样的任务大部分时间是在等待网络传输。如果用同步阻塞的方式来做，一个下载任务在等待的时候就会把整个线程阻塞掉。而用异步的方式，协程在发起请求之后就把控制权交出，调度程序接收到数据之后再重新激活协程，这样就能高效地完成 IO 操作，同时看上去又是用同步的方式编程，不需要像异步编程那样写一大堆难以阅读的回调逻辑。 这样的场景在微服务架构的应用中很常见，我们来简化一个实际应用场景，分析下如何使用协程。在下面的示例中，应用 A 从客户端接收大量的并发请求，而应用 A 需要访问应用 B 的服务接口，从中获得一些信息，然后返回给客户端。 要满足这样的场景，我们最容易想到的就是，编写同步通讯的程序，其实就是同步调用。假设应用 A 对于每一个客户端的请求，都会起一个线程做处理。而你呢，则在这个线程里发起一个针对应用 B 的请求。在等待网络返回结果的时候，当前线程会被阻塞住。 这个架构是最简单的，你如果采用 Java 的 Servlet 容器来编写程序的话，很可能会采用这个结构。但它有一些缺陷： 对于每个客户端请求，都要起一个线程。如果请求应用 B 的时延比较长，那么在应用 A 里会积压成千上万的线程，从而浪费大量的服务器资源。而且，当线程超过一定数量，应用服务器就会拒绝后续的请求。大量的请求毫无节制地涌向应用 B，使得应用 B 难以承受负载，从而导致响应变慢，甚至宕机。 因为同步调用的这种缺点，近年来异步编程模型得到了更多的应用，典型的就是 Node.js。在异步编程模型中，网络通讯等 IO 操作不必阻塞线程，而是通过回调来让主程序继续执行后续的逻辑。 上图中，我们只用到了 4 个线程，对应操作系统的 4 个真线程，可以减少线程切换的开销。在每个线程里，维护一个任务队列。首先，getDataFromApp2() 会被放到任务队列；当数据返回以后，系统的调度器会把 sendBack() 函数放进任务队列。 这个例子比较简单，只有一层回调，你还能读懂它的逻辑。但是，采用这种异步编程模式，经常会导致多层回调，让代码很难阅读。这种现象，被叫做“回调地狱（Callback Hell）”。 这时候，就显示出协程的优势了。协程可以让你用自己熟悉的命令式编程的风格，来编写异步的程序。比如，对于上面的示例程序，用协程可以这样写，看上去跟编写同步调用的代码没啥区别。 requestHandler(){ ...; await getDataFromApp2(); ...; sendBack(); } 当然，我要强调一下，在协程用于同步和异步编程的时候，其调度机制是不同的。跟异步编程配合的时候，要把异步 IO 机制与协程调度机制关联起来。好了，现在你已经了解了协程的特点和适用场景。那么问题来了，如何让一门语言支持协程呢？要回答这个问题，我们就要先学习一下协程的运行原理。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:19:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"协程的运行原理 当我们使用函数的时候，简单地保持一个调用栈就行了。当 fun1 调用 fun2 的时候，就往栈里增加一个新的栈帧，用于保存 fun2 的本地变量、参数等信息；这个函数执行完毕的时候，fun2 的栈帧会被弹出（恢复栈顶指针 sp），并跳转到返回地址（调用 fun2 的下一条指令），继续执行调用者 fun1 的代码。 但如果调用的是协程 coroutine1，该怎么处理协程的栈帧呢？因为协程并没有执行完，显然还不能把它简单地丢掉。这种情况下，程序可以从堆里申请一块内存，保存协程的活动记录，包括本地变量的值、程序计数器的值（当前执行位置）等等。这样，当下次再激活这个协程的时候，可以在栈帧和寄存器中恢复这些信息。 图 6：调用协程时的控制流和栈桢管理 把活动记录保存到堆里，是不是有些眼熟？其实，这有点像闭包的运行机制。程序在使用闭包的时候，也需要在堆里保存闭包中的自由变量的信息，并且在下一次调用的时候，从堆里恢复。只不过，闭包不需要保存本地变量，只保存自由变量就行了；也不需要保存程序计数器的值，因为再一次调用闭包函数的时候，还是从头执行，而协程则是接着执行 yield 之后的语句。fun1 通过 resume 语句，让协程继续运行。这个时候，协程会去调用一个普通函数 fun2，而 fun2 的栈帧也会加到栈上。 图 7：在协程里调用普通函数时的栈桢情况 如果 fun2 执行完毕，那么就会返回到协程。而协程也会接着执行下一个语句，这个语句是一个专门针对协程的返回语句，我们叫它 co_return 吧，以便区别于传统的 return。在执行了 co_return 以后，协程就结束了，无法再 resume。这样的话，保存在堆里的活动记录也就可以销毁了。 图 8：协程结束时对栈桢的处理 通过上面的例子，你应该已经了解了协程的运行原理。那么我们学习编译原理会关心的问题是：**实现协程的调度，包括协程信息的保存与恢复、指令的跳转，需要编译器的帮忙吗？还是用一个库就可以实现？**实际上，对于 C 和 C++ 这样的语言来说，确实用一个库就可以实现。因为 C 和 C++ 比较灵活，比如可以用 setjmp、longjmp 等函数，跨越函数的边界做指令的跳转。但如果用库实现，通常要由程序管理哪些状态信息需要被保存下来。为此，你可能要专门设计一个类型，来参与实现协程状态信息的维护。而如果用编译器帮忙，那么就可以自动确定需要保存的协程的状态信息，并确定需要申请的内存大小。一个协程和函数的区别，就仅仅在于是否使用了 yield 和 co_return 语句而已，减轻了程序员编程的负担。好了，刚才我们讨论了，在实现协程的时候，要能够正确保存协程的活动记录。在具体实现上，有 Stackful 和 Stackless 两种机制。采用不同的机制，对于协程能支持的特性也很有关系。所以接下来，我带你再进一步地分析一下 Stackful 和 Stackless 这两种机制。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:19:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"Stackful 和 Stackless 的协程 到目前为止，看上去协程跟普通函数（子程序）的差别也不大嘛，你看： 都是由一个主程序调用，运行一段时间以后再把控制流交回给主程序；都使用栈来管理本地变量和参数等信息，只不过协程在没有完全运行完毕时，要用堆来保存活动记录；在协程里也可以调用其他的函数。 可是，在有的情况下，我们没有办法直接在 coroutine1 里确定是否要暂停线程的执行，可能需要在下一级的子程序中来确定。比如说，coroutine1 函数变得太大，我们重构后，把它的功能分配到了几个子程序中。那么暂停协程的功能，也会被分配到子程序中。 图 9：在辅助函数里暂停协程时的控制流和栈桢情况 这个时候，在 helper() 中暂停协程，会让控制流回到 fun1 函数。而当在 fun1 中调用 resume 的时候，控制流应该回到 helper() 函数中 yield 语句的下一条，继续执行。coroutine1() 和 helper() 加在一起，起到了跟原来只有一个 coroutine1() 一样的效果。这个时候，在栈里不仅要加载 helper() 的活动记录，还要加载它的上一级，也就是 coroutine1() 的活动记录，这样才能维护正确的调用顺序。当 helper() 执行完毕的时候，控制流会回到 coroutine1()，继续执行里面的逻辑。在这个场景下，不仅要从堆里恢复多个活动记录，还要维护它们之间的正确顺序。上面的示例中，还只有两级调用。如果存在多级的调用，那就更麻烦了。那么，怎么解决这个技术问题呢？你会发现，其实协程的逐级调用过程，形成了自己的调用栈，这个调用栈需要作为一个整体来使用，不能拆成一个个单独的活动记录。 既然如此，那我们就加入一个辅助的运行栈好了。这个栈通常被叫做 Side Stack。每个协程，都有一个自己专享的协程栈。 图 10：协程的 Side Stack 好了，现在是时候给你介绍两个术语了：这种需要一个辅助的栈来运行协程的机制，叫做 Stackful Coroutine；而在主栈上运行协程的机制，叫做 Stackless Coroutine。 对于 Stackless 的协程来说，只能在顶层的函数里把控制权交回给调用者。如果这个协程调用了其他函数或者协程，必须等它们返回后，才能去执行暂停协程的操作。从这种角度看，Stackless 的特征更像一个函数。而对于 Stackful 的协程来说，可以在协程栈的任意一级，暂停协程的运行。从这个角度看，Stackful 的协程像是一个线程，不管有多少级的调用，随时可以让这个协程暂停，交出控制权。除此之外，我们再仔细去想，因为设计上的不同，Stackless 和 Stackful 的协程其实还会产生其他的差别： Stackless 的协程用的是主线程的栈，也就是说它基本上会被绑定在创建它的线程上了。而 Stackful 的协程，可以从一个线程脱离，附加到另一个线程上。Stackless 的协程的生命周期，一般来说受制于它的创建者的生命周期。而 Stackful 的协程的生命周期，可以超过它的创建者的生命周期。 好了，以上就是对 Stackless 和 Stackful 的协程的概念和区别了。其实，对于协程，我们可能还会听说一种分类方法，就是对称的和非对称的。到目前为止，我们讲到的协程都是非对称的。有一个主程序，而协程像是子程序。主程序和子程序控制程序执行的原语是不同的，一个用于激活协程，另一个用于暂停协程。而对称的协程，相互之间是平等的关系，它们使用相同的原语在协程之间移交控制权。那么，C++、Python、Java、JavaScript、Julia 和 Go 这些常见语言中，哪些是支持协程的？是 Stackless 的 ，还是 Stackful 的？是对称的，还是非对称的？需要编译器做什么配合？ 接下来，我们就一起梳理下。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:19:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"不同语言的协程实现和差异 Python 语言的协程实现 我们前面讲协程的运行原理用的示例程序，就是用 Python 写的。这是 Python 的一种协程的实现，支持的是同步处理，叫做 generator 模式。3.4 版本之后，Python 支持一种异步 IO 的协程模式，采用了 async/await 关键字，能够以同步的语法编写异步程序。总体来说，Python 是一种解释型的语言，而且内部所有成员都是对象，所以实现协程的机制是很简单的，保存协程的执行状态也很容易。只不过，你不可能把 Python 用于像刚才微信那样高并发的场景，因为解释型语言对资源的消耗太高了。尽管如此，在把 Python 当作脚本语言使用的场景中，比如编写网络爬虫，采用它提供的协程加异步编程的机制，还是能够带来很多好处的。 Julia 和 Go 语言的协程实现 Julia 语言的协程机制，跟以上几种语言都不同。它提供的是对称的协程机制。多个协程可以通过 channel 通讯，当从 channel 里取不出信息时，或者 channel 已满不能再发信息时，自然就停下来了。 当我谈到 channel 的时候，熟悉 Go 语言的同学马上就会想到 Goroutine。Goroutine 是 Go 语言的协程机制，也是用 channel 实现协程间的协作的。 我把对 Go 语言协程机制的介绍放在最后，是因为 Goroutine 实在是很强大。我觉得，所有对并发编程有兴趣的同学，都要看一看 Goroutine 的实现机制，都会得到很大的启发。 我的感受是，Goroutine 简直是实现轻量级并发功能的集大成者，几乎考虑到了你能想到的所有因素。介绍 Goroutine 的文章有很多，我就不去重复已有的内容了，你可以看看“How Stacks are Handled in Go”这篇文章。现在，我就顺着本讲的知识点，对 Goroutine 的部分特点做一点介绍。 首先我们来看一下，Goroutine 是 Stackful 还是 Stackless？答案是 Stackful 的。就像我们前面已经总结过的，Stackful 协程的特点主要是两点：**协程的生命周期可以超过其创建者，以及协程可以从一个线程转移到另一个线程。**后者在 Goroutine 里特别有用。当一个协程调用了一个系统功能，导致线程阻塞的时候，那么排在这条线程上的其他 Goroutine 岂不是也要被迫等待？为了避免这种尴尬，Goroutine 的调度程序会把被阻塞的线程上的其他 Goroutine 迁移到其他线程上。 我们讲 libco 的时候还讲过，Stackful 的缺点是要预先分配比较多的内存用作协程的栈空间，比如 libco 要为每个协程分配 128K 的栈。而 Go 语言只需要为每个 Goroutine 分配 2KB 的栈。你可能会问了，万一空间不够了怎么办，不会导致内存访问错误吗？ 不会的。Go 语言的函数在运行的时候，会有一小块序曲代码，用来检查栈空间够不够用。如果不够用，就马上申请新的内存。需要注意的是，像这样的机制，必须有编译器的配合才行，编译器可以为每个函数生成这样的序曲代码。如果你用库来实现协程，就无法实现这样的功能。 通过这个例子，你也可以体会到把某个特性做成语言原生的，以及用库去实现的差别。 我想说的 Go 语言协程机制的第二个特点，就是 channel 机制。channel 提供了 Goroutine 之间互相通讯，从而能够协调行为的机制。Go 语言的运行时保证了在同一个时刻，只有一个 Goroutine 能够读写 channel，这就避免了我们前一讲提到的，用锁来保证多个线程访问共享数据的难题。当然，channel 在底层也采用了锁的机制，毕竟现在不需要程序员去使用这么复杂且容易出错的机制了。 Go 语言协程机制的第三个特点，是关于协程的调度时机。今天这一讲，我们其实看到了两种调度时机：对于 generator 类型的协程，基本上是同步调度的，协程暂停以后，控制立即就回到主程序；第二个调度机制，是跟异步 IO 机制配合。 而我关心的，是能否实现像线程那样的抢占式（preemptive）的调度。操作系统的线程调度器，在进行调度的时候，可以不管当前线程运行到了什么位置，直接中断它的运行，并把相关的寄存器的值保存下来，然后去运行另一个线程。这种抢占式的调度的一个最大的好处，是不会让某个程序霸占 CPU 资源不放，而是公平地分配给各个程序。而协程也存在着类似的问题。如果一个协程长时间运行，那么排在这条线程上的其他协程，就被剥夺了运行的机会。 Goroutine 在解决这个问题上也做了一些努力。比如，在下面的示例程序中，foo 函数中的循环会一直运行。这时候，编译器就可以在 bar() 函数的序曲中，插入一些代码，检查当前协程是否运行时间太久，从而主动让出控制权。不过，如果 bar() 函数被内联了，处理方式就要有所变化。但总的来说，由于有编译器的参与，这种类似抢占的逻辑是可以实现的。 func foo(){ while true{ bar(); //可以在bar函数的序曲中做检查。 } } 在 Goroutine 实现了各种丰富的调度机制以后，它已经变得不完全由用户的程序来主导协程的调度了，而是能够更加智能、更加优化地实现协程的调度，由操作系统的线程调度器、Go 语言的调度器和用户程序三者配合实现。这也是 Go 语言的一个重要优点。 那么，我们从 C、C++、Python、Java、JavaScript、Julia 和 Go 语言中，就能总结出协程实现上的特点了： 除了 Julia 和 Go，其他语言采用的都是非对称的协程机制。Go 语言是采用协程最彻底的。在采用了协程以后，已经不需要用过去的线程。像 C++、Go 这样编译成机器码执行的语言，对协程栈的良好管理，能够大大降低内存占用，增加支持的协程的数量。协程与异步 IO 结合是一个趋势。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:19:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 今天这一讲，我们学习了协程的定义、使用场景、实现原理和不同语言的具体实现机制。我们特别从编译技术的角度，关注了协程对栈的使用机制，看看它与传统的程序有什么不同。在这个过程中，一方面，你会通过今天的课程对协程产生深入的认识；另一方面，你会更加深刻地认识到编译技术是如何跟语言特性的设计和运行时紧密配合的。协程可以用库实现，也可以借助编译技术成为一门语言的原生特性。采用编译技术，能帮助我们自动计算活动记录的大小，实现自己独特的栈管理机制，实现抢占式调度等功能。本讲的思维导图我也放在了下面，供你参考： ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:19:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"35 | 并发中的编译技术（三）：Erlang语言厉害在哪里？ 不论是线程模型、还是协程模型，当涉及到多个线程访问共享数据的时候，都会出现竞争问题，从而需要用到锁。锁会让其他需要访问该数据的线程等待，从而导致系统整体处理能力的降低。并且，编程人员还要特别注意，避免出现死锁。比如，线程 A 持有了锁 x，并且想要获得锁 y；而线程 B 持有了锁 y，想要获得锁 x，结果这两个线程就会互相等待，谁也进行不下去。像数据库这样的系统，检测和消除死锁是一项重要的功能，以防止互相等待的线程越来越多，对数据库操作不响应，并最终崩溃掉。 既然使用锁这么麻烦，那在并发计算中，能否不使用锁呢？这就出现了 Actor 模型。那么，什么是 Actor 模型？为什么它可以不用锁就实现并发？这个并发模型有什么特点？需要编译技术做什么配合？ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:20:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"什么是 Actor 模型？ 在线程和协程模型中，之所以用到锁，是因为两个线程共享了内存，而它们会去修改同一个变量的值。那，如果避免共享内存，是不是就可以消除这个问题了呢？没错，这就是 Actor 模型的特点。Actor 模型是 1973 年由 Carl Hewitt 提出的。在 Actor 模型中，并发的程序之间是不共享内存的。它们通过互相发消息来实现协作，很多个一起协作的 Actor 就构成了一个支持并发计算的系统。我们看一个有三个 Actor 的例子。 你会注意到，每个 Actor 都有一个邮箱，用来接收其他 Actor 发来的消息；每个 Actor 也都可以给其他 Actor 发送消息。这就是 Actor 之间交互的方式。Actor A 给 Actor B 发完消息后就返回，并不会等着 Actor B 处理完毕，所以它们之间的交互是异步的。如果 Actor B 要把结果返回给 A，也是通过发送消息的方式。 这就是 Actor 大致的工作原理了。因为 Actor 之间只是互发消息，没有共享的变量，当然也就不需要用到锁了。 但是，你可能会问：如果不共享内存，能解决传统上需要对资源做竞争性访问的需求吗？比如，卖电影票、卖火车票、秒杀或者转账的场景。我们以卖电影票为例讲解一下。 在用传统的线程或者协程来实现卖电影票功能的时候，对票的状态进行修改，需要用锁的机制实现同步互斥，以保证同一个时间段只有一个线程可以去修改票的状态、把它分配给某个用户，从而避免多个线程同时访问而出现一张票卖给多个人的情况。这种情况下，多个程序是串行执行的，所以系统的性能就很差。 如果用 Actor 模式会怎样呢？ 你可以把电影院的前半个场地和后半个场地的票分别由 Actor B 和 C 负责销售：Actor A 在接收到定前半场座位的请求的时候，就发送给 Actor B，后半场的就发送给 Actor C，Actor B 和 C 依次处理这些请求；如果 Actor B 或 C 接收到的两个信息都想要某个座位，那么针对第二个请求会返回订票失败的消息。 你发现没有？在这个场景中，Actor B 和 C 仍然是顺序处理各个请求。但因为是两个 Actor 并发地处理请求，所以系统整体的性能会提升到原来的两倍。甚至，你可以让每排座位、每个座位都由一个 Actor 负责，使得系统的性能更高。因为在系统中创建一个 Actor 的成本是很低的。Actor 跟协程类似，很轻量级，一台服务器里创建几十万、上百万个 Actor 也没有问题。如果每个 Actor 负责一个座位，那一台服务器也能负责几十万、上百万个座位的销售，也是可以接受的。 当然，实际的场景要比这个复杂，比如一次购买多张相邻的票等，但原理是一样的。用这种架构，可以大大提高并发能力，处理海量订票、秒杀等场景不在话下。其实，我个人比较喜欢 Actor 这种模式，因为它跟现实世界里的分工协作很相似。比如，餐厅里不同岗位的员工，他们通过互相发信息来实现协作，从而并发地服务很多就餐的顾客。分析到这里，我再把 Actor 模式跟你非常熟悉的一个概念，面向对象编程（Object Oriented Programming，OOP）关联起来。你可能会问：Actor 和面向对象怎么还有关联？ 是的。面向对象语言之父阿伦 · 凯伊（Alan Kay），Smalltalk 的发明人，在谈到面向对象时是这样说的：对象应该像生物的细胞，或者是网络上的计算机，它们只能通过消息互相通讯。对我来说 OOP 仅仅意味着消息传递、本地保留和保护以及隐藏状态过程，并且尽量推迟万物之间的绑定关系。 总结起来，Alan 对面向对象的理解，强调消息传递、封装和动态绑定，没有谈多态、继承等。对照这个理解，你会发现 Actor 模式比现有的流行的面向对象编程语言，更加接近面向对象的实现。 无论如何，通过把 Actor 和你熟悉的面向对象做关联，我相信能够拉近你跟 Actor 之间的距离，甚至会引发你以新的视角来审视目前流行的面向对象范式。 好了，到现在，你可以说是对 Actor 模型比较熟悉了，也可以这么理解：Actor 有点像面向对象程序里的对象，里面可以封装一些数据和算法；但你不能调用它的方法，只能给它发消息，它会异步地、并发地处理这些消息。 但是，你可能会提出一个疑问：Actor 模式不用锁的机制就能实现并发程序之间的协作，这一点很好，那么它有没有什么缺点呢？ 我们知道，任何设计方案都是一种取舍。一个方案有某方面的优势，可能就会有其他方面的劣势。采用 Actor 模式，会有两方面的问题。 第一，由于 Actor 之间不共享任何数据，因此不仅增加了数据复制的时间，还增加了内存占用量。但这也不完全是缺点：一方面，你可以通过在编写程序时，尽量降低消息对象的大小，从而减少数据复制导致的开销；另一方面，消息传递的方式对于本机的 Actor 和集群中的 Actor 是一样的，这就使得编写分布式的云端应用更简单，从而在云计算时代可以获得更好的应用。 第二，基于消息的并发机制，基本上是采用异步的编程模式，这就和通常程序的编程风格有很大的不同。你发出一个消息，并不会马上得到结果，而要等待另一个 Actor 发送消息回来。这对于习惯于编写同步代码的同学，可能是一个挑战。 好了，我们已经讨论了 Actor 机制的特点。接下来我们再看看，什么语言和框架实现了 Actor 模式。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:20:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"支持 Actor 模型的语言和框架 支持 Actor 的最有名的语言是 Erlang。Erlang 是爱立信公司发明的，它的正式版本是在 1987 年发布，其核心设计者是乔 · 阿姆斯特朗（Joe Armstrong），最早是用于开发电信领域的软件系统。在 Erlang 中，每个 Actor 叫作一个进程（Process）。但这个“进程”其实不是操作系统意义上的进程，而是 Erlang 运行时的并发调度单位。Erlang 有两个显著的优点：首先，对并发的支持非常好，所以它也被叫做面向并发的编程语言（COP）。第二，用 Erlang 可以编写高可靠性的软件，可以达到 9 个 9。这两个优点都与 Actor 模式有关： Erlang 的软件由很多 Actor 构成；这些 Actor 可以分布在多台机器上，相互之间的通讯跟在同一台机器上没有区别；某个 Actor 甚至机器出现故障，都不影响整体系统，可以在其他机器上重新启动该 Actor；Actor 的代码可以在运行时更新。 所以，由 Actor 构成的系统真的像一个生命体，每个 Actor 像一个细胞。细胞可以有新陈代谢，而生命体却一直存在。可以说，用 Erlang 编写的基于 Actor 模式的软件，非常好地体现了复杂系统的精髓。到这里，你是不是就能解释“Erlang 语言厉害在哪里”这个问题了。鉴于 Actor 为 Erlang 带来的并发能力和高可靠性，有一些比较流行的开源系统就是用 Erlang 编写的。比如，消息队列系统 RabbitMQ、分布式的文档数据库系统 CouchDB，都很好地体现了 Erlang 的并发能力和健壮性。除了 Erlang 以外，Scala 语言也提供了对 Actor 的支持，它是通过 Akka 库实现的，运行在 JVM 上。我还关注了微软的一个 Orleans 项目，它在.NET 平台上支持 Actor 模式，并进一步做了一些有趣的创新。那接下来我们继续探讨一下，这些语言和框架是如何实现 Actor 机制的，以及需要编译器做什么配合。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:20:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"Actor 模型的实现 在上一讲研究过协程的实现机制以后，我们现在再分析 Actor 的实现机制时，其实就应该会把握要点了。比如说，我们会去看它的调度机制和内存管理机制等。鉴于 Erlang 算是支持 Actor 的最有名、使用最多的语言，接下来我会以 Erlang 的实现机制带你学习 Actor 机制是如何实现的。首先，我们知道，肯定要有个调度器，把海量的 Actor 在多个线程上调度。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:20:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"并发调度机制 那我们需要细究一下：对于 Actor，该如何做调度呢？什么时候把一个 Actor 停下，让另一个 Actor 运行呢？协程也好，Actor 也好，都是在应用级做调度，而不是像线程那样，在应用完全不知道的情况下，就被操作系统调度了。对于协程，我们是通过一些像 yield 这样的特殊语句，触发调度机制。那，Actor 在什么时候调度比较好呢？前面我们也讲过了，Actor 的运行规律，是每次从邮箱取一条消息并进行处理。那么，我们自然会想到，一个可选的调度时机，就是让 Actor 每处理完一条消息，就暂停一下，让别的 Actor 有机会运行。当然，如果处理一条消息所花费的时间太短，比如有的消息是可以被忽略的，那么处理多条消息，累积到一定时间再去调度也行。了解了调度时机，我们再挑战第二个比较难的话题：如果处理一条消息就要花费很长时间怎么办呢？能否实现抢占式的调度呢，就像 Goroutine 那样？ 当然可以，但这个时候就肯定需要编译器和运行时的配合了。Erlang 的运行机制，是基于一个寄存器机解释执行。这使得调度器可以在合适的时机，去停下某个 Actor 的运行，调度其他 Actor 过来运行。Erlang 做抢占式调度的机制是对 Reduction 做计数，Reduction 可以看作是占时不长的一小块工作量。如果某个 Actor 运行了比较多的 Reduction，那就可以对它做调度，从而提供了软实时的能力（具体可以参考这篇文章）。 在比较新的版本中，Erlang 也加入了编译成本地代码的特性，那么在生成的本地代码中，也需要编译器加入对 Reduction 计数的代码，这就有点像 Goroutine 了。这也是 Erlang 和 Scala/Akka 的区别。Akka 没有得到编译器和 JVM 在底层的支持，也就没办法实现抢占式的调度。这有可能让某些特别耗时的 Actor 影响了其他 Actor，使得系统的响应时间不稳定。最后一个涉及调度的话题，是 I/O 与调度的关系。这个关系如果处理得不好，那么对系统整体的性能影响会很大。通常我们编写 I/O 功能时，会采用同步编程模式来获取数据。这个时候，操作系统会阻塞当前的线程，直到成功获取了数据以后，才可以继续执行。 getSomeData(); //操作系统会阻塞住线程，直到获得了数据。 do something else //继续执行 采用这种模式开发一个服务端程序，会导致大量线程被阻塞住，等待 I/O 的结果。由于每个线程都需要不少的内存，并且线程切换的成本也比较高，因此就导致一台服务器能够服务的客户端数量大大降低。如果这时候，你在运行时查看服务程序的状态，就会发现大量线程在等待，CPU 利用率也不高，而新的客户端又连接不上来，造成服务器资源的浪费。并且，如果采用协程等应用级的并发机制，一个线程被阻塞以后，排在这个线程上的其他协程也只能等待，从而导致服务响应时间变得不可靠，有时快，有时慢。我们在前一讲了解过 Goroutine 的调度器。它在遇到这种情况的时候，就会把这条线程上的其他 Goroutine 挪到没被阻塞的线程上，从而尽快得到运行机会。 由于阻塞式 I/O 的缺点，现在很多语言也提供了非阻塞 I/O 的机制。在这种机制下，程序在做 I/O 请求的时候并不能马上获得数据。当操作系统准备好数据以后，应用程序可以通过轮询或被回调的方式获取数据。Node.js 就是采用这种 I/O 模式的典型代表。上一讲提到的 C++ 协程库 libco，也把非阻塞的网络通讯机制和协程机制做了一个很好的整合，大大增加了系统的整体性能。而 Erlang 在很早以前就解决了这个问题。在 Erlang 的最底层，所有的 I/O 都是用事件驱动的方式来实现的。系统收到了一块数据，就调用应用来处理，整个过程都是非阻塞的。说完了并发调度机制，我们再来看看运行时的另一个重要特征，内存管理机制。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:20:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"内存管理机制 内存管理机制要考虑栈、堆都怎么设计，以及垃圾收集机制等内容。 图 3：Erlang 的内存模型 首先说栈。每个 Actor 也需要有自己的栈空间，在执行 Actor 里面的逻辑的时候，用于保存本地变量。这跟上一节讲过的 Stateful 的协程很像。再来看看堆。Erlang 的堆与其他语言有很大的区别，它的每个 Actor 都有自己的堆空间，而不是像其他编程模型那样，不同的线程共享堆空间。这也很容易理解，因为 Actor 模型的特点，就是并发的程序之间没有共享的内存，所以当然也就不需要共享的堆了。再进一步，由于每个 Actor 都有自己的堆，因此会给垃圾收集带来很大的便利： 因为整个程序划分成了很多个 Actor，每个 Actor 都有自己的堆，所以每个 Actor 的垃圾都比较少，不用一次回收整个应用的垃圾，所以回收速度会很快。由于没有共享内存，所以垃圾收集器不需要停下整个应用，而只需要停下被收集的 Actor。这就避免了“停下整个世界（STW）”问题，而这个问题是 Java、Go 等语言面临的重大技术挑战。如果一个 Actor 的生命周期结束，那么它占用的内存会被马上释放掉。这意味着，对于有些生命周期比较短的 Actor 来说，可能压根儿都不需要做垃圾收集。 好了，基于 Erlang，我们学习了 Actor 的运行时机制的两个重要特征：一是并发调度机制，二是内存管理机制。那么，与此相配合，需要编译器做什么工作呢？ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:20:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"编译器的配合工作 我们说过，Erlang 首先是解释执行的，是用一个寄存器机来运行字节码。那么，编译器的任务，就是生成正确的字节码。之前我们已经分别研究过 Graal、Python 和 V8 Ignition 的字节码了。我们知道，字节码的设计很大程度上体现了语言的设计特点，体现了与运行时的交互过程。Erlang 的字节码设计当然也是如此。比如，针对消息的发送和接收，它专门提供了 send 指令和 receive 指令，这体现了 Erlang 的并发特征。再比如，Erlang 还提供了与内存管理有关的指令，比如分配一个新的栈桢等，体现了 Erlang 在内存管理上的特点。 不过，我们知道，仅仅以字节码的方式解释执行，不能满足计算密集型的需求。所以，Erlang 也正在努力提供编译成机器码运行的特性，这也需要编译器的支持。那你可以想象出，生成的机器码，一定也会跟运行时配合，来实现 Erlang 特有的并发机制和内存管理机制。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:20:6","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 今天这一讲，我们介绍了另一种并发模型：Actor 模型。Actor 模型的特点，是避免在并发的程序之间共享任何信息，从而程序就不需要使用锁机制来保证数据的一致性。但是，采用 Actor 机制也会因为数据拷贝导致更大的开销，并且你需要习惯异步的编程风格。Erlang 是实现 Actor 机制的典型代表。它被称为面向并发的编程语言，并且能够提供很高的可靠性。这都源于它善用了 Actor 的特点：由 Actor 构成的系统更像一个生命体一般的复杂系统。在实现 Actor 模型的时候，你要在运行时里实现独特的调度机制和内存管理机制，这些也需要编译器的支持。本讲的思维导图我也放在了下面，供你参考： 好了，今天这一讲加上第 33和34 讲，我们用了三讲，介绍了不同计算机语言是如何实现并发机制的。不难看出，并发机制确实是计算机语言设计中的一个重点。不同的并发机制，会非常深刻地影响计算机语言的运行时的实现，以及所采用的编译技术。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:20:7","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"37 | 高级特性（二）：揭秘泛型编程的实现机制 对泛型的支持，是现代语言中的一个重要特性。它能有效地降低程序员编程的工作量，避免重复造轮子，写很多雷同的代码。像 C++、Java、Scala、Kotlin、Swift 和 Julia 这些语言都支持泛型。至于 Go 语言，它的开发团队也对泛型技术方案讨论了很久，并可能会在 2021 年的版本中正式支持泛型。可见，泛型真的是成为各种强类型语言的必备特性了。那么，泛型有哪些特点？在设计和实现上有哪些不同的方案？编译器应该进行什么样的配合呢？今天这一讲，我就带你一起探讨泛型的实现原理，借此加深你对编译原理相关知识点的认知，让你能够在自己的编程中更好地使用泛型技术。首先，我们来了解一下什么是泛型。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:21:0","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"什么是泛型？ 在日常编程中，我们经常会遇到一些代码逻辑，它们除了类型不同，其他逻辑是完全一样的。你可以看一下这段示例代码，里面有两个类，其中一个类是保存 Integer 的列表，另一个类是保存 Student 对象的列表。 public class IntegerList{ List data = new ArrayList(); public void add(Integer elem){ data.add(elem); } public Integer get(int index){ return (Integer) data.get(index); } } public class StudentList{ List data = new ArrayList(); public void add(Student elem){ data.add(elem); } public Student get(int index){ return (Student) data.get(index); } } 我们都知道，程序员是很不喜欢重复的代码的。像上面这样的代码，如果要为每种类型都重新写一遍，简直会把人逼疯！泛型的典型用途是针对集合类型，能够更简单地保存各种类型的数据，比如 List、Map 这些。在 Java 语言里，如果用通用的集合类来保存特定类型的对象，就要做很多强制转换工作。而且，我们还要小心地做类型检查。比如： List strList = new ArrayList(); //字符串列表 strList.add(\"Richard\"); String name = (String)strList.get(i); //类型转换 for (Object obj in strList){ String str = (String)obj; //类型转换 ... } strList.add(Integer.valueOf(1)); //类型错误 而 Java 里的泛型功能，就能完全消除这些麻烦工作，让程序更整洁，并且也可以减少出错机会。 List\u003cString\u003e strList = new ArrayList\u003cString\u003e(); //字符串列表 strList.add(\"Richard\"); String name = strList.get(i); //类型转换 for (String str in strList){ //无需类型转换 ... } strList.add(Integer.valueOf(1)); //编译器报错 像示例程序里用到的List\u003c String\u003e，是在常规的类型后面加了一个参数，使得这个列表变成了专门存储字符串的列表。如果你再查看一下 List 和 ArrayList 的源代码，会发现它们比普通的接口和类的声明多了一个类型参数\u003c E\u003e，而这个参数可以用在接口和方法的内部所有需要类型的地方：变量的声明、方法的参数和返回值、类所实现的接口，等等。 public interface List\u003cE\u003e extends Collection\u003cE\u003e{ E get(int index); boolean add(E e); ... } 所以说，泛型就是把类型作为参数，出现在类 / 接口 / 结构体、方法 / 函数和变量的声明中。由于类型是作为参数出现的，因此泛型也被称作参数化类型。参数化类型还可以用于更复杂的情况。比如，你可以使用 1 个以上的类型参数，像 Map 就可以使用两个类型参数，一个是 key 的类型（K），一个是 value 的类型（V）。 public interface Map\u003cK,V\u003e { ... } 另外，你还可以对类型参数添加约束条件。比如，你可以要求类型参数必须是某个类型的子类，这是指定了上界（Upper Bound）；你还可以要求类型参数必须是某个类型的一个父类，这是指定了下界（Lower Bound）。实际上，从语言设计的角度来看，你可以对参数施加很多可能的约束条件，比如必须是几个类型之一，等等。**基于泛型的程序，由于传入的参数不同，程序会实现不同的功能。这也被叫做一种多态现象，叫做参数化多态（Parametric Polymorphism）。**它跟面向对象中的多态一样，都能让我们编写更加通用的程序。 好了，现在我们已经了解了泛型的含义了。那么，它们是如何在语言中实现的呢？需要用到什么编译技术？ ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:21:1","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"泛型的实现 接下来，我们一起来看一看几种有代表性的语言实现泛型的技术，包括 Java、C#、C++ 等。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:21:2","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"类型擦除技术 在 Java 里，泛型是通过类型擦除（Type Erasure）技术来实现的。前面在分析Java 编译器时，你就能发现，其实类型参数只存在于编译过程中，用于做类型检查和类型推断。在此之后，这些类型信息就可以被擦除。ArrayList 和ArrayList\u003c String\u003e对应的字节码是一样的，在运行时没有任何区别。所以，我们可以说，在 Java 语言里，泛型其实是一种语法糖，有助于减少程序员的编程负担，并能提供额外的类型检查功能。除了 Java 以外，其他基于 JVM 的语言，比如 Scala 和 Kotlin，其泛型机制基本上都是类型擦除技术。 类型擦除技术的优点是实现起来特别简单。运用我们学过的属性计算、类型检查和推断等相关技术基本就够用了。不过类型擦除技术也有一定的局限性。 问题之一，是它只能适用于引用类型，也就是对象，而不适用于值类型，也就是 Java 中的基础数据类型（Primitive Type）。比如，你不能声明一个List，来保存单纯的整型数据，你在列表里只能保存对象化的 Integer。而我们学习过 Java 对象的内存模型，知道一个 Integer 对象所占的内存，是一个 int 型基础数据的好几倍，因为对象头要有十几个字节的固定开销。再加上由此引起的对象创建和垃圾收集的性能开销，导致用 Java 的集合对象来保存大量的整型、浮点型等基础数据是非常不划算的。我们在这种情况下，还是要退回到使用数组才行。 问题之二，就是因为类型信息在编译期被擦除了，所以程序无法在运行时使用这些类型信息。比如，在下面的示例代码中，如果你想要根据传入的类型 T 创建一个新实例，就会导致编译错误。 public static \u003cT\u003e void append(ArrayList\u003cT\u003e a) { T b= new T(); // 编译错误 a.add(b); } 同样，由于在运行期没有类型信息，所以如果要用反射机制来调用程序的时候，我们也没有办法像在编译期那样进行类型检查。所以，你完全可以往一个旨在保存 String 的列表里添加一个 Interger 对象。而缺少类型检查，可能会导致程序在执行过程中出错。另外，还有一些由于类型擦除而引起的问题。比如，在使用参数化类型的情况下，方法的重载（Overload）会失败。再比如，下面的示例代码中，两个 foo 方法看似参数不同。但如果进行了类型擦除以后，它们就没什么区别，所以是不能共存的。 public void foo(List\u003cInteger\u003e p) { ... } public void foo(List\u003cDouble\u003e p) { ... } 你要注意，不仅仅是 Java 语言的泛型技术有这样的缺点，其他基于 JVM 实现的语言也有类似的缺点（比如没有办法在运行时使用参数化类型的信息）。这其实是由于 JVM 的限制导致的。为了理解这个问题，我们可以看一下基于.NET 平台的语言 ，比如 C# 所采用的泛型技术。C# 使用的不是类型擦除技术，而是一种叫做具体化（reification）的技术。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:21:3","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"具体化技术（Reification） 说起来，C# 语言的设计者，安德斯 · 海尔斯伯格（Anders Hejlsberg），是一位令人尊敬的传奇人物。像我这一代的程序员，差不多都使用过他在 DOS 操作系统上设计的 Pascal 编译器。后来他在此基础上，设计出了 Delphi，也是深受很多人喜爱的一款开发工具。出于对语言设计者的敬佩，虽然我自己从没用 C# 写过程序，但我从来没有低估过 C# 的技术。在泛型方面，C# 的技术方案成功地避免了 Java 泛型的那些缺点。C# 语言编译也会形成 IR，然后在.NET 平台上运行。在 C# 语言中，对应于 Java 字节码的 IR 被叫做 IL，是中间语言（Intermediate Language）的缩写。 我们知道了，在 Java 的泛型实现中，编译完毕以后类型信息就会被擦除。而在 C# 生成的 IL 中，则保留了类型参数的类型信息。所以，List\u003c Student\u003e和List\u003c Teacher\u003e是两个完全不同的类型。也因为 IL 保存了类型信息，因此我们可以在运行时使用这些类型信息，比如根据类型参数创建对象；而且如果通过反射机制来运行 C# 程序的话，也会进行类型检查。还有很重要的一点，就是 C# 的泛型能够支持值类型，比如基础的整型、浮点型数据；再比如，针对List\u003c int\u003e和List\u003c long\u003e，C# 的泛型能够真的生成一份完全不同的可运行的代码。它也不需要把值类型转换成对象，从而导致额外的内存开销和性能开销。 把参数化类型变成实际的类型的过程，是在运行时通过 JIT 技术实现的。这就是具体化（Reification）的含义。把一个参数化的类型，变成一个运行时真实存在的类型，它可以跟非参数化的类型起到完全相同的作用。不过，为了支持泛型，其实.NET 扩展了 C# 生成的 IL，以便在 IL 里能够记录参数化类型信息。而 JVM 则没有改变它的字节码，从而完全是靠编译器来处理泛型。好了，现在我们已经见识到了两种不同的泛型实现机制。还有一种泛型实现机制，也是经常被拿来比较的，这就是 C++ 的泛型机制，它的泛型机制依托的是模板元编程技术。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:21:4","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"基于元编程技术来支持泛型 在上一讲，我们介绍过 C++ 的模板元编程技术。模板元编程很强大，程序里的很多要素都可以模板化，那么类型其实也可以被模板化。你已经知道，元编程技术是把程序本身作为处理对象的。采用 C++ 的模板元编程技术，我们实际上是为每一种类型参数都生成了新的程序，编译后生成的目标代码也是不同的。 所以，C++ 的模板技术也能做到 Java 的类型擦除技术所做不到的事情，比如提供对基础数据类型的支持。在 C++ 的标准模板库（STL）中，提供了很多容器类型。它们能像保存对象一样保存像整型、浮点型这样的基础数据类型。不过使用模板技术来实现泛型也有一些缺点。因为本质上，模板技术有点像宏，它是把程序中某些部分进行替换，来生成新的程序。在这个过程中，它并不会检查针对参数类型执行的某些操作是否是合法的。编译器只会针对生成后的程序做检查，并报错。这个时候，错误信息往往是比较模糊的，不太容易定位。这也是模板元编程技术固有的短板。究其原因，是模板技术不是单单为了泛型的目的而实现的。不过，如果了解了泛型机制的原理，你会发现，其实可以通过增强 C++ 编译器，来提升它的类型检查能力。甚至，对类型参数指定上界和下界等约束条件，也是可以的。不过这要看 C++ 标准委员会的决定了。总的来说，C++ 的泛型技术像 Java 的一样，都是在运行期之前就完成了所有的工作，而不像.NET 那样，在运行期针对某个参数化的类型产生具体的本地代码。好了，了解了泛型的几种实现策略以后，接下来，我们接着讨论一个更深入的话题：把类型参数化以后，对于计算机语言的类型系统有什么挑战？这个问题很重要，因为在语义分析阶段，我们已经知道如何做普通类型的分析和处理。而要处理参数化的类型，我们还必须更加清楚支持参数化以后，类型体系会有什么变化。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:21:5","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"泛型对类型系统的增强 在现代语言中，通常会建立一个层次化的类型系统，其中一些类型是另一些类型的子类型。什么是子类型呢？就是在任何一个用到父类型的地方，都可以用其子类型进行替换。比如，Cat 是 Animal 的子类型，在任何用到 Animal 的地方，都可以用 Cat 来代替。不过，当类型可以带有参数之后，类型之间的关系就变得复杂了。比如说： Collection\u003c Cat\u003e和List\u003c Cat\u003e是什么关系呢？ List\u003c Animal\u003e和List\u003c Cat\u003e之间又是什么关系呢？ 对于第一种情况，其实它们的类型参数是一样的，都是 Cat。而 List 本来是 Collection 的子类型，那么List\u003cCat\u003e也是Collection\u003cCat\u003e的子类型，我们永远可以用List\u003cCat\u003e来替换Collection\u003cCat\u003e。这种情况比较简单。 但是对于第二种情况，List\u003cCat\u003e是否是List\u003cAnimal\u003e的子类型呢？这个问题就比较难了。不同语言的实现是不一样的。在 Java、Julia 等语言中，List\u003cCat\u003e和List\u003cAnimal\u003e之间没有任何的关系。 在由多个类型复合而形成的类型中（比如泛型），复合类型之间的关系随其中的成员类型的关系而变化的方式，分为不变（Invariance）、协变（Covariance）和逆变（Contravariance）三种情况。理解清楚这三种变化，对于我们理解引入泛型后的类型体系非常重要，这也是编译器进行正确的类型计算的基础。首先说说不变。在 Java 语言中，List和List之间并没有什么关系，在下面的示例代码中，如果我们把List赋值给List，编译器会报错。因此，我们说List基于 T 是不变的。 List\u003cCat\u003e catList = new ArrayList\u003c\u003e(); List\u003cAnimal\u003e animalList = catList; //报错，不是子类型 那么协变是什么呢？就是复合类型的变化方向，跟成员类型是相同的。我给你举两个在 Java 语言中关于协变的例子。第一个例子。假设 Animal 有个 reproduce() 方法，也就是繁殖。而 Cat 覆盖（Override）了这个方法，但这个方法的返回值是 Cat 而不是 Animal。因为猫肯定繁殖出的是小猫，而不是其他动物。这样，当我们调用 Cat.reproduce() 方法的时候，就不用对其返回值做强制转换。这个时候，我们说 reproduce() 方法的返回值与它所在类的类型，是协变的，也就是一起变化。 class Animal{ public abstract Animal reproduce(); } class Cat extends Animal{ @Override public Cat reproduce() { //方法的返回值可以是Animal的子类型 ... } } 第二个例子。在 Java 语言中，数组是协变的。也就是Cat[]其实是Animal[]的子类型，在下面的示例代码中，一个猫的数组可以赋值给一个动物数组。 Cat[] cats = {new Cat(), new Cat()}; //创建Cat数组 Animal[] animals = cats; //赋值给Animal数组 animals[0] = new Dog(); //修改第一个元素的值 Cat aCat = cats[0]; //运行时错误 但你接下来会看到，Animal 数组中的值可以被修改为 Dog，这会导致 Cat 数组中的元素类型错误。至于为什么 Java 语言要把数组设计为协变的，以及由此导致的一些问题，我们暂且不管它。我们要问的是，List这样的泛型可以变成协变关系吗？答案是可以的。我前面也提过，我们可以在类型参数中指定上界。List是List\u003c? Extends Animal\u003e的子类型，List\u003c? Extends Animal\u003e的意思，是任何以 Animal 为祖先的子类。我们可以把一个List赋值给List\u003c? Extends Animal\u003e。你可以看一下示例代码： List\u003cCat\u003e catList = new ArrayList\u003c\u003e(); List\u003c? extends Animal\u003e animalList = catList; //子类型 catList.add(new Cat()); Animal animal = animalList.get(0); 实际上，不仅仅List是List\u003c? extends Animal\u003e的子类型，连List也是List\u003c? extends Animal\u003e的子类型。你可以自己测试一下。 我们再来说说逆变。逆变的意思是：虽然 Cat 是 Animal 的子类型，但包含了 Cat 的复合类型，竟然是包含了 Animal 的复合类型的父类型！它们颠倒过来了？这有点违反直觉。在真实世界里有这样的例子吗？当然有。比如，假设有两个函数，getWeight()函数是返回 Cat 的重量，getWeight()函数是返回 Animal 的重量。你知道，从函数式编程的观点，每个函数也都是有类型的。那么这两个函数，谁是谁的子类型呢？实际上，求 Animal 重量的函数，其实是求 Cat 重量的函数的子类型。怎么说呢？来假设一下。如果你想用一个 getTotalWeight() 函数，求一群 Cat 的总重量，你会怎么办呢？你可以把求 Cat 重量的函数作为参数传进去，这肯定没问题。但是，你也可以把求 Animal 重量的函数传进去。因为既然它能返回普通动物的重量，那么也一定能返回猫的重量。 //伪代码，求Cat的总重量 getTotalWeight(List\u003cCat\u003e cats, function fun) 而根据类型理论，如果类型 B 能顶替类型 A 的位置，那么 B 就是 A 的子类型。所以，getWeigh()反倒是getWeight()的子类型，这种情况就叫做逆变。总的来说，加入泛型以后，计算机语言的类型体系变得更加复杂了。我们在编写编译器的时候，一定要弄清楚这些变化关系，这样才能执行正确的类型计算。那么，在了解了加入泛型以后对类型体系的影响后，我们接着借助 Julia 语言，来进一步验证一下如何进行正确的类型计算。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:21:6","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"Julia 中的泛型和类型计算 Julia 设计了一个精巧的类型体系。这个类型体系有着共同的根，也就是 Any。在这个类型层次中，橙色的类型是叶子节点，它们是具体的类型，也就是可以创建具体的实例。而中间层次的节点（蓝色），都是抽象的，主要是用于类型的计算。 Julia的类型体系你在第 22 讲中，已经了解到了 Julia 做函数编译的特点。在编写函数的时候，你可以根本不用指定参数的类型，编译器会根据传入的参数的实际类型，来编译成相应版本的机器码。另外，你也可以为函数编写多个版本的方法，每个版本的参数采用不同的类型。编译器会根据实际参数的类型，动态分派到不同的版本。而这个动态分派机制，就需要用到类型的计算。比如说，有一个函数 foo()，定义了三个版本的方法，其参数分别是没有指定类型（也就是 Any）、Real 类型和 Float64 类型。如果参数是 Float64 类型，那它当然会被分派到第三个方法。如果是 Float32 类型，那么就会被分派到第二个方法。如果是一个字符串类型呢，则会被分派到第一个方法。 julia\u003e function foo(x) #方法1 ... end julia\u003e function foo(x::Real) #方法2 ... end julia\u003e function foo(x::Float64) #方法3 ... end 再进一步，Julia 还支持在定义结构体和函数的时候使用泛型。比如，下面的一个 Point 结构中，坐标 x 和 y 的类型是参数化的。 julia\u003e struct Point{T} x::T y::T end julia\u003e Point{Float64} Point{Float64} julia\u003e Point{Float64} \u003c: Point #在Julia里，如果一个类型更具体，则\u003c:为真 true julia\u003e Point{Float64} \u003c: Point{Real} #Invariant false julia\u003e p1 = Point(1.0,2.3) #创建一个Point实例 Point{Float64}(1.0, 2.3) #自动推断出类型 如果我们再为 foo() 函数添加几个方法，其参数类型分别是 Point 类型、Point{Real}类型和 Point{Float64}类型，那动态分派的算法也必须能够做正确的分派。所以，在这里，我们就必须能够识别出带有参数的类型之间的关系。 julia\u003e function foo(x::Point) #方法4 ... end julia\u003e function foo(x::Point{Real}) #方法5 ... end julia\u003e function foo(x::Point{Float64}) #方法6 ... end 通过以上的示例代码你可以看到，Point{Float64} \u003c: Point，也就是 Point{Float64}是 Point 的子类型。这个关系是有意义的。Julia 的逻辑是，Point{Float64} 比 Point 更具体，能够在程序里替代 Point。而 Point{Float64} 和 Point{Real}之间是没有什么关系的，虽然 Float64 是 Real 的子类型。这说明，Point{T}基于 T 是不变的（Invariant），这跟 Java 语言的泛型处理是一样的。所以，在 Julia 编译的时候，如果我们给 foo() 传递一个 Point{Float64}参数，那么应该被分派到方法 6。而如果传递一个 Point{Float32}参数呢？分派算法不会选择方法 5，因为 Point{Float32}不是 Point{Real}的子类型。因此，分配算法会选择方法 4，因为 Point{Float32}是 Point 的子类型。那么，如何让 Point{T}基于 T 协变呢？这样我们就可以针对 Real 类型写一些通用的算法，让采用 Float32、Float16 等类型的 Point，都按照这个算法去编译了。 答案就是需要指定上界。我们可以把 Point{Real}改为 Point{\u003c:Real}，它是 Point{Float32}、Point{Float16}等的父类型。好，总结起来，Julia 的泛型和类型计算是很有特点的。泛型提供的参数化多态（Parametric Polymorphism）跟 Julia 原来的方法多态（Method Polymorphism）很好地融合在了一起，让我们能够最大程度地去编写通用的程序。而被泛型增强后的类型体系，也对动态分派算法提出了更高的要求。 ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:21:7","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"课程小结 这一讲，我们学习了泛型这个现代语言中非常重要的特性的实现机制。在实现泛型机制的时候，我们首先必须弄清楚引入泛型以后，对类型体系的影响。你要掌握不变、协变和逆变这三个基本概念和它们的应用场景，从而能够正确地用于类型计算的过程中。在泛型的具体实现机制上，有类型擦除、具体化和模板元编程等不同的方法。好的实现机制应该有能力同时兼顾值类型和复合类型，同时又便于调试。按照惯例，我也把本讲的内容总结成了思维导图，供你参考： ","date":"2022-07-18 23:55:56","objectID":"/cp_practice/:21:8","tags":["compilation principle"],"title":"Cp_practice","uri":"/cp_practice/"},{"categories":["School courses"],"content":"编译原理之美 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:0:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"开篇词 | 为什么你要学习编译原理？ 宫文学 2019-08-14 我会通过具体的案例带你理解抽象的原理。比如语义分析阶段有个 I 属性和 S 属性，传统课本里只专注 I 属性和 S 属性的特点和计算过程，很抽象。那么我会分析常用语言做语义分析时，哪些属性是 I 属性，哪些是 S 属性，以及如何进一步运用这些属性，来让你更直观地了解它们。我也会重视过程，带你一步步趟过雷区。我写了示例程序，带你逐渐迭代出一门脚本语言和一门编译型语言。当然了，我们会遇到一些挑战和问题，而在解决问题的过程中，你会切切实实体会到某个技术在哪个环节会发挥什么作用。最重要的是，你会因此逐渐战胜畏难情绪，不再担心看不懂、学不会。我还会让你在工作中真正运用到编译技术。课程里的代码，可以给你的工作提供参考。我介绍的 Antlr 和 LLVM 工具，前者能帮你做编译器前端的工作，后者能帮你完成编译器后端的工作。在课程中，你能真正运用编译技术解决报表设计等实际问题。 为了帮你迅速了解课程的知识结构体系，我画了一张思维导图。课程从三方面展开，包括实现一门脚本语言、实现一门编译型语言和面向未来的编程语言。 课程的第一部分主要聚焦编译器前端技术，也就是通常说的词法分析、语法分析和语义分析。我会带你了解它们的原理，实现一门脚本语言。我也会教你用工具提升编译工作的效率，还会在几个应用场景中检验我们的学习成果。第二部分主要聚焦编译器后端技术，也就是如何生成目标代码和对代码进行优化的过程。我会带你纯手工生成汇编代码，然后引入中间代码和后端工具 LLVM，最后生成可执行的文件能支持即时编译，并经过了多层优化。第三部分是对编译技术发展趋势的一些分析。这些分析会帮助你更好地把握未来技术发展的脉搏。比如人工智能与编译技术结合是否会出现人工智能编程？云计算与编译技术结合是否会催生云编程的新模式？等等。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:1:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现一门脚本语言 原理 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:2:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"01 | 理解代码：编译器的前端技术 在开篇词里，我分享了一些使用编译技术的场景。其中有的场景，你只要掌握编译器的前端技术就能解决。比如文本分析场景，软件需要用户自定义功能的场景以及前端编程语言的翻译场景等。而且咱们大学讲的编译原理，也是侧重讲解前端技术，可见编译器的前端技术有多么重要。当然了，这里的“前端（Front End）”指的是编译器对程序代码的分析和理解过程。它通常只跟语言的语法有关，跟目标机器无关。而与之对应的**“后端（Back End）”则是生成目标代码的过程，跟目标机器有关**。为了方便你理解，我用一张图直观地展现了编译器的整个编译过程。 你可以看到，编译器的“前端”技术分为词法分析、语法分析和语义分析三个部分。而它主要涉及自动机和形式语言方面的基础的计算理论。这些抽象的理论也许会让你“撞墙”，不过不用担心，我今天会把难懂的理论放到一边，用你听得懂的大白话，联系实际使用的场景，带你直观地理解它们，让你学完本节课之后，实现以下目标： 对编译过程以及其中的技术点有个宏观、概要的了解。能够在大脑里绘制一张清晰的知识地图，以应对工作需要。比如分析一个日志文件时，你能知道所对应的技术点，从而针对性地解决问题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:3:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"词法分析（Lexical Analysis） 通常，编译器的第一项工作叫做词法分析。就像阅读文章一样，文章是由一个个的中文单词组成的。程序处理也一样，只不过这里不叫单词，而是叫做“词法记号”，英文叫 Token。我嫌“词法记号”这个词太长，后面直接将它称作 Token 吧。举个例子，看看下面这段代码，如果我们要读懂它，首先要怎么做呢？ c #include \u003cstdio.h\u003e int main(int argc, char* argv[]){ int age = 45; if (age \u003e= 17+8+20) { printf(\"Hello old man!\\\\n\"); } else{ printf(\"Hello young man!\\\\n\"); } return 0; } 我们会识别出 if、else、int 这样的关键字，main、printf、age 这样的标识符，+、-、= 这样的操作符号，还有花括号、圆括号、分号这样的符号，以及数字字面量、字符串字面量等。这些都是 Token。那么，如何写一个程序来识别 Token 呢？可以看到，英文内容中通常用空格和标点把单词分开，方便读者阅读和理解。但在计算机程序中，仅仅用空格和标点分割是不行的。比如“age \u003e= 45”应该分成“age”“\u003e=”和“45”这三个 Token，但在代码里它们可以是连在一起的，中间不用非得有空格。这和汉语有点儿像，汉语里每个词之间也是没有空格的。但我们会下意识地把句子里的词语正确地拆解出来。比如把“我学习编程”这个句子拆解成“我”“学习”“编程”，这个过程叫做“分词”。如果你要研发一款支持中文的全文检索引擎，需要有分词的功能。其实，我们可以通过制定一些规则来区分每个不同的 Token，我举了几个例子，你可以看一下。 识别 age 这样的标识符。它以字母开头，后面可以是字母或数字，直到遇到第一个既不是字母又不是数字的字符时结束。识别 \u003e= 这样的操作符。 当扫描到一个 \u003e 字符的时候，就要注意，它可能是一个 GT（Greater Than，大于）操作符。但由于 GE（Greater Equal，大于等于）也是以 \u003e 开头的，所以再往下再看一位，如果是 =，那么这个 Token 就是 GE，否则就是 GT。识别 45 这样的数字字面量。当扫描到一个数字字符的时候，就开始把它看做数字，直到遇到非数字的字符。 这些规则可以通过手写程序来实现。事实上，很多编译器的词法分析器都是手写实现的，例如 GNU(an operating system similar to Unix with a collection of compatible software, developed and distributed as a free alternative to commercial systems.一种类似于Unix的操作系统，具有一系列兼容软件，作为商业系统的免费替代品开发和分发。) 的 C 语言编译器。如果嫌手写麻烦，或者你想花更多时间陪恋人或家人，也可以偷点儿懒，用词法分析器的生成工具来生成，比如 Lex（或其 GNU 版本，Flex）。这些生成工具是基于一些规则来工作的，这些规则用“正则文法”表达，符合正则文法的表达式称为“正则表达式”。生成工具可以读入正则表达式，生成一种叫“有限自动机”的算法，来完成具体的词法分析工作。不要被“正则文法（Regular Grammar）”和“有限自动机（Finite-state Automaton，FSA，or Finite Automaton）”吓到。正则文法是一种最普通、最常见的规则，写正则表达式的时候用的就是正则文法。我们前面描述的几个规则，都可以看成口语化的正则文法。有限自动机是有限个状态的自动机器。我们可以拿抽水马桶举例，它分为两个状态：“注水”和“水满”。摁下冲马桶的按钮，它转到“注水”的状态，而浮球上升到一定高度，就会把注水阀门关闭，它转到“水满”状态。 词法分析器也是一样，它分析整个程序的字符串，当遇到不同的字符时，会驱使它迁移到不同的状态。例如，词法分析程序在扫描 age 的时候，处于“标识符”状态，等它遇到一个 \u003e 符号，就切换到“比较操作符”的状态。词法分析过程，就是这样一个个状态迁移的过程。 你也许熟悉正则表达式，因为我们在编程过程中经常用正则表达式来做用户输入的校验，例如是否输入了一个正确的电子邮件地址，这其实就是在做词法分析，你应该用过。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:3:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"语法分析 （Syntactic Analysis, or Parsing） 编译器下一个阶段的工作是语法分析。词法分析是识别一个个的单词，而语法分析就是在词法分析的基础上识别出程序的语法结构。这个结构是一个树状结构，是计算机容易理解和执行的。 以自然语言为例。自然语言有定义良好的语法结构，比如，“我喜欢又聪明又勇敢的你”这个句子包含了“主、谓、宾”三个部分。主语是“我”，谓语是“喜欢”，宾语部分是“又聪明又勇敢的你”。其中宾语部分又可以拆成两部分，“又聪明又勇敢”是定语部分，用来修饰“你”。定语部分又可以分成“聪明”和“勇敢”两个最小的单位。这样拆下来，会构造一棵树，里面的每个子树都有一定的结构，而这个结构要符合语法。比如，汉语是用“主谓宾”的结构，日语是用“主宾谓”的结构。这时，我们说汉语和日语的语法规则是不同的。 程序也有定义良好的语法结构，它的语法分析过程，就是构造这么一棵树。一个程序就是一棵树，这棵树叫做抽象语法树（Abstract Syntax Tree，AST）。树的每个节点（子树）是一个语法单元，这个单元的构成规则就叫“语法”。每个节点还可以有下级节点。层层嵌套的树状结构，是我们对计算机程序的直观理解。计算机语言总是一个结构套着另一个结构，大的程序套着子程序，子程序又可以包含子程序。接下来，我们直观地看一下这棵树长什么样子。 我在 Mac 电脑上打下这个命令： clang -cc1 -ast-dump hello.c 这个命令是运行苹果公司的 C 语言编译器来编译 hello.c，-ast-dump 参数使它输出 AST，而不是做常规的编译。我截取了一部分输出结果给你看，从中你可以看到这棵树的结构。 试着修改程序，添加不同的语句，你会看到不同的语法树。 如果你觉得这棵树还不够直观，可以参考我提供的网址，它能够生成 JavaScript 语言的 AST，并以更加直观的方式呈现。在这个网址里输入一个可以计算的表达式，例如“2+3*5”，你会得到一棵类似下图的 AST。 形成 AST 以后有什么好处呢？就是计算机很容易去处理。比如，针对表达式形成的这棵树，从根节点遍历整棵树就可以获得表达式的值。基于这个原理，我在后面的课程中会带你实现一个计算器，并实现自定义公式功能。如果再把循环语句、判断语句、赋值语句等节点加到 AST 上，并解释执行它，那么你实际上就实现了一个脚本语言。而执行脚本语言的过程，就是遍历 AST 的过程。当然，在后面的课程中，我也会带你实际实现一个脚本语言。 好了，你已经知道了 AST 的作用，那么怎样写程序构造它呢？ 一种非常直观的构造思路是自上而下进行分析。首先构造根节点，代表整个程序，之后向下扫描 Token 串，构建它的子节点。当它看到一个 int 类型的 Token 时，知道这儿遇到了一个变量声明语句，于是建立一个“变量声明”节点；接着遇到 age，建立一个子节点，这是第一个变量；之后遇到 =，意味着这个变量有初始化值，那么建立一个初始化的子节点；最后，遇到“字面量”，其值是 45。这样，一棵子树就扫描完毕了。程序退回到根节点，开始构建根节点的第二个子节点。这样递归地扫描，直到构建起一棵完整的树。 这个算法就是非常常用的递归下降算法（Recursive Descent Parsing）。是不是很简单？你完全可以动手写出来。递归下降算法是一种自顶向下的算法，与之对应的，还有自底向上的算法。这个算法会先将最下面的叶子节点识别出来，然后再组装上一级节点。有点儿像搭积木，我们总是先构造出小的单元，然后再组装成更大的单元。原理就是这么简单。也许你会想，除了手写，有没有偷懒的、更省事的方法呢？多一些时间去陪家人总不是坏事。 你现在已经有了一定的经验，大可以去找找看有没有现成的工具，比如 Yacc（或 GNU 的版本，Bison）、Antlr、JavaCC 等。实际上，你可以在维基百科里找到一个挺大的清单，我把它放到了 CSDN 的博客上，其中对各种工具的特性做了比较。顺理成章地，你还能找到很多开源的语法规则文件，改一改，就能用工具生成你的语法分析器。很多同学其实已经做过语法解析的工作，比如编写一个自定义公式的功能，对公式的解析就是语法分析过程。另一个例子是分析日志文件等文本文件，对每行日志的解析，本质上也是语法分析过程。解析用 XML、JSON 写的各种配置文件、模型定义文件的过程，其实本质也是语法分析过程，甚至还包含了语义分析工作。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:3:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"语义分析（Semantic Analysis） 好了，讲完了词法分析、语法分析，编译器接下来做的工作是语义分析。说白了，语义分析就是要让计算机理解我们的真实意图，把一些模棱两可的地方消除掉。以“You can never drink too much water.” 这句话为例。它的确切含义是什么？是“你不能喝太多水”，还是“你喝多少水都不嫌多”？实际上，这两种解释都是可以的，我们只有联系上下文才能知道它的准确含义。你可能会觉得理解自然语言的含义已经很难了，所以计算机语言的语义分析也一定很难。其实语义分析没那么复杂，因为计算机语言的语义一般可以表达为一些规则，你只要检查是否符合这些规则就行了。比如： 某个表达式的计算结果是什么数据类型？如果有数据类型不匹配的情况，是否要做自动转换？如果在一个代码块的内部和外部有相同名称的变量，我在执行的时候到底用哪个？ 就像“我喜欢又聪明又勇敢的你”中的“你”，到底指的是谁，需要明确。在同一个作用域内，不允许有两个名称相同的变量，这是唯一性检查。你不能刚声明一个变量 a，紧接着又声明同样名称的一个变量 a，这就不允许了。 语义分析基本上就是做这样的事情，也就是根据语义规则进行分析判断。语义分析工作的某些成果，会作为属性标注在抽象语法树上，比如在 age 这个标识符节点和 45 这个字面量节点上，都会标识它的数据类型是 int 型的。在这个树上还可以标记很多属性，有些属性是在之前的两个阶段就被标注上了，比如所处的源代码行号，这一行的第几个字符。这样，在编译程序报错的时候，就可以比较清楚地了解出错的位置。做了这些属性标注以后，编译器在后面就可以依据这些信息生成目标代码了，我们在编译技术的后端部分会去讲。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:3:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 讲完语义分析，本节课也就告一段落了，我来总结一下本节课的重点内容： 词法分析是把程序分割成一个个 Token 的过程，可以通过构造有限自动机来实现。语法分析是把程序的结构识别出来，并形成一棵便于由计算机处理的抽象语法树。可以用递归下降的算法来实现。语义分析是消除语义模糊，生成一些属性信息，让计算机能够依据这些信息生成目标代码。 我想让你知道，上述编译过程其实跟你的实际工作息息相关。比如，词法分析就是你工作中使用正则表达式的过程。而语法分析在你解析文本文件、配置文件、模型定义文件，或者做自定义公式功能的时候都会用到。我还想让你知道，编译技术并没有那么难，它的核心原理是很容易理解的。学习之后，你能很快上手，如果善用一些辅助生成工具会更省事。所以，我希望你通过学习这篇文章，已经破除了一些心理障碍，并跃跃欲试，想要动手做点儿什么了！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:3:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"02 | 正则文法和有限自动机：纯手工打造词法分析器 上一讲，我提到词法分析的工作是将一个长长的字符串识别出一个个的单词，这一个个单词就是 Token。而且词法分析的工作是一边读取一边识别字符串的，不是把字符串都读到内存再识别。你在听一位朋友讲话的时候，其实也是同样的过程，一边听，一边提取信息。那么问题来了，字符串是一连串的字符形成的，怎么把它断开成一个个的 Token 呢？分割的依据是什么呢？本节课，我会通过讲解正则表达式（Regular Expression）和有限自动机的知识带你解决这个问题。其实，我们手工打造词法分析器的过程，就是写出正则表达式，画出有限自动机的图形，然后根据图形直观地写出解析代码的过程。而我今天带你写的词法分析器，能够分析以下 3 个程序语句： age \u003e= 45 int age = 40 2+3*5 它们分别是关系表达式、变量声明和初始化语句，以及算术表达式。接下来，我们先来解析一下“age \u003e= 45”这个关系表达式，这样你就能理解有限自动机的概念，知道它是做词法解析的核心机制了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:4:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"解析 age \u003e= 45 在“01 | 理解代码：编译器的前端技术”里，我举了一个词法分析的例子，并且提出词法分析要用到有限自动机。当时，我画了这样一个示意图： 我们来描述一下标识符、比较操作符和数字字面量这三种 Token 的词法规则。标识符：第一个字符必须是字母，后面的字符可以是字母或数字。比较操作符：\u003e 和 \u003e=（其他比较操作符暂时忽略）。数字字面量：全部由数字构成（像带小数点的浮点数，暂时不管它）。 我们就是依据这样的规则，来构造有限自动机的。这样，词法分析程序在遇到 age、\u003e= 和 45 时，会分别识别成标识符、比较操作符和数字字面量。不过上面的图只是一个简化的示意图，一个严格意义上的有限自动机是下面这种画法： 我来解释一下上图的 5 种状态。 初始状态：刚开始启动词法分析的时候，程序所处的状态。2. 标识符状态：在初始状态时，当第一个字符是字母的时候，迁移到状态 2。当后续字符是字母和数字时，保留在状态 2。如果不是，就离开状态 2，写下该 Token，回到初始状态。3. 大于操作符（GT）：在初始状态时，当第一个字符是 \u003e 时，进入这个状态。它是比较操作符的一种情况。4. 大于等于操作符（GE）：如果状态 3 的下一个字符是 =，就进入状态 4，变成 \u003e=。它也是比较操作符的一种情况。5. 数字字面量：在初始状态时，下一个字符是数字，进入这个状态。如果后续仍是数字，就保持在状态 5。 这里我想补充一下，你能看到上图中的圆圈有单线的也有双线的。双线的意思是这个状态已经是一个合法的 Token 了，单线的意思是这个状态还是临时状态。按照这 5 种状态迁移过程，你很容易编成程序（我用 Java 写了代码示例，你可以用自己熟悉的语言编写）。我们先从状态 1 开始，在遇到不同的字符时，分别进入 2、3、5 三个状态： c DfaState newState = DfaState.Initial; if (isAlpha(ch)) { //第一个字符是字母 newState = DfaState.Id; //进入Id状态 token.type = TokenType.Identifier; tokenText.append(ch); } else if (isDigit(ch)) { //第一个字符是数字 newState = DfaState.IntLiteral; token.type = TokenType.IntLiteral; tokenText.append(ch); } else if (ch == '\u003e') { //第一个字符是\u003e newState = DfaState.GT; token.type = TokenType.GT; tokenText.append(ch); } 上面的代码中，我用 Java 中的枚举（enum）类型定义了一些枚举值来代表不同的状态，让代码更容易读。其中 Token 是自定义的一个数据结构，它有两个主要的属性：一个是“type”，就是 Token 的类型，它用的也是一个枚举类型的值；一个是“text”，也就是这个 Token 的文本值。我们接着处理进入 2、3、5 三个状态之后的状态迁移过程： c case Initial: state = initToken(ch); //重新确定后续状态 break; case Id: if (isAlpha(ch) || isDigit(ch)) { tokenText.append(ch); //保持标识符状态 } else { state = initToken(ch); //退出标识符状态，并保存Token } break; case GT: if (ch == '=') { token.type = TokenType.GE; //转换成GE state = DfaState.GE; tokenText.append(ch); } else { state = initToken(ch); //退出GT状态，并保存Token } break; case GE: state = initToken(ch); //退出当前状态，并保存Token break; case IntLiteral: if (isDigit(ch)) { tokenText.append(ch); //继续保持在数字字面量状态 } else { state = initToken(ch); //退出当前状态，并保存Token } break; 运行这个示例程序，你就会成功地解析类似“age \u003e= 45”这样的程序语句。不过，你可以先根据我的讲解自己实现一下，然后再去参考这个示例程序。示例程序的输出如下，其中第一列是 Token 的类型，第二列是 Token 的文本值： Identifier age GE \u003e= IntLiteral 45 上面的例子虽然简单，但其实已经讲清楚了词法原理，就是依据构造好的有限自动机，在不同的状态中迁移，从而解析出 Token 来。你只要再扩展这个有限自动机，增加里面的状态和迁移路线，就可以逐步实现一个完整的词法分析器了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:4:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"初识正则表达式 但是，这里存在一个问题。我们在描述词法规则时用了自然语言。比如，在描述标识符的规则时，我们是这样表达的： 第一个字符必须是字母，后面的字符可以是字母或数字。 这样描述规则并不精确，我们需要换一种严谨的表达方式，这种方式就是正则表达式。上面的例子涉及了 4 种 Token，这 4 种 Token 用正则表达式表达，是下面的样子： Id : [a-zA-Z_] ([a-zA-Z_] | [0-9])* IntLiteral: [0-9]+ GT : '\u003e' GE : '\u003e=' 我先来解释一下这几个规则中用到的一些符号： 需要注意的是，不同语言的标识符、整型字面量的规则可能是不同的。比如，有的语言可以允许用 Unicode 作为标识符，也就是说变量名称可以是中文的。还有的语言规定，十进制数字字面量的第一位不能是 0。这时候正则表达式会有不同的写法，对应的有限自动机自然也不同。而且，不同工具的正则表达式写法会略有不同，但大致是差不多的。我在本节课讲正则表达式，主要是为了让词法规则更为严谨，当然了，也是为后面的内容做铺垫。在后面的课程中，我会带你用工具生成词法分析器，而工具读取的就是用正则表达式描述的词法规则。到时候，我们会把所有常用的词法都用正则表达式描述出来。不过在这之前，如果你想主动了解更完整的正则表达式规则，完全可以参考自己所采用的正则表达式工具的文档。比如，Java 的正则式表达式工具在 java.util.regex 包中，在其 Javadoc 中有详细的规则说明。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:4:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"解析 int age = 40，处理标识符和关键字规则的冲突 说完正则表达式，我们接着去处理其他词法，比如解析“int age = 40”这个语句，以这个语句为例研究一下词法分析中会遇到的问题：多个规则之间的冲突。如果我们把这个语句涉及的词法规则用正则表达式写出来，是下面这个样子： Int: 'int' Id : [a-zA-Z_] ([a-zA-Z_] | [0-9])* Assignment : '=' 这时候，你可能会发现这样一个问题：int 这个关键字，与标识符很相似，都是以字母开头，后面跟着其他字母。换句话说，int 这个字符串，既符合标识符的规则，又符合 int 这个关键字的规则，这两个规则发生了重叠。这样就起冲突了，我们扫描字符串的时候，到底该用哪个规则呢？当然，我们心里知道，int 这个关键字的规则，比标识符的规则优先级高。普通的标识符是不允许跟这些关键字重名的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:4:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"在这里，我们来回顾一下：什么是关键字？ 关键字是语言设计中作为语法要素的词汇，例如表示数据类型的 int、char，表示程序结构的 while、if，表述特殊数据取值的 null、NAN 等。除了关键字，还有一些词汇叫保留字。保留字在当前的语言设计中还没用到，但是保留下来，因为将来会用到。我们命名自己的变量、类名称，不可以用到跟关键字和保留字相同的字符串。那么我们在词法分析器中，如何把关键字和保留字跟标识符区分开呢？ 以“int age = 40”为例，我们把有限自动机修改成下面的样子，借此解决关键字和标识符的冲突。 这个思路其实很简单。在识别普通的标识符之前，你先看看它是关键字还是保留字就可以了。具体做法是： 当第一个字符是 i 的时候，我们让它进入一个特殊的状态。接下来，如果它遇到 n 和 t，就进入状态 4。但这还没有结束，如果后续的字符还有其他的字母和数字，它又变成了普通的标识符。比如，我们可以声明一个 intA（int 和 A 是连着的）这样的变量，而不会跟 int 关键字冲突。 相应的代码也修改一下，文稿里的第一段代码要改成： c if (isAlpha(ch)) { if (ch == 'i') { newState = DfaState.Id_int1; //对字符i特殊处理 } else { newState = DfaState.Id; } ... //后续代码 } 第二段代码要增加下面的语句： c case Id_int1: if (ch == 'n') { state = DfaState.Id_int2; tokenText.append(ch); } else if (isDigit(ch) || isAlpha(ch)){ state = DfaState.Id; //切换回Id状态 tokenText.append(ch); } else { state = initToken(ch); } break; case Id_int2: if (ch == 't') { state = DfaState.Id_int3; tokenText.append(ch); } else if (isDigit(ch) || isAlpha(ch)){ state = DfaState.Id; //切换回Id状态 tokenText.append(ch); } else { state = initToken(ch); } break; case Id_int3: if (isBlank(ch)) { token.type = TokenType.Int; state = initToken(ch); } else{ state = DfaState.Id; //切换回Id状态 tokenText.append(ch); } break; 接着，我们运行示例代码，就会输出下面的信息： Int int Identifier age Assignment = IntLiteral 45 而当你试着解析“intA = 10”程序的时候，会把 intA 解析成一个标识符。输出如下： Identifier intA Assignment = IntLiteral 10 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:4:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"解析算术表达式 解析完“int age = 40”之后，我们再按照上面的方法增加一些规则，这样就能处理算术表达式，例如“2+3*5”。 增加的词法规则如下： Plus : '+' Minus : '-' Star : '*' Slash : '/' 然后再修改一下有限自动机和代码，就能解析“2+3*5”了，会得到下面的输出： IntLiteral 2 Plus + IntLiteral 3 Star * IntLiteral 5 好了，现在我们已经能解析不少词法了，之后的课程里，我会带你实现一个公式计算器，所以在这里要先准备好所需要的词法分析功能。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:4:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我们实现了一个简单的词法分析器。你可以看到，要实现一个词法分析器，首先需要写出每个词法的正则表达式，并画出有限自动机，之后，只要用代码表示这种状态迁移过程就可以了。 我们总是说理解原理以后，实现并不困难。今天的分享，你一定有所共鸣。反之，如果你在编程工作中遇到困难，往往是因为不清楚原理，没有将原理吃透。而这门课就是要帮助你真正吃透编译技术中的几个核心原理，让你将知识应用到实际工作中，解决工作中遇到的困难。小试了词法分析器之后，在下一讲，我会带你手工打造一下语法分析器，并实现一个公式计算器的功能。 另外，为了便于你更好地学习，我将本节课的示例程序放到了GitHub上，你可以看一下。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:4:6","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"03 | 语法分析（一）：纯手工打造公式计算器 我想你应该知道，公式是 Excel 电子表格软件的灵魂和核心。除此之外，在 HR 软件中，可以用公式自定义工资。而且，如果你要开发一款通用报表软件，也会大量用到自定义公式来计算报表上显示的数据。总而言之，很多高级一点儿的软件，都会用到自定义公式功能。既然公式功能如此常见和重要，我们不妨实现一个公式计算器，给自己的软件添加自定义公式功能吧！ 本节课将继续“手工打造”之旅，让你纯手工实现一个公式计算器，借此掌握语法分析的原理和递归下降算法（Recursive Descent Parsing），并初步了解上下文无关文法（Context-free Grammar，CFG）。 我所举例的公式计算器支持加减乘除算术运算，比如支持“2 + 3 * 5”的运算。在学习语法分析时，我们习惯把上面的公式称为表达式。这个表达式看上去很简单，但你能借此学到很多语法分析的原理，例如左递归、优先级和结合性等问题。当然了，要实现上面的表达式，你必须能分析它的语法。不过在此之前，我想先带你解析一下变量声明语句的语法，以便让你循序渐进地掌握语法分析。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:5:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"解析变量声明语句：理解“下降”的含义 在“01 | 理解代码：编译器的前端技术”里，我提到语法分析的结果是生成 AST。算法分为自顶向下和自底向上算法，其中，递归下降算法是一种常见的自顶向下算法。与此同时，我给出了一个简单的代码示例，也针对“int age = 45”这个语句，画了一个语法分析算法的示意图： 我们首先把变量声明语句的规则，用形式化的方法表达一下。它的左边是一个非终结符（Non-terminal）。右边是它的产生式（Production Rule）。在语法解析的过程中，左边会被右边替代。如果替代之后还有非终结符，那么继续这个替代过程，直到最后全部都是终结符（Terminal），也就是 Token。只有终结符才可以成为 AST 的叶子节点。这个过程，也叫做推导（Derivation）过程： intDeclaration : Int Identifier ('=' additiveExpression)?; 你可以看到，int 类型变量的声明，需要有一个 Int 型的 Token，加一个变量标识符，后面跟一个可选的赋值表达式。我们把上面的文法翻译成程序语句，伪代码如下： //伪代码 MatchIntDeclare(){ MatchToken(Int)； //匹配Int关键字 MatchIdentifier(); //匹配标识符 MatchToken(equal); //匹配等号 MatchExpression(); //匹配表达式 } 实际代码在 SimpleCalculator.java 类的 IntDeclare() 方法中： SimpleASTNode node = null; Token token = tokens.peek(); //预读 if (token != null \u0026\u0026 token.getType() == TokenType.Int) { //匹配Int token = tokens.read(); //消耗掉int if (tokens.peek().getType() == TokenType.Identifier) { //匹配标识符 token = tokens.read(); //消耗掉标识符 //创建当前节点，并把变量名记到AST节点的文本值中， //这里新建一个变量子节点也是可以的 node = new SimpleASTNode(ASTNodeType.IntDeclaration, token.getText()); token = tokens.peek(); //预读 if (token != null \u0026\u0026 token.getType() == TokenType.Assignment) { tokens.read(); //消耗掉等号 SimpleASTNode child = additive(tokens); //匹配一个表达式 if (child == null) { throw new Exception(\"invalide variable initialization, expecting an expression\"); } else{ node.addChild(child); } } } else { throw new Exception(\"variable name expected\"); } } 直白地描述一下上面的算法：解析变量声明语句时，我先看第一个 Token 是不是 int。如果是，那我创建一个 AST 节点，记下 int 后面的变量名称，然后再看后面是不是跟了初始化部分，也就是等号加一个表达式。我们检查一下有没有等号，有的话，接着再匹配一个表达式。 我们通常会对产生式的每个部分建立一个子节点，比如变量声明语句会建立四个子节点，分别是 int 关键字、标识符、等号和表达式。后面的工具就是这样严格生成 AST 的。但是我这里做了简化，只生成了一个子节点，就是表达式子节点。变量名称记到 ASTNode 的文本值里去了，其他两个子节点没有提供额外的信息，就直接丢弃了。另外，从上面的代码中我们看到，程序是从一个 Token 的流中顺序读取。代码中的 peek() 方法是预读，只是读取下一个 Token，但并不把它从 Token 流中移除。在代码中，我们用 peek() 方法可以预先看一下下一个 Token 是否是等号，从而知道后面跟着的是不是一个表达式。而 read() 方法会从 Token 流中移除，下一个 Token 变成了当前的 Token。 这里需要注意的是，通过 peek() 方法来预读，实际上是对代码的优化，这有点儿预测的意味。我们后面会讲带有预测的自顶向下算法，它能减少回溯的次数。我们把解析变量声明语句和表达式的算法分别写成函数。在语法分析的时候，调用这些函数跟后面的 Token 串做模式匹配。匹配上了，就返回一个 AST 节点，否则就返回 null。如果中间发现跟语法规则不符，就报编译错误。在这个过程中，上级文法嵌套下级文法，上级的算法调用下级的算法。表现在生成 AST 中，上级算法生成上级节点，下级算法生成下级节点。这就是“下降”的含义。 分析上面的伪代码和程序语句，你可以看到这样的特点：**程序结构基本上是跟文法规则同构的。这就是递归下降算法的优点，非常直观。**接着说回来，我们继续运行这个示例程序，输出 AST： Programm Calculator IntDeclaration age AssignmentExp = IntLiteral 45 前面的文法和算法都很简单，这样级别的文法没有超出正则文法。也就是说，并没有超出我们做词法分析时用到的文法。好了，解析完变量声明语句，带你理解了“下降”的含义之后，我们来看看如何用上下文无关文法描述算术表达式。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:5:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"用上下文无关文法描述算术表达式 我们解析算术表达式的时候，会遇到更复杂的情况，这时，正则文法不够用，我们必须用上下文无关文法来表达。你可能会问：“正则文法为什么不能表示算术表达式？”别着急，我们来分析一下算术表达式的语法规则。算术表达式要包含加法和乘法两种运算（简单起见，我们把减法与加法等同看待，把除法也跟乘法等同看待），加法和乘法运算有不同的优先级。我们的规则要能匹配各种可能的算术表达式： 2+35 23+5 2*3…… 思考一番之后，我们把规则分成两级：第一级是加法规则，第二级是乘法规则。把乘法规则作为加法规则的子规则，这样在解析形成 AST 时，乘法节点就一定是加法节点的子节点，从而被优先计算。 additiveExpression : multiplicativeExpression | additiveExpression Plus multiplicativeExpression ; multiplicativeExpression : IntLiteral | multiplicativeExpression Star IntLiteral ; 你看，我们可以通过文法的嵌套，实现对运算优先级的支持。这样我们在解析“2 + 3 * 5”这个算术表达式时会形成类似下面的 AST： 如果要计算表达式的值，只需要对根节点求值就可以了。为了完成对根节点的求值，需要对下级节点递归求值，所以我们先完成“3 * 5 = 15”，然后再计算“2 + 15 = 17”。有了这个认知，我们在解析算术表达式的时候，便能拿加法规则去匹配。在加法规则中，会嵌套地匹配乘法规则。我们通过文法的嵌套，实现了计算的优先级。应该注意的是，加法规则中还递归地又引用了加法规则。通过这种递归的定义，我们能展开、形成所有各种可能的算术表达式。比如“2+3*5” 的推导过程： --\u003eadditiveExpression + multiplicativeExpression --\u003emultiplicativeExpression + multiplicativeExpression --\u003eIntLiteral + multiplicativeExpression --\u003eIntLiteral + multiplicativeExpression * IntLiteral --\u003eIntLiteral + IntLiteral * IntLiteral 这种文法已经没有办法改写成正则文法了，它比正则文法的表达能力更强，叫做“上下文无关文法”。正则文法是上下文无关文法的一个子集。它们的区别呢，就是上下文无关文法允许递归调用，而正则文法不允许。上下文无关的意思是，无论在任何情况下，文法的推导规则都是一样的。比如，在变量声明语句中可能要用到一个算术表达式来做变量初始化，而在其他地方可能也会用到算术表达式。不管在什么地方，算术表达式的语法都一样，都允许用加法和乘法，计算优先级也不变。好在你见到的大多数计算机语言，都能用上下文无关文法来表达它的语法。那有没有上下文相关的情况需要处理呢？也是有的，但那不是语法分析阶段负责的，而是放在语义分析阶段来处理的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:5:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"解析算术表达式：理解“递归”的含义 在讲解上下文无关文法时，我提到了文法的递归调用，你也许会问，是否在算法上也需要递归的调用呢？要不怎么叫做“递归下降算法”呢？的确，我们之前的算法只算是用到了“下降”，没有涉及“递归”，现在，我们就来看看如何用递归的算法翻译递归的文法。我们先按照前面说的，把文法直观地翻译成算法。但是，我们遇到麻烦了。这个麻烦就是出现了无穷多次调用的情况。我们来看个例子。为了简单化，我们采用下面这个简化的文法，去掉了乘法的层次： additiveExpression : IntLiteral | additiveExpression Plus IntLiteral ; 在解析 “2 + 3”这样一个最简单的加法表达式的时候，我们直观地将其翻译成算法，结果出现了如下的情况：首先匹配是不是整型字面量，发现不是；然后匹配是不是加法表达式，这里是递归调用；会重复上面两步，无穷无尽。 “additiveExpression Plus multiplicativeExpression”这个文法规则的第一部分就递归地引用了自身，这种情况叫做左递归。通过上面的分析，我们知道左递归是递归下降算法无法处理的，这是递归下降算法最大的问题。 怎么解决呢？把“additiveExpression”调换到加号后面怎么样？我们来试一试。 additiveExpression : multiplicativeExpression | multiplicativeExpression Plus additiveExpression ; 我们接着改写成算法，这个算法确实不会出现无限调用的问题： private SimpleASTNode additive(TokenReader tokens) throws Exception { SimpleASTNode child1 = multiplicative(); //计算第一个子节点 SimpleASTNode node = child1; //如果没有第二个子节点，就返回这个 Token token = tokens.peek(); if (child1 != null \u0026\u0026 token != null) { if (token.getType() == TokenType.Plus) { token = tokens.read(); SimpleASTNode child2 = additive(); //递归地解析第二个节点 if (child2 != null) { node = new SimpleASTNode(ASTNodeType.AdditiveExp, token.getText()); node.addChild(child1); node.addChild(child2); } else { throw new Exception(\"invalid additive expression, expecting the right part.\"); } } } return node; } 为了便于你理解，我解读一下上面的算法：我们先尝试能否匹配乘法表达式，如果不能，那么这个节点肯定不是加法节点，因为加法表达式的两个产生式都必须首先匹配乘法表达式。遇到这种情况，返回 null 就可以了，调用者就这次匹配没有成功。如果乘法表达式匹配成功，那就再尝试匹配加号右边的部分，也就是去递归地匹配加法表达式。如果匹配成功，就构造一个加法的 ASTNode 返回。 同样的，乘法的文法规则也可以做类似的改写： multiplicativeExpression : IntLiteral | IntLiteral Star multiplicativeExpression ; 现在我们貌似解决了左递归问题，运行这个算法解析 “2+3*5”，得到下面的 AST： Programm Calculator AdditiveExp + IntLiteral 2 MulticativeExp * IntLiteral 3 IntLiteral 5 是不是看上去一切正常？可如果让这个程序解析“2+3+4”呢？ Programm Calculator AdditiveExp + IntLiteral 2 AdditiveExp + IntLiteral 3 IntLiteral 4 问题是什么呢？计算顺序发生错误了。连续相加的表达式要从左向右计算，这是加法运算的结合性规则。但按照我们生成的 AST，变成从右向左了，先计算了“3+4”，然后才跟“2”相加。这可不行！为什么产生上面的问题呢？是因为我们修改了文法，把文法中加号左右两边的部分调换了一下。造成的影响是什么呢？你可以推导一下“2+3+4”的解析过程： 首先调用乘法表达式匹配函数 multiplicative()，成功，返回了一个字面量节点 2。接着看看右边是否能递归地匹配加法表达式。匹配的结果，真的返回了一个加法表达式“3+4”，这个变成了第二个子节点。错误就出在这里了。这样的匹配顺序，“3+4”一定会成为子节点，在求值时被优先计算。 所以，我们前面的方法其实并没有完美地解决左递归，因为它改变了加法运算的结合性规则。那么，我们能否既解决左递归问题，又不产生计算顺序的错误呢？答案是肯定的。不过我们下一讲再来解决它。目前先忍耐一下，凑合着用这个“半吊子”的算法吧。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:5:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现表达式求值 上面帮助你理解了“递归”的含义，接下来，我要带你实现表达式的求值。其实，要实现一个表达式计算，只需要基于 AST 做求值运算。这个计算过程比较简单，只需要对这棵树做深度优先的遍历就好了。深度优先的遍历也是一个递归算法。以上文中“2 + 3 * 5”的 AST 为例看一下。 对表达式的求值，等价于对 AST 根节点求值。首先求左边子节点，算出是 2。接着对右边子节点求值，这时候需要递归计算下一层。计算完了以后，返回是 15（3*5）。把左右节点相加，计算出根节点的值 17。 代码参见 SimpleCalculator.Java 中的 evaluate() 方法。还是以“2+3*5”为例。它的求值过程输出如下，你可以看到求值过程中遍历了整棵树： Calculating: AdditiveExp //计算根节点 Calculating: IntLiteral //计算第一个子节点 Result: 2 //结果是2 Calculating: MulticativeExp //递归计算第二个子节点 Calculating: IntLiteral Result: 3 Calculating: IntLiteral Result: 5 Result: 15 //忽略递归的细节，得到结果是15 Result: 17 //根节点的值是17 你可以运行一下示例程序看看输出结果，而且我十分建议你修改表达式，自己做做实验，并试着让表达式不符合语法，看看语法分析程序能不能找出错误来。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:5:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 今天我们实现了一个简单的公式计算器，尽管简单，相信你已经有了收获。那么我来总结一下今天的重点：初步了解上下文无关文法，知道它能表达主流的计算机语言，以及与正则文法的区别。理解递归下降算法中的“下降”和“递归”两个特点。它跟文法规则基本上是同构的，通过文法一定能写出算法。通过遍历 AST 对表达式求值，加深对计算机程序执行机制的理解。 在后面的课程中，我们会在此基础上逐步深化，比如在变量声明中可以使用表达式，在表达式中可以使用变量，例如能够执行像这样的语句： int A = 17； int B = A + 10*2; 实现了上述功能以后，这个程序就越来越接近一个简单的脚本解释器了！当然，在此之前，我们还必须解决左递归的问题。所以下一讲，我会带你填掉左递归这个坑。我们学习和工作的过程，就是在不停地挖坑、填坑，你要有信心，只要坚强走过填坑这段路，你的职业生涯将会愈发平坦！ 递归算法是很好的自顶向下解决问题的方法，是计算机领域的一个核心的思维方式。拥有这种思维方式，可以说是程序员相对于非程序员的一种优势。 另外，为了便于你更好地学习，我将本节课的示例程序放到了码云和GitHub上，你可以看一下。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:5:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"04 | 语法分析（二）：解决二元表达式中的难点 在“03 | 语法分析（一）：纯手工打造公式计算器”中，我们已经初步实现了一个公式计算器。而且你还在这个过程中，直观地获得了写语法分析程序的体验，在一定程度上破除了对语法分析算法的神秘感。当然了，你也遇到了一些问题，比如怎么消除左递归，怎么确保正确的优先级和结合性。所以本节课的主要目的就是解决这几个问题，让你掌握像算术运算这样的二元表达式（Binary Expression）。不过在课程开始之前，我想先带你简单地温习一下什么是左递归（Left Recursive）、优先级（Priority）和结合性（Associativity）。在二元表达式的语法规则中，如果产生式的第一个元素是它自身，那么程序就会无限地递归下去，这种情况就叫做左递归。比如加法表达式的产生式“加法表达式 + 乘法表达式”，就是左递归的。而优先级和结合性则是计算机语言中与表达式有关的核心概念。它们都涉及了语法规则的设计问题。 我们要想深入探讨语法规则设计，需要像在词法分析环节一样，先了解如何用形式化的方法表达语法规则。“工欲善其事必先利其器”。熟练地阅读和书写语法规则，是我们在语法分析环节需要掌握的一项基本功。所以本节课我会先带你了解如何写语法规则，然后在此基础上，带你解决上面提到的三个问题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:6:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"书写语法规则，并进行推导 我们已经知道，语法规则是由上下文无关文法表示的，而上下文无关文法是由一组替换规则（又叫产生式）组成的，比如算术表达式的文法规则可以表达成下面这种形式： add -\u003e mul | add + mul mul -\u003e pri | mul * pri pri -\u003e Id | Num | (add) 按照上面的产生式，add 可以替换成 mul，或者 add + mul。这样的替换过程又叫做“推导”。以“2+3*5” 和 “2+3+4”这两个算术表达式为例，这两个算术表达式的推导过程分别如下图所示： 通过上图的推导过程，你可以清楚地看到这两个表达式是怎样生成的。而分析过程中形成的这棵树，其实就是 AST。只不过我们手写的算法在生成 AST 的时候，通常会做一些简化，省略掉中间一些不必要的节点。比如，“add-add-mul-pri-Num”这一条分支，实际手写时会被简化成“add-Num”。其实，简化 AST 也是优化编译过程的一种手段，如果不做简化，呈现的效果就是上图的样子。那么，上图中两颗树的叶子节点有哪些呢？Num、+ 和 * 都是终结符，终结符都是词法分析中产生的 Token。而那些非叶子节点，就是非终结符。文法的推导过程，就是把非终结符不断替换的过程，让最后的结果没有非终结符，只有终结符。而在实际应用中，语法规则经常写成下面这种形式： add ::= mul | add + mul mul ::= pri | mul * pri pri ::= Id | Num | (add) 这种写法叫做“巴科斯范式”，简称 BNF。Antlr 和 Yacc 这两个工具都用这种写法。为了简化书写，我有时会在课程中把“::=”简化成一个冒号。你看到的时候，知道是什么意思就可以了。你有时还会听到一个术语，叫做扩展巴科斯范式 (EBNF)。它跟普通的 BNF 表达式最大的区别，就是里面会用到类似正则表达式的一些写法。比如下面这个规则中运用了 * 号，来表示这个部分可以重复 0 到多次： add -\u003e mul (+ mul)* 其实这种写法跟标准的 BNF 写法是等价的，但是更简洁。为什么是等价的呢？因为一个项多次重复，就等价于通过递归来推导。从这里我们还可以得到一个推论：就是上下文无关文法包含了正则文法，比正则文法能做更多的事情。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:6:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"确保正确的优先级 掌握了语法规则的写法之后，我们来看看如何用语法规则来保证表达式的优先级。刚刚，我们由加法规则推导到乘法规则，这种方式保证了 AST 中的乘法节点一定会在加法节点的下层，也就保证了乘法计算优先于加法计算。听到这儿，你一定会想到，我们应该把关系运算（\u003e、=、\u003c）放在加法的上层，逻辑运算（and、or）放在关系运算的上层。的确如此，我们试着将它写出来： exp -\u003e or | or = exp or -\u003e and | or || and and -\u003e equal | and \u0026\u0026 equal equal -\u003e rel | equal == rel | equal != rel rel -\u003e add | rel \u003e add | rel \u003c add | rel \u003e= add | rel \u003c= add add -\u003e mul | add + mul | add - mul mul -\u003e pri | mul * pri | mul / pri 这里表达的优先级从低到高是：赋值运算、逻辑运算（or）、逻辑运算（and）、相等比较（equal）、大小比较（rel）、加法运算（add）、乘法运算（mul）和基础表达式（pri）。实际语言中还有更多不同的优先级，比如位运算等。而且优先级是能够改变的，比如我们通常会在语法里通过括号来改变计算的优先级。不过这怎么表达成语法规则呢？其实，我们在最低层，也就是优先级最高的基础表达式（pri）这里，用括号把表达式包裹起来，递归地引用表达式就可以了。这样的话，只要在解析表达式的时候遇到括号，那么就知道这个是最优先的。这样的话就实现了优先级的改变： pri -\u003e Id | Literal | (exp) 了解了这些内容之后，到目前为止，你已经会写整套的表达式规则了，也能让公式计算器支持这些规则了。另外，在使用一门语言的时候，如果你不清楚各种运算确切的优先级，除了查阅常规的资料，你还多了一项新技能，就是阅读这门语言的语法规则文件，这些规则可能就是用 BNF 或 EBNF 的写法书写的。弄明白优先级的问题以后，我们再来讨论一下结合性这个问题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:6:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"确保正确的结合性 在上一讲中，我针对算术表达式写的第二个文法是错的，因为它的计算顺序是错的。“2+3+4”这个算术表达式，先计算了“3+4”然后才和“2”相加，计算顺序从右到左，正确的应该是从左往右才对。 这就是运算符的结合性问题。什么是结合性呢？同样优先级的运算符是从左到右计算还是从右到左计算叫做结合性。我们常见的加减乘除等算术运算是左结合的，“.”符号也是左结合的。比如“rectangle.center.x” 是先获得长方形（rectangle）的中心点（center），再获得这个点的 x 坐标。计算顺序是从左向右的。那有没有右结合的例子呢？肯定是有的。赋值运算就是典型的右结合的例子，比如“x = y = 10”。我们再来回顾一下“2+3+4”计算顺序出错的原因。用之前错误的右递归的文法解析这个表达式形成的简化版本的 AST 如下： 根据这个 AST 做计算会出现计算顺序的错误。不过如果我们将递归项写在左边，就不会出现这种结合性的错误。于是我们得出一个规律：对于左结合的运算符，递归项要放在左边；而右结合的运算符，递归项放在右边。所以你能看到，我们在写加法表达式的规则的时候，是这样写的： add -\u003e mul | add + mul 这是我们犯错之后所学到的知识。那么问题来了，大多数二元运算都是左结合的，那岂不是都要面临左递归问题？不用担心，我们可以通过改写左递归的文法，解决这个问题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:6:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"消除左递归 我提到过左递归的情况，也指出递归下降算法不能处理左递归。这里我要补充一点，并不是所有的算法都不能处理左递归，对于另外一些算法，左递归是没有问题的，比如 LR 算法。消除左递归，用一个标准的方法，就能够把左递归文法改写成非左递归的文法。以加法表达式规则为例，原来的文法是“add -\u003e add + mul”，现在我们改写成： add -\u003e mul add' add' -\u003e + mul add' | ε 文法中，ε（读作 epsilon）是空集的意思。接下来，我们用刚刚改写的规则再次推导一下 “2+3+4”这个表达式，得到了下图中左边的结果： 左边的分析树是推导后的结果。问题是，由于 add’的规则是右递归的，如果用标准的递归下降算法，我们会跟上一讲一样，又会出现运算符结合性的错误。我们期待的 AST 是右边的那棵，它的结合性才是正确的。那么有没有解决办法呢？答案是有的。我们仔细分析一下上面语法规则的推导过程。只有第一步是按照 add 规则推导，之后都是按照 add’规则推导，一直到结束。如果用 EBNF 方式表达，也就是允许用 * 号和 + 号表示重复，上面两条规则可以合并成一条： add -\u003e mul (+ mul)* 写成这样有什么好处呢？能够优化我们写算法的思路。对于 (+ mul)* 这部分，我们其实可以写成一个循环，而不是一次次的递归调用。伪代码如下： mul(); while(next token is +){ mul() createAddNode } 我们扩展一下话题。在研究递归函数的时候，有一个概念叫做尾递归，尾递归函数的最后一句是递归地调用自身。编译程序通常都会把尾递归转化为一个循环语句，使用的原理跟上面的伪代码是一样的。相对于递归调用来说，循环语句对系统资源的开销更低，因此，把尾递归转化为循环语句也是一种编译优化技术。好了，我们继续左递归的话题。现在我们知道怎么写这种左递归的算法了，大概是下面的样子： private SimpleASTNode additive(TokenReader tokens) throws Exception { SimpleASTNode child1 = multiplicative(tokens); //应用add规则 SimpleASTNode node = child1; if (child1 != null) { while (true) { //循环应用add' Token token = tokens.peek(); if (token != null \u0026\u0026 (token.getType() == TokenType.Plus || token.getType() == TokenType.Minus)) { token = tokens.read(); //读出加号 SimpleASTNode child2 = multiplicative(tokens); //计算下级节点 node = new SimpleASTNode(ASTNodeType.Additive, token.getText()); node.addChild(child1); //注意，新节点在顶层，保证正确的结合性 node.addChild(child2); child1 = node; } else { break; } } } return node; } 修改完后，再次运行语法分析器分析“2+3+4+5”，会得到正确的 AST： Programm Calculator AdditiveExp + AdditiveExp + AdditiveExp + IntLiteral 2 IntLiteral 3 IntLiteral 4 IntLiteral 5 这样，我们就把左递归问题解决了。左递归问题是我们用递归下降算法写语法分析器遇到的最大的一只“拦路虎”。解决这只“拦路虎”以后，你的道路将会越来越平坦。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:6:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 今天我们针对优先级、结合性和左递归这三个问题做了更系统的研究。我来带你梳理一下本节课的重点知识：优先级是通过在语法推导中的层次来决定的，优先级越低的，越先尝试推导。结合性是跟左递归还是右递归有关的，左递归导致左结合，右递归导致右结合。左递归可以通过改写语法规则来避免，而改写后的语法又可以表达成简洁的 EBNF 格式，从而启发我们用循环代替右递归。 为了研究和解决这三个问题，我们还特别介绍了语法规则的产生式写法以及 BNF、EBNF 写法。在后面的课程中我们会不断用到这个技能，还会用工具来生成语法分析器，我们提供给工具的就是书写良好的语法规则。到目前为止，你已经闯过了语法分析中比较难的一关。再增加一些其他的语法，你就可以实现出一个简单的脚本语言了！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:6:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"05 | 语法分析（三）：实现一门简单的脚本语言 前两节课结束后，我们已经掌握了表达式的解析，并通过一个简单的解释器实现了公式的计算。但这个解释器还是比较简单的，看上去还不大像一门语言。那么如何让它支持更多的功能，更像一门脚本语言呢？本节课，我会带你寻找答案。我将继续带你实现一些功能，比如： 支持变量声明和初始化语句，就像“int age” “int age = 45”和“int age = 17+8+20”；支持赋值语句“age = 45”；在表达式中可以使用变量，例如“age + 10 *2”；实现一个命令行终端，能够读取输入的语句并输出结果。 实现这些功能之后，我们的成果会更像一个脚本解释器。而且在这个过程中，我还会带你巩固语法分析中的递归下降算法，和你一起讨论“回溯”这个特征，让你对递归下降算法的特征理解得更加全面。不过，为了实现这些新的语法，我们首先要把它们用语法规则描述出来。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:7:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"增加所需要的语法规则 首先，一门脚本语言是要支持语句的，比如变量声明语句、赋值语句等等。单独一个表达式，也可以视为语句，叫做“表达式语句”。你在终端里输入 2+3；，就能回显出 5 来，这就是表达式作为一个语句在执行。按照我们的语法，无非是在表达式后面多了个分号而已。C 语言和 Java 都会采用分号作为语句结尾的标识，我们也可以这样写。我们用扩展巴科斯范式（EBNF）写出下面的语法规则： programm: statement+; statement : intDeclaration | expressionStatement | assignmentStatement ; 变量声明语句以 int 开头，后面跟标识符，然后有可选的初始化部分，也就是一个等号和一个表达式，最后再加分号： intDeclaration : 'int' Id ( '=' additiveExpression)? ';'; 表达式语句目前只支持加法表达式，未来可以加其他的表达式，比如条件表达式，它后面同样加分号： expressionStatement : additiveExpression ';'; 赋值语句是标识符后面跟着等号和一个表达式，再加分号： assignmentStatement : Identifier '=' additiveExpression ';'; 为了在表达式中可以使用变量，我们还需要把 primaryExpression 改写，除了包含整型字面量以外，还要包含标识符和用括号括起来的表达式： primaryExpression : Identifier| IntLiteral | '(' additiveExpression ')'; 这样，我们就把想实现的语法特性，都用语法规则表达出来了。接下来，我们就一步一步实现这些特性。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:7:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"让脚本语言支持变量 之前实现的公式计算器只支持了数字字面量的运算，如果能在表达式中用上变量，会更有用，比如能够执行下面两句： int age = 45; age + 10 * 2; 这两个语句里面的语法特性包含了变量声明、给变量赋值，以及在表达式里引用变量。为了给变量赋值，我们必须在脚本语言的解释器中开辟一个存储区，记录不同的变量和它们的值： private HashMap\u003cString, Integer\u003e variables = new HashMap\u003cString, Integer\u003e(); 我们简单地用了一个 HashMap 作为变量存储区。在变量声明语句和赋值语句里，都可以修改这个变量存储区中的数据，而获取变量值可以采用下面的代码： if (variables.containsKey(varName)) { Integer value = variables.get(varName); //获取变量值 if (value != null) { result = value; //设置返回值 } else { //有这个变量，没有值 throw new Exception(\"variable \" + varName + \" has not been set any value\"); } } else{ //没有这个变量。 throw new Exception(\"unknown variable: \" + varName); } 通过这样的一个简单的存储机制，我们就能支持变量了。当然，这个存储机制可能过于简单了，我们后面讲到作用域的时候，这么简单的存储机制根本不够。不过目前我们先这么用着，以后再考虑改进它。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:7:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"解析赋值语句 接下来，我们来解析赋值语句，例如“age = age + 10 * 2；”： java private SimpleASTNode assignmentStatement(TokenReader tokens) throws Exception { SimpleASTNode node = null; Token token = tokens.peek(); //预读，看看下面是不是标识符 if (token != null \u0026\u0026 token.getType() == TokenType.Identifier) { token = tokens.read(); //读入标识符 node = new SimpleASTNode(ASTNodeType.AssignmentStmt, token.getText()); token = tokens.peek(); //预读，看看下面是不是等号 if (token != null \u0026\u0026 token.getType() == TokenType.Assignment) { tokens.read(); //取出等号 SimpleASTNode child = additive(tokens); if (child == null) { //出错，等号右面没有一个合法的表达式 throw new Exception(\"invalide assignment statement, expecting an expression\"); } else{ node.addChild(child); //添加子节点 token = tokens.peek(); //预读，看看后面是不是分号 if (token != null \u0026\u0026 token.getType() == TokenType.SemiColon) { tokens.read(); //消耗掉这个分号 } else { //报错，缺少分号 throw new Exception(\"invalid statement, expecting semicolon\"); } } } else { tokens.unread(); //回溯，吐出之前消化掉的标识符 node = null; } } return node; } 为了方便你理解，我来解读一下上面这段代码的逻辑：我们既然想要匹配一个赋值语句，那么首先应该看看第一个 Token 是不是标识符。如果不是，那么就返回 null，匹配失败。如果第一个 Token 确实是标识符，我们就把它消耗掉，接着看后面跟着的是不是等号。如果不是等号，那证明我们这个不是一个赋值语句，可能是一个表达式什么的。那么我们就要回退刚才消耗掉的 Token，就像什么都没有发生过一样，并且返回 null。回退的时候调用的方法就是 unread()。如果后面跟着的确实是等号，那么在继续看后面是不是一个表达式，表达式后面跟着的是不是分号。如果不是，就报错就好了。这样就完成了对赋值语句的解析。 利用上面的代码，我们还可以改造一下变量声明语句中对变量初始化的部分，让它在初始化的时候支持表达式，因为这个地方跟赋值语句很像，例如“int newAge = age + 10 * 2；”。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:7:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"理解递归下降算法中的回溯 不知道你有没有发现，我在设计语法规则的过程中，其实故意设计了一个陷阱，这个陷阱能帮我们更好地理解递归下降算法的一个特点：回溯。理解这个特点能帮助你更清晰地理解递归下降算法的执行过程，从而再去想办法优化它。考虑一下 age = 45；这个语句。肉眼看过去，你马上知道它是个赋值语句，但是当我们用算法去做模式匹配时，就会发生一些特殊的情况。看一下我们对 statement 语句的定义： statement : intDeclaration | expressionStatement | assignmentStatement ; 我们首先尝试 intDeclaration，但是 age = 45；语句不是以 int 开头的，所以这个尝试会返回 null。然后我们接着尝试 expressionStatement，看一眼下面的算法： java private SimpleASTNode expressionStatement() throws Exception { int pos = tokens.getPosition(); //记下初始位置 SimpleASTNode node = additive(); //匹配加法规则 if (node != null) { Token token = tokens.peek(); if (token != null \u0026\u0026 token.getType() == TokenType.SemiColon) { //要求一定以分号结尾 tokens.read(); } else { node = null; tokens.setPosition(pos); // 回溯 } } return node; } 出现了什么情况呢？age = 45；语句最左边是一个标识符。根据我们的语法规则，标识符是一个合法的 addtiveExpresion，因此 additive() 函数返回一个非空值。接下来，后面应该扫描到一个分号才对，但是显然不是，标识符后面跟的是等号，这证明模式匹配失败。失败了该怎么办呢？我们的算法一定要把 Token 流的指针拨回到原来的位置，就像一切都没发生过一样。因为我们不知道 addtive() 这个函数往下尝试了多少步，因为它可能是一个很复杂的表达式，消耗掉了很多个 Token，所以我们必须记下算法开始时候的位置，并在失败时回到这个位置。尝试一个规则不成功之后，恢复到原样，再去尝试另外的规则，这个现象就叫做“回溯”。 因为有可能需要回溯，所以递归下降算法有时会做一些无用功。在 assignmentStatement 的算法中，我们就通过 unread()，回溯了一个 Token。而在 expressionStatement 中，我们不确定要回溯几步，只好提前记下初始位置。匹配 expressionStatement 失败后，算法去尝试匹配 assignmentStatement。这次获得了成功。试探和回溯的过程，是递归下降算法的一个典型特征。通过上面的例子，你应该对这个典型特征有了更清晰的理解。递归下降算法虽然简单，但它通过试探和回溯，却总是可以把正确的语法匹配出来，这就是它的强大之处。当然，缺点是回溯会拉低一点儿效率。但我们可以在这个基础上进行改进和优化，实现带有预测分析的递归下降，以及非递归的预测分析。有了对递归下降算法的清晰理解，我们去学习其他的语法分析算法的时候，也会理解得更快。 我们接着再讲回溯牵扯出的另一个问题：**什么时候该回溯，什么时候该提示语法错误？**大家在阅读示例代码的过程中，应该发现里面有一些错误处理的代码，并抛出了异常。比如在赋值语句中，如果等号后面没有成功匹配一个加法表达式，我们认为这个语法是错的。因为在我们的语法中，等号后面只能跟表达式，没有别的可能性。 token = tokens.read(); //读出等号 node = additive(); //匹配一个加法表达式 if (node == null) { //等号右边一定需要有另一个表达式 throw new Exception(\"invalide assignment expression, expecting an additive expression\"); } 你可能会意识到一个问题，当我们在算法中匹配不成功的时候，我们前面说的是应该回溯呀，应该再去尝试其他可能性呀，为什么在这里报错了呢？换句话说，什么时候该回溯，什么时候该提示这里发生了语法错误呢？其实这两种方法最后的结果是一样的。我们提示语法错误的时候，是说我们知道已经没有其他可能的匹配选项了，不需要浪费时间去回溯。就比如，在我们的语法中，等号后面必然跟表达式，否则就一定是语法错误。你在这里不报语法错误，等试探完其他所有选项后，还是需要报语法错误。所以说，提前报语法错误，实际上是我们写算法时的一种优化。在写编译程序的时候，我们不仅仅要能够解析正确的语法，还要尽可能针对语法错误提供友好的提示，帮助用户迅速定位错误。错误定位越是准确、提示越是友好，我们就越喜欢它。好了，到目前为止，已经能够能够处理几种不同的语句，如变量声明语句，赋值语句、表达式语句，那么我们把所有这些成果放到一起，来体会一下使用自己的脚本语言的乐趣吧！我们需要一个交互式的界面来输入程序，并执行程序，这个交互式的界面就叫做 REPL。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:7:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现一个简单的 REPL 脚本语言一般都会提供一个命令行窗口，让你输入一条一条的语句，马上解释执行它，并得到输出结果，比如 Node.js、Python 等都提供了这样的界面。这个输入、执行、打印的循环过程就叫做 REPL（Read-Eval-Print Loop）。你可以在 REPL 中迅速试验各种语句，REPL 即时反馈的特征会让你乐趣无穷。所以，即使是非常资深的程序员，也会经常用 REPL 来验证自己的一些思路，它相当于一个语言的 PlayGround（游戏场），是个必不可少的工具。在 SimpleScript.java 中，我们也实现了一个简单的 REPL。基本上就是从终端一行行的读入代码，当遇到分号的时候，就解释执行，代码如下： java SimpleParser parser = new SimpleParser(); SimpleScript script = new SimpleScript(); BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); //从终端获取输入 String scriptText = \"\"; System.out.print(\"\\n\u003e\"); //提示符 while (true) { //无限循环 try { String line = reader.readLine().trim(); //读入一行 if (line.equals(\"exit();\")) { //硬编码退出条件 System.out.println(\"good bye!\"); break; } scriptText += line + \"\\n\"; if (line.endsWith(\";\")) { //如果没有遇到分号的话，会再读一行 ASTNode tree = parser.parse(scriptText); //语法解析 if (verbose) { parser.dumpAST(tree, \"\"); } script.evaluate(tree, \"\"); //对AST求值，并打印 System.out.print(\"\\n\u003e\"); //显示一个提示符 scriptText = \"\"; } } catch (Exception e) { //如果发现语法错误，报错，然后可以继续执行 System.out.println(e.getLocalizedMessage()); System.out.print(\"\\n\u003e\"); //提示符 scriptText = \"\"; } } 运行 java craft.SimpleScript，你就可以在终端里尝试各种语句了。如果是正确的语句，系统马上会反馈回结果。如果是错误的语句，REPL 还能反馈回错误信息，并且能够继续处理下面的语句。我们前面添加的处理语法错误的代码，现在起到了作用！下面是在我电脑上的运行情况： 如果你用 java craft.SimpleScript -v 启动 REPL，则进入 Verbose 模式，它还会每次打印出 AST，你可以尝试一下。退出 REPL 需要在终端输入 ctl+c，或者调用 exit() 函数。我们目前的解释器并没有支持函数，所以我们是在 REPL 里硬编码来实现 exit() 函数的。后面的课程里，我会带你真正地实现函数特性。我希望你能编译一下这个程序，好好的玩一玩它，然后再修改一下源代码，增加一些你感兴趣的特性。我们学习跟打游戏一样，好玩、有趣才能驱动我们不停地学下去，一步步升级打怪。我个人觉得，我们作为软件工程师，拿出一些时间来写点儿有趣的东西作为消遣，乐趣和成就感也是很高的，况且还能提高水平。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:7:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课我们通过对三种语句的支持，实现了一个简单的脚本语言。REPL 运行代码的时候，你会有一种真真实实的感觉，这确实是一门脚本语言了，虽然它没做性能的优化，但你运行的时候也还觉得挺流畅。学完这讲以后，你也能找到了一点感觉：Shell 脚本也好，PHP 也好，JavaScript 也好，Python 也好，其实都可以这样写出来。回顾过去几讲，你已经可以分析词法、语法、进行计算，还解决了左递归、优先级、结合性的问题。甚至，你还能处理语法错误，让脚本解释器不会因为输入错误而崩溃。想必这个时候你已经开始相信我的承诺了：每个人都可以写一个编译器。这其实也是我最想达到的效果。相信自己，只要你不给自己设限，不设置玻璃天花板，其实你能够做出很多让自己惊讶、让自己骄傲的成就。收获对自己的信心，掌握编译技术，将是你学习这门课程后最大的收获！ 另外，第 2 讲到第 5 讲的代码，都在代码库中的 lab 子目录的 craft 子目录下，代码库在码云和GitHub上都有，希望你能下载玩一玩。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:7:6","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"06 | 编译器前端工具（一）：用Antlr生成词法、语法分析器 前面的课程中，我重点讲解了词法分析和语法分析，在例子中提到的词法和语法规则也是高度简化的。虽然这些内容便于理解原理，也能实现一个简单的原型，在实际应用中却远远不够。实际应用中，一个完善的编译程序还要在词法方面以及语法方面实现很多工作，我这里特意画了一张图，你可以直观地看一下。 如果让编译程序实现上面这么多工作，完全手写效率会有点儿低，那么我们有什么方法可以提升效率呢？答案是借助工具。编译器前端工具有很多，比如 Lex（以及 GNU 的版本 Flex）、Yacc（以及 GNU 的版本 Bison）、JavaCC 等等。你可能会问了：“那为什么我们这节课只讲 Antlr，不选别的工具呢？”主要有两个原因。第一个原因是 Antlr 能支持更广泛的目标语言，包括 Java、C#、JavaScript、Python、Go、C++、Swift。无论你用上面哪种语言，都可以用它生成词法和语法分析的功能。而我们就使用它生成了 Java 语言和 C++ 语言两个版本的代码。第二个原因是 Antlr 的语法更加简单。它能把类似左递归的一些常见难点在工具中解决，对提升工作效率有很大的帮助。这一点，你会在后面的课程中直观地感受到。而我们今天的目标就是了解 Antlr，然后能够使用 Antlr 生成词法分析器与语法分析器。在这个过程中，我还会带你借鉴成熟的词法和语法规则，让你快速成长。接下来，我们先来了解一下 Antlr 这个工具。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:8:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"初识 Antlr Antlr 是一个开源的工具，支持根据规则文件生成词法分析器和语法分析器，它自身是用 Java 实现的。你可以下载 Antlr 工具，并根据说明做好配置。同时，你还需要配置好机器上的 Java 环境（可以在Oracle 官网找到最新版本的 JDK）。 因为我用的是 Mac，所以我用 macOS 平台下的软件包管理工具 Homebrew 安装了 Antlr，它可以自动设置好 antlr 和 grun 两个命令（antlr 和 grun 分别是 java org.antlr.v4.Tool 和 java org.antlr.v4.gui.TestRig 这两个命令的别名）。这里需要注意的是，你要把 Antlr 的 JAR 文件设置到 CLASSPATH 环境变量中，以便顺利编译所生成的 Java 源代码。GitHub上还有很多供参考的语法规则，你可以下载到本地硬盘随时查阅。现在你已经对 Antlr 有了初步的了解，也知道如何安装它了。接下来，我带你实际用一用 Antlr，让你用更轻松的方式生成词法分析器和语法分析器。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:8:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"用 Antlr 生成词法分析器 你可能对 Antlr 还不怎么熟悉，所以我会先带你使用前面课程中，你已经比较熟悉的那些词法规则，让 Antlr 生成一个新的词法分析器，然后再借鉴一些成熟的规则文件，把词法分析器提升到更加专业、实用的级别。Antlr 通过解析规则文件来生成编译器。规则文件以.g4 结尾，词法规则和语法规则可以放在同一个文件里。不过为了清晰起见，我们还是把它们分成两个文件，先用一个文件编写词法规则。为了让你快速进入状态，我们先做一个简单的练习预热一下。我们创建一个 Hello.g4 文件，用于保存词法规则，然后把之前用过的一些词法规则写进去。 lexer grammar Hello; //lexer关键字意味着这是一个词法规则文件，名称是Hello，要与文件名相同 //关键字 If : 'if'; Int : 'int'; //字面量 IntLiteral: [0-9]+; StringLiteral: '\"' .*? '\"' ; //字符串字面量 //操作符 AssignmentOP: '=' ; RelationalOP: '\u003e'|'\u003e='|'\u003c' |'\u003c=' ; Star: '*'; Plus: '+'; Sharp: '#'; SemiColon: ';'; Dot: '.'; Comm: ','; LeftBracket : '['; RightBracket: ']'; LeftBrace: '{'; RightBrace: '}'; LeftParen: '('; RightParen: ')'; //标识符 Id : [a-zA-Z_] ([a-zA-Z_] | [0-9])*; //空白字符，抛弃 Whitespace: [ \\t]+ -\u003e skip; Newline: ( '\\r' '\\n'?|'\\n')-\u003e skip; 你能很直观地看到，每个词法规则都是大写字母开头，这是 Antlr 对词法规则的约定。而语法规则是以小写字母开头的。其中，每个规则都是用我们已经了解的正则表达式编写的。接下来，我们来编译词法规则，在终端中输入命令： antlr Hello.g4 这个命令是让 Antlr 编译规则文件，并生成 Hello.java 文件和其他两个辅助文件。你可以打开看一看文件里面的内容。接着，我用下面的命令编译 Hello.java： javac *.java 结果会生成 Hello.class 文件，这就是我们生成的词法分析器。接下来，我们来写个脚本文件，让生成的词法分析器解析一下： int age = 45; if (age \u003e= 17+8+20){ printf(\"Hello old man!\"); } 我们将上面的脚本存成 hello.play 文件，然后在终端输入下面的命令： grun Hello tokens -tokens hello.play grun 命令实际上是调用了我们刚才生成的词法分析器，即 Hello 类，打印出对 hello.play 词法分析的结果： 从结果中看到，我们的词法分析器把每个 Token 都识别了，还记录了它们在代码中的位置、文本值、类别。上面这些都是 Token 的属性。以第二行[@1, 4:6=‘age’,\u003c Id \u003e,1:4]为例，其中 @1 是 Token 的流水编号，表明这是 1 号 Token；4:6 是 Token 在字符流中的开始和结束位置；age 是文本值，Id 是其 Token 类别；最后的 1:4 表示这个 Token 在源代码中位于第 1 行、第 4 列。非常好，现在我们已经让 Antlr 顺利跑起来了！接下来，让词法规则更完善、更严密一些吧！怎么做呢？当然是参考成熟的规则文件。 从 Antlr 的一些示范性的规则文件中，我选了 Java 的作为参考。先看看我们之前写的字符串字面量的规则： StringLiteral: '\"' .*? '\"' ; //字符串字面量 我们的版本相当简化，就是在双引号可以包含任何字符。可这在实际中不大好用，因为连转义功能都没有提供。我们对于一些不可见的字符，比如回车，要提供转义功能，如“\\n”。同时，如果字符串里本身有双引号的话，也要将它转义，如“\\”。Unicode 也要转义。最后，转义字符本身也需要转义，如“\\”。下面这一段内容是 Java 语言中的字符串字面量的完整规则。你可以看一下文稿，这个规则就很细致了，把各种转义的情况都考虑进去了： STRING_LITERAL: '\"' (~[\"\\\\\\r\\n] | EscapeSequence)* '\"'; fragment EscapeSequence : '\\\\' [btnfr\"'\\\\] | '\\\\' ([0-3]? [0-7])? [0-7] | '\\\\' 'u'+ HexDigit HexDigit HexDigit HexDigit ; fragment HexDigit : [0-9a-fA-F] ; 在这个规则文件中，fragment 指的是一个语法片段，是为了让规则定义更清晰。它本身并不生成 Token，只有 StringLiteral 规则才会生成 Token。当然了，除了字符串字面量，数字字面量、标识符的规则也可以定义得更严密。不过，因为这些规则文件都很严密，写出来都很长，在这里我就不一一展开了。如果感兴趣，我推荐你在下载的规则文件中找到这些部分看一看。你还可以参考不同作者写的词法规则，体会一下他们的设计思路。和高手过招，会更快地提高你的水平。我也拷贝了一些成熟的词法规则，编写了一个 CommonLexer.g4 的规则文件，这个词法规则是我们后面工作的基础，它基本上已经达到了专业、实用的程度。在带你借鉴了成熟的规则文件之后，我想穿插性地讲解一下在词法规则中对 Token 归类的问题。在设计词法规则时，你经常会遇到这个问题，解决这个问题，词法规则会更加完善。在前面练习的规则文件中，我们把 \u003e=、\u003e、\u003c 都归类为关系运算符，算作同一类 Token，而 +、* 等都单独作为另一类 Token。那么，哪些可以归并成一类，哪些又是需要单独列出的呢？ 其实，这主要取决于语法的需要。也就是在语法规则文件里，是否可以出现在同一条规则里。它们在语法层面上没有区别，只是在语义层面上有区别。比如，加法和减法虽然是不同的运算，但它们可以同时出现在同一条语法规则中，它们在运算时的特性完全一致，包括优先级和结合性，乘法和除法可以同时出现在乘法规则中。你把加号和减号合并成一类，把乘号和除号合并成一类是可以的。把这 4 个运算符每个都单独作为一类，也是可以的。但是，不能把加号和乘号作为同一类，因为它们在算术运算中的优先级不同，肯定出现在不同的语法规则中。我们再来回顾一下在“02 | 正则文法和有限自动机：纯手工打造词法分析器”里做词法分析时遇到的一个问题。当时，我们分析了词法冲突的问题，即标识符和关键字的规则是有重叠的。Antlr 是怎么解决这个问题的呢？很简单，它引入了优先级的概念。在 Antlr 的规则文件中，越是前面声明的规则，优先级越高。所以，我们把关键字的规则放在 ID 的规则前面。算法在执行的时候，会首先检查是否为关键字，然后才会检查是否为 ID，也就是标识符。 这跟我们当时构造有限自动机做词法分析是一样的。那时，我们先判断是不是关键字，如果不是关键字，才识别为标识符。而在 Antlr 里，仅仅通过声明的顺序就解决了这个问题，省了很多事儿啊！再说个有趣的题外话。之前国内有人提“中文编程语言”的概念，也就是语法中的关键字采用中文，比如“如果”“那么”等。他们似乎觉得这样更容易理解和掌握。我不太提倡这种想法，别的不说，用中文写关键字和变量名，需要输入更多的字符，有点儿麻烦。中国的英语教育很普及，用英语来写代码，其实就够了。不过，你大可以试一下，让自己的词法规则支持中文关键字。比如，把“If”的规则改成同时支持英文的“if”，以及中文的“如果”： If: 'if' | '如果'; 再把测试用的脚本 hello.play 中的“if”也改成“如果”，写成： 如果 (age \u003e= 17+8+20){ 重新生成词法分析器并运行，你会发现输出中有这么一行： [@5,14:15='如果',\u003cIf\u003e,2:0] 这个 Token 的文本值是“如果”，但类别仍然是“If”。所以，要想实现所谓的“中文编程语言”，把 C、Java 等语言的词法规则改一改，再把编译器重新编译一下就行了！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:8:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"用 Antlr 生成语法分析器 说回我们的话题。现在，你已经知道如何用 Antlr 做一个词法分析器，还知道可以借鉴成熟的规则文件，让自己的词法规则文件变得更完善、更专业。接下来，试着用 Antlr 生成一个语法分析器，替代之前手写的语法分析器吧！这一次的文件名叫做 PlayScript.g4。playscript 是为我们的脚本语言起的名称，文件开头是这样的： grammar PlayScript; import CommonLexer; //导入词法定义 /*下面的内容加到所生成的Java源文件的头部，如包名称，import语句等。*/ @header { package antlrtest; } 然后把之前做过的语法定义放进去。Antlr 内部有自动处理左递归的机制，你可以放心大胆地把语法规则写成下面的样子： expression : assignmentExpression | expression ',' assignmentExpression ; assignmentExpression : additiveExpression | Identifier assignmentOperator additiveExpression ; assignmentOperator : '=' | '*=' | '/=' | '%=' | '+=' | '-=' ; additiveExpression : multiplicativeExpression | additiveExpression '+' multiplicativeExpression | additiveExpression '-' multiplicativeExpression ; multiplicativeExpression : primaryExpression | multiplicativeExpression '*' primaryExpression | multiplicativeExpression '/' primaryExpression | multiplicativeExpression '%' primaryExpression ; 你可能会问：“既然用 Antlr 可以不管左递归问题，那之前为什么要费力气解决它呢？”那是因为当你遇到某些问题却没有现成工具时，还是要用纯手工的方法去解决问题。而且，有的工具可能没有这么智能，你需要写出符合这个工具的规则文件，比如说不能有左递归的语法规则。还是那句话：懂得基础原理，会让你站得更高。我们继续运行下面的命令，生成语法分析器： antlr PlayScript.g4 javac antlrtest/*.java 然后测试一下生成的语法分析器： grun antlrtest.PlayScript expression -gui 这个命令的意思是：测试 PlayScript 这个类的 expression 方法，也就是解析表达式的方法，结果用图形化界面显示。我们在控制台界面中输入下面的内容： age + 10 * 2 + 10 ^D 其中 ^D 是按下 Ctl 键的同时按下 D，相当于在终端输入一个 EOF 字符，即文件结束符号（Windows 操作系统要使用 ^Z）。当然，你也可以提前把这些语句放到文件中，把文件名作为命令参数。之后，语法分析器会分析这些语法，并弹出一个窗口来显示 AST： 看得出来，AST 完全正确，优先级和结合性也都没错。所以，Antlr 生成的语法分析器还是很靠谱的。以后，你专注写语法规则就行了，可以把精力放在语言的设计和应用上。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:8:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 今天，我带你了解了 Antlr，并用 Antlr 生成了词法分析器和语法分析器。有了工具的支持，你可以把主要的精力放在编写词法和语法规则上，提升了工作效率。除此之外，我带你借鉴了成熟的词法规则和语法规则。你可以将这些规则用到自己的语言设计中。采用工具和借鉴成熟规则十分重要，站在别人的肩膀上能让自己更快成长。在后面的课程中，我会带你快速实现报表工具、SQL 解析器这种需要编译功能的应用。那时，你就更能体会到，用编译技术实现一个功能的过程，是非常高效的！与此同时，我也会带你扩展更多的语法规则，并生成一个更强大的脚本语言解释器。这样，你就会实现流程控制语句，接着探索函数、闭包、面向对象功能的实现机制。几节课之后，你的手里就真的有一门不错的脚本语言了！ 本讲的示例代码位于 lab/antlrtest，代码链接我放在了文末，供你参考。Hello.g4（用 Antlr 重写了前几讲的词法规则）：码云 GitHub CommonLexer.g4（比较成熟的词法文件）：码云 GitHub PlayScript.g4（用 Antlr 重写了前几讲的语法规则）：码云 GitHub ASTEvaluator.java（对 AST 遍历，实现整数的算术运算）：码云 GitHub PlayScript.java（一个测试程序，实现词法分析、语法分析、公式计算）：码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:8:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"07 | 编译器前端工具（二）：用Antlr重构脚本语言 上一讲，我带你用 Antlr 生成了词法分析器和语法分析器，也带你分析了，跟一门成熟的语言相比，在词法规则和语法规则方面要做的一些工作。在词法方面，我们参考 Java 的词法规则文件，形成了一个 CommonLexer.g4 词法文件。在这个过程中，我们研究了更完善的字符串字面量的词法规则，还讲到要通过规则声明的前后顺序来解决优先级问题，比如关键字的规则一定要在标识符的前面。目前来讲，我们已经完善了词法规则，所以今天我们来补充和完善一下语法规则，看一看怎样用最高效的速度，完善语法功能。比如一天之内，我们是否能为某个需要编译技术的项目实现一个可行性原型？而且，我还会带你熟悉一下常见语法设计的最佳实践。这样当后面的项目需要编译技术做支撑时，你就会很快上手，做出成绩了！接下来，我们先把表达式的语法规则梳理一遍，让它达到成熟语言的级别，然后再把语句梳理一遍，包括前面几乎没有讲过的流程控制语句。最后再升级解释器，用 Visitor 模式实现对 AST 的访问，这样我们的代码会更清晰，更容易维护了。好了，让我们正式进入课程，先将表达式的语法完善一下吧！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:9:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"完善表达式（Expression）的语法 在“06 | 编译器前端工具（一）：用 Antlr 生成词法、语法分析器”中，我提到 Antlr 能自动处理左递归的问题，所以在写表达式时，我们可以大胆地写成左递归的形式，节省时间。但这样，我们还是要为每个运算写一个规则，逻辑运算写完了要写加法运算，加法运算写完了写乘法运算，这样才能实现对优先级的支持，还是有些麻烦。其实，Antlr 能进一步地帮助我们。我们可以把所有的运算都用一个语法规则来涵盖，然后用最简洁的方式支持表达式的优先级和结合性。在我建立的 PlayScript.g4 语法规则文件中，只用了一小段代码就将所有的表达式规则描述完了： expression : primary | expression bop='.' ( IDENTIFIER | functionCall | THIS ) | expression '[' expression ']' | functionCall | expression postfix=('++' | '--') | prefix=('+'|'-'|'++'|'--') expression | prefix=('~'|'!') expression | expression bop=('*'|'/'|'%') expression | expression bop=('+'|'-') expression | expression ('\u003c' '\u003c' | '\u003e' '\u003e' '\u003e' | '\u003e' '\u003e') expression | expression bop=('\u003c=' | '\u003e=' | '\u003e' | '\u003c') expression | expression bop=INSTANCEOF typeType | expression bop=('==' | '!=') expression | expression bop='\u0026' expression | expression bop='^' expression | expression bop='|' expression | expression bop='\u0026\u0026' expression | expression bop='||' expression | expression bop='?' expression ':' expression | \u003cassoc=right\u003e expression bop=('=' | '+=' | '-=' | '*=' | '/=' | '\u0026=' | '|=' | '^=' | '\u003e\u003e=' | '\u003e\u003e\u003e=' | '\u003c\u003c=' | '%=') expression ; 这个文件几乎包括了我们需要的所有的表达式规则，包括几乎没提到的点符号表达式、递增和递减表达式、数组表达式、位运算表达式规则等，已经很完善了。那么它是怎样支持优先级的呢？原来，优先级是通过右侧不同产生式的顺序决定的。在标准的上下文无关文法中，产生式的顺序是无关的，但在具体的算法中，会按照确定的顺序来尝试各个产生式。你不可能一会儿按这个顺序，一会儿按那个顺序。然而，同样的文法，按照不同的顺序来推导的时候，得到的 AST 可能是不同的。我们需要注意，这一点从文法理论的角度，是无法接受的，但从实践的角度，是可以接受的。比如 LL 文法和 LR 文法的概念，是指这个文法在 LL 算法或 LR 算法下是工作正常的。又比如我们之前做加法运算的那个文法，就是递归项放在右边的那个，在递归下降算法中会引起结合性的错误，但是如果用 LR 算法，就完全没有这个问题，生成的 AST 完全正确。 additiveExpression : IntLiteral | IntLiteral Plus additiveExpression ; Antlr 的这个语法实际上是把产生式的顺序赋予了额外的含义，用来表示优先级，提供给算法。所以，我们可以说这些文法是 Antlr 文法，因为是与 Antlr 的算法相匹配的。当然，这只是我起的一个名字，方便你理解，免得你产生困扰。我们再来看看 Antlr 是如何依据这个语法规则实现结合性的。在语法文件中，Antlr 对于赋值表达式做了 \u003c assoc=right\u003e 的属性标注，说明赋值表达式是右结合的。如果不标注，就是左结合的，交给 Antlr 实现了！我们不妨继续猜测一下 Antlr 内部的实现机制。我们已经分析了保证正确的结合性的算法，比如把递归转化成循环，然后在构造 AST 时，确定正确的父子节点关系。那么 Antlr 是不是也采用了这样的思路呢？或者说还有其他方法？你可以去看看 Antlr 生成的代码验证一下。 在思考这个问题的同时你会发现，学习原理是很有用的。因为当你面对 Antlr 这样工具时，能够猜出它的实现机制。通过这个简化的算法，AST 被成功简化，不再有加法节点、乘法节点等各种不同的节点，而是统一为表达式节点。你可能会问了：“如果都是同样的表达式节点，怎么在解析器里把它们区分开呢？怎么知道哪个节点是做加法运算或乘法运算呢？”很简单，我们可以查找一下当前节点有没有某个运算符的 Token。比如，如果出现了或者运算的 Token（“||”），就是做逻辑或运算，而且语法里面的 bop=、postfix=、prefix= 这些属性，作为某些运算符 Token 的别名，也会成为表达式节点的属性。通过查询这些属性的值，你可以很快确定当前运算的类型。到目前为止，我们彻底完成了表达式的语法工作，可以放心大胆地在脚本语言里使用各种表达式，把精力放在完善各类语句的语法工作上了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:9:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"完善各类语句（Statement）的语法 我先带你分析一下 PlayScript.g4 文件中语句的规则： statement : blockLabel=block | IF parExpression statement (ELSE statement)? | FOR '(' forControl ')' statement | WHILE parExpression statement | DO statement WHILE parExpression ';' | SWITCH parExpression '{' switchBlockStatementGroup* switchLabel* '}' | RETURN expression? ';' | BREAK IDENTIFIER? ';' | SEMI | statementExpression=expression ';' ; 同表达式一样，一个 statement 规则就可以涵盖各类常用语句，包括 if 语句、for 循环语句、while 循环语句、switch 语句、return 语句等等。表达式后面加一个分号，也是一种语句，叫做表达式语句。从语法分析的难度来看，上面这些语句的语法比表达式的语法简单的多，左递归、优先级和结合性的问题这里都没有出现。这也算先难后易，苦尽甘来了吧。实际上，我们后面要设计的很多语法，都没有想象中那么复杂。既然我们尝到了一些甜头，不如趁热打铁，深入研究一下 if 语句和 for 语句？看看怎么写这些语句的规则？多做这样的训练，再看到这些语句，你的脑海里就能马上反映出它的语法规则。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:9:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"1. 研究一下 if 语句 在 C 和 Java 等语言中，if 语句通常写成下面的样子： if (condition) 做一件事情; else 做另一件事情; 但更多情况下，if 和 else 后面是花括号起止的一个语句块，比如： if (condition){ 做一些事情； } else{ 做另一些事情； } 它的语法规则是这样的： statement : ... | IF parExpression statement (ELSE statement)? ... ; parExpression : '(' expression ')'; 我们用了 IF 和 ELSE 这两个关键字，也复用了已经定义好的语句规则和表达式规则。你看，语句规则和表达式规则一旦设计完毕，就可以被其他语法规则复用，多么省心！但是 if 语句也有让人不省心的地方，比如会涉及到二义性文法问题。所以，接下来我们就借 if 语句，分析一下二义性文法这个现象。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:9:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"2. 解决二义性文法 学计算机语言的时候，提到 if 语句，会特别提一下嵌套 if 语句和悬挂 else 的情况，比如下面这段代码： if (a \u003e b) if (c \u003e d) 做一些事情； else 做另外一些事情； 在上面的代码中，我故意取消了代码的缩进。那么，你能不能看出 else 是跟哪个 if 配对的呢？一旦你语法规则写得不够好，就很可能形成二义性，也就是用同一个语法规则可以推导出两个不同的句子，或者说生成两个不同的 AST。这种文法叫做二义性文法，比如下面这种写法： stmt -\u003e if expr stmt | if expr stmt else stmt | other 按照这个语法规则，先采用第一条产生式推导或先采用第二条产生式推导，会得到不同的 AST。左边的这棵 AST 中，else 跟第二个 if 配对；右边的这棵 AST 中，else 跟第一个 if 配对。 大多数高级语言在解析这个示例代码时都会产生第一个 AST，即 else 跟最邻近的 if 配对，也就是下面这段带缩进的代码表达的意思： if (a \u003e b) if (c \u003e d) 做一些事情； else 做另外一些事情； 那么，有没有办法把语法写成没有二义性的呢？当然有了。 stmt -\u003e fullyMatchedStmt | partlyMatchedStmt fullyMatchedStmt -\u003e if expr fullyMatchedStmt else fullyMatchedStmt | other partlyMatchedStmt -\u003e if expr stmt | if expr fullyMatchedStmt else partlyMatchedStmt 按照上面的语法规则，只有唯一的推导方式，也只能生成唯一的 AST： 其中，解析第一个 if 语句时只能应用 partlyMatchedStmt 规则，解析第二个 if 语句时，只能适用 fullyMatchedStmt 规则。这时，我们就知道可以通过改写语法规则来解决二义性文法。至于怎么改写规则，确实不像左递归那样有清晰的套路，但是可以多借鉴成熟的经验。再说回我们给 Antlr 定义的语法，这个语法似乎并不复杂，怎么就能确保不出现二义性问题呢？因为 Antlr 解析语法时用到的是 LL 算法。LL 算法是一个深度优先的算法，所以在解析到第一个 statement 时，就会建立下一级的 if 节点，在下一级节点里会把 else 子句解析掉。如果 Antlr 不用 LL 算法，就会产生二义性。这再次验证了我们前面说的那个知识点：文法要经常和解析算法配合。分析完 if 语句，并借它说明了二义性文法之后，我们再针对 for 语句做一个案例研究。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:9:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"3. 研究一下 for 语句 for 语句一般写成下面的样子： for (int i = 0; i \u003c 10; i++){ println(i); } 相关的语法规则如下： statement : ... | FOR '(' forControl ')' statement ... ; forControl : forInit? ';' expression? ';' forUpdate=expressionList? ; forInit : variableDeclarators | expressionList ; expressionList : expression (',' expression)* ; 从上面的语法规则中看到，for 语句归根到底是由语句、表达式和变量声明构成的。代码中的 for 语句，解析后形成的 AST 如下： 熟悉了 for 语句的语法之后，我想提一下语句块（block）。在 if 语句和 for 语句中，会用到它，所以我捎带着把语句块的语法构成写了一下，供你参考： block : '{' blockStatements '}' ; blockStatements : blockStatement* ; blockStatement : variableDeclarators ';' //变量声明 | statement | functionDeclaration //函数声明 | classDeclaration //类声明 ; 现在，我们已经拥有了一个相当不错的语法体系，除了要放到后面去讲的函数、类有关的语法之外，我们几乎完成了 playscript 的所有的语法设计工作。接下来，我们再升级一下脚本解释器，让它能够支持更多的语法，同时通过使用 Visitor 模式，让代码结构更加完善。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:9:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"用 Vistor 模式升级脚本解释器 我们在纯手工编写的脚本语言解释器里，用了一个 evaluate() 方法自上而下地遍历了整棵树。随着要处理的语法越来越多，这个方法的代码量会越来越大，不便于维护。而 Visitor 设计模式针对每一种 AST 节点，都会有一个单独的方法来负责处理，能够让代码更清晰，也更便于维护。Antlr 能帮我们生成一个 Visitor 处理模式的框架，我们在命令行输入： antlr -visitor PlayScript.g4 -visitor 参数告诉 Antlr 生成下面两个接口和类： public interface PlayScriptVisitor\u003cT\u003e extends ParseTreeVisitor\u003cT\u003e {...} public class PlayScriptBaseVisitor\u003cT\u003e extends AbstractParseTreeVisitor\u003cT\u003e implements PlayScriptVisitor\u003cT\u003e {...} 在 PlayScriptBaseVisitor 中，可以看到很多 visitXXX() 这样的方法，每一种 AST 节点都对应一个方法，例如： @Override public T visitPrimitiveType(PlayScriptParser.PrimitiveTypeContext ctx) {...} 其中泛型 \u003c T \u003e 指的是访问每个节点时返回的数据的类型。在我们手工编写的版本里，当时只处理整数，所以返回值一律用 Integer，现在我们实现的版本要高级一点，AST 节点可能返回各种类型的数据，比如：浮点型运算的时候，会返回浮点数；字符类型运算的时候，会返回字符型数据；还可能是程序员自己设计的类型，如某个类的实例。 所以，我们就让 Visitor 统一返回 Object 类型好了，能够适用于各种情况。这样，我们的 Visitor 就是下面的样子（泛型采用了 Object）： public class MyVisitor extends PlayScriptBaseVisitor\u003cObject\u003e{ ... } 这样，在 visitExpression() 方法中，我们可以编写各种表达式求值的代码，比如，加法和减法运算的代码如下： java public Object visitExpression(ExpressionContext ctx) { Object rtn = null; //二元表达式 if (ctx.bop != null \u0026\u0026 ctx.expression().size() \u003e= 2) { Object left = visitExpression(ctx.expression(0)); Object right = visitExpression(ctx.expression(1)); ... Type type = cr.node2Type.get(ctx);//数据类型是语义分析的成果 switch (ctx.bop.getType()) { case PlayScriptParser.ADD: //加法运算 rtn = add(leftObject, rightObject, type); break; case PlayScriptParser.SUB: //减法运算 rtn = minus(leftObject, rightObject, type); break; ... } } ... } 其中 ExpressionContext 就是 AST 中表达式的节点，叫做 Context，意思是你能从中取出这个节点所有的上下文信息，包括父节点、子节点等。其中，每个子节点的名称跟语法中的名称是一致的，比如加减法语法规则是下面这样： expression bop=('+'|'-') expression 那么我们可以用 ExpressionContext 的这些方法访问子节点： ctx.expression(); //返回一个列表，里面有两个成员，分别是左右两边的子节点 ctx.expression(0); //运算符左边的表达式，是另一个ExpressionContext对象 ctx.expression(1); //云算法右边的表达式 ctx.bop(); //一个Token对象，其类型是PlayScriptParser.ADD或SUB ctx.ADD(); //访问ADD终结符，当做加法运算的时候，该方法返回非空值 ctx.MINUS()； //访问MINUS终结符 在做加法运算的时候我们还可以递归的对下级节点求值，就像代码里的 visitExpression(ctx.expression(0))。同样，要想运行整个脚本，我们只需要 visit 根节点就行了。所以，我们可以用这样的方式，为每个 AST 节点实现一个 visit 方法。从而把整个解释器升级一遍。除了实现表达式求值，我们还可以为今天设计的 if 语句、for 语句来编写求值逻辑。以 for 语句为例，代码如下： // 初始化部分执行一次 if (forControl.forInit() != null) { rtn = visitForInit(forControl.forInit()); } while (true) { Boolean condition = true; // 如果没有条件判断部分，意味着一直循环 if (forControl.expression() != null) { condition = (Boolean) visitExpression(forControl.expression()); } if (condition) { // 执行for的语句体 rtn = visitStatement(ctx.statement(0)); // 执行forUpdate，通常是“i++”这样的语句。这个执行顺序不能出错。 if (forControl.forUpdate != null) { visitExpressionList(forControl.forUpdate); } } else { break; } } 你需要注意 for 语句中各个部分的执行规则，比如：forInit 部分只能执行一次；每次循环都要执行一次 forControl，看看是否继续循环；接着执行 for 语句中的语句体；最后执行 forUpdate 部分，通常是一些“i++”这样的语句。 支持了这些流程控制语句以后，我们的脚本语言就更丰富了！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:9:6","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 今天，我带你用 Antlr 高效地完成了很多语法分析工作，比如完善表达式体系，完善语句体系。除此之外，我们还升级了脚本解释器，使它能够执行更多的表达式和语句。在实际工作中，针对面临的具体问题，我们完全可以像今天这样迅速地建立可以运行的代码，专注于解决领域问题，快速发挥编译技术的威力。而且在使用工具时，针对工具的某个特性，比如对优先级和结合性的支持，我们大致能够猜到工具内部的实现机制，因为我们已经了解了相关原理。 我把一门功能比较全的脚本语言的示例放在了 playscript-java 项目下，以后几讲的内容都会参考这里面的示例代码。playscript-java（项目目录）： 码云 GitHub PlayScript.java（入口程序）： 码云 GitHub PlayScript.g4（语法规则）： 码云 GitHub ASTEvaluator.java（解释器）： 码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:9:7","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"08 | 作用域和生存期：实现块作用域和函数 目前，我们已经用 Antlr 重构了脚本解释器，有了工具的帮助，我们可以实现更高级的功能，比如函数功能、面向对象功能。当然了，在这个过程中，我们还要克服一些挑战，比如：如果要实现函数功能，要升级变量管理机制；引入作用域机制，来保证变量的引用指向正确的变量定义；提升变量存储机制，不能只把变量和它的值简单地扔到一个 HashMap 里，要管理它的生存期，减少对内存的占用。 本节课，我将借实现块作用域和函数功能，带你探讨作用域和生存期及其实现机制，并升级变量管理机制。那么什么是作用域和生存期，它们的重要性又体现在哪儿呢？“作用域”和“生存期”是计算机语言中更加基础的概念，它们可以帮你深入地理解函数、块、闭包、面向对象、静态成员、本地变量和全局变量等概念。而且一旦你深入理解，了解作用域与生存期在编译期和运行期的机制之后，就能解决在学习过程中可能遇到的一些问题，比如： 闭包的机理到底是什么？为什么需要栈和堆两种机制来管理内存？它们的区别又是什么？一个静态的内部类和普通的内部类有什么区别？ 了解上面这些内容之后，接下来，我们来具体看看什么是作用域。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:10:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"作用域（Scope） 作用域是指计算机语言中变量、函数、类等起作用的范围，我们来看一个具体的例子。下面这段代码是用 C 语言写的，我们在全局以及函数 fun 中分别声明了 a 和 b 两个变量，然后在代码里对这些变量做了赋值操作： /* scope.c 测试作用域。 */ #include \u003cstdio.h\u003e int a = 1; void fun() { a = 2; //b = 3; //出错，不知道b是谁 int a = 3; //允许声明一个同名的变量吗？ int b = a; //这里的a是哪个？ printf(\"in fun: a=%d b=%d \\n\", a, b); } int b = 4; //b的作用域从这里开始 int main(int argc, char **argv){ printf(\"main--1: a=%d b=%d \\n\", a, b); fun(); printf(\"main--2: a=%d b=%d \\n\", a, b); //用本地变量覆盖全局变量 int a = 5; int b = 5; printf(\"main--3: a=%d b=%d \\n\", a, b); //测试块作用域 if (a \u003e 0){ int b = 3; //允许在块里覆盖外面的变量 printf(\"main--4: a=%d b=%d \\n\", a, b); } else{ int b = 4; //跟if块里的b是两个不同的变量 printf(\"main--5: a=%d b=%d \\n\", a, b); } printf(\"main--6: a=%d b=%d \\n\", a, b); } 这段代码编译后运行，结果是： main--1: a=1 b=4 in fun: a=3 b=3 main--2: a=2 b=4 main--3: a=5 b=5 main--4: a=5 b=3 main--6: a=5 b=5 我们可以得出这样的规律：变量的作用域有大有小，外部变量在函数内可以访问，而函数中的本地变量，只有本地才可以访问。变量的作用域，从声明以后开始。在函数里，我们可以声明跟外部变量相同名称的变量，这个时候就覆盖了外部变量。 下面这张图直观地显示了示例代码中各个变量的作用域： 另外，C 语言里还有块作用域的概念，就是用花括号包围的语句，if 和 else 后面就跟着这样的语句块。块作用域的特征跟函数作用域的特征相似，都可以访问外部变量，也可以用本地变量覆盖掉外部变量。你可能会问：“其他语言也有块作用域吗？特征是一样的吗？”其实，各个语言在这方面的设计机制是不同的。比如，下面这段用 Java 写的代码里，我们用了一个 if 语句块，并且在 if 部分、else 部分和外部分别声明了一个变量 c： /** * Scope.java * 测试Java的作用域 */ public class ScopeTest{ public static void main(String args[]){ int a = 1; int b = 2; if (a \u003e 0){ //int b = 3; //不允许声明与外部变量同名的变量 int c = 3; } else{ int c = 4; //允许声明另一个c，各有各的作用域 } int c = 5; //这里也可以声明一个新的c } } 你能看到，Java 的块作用域跟 C 语言的块作用域是不同的，它不允许块作用域里的变量覆盖外部变量。那么和 C、Java 写起来很像的 JavaScript 呢？来看一看下面这段测试 JavaScript 作用域的代码： /** * Scope.js * 测试JavaScript的作用域 */ var a = 5; var b = 5; console.log(\"1: a=%d b=%d\", a, b); if (a \u003e 0) { a = 4; console.log(\"2: a=%d b=%d\", a, b); var b = 3; //看似声明了一个新变量，其实还是引用的外部变量 console.log(\"3: a=%d b=%d\", a, b); } else { var b = 4; console.log(\"4: a=%d b=%d\", a, b); } console.log(\"5: a=%d b=%d\", a, b); for (var b = 0; b\u003c 2; b++){ //这里是否能声明一个新变量，用于for循环？ console.log(\"6-%d: a=%d b=%d\",b, a, b); } console.log(\"7: a=%d b=%d\", a, b); 这段代码编译后运行，结果是： 1: a=5 b=5 2: a=4 b=5 3: a=4 b=3 5: a=4 b=3 6-0: a=4 b=0 6-1: a=4 b=1 7: a=4 b=2 你可以看到，JavaScript 是没有块作用域的。我们在块里和 for 语句试图重新定义变量 b，语法上是允许的，但我们每次用到的其实是同一个变量。对比了三种语言的作用域特征之后，你是否发现原来看上去差不多的语法，内部机理却不同？这种不同其实是语义差别的一个例子。你要注意的是，现在我们讲的很多内容都已经属于语义的范畴了，对作用域的分析就是语义分析的任务之一。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:10:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"生存期（Extent） 了解了什么是作用域之后，我们再理解一下跟它紧密相关的生存期。它是变量可以访问的时间段，也就是从分配内存给它，到收回它的内存之间的时间。在前面几个示例程序中，变量的生存期跟作用域是一致的。出了作用域，生存期也就结束了，变量所占用的内存也就被释放了。这是本地变量的标准特征，这些本地变量是用栈来管理的。但也有一些情况，变量的生存期跟语法上的作用域不一致，比如在堆中申请的内存，退出作用域以后仍然会存在。下面这段 C 语言的示例代码中，fun 函数返回了一个整数的指针。出了函数以后，本地变量 b 就消失了，这个指针所占用的内存（\u0026b）就收回了，其中 \u0026b 是取 b 的地址，这个地址是指向栈里的一小块空间，因为 b 是栈里申请的。在这个栈里的小空间里保存了一个地址，指向在堆里申请的内存。这块内存，也就是用来实际保存数值 2 的空间，并没有被收回，我们必须手动使用 free() 函数来收回。 /* extent.c 测试生存期。 */ #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e int * fun(){ int * b = (int*)malloc(1*sizeof(int)); //在堆中申请内存 *b = 2; //给该地址赋值2 return b; } int main(int argc, char **argv){ int * p = fun(); *p = 3; printf(\"after called fun: b=%lu *b=%d \\n\", (unsigned long)p, *p); free(p); } 类似的情况在 Java 里也有。Java 的对象实例缺省情况下是在堆中生成的。下面的示例代码中，从一个方法中返回了对象的引用，我们可以基于这个引用继续修改对象的内容，这证明这个对象的内存并没有被释放： /** * Extent2.java * 测试Java的生存期特性 */ public class Extent2{ StringBuffer myMethod(){ StringBuffer b = new StringBuffer(); //在堆中生成对象实例 b.append(\"Hello \"); System.out.println(System.identityHashCode(b)); //打印内存地址 return b; //返回对象引用，本质是一个内存地址 } public static void main(String args[]){ Extent2 extent2 = new Extent2(); StringBuffer c = extent2.myMethod(); //获得对象引用 System.out.println(c); c.append(\"World!\"); //修改内存中的内容 System.out.println(c); //跟在myMethod()中打印的值相同 System.out.println(System.identityHashCode(c)); } } 因为 Java 对象所采用的内存超出了申请内存时所在的作用域，所以也就没有办法自动收回。所以 Java 采用的是自动内存管理机制，也就是垃圾回收技术。那么为什么说作用域和生存期是计算机语言更加基础的概念呢？其实是因为它们对应到了运行时的内存管理的基本机制。虽然各门语言设计上的特性是不同的，但在运行期的机制都很相似，比如都会用到栈和堆来做内存管理。好了，理解了作用域和生存期的原理之后，我们就来实现一下，先来设计一下作用域机制，然后再模拟实现一个栈。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:10:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现作用域和栈 在之前的 PlayScript 脚本的实现中，处理变量赋值的时候，我们简单地把变量存在一个哈希表里，用变量名去引用，就像下面这样： public class SimpleScript { private HashMap\u003cString, Integer\u003e variables = new HashMap\u003cString, Integer\u003e(); ... } 但如果变量存在多个作用域，这样做就不行了。这时，我们就要设计一个数据结构，区分不同变量的作用域。分析前面的代码，你可以看到作用域是一个树状的结构，比如 Scope.c 的作用域： 面向对象的语言不太相同，它不是一棵树，是一片树林，每个类对应一棵树，所以它也没有全局变量。在我们的 playscript 语言中，我们设计了下面的对象结构来表示 Scope： //编译过程中产生的变量、函数、类、块，都被称作符号 public abstract class Symbol { //符号的名称 protected String name = null; //所属作用域 protected Scope enclosingScope = null; //可见性，比如public还是private protected int visibility = 0; //Symbol关联的AST节点 protected ParserRuleContext ctx = null; } //作用域 public abstract class Scope extends Symbol{ // 该Scope中的成员，包括变量、方法、类等。 protected List\u003cSymbol\u003e symbols = new LinkedList\u003cSymbol\u003e(); } //块作用域 public class BlockScope extends Scope{ ... } //函数作用域 public class Function extends Scope implements FunctionType{ ... } //类作用域 public class Class extends Scope implements Type{ ... } 目前我们划分了三种作用域，分别是块作用域（Block）、函数作用域（Function）和类作用域（Class）。我们在解释执行 playscript 的 AST 的时候，需要建立起作用域的树结构，对作用域的分析过程是语义分析的一部分。也就是说，并不是有了 AST，我们马上就可以运行它，在运行之前，我们还要做语义分析，比如对作用域做分析，让每个变量都能做正确的引用，这样才能正确地执行这个程序。解决了作用域的问题以后，再来看看如何解决生存期的问题。还是看 Scope.c 的代码，随着代码的执行，各个变量的生存期表现如下： 进入程序，全局变量逐一生效；进入 main 函数，main 函数里的变量顺序生效；进入 fun 函数，fun 函数里的变量顺序生效；退出 fun 函数，fun 函数里的变量失效；进入 if 语句块，if 语句块里的变量顺序生效；退出 if 语句块，if 语句块里的变量失效；退出 main 函数，main 函数里的变量失效；退出程序，全局变量失效。 通过下面这张图，你能直观地看到运行过程中栈的变化： 代码执行时进入和退出一个个作用域的过程，可以用栈来实现。每进入一个作用域，就往栈里压入一个数据结构，这个数据结构叫做栈桢（Stack Frame）。栈桢能够保存当前作用域的所有本地变量的值，当退出这个作用域的时候，这个栈桢就被弹出，里面的变量也就失效了。你可以看到，栈的机制能够有效地使用内存，变量超出作用域的时候，就没有用了，就可以从内存中丢弃。我在 ASTEvaluator.java 中，用下面的数据结构来表示栈和栈桢，其中的 PlayObject 通过一个 HashMap 来保存各个变量的值： private Stack\u003cStackFrame\u003e stack = new Stack\u003cStackFrame\u003e(); public class StackFrame { //该frame所对应的scope Scope scope = null; //enclosingScope所对应的frame StackFrame parentFrame = null; //实际存放变量的地方 PlayObject object = null; } public class PlayObject { //成员变量 protected Map\u003cVariable, Object\u003e fields = new HashMap\u003cVariable, Object\u003e(); } 目前，我们只是在概念上模仿栈桢，当我们用 Java 语言实现的时候，PlayObject 对象是存放在堆里的，Java 的所有对象都是存放在堆里的，只有基础数据类型，比如 int 和对象引用是放在栈里的。虽然只是模仿，这不妨碍我们建立栈桢的概念，在后端技术部分，我们会实现真正意义上的栈桢。要注意的是，栈的结构和 Scope 的树状结构是不一致的。也就是说，栈里的上一级栈桢，不一定是 Scope 的父节点。要访问上一级 Scope 中的变量数据，要顺着栈桢的 parentFrame 去找。我在上图中展现了这种情况，在调用 fun 函数的时候，栈里一共有三个栈桢：全局栈桢、main() 函数栈桢和 fun() 函数栈桢，其中 main() 函数栈桢的 parentFrame 和 fun() 函数栈桢的 parentFrame 都是全局栈桢。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:10:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现块作用域 目前，我们已经做好了作用域和栈，在这之后，就能实现很多功能了，比如让 if 语句和 for 循环语句使用块作用域和本地变量。以 for 语句为例，visit 方法里首先为它生成一个栈桢，并加入到栈中，运行完毕之后，再从栈里弹出： BlockScope scope = (BlockScope) cr.node2Scope.get(ctx); //获得Scope StackFrame frame = new StackFrame(scope); //创建一个栈桢 pushStack(frame); //加入栈中 ... //运行完毕，弹出栈 stack.pop(); 当我们在代码中需要获取某个变量的值的时候，首先在当前桢中寻找。找不到的话，就到上一级作用域对应的桢中去找： StackFrame f = stack.peek(); //获取栈顶的桢 PlayObject valueContainer = null; while (f != null) { //看变量是否属于当前栈桢里 if (f.scope.containsSymbol(variable)){ valueContainer = f.object; break; } //从上一级scope对应的栈桢里去找 f = f.parentFrame; } 运行下面的测试代码，你会看到在执行完 for 循环以后，我们仍然可以声明另一个变量 i，跟 for 循环中的 i 互不影响，这证明它们确实属于不同的作用域： String script = \"int age = 44; for(int i = 0;i\u003c10;i++) { age = age + 2;} int i = 8;\"; 进一步的，我们可以实现对函数的支持。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:10:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现函数功能 先来看一下与函数有关的语法： //函数声明 functionDeclaration : typeTypeOrVoid? IDENTIFIER formalParameters ('[' ']')* functionBody ; //函数体 functionBody : block | ';' ; //类型或void typeTypeOrVoid : typeType | VOID ; //函数所有参数 formalParameters : '(' formalParameterList? ')' ; //参数列表 formalParameterList : formalParameter (',' formalParameter)* (',' lastFormalParameter)? | lastFormalParameter ; //单个参数 formalParameter : variableModifier* typeType variableDeclaratorId ; //可变参数数量情况下，最后一个参数 lastFormalParameter : variableModifier* typeType '...' variableDeclaratorId ; //函数调用 functionCall : IDENTIFIER '(' expressionList? ')' | THIS '(' expressionList? ')' | SUPER '(' expressionList? ')' ; 在函数里，我们还要考虑一个额外的因素：参数。在函数内部，参数变量跟普通的本地变量在使用时没什么不同，在运行期，它们也像本地变量一样，保存在栈桢里。我们设计一个对象来代表函数的定义，它包括参数列表和返回值的类型： public class Function extends Scope implements FunctionType{ // 参数 protected List\u003cVariable\u003e parameters = new LinkedList\u003cVariable\u003e(); //返回值 protected Type returnType = null; ... } 在调用函数时，我们实际上做了三步工作：建立一个栈桢；计算所有参数的值，并放入栈桢；执行函数声明中的函数体。 我把相关代码放在了下面，你可以看一下： //函数声明的AST节点 FunctionDeclarationContext functionCode = (FunctionDeclarationContext) function.ctx; //创建栈桢 functionObject = new FunctionObject(function); StackFrame functionFrame = new StackFrame(functionObject); // 计算实参的值 List\u003cObject\u003e paramValues = new LinkedList\u003cObject\u003e(); if (ctx.expressionList() != null) { for (ExpressionContext exp : ctx.expressionList().expression()) { Object value = visitExpression(exp); if (value instanceof LValue) { value = ((LValue) value).getValue(); } paramValues.add(value); } } //根据形参的名称，在栈桢中添加变量 if (functionCode.formalParameters().formalParameterList() != null) { for (int i = 0; i \u003c functionCode.formalParameters().formalParameterList().formalParameter().size(); i++) { FormalParameterContext param = functionCode.formalParameters().formalParameterList().formalParameter(i); LValue lValue = (LValue) visitVariableDeclaratorId(param.variableDeclaratorId()); lValue.setValue(paramValues.get(i)); } } // 调用方法体 rtn = visitFunctionDeclaration(functionCode); // 运行完毕，弹出栈 stack.pop(); 你可以用 playscript 测试一下函数执行的效果，看看参数传递和作用域的效果： String script = \"int b= 10; int myfunc(int a) {return a+b+3;} myfunc(2);\"; ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:10:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我带你实现了块作用域和函数，还跟你一起探究了计算机语言的两个底层概念：作用域和生存期。你要知道：对作用域的分析是语义分析的一项工作。Antlr 能够完成很多词法分析和语法分析的工作，但语义分析工作需要我们自己做。变量的生存期涉及运行期的内存管理，也引出了栈桢和堆的概念，我会在编译器后端技术时进一步阐述。 我建议你在学习新语言的时候，先了解它在作用域和生存期上的特点，然后像示例程序那样做几个例子，借此你会更快理解语言的设计思想。比如，为什么需要命名空间这个特性？全局变量可能带来什么问题？类的静态成员与普通成员有什么区别？等等。下一讲，我们会尝试实现面向对象特性，看看面向对象语言在语义上是怎么设计的，以及在运行期有什么特点。 今天讲的功能照样能在 playscript-java 项目中找到示例代码，其中还有用 playscript 写的脚本，你可以多玩一玩。playscript-java（项目目录）： 码云 GitHub PlayScript.java（入口程序）：码云 GitHub PlayScript.g4（语法规则）：码云 GitHub ASTEvaluator.java（解释器）：码云 GitHub BlockScope.play（演示块作用域）：码云 GitHub function.play（演示基础函数功能）：码云 GitHub lab/scope 目录（各种语言的作用域测试）：码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:10:6","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"09 | 面向对象：实现数据和方法的封装 在现代计算机语言中，面向对象是非常重要的特性，似乎常用的语言都支持面向对象特性，比如 Swift、C++、Java……不支持的反倒是异类了。而它重要的特点就是封装。也就是说，对象可以把数据和对数据的操作封装在一起，构成一个不可分割的整体，尽可能地隐藏内部的细节，只保留一些接口与外部发生联系。 在对象的外部只能通过这些接口与对象进行交互，无需知道对象内部的细节。这样能降低系统的耦合，实现内部机制的隐藏，不用担心对外界的影响。那么它们是怎样实现的呢？本节课，我将从语义设计和运行时机制的角度剖析面向对象的特性，带你深入理解面向对象的实现机制，让你能在日常编程工作中更好地运用面向对象的特性。比如，在学完这讲之后，你会对对象的作用域和生存期、对象初始化过程等有更清晰的了解。而且你不会因为学习了 Java 或 C++ 的面向对象机制，在学习 JavaScript 和 Ruby 的面向对象机制时觉得别扭，因为它们的本质是一样的。接下来，我们先简单地聊一下什么是面向对象。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:11:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"面向对象的语义特征 我的一个朋友，在 10 多年前做过培训师，为了吸引学员的注意力，他在讲“什么是面向对象”时说：“面向对象是世界观，是方法论。”虽然有点儿语不惊人死不休的意思，但我必须承认，所有的计算机语言都是对世界进行建模的方式，只不过建模的视角不同罢了。面向对象的设计思想，在上世纪 90 年代被推崇，几乎被视为最好的编程模式。实际上，各种不同的编程思想，都会表现为这门语言的语义特征，所以，我就从语义角度，利用类型、作用域、生存期这样的概念带你深入剖析一下面向对象的封装特性，其他特性在后面的课程中再去讨论。 从类型角度 类型处理是语义分析时的重要工作。现代计算机语言可以用自定义的类来声明变量，这是一个巨大的进步。因为早期的计算机语言只支持一些基础的数据类型，比如各种长短不一的整型和浮点型，像字符串这种我们编程时离不开的类型，往往是在基础数据类型上封装和抽象出来的。所以，我们要扩展语言的类型机制，让程序员可以创建自己的类型。 从作用域角度 首先是类的可见性。作为一种类型，它通常在整个程序的范围内都是可见的，可以用它声明变量。当然，一些像 Java 的语言，也能限制某些类型的使用范围，比如只能在某个命名空间内，或者在某个类内部。对象的成员的作用域是怎样的呢？我们知道，对象的属性（“属性”这里指的是类的成员变量）可以在整个对象内部访问，无论在哪个位置声明。也就是说，对象属性的作用域是整个对象的内部，方法也是一样。这跟函数和块中的本地变量不一样，它们对声明顺序有要求，像 C 和 Java 这样的语言，在使用变量之前必须声明它。 从生存期的角度 对象的成员变量的生存期，一般跟对象的生存期是一样的。在创建对象的时候，就对所有成员变量做初始化，在销毁对象的时候，所有成员变量也随着一起销毁。当然，如果某个成员引用了从堆中申请的内存，这些内存需要手动释放，或者由垃圾收集机制释放。但还有一些成员，不是与对象绑定的，而是与类型绑定的，比如 Java 中的静态成员。静态成员跟普通成员的区别，就是作用域和生存期不同，它的作用域是类型的所有对象实例，被所有实例共享。生存期是在任何一个对象实例创建之前就存在，在最后一个对象销毁之前不会消失。 你看，我们用这三个语义概念，就把面向对象的封装特性解释清楚了，无论语言在顶层怎么设计，在底层都是这么实现的。了解了面向对象在语义上的原理之后，我们来实际动手解析一下代码中的类，这样能更深刻地体会这些原理。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:11:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"设计类的语法，并解析它 我们要在语言中支持类的定义，在 PlayScript.g4 中，可以这样定义类的语法规则： classDeclaration : CLASS IDENTIFIER (EXTENDS typeType)? (IMPLEMENTS typeList)? classBody ; classBody : '{' classBodyDeclaration* '}' ; classBodyDeclaration : ';' | memberDeclaration ; memberDeclaration : functionDeclaration | fieldDeclaration ; functionDeclaration : typeTypeOrVoid IDENTIFIER formalParameters ('[' ']')* (THROWS qualifiedNameList)? functionBody ; 我来简单地讲一下这个语法规则：类声明以 class 关键字开头，有一个标识符是类型名称，后面跟着类的主体。类的主体里要声明类的成员。在简化的情况下，可以只关注类的属性和方法两种成员。我们故意把类的方法也叫做 function，而不是 method，是想把对象方法和函数做一些统一的设计。函数声明现在的角色是类的方法。类的成员变量的声明和普通变量声明在语法上没什么区别。 你能看到，我们构造像 class 这样高级别的结构时，越来越得心应手了，之前形成的一些基础的语法模块都可以复用，比如变量声明、代码块（block）等。用上面的语法写出来的 playscript 脚本的效果如下，在示例代码里也有，你可以运行它： /* ClassTest.play 简单的面向对象特性。 */ class Mammal{ //类属性 string name = \"\"; //构造方法 Mammal(string str){ name = str; } //方法 void speak(){ println(\"mammal \" + name +\" speaking...\"); } } Mammal mammal = Mammal(\"dog\"); //playscript特别的构造方法，不需要new关键字 mammal.speak(); //访问对象方法 println(\"mammal.name = \" + mammal.name); //访问对象的属性 //没有构造方法，创建的时候用缺省构造方法 class Bird{ int speed = 50; //在缺省构造方法里初始化 void fly(){ println(\"bird flying...\"); } } Bird bird = Bird(); //采用缺省构造方法 println(\"bird.speed : \" + bird.speed + \"km/h\"); bird.fly(); 接下来，我们让 playscript 解释器处理这些看上去非常现代化的代码，怎么处理呢？做完词法分析和语法分析之后，playscript 会在语义分析阶段扫描 AST，识别出所有自定义的类型，以便在其他地方引用这些类型来声明变量。因为类型的声明可以在代码中的任何位置，所以最好用单独的一次遍历来识别和记录类型（类型扫描的代码在 TypeAndScopeScanner.java 里）。接着，我们在声明变量时，就可以引用这个类型了。语义分析的另一个工作，就是做变量类型的消解。当我们声明“Bird bird = Bird(); ”时，需要知道 Bird 对象的定义在哪里，以便正确地访问它的成员（变量类型的消解在 TypeResolver.java 里）。在做语义分析时，要把类型的定义保存在一个数据结构中，我们来实现一下： public class Class extends Scope implements Type{ ... } public abstract class Scope extends Symbol{ // 该Scope中的成员，包括变量、方法、类等。 protected List\u003cSymbol\u003e symbols = new LinkedList\u003cSymbol\u003e( } public interface Type { public String getName(); //类型名称 public Scope getEnclosingScope(); } 在这个设计中，我们看到 Class 就是一个 Scope，Scope 里面原来就能保存各种成员，现在可以直接复用，用来保存类的属性和方法，画成类图如下： 图里有几个类，比如 Symbol、Variable、Scope、Function 和 BlockScope，它们是我们的符号体系的主要成员。在做词法分析时，我们会解析出很多标识符，这些标识符出现在不同的语法规则里，包括变量声明、表达式，以及作为类名、方法名等出现。在语义分析阶段，我们要把这些标识符一一识别出来，这个是一个变量，指的是一个本地变量；那个是一个方法名等。变量、类和函数的名称，我们都叫做符号，比如示例程序中的 Mammal、Bird、mammal、bird、name、speed 等。编译过程中的一项重要工作就是建立符号表，它帮助我们进一步地编译或执行程序，而符号表就用上面几个类来保存信息。 在符号表里，我们保存它的名称、类型、作用域等信息。对于类和函数，我们也有相应的地方来保存类变量、方法、参数、返回值等信息。你可以看一看示例代码里面是如何解析和记录这些符号的。解析完这些语义信息以后，我们来看运行期如何执行具有面向对象特征的程序，比如如何实例化一个对象？如何在内存里管理对象的数据？以及如何访问对象的属性和方法？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:11:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"对象是怎么实例化的 首先通过构造方法来创建对象。在语法中，我们没有用 new 这个关键字来表示对象的创建，而是省略掉了 new，直接调用一个跟类名称相同的函数，这是我们独特的设计，示例代码如下： Mammal mammal = Mammal(\"dog\"); //playscript特别的构造方法，不需要new关键字 Bird bird = Bird(); //采用缺省构造方法 但在语义检查的时候，在当前作用域中是肯定找不到这样一个函数的，因为类的初始化方法是在类的内部定义的，我们只要检查一下，Mammal 和 Bird 是不是一个类名就可以了。再进一步，Mammal 类中确实有个构造方法 Mammal()，而 Bird 类中其实没有一个显式定义的构造方法，但这并不意味着变量成员不会被初始化。我们借鉴了 Java 的初始化机制，就是提供缺省初始化方法，在缺省初始化方法里，会执行对象成员声明时所做的初始化工作。所以，上面的代码里，我们调用 Bird()，实际上就是调用了这个缺省的初始化方法。无论有没有显式声明的构造方法，声明对象的成员变量时的初始化部分，一定会执行。对于 Bird 类，实际上就会执行“int speed = 50;”这个语句。 在 RefResolver.java 中做语义分析的时候，下面的代码能够检测出某个函数调用其实是类的构造方法，或者是缺省构造方法： // 看看是不是类的构建函数，用相同的名称查找一个class Class theClass = at.lookupClass(scope, idName); if (theClass != null) { function = theClass.findConstructor(paramTypes); if (function != null) { at.symbolOfNode.put(ctx, function); } //如果是与类名相同的方法，并且没有参数，那么就是缺省构造方法 else if (ctx.expressionList() == null){ at.symbolOfNode.put(ctx, theClass); // TODO 直接赋予class } else{ at.log(\"unknown class constructor: \" + ctx.getText(), ctx); } at.typeOfNode.put(ctx, theClass); // 这次函数调用是返回一个对象 } 当然，类的构造方法跟普通函数还是有所不同的，例如我们不允许构造方法定义返回值，因为它的返回值一定是这个类的一个实例对象。对象做了缺省初始化以后，再去调用显式定义的构造方法，这样才能完善整个对象实例化的过程。不过问题来了，我们可以把普通的本地变量的数据保存在栈里，那么如何保存对象的数据呢？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:11:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"如何在内存里管理对象的数据 其实，我们也可以把对象的数据像其他数据一样，保存在栈里。 C 语言的结构体 struct 和 C++ 语言的对象，都可以保存在栈里。保存在栈里的对象是直接声明并实例化的，而不是用 new 关键字来创建的。如果用 new 关键字来创建，实际上是在堆里申请了一块内存，并赋值给一个指针变量，如下图所示： 当对象保存在堆里的时候，可以有多个变量都引用同一个对象，比如图中的变量 a 和变量 b 就可以引用同一个对象 object1。类的成员变量也可以引用别的对象，比如 object1 中的类成员引用了 object2 对象。对象的生存期可以超越创建它的栈桢的生存期。我们可以对比一下这两种方式的优缺点。如果对象保存在栈里，那么它的生存期与作用域是一样的，可以自动的创建和销毁，因此不需要额外的内存管理。缺点是对象没办法长期存在并共享。而在堆里创建的对象虽然可以被共享使用，却增加了内存管理的负担。所以在 C 语言和 C++ 语言中，要小心管理从堆中申请的内存，在合适的时候释放掉这些内存。在 Java 语言和其他一些语言中，采用的是垃圾收集机制，也就是说当一个对象不再被引用时，就把内存收集回来。分析到这儿的时候，我们其实可以帮 Java 语言优化一下内存管理。比如我们在分析代码时，如果发现某个对象的创建和使用都局限在某个块作用域中，并没有跟其他作用域共享，那么这个对象的生存期与当前栈桢是一致的，可以在栈里申请内存，而不是在堆里。这样可以免除后期的垃圾收集工作。 分析完对象的内存管理方式之后，回到 playscript 的实现。在 playscript 的 Java 版本里，我们用一个 ClassObject 对象来保存对象数据，而 ClassObject 是 PlayObject 的子类。上一讲，我们已经讲过 PlayObject，它被栈桢用来保存本地变量，可以通过传入 Variable 来访问对象的属性值： //类的实例 public class ClassObject extends PlayObject{ //类型 protected Class type = null; ... } //保存对象数据 public class PlayObject { //成员变量 protected Map\u003cVariable, Object\u003e fields = new HashMap\u003cVariable, Object\u003e(); public Object getValue(Variable variable){ Object rtn = fields.get(variable); return rtn; } public void setValue(Variable variable, Object value){ fields.put(variable, value); } } 在运行期，当需要访问一个对象时，我们也会用 ClassObject 来做一个栈桢，这样就可以像访问本地变量一样访问对象的属性了。而不需要访问这个对象的时候，就把它从栈中移除，如果没有其他对象引用这个对象，那么它会被 Java 的垃圾收集机制回收。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:11:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"访问对象的属性和方法 在示例代码中，我们用点操作符来访问对象的属性和方法，比如： mammal.speak(); //访问对象方法 println(\"mammal.name = \" + mammal.name); //访问对象的属性 属性和方法的引用也是一种表达式，语法定义如下： expression : ... | expression bop='.' ( IDENTIFIER //对象属性 | functionCall //对象方法 ) ... ; 注意，点符号的操作可以是级联的，比如： obj1.obj2.field1; obj1.getObject2().field1; 所以，对表达式的求值，要能够获得正确的对象引用，你可以运行一下 ClassTest.play 脚本，或者去看看我的参考实现。另外，对象成员还可以设置可见性。也就是说，有些成员只有对象内部才能用，有些可以由外部访问。这个怎么实现呢？这只是个语义问题，是在编译阶段做语义检查的时候，不允许私有的成员被外部访问，报编译错误就可以了，在其他方面，并没有什么不同。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:11:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 我们针对面向对象的封装特性，从类型、作用域和生存期的角度进行了重新解读，这样能够更好地把握面向对象的本质特征。我们还设计了与面向对象的相关的语法并做了解析，然后讨论了面向对象程序的运行期机制，例如如何实例化一个对象，如何在内存里管理对象的数据，以及如何访问对象的属性和方法。通过对类的语法和语义的剖析和运行机制的落地，我相信你会对面向对象的机制有更加本质的认识，也能更好地使用语言的面向对象特性了。 我将本节课相关代码的链接放在了文末，供你参考。playscript-java（项目目录）： 码云 GitHub PlayScript.java（入口程序）： 码云 GitHub PlayScript.g4（语法规则）： 码云 GitHub ASTEvaluator.java（解释器）： 码云 GitHub TypeAndScopeScanner.java（识别对象声明）： 码云 GitHub TypeResolver.java（消解变量声明中引用的类型）： 码云 GitHub RefResolver.java（消解变量引用和函数调用）： 码云 GitHub ClassTest.play（演示面向对象的基本特征）： 码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:11:6","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"10 | 闭包： 理解了原理，它就不反直觉了 在讲作用域和生存期时，我提到函数里的本地变量只能在函数内部访问，函数退出之后，作用域就没用了，它对应的栈桢被弹出，作用域中的所有变量所占用的内存也会被收回。但偏偏跑出来闭包（Closure）这个怪物。 在 JavaScript 中，用外层函数返回一个内层函数之后，这个内层函数能一直访问外层函数中的本地变量。按理说，这个时候外层函数已经退出了，它里面的变量也该作废了。可闭包却非常执着，即使外层函数已经退出，但内层函数仿佛不知道这个事实一样，还继续访问外层函数中声明的变量，并且还真的能够正常访问。不过，闭包是很有用的，对库的编写者来讲，它能隐藏内部实现细节；对面试者来讲，它几乎是前端面试必问的一个问题，比如如何用闭包特性实现面向对象编程？等等。本节课，我会带你研究闭包的实现机制，让你深入理解作用域和生存期，更好地使用闭包特性。为此，要解决两个问题： 函数要变成 playscript 的一等公民。也就是要能把函数像普通数值一样赋值给变量，可以作为参数传递给其他函数，可以作为函数的返回值。要让内层函数一直访问它环境中的变量，不管外层函数退出与否。 我们先通过一个例子，研究一下闭包的特性，看看它另类在哪里。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:12:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"闭包的内在矛盾 来测试一下 JavaScript 的闭包特性： js /** * clojure.js * 测试闭包特性 * 作者：宫文学 */ var a = 0; var fun1 = function(){ var b = 0; // 函数内的局部变量 var inner = function(){ // 内部的一个函数 a = a+1; b = b+1; return b; // 返回内部的成员 } return inner; // 返回一个函数 } console.log(\"outside: a=%d\", a); var fun2 = fun1(); // 生成闭包 for (var i = 0; i\u003c 2; i++){ console.log(\"fun2: b=%d a=%d\",fun2(), a); //通过fun2()来访问b } var fun3 = fun1(); // 生成第二个闭包 for (var i = 0; i\u003c 2; i++){ console.log(\"fun3: b=%d a=%d\",fun3(), a); // b等于1，重新开始 } 在 Node.js 环境下运行上面这段代码的结果如下： outside: a=0 fun2: b=1 a=1 fun2: b=2 a=2 fun3: b=1 a=3 fun3: b=2 a=4 观察这个结果，可以得出两点：内层的函数能访问它“看得见”的变量，包括自己的本地变量、外层函数的变量 b 和全局变量 a。内层函数作为返回值赋值给其他变量以后，外层函数就结束了，但内层函数仍能访问原来外层函数的变量 b，也能访问全局变量 a。 这样似乎让人感到困惑：站在外层函数的角度看，明明这个函数已经退出了，变量 b 应该失效了，为什么还可以继续访问？但是如果换个立场，站在 inner 这个函数的角度来看，声明 inner 函数的时候，告诉它可以访问 b，不能因为把 inner 函数赋值给了其他变量，inner 函数里原本正确的语句就不能用了啊。 其实，只要函数能作为值传来传去，就一定会产生作用域不匹配的情况，这样的内在矛盾是语言设计时就决定了的。我认为，闭包是为了让函数能够在这种情况下继续运行所提供的一个方案。这个方案有一些不错的特点，比如隐藏函数所使用的数据，歪打正着反倒成了一个优点了！在这里，我想补充一下静态作用域（Static Scope）这个知识点，如果一门语言的作用域是静态作用域，那么符号之间的引用关系能够根据程序代码在编译时就确定清楚，在运行时不会变。某个函数是在哪声明的，就具有它所在位置的作用域。它能够访问哪些变量，那么就跟这些变量绑定了，在运行时就一直能访问这些变量。看一看下面的代码，对于静态作用域而言，无论在哪里调用 foo() 函数，访问的变量 i 都是全局变量： int i = 1; void foo(){ println(i); // 访问全局变量 } foo(); // 访问全局变量 void bar(){ int i = 2; foo(); // 在这里调用foo()，访问的仍然是全局变量 } 我们目前使用的大多数语言都是采用静态作用域的。playscript 语言也是在编译时就形成一个 Scope 的树，变量的引用也是在编译时就做了消解，不再改变，所以也是采用了静态作用域。反过来讲，如果在 bar() 里调用 foo() 时，foo() 访问的是 bar() 函数中的本地变量 i，那就说明这门语言使用的是动态作用域（Dynamic Scope）。也就是说，变量引用跟变量声明不是在编译时就绑定死了的。在运行时，它是在运行环境中动态地找一个相同名称的变量。在 macOS 或 Linux 中用的 bash 脚本语言，就是动态作用域的。 静态作用域可以由程序代码决定，在编译时就能完全确定，所以又叫做词法作用域（Lexcical Scope）。不过这个词法跟我们做词法分析时说的词法不大一样。这里，跟 Lexical 相对应的词汇可以认为是 Runtime，一个是编写时，一个是运行时。用静态作用域的概念描述一下闭包，我们可以这样说：因为我们的语言是静态作用域的，它能够访问的变量，需要一直都能访问，为此，需要把某些变量的生存期延长。当然了，闭包的产生还有另一个条件，就是让函数成为一等公民。这是什么意思？我们又怎样实现呢？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:12:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"函数作为一等公民 在 JavaScript 和 Python 等语言里，函数可以像数值一样使用，比如给变量赋值、作为参数传递给其他函数，作为函数返回值等等。这时，我们就说函数是一等公民。作为一等公民的函数很有用，比如它能处理数组等集合。我们给数组的 map 方法传入一个回调函数，结果会生成一个新的数组。整个过程很简洁，没有出现啰嗦的循环语句，这也是很多人提倡函数式编程的原因之一： var newArray = [\"1\",\"2\",\"3\"].map( fucntion(value,index,array){ return parseInt(value,10) }) 那么在 playscript 中，怎么把函数作为一等公民呢？我们需要支持函数作为基础类型，这样就可以用这种类型声明变量。但问题来了，如何声明一个函数类型的变量呢？在 JavaScript 这种动态类型的语言里，我们可以把函数赋值给任何一个变量，就像前面示例代码里的那样：inner 函数作为返回值，被赋给了 fun2 和 fun3 两个变量。然而在 Go 语言这样要求严格类型匹配的语言里，就比较复杂了： type funcType func(int) int // Go语言，声明了一个函数类型funcType var myFun funType // 用这个函数类型声明了一个变量 它对函数的原型有比较严格的要求：函数必须有一个 int 型的参数，返回值也必须是 int 型的。而 C 语言中函数指针的声明也是比较严格的，在下面的代码中，myFun 指针能够指向一个函数，这个函数也是有一个 int 类型的参数，返回值也是 int： int (*myFun) (int); //C语言，声明一个函数指针 playscript 也采用这种比较严格的声明方式，因为我们想实现一个静态类型的语言： function int (int) myFun; //playscript中声明一个函数型的变量 写成上面这样是因为我个人喜欢把变量名称左边的部分看做类型的描述，不像 Go 语言把类型放在变量名称后面。最难读的就是 C 语言那种声明方式了，竟然把变量名放在了中间。当然，这只是个人喜好。把上面描述函数类型的语法写成 Antlr 的规则如下： functionType : FUNCTION typeTypeOrVoid '(' typeList? ')' ; typeList : typeType (',' typeType)* ; 在 playscript 中，我们用 FuntionType 接口代表一个函数类型，通过这个接口可以获得返回值类型、参数类型这两个信息： package play; import java.util.List; /** * 函数类型 */ public interface FunctionType extends Type { public Type getReturnType(); //返回值类型 public List\u003cType\u003e getParamTypes(); //参数类型 } 试一下实际使用效果如何，用 Antlr 解析下面这句的语法： function int(long, float) fun2 = fun1(); 它的意思是：调用 fun1() 函数会返回另一个函数，这个函数有两个参数，返回值是 int 型的。我们用 grun 显示一下 AST，你可以看到，它已经把 functionType 正确地解析出来了： 目前，我们只是设计完了语法，还要实现运行期的功能，让函数真的能像数值一样传来传去，就像下面的测试代码，它把 foo() 作为值赋给了 bar()： /* FirstClassFunction.play 函数作为一等公民。 也就是函数可以数值，赋给别的变量。 支持函数类型，即FunctionType。 */ int foo(int a){ println(\"in foo, a = \" + a); return a; } int bar (function int(int) fun){ int b = fun(6); println(\"in bar, b = \" + b); return b; } function int(int) a = foo; //函数作为变量初始化值 a(4); function int(int) b; b = foo; //函数用于赋值语句 b(5); bar(foo); //函数做为参数 运行这段代码，你会发现它实现了用函数来赋值，而实现这个功能的重点，是做好语义分析。比如编译程序要能识别赋值语句中的 foo 是一个函数，而不是一个传统的值。在调用 a() 和 b() 的时候，它也要正确地调用 foo() 的代码，而不是报“找不到 a() 函数的定义”这样的错误。实现了一等公民函数的功能以后，我们进入本讲最重要的一环：实现闭包功能。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:12:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现我们自己的闭包机制 在这之前，我想先设计好测试用例，所以先把一开始提到的那个 JavaScript 的例子用 playscript 的语法重写一遍，来测试闭包功能： /** * clojure.play * 测试闭包特性 */ int a = 0; function int() fun1(){ //函数的返回值是一个函数 int b = 0; //函数内的局部变量 int inner(){ //内部的一个函数 a = a+1; b = b+1; return b; //返回内部的成员 } return inner; //返回一个函数 } function int() fun2 = fun1(); for (int i = 0; i\u003c 3; i++){ println(\"b = \" + fun2() + \", a = \"+a); } function int() fun3 = fun1(); for (int i = 0; i\u003c 3; i++){ println(\"b = \" + fun3() + \", a = \"+a); } 代码的运行效果跟 JavaScript 版本的程序是一样的： b = 1, a = 1 b = 2, a = 2 b = 3, a = 3 b = 1, a = 4 b = 2, a = 5 b = 3, a = 6 这段代码的 AST 我也让 grun 显示出来了，并截了一部分图，你可以直观地看一下外层函数和内层函数的关系： 现在，测试用例准备好了，我们着手实现一下闭包的机制。前面提到，闭包的内在矛盾是运行时的环境和定义时的作用域之间的矛盾。那么我们把内部环境中需要的变量，打包交给闭包函数，它就可以随时访问这些变量了。在 AST 上做一下图形化的分析，看看给 fun2 这个变量赋值的时候，发生了什么事情： 简单地描述一下给 fun2 赋值时的执行过程：先执行 fun1() 函数，内部的 inner() 函数作为返回值返回给调用者。这时，程序能访问两层作用域，最近一层是 fun1()，里面有变量 b；外层还有一层，里面有全局变量 a。这时是把环境变量打包的最后的机会，否则退出 fun1() 函数以后，变量 b 就消失了。然后把内部函数连同打包好的环境变量的值，创建一个 FunctionObject 对象，作为 fun1() 的返回值，给到调用者。给 fun2 这个变量赋值。调用 fun2() 函数。函数执行时，有一个私有的闭包环境可以访问 b 的值，这个环境就是第二步所创建的 FunctionObject 对象。 最终，我们实现了闭包的功能。在这个过程中，我们要提前记录下 inner() 函数都引用了哪些外部变量，以便对这些变量打包。这是在对程序做语义分析时完成的，你可以参考一下 ClosureAnalyzer.java 中的代码： /** * 为某个函数计算闭包变量，也就是它所引用的外部环境变量。 * 算法：计算所有的变量引用，去掉内部声明的变量，剩下的就是外部的。 * @param function * @return */ private Set\u003cVariable\u003e calcClosureVariables(Function function){ Set\u003cVariable\u003e refered = variablesReferedByScope(function); Set\u003cVariable\u003e declared = variablesDeclaredUnderScope(function); refered.removeAll(declared); return refered; } 下面是 ASTEvaluator.java 中把环境变量打包进闭包中的代码片段，它是在当前的栈里获取数据的： /** * 为闭包获取环境变量的值 * @param function 闭包所关联的函数。这个函数会访问一些环境变量。 * @param valueContainer 存放环境变量的值的容器 */ private void getClosureValues(Function function, PlayObject valueContainer){ if (function.closureVariables != null) { for (Variable var : function.closureVariables) { // 现在还可以从栈里取，退出函数以后就不行了 LValue lValue = getLValue(var); Object value = lValue.getValue(); valueContainer.fields.put(var, value); } } } 你可以把测试用例跑一跑，修改一下，试试其他闭包特性。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:12:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"体验一下函数式编程 现在，我们已经实现了闭包的机制，函数也变成了一等公民。不经意间，我们似乎在一定程度上支持了函数式编程（functional programming）。它是一种语言风格，有很多优点，比如简洁、安全等。备受很多程序员推崇的 LISP 语言就具备函数式编程特征，Java 等语言也增加了函数式编程的特点。函数式编程的一个典型特点就是高阶函数（High-order function）功能，高阶函数是这样一种函数，它能够接受其他函数作为自己的参数，javascript 中数组的 map 方法，就是一个高阶函数。我们通过下面的例子测试一下高阶函数功能： java /** LinkedList.play 实现了一个简单的链表，并演示了高阶函数的功能，比如在javascript中常用的map功能， 它能根据遍历列表中的每个元素，执行一个函数，并返回一个新的列表。给它传不同的函数，会返回不同的列表。 */ //链表的节点 class ListNode{ int value; ListNode next; //下一个节点 ListNode (int v){ value = v; } } //链表 class LinkedList{ ListNode start; ListNode end; //添加新节点 void add(int value){ ListNode node = ListNode(value); if (start == null){ start = node; end = node; } else{ end.next = node; end = node; } } //打印所有节点内容 void dump(){ ListNode node = start; while (node != null){ println(node.value); node = node.next; } } //高阶函数功能，参数是一个函数，对每个成员做一个计算，形成一个新的LinkedList LinkedList map(function int(int) fun){ ListNode node = start; LinkedList newList = LinkedList(); while (node != null){ int newValue = fun(node.value); newList.add(newValue); node = node.next; } return newList; } } //函数：平方值 int square(int value){ return value * value; } //函数：加1 int addOne(int value){ return value + 1; } LinkedList list = LinkedList(); list.add(2); list.add(3); list.add(5); println(\"original list:\"); list.dump(); println(); println(\"add 1 to each element:\"); LinkedList list2 = list.map(addOne); list2.dump(); println(); println(\"square of each element:\"); LinkedList list3 = list.map(square); list3.dump(); 运行后得到的结果如下： original list: 2 3 5 add 1 to each element: 3 4 6 square of each element: 4 9 25 高阶函数功能很好玩，你可以修改程序，好好玩一下。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:12:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 闭包这个概念，对于初学者来讲是一个挑战。其实，闭包就是把函数在静态作用域中所访问的变量的生存期拉长，形成一份可以由这个函数单独访问的数据。正因为这些数据只能被闭包函数访问，所以也就具备了对信息进行封装、隐藏内部细节的特性。听上去是不是有点儿耳熟？封装，把数据和对数据的操作封在一起，这不就是面向对象编程嘛！一个闭包可以看做是一个对象。反过来看，一个对象是不是也可以看做一个闭包呢？对象的属性，也可以看做被方法所独占的环境变量，其生存期也必须保证能够被方法一直正常的访问。你看，两个不相干的概念，在用作用域和生存期这样的话语体系去解读之后，就会很相似，在内部实现上也可以当成一回事。现在，你应该更清楚了吧？ 本节课的示例代码放在了文末，供你参考。playscript-java（项目目录）： 码云 GitHub PlayScript.java（入口程序）： 码云 GitHub PlayScript.g4（语法规则）： 码云 GitHub ASTEvaluator.java（解释器，找找闭包运行期时怎么实现的）： 码云 GitHub ClosureAnalyzer.java（分析闭包所引用的环境变量）：码云 GitHub RefResolver.java（在这里看看函数型变量是怎么消解的）： 码云 GitHub closure.play（演示基本的闭包特征）： 码云 GitHub closure-fibonacci.play（用闭包实现了斐波那契数列计算）：码云 GitHub closure-mammal.play（用闭包实现了面向对象特性，请找找它比普通闭包强在哪里）：码云 GitHub FirstClassFunction.play（演示一等公民函数的特征）：码云 GitHub LinkedList.play（演示了高阶函数 map）：码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:12:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"11 | 语义分析（上）：如何建立一个完善的类型系统？ 在做语法分析时我们可以得到一棵语法树，而基于这棵树能做什么，是语义的事情。比如，+ 号的含义是让两个数值相加，并且通常还能进行缺省的类型转换。所以，如果要区分不同语言的差异，不能光看语言的语法。比如 Java 语言和 JavaScript 在代码块的语法上是一样的，都是用花括号，但在语义上是不同的，一个有块作用域，一个没有。这样看来，相比词法和语法的设计与处理，语义设计和分析似乎要复杂很多。虽然我们借作用域、生存期、函数等特性的实现涉猎了很多语义分析的场景，但离系统地掌握语义分析，还差一点儿火候。所以，为了帮你攻破语义分析这个阶段，我会用两节课的时间，再梳理一下语义分析中的重要知识，让你更好地建立起相关的知识脉络。今天这节课，我们把注意力集中在类型系统这个话题上。 围绕类型系统产生过一些争论，有的程序员会拥护动态类型语言，有的会觉得静态类型语言好。要想探究这个问题，我们需要对类型系统有个清晰的了解，最直接的方式，就是建立一个完善的类型系统。那么什么是类型系统？我们又该怎样建立一个完善的类型系统呢？其实，类型系统是一门语言所有的类型的集合，操作这些类型的规则，以及类型之间怎么相互作用的（比如一个类型能否转换成另一个类型）。如果要建立一个完善的类型系统，形成对类型系统比较完整的认知，需要从两个方面出发： 根据领域的需求，设计自己的类型系统的特征。在编译器中支持类型检查、类型推导和类型转换。 先从第一个方面出发看一下。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:13:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"设计类型系统的特征 在进入这个话题之前，我想先问你一个有意义的问题：类型到底是什么？我们说一个类型的时候，究竟在说什么？要知道，在机器代码这个层面，其实是分不出什么数据类型的。在机器指令眼里，那就是 0101，它并不对类型做任何要求，不需要知道哪儿是一个整数，哪儿代表着一个字符，哪儿又是内存地址。你让它做什么操作都可以，即使这个操作没有意义，比如把一个指针值跟一个字符相加。那么高级语言为什么要增加类型这种机制呢？ 对类型做定义很难，但大家公认的有一个说法：类型是针对一组数值，以及在这组数值之上的一组操作。比如，对于数字类型，你可以对它进行加减乘除算术运算，对于字符串就不行。所以，类型是高级语言赋予的一种语义，有了类型这种机制，就相当于定了规矩，可以检查施加在数据上的操作是否合法。因此类型系统最大的好处，就是可以通过类型检查降低计算出错的概率。所以，现代计算机语言都会精心设计一个类型系统，而不是像汇编语言那样完全不区分类型。不过，类型系统的设计有很多需要取舍和权衡的方面，比如： 面向对象的拥护者希望所有的类型都是对象，而重视数据计算性能的人认为应该支持非对象化的基础数据类型；你想把字符串作为原生数据类型，还是像 Java 那样只是一个普通的类？是静态类型语言好还是动态类型语言好？…… 虽然类型系统的设计有很多需要取舍和权衡的方面，但它最需要考虑的是，是否符合这门语言想解决的问题，我们用静态类型语言和动态类型语言分析一下。根据类型检查是在编译期还是在运行期进行的，我们可以把计算机语言分为两类： 静态类型语言（全部或者几乎全部的类型检查是在编译期进行的）。动态类型语言（类型的检查是在运行期进行的）。 静态类型语言的拥护者说：因为编译期做了类型检查，所以程序错误较少，运行期不用再检查类型，性能更高。像 C、Java 和 Go 语言，在编译时就对类型做很多处理，包括检查类型是否匹配，以及进行缺省的类型转换，大大降低了程序出错的可能性，还能让程序运行效率更高，因为不需要在运行时再去做类型检查和转换。 而动态类型语言的拥护者说：静态语言太严格，还要一遍遍编译，编程效率低，用动态类型语言方便进行快速开发。JavaScript、Python、PHP 等都是动态类型的。 客观地讲，这些说法都有道理。目前的趋势是，某些动态类型语言在想办法增加一些机制，在编译期就能做类型检查，比如用 TypeScript 代替 JavaScript 编写程序，做完检查后再输出成 JavaScript。而某些静态语言呢，却又发明出一些办法，部分地绕过类型检查，从而提供动态类型语言的灵活性。再延伸一下，跟静态类型和动态类型概念相关联的，还有强类型和弱类型。强类型语言中，变量的类型一旦声明就不能改变，弱类型语言中，变量类型在运行期时可以改变。二者的本质区别是，强类型语言不允许违法操作，因为能够被检查出来，弱类型语言则从机制上就无法禁止违法操作，所以是不安全的。比如你写了一个表达式 a*b。如果 a 和 b 这两个变量是数值，这个操作就没有问题，但如果 a 或 b 不是数值，那就没有意义了，弱类型语言可能就检查不出这类问题。 也就是，静态类型和动态类型说的是什么时候检查的问题，强类型和弱类型说的是就算检查，也检查不出来，或者没法检查的问题，这两组概念经常会被搞混，所以我在这里带你了解一下。 接着说回来。关于类型特征的取舍，是根据领域问题而定的。举例来说，很多人可能都觉得强类型更好，但对于儿童编程启蒙来说，他们最好尽可能地做各种尝试，如果必须遵守与类型有关的规则，程序总是跑不起来，可能会打击到他们。对于 playscript 而言，因为目前是用来做教学演示的，所以我们尽可能地多涉及与类型处理有关的情况，供大家体会算法，或者在自己的工作中借鉴。 首先，playscript 是静态类型和强类型的，所以几乎要做各种类型检查，你可以参考看看这些都是怎么做的。第二，我们既支持对象，也支持原生的基础数据类型。这两种类型的处理特点不一样，你也可以借鉴一下。后面面向对象的一讲，我会再讲与之相关的子类型（Subtyping）和运行时类型信息（Run Time Type Information, RTTI）的概念，这里就不展开了。第三，我们还支持函数作为一等公民，也就是支持函数的类型。函数的类型是它的原型，包括返回值和参数，原型一样的函数，就看做是同样类型的，可以进行赋值。这样，你也就可以了解实现函数式编程特性时，要处理哪些额外的类型问题。 接下来，我们来说一说如何做类型检查、类型推导和类型转换。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:13:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"如何做类型检查、类型推导和类型转换 先来看一看，如果编写一个编译器，我们在做类型分析时会遇到哪些问题。以下面这个最简单的表达式为例，这个表达式在不同的情况下会有不同的运行结果： a = b + 10 如果 b 是一个浮点型，b+10 的结果也是浮点型。如果 b 是字符串型的，有些语言也是允许执行 + 号运算的，实际的结果是字符串的连接。这个分析过程，就是类型推导（Type Inference）。当右边的值计算完，赋值给 a 的时候，要检查左右两边的类型是否匹配。这个过程，就是类型检查（Type Checking）。如果 a 的类型是浮点型，而右边传过来的是整型，那么一般就要进行缺省的类型转换（Type Conversion）。 类型的检查、推导和转换是三个工作，可是采用的技术手段差不多，所以我们放在一起讲，先来看看类型的推导。在早期的 playscript 的实现中，是假设运算符两边的类型都是整型的，并做了强制转换。这在实际应用中，当然不够用，因为我们还需要用到其他的数据类型。那怎么办呢？在运行时再去判断和转换吗？当然可以，但我们还有更好的选择，就是在编译期先判断出表达式的类型来。比如下面这段代码，是在 RefResolve.java 中，推导表达式的类型： java case PlayScriptParser.ADD: if (type1 == PrimitiveType.String || type2 == PrimitiveType.String){ type = PrimitiveType.String; } else if (type1 instanceof PrimitiveType \u0026\u0026 type2 instanceof PrimitiveType){ //类型“向上”对齐，比如一个int和一个float，取float type = PrimitiveType.getUpperType(type1,type2); }else{ at.log(\"operand should be PrimitiveType for additive operation\", ctx); } break; 这段代码提到，如果操作符号两边有一边数据类型是 String 类型的，那整个表达式就是 String 类型的。如果是其他基础类型的，就要按照一定的规则进行类型的转换，并确定运算结果的类型。比如，+ 号一边是 double 类型的，另一边是 int 类型的，那就要把 int 型的转换成 double 型的，最后计算结果也是 double 类型的。做了类型的推导以后，我们就可以简化运行期的计算，不需要在运行期做类型判断了： private Object add(Object obj1, Object obj2, Type targetType) { Object rtn = null; if (targetType == PrimitiveType.String) { rtn = String.valueOf(obj1) + String.valueOf(obj2); } else if (targetType == PrimitiveType.Integer) { rtn = ((Number)obj1).intValue() + ((Number)obj2).intValue(); } else if (targetType == PrimitiveType.Float) { rtn = ((Number)obj1).floatValue()+ ((Number)obj2).floatValue(); } ... return rtn; } 通过这个类型推导的例子，我们又可以引出 S 属性（Synthesized Attribute）的知识点。如果一种属性能够从下级节点推导出来，那么这种属性就叫做 S 属性，字面意思是综合属性，就是在 AST 中从下级的属性归纳、综合出本级的属性。更准确地说，是通过下级节点和自身来确定的。 与 S 属性相对应的是 I 属性（Inherited Attribute），也就是继承属性，即 AST 中某个节点的属性是由上级节点、兄弟节点和它自身来决定的，比如： int a; 变量 a 的类型是 int，这个很直观，因为变量声明语句中已经指出了 a 的类型，但这个类型可不是从下级节点推导出来的，而是从兄弟节点推导出来的。在 PlayScript.g4 中，变量声明的相关语法如下： variableDeclarators : typeType variableDeclarator (',' variableDeclarator)* ; variableDeclarator : variableDeclaratorId ('=' variableInitializer)? ; variableDeclaratorId : IDENTIFIER ('[' ']')* ; typeType : (classOrInterfaceType| functionType | primitiveType) ('[' ']')* ; 把 int a; 这样一个简单的变量声明语句解析成 AST，就形成了一棵有两个分枝的树： 这棵树的左枝，可以从下向上推导类型，所以类型属性也就是 S 属性。而右枝则必须从根节点（也就是 variableDeclarators）往下继承类型属性，所以对于 a 这个节点来说，它的类型属性是 I 属性。这里插一句，RefResolver.java 实现了 PlayScriptListener 接口。这样，我们可以用标准的方法遍历 AST。代码中的 enterXXX() 方法表示刚进入这个节点，exitXXX() 方法表示退出这个节点，这时所有的子节点都已经遍历过了。在计算 S 属性时，我一定是在 exitXXX() 方法中，因为可以利用下级节点的类型推导出自身节点的类型。很多现代语言会支持自动类型推导，例如 Go 语言就有两种声明变量的方式： var a int = 10 //第一种 a := 10 //第二种 第一种方式，a 的类型是显式声明的；第二种方式，a 的类型是由右边的表达式推导出来的。从生成的 AST 中，你能看到它们都是经历了从下到上的综合，再从上到下的继承的过程： 说完了类型推导，我们再看看类型检查。类型检查主要出现在几个场景中： 赋值语句（检查赋值操作左边和右边的类型是否匹配）。变量声明语句（因为变量声明语句中也会有初始化部分，所以也需要类型匹配）。函数传参（调用函数的时候，传入的参数要符合形参的要求）。函数返回值（从函数中返回一个值的时候，要符合函数返回值的规定）。 类型检查还有一个特点：以赋值语句为例，左边的类型，是 I 属性，是从声明中得到的；右边的类型是 S 属性，是自下而上综合出来的。当左右两边的类型相遇之后，就要检查二者是否匹配，被赋值的变量要满足左边的类型要求。如果匹配，自然没有问题，如果不完全匹配，也不一定马上报错，**而是要看看是否能进行类型转换。**比如，一般的语言在处理整型和浮点型的混合运算时，都能进行自动的转换。像 JavaScript 和 SQL，甚至能够在算术运算时，自动将字符串转换成数字。在 MySQL 里，运行下面的语句，会得到 3，它自动将’2’转换成了数字： select 1 + '2'; 这个过程其实是有风险的，这就像在强类型的语言中开了一个后门，绕过或部分绕过了编译器的类型检查功能。把父类转成子类的场景中，编译器顶多能检查这两个类之间是否有继承关系，如果连继承关系都没有，这当然能检查出错误，制止这种转换。但一个基类的子类可能是很多的，具体这个转换对不对，只有到运行期才能检查出错误来。C 语言因为可以强制做各种转换，这个后门开的就更大了。不过这也是 C 语言要达到它的设计目的，必须具备的特性。 关于类型的处理，大家可以参考 playscript 的示例代码，里面有三个类可以看一看：TypeResolver.java（做了自上而下的类型推导，也就是 I 属性的计算，包括变量 - 声明、类的继承声明、函数声明）。RefResolver.java（有自下而上的类型推导的逻辑）。TypeChecker.java（类型检查）。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:13:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课我们重点探讨了语义分析和语言设计中的一个重要话题：类型系统。理解类型系统，了解它的本质对我们学习语言会有很大的帮助。我希望在这个过程中，你不会再被静态类型和动态类型，强类型和弱类型这样的概念难倒，甚至可以质疑已有的一些观念。比如，如果你仔细研究，会发现静态类型和动态类型不是绝对的，静态类型的语言如 Java，也会在运行期去处理一些类型检查。强类型和弱类型可能也不是绝对的，就像 C 语言，你如果不允许做任何强制类型转换，不允许指针越界，那它也就完全变成强类型的了。掌握对计算机语言更深一点儿的理解能力，将会是你学习编译原理的额外回报！ 本节课相关的示例代码放在文末，供你参考。playscript-java（项目目录）： 码云 GitHub PlayScript.g4（语法规则）： 码云 GitHub TypeAndScopeScanner.java（类型和作用域扫描）： 码云 GitHub TypeResolver.java（自上而下的类型推导）： 码云 GitHub RefResolver.java（自下而上的类型推导）： 码云 GitHub TypeChecker.java（类型检查）： 码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:13:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"12 | 语义分析（下）：如何做上下文相关情况的处理？ 我们知道，词法分析和语法分析阶段，进行的处理都是上下文无关的。可仅凭上下文无关的处理，是不能完成一门强大的语言的。比如先声明变量，再用变量，这是典型的上下文相关的情况，我们肯定不能用上下文无关文法表达这种情况，所以语法分析阶段处理不了这个问题，只能在语义分析阶段处理。**语义分析的本质，就是针对上下文相关的情况做处理。**我们之前讲到的作用域，是一种上下文相关的情况，因为如果作用域不同，能使用的变量也是不同的。类型系统也是一种上下文相关的情况，类型推导和类型检查都要基于上下文中相关的 AST 节点。本节课，我们再讲两个这样的场景：引用的消解、左值和右值，然后再介绍上下文相关情况分析的一种方法：属性计算。这样，你会把语义分析就是上下文处理的本质掌握得更清楚，并掌握属性计算这个强大的方法。我们先来说说引用的消解这个场景。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:14:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"语义分析场景：引用的消解 在程序里使用变量、函数、类等符号时，我们需要知道它们指的是谁，要能对应到定义它们的地方。下面的例子中，当使用变量 a 时，我们需要知道它是全局变量 a，还是 fun() 函数中的本地变量 a。因为不同作用域里可能有相同名称的变量，所以必须找到正确的那个。这个过程，可以叫引用消解。 /* scope.c 测试作用域 */ #include \u003cstdio.h\u003e int a = 1; void fun() { a = 2; //这是指全局变量a int a = 3; //声明一个本地变量 int b = a; //这个a指的是本地变量 printf(\"in func: a=%d b=%d \\n\", a, b); } 在集成开发环境中，当我们点击一个变量、函数或类，可以跳到定义它的地方。另一方面，当我们重构一个变量名称、方法名称或类名称的时候，所有引用它的地方都会同步修改。这是因为 IDE 分析了符号之间的交叉引用关系。 函数的引用消解比变量的引用消解还要更复杂一些。 它不仅要比对函数名称，还要比较参数和返回值（可以叫函数原型，又或者叫函数的类型）。我们在把函数提升为一等公民的时候，提到函数类型（FunctionType）的概念。两个函数的类型相同，需要返回值、参数个数、每个参数的类型都能匹配得上才行。 在面向对象编程语言中，函数引用的消解也很复杂。 当一个参数需要一个对象的时候，程序中提供其子类的一个实例也是可以的，也就是子类可以用在所有需要父类的地方，例如下面的代码： class MyClass1{} //父类 class MyClass2 extends MyClass1{} //子类 MyClass1 obj1; MyClass2 obj2; function fun(MyClass1 obj){} //参数需要父类的实例 fun(obj2); //提供子类的实例 在 C++ 语言中，引用的消解还要更加复杂。 它还要考虑某个实参是否能够被自动转换成形参所要求的类型，比如在一个需要 double 类型的地方，你给它传一个 int 也是可以的。 命名空间也是做引用消解的时候需要考虑的因素。 像 Java、C++ 都支持命名空间。如果在代码前头引入了某个命名空间，我们就可以直接引用里面的符号，否则需要冠以命名空间。例如： play.PlayScriptCompiler.Compile() //Java语言 play::PlayScriptCompiler.Compile() //C++语言 而做引用消解可能会产生几个结果：解析出了准确的引用关系。重复定义（在声明新的符号的时候，发现这个符号已经被定义过了）。引用失败（找不到某个符号的定义）。如果两个不同的命名空间中都有相同名称的符号，编程者需要明确指定。 在 playscript 中，引用消解的结果被存到了 AnnotatedTree.java 类中的 symbolOfNode 属性中去了，从它可以查到某个 AST 节点引用的到底是哪个变量或函数，从而在运行期正确的执行，你可以看一下代码，了解引用消解和使用的过程。了解完引用的消解之后，接下来，我们再讲一个很有意思的场景：左值和右值。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:14:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"语义分析场景：左值和右值 在开发编译器或解释器的过程中，你一定会遇到左值和右值的问题。比如，在 playscript 的 ASTEvaluate.java 中，我们在 visitPrimary 节点可以对变量求值。如果是下面语句中的 a，没有问题，把 a 变量的值取出来就好了： a + 3; 可是，如果针对的是赋值语句，a 在等号的左边，怎么对 a 求值呢？ a = 3; 假设 a 变量原来的值是 4，如果还是把它的值取出来，那么成了 3=4，这就变得没有意义了。所以，不能把 a 的值取出来，而应该取出 a 的地址，或者说 a 的引用，然后用赋值操作把 3 这个值写到 a 的内存地址。这时，我们说取出来的是 a 的左值（L-value）。左值最早是在 C 语言中提出的，通常出现在表达式的左边，如赋值语句的左边。左值取的是变量的地址（或者说变量的引用），获得地址以后，我们就可以把新值写进去了。 与左值相对应的就是右值（R-value），右值就是我们通常所说的值，不是地址。在上面这两种情况下，变量 a 在 AST 中都是对应同一个节点，也就是 primary 节点。那这个节点求值时是该返回左值还是右值呢？这要借助上下文来分析和处理。如果这个 primary 节点存在于下面这几种情况中，那就需要取左值： 赋值表达式的左边；带有初始化的变量声明语句中的变量；当给函数形参赋值的时候；一元操作符： ++ 和–。其他需要改变变量内容的操作。 在讨论 primary 节点在哪种情况下取左值时，我们可以引出另一个问题：不是所有的表达式，都能生成一个合格的左值。也就是说，出现在赋值语句左边的，必须是能够获得左值的表达式。比如一个变量是可以的，一个类的属性也是可以的。但如果是一个常量，或者 2+3 这样的表达式在赋值符号的左边，那就不行。所以，判断表达式能否生成一个合格的左值也是语义检查的一项工作。 借上节课讲过的 S 属性和 I 属性的概念，我们把刚才说的两个情况总结成 primay 节点的两个属性，你可以判断一下，这两个属性是 S 属性还是 I 属性？ 属性 1：某 primary 节点求值时，是否应该求左值？属性 2：某 primary 节点求值时，能否求出左值？ 你可能发现了，这跟我们类型检查有点儿相似，一个是 I 属性，一个是 S 属性，两个一比对，就能检查求左值的表达式是否合法。从这儿我们也能看出，处理上下文相关的情况，经常用属性计算的方法。接下来，我们就谈谈如何做属性计算。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:14:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"如何做属性计算 属性计算是做上下文分析，或者说语义分析的一种算法。按照属性计算的视角，我们之前所处理的各种语义分析问题，都可以看做是对 AST 节点的某个属性进行计算。比如，针对求左值场景中的 primary 节点，它需要计算的属性包括： 它的变量定义是哪个（这就引用到定义该变量的 Symbol）。它的类型是什么?它的作用域是什么？这个节点求值时，是否该返回左值？能否正确地返回一个左值？它的值是什么？ 从属性计算的角度看，对表达式求值，或运行脚本，只是去计算 AST 节点的 Value 属性，Value 这个属性能够计算，其他属性当然也能计算。属性计算需要用到属性文法。在词法、语法分析阶段，我们分别学习了正则文法和上下文无关文法，在语义分析阶段我们要了解的是属性文法（Attribute Grammar）。属性文法的主要思路是计算机科学的重要开拓者，高德纳（Donald Knuth）在《The Genesis of Attribute Grammers》中提出的。它是在上下文无关文法的基础上做了一些增强，使之能够计算属性值。下面是上下文无关文法表达加法和乘法运算的例子： add → add + mul add → mul mul → mul * primary mul → primary primary → \"(\" add \")\" primary → integer 然后看一看对 value 属性进行计算的属性文法： add1 → add1 + mul [ add1.value = add2.value + mul.value ] add → mul [ add.value = mul.value ] mul1 → mul2 * primary [ mul1.value = mul2.value * primary.value ] mul → primary [ mul.value = primary.value ] primary → \"(\" add \")\" [ primary.value = add.value ] primary → integer [ primary.value = strToInt(integer.str) ] 利用属性文法，我们可以定义规则，然后用工具自动实现对属性的计算。有同学曾经问：“我们解析表达式 2+3 的时候，得到一个 AST，但我怎么知道它运算的时候是做加法呢？”因为我们可以在语法规则的基础上制定属性文法，在解析语法的过程中或者形成 AST 之后，我们就可以根据属性文法的规则做属性计算。比如在 Antlr 中，你可以在语法规则文件中插入一些代码，在语法分析的过程中执行你的代码，完成一些必要的计算。 总结一下属性计算的特点：它会基于语法规则，增加一些与语义处理有关的规则。 所以，我们也把这种语义规则的定义叫做语法制导的定义（Syntax directed definition，SDD），如果变成计算动作，就叫做语法制导的翻译（Syntax directed translation，SDT）。属性计算，可以伴随着语法分析的过程一起进行，也可以在做完语法分析以后再进行。这两个阶段不一定完全切分开。甚至，我们有时候会在语法分析的时候做一些属性计算，然后把计算结果反馈回语法分析的逻辑，帮助语法分析更好地执行（这是在工程实践中会运用到的一个技巧，我这里稍微做了一个延展，帮大家开阔一下思路，免得把知识学得太固化了）。 那么，在解析语法的时候，如何同时做属性计算呢？我们知道，解析语法的过程，是逐步建立 AST 的过程。在这个过程中，计算某个节点的属性所依赖的其他节点可能被创建出来了。比如在递归下降算法中，当某个节点建立完毕以后，它的所有子节点一定也建立完毕了，所以 S 属性就可以计算出来了。同时，因为语法解析是从左向右进行的，它左边的兄弟节点也都建立起来了。如果某个属性的计算，除了可能依赖子节点以外，只依赖左边的兄弟节点，不依赖右边的，这种属性就叫做 L 属性。它比 S 属性的范围更大一些，包含了部分的 I 属性。由于我们常用的语法分析的算法都是从左向右进行的，所以就很适合一边解析语法，一边计算 L 属性。 比如，C 语言和 Java 语言进行类型分析，都可以用 L 属性的计算来实现。因为这两门语言的类型要么是从下往上综合出来的，属于 S 属性。要么是在做变量声明的时候，由声明中的变量类型确定的，类型节点在变量的左边。 2+3; //表达式类型是整型 float a; //a的类型是浮点型 那问题来了，Go 语言的类型声明是放在变量后面的，这意味着类型节点一定是在右边的，那就不符合 L 属性文法了： var a int = 10 没关系，我们没必要在语法分析阶段把属性全都计算出来，等到语法分析完毕后，再对 AST 遍历一下就好了。这时所有节点都有了，计算属性也就不是难事了。 在我们的 playscript 语言里，就采取了这种策略，实际上，为了让算法更清晰，我把语义分析过程拆成了好几个任务，对 AST 做了多次遍历。 第 1 遍：类型和作用域解析（TypeAndScopeScanner.java）。把自定义类、函数和和作用域的树都分析出来。这么做的好处是，你可以使用在前，声明在后。比如你声明一个 Mammal 对象，而 Mammal 类的定义是在后面才出现的；在定义一个类的时候，对于类的成员也会出现使用在声明之前的情况，把类型解析先扫描一遍，就能方便地支持这个特性。在写属性计算的算法时，计算的顺序可能是个最重要的问题。因为某属性的计算可能要依赖别的节点的属性先计算完。我们讨论的 S 属性、I 属性和 L 属性，都是在考虑计算顺序。像使用在前，声明在后这种情况，就更要特殊处理了。 第 2 遍：类型的消解（TypeResolver.java）。把所有出现引用到类型的地方，都消解掉，比如变量声明、函数参数声明、类的继承等等。做完消解以后，我们针对 Mammal m; 这样语句，就明确的知道了 m 的类型。这实际上是对 I 属性的类型的计算。 第 3 遍：引用的消解和 S 属性的类型的推导（RefResolver.java）。这个时候，我们对所有的变量、函数调用，都会跟它的定义关联起来，并且完成了所有的类型计算。 第 4 遍：做类型检查（TypeChecker.java）。比如当赋值语句左右两边的类型不兼容的时候，就可以报错。 第 5 遍：做一些语义合法性的检查（SematicValidator.java）。比如 break 只能出现在循环语句中，如果某个函数声明了返回值，就一定要有 return 语句，等等。 语义分析的结果保存在 AnnotatedTree.java 类里，意思是被标注了属性的语法树。注意，这些属性在数据结构上，并不一定是 AST 节点的属性，我们可以借助 Map 等数据结构存储，只是在概念上，这些属性还是标注在树节点上的。AnnotatedTree 类的结构如下： java public class AnnotatedTree { // AST protected ParseTree ast = null; // 解析出来的所有类型，包括类和函数 protected List\u003cType\u003e types = new LinkedList\u003cType\u003e(); // AST节点对应的Symbol protected Map\u003cParserRuleContext, Symbol\u003e symbolOfNode = new HashMap\u003cParserRuleContext, Symbol\u003e(); // AST节点对应的Scope，如for、函数调用会启动新的Scope protected Map\u003cParserRuleContext, Scope\u003e node2Scope = new HashMap\u003cParserRuleContext, Scope\u003e(); // 每个节点推断出来的类型 protected Map\u003cParserRuleContext, Type\u003e typeOfNode = new HashMap\u003cParserRuleContext, Type\u003e(); // 命名空间，作用域的根节点 NameSpace nameSpace = null; ... } 我建议你看看这些语义分析的代码，了解一下如何保证语义分析的全面性。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:14:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课我带你继续了解了语义分析的相关知识：语义分析的本质是对上下文相关情况的处理，能做词法分析和语法分析所做不到的事情。了解引用消解，左值和右值的场景，可以增加对语义分析的直观理解。掌握属性计算和属性文法，可以使我们用更加形式化、更清晰的算法来完成语义分析的任务。 在我看来，语义分析这个阶段十分重要。因为词法和语法都有很固定的套路，甚至都可以工具化的实现。但语言设计的核心在于语义，特别是要让语义适合所解决的问题。比如 SQL 语言针对的是数据库的操作，那就去充分满足这个目标就好了。我们在前端技术的应用篇中，也会复盘讨论这个问题，不断实现认知的迭代升级。如果想做一个自己领域的 DSL，学习了这几讲语义分析的内容之后，你会更好地做语义特性的设计与取舍，也会对如何完成语义分析有清晰的思路。 本节课相关的示例代码放在文末，供你参考。playscript-java（项目目录）： 码云 GitHub PlayScript.g4（语法规则）： 码云 GitHub TypeAndScopeScanner.java（类型和作用域扫描）： 码云 GitHub TypeResolver.java（消解变量声明中引用的类型）： 码云 GitHub RefResolver.java（变量和函数应用的消解，及 S 属性的类型推断）： 码云 GitHub TypeChecker.java（类型检查）： 码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:14:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"13 | 继承和多态：面向对象运行期的动态特性 面向对象是一个比较大的话题。在“09 | 面向对象：实现数据和方法的封装”中，我们了解了面向对象的封装特性，也探讨了对象成员的作用域和生存期特征等内容。本节课，我们再来了解一下面向对象的另外两个重要特征：继承和多态。 你也许会问，为什么没有在封装特性之后，马上讲继承和多态呢？那是因为继承和多态涉及的语义分析阶段的知识点比较多，特别是它对类型系统提出了新的概念和挑战，所以我们先掌握语义分析，再了解这部分内容，才是最好的选择。继承和多态对类型系统提出的新概念，就是子类型。我们之前接触的类型往往是并列关系，你是整型，我是字符串型，都是平等的。而现在，一个类型可以是另一个类型的子类型，比如我是一只羊，又属于哺乳动物。这会导致我们在编译期无法准确计算出所有的类型，从而无法对方法和属性的调用做完全正确的消解（或者说绑定）。这部分工作要留到运行期去做，也因此，面向对象编程会具备非常好的优势，因为它会导致多态性。这个特性会让面向对象语言在处理某些类型的问题时，更加优雅。而我们要想深刻理解面向对象的特征，就必须了解子类型的原理和运行期的机制。所以，接下来，我们从类型体系的角度理解继承和多态，然后看看在编译期需要做哪些语义分析，再考察继承和多态的运行期特征。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:15:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"从类型体系的角度理解继承和多态 继承的意思是一个类的子类，自动具备了父类的属性和方法，除非被父类声明为私有的。比如一个类是哺乳动物，它有体重（weight）的属性，还会做叫 (speak) 的操作。如果基于哺乳动物这个父类创建牛和羊两个子类，那么牛和羊就自动继承了哺乳动物的属性，有体重，还会叫。 所以继承的强大之处，就在于重用。也就是有些逻辑，如果在父类中实现，在子类中就不必重复实现。 多态的意思是同一个类的不同子类，在调用同一个方法时会执行不同的动作。这是因为每个子类都可以重载掉父类的某个方法，提供一个不同的实现。哺乳动物会“叫”，而牛和羊重载了这个方法，发出“哞~”和“咩~”的声音。这似乎很普通，但如果创建一个哺乳动物的数组，并在里面存了各种动物对象，遍历这个数组并调用每个对象“叫”的方法时，就会发出“哞~”“咩~”“喵~”等各种声音，这就有点儿意思了。 下面这段示例代码，演示了继承和多态的特性，a 的 speak() 方法和 b 的 speak() 方法会分别打印出牛叫和羊叫，调用的是子类的方法，而不是父类的方法： /** mammal.play 演示面向对象编程：继承和多态。 */ class Mammal{ int weight = 20; boolean canSpeak(){ return true; } void speak(){ println(\"mammal speaking...\"); } } class Cow extends Mammal{ void speak(){ println(\"moo~~ moo~~\"); } } class Sheep extends Mammal{ void speak(){ println(\"mee~~ mee~~\"); println(\"My weight is: \" + weight); //weight的作用域覆盖子类 } } //将子类的实例赋给父类的变量 Mammal a = Cow(); Mammal b = Sheep(); //canSpeak()方法是继承的 println(\"a.canSpeak() : \" + a.canSpeak()); println(\"b.canSpeak() : \" + b.canSpeak()); //下面两个的叫声会不同，在运行期动态绑定方法 a.speak(); //打印牛叫 b.speak(); //打印羊叫 所以，多态的强大之处，在于虽然每个子类不同，但我们仍然可以按照统一的方式使用它们，做到求同存异。以前端工程师天天打交道的前端框架为例，这是最能体现面向对象编程优势的领域之一。 前端界面往往会用到各种各样的小组件，比如静态文本、可编辑文本、按钮等等。如果我们想刷新组件的显示，没必要针对每种组件调用一个方法，把所有组件的类型枚举一遍，可以直接调用父类中统一定义的方法 redraw()，非常简洁。即便将来添加新的前端组件，代码也不需要修改，程序也会更容易维护。 总结一下：面向对象编程时，我们可以给某个类创建不同的子类，实现一些个性化的功能；写程序时，我们可以站在抽象度更高的层次上，不去管具体的差异。 如果把上面的结论抽象成一般意义上的类型理论，就是子类型（subtype）。子类型（或者动名词：子类型化），是对我们前面讲的类型体系的一个补充。子类型的核心是提供了 is-a 的操作。也就是对某个类型所做的所有操作都可以用子类型替代。因为子类型 is a 父类型，也就是能够兼容父类型，比如一只牛是哺乳动物。这意味着只要对哺乳动物可以做的操作，都可以对牛来做，这就是子类型的好处。它可以放宽对类型的检查，从而导致多态。你可以粗略地把面向对象的继承看做是子类型化的一个体现，它的结果就是能用子类代替父类，从而导致多态。子类型有两种实现方式：一种就是像 Java 和 C++ 语言，需要显式声明继承了什么类，或者实现了什么接口。这种叫做名义子类型（Nominal Subtyping）。另一种是结构化子类型（Structural Subtyping），又叫鸭子类型（Duck Type）。也就是一个类不需要显式地说自己是什么类型，只要它实现了某个类型的所有方法，那就属于这个类型。鸭子类型是个直观的比喻，如果我们定义鸭子的特征是能够呱呱叫，那么只要能呱呱叫的，就都是鸭子。了解了继承和多态之后，我们看看在编译期如何对继承和多态的特性做语义分析。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:15:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"如何对继承和多态的特性做语义分析 针对哺乳动物的例子，我们用前面语义分析的知识，看看如何在编译期针对继承和多态做语义分析，也算对语义分析的知识点进行应用和复盘。首先，从类型处理的角度出发，我们要识别出新的类型：Mammal、Cow 和 Sheep。之后，就可以用它们声明变量了。第二，我们要设置正确的作用域。从作用域的角度来看，一个类的属性（或者说成员变量），是可以规定能否被子类访问的。以 Java 为例，除了声明为 private 的属性以外，其他属性在子类中都是可见的。所以父类的属性的作用域，可以说是以树状的形式覆盖到了各级子类： 第三，要对变量和函数做类型的引用消解。也就是要分析出 a 和 b 这两个变量的类型。那么 a 和 b 的类型是什么呢？是父类 Mammal？还是 Cow 或 Sheep？注意，代码里是用 Mammal 来声明这两个变量的。按照类型推导的算法，a 和 b 都是 Mammal，这是个 I 属性计算的过程。也就是说，在编译期，我们无法知道变量被赋值的对象确切是哪个子类型，只知道声明变量时，它们是哺乳动物类型，至于是牛还是羊，就不清楚了。你可能会说：“不对呀，我在编译的时候能知道 a 和 b 的准确类型啊，因为我看到了 a 是一个 Cow 对象，而 b 是一个 Sheep，代码里有这两个对象的创建过程，我可以推导出 a 和 b 的实际类型呀。”没错，语言的确有自动类型推导的特性，但你忽略了限制条件。比如，强类型机制要求变量的类型一旦确定，在运行过程就不能再改，所以要让 a 和 b 能够重新指向其他的对象，并保持类型不变。从这个角度出发，a 和 b 的类型只能是父类 Mammal。 所以说，编译期无法知道变量的真实类型，可能只知道它的父类型，也就是知道它是一个哺乳动物，但不知道它具体是牛还是羊。这会导致我们没法正确地给 speak() 方法做引用消解。正确的消解，是要指向 Cow 和 Sheep 的 speak 方法，而我们只能到运行期再解决这个问题。所以接下来，我们就讨论一下如何在运行期实现方法的动态绑定。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:15:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"如何在运行期实现方法的动态绑定 在运行期，我们能知道 a 和 b 这两个变量具体指向的是哪个对象，对象里是保存了真实类型信息的。具体来说，在 playscript 中，ClassObject 的 type 属性会指向一个正确的 Class，这个类型信息是在创建对象的时候被正确赋值的： 在调用类的属性和方法时，我们可以根据运行时获得的，确定的类型信息进行动态绑定。下面这段代码是从本级开始，逐级查找某个方法的实现，如果本级和父类都有这个方法，那么本级的就会覆盖掉父类的，这样就实现了多态： java protected Function getFunction(String name, List\u003cType\u003e paramTypes){ //在本级查找这个这个方法 Function rtn = super.getFunction(name, paramTypes); //TODO 是否要检查visibility //如果在本级找不到，那么递归的从父类中查找 if (rtn == null \u0026\u0026 parentClass != null){ rtn = parentClass.getFunction(name,paramTypes); } return rtn; } 如果当前类里面没有实现这个方法，它可以直接复用某一级的父类中的实现，这实际上就是继承机制在运行期的原理。 你看，只有了解运行期都发生了什么，才能知道继承和多态是怎么发生的吧。这里延伸一下。我们刚刚谈到，在运行时可以获取类型信息，这种机制就叫做运行时类型信息（Run Time Type Information, RTTI）。C++、Java 等都有这种机制，比如 Java 的 instanceof 操作，就能检测某个对象是不是某个类或者其子类的实例。汇编语言是无类型的，所以一般高级语言在编译成目标语言之后，这些高层的语义就会丢失。如果要在运行期获取类型信息，需要专门实现 RTTI 的功能，这就要花费额外的存储开销和计算开销。就像在 playscript 中，我们要在 ClassObject 中专门拿出一个字段来存 type 信息。现在，我们已经了解如何在运行期获得类型信息，实现方法的动态绑定。接下来，我带你了解一下运行期的对象的逐级初始化机制。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:15:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"继承情况下对象的实例化 在存在继承关系的情况下，创建对象时，不仅要初始化自己这一级的属性变量，还要把各级父类的属性变量也都初始化。比如，在实例化 Cow 的时候，还要对 Mammal 的成员变量 weight 做初始化。所以我们要修改 playscript 中对象实例化的代码，从最顶层的祖先起，对所有的祖先层层初始化： //从父类到子类层层执行缺省的初始化方法，即不带参数的初始化方法 protected ClassObject createAndInitClassObject(Class theClass) { ClassObject obj = new ClassObject(); obj.type = theClass; Stack\u003cClass\u003e ancestorChain = new Stack\u003cClass\u003e(); // 从上到下执行缺省的初始化方法 ancestorChain.push(theClass); while (theClass.getParentClass() != null) { ancestorChain.push(theClass.getParentClass()); theClass = theClass.getParentClass(); } // 执行缺省的初始化方法 StackFrame frame = new StackFrame(obj); pushStack(frame); while (ancestorChain.size() \u003e 0) { Class c = ancestorChain.pop(); defaultObjectInit(c, obj); } popStack(); return obj; } 在逐级初始化的过程中，我们要先执行缺省的成员变量初始化，也就是变量声明时所带的初始化部分，然后调用这一级的构造方法。如果不显式指定哪个构造方法，就会执行不带参数的构造方法。不过有的时候，子类会选择性地调用父类某一个构造方法，就像 Java 可以在构造方法里通过 super() 来显式地调用父类某个具体构造方法。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:15:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"如何实现 this 和 super 现在，我们已经了解了继承和多态在编译期和运行期的特性。接下来，我们通过一个示例程序，把本节课的所有知识复盘检验一下，加深对它们的理解，也加深对 this 和 super 机制的理解。这个示例程序是用 Java 写的，在 Java 语言中，为面向对象编程专门提供了两个关键字：this 和 super，这两个关键字特别容易引起混乱。比如在下面的 ThisSuperTest.Java 代码中，Mammal 和它的子类 Cow 都有 speak() 方法。如果我们要创建一个 Cow 对象，会调用 Mammal 的构造方法 Mammal(int weight)，而在这个构造方法里调用的 this.speak() 方法，是 Mammal 的，还是 Cow 的呢？ java package play; public class ThisSuperTest { public static void main(String args[]){ //创建Cow对象的时候，会在Mammal的构造方法里调用this.reportWeight()，这里会显示什么 Cow cow = new Cow(); System.out.println(); //这里调用，会显示什么 cow.speak(); } } class Mammal{ int weight; Mammal(){ System.out.println(\"Mammal() called\"); this.weight = 100; } Mammal(int weight){ this(); //调用自己的另一个构造函数 System.out.println(\"Mammal(int weight) called\"); this.weight = weight; //这里访问属性，是自己的weight System.out.println(\"this.weight in Mammal : \" + this.weight); //这里的speak()调用的是谁，会显示什么数值 this.speak(); } void speak(){ System.out.println(\"Mammal's weight is : \" + this.weight); } } class Cow extends Mammal{ int weight = 300; Cow(){ super(200); //调用父类的构造函数 } void speak(){ System.out.println(\"Cow's weight is : \" + this.weight); System.out.println(\"super.weight is : \" + super.weight); } } 运行结果如下： Mammal() called Mammal(int weight) called this.weight in Mammal : 200 Cow's weight is : 0 super.weight is : 200 Cow's weight is : 300 super.weight is : 200 答案是 Cow 的 speak() 方法，而不是 Mammal 的。怎么回事？代码里不是调用的 this.speak() 吗？怎么这个 this 不是 Mammal，却变成了它的子类 Cow 呢？其实，在这段代码中，this 用在了三个地方： this.weight 是访问自己的成员变量，因为成员变量的作用域是这个类本身，以及子类。this() 是调用自己的另一个构造方法，因为这是构造方法，肯定是做自身的初始化。换句话说，构造方法不存在多态问题。this.speak() 是调用一个普通的方法。这时，多态仍会起作用。运行时会根据对象的实际类型，来绑定到 Cow 的 speak() 方法上。 只不过，在 Mammal 的构造方法中调用 this.speak() 时，虽然访问的是 Cow 的 speak() 方法，打印的是 Cow 中定义的 weight 成员变量，但它的值却是 0，而不是成员变量声明时“int weight = 300;”的 300。为什么呢？要想知道这个答案，我们需要理解多层继承情况下对象的初始化过程。在 Mammal 的构造方法中调用 speak() 的时候，Cow 的初始化过程还没有开始呢，所以“int weight = 300;”还没有执行，Cow 的 weight 属性还是缺省值 0。 怎么样？一个小小的例子，却需要用到三个方面的知识：面向对象的成员变量的作用域、多态、对象初始化。Java 程序员可以拿这个例子跟同事讨论一下，看看是不是很好玩。 讨论完 this，super 就比较简单了，它的语义要比 this 简单，不会出现歧义。super 的调用，也是分成三种情况：super.weight。这是调用父类或更高的祖先的 weight 属性，而不是 Cow 这一级的 weight 属性。不一定非是直接父类，也可以是祖父类中的。根据变量作用域的覆盖关系，只要是比 Cow 这一级高的就行。super(200)。这是调用父类的构造方法，必须是直接父类的。super.speak()。跟访问属性的逻辑一样，是调用父类或更高的祖先的 speak() 方法。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:15:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 这节课我带你实现了面向对象中的另两个重要特性：继承和多态。在这节课中，我建议你掌握的重点内容是： 从类型的角度，面向对象的继承和多态是一种叫做子类型的现象，子类型能够放宽对类型的检查，从而支持多态。在编译期，无法准确地完成对象方法和属性的消解，因为无法确切知道对象的子类型。在运行期，我们能够获得对象的确切的子类型信息，从而绑定正确的方法和属性，实现继承和多态。另一个需要注意的运行期的特征，是对象的逐级初始化过程。 面向对象涉及了这么多精彩的知识点，拿它作为前端技术原理篇的最后一讲，是正确的选择。到目前为止，我们已经讲完了前端技术的原理篇，也如约拥有了一门具备丰富特性的脚本语言，甚至还支持面向对象编程、闭包、函数式编程这些很高级的特性。一般的应用项目所需要的语言特性，很难超过这个范围了。接下来的两节，我们就通过两个具体的应用案例，来检验一下学到的编译原理前端技术，看看它的威力！ 本节课的示例代码我放在了文末，供你参考。playscript-java（项目目录）： 码云 GitHub ASTEvaluator.java（解释器，请找一下运行期方法和属性动态绑定，以及对象实例逐级初始化的代码）： 码云 GitHub ThisSuperTest.java（测试 Java 的 this 和 super 特性）：码云 GitHub this-and-super.play (playscript 的 this 和 super 特性)：码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:15:6","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现一门脚本语言 应用 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:16:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"14 | 前端技术应用（一）：如何透明地支持数据库分库分表？ 从今天开始，我们正式进入了应用篇，我会用两节课的时间，带你应用编译器的前端技术。这样，你会把学到的编译技术和应用领域更好地结合起来，学以致用，让技术发挥应有的价值。还能通过实践加深对原理的理解，形成一个良好的循环。这节课，我们主要讨论，一个分布式数据库领域的需求。我会带你设计一个中间层，让应用逻辑不必关心数据库的物理分布。这样，无论把数据库拆成多少个分库，编程时都会像面对一个物理库似的没什么区别。接下来，我们先了解一下分布式数据库的需求和带来的挑战。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:17:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"分布式数据库解决了什么问题，又带来了哪些挑战 随着技术的进步，我们编写的应用所采集、处理的数据越来越多，处理的访问请求也越来越多。而单一数据库服务器的处理能力是有限的，当数据量和访问量超过一定级别以后，就要开始做分库分表的操作。比如，把一个数据库的大表拆成几张表，把一个库拆成几个库，把读和写的操作分离开等等。我们把这类技术统称为分布式数据库技术。 分库分表（Sharding）有时也翻译成“数据库分片”。分片可以依据各种不同的策略，比如我开发过一个与社区有关的应用系统，这个系统的很多业务逻辑都是围绕小区展开的。对于这样的系统，按照地理分布的维度来分片就很合适，因为每次对数据库的操作基本上只会涉及其中一个分库。假设我们有一个订单表，那么就可以依据一定的规则对订单或客户进行编号，编号中就包含地理编码。比如 SDYT 代表山东烟台，BJHD 代表北京海淀，不同区域的数据放在不同的分库中： 通过数据库分片，我们可以提高数据库服务的性能和可伸缩性。当数据量和访问量增加时，增加数据库节点的数量就行了。不过，虽然数据库的分片带来了性能和伸缩性的好处，但它也带来了一些挑战。 最明显的一个挑战，是数据库分片逻辑侵入到业务逻辑中。过去，应用逻辑只访问一个数据库，现在需要根据分片的规则，判断要去访问哪个数据库，再去跟这个数据库服务器连接。如果增加数据库分片，或者对分片策略进行调整，访问数据库的所有应用模块都要修改。这会让软件的维护变得更复杂，显然也不太符合软件工程中模块低耦合、把变化隔离的理念。 所以如果有一种技术，能让我们访问很多数据库分片时，像访问一个数据库那样就好了。数据库的物理分布，对应用是透明的。可是，“理想很吸引人，现实很骨感”。要实现这个技术，需要解决很多问题： 首先是跨库查询的难题。如果 SQL 操作都针对一个库还好，但如果某个业务需求恰好要跨多个库，比如上面的例子中，如果要查询多个小区的住户信息，那么就要在多个库中都执行查询，然后把查询结果合并，一般还要排序。如果我们前端显示的时候需要分页，每页显示一百行，那就更麻烦了。我们不可能从 10 个分库中各查出 10 行，合并成 100 行，这 100 行不一定排在前面，最差的情况，可能这 100 行恰好都在其中一个分库里。所以，你可能要从每个分库查出 100 行来，合并、排序后，再取出前 100 行。如果涉及数据库表跨库做连接，你想象一下，那就更麻烦了。 其次就是跨库做写入的难题。如果对数据库写入时遇到了跨库的情况，那么就必须实现分布式事务。所以，虽然分布式数据库的愿景很吸引人，但我们必须解决一系列技术问题。 这一讲，我们先解决最简单的问题，也就是当每次数据操作仅针对一个分库的时候，能否自动确定是哪个分库的问题。解决这个问题我们不需要依据别的信息，只需要提供 SQL 就行了。这就涉及对 SQL 语句的解析了，自然要用到编译技术。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:17:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"解析 SQL 语句，判断访问哪个数据库 我画了一张简化版的示意图：假设有两张表，分别是订单表和客户表，它们的主键是 order_id 和 cust_id： 我们采用的分片策略，是依据这两个主键的前 4 位的编码来确定数据库分片的逻辑，比如：前四位是 SDYT，那就使用山东烟台的分片，如果是 BJHD，就使用北京海淀的分片，等等。在我们的应用中，会对订单表进行一些增删改查的操作，比如会执行下面的 SQL 语句： //查询 select * from orders where order_id = 'SDYT20190805XXXX' select * from orders where cust_id = 'SDYT987645' //插入 insert into orders (order_id，...其他字段) values( \"BJHD20190805XXXX\",...) //修改 update orders set price=298.00 where order_id='FJXM20190805XXXX' //删除 delete from orders where order_id='SZLG20190805XXXX' 我们要能够解析这样的 SQL 语句，根据主键字段的值，决定去访问哪个分库或者分表。这就需要用到编译器前端技术，包括词法分析、语法分析和语义分析。 听到这儿，你可能会质疑：“解析 SQL 语句？是在开玩笑吗？”你可能觉得这个任务太棘手，犹豫着是否要忍受业务逻辑和技术逻辑混杂的缺陷，把判断分片的逻辑写到应用代码里，或者想解决这个问题，又或者想自己写一个开源项目，帮到更多的人。无论你的内心活动如何，应用编译技术，能让你有更强的信心解决这个问题。那么如何去做呢？要想完成解析 SQL 的任务，在词法分析和语法分析这两个阶段，我建议你采用工具快速落地，比如 Antlr。你要找一个现成的 SQL 语句的语法规则文件。 GitHub 中，那个收集了很多示例 Antlr 规则文件的项目里，有两个可以参考的规则：一个是PLSQL的（它是 Oracle 数据库的 SQL 语法）；一个是SQLite的（这是一个嵌入式数据库）。 实际上，我还找到 MySQL workbench 所使用的一个产品级的规则文件。MySQL workbench 是一个图形化工具，用于管理和访问 MySQL。这个规则文件还是很靠谱的，不过它里面嵌了很多属性计算规则，而且是 C++ 语言写的，我嫌处理起来麻烦，就先弃之不用，暂且采用 SQLite 的规则文件来做示范。 先来看一下这个文件里的一些规则，例如 select 语句相关的语法： factored_select_stmt : ( K_WITH K_RECURSIVE? common_table_expression ( ',' common_table_expression )* )? select_core ( compound_operator select_core )* ( K_ORDER K_BY ordering_term ( ',' ordering_term )* )? ( K_LIMIT expr ( ( K_OFFSET | ',' ) expr )? )? ; common_table_expression : table_name ( '(' column_name ( ',' column_name )* ')' )? K_AS '(' select_stmt ')' ; select_core : K_SELECT ( K_DISTINCT | K_ALL )? result_column ( ',' result_column )* ( K_FROM ( table_or_subquery ( ',' table_or_subquery )* | join_clause ) )? ( K_WHERE expr )? ( K_GROUP K_BY expr ( ',' expr )* ( K_HAVING expr )? )? | K_VALUES '(' expr ( ',' expr )* ')' ( ',' '(' expr ( ',' expr )* ')' )* ; result_column : '*' | table_name '.' '*' | expr ( K_AS? column_alias )? ; 我们可以一边看这个语法规则，一边想几个 select 语句做一做验证。你可以思考一下，这个规则是怎么把 select 语句拆成不同的部分的。SQL 里面也有表达式，我们研究一下它的表达式的规则： expr : literal_value | BIND_PARAMETER | ( ( database_name '.' )? table_name '.' )? column_name | unary_operator expr | expr '||' expr | expr ( '*' | '/' | '%' ) expr | expr ( '+' | '-' ) expr | expr ( '\u003c\u003c' | '\u003e\u003e' | '\u0026' | '|' ) expr | expr ( '\u003c' | '\u003c=' | '\u003e' | '\u003e=' ) expr | expr ( '=' | '==' | '!=' | '\u003c\u003e' | K_IS | K_IS K_NOT | K_IN | K_LIKE | K_GLOB | K_MATCH | K_REGEXP ) expr | expr K_AND expr | expr K_OR expr | function_name '(' ( K_DISTINCT? expr ( ',' expr )* | '*' )? ')' | '(' expr ')' | K_CAST '(' expr K_AS type_name ')' | expr K_COLLATE collation_name | expr K_NOT? ( K_LIKE | K_GLOB | K_REGEXP | K_MATCH ) expr ( K_ESCAPE expr )? | expr ( K_ISNULL | K_NOTNULL | K_NOT K_NULL ) | expr K_IS K_NOT? expr | expr K_NOT? K_BETWEEN expr K_AND expr | expr K_NOT? K_IN ( '(' ( select_stmt | expr ( ',' expr )* )? ')' | ( database_name '.' )? table_name ) | ( ( K_NOT )? K_EXISTS )? '(' select_stmt ')' | K_CASE expr? ( K_WHEN expr K_THEN expr )+ ( K_ELSE expr )? K_END | raise_function ; 你可能会觉得 SQL 的表达式的规则跟其他语言的表达式规则很像。比如都支持加减乘除、关系比较、逻辑运算等等。而且从这个规则文件里，你一下子就能看出各种运算的优先级，比如你会注意到，字符串连接操作“||”比乘法和除法的优先级更高。所以，研究一门语言时积累的经验，在研究下一门语言时仍然有用。有了规则文件之后，接下来，我们用 Antlr 生成词法分析器和语法分析器： antlr -visitor -package dsql.parser SQLite.g4 在这个命令里，我用 -package 参数指定了生成的 Java 代码的包是 dsql.parser。dsql 是分布式 SQL 的意思。接着，我们可以写一点儿程序测试一下所生成的词法分析器和语法分析器： String sql = \"select order_id from orders where cust_id = 'SDYT987645'\"; //词法分析 SQLiteLexer lexer = new SQLiteLexer(CharStreams.fromString(sql)); CommonTokenStream tokens = new CommonTokenStream(lexer); //语法分析 SQLiteParser parser = new SQLiteParser(tokens); ParseTree tree = parser.sql_stmt(); //输出lisp格式的AST System.out.println(tree.toStringTree(parser)); 这段程序的输出是 LISP 格式的 AST，我调整了一下缩进，让它显得更像一棵树： (sql_stmt (factored_select_stmt (select_core select (result_column (expr (column_name (any_name order_id)))) from (table_or_subquery (table_name (any_name orders))) where (expr (expr (column_name (any_name cust_id))) = (expr (literal_value ('SDYT987645')))))) 从 AST 中，我们可以清晰地看出这个 select 语句是如何被解析成结构化数据的，再继续写","date":"2022-07-18 23:55:09","objectID":"/cp_base/:17:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"我们的示例离实用还有多大差距？ 目前，我们已经初步解决了数据库访问透明化的问题。当然，这只是一个示例，如果要做得严密、实用，我们还要补充一些工作。 我们需要做一些语义分析工作，确保 SQL 语句的合法性。语法分析并不能保证程序代码完全合法，我们必须进行很多语义的检查才行。我给订单表起的名字，是 orders。如果你把表名称改为 order，那么必须用引号引起来，写成’order’，不带引号的 order 会被认为是一个关键字。因为在 SQL 中我们可以使用 order by 这样的子句，这时候，order 这个表名就会被混淆，进而被解析错误。这个语法解析程序会在表名的地方出现一个 order 节点，这在语义上是不合法的，需要被检查出来并报错。 如果要检查语义的正确性，我们还必须了解数据库的元数据。否则，就没有办法判断在SQL 语句中是否使用了正确的字段，以及正确的数据类型。除此之外，我们还需要扩展到能够识别跨库操作，比如下面这样一个 where 条件： order_id = 'FJXM20190805XXXX' or order_id = 'SZLG20190805XXXX' 分析这个查询条件，可以知道数据是存在两个不同的数据库中的。但是我们要让解析程序分析出这个结果，甚至让它针对更加复杂的条件，也能分析出来。这就需要更加深入的语义分析功能了。 最后，解析器的速度也是一个需要考虑的因素。因为执行每个 SQL 都需要做一次解析，而这个时间就加在了每一次数据库访问上。所以，SQL 解析的时间越少越好。因此，有的项目就会尽量提升解析效率。阿里有一个开源项目 Druid，是一个数据库连接池。这个项目强调性能，因此他们纯手写了一个 SQL 解析器，尽可能地提升性能。总之，要实现一个完善的工具，让工具达到产品级的质量，有不少工作要做。如果要支持更强的分布式数据库功能，还要做更多的工作。不过，你应该不会觉得这事儿有多么难办了吧？至少在编译技术这部分你是有信心的。 在这里，我还想讲一讲 SQL 防注入这个问题。SQL 注入攻击是一种常见的攻击手段。你向服务器请求一个 url 的时候，可以把恶意的 SQL 嵌入到参数里面，这样形成的 SQL 就是不安全的。以前面的 SQL 语句为例，这个 select 语句本来只是查询一个订单，订单编号“SDYT20190805XXXX”作为参数传递给服务端的一个接口，服务端收到参数以后，用单引号把这个参数引起来，并加上其他部分，就组装成下面的 SQL 并执行： //原来的SQL select * from orders where order_id = 'SDYT20190805XXXX' 如果我们遇到了一个恶意攻击者，他可能把参数写成“SDYT20190805XXXX’；drop table customers; –”。服务器接到这个参数以后，仍然把它拿单引号引起来，并组装成 SQL，组装完毕以后就是下面的语句： //被注入恶意SQL后 select * from orders where order_id = 'SDYT20190805XXXX'; drop table customers; --' 如果你看不清楚，我分行写一下，这样你就知道它是怎么把你宝贵的客户信息全都删掉的： //被注入恶意SQL后 select * from orders where order_id = 'SDYT20190805XXXX'; drop table customers; // 把顾客表给删了 --' //把你加的单引号变成了注释，这样SQL不会出错 所以 SQL 注入有很大的危害。而我们一般用检查客户端传过来的参数的方法，看看有没有 SQL 语句中的关键字，来防止 SQL 注入。不过这是比较浅的防御，有时还会漏过一些非法参数，所以要在 SQL 执行之前，做最后一遍检查。而这个时候，就要运用编译器前端技术来做 SQL 的解析了。借此，我们能检查出来异常：**明明这个功能是做查询的，为什么形成的 SQL 会有删除表的操作？**通过这个例子，我们又分析了一种场景：开发一个安全可靠的系统，用编译技术做 SQL 分析是必须做的一件事情。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:17:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 今天，我带你利用学到的编译器前端技术，解析了 SQL 语句，并针对分布式数据库透明查询的功能做了一次概念证明。 SQL 是程序员经常打交道的语言。有时，我们会遇到需要解析 SQL 语言的需求，除了分布式数据库场景的需求以外，Hibernate 对 HQL 的解析，也跟解析 SQL 差不多。而且，最近有一种技术，能够通过 RESTful 这样的接口做通用的查询，其实也是一种类 SQL 的子语言。当然了，今天我们只是基于工具做解析。一方面，有时候我们就是需要做个原型系统或者最小的能用的系统，有时间有资源了，再追求完美也不为过，比如追求编译速度的提升。另一方面，你能看到 MySQL workbench 也是用 Antlr 来作帮手的，在很多情况下，Antlr 这样的工具生成的解析器足够用，甚至比你手写的还要好，所以，我们大可以节省时间，用工具做解析。可能你会觉得，实际应用的难度似乎要低于学习原理的难度。如果你有这个感觉，那就对了，这说明你已经掌握了原理篇的内容，所以日常的一些应用根本不是问题，你可以找出更多的应用场景来练练手。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:17:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"15 | 前端技术应用（二）：如何设计一个报表工具？ 众所周知，很多软件都需要面向开发者甚至最终用户提供自定义功能，在开篇词里，我提到自己曾经做过工作流软件和电子表单软件，它们都需要提供自定义功能，报表软件也是其中的典型代表。在每个应用系统中，我们对数据的处理大致会分成两类：一类是在线交易，叫做 OLTP，比如在网上下订单；一类是在线分析，叫做 OLAP，它是对应用中积累的数据进行进一步分析利用。而报表工具就是最简单，但也是最常用的数据分析和利用的工具。本节课，我们就来分析一下，如果我们要做一个通用的报表工具，需要用到哪些编译技术，又该怎样去实现。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:18:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"报表工具所需要的编译技术 如果要做一个报表软件，我们要想清楚软件面对的用户是谁。有一类报表工具面向的用户是程序员，那么这种软件可以暴露更多技术细节。比如，如果报表要从数据库获取数据，你可以写一个 SQL 语句作为数据源。还有一类软件是给业务级的用户使用的，很多 BI 软件包都是这种类型。带有 IT 背景的顾问给用户做一些基础配置，然后用户就可以用这个软件包了。Excel 可以看做是这种报表工具，IT 人员建立 Excel 与数据库之间的连接，剩下的就是业务人员自己去操作了。这些业务人员可以采用一个图形化的界面设计报表，对数据进行加工处理。我们来看看几个场景。 第一个场景是计算字段。计算字段的意思是，原始数据里没有这个数据，我们需要基于原始数据，通过一个自定义的公式来把它计算出来。比如在某个 CRM 系统中保存着销售数据，我们有每个部门的总销售额，也有每个部门的人数，要想在报表中展示每个部门的人均销售额，这个时候就可以用到计算公式功能，计算公式如下： 人均销售额=部门销售额/部门人数 得到的结果如下图所示： 进一步，我们可以在计算字段中支持函数。比如我们可以把各个部门按照人均销售额排名次。这可以用一个函数来计算： =rank(人均销售额) rank 就是排名次的意思，其他统计函数还包括：min()，求最小值。max()，求最大值。avg()，求平均值。sum()，求和。 还有一些更有意思的函数，比如：runningsum()，累计汇总值。runningavg()，累计平均值。 这些有意思的函数是什么意思呢？因为很多明细性的报表，都是逐行显示的，累计汇总值和累计平均值，就是累计到当前行的计算结果。当然了，我们还可以支持更多的函数，比如当前日期、当前页数等等。更有意思的是，上述字段也好、函数也好，都可以用来组合成计算字段的公式，比如： =部门销售额/sum(部门销售额) //本部门的销售额在全部销售额的占比 =max(部门销售额)-部门销售额 //本部门的销售额与最高部门的差距 =max(部门销售额/部门人数)-部门销售额/部门人数 //本部门人均销售额与最高的那个部门的差 =sum(部门销售额)/sum(人数)-部门销售额/部门人数 //本部门的人均销售额与全公司人均销售额的差 怎么样，是不是越来越有意思了呢？现在你已经知道了在报表中会用到普通字段和各种各样的计算公式，那么，我们如何用这样的字段和公式来定义一张报表呢？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:18:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"如何设计报表 假设我们的报表是一行一行地展现数据，也就是最简单的那种。那我们将报表的定义做成一个 XML 文件，可能是下面这样的，它定义了表格中每一列的标题和所采用字段或公式： xml \u003cplayreport title=\"Report 1\"\u003e \u003csection\u003e \u003ccolumn\u003e \u003ctitle\u003e部门\u003c/title\u003e \u003cfield\u003edept\u003c/field\u003e \u003c/column\u003e \u003ccolumn\u003e \u003ctitle\u003e人数\u003c/title\u003e \u003cfield\u003enum_person\u003c/field\u003e \u003c/column\u003e \u003ccolumn\u003e \u003ctitle\u003e销售额\u003c/title\u003e \u003cfield\u003esales_amount\u003c/field\u003e \u003c/column\u003e \u003ccolumn\u003e \u003ctitle\u003e人均销售额\u003c/title\u003e \u003cfield\u003esales_amount/num_person\u003c/field\u003e \u003c/column\u003e \u003c/section\u003e \u003cdatasource\u003e \u003cconnection\u003e数据库连接信息...\u003c/connection\u003e \u003csql\u003eselect dept, num_person, sales_amount from sales\u003c/sql\u003e \u003c/datasource\u003e \u003c/playreport\u003e 这个报表定义文件还是蛮简单的，它主要表达的是数据逻辑，忽略了表现层的信息。如果我们想要优先表达表现层的信息，例如字体大小、界面布局等，可以采用 HTML 模板的方式来定义报表，其实就是在一个 HTML 中嵌入了公式，比如： html \u003chtml\u003e \u003cbody\u003e \u003cdiv class=\"report\" datasource=\"这里放入数据源信息\"\u003e \u003cdiv class=\"table_header\"\u003e \u003cdiv class=\"column_header\"\u003e部门\u003c/div\u003e \u003cdiv class=\"column_header\"\u003e人数\u003c/div\u003e \u003cdiv class=\"column_header\"\u003e销售额\u003c/div\u003e \u003cdiv class=\"column_header\"\u003e人均销售额\u003c/div\u003e \u003c/div\u003e \u003cdiv class=\"table_body\"\u003e \u003cdiv class=\"field\"\u003e{=dept}\u003c/div\u003e \u003cdiv class=\"field\"\u003e{=num_person}\u003c/div\u003e \u003cdiv class=\"field\"\u003e{=sales_amount}\u003c/div\u003e \u003cdiv class=\"field\"\u003e{=sales_amount/num_person}\u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e 这样的 HTML 模板看上去是不是很熟悉？其实在很多语言里，比如 PHP，都提供模板引擎功能，实现界面设计和应用代码的分离。这样一个模板，可以直接解释执行，或者先翻译成 PHP 或 Java 代码，然后再执行。只要运用我们学到的编译技术，这些都可以实现。我想你应该会发现，这样的一个模板文件，其实就是一个特定领域语言，也就是我们常说的 DSL。DSL 可以屏蔽掉实现细节，让我们专注于领域问题，像上面这样的 DSL，哪怕没有技术背景的工作人员，也可以迅速地编写出来。而这个简单的报表，在报表设计界面上可能是下图这样的形式： 分析完如何设计报表之后，接下来，我们看看如何定义报表所需要的公式规则。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:18:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"编写所需要的语法规则 我们设计了 PlayReport.g4 规则文件，这里面的很多规则，是把 PlayScript.g4 里的规则拿过来改一改用的： bracedExpression : '{' '=' expression '}' ; expression : primary | functionCall | expression bop=('*'|'/'|'%') expression | expression bop=('+'|'-') expression | expression bop=('\u003c=' | '\u003e=' | '\u003e' | '\u003c') expression | expression bop=('==' | '!=') expression | expression bop='\u0026\u0026' expression | expression bop='||' expression ; primary : '(' expression ')' | literal | IDENTIFIER ; expressionList : expression (',' expression)* ; functionCall : IDENTIFIER '(' expressionList? ')' ; literal : integerLiteral | floatLiteral | CHAR_LITERAL | STRING_LITERAL | BOOL_LITERAL | NULL_LITERAL ; integerLiteral : DECIMAL_LITERAL | HEX_LITERAL | OCT_LITERAL | BINARY_LITERAL ; floatLiteral : FLOAT_LITERAL | HEX_FLOAT_LITERAL ; 这里面，其实就是用了表达式的语法，包括支持加减乘除等各种运算，用来书写公式。我们还特意支持 functionCall 功能，也就是能够调用函数。因为我们内部实现了很多内置函数，比如求最大值、平均值等，可以在公式里调用这些函数。现在呢，我们已经做好了一个最简单的报表定义，接下来，就一起实现一个简单的报表引擎，这样就能实际生成报表了！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:18:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现一个简单的报表引擎 报表引擎的工作，是要根据报表的定义和数据源中的数据，生成最后报表的呈现格式。具体来说，可以分为以下几步： 解析报表的定义。我们首先要把报表定义形成 Java 对象。这里只是简单地生成了一个测试用的报表模板。从数据源获取数据。我们设计了一个 TabularData 类，用来保存类似数据库表那样的数据。实现一个 FieldEvaluator 类，能够在运行时对字段和公式进行计算。这个类是 playscript 中 ASTEvaluator 类的简化版。我们甚至连语义分析都简化了。数据类型信息作为 S 属性，在求值的同时自底向上地进行类型推导。当然，如果做的完善一点儿，我们还需要多做一点儿语义分析，比如公式里的字段是不是数据源中能够提供的？而这时需要用到报表数据的元数据。渲染报表。我们要把上面几个功能组合在一起，对每一行、每一列求值，获得最后的报表输出。 主控程序我放在了下面，用一个示例报表模板和报表数据来生成报表： java public static void main(String args[]) { System.out.println(\"Play Report!\"); PlayReport report = new PlayReport(); //打印报表1 String reportString = report.renderReport(ReportTemplate.sampleReport1(), TabularData.sampleData()); System.out.println(reportString); } renderReport 方法用来渲染报表，它会调用解析器和报表数据的计算器： java public String renderReport(ReportTemplate template, TabularData data){ StringBuffer sb = new StringBuffer(); //输出表格头 for (String columnHeader: template.columnHeaders){ sb.append(columnHeader).append('\\t'); } sb.append(\"\\n\"); //编译报表的每个字段 List\u003cBracedExpressionContext\u003e fieldASTs = new LinkedList\u003cBracedExpressionContext\u003e(); for (String fieldExpr : template.fields){ //这里会调用解析器 BracedExpressionContext tree = parse(fieldExpr); fieldASTs.add(tree); } //计算报表字段 FieldEvaluator evaluator = new FieldEvaluator(data); List\u003cString\u003e fieldNames = new LinkedList\u003cString\u003e(); for (BracedExpressionContext fieldAST: fieldASTs){ String fieldName = fieldAST.expression().getText(); fieldNames.add(fieldName); if (!data.hasField(fieldName)){ Object field = evaluator.visit(fieldAST); data.setField(fieldName, field); } } //显示每一行数据 for (int row = 0; row\u003c data.getNumRows(); row++){ for (String fieldName: fieldNames){ Object value = data.getFieldValue(fieldName, row); sb.append(value).append(\"\\t\"); } sb.append(\"\\n\"); } return sb.toString(); } 程序的运行结果如下，它首先打印输出了每个公式的解析结果，然后输出报表： Play Report! (bracedExpression { = (expression (primary dept)) }) (bracedExpression { = (expression (primary num_person)) }) (bracedExpression { = (expression (primary sales_amount)) }) (bracedExpression { = (expression (expression (primary sales_amount)) / (expression (primary num_person))) }) 部门 人数 销售额 人均销售额 电话销售部 10 2345.0 234.5 现场销售部 20 5860.0 293.0 电子商务部 15 3045.0 203.0 渠道销售部 20 5500.0 275.0 微商销售部 12 3624.0 302.0 你可以看到，报表工具准确地得出了计算字段的数据。接下来，我再讲一讲报表数据计算的细节。如果你看一看 FieldEvaluator.java 这个类，就会发现我实际上实现了一个简单的向量数据的计算器。在计算机科学里，向量是数据的有序列表，可以看做一个数组。相对应的，标量只是一个单独的数据。运用向量计算，我们在计算人均销售额的时候，会把“销售额”和“人数”作为两个向量，每个向量都有 5 个数据。把这两个向量相除，会得到第三个向量，就是“人均销售额”。这样就不需要为每行数据运行一次计算器，会提高性能，也会简化程序。其实，这个向量计算器还能够把向量和标量做混合运算。因为我们的报表里有时候确实会用到标量，比如对销售额求最大值{=max(sales_amount)}，就是一个标量。而如果计算销售额与最大销售额的差距{=max(sales_amount)-sales_amount}，就是标量和向量的混合运算，返回结果是一个向量。TabularData.java 这个类是用来做报表数据的存储的。我简单地用了一个 Map，把字段的名称对应到一个向量或标量上，其中字段的名称可以是公式： 在报表数据计算过程中，我们还做了一个优化。公式计算的中间结果会被存起来，如果下一个公式刚好用到这个数据，可以复用。比如，在计算 rank(sales_amount/num_person) 这个公式的时候，它会查一下括号中的 sales_amount/num_person 这个子公式的值是不是以前已经计算过，如果计算过，就复用，否则，就计算一下，并且把这个中间结果也存起来。我们把这个报表再复杂化一点，形成下面一个报表模板。这个报表模板用到了好几个函数，包括排序、汇总值、累计汇总值和最大值，并通过公式定义出一些相对复杂的计算字段，包括最高销售额、销售额的差距、销售额排序、人均销售额排序、销售额累计汇总、部门销售额在总销售额中的占比，等等。 java public static ReportTemplate sampleReport2(){ ReportTemplate template = new ReportTemplate(); template.columnHeaders.add(\"部门\"); template.columnHeaders.add(\"人数\"); template.columnHeaders.add(\"销售额\"); template.columnHeaders.add(\"最高额\"); template.columnHeaders.add(\"差距\"); template.columnHeaders.add(\"排序\"); template.columnHeaders.add(\"人均\"); template.columnHeaders.add(\"人均排序\"); template.columnHeaders.add(\"累计汇总\"); template.columnHeaders.add(\"占比%\"); template.fields.add(\"{=dept}\"); template.fields.add(\"{=num_person}\"); template.fields.add(\"{=sales_amount}\"); template.fields.add(\"{=max(sales_amount)}\"); template.fields.add(\"{=max(sales_amount)-sales_amount}\"); template.fields.add(\"{=rank(sales_amount)}\"); template.fields.add(\"{=sales_amount/num_person}\"); template.fields.add(\"{=rank(sales_amount/num_person)}\"); template.fields.add(\"{=runningsum(sales_amount)}\"); template.fields.add(\"{=sales_amount/sum(sales_amount)*100}\"); return template; } 最后输出的报表截屏如下，怎么样，现在看起来功能还","date":"2022-07-18 23:55:09","objectID":"/cp_base/:18:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课我们做了一个示例性的报表工具。你能在这个过程中看到，像报表工具这样的软件，如果有编译技术的支持，真的可以做得很灵活、很强大。你完全可以借鉴本节课的思路，去尝试做一下其他需要自定义功能的软件工具或产品。与此同时，我们能看到编译技术可以跟某个应用领域结合在一起，内置在产品中，同时形成领域的 DSL，比如报表的模板文件。这样，我们就相当于赋予了普通用户在某个领域内的编程能力，比如用户只需要编写一个报表模板，就可以生成报表了。了解这些内容之后，我来带你回顾一下，这个应用是怎么运用编译器前端技术的。词法分析和语法分析都很简单，我们就是简单地用了表达式和函数调用的功能。而语义分析除了需要检查类型以外，还要检查所用到的字段和函数是否合法，这是另一种意义上的引用消解。而且这个例子中的运算的含义是向量运算，同样是加减乘除，每个操作都会处理一组数据，这也是一种语义上的区别。我希望在学习了这两节课之后，你能对如何在某个应用领域应用编译技术有更直观的了解，甚至有了很多的启发。 本节课的示例代码我放在文末，供你参考。lab/report（报表项目示例代码） 码云 GitHub PlayReport.java（主程序入口） 码云 GitHub FieldEvaluator.java（做报表计算的代码） 码云 GitHub ReportTemplate.java（报表模板） 码云 GitHub TabularData.java（报表数据） 码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:18:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现一门脚本语言 算法 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:19:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"16 | NFA和DFA：如何自己实现一个正则表达式工具？ 回顾之前讲的内容，原理篇重在建立直观理解，帮你建立信心，这是第一轮的认知迭代。应用篇帮你涉足应用领域，在解决领域问题时发挥编译技术的威力，积累运用编译技术的一手经验，也启发你用编译技术去解决更多的领域问题，这是第二轮的认知迭代。而为时三节课的算法篇将你是第三轮的认知迭代。在第三轮的认知迭代中，我会带你掌握前端技术中的核心算法。而本节课，我就借“怎样实现正则表达式工具？”这个问题，探讨第一组算法：与正则表达式处理有关的算法。 在词法分析阶段，我们可以手工构造有限自动机（FSA，或 FSM）实现词法解析，过程比较简单。现在我们不再手工构造词法分析器，而是直接用正则表达式解析词法。你会发现，我们只要写一些规则，就能基于这些规则分析和处理文本。这种能够理解正则表达式的功能，除了能生成词法分析器，还有很多用途。比如 Linux 的三个超级命令，又称三剑客（grep、awk 和 sed），都是因为能够直接支持正则表达式，功能才变得强大的。接下来，我就带你完成编写正则表达式工具的任务，与此同时，你就能用正则文法生成词法分析器了： 首先，把正则表达式翻译成非确定的有限自动机（Nondeterministic Finite Automaton，NFA）。其次，基于 NFA 处理字符串，看看它有什么特点。然后，把非确定的有限自动机转换成确定的有限自动机（Deterministic Finite Automaton，DFA）最后，运行 DFA，看看它有什么特点。 强调一下，不要被非确定的有限自动机、确定的有限自动机这些概念吓倒，我肯定让你学明白。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:20:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"认识 DFA 和 NFA 在讲词法分析时，我提到有限自动机（FSA）有有限个状态。识别 Token 的过程，就是 FSA 状态迁移的过程。其中，FSA 分为确定的有限自动机（DFA）和非确定的有限自动机（NFA）。DFA 的特点是，在任何一个状态，我们基于输入的字符串，都能做一个确定的转换，比如： NFA 的特点是，它存在某些状态，针对某些输入，不能做一个确定的转换，这又细分成两种情况：对于一个输入，它有两个状态可以转换。存在ε转换。也就是没有任何输入的情况下，也可以从一个状态迁移到另一个状态。 比如，“a[a-zA-Z0-9]*bc”这个正则表达式对字符串的要求是以 a 开头，以 bc 结尾，a 和 bc 之间可以有任意多个字母或数字。在图中状态 1 的节点输入 b 时，这个状态是有两条路径可以选择的，所以这个有限自动机是一个 NFA。 这个 NFA 还有引入ε转换的画法，它们是等价的。实际上，第二个 NFA 可以用我们今天讲的算法，通过正则表达式自动生成出来。 需要注意的是，无论是 NFA 还是 DFA，都等价于正则表达式。也就是，所有的正则表达式都能转换成 NFA 或 DFA，所有的 NFA 或 DFA，也都能转换成正则表达式。理解了 NFA 和 DFA 之后，来看看我们如何从正则表达式生成 NFA。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:20:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"从正则表达式生成 NFA 我们需要把它分为两个子任务：第一个子任务，是把正则表达式解析成一个内部的数据结构，便于后续的程序使用。因为正则表达式也是个字符串，所以要先做一个小的编译器，去理解代表正则表达式的字符串。我们可以偷个懒，直接针对示例的正则表达式生成相应的数据结构，不需要做出这个编译器。用来测试的正则表达式可以是 int 关键字、标识符，或者数字字面量： int | [a-zA-Z][a-zA-Z0-9]* | [0-9]+ 我用下面这段代码创建了一个树状的数据结构，来代表用来测试的正则表达式： java private static GrammarNode sampleGrammar1() { GrammarNode node = new GrammarNode(\"regex1\",GrammarNodeType.Or); //int关键字 GrammarNode intNode = node.createChild(GrammarNodeType.And); intNode.createChild(new CharSet('i')); intNode.createChild(new CharSet('n')); intNode.createChild(new CharSet('t')); //标识符 GrammarNode idNode = node.createChild(GrammarNodeType.And); GrammarNode firstLetter = idNode.createChild(CharSet.letter); GrammarNode letterOrDigit = idNode.createChild(CharSet.letterOrDigit); letterOrDigit.setRepeatTimes(0, -1); //数字字面量 GrammarNode literalNode = node.createChild(CharSet.digit); literalNode.setRepeatTimes(1, -1); return node; } 打印输出的结果如下： RegExpression Or Union i n t Union [a-z]|[A-Z] [0-9]|[a-z]|[A-Z]* [0-9]+ 画成图会更直观一些： 测试数据生成之后，第二个子任务就是把表示正则表达式的数据结构，转换成一个 NFA。这个过程比较简单，因为针对正则表达式中的每一个结构，我们都可以按照一个固定的规则做转换。识别ε的 NFA：不接受任何输入，也能从一个状态迁移到另一个状态，状态图的边上标注ε。 识别 i 的 NFA：当接受字符 i 的时候，引发一个转换，状态图的边上标注 i。 转换“s|t”这样的正则表达式：它的意思是或者 s，或者 t，二者选一。s 和 t 本身是两个子表达式，我们可以增加两个新的状态：**开始状态和接受状态（最终状态）**也就是图中带双线的状态，它意味着被检验的字符串此时是符合正则表达式的。然后用ε转换分别连接代表 s 和 t 的子图。它的含义也比较直观，要么走上面这条路径，那就是 s，要么走下面这条路径，那就是 t。 转换“st”这样的正则表达式：s 之后接着出现 t，转换规则是把 s 的开始状态变成 st 整体的开始状态，把 t 的结束状态变成 st 整体的结束状态，并且把 s 的结束状态和 t 的开始状态合二为一。这样就把两个子图接了起来，走完 s 接着走 t。 对于“?”“”和“+”这样的操作：意思是可以重复 0 次、0 到多次、1 到多次，转换时要增加额外的状态和边。以“s”为例，做下面的转换： 你能看出，它可以从 i 直接到 f，也就是对 s 匹配零次，也可以在 s 的起止节点上循环多次。“s+”：没有办法跳过 s，s 至少经过一次。 按照这些规则，我们可以编写程序进行转换。你可以参考示例代码Regex.java中的 regexToNFA 方法。转换完毕以后，将生成的 NFA 打印输出，列出了所有的状态，以及每个状态到其他状态的转换，比如“0 ε -\u003e 2”的意思是从状态 0 通过ε转换，到达状态 2 ： NFA states: 0 ε -\u003e 2 ε -\u003e 8 ε -\u003e 14 2 i -\u003e 3 3 n -\u003e 5 5 t -\u003e 7 7 ε -\u003e 1 1 (end) acceptable 8 [a-z]|[A-Z] -\u003e 9 9 ε -\u003e 10 ε -\u003e 13 10 [0-9]|[a-z]|[A-Z] -\u003e 11 11 ε -\u003e 10 ε -\u003e 13 13 ε -\u003e 1 14 [0-9] -\u003e 15 15 ε -\u003e 14 ε -\u003e 1 我用图片直观地展示了输出结果，图中分为上中下三条路径，你能清晰地看出解析 int 关键字、标识符和数字字面量的过程： 生成 NFA 之后，如何利用它识别某个字符串是否符合这个 NFA 代表的正则表达式呢？以上图为例，当我们解析 intA 这个字符串时，首先选择最上面的路径去匹配，匹配完 int 这三个字符以后，来到状态 7，若后面没有其他字符，就可以到达接受状态 1，返回匹配成功的信息。可实际上，int 后面是有 A 的，所以第一条路径匹配失败。失败之后不能直接返回“匹配失败”的结果，因为还有其他路径，所以我们要回溯到状态 0，去尝试第二条路径，在第二条路径中，尝试成功了。运行 Regex.java 中的 matchWithNFA() 方法，你可以用 NFA 来做正则表达式的匹配： java /** * 用NFA来匹配字符串 * @param state 当前所在的状态 * @param chars 要匹配的字符串，用数组表示 * @param index1 当前匹配字符开始的位置。 * @return 匹配后，新index的位置。指向匹配成功的字符的下一个字符。 */ private static int matchWithNFA(State state, char[] chars, int index1){ System.out.println(\"trying state : \" + state.name + \", index =\" + index1); int index2 = index1; for (Transition transition : state.transitions()){ State nextState = state.getState(transition); //epsilon转换 if (transition.isEpsilon()){ index2 = matchWithNFA(nextState, chars, index1); if (index2 == chars.length){ break; } } //消化掉一个字符，指针前移 else if (transition.match(chars[index1])){ index2 ++; //消耗掉一个字符 if (index2 \u003c chars.length) { index2 = matchWithNFA(nextState, chars, index1 + 1); } //如果已经扫描完所有字符 //检查当前状态是否是接受状态，或者可以通过epsilon到达接受状态 //如果状态机还没有到达接受状态，本次匹配失败 else { if (acceptable(nextState)) { break; } else{ index2 = -1; } } } } return index2; } 其中，在匹配“intA”时，你会看到它的回溯过程： NFA matching: 'intA' trying state : 0, index =0 trying state : 2, index =0 //先走第一条路径，即int关键字这个路径 trying state : 3, index =1 trying state : 5, index =2 trying state : 7, index =3 trying state : 1, index =3 //到了末尾了，发现还有字符'A'没有匹配上 trying state : 8, index =0 //回溯，尝试第二条路径，即标识符 trying state : 9, index =1 trying state : 10, index =1 //在10和11这里循环多次 trying state : 11, index =2 trying state : 10, index =2 trying state : 11, index =3 trying state : 10, index =3 true 从中可以看到用 NFA 算法的特点：因为存在多条可能的路径，所以需要试探和回溯，在比较极端的情况下，回溯次数会非常多，性能会变得非常慢。特别是当处理类似 s* 这样的语句时，因为 s 可以重复 0 到无穷次，所以在匹配字符串时，可能需要尝试很多次。 注意，在我们生成的 NFA 中，如果一个状态有两条路径到其他状态，算法会依据一定的顺序来尝试不同的路径。9 和 11 两个状态都有两条向外走的线，其中红色的线是更优先的路径，也就是尝试让 * 号匹配尽量多的字符。这种算法策略叫做“贪婪（greedy）”策略。在有的情况下，我们会希望让算法采用非贪婪策略，或者叫“忽略优先”策略，以便让效率更高。有的正则表达式工具会支持多加一个?，","date":"2022-07-18 23:55:09","objectID":"/cp_base/:20:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"把 NFA 转换成 DFA 的确有这样的算法，那就是子集构造法，它的思路如下。首先 NFA 有一个初始状态（从状态 0 通过ε转换可以到达的所有状态，也就是说，在不接受任何输入的情况下，从状态 0 也可以到达的状态）。这个状态的集合叫做“状态 0 的ε闭包”，简单一点儿，我们称之为 s0，s0 包含 0、2、8、14 这几个状态。 将字母 i 给到 s0 中的每一个状态，看它们能转换成什么状态，再把这些状态通过ε转换就能到达的状态也加入进来，形成一个包含“3、9、10、13、1”5 个状态的集合 s1。其中 3 和 9 是接受了字母 i 所迁移到的状态，10、13、1 是在状态 9 的ε闭包中。 在 s0 和 s1 中间画条迁移线，标注上 i，意思是 s0 接收到 i 的情况下，转换到 s1： 在这里，我们把 s0 和 s1 分别看成一个状态。也就是说，要生成的 DFA，它的每个状态，是原来的 NFA 的某些状态的集合。在上面的推导过程中，我们有两个主要的计算： 1.ε-closure(s)，即集合 s 的ε闭包。也就是从集合 s 中的每个节点，加上从这个节点出发通过ε转换所能到达的所有状态。2.move(s, ‘i’)，即从集合 s 接收一个字符 i，所能到达的新状态的集合。 所以，s1 = ε-closure(move(s0,‘i’))按照上面的思路继续推导，识别 int 关键字的识别路径也就推导出来了： 我们把上面这种推导的思路写成算法，参见Regex.java中的 NFA2DFA() 方法。我写了一段伪代码，方便你阅读： 计算s0，即状态0的ε闭包 把s0压入待处理栈 把s0加入所有状态集的集合S 循环：待处理栈内还有未处理的状态集 循环：针对字母表中的每个字符c 循环：针对栈里的每个状态集合s(i)（未处理的状态集） 计算s(m) = move(s(i), c)（就是从s(i)出发，接收字符c能够 迁移到的新状态的集合） 计算s(m)的ε闭包，叫做s(j) 看看s(j)是不是个新的状态集，如果已经有这个状态集了，把它找出来 否则，把s(j)加入全集S和待处理栈 建立s(i)到s(j)的连线，转换条件是c 运行 NFA2DFA() 方法，然后打印输出生成的 DFA。画成图，你就能很直观地看出迁移的路径了： 从初始状态开始，如果输入是 i，那就走 int 识别这条线，也就是按照 19、21、22 这条线依次迁移，如果中间发现不符合 int 模式，就跳转到 20，也就是标识符状态。注意，在上面的 DFA 中，只要包含接受状态 1 的，都是 DFA 的接受状态。进一步区分的话，22 是 int 关键字的接受状态，因为它包含了 int 关键字原来的接受状态 7。同理，17 是数字字面量的接受状态，18、19、20、21 都是标识符的接受状态。而且，你会发现，算法生成的 DFA 跟手工构造 DFA 是很接近的！我们在第二讲手工构造了 DFA 识别 int 关键字和标识符，比本节课少识别一个数字字面量： 不过，光看对 int 关键字和标识符的识别，我们算法生成的 DFA 和手工构造的 DFA，非常相似！手工构造的相当于把 18 和 20 两个状态合并了，所以，这个算法是非常有效的！你可以运行一下示例程序 Regex.java 中的 matchWithDFA() 的方法，看看效果： java private static boolean matchWithDFA(DFAState state, char[] chars, int index){ System.out.println(\"trying DFAState : \" + state.name + \", index =\" + index); //根据字符，找到下一个状态 DFAState nextState = null; for (Transition transition : state.transitions()){ if (transition.match(chars[index])){ nextState = (DFAState)state.getState(transition); break; } } if (nextState != null){ //继续匹配字符串 if (index \u003c chars.length-1){ return matchWithDFA(nextState,chars, index + 1); } else{ //字符串已经匹配完毕 //看看是否到达了接受状态 if(state.isAcceptable()){ return true; } else{ return false; } } } else{ return false; } } 运行时会打印输出匹配过程，而执行过程中不产生任何回溯。现在，我们可以自动生成 DFA 了，可以根据 DFA 做更高效的计算。不过，有利就有弊，DFA 也存在一些缺点。比如，DFA 可能有很多个状态。假设原来 NFA 的状态有 n 个，那么把它们组合成不同的集合，可能的集合总数是 2 的 n 次方个。针对我们示例的 NFA，它有 13 个状态，所以最坏的情况下，形成的 DFA 可能有 2 的 13 次方，也就是 8192 个状态，会占据更多的内存空间。而且生成这个 DFA 本身也需要消耗一定的计算时间。当然了，这种最坏的状态很少发生，我们示例的 NFA 生成 DFA 后，只有 7 个状态。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:20:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我带你实现了一个正则表达式工具，或者说根据正则表达式自动做了词法分析，它们的主要原理是相同的。首先，我们需要解析正则表达式，形成计算机内部的数据结构，然后要把这个正则表达式生成 NFA。我们可以基于 NFA 进行字符串的匹配，或者把 NFA 转换成 DFA，再进行字符串匹配。NFA 和 DFA 有各自的优缺点：NFA 通常状态数量比较少，可以直接用来进行计算，但可能会涉及回溯，从而性能低下；DFA 的状态数量可能很大，占用更多的空间，并且生成 DFA 本身也需要消耗计算资源。所以，我们根据实际需求选择采用 NFA 还是 DFA 就可以了。不过，一般来说，正则表达式工具可以直接基于 NFA。而词法分析器（如 Lex），则是基于 DFA。原因很简单，因为在生成词法分析工具时，只需要计算一次 DFA，就可以基于这个 DFA 做很多次词法分析。 本节课的示例代码我放在了文末，供你参考。lab/16-18（算法篇的示例代码）：码云 GitHub Regex.java（正则表达式有关的算法）：码云 GitHub Lexer.java（基于正则文法自动做词法解析）：码云 GitHub GrammarNode.java（用于表达正则文法）：码云 GitHub State.java（自动机的状态）：码云 GitHub DFAState.java（DFA 的状态）：码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:20:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"17 | First和Follow集合：用LL算法推演一个实例 在前面的课程中，我讲了递归下降算法。这个算法很常用，但会有回溯的现象，在性能上会有损失。所以我们要把算法升级一下，实现带有预测能力的自顶向下分析算法，避免回溯。而要做到这一点，就需要对自顶向下算法有更全面的了解。另外，在留言区，有几个同学问到了一些问题，涉及到对一些基本知识点的理解，比如： 基于某个语法规则做解析的时候，什么情况下算是成功，什么情况下算是失败？使用深度优先的递归下降算法时，会跟广度优先的思路搞混。 要搞清这些问题，也需要全面了解自顶向下算法。比如，了解 Follow 集合和 $ 符号的用法，能帮你解决第一个问题；了解广度优先算法能帮你解决第二个问题。所以，本节课，我先把自顶向下分析的算法体系梳理一下，让你先建立更加清晰的全景图，然后我再深入剖析 LL 算法的原理，讲清楚 First 集合与 Follow 集合这对核心概念，最终让你把自顶向下的算法体系吃透。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:21:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"自顶向下分析算法概述 自顶向下分析的算法是一大类算法。总体来说，它是从一个非终结符出发，逐步推导出跟被解析的程序相同的 Token 串。这个过程可以看做是一张图的搜索过程，这张图非常大，因为针对每一次推导，都可能产生一个新节点。下面这张图只是它的一个小角落。 算法的任务，就是在大图中，找到一条路径，能产生某个句子（Token 串）。比如，我们找到了三条橘色的路径，都能产生“2+3*5”这个表达式。根据搜索的策略，有深度优先（Depth First）和广度优先（Breadth First）两种，这两种策略的推导过程是不同的。 深度优先是沿着一条分支把所有可能性探索完。以“add-\u003emul+add”产生式为例，它会先把 mul 这个非终结符展开，比如替换成 pri，然后再把它的第一个非终结符 pri 展开。只有把这条分支都向下展开之后，才会回到上一级节点，去展开它的兄弟节点。递归下降算法就是深度优先的，这也是它不能处理左递归的原因，因为左边的分支永远也不能展开完毕。 而针对“add-\u003eadd+mul”这个产生式，广度优先会把 add 和 mul 这两个都先展开，这样就形成了四条搜索路径，分别是 mul+mul、add+mul+mul、add+pri 和 add+mul*pri。接着，把它们的每个非终结符再一次展开，会形成 18 条新的搜索路径。所以，广度优先遍历，需要探索的路径数量会迅速爆炸，成指数级上升。哪怕用下面这个最简单的语法，去匹配“2+3”表达式，都需要尝试 20 多次，更别提针对更复杂的表达式或者采用更加复杂的语法规则了。 //一个很简单的语法 add -\u003e pri //1 add -\u003e add + pri //2 pri -\u003e Int //3 pri -\u003e (add) //4 这样看来，指数级上升的内存消耗和计算量，使得广度优先根本没有实用价值。虽然上面的算法有优化空间，但无法从根本上降低算法复杂度。当然了，它也有可以使用左递归文法的优点，不过我们不会为了这个优点去忍受算法的性能。而深度优先算法在内存占用上是线性增长的。考虑到回溯的情况，在最坏的情况下，它的计算量也会指数式增长，但我们可以通过优化，让复杂度降为线性增长。了解广度优先算法，你的思路会得到拓展，对自顶向下算法的本质有更全面的理解。另外，在写算法时，你也不会一会儿用深度优先，一会儿用广度优先了。针对深度优先算法的优化方向是减少甚至避免回溯，思路就是给算法加上预测能力。比如，我在解析 statement 的时候，看到一个 if，就知道肯定这是一个条件语句，不用再去尝试其他产生式了。 LL 算法就属于这类预测性的算法。第一个 L，是 Left-to-right，代表从左向右处理程序代码。第二个 L，是 Leftmost，意思是最左推导。按照语法规则，一个非终结符展开后，会形成多个子节点，其中包含终结符和非终结符。最左推导是指，从左到右依次推导展开这些非终结符。采用 Leftmost 的方法，在推导过程中，句子的左边逐步都会被替换成终结符，只有右边的才可能包含非终结符。以“2+3*5”为例，它的推导顺序从左到右，非终结符逐步替换成了终结符： 下图是上述推导过程建立起来的 AST，“1、2、3……”等编号是 AST 节点创建的顺序： 好了，我们把自顶向下分析算法做了总体概述，并讲清楚了最左推导的含义，现在来看看 LL 算法到底是怎么回事。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:21:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"计算和使用 First 集合 LL 算法是带有预测能力的自顶向下算法。在推导的时候，我们希望当存在多个候选的产生式时，瞄一眼下一个（或多个）Token，就知道采用哪个产生式。如果只需要预看一个 Token，就是 LL(1) 算法。拿 statement 的语法举例子，它有好几个产生式，分别产生 if 语句、while 语句、switch 语句…… statement : block | IF parExpression statement (ELSE statement)? | FOR '(' forControl ')' statement | WHILE parExpression statement | DO statement WHILE parExpression ';' | SWITCH parExpression '{' switchBlockStatementGroup* switchLabel* | RETURN expression? ';' | BREAK IDENTIFIER? ';' | CONTINUE IDENTIFIER? ';' | SEMI | statementExpression=expression ';' | identifierLabel=IDENTIFIER ':' statement ; 如果我看到下一个 Token 是 if，那么后面跟着的肯定是 if 语句，这样就实现了预测，不需要一个一个产生式去试。问题来了，if 语句的产生式的第一个元素就是一个终结符，这自然很好判断，可如果是一个非终结符，比如表达式语句，那该怎么判断呢？我们可以为 statement 的每条分支计算一个集合，集合包含了这条分支所有可能的起始 Token。如果每条分支的起始 Token 是不一样的，也就是这些集合的交集是空集，那么就很容易根据这个集合来判断该选择哪个产生式。我们把这样的集合，就叫做这个产生式的 First 集合。 First 集合的计算很直观，假设我们要计算的产生式是 x：如果 x 以 Token 开头，那么 First(x) 包含的元素就是这个 Token，比如 if 语句的 First 集合就是{IF}。如果 x 的开头是非终结符 a，那么 First(x) 要包含 First(a) 的所有成员。比如 expressionStatment 是以 expression 开头，因此它的 First 集合要包含 First(expression) 的全体成员。如果 x 的第一个元素 a 能够产生ε，那么还要再往下看一个元素 b，把 First(b) 的成员也加入到 First(x)，以此类推。如果所有元素都可能返回ε，那么 First(x) 也应该包含ε，意思是 x 也可能产生ε。比如下面的 blockStatements 产生式，它的第一个元素是 blockStatement*，也就意味着 blockStatement 的数量可能为 0，因此可能产生ε。那么 First(blockStatements) 除了要包含 First(blockStatement) 的全部成员，还要包含后面的“；”。 blockStatements : blockStatement* ; 最后，如果 x 是一个非终结符，它有多个产生式可供选择，那么 First(x) 应包含所有产生式的 First() 集合的成员。比如 statement 的 First 集合要包含 if、while 等所有产生式的 First 集合的成员。并且，如果这些产生式只要有一个可能产生ε，那么 x 就可能产生ε，因此 First(x) 就应该包含ε。 在本讲的示例程序里，我们可以用SampleGrammar.expressionGrammar()方法获得一个表达式的语法，把它 dump() 一下，这其实是消除了左递归的表达式语法： expression : assign ; assign : equal | assign1 ; assign1 : '=' equal assign1 | ε; equal : rel equal1 ; equal1 : ('==' | '!=') rel equal1 | ε ; rel : add rel1 ; rel1 : ('\u003e=' | '\u003e' | '\u003c=' | '\u003c') add rel1 | ε ; add : mul add1 ; add1 : ('+' | '-') mul add1 | ε ; mul : pri mul1 ; mul1 : ('*' | '/') pri mul1 | ε ; pri : ID | INT_LITERAL | LPAREN expression RPAREN ; 我们用 GrammarNode 类代表语法的节点，形成一张语法图（蓝色节点的下属节点之间是“或”的关系，也就是语法中的竖线）。 基于这个数据结构能计算每个非终结符的 First 集合，可以参考LLParser类的 caclFirstSets() 方法。运行示例程序可以打印出表达式语法中各个非终结符的 First 集合。在计算时你要注意，因为上下文无关文法是允许递归嵌套的，所以这些 GrammarNode 节点构成的是一个图，而不是树，不能通过简单的遍历树的方法来计算 First 集合。比如，pri 节点是 expression 的后代节点，但 pri 又引用了 expression（pri-\u003e(expression)）。这样，计算 First(expression) 需要用到 First(pri)，而计算 First(pri) 又需要依赖 First(expression)。破解这个僵局的方法是用“不动点法”来计算。多次遍历图中的节点，看看每次有没有计算出新的集合成员。比如，第一遍计算的时候，当求 First(pri) 的时候，它所依赖的 First(expression) 中的成员可能不全，等下一轮继续计算时，发现有新的集合成员，再加进来就好了，直到所有集合的成员都没有变动为止。现在我们可以用 First 集合进行分支判断了，不过还要处理产生式可能为ε的情况，比如“+mul add1 | ε”或“blockStatement*”都会产生ε。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:21:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"计算和使用 Follow 集合 对ε的处理分成两种情况。 第一种情况，是产生式中的部分元素会产生ε。比如，在 Java 语法里，声明一个类成员的时候，可能会用 public、private 这些来修饰，但也可以省略不写。在语法规则中，这个部分是“accessModifier?”，它就可能产生ε。 memberDeclaration : accessModifier? type identifier ';' ; accessModifier : 'public' | 'private' ; type : 'int' | 'long' | 'double' ; 所以，当我们遇到下面这两个语句的时候，都可以判断为类成员的声明： public int a; int b; 这时，type 能够产生的终结符 ‘int’、‘long’和‘double’也在 memberDeclaration 的 First 集合中。这样，我们实际上把 accessModifier 给穿透了，直接到了下一个非终结符 type。所以这类问题依靠 First 集合仍然能解决。在解析的过程中，如果下一个 Token 是 ‘int’，我们可以认为 accessModifier 返回了ε，忽略它，继续解析下一个元素 type，因为它的 First 集合中才会包含 ‘int’。 第二种情况是产生式本身（而不是其组成部分）产生ε。这类问题仅仅依靠 First 集合是无法解决的，要引入另一个集合：Follow 集合。它是所有可能跟在某个非终结符之后的终结符的集合。以 block 语句为例，在 PlayScript.g4 中，大致是这样定义的： block : '{' blockStatements '}' ; blockStatements : blockStatement* ; blockStatement : variableDeclarators ';' | statement | functionDeclaration | classDeclaration ; 也就是说，block 是由 blockStatements 构成的，而 blockStatements 可以由 0 到 n 个 blockStatement 构成，因此可能产生ε。接下来，我们来看看解析 block 时会发生什么。假设花括号中一个语句也没有，也就是 blockStatments 实际上产生了ε。那么在解析 block 时，首先读取了一个 Token，即“{”，然后处理 blockStatements，我们再预读一个 Token，发现是“}”，那这个右花括号是 blockStatement 的哪个产生式的呢？实际上它不在任何一个产生式的 First 集合中，下面是进行判断的伪代码： nextToken = tokens.peek(); //得到'}' nextToken in First(variableDeclarators) ? //no nextToken in First(statement) ? //no nextToken in First(functionDeclaration) ? //no nextToken in First(classDeclaration) ? //no 我们找不到任何一个可用的产生式。这可怎么办呢？除了可能是 blockStatments 本身产生了ε之外，还有一个可能性就是出现语法错误了。而要继续往下判断，就需要用到 Follow 集合。像 blockStatements 的 Follow 集合只有一个元素，就是右花括号“}”。所以，我们只要再检查一下 nextToken 是不是花括号就行了： //伪代码 nextToken = tokens.peek(); //得到'}' nextToken in First(variableDeclarators) ? //no nextToken in First(statement) ? //no nextToken in First(functionDeclaration) ? //no nextToken in First(classDeclaration) ? //no if (nextToken in Follow(blockStatements)) //检查Follow集合 return Epsilon; //推导出ε else error; //语法错误 那么怎么计算非终结符 x 的 Follow 集合呢？扫描语法规则，看看 x 后面都可能跟哪些符号。对于后面跟着的终结符，都加到 Follow(x) 集合中去。如果后面是非终结符，就把它的 First 集合加到自己的 Follow 集合中去。最后，如果后面的非终结符可能产出ε，就再往后找，直到找到程序终结符号。 这个符号通常记做 $，意味一个程序的结束。比如在表达式的语法里，expression 后面可能跟这个符号，expression 的所有右侧分支的后代节点也都可能跟这个符号，也就是它们都可能出现在程序的末尾。但另一些非终结符，后面不会跟这个符号，如 blockstatements，因为它后面肯定会有“}”。你可以参考LLParser类的 caclFollowSets() 方法，这里也要用到不动点法做计算。运行程序可以打印出示例语法的的 Follow 集合。我把程序打印输出的 First 和 follow 集合整理如下（其实打印输出还包含一些中间节点，这里就不展示了）： 在表达式的解析中，我们会综合运用 First 和 Follow 集合。比如，对于“add1 -\u003e + mul add1 | ε”，如果预读的下一个 Token 是 +，那就按照第一个产生式处理，因为 + 在 First(“+ mul add1”) 集合中。如果预读的 Token 是 \u003e 号，那它肯定不在 First(add1) 中，而我们要看它是否属于 Follow(add1)，如果是，那么 add1 就产生一个ε，否则就报错。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:21:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"LL 算法和文法 现在我们已经建立了对 First 集合、Follow 集合和 LL 算法计算过程的直觉认知。这样再写出算法的实现，就比较容易了。用 LL 算法解析语法的时候，我们可以选择两种实现方式。 第一种，还是采用递归下降算法，只不过现在的递归下降算法是没有任何回溯的。无论走到哪一步，我们都能准确地预测出应该采用哪个产生式。第二种，是采用表驱动的方式。这个时候需要基于我们计算出来的 First 和 Follow 集合构造一张预测分析表。根据这个表，查找在遇到什么 Token 的情况下，应该走哪条路径。 这两种方式是等价的，你可以根据自己的喜好来选择，我用的是第一种。关于算法，我们就说这么多，接下来，我们谈谈如何设计符合 LL(k) 特别是 LL(1) 算法的文法。我们已经知道左递归的文法是要避免的，也知道要如何避免。除此之外，我们要尽量抽取左公因子，这样可以避免 First 集合产生交集。举例来说，变量声明和函数声明的规则在前半截都差不多，都是类型后面跟着标识符： statement : variableDeclare | functionDeclare | other; variableDeclare : type Identifier ('=' expression)? ; funcationDeclare : type Identifier '(' parameterList ')' block ; 具体例子如下： int age； int cacl(int a, int b){ return a + b; } 这样的语法规则，如果按照 LL(1) 算法，First(variableDeclare) 和 First(funcationDeclare) 是相同的，没法决定走哪条路径。你就算用 LL(2)，也是一样的，要用到 LL(3) 才行。但对于 LL(k) k \u003e 1 来说，程序开销有点儿大，因为要计算更多的集合，构造更复杂的预测分析表。不过这个问题很容易解决，只要把它们的左公因子提出来就可以了： statement: declarator | other; declarator : declarePrefix （variableDeclarePostfix |functionDeclarePostfix) ; variableDeclarePostfix : ('=' expression)? ; functionDeclarePostfix : '(' parameterList ')' block ; 这样，解析程序先解析它们的公共部分，即 declarePrefix，然后再看后面的差异。这时，它俩的 First 集合，一个{ = ; }，一个是{ ( }，两者没有交集，能够很容易区分。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:21:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课我们比较全面地梳理了自顶向下算法。语法解析过程可以看做是对图的遍历过程，遍历时可以采取深度优先或广度优先的策略，这里要注意，你可能在做深度优先遍历的时候，误用广度优先的思路。针对 LL 算法，我们通过实例分析了 First 集合和 Follow 集合的使用场景和计算方式。掌握了这两个核心概念，特别是熟悉它们的使用场景，你会彻底掌握 LL 算法。 本节课的示例代码我放在了文末，供你参考。lab/16～18（算法篇的示例代码）：码云 GitHub LLParser.java（LL 算法的语法解析器）：码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:21:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"18 | 移进和规约：用LR算法推演一个实例 到目前为止，我们所讨论的语法分析算法，都是自顶向下的。与之相对应的，是自底向上的算法，比如本节课要探讨的 LR 算法家族。LR 算法是一种自底向上的算法，它能够支持更多的语法，而且没有左递归的问题。第一个字母 L，与 LL 算法的第一个 L 一样，代表从左向右读入程序。第二个字母 R，指的是 RightMost（最右推导），也就是在使用产生式的时候，是从右往左依次展开非终结符。例如，对于“add-\u003eadd+mul”这样一个产生式，是优先把 mul 展开，然后再是 add。在接下来的讲解过程中，你会看到这个过程。自顶向下的算法，是递归地做模式匹配，从而逐步地构造出 AST。那么自底向上的算法是如何构造出 AST 的呢？答案是用移进 - 规约的算法。本节课，我就带你通过移进 - 规约方法，自底向上地构造 AST，完成语法的解析。接下来，我们先通过一个例子看看自底向上语法分析的过程。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:22:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"通过实例了解自底向上语法分析的过程 我们选择熟悉的语法规则： add -\u003e mul add -\u003e add + mul mul -\u003e pri mul -\u003e mul * pri pri -\u003e Int | (add) 然后来解析“2+3*5”这个表达式，AST 如下： 我们分步骤看一下解析的具体过程。第 1 步，看到第一个 Token，是 Int，2。我们把它作为 AST 的第一个节点，同时把它放到一个栈里（就是图中红线左边的部分）。这个栈代表着正在处理的一些 AST 节点，把 Token 移到栈里的动作叫做移进（Shift）。 第 2 步，根据语法规则，Int 是从 pri 推导出来的（pri-\u003eInt），那么它的上级 AST 肯定是 pri，所以，我们给它加了一个父节点 pri，同时，也把栈里的 Int 替换成了 pri。这个过程是语法推导的逆过程，叫做规约（Reduce）。Reduce 这个词你在学 Map-Reduce 时可能接触过，它相当于我们口语化的“倒推”。具体来讲，它是从工作区里倒着取出 1 到 n 个元素，根据某个产生式，组合出上一级的非终结符，也就是 AST 的上级节点，然后再放进工作区（也就是竖线的左边）。这个时候，栈里可能有非终结符，也可能有终结符，它仿佛是我们组装 AST 的一个工作区。竖线的右边全都是 Token（也就是终结符），它们在等待处理。 第 3 步，与第 2 步一样，因为 pri 只能是 mul 推导出来的，产生式是“mul-\u003epri”，所以我们又做了一次规约。 第 4 步，我们根据“add-\u003emul”产生式，将 mul 规约成 add。至此，我们对第一个 Token 做了 3 次规约，已经到头了。这里为什么做规约，而不是停在 mul 上，移进 + 号，是有原因的。因为没有一个产生式，是 mul 后面跟 + 号，而 add 后面却可以跟 + 号。 第 5 步，移进 + 号。现在栈里有两个元素了，分别是 add 和 +。 第 6 步，移进 Int，也就是数字 3。栈里现在有 3 个元素。 第 7 到第 8 步，Int 规约到 pri，再规约到 mul。到目前为止，我们做规约的方式都比较简单，就是对着栈顶的元素，把它反向推导回去。 第 9 步，我们面临 3 个选择，比较难。第一个选择是继续把 mul 规约成 add，第二个选择是把“add+mul”规约成 add。这两个选择都是错误的，因为它们最终无法形成正确的 AST。 第三个选择，也就是按照“mul-\u003emul*pri”，继续移进 * 号 ，而不是做规约。只有这样，才能形成正确的 AST，就像图中的虚线。 第 10 步，移进 Int，也就是数字 5。 第 11 步，Int 规约成 pri。 第 12 步，mul*pri 规约成 mul。注意，这里也有两个选择，比如把 pri 继续规约成 mul。但它显然也是错误的选择。 第 13 步，add+mul 规约成 add。 至此，我们就构建完成了一棵正确的 AST，并且，栈里也只剩下了一个元素，就是根节点。整个语法解析过程，实质是反向最右推导（Reverse RightMost Derivation）。什么意思呢？如果把 AST 节点根据创建顺序编号，就是下面这张图呈现的样子，根节点编号最大是 13： 但这是规约的过程，如果是从根节点开始的推导过程，顺序恰好是反过来的，先是 13 号，再是右子节点 12 号，再是 12 号的右子节点 11 号，以此类推。我们把这个最右推导过程写在下面： 在语法解析的时候，我们是从底下反推回去，所以叫做反向的最右推导过程。从这个意义上讲，LR 算法中的 R，带有反向（Reverse）和最右（Reightmost）这两层含义。在最右推导过程中，我加了下划线的部分，叫做一个句柄（Handle）。句柄是一个产生式的右边部分，以及它在一个右句型（最右推导可以得到的句型）中的位置。以最底下一行为例，这个句柄“Int”是产生式“pri-\u003eInt”的右边部分，它的位置是句型“Int + Int * Int”的第一个位置。简单来说，句柄，就是产生式是在这个位置上做推导的，如果需要做反向推导的话，也是从这个位置去做规约。针对这个简单的例子，我们可以用肉眼进行判断，找到正确的句柄，做出正确的选择。不过，要把这种判断过程变成严密的算法，做到在每一步都采取正确的行动，知道该做移进还是规约，做规约的话，按照哪个产生式，这就是 LR 算法要解决的核心问题了。那么，如何找到正确的句柄呢？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:22:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"找到正确的句柄 我们知道，最右推导是从最开始的产生式出发，经过多步推导（多步推导记做 -\u003e*），一步步形成当前的局面 （也就是左边栈里有一些非终结符和终结符，右边还可以预看 1 到 k 个 Token）。 add -\u003e* 栈 | Token 我们要像侦探一样，根据手头掌握的信息，反向推导出这个多步推导的路径，从而获得正确的句柄。我们依据的是左边栈里的信息，以及右边的 Token 串。对于 LR(0) 算法来说，我们只依据左边的栈，就能找到正确的句柄，对于 LR(1) 算法来说，我们可以从右边预看一个 Token。我们的思路是根据语法规则，复现这条推导路径。以第 8 步为例，下图是它的推导过程，橙色的路径是唯一能够到达第 8 步的路径。知道了正向推导的路径，自然知道接下来该做什么，在第 8 步，我们正确的选择是做移进。 为了展示这个推导过程，我引入了一个新概念：项目（Item）。Item 代表带有“.”符号的产生式。比如“pri-\u003e(add)”可以产生 4 个 Item，“.”分别在不同的位置。“.”可以看做是前面示意图中的竖线，左边的看做已经在栈里的部分，“.”右边的看做是期待获得的部分： pri-\u003e.(add) pri-\u003e(.add) pri-\u003e(add.) pri-\u003e(add). 上图其实是一个 NFA，利用这个 NFA，我们表达了所有可能的推导步骤。每个 Item（或者状态），在接收到一个符号的时候，就迁移到下一个状态，比如“add-\u003e.add+mul”在接收到一个 add 的时候，就迁移到“add-\u003eadd.+mul”，再接收到一个“+”，就迁移到“add-\u003eadd+.mul”。在这个状态图的左上角，我们用一个辅助性的产生式“start-\u003eadd”，作为整个 NFA 的唯一入口。从这个入口出发，可以用这个 NFA 来匹配栈里内容，比如在第 8 步的时候，栈以及右边下一个 Token 的状态如下，其中竖线左边是栈的内容： add + mul | * 在 NFA 中，我们从 start 开始遍历，基于栈里的内容，能找到图中橙色的多步推导路径。在这个状态迁移过程中，导致转换的符号分别是“ε、add、+、ε、mul”，忽略其中的ε，就是栈里的内容。在 NFA 中，我们查找到的 Item 是“mul-\u003emul.pri”。这个时候“.”在 Item 的中间。因此下一个操作只能是一个 Shift 操作，也就是把下一个 Token， 号，移进到栈里。如果“.”在 Item 的最后，则对应一个规约操作，比如在第 12 步，栈里的内容是： add + mul | $ //$代表Token串的结尾 这个时候的 Item 是“add-\u003eadd+mul.”。对于所有点符号在最后面的 Item，我们已经没有办法继续向下迁移了，这个时候需要做一个规约操作，也就是基于“add + mul”规约到 add，也就是到“add-\u003e.add+mul”这个状态。对于任何的ε转换，其逆向操作也是规约，比如图中从“add-\u003e.add+mul”规约到“start-\u003e.add”。但做规约操作之前，我们仍然需要检查后面跟着的 Token，是不是在 Follow(add) 中。对于 add 来说，它的 Follow 集合包括{$ + ）}。如果是这些 Token，那就做规约。否则，就报编译错误。所以，现在清楚了，我们能通过这个有限自动机，跟踪计算出正确的推导过程。当然了，在16 讲里，我提到每个 NFA 都可以转换成一个 DFA。所以，你可以直接在上面的 NFA 里去匹配，也可以把 NFA 转成 DFA，避免 NFA 的回溯现象，让算法效率更高。转换完毕的 DFA 如下： 在这个 DFA 中，我同样标注了在第 8 步时的推导路径。为了更清晰地理解 LR 算法的本质，我们基于这个 DFA 再把语法解析的过程推导一遍。第 1 步，移进一个 Int，从状态 1 迁移到 9。Item 是“pri-\u003eInt.”。 第 2 步，依据“pri-\u003eInt”做规约，从状态 9 回到状态 1。因为现在栈里有个 pri 元素，所以又迁移进了状态 8。 第 3 步，依据“mul-\u003epri”做规约，从状态 8 回到状态 1，再根据栈里的 mul 元素进入状态 7。注意，在状态 7 的时候，下一步的走向有两个可能的方向，分别是“add-\u003emul.”和“mul-\u003emul.*pri”这两个 Item 代表的方向。基于“add-\u003emul.”会做规约，而基于“mul-\u003emul.*pri”会做移进，这就需要看看后面的 Token 了。如果后面的 Token 是 * 号，那其实要选第二个方向。但现在后面是 + 号，所以意味着这里只能做规约。 第 4 步，依据“add-\u003emul”做规约，从状态 7 回到状态 1，再依据 add 元素进入状态 2。 第 5 步，移进 + 号。这对应状态图上的两次迁移，首先根据栈里的第一个元素 add，从 1 迁移到 2。然后再根据“+”，从 2 到 3。Item 的变化是：状态 1：start-\u003e.add状态 1：add-\u003e.add+mul状态 2：add-\u003eadd.+mul状态 3：add-\u003eadd+.mul 你看，通过移进这个加号，我们实际上知道了这个表达式顶部必然有一个“add+mul”的结构。 第 6 到第 8 步，移进 Int，并一直规约到 mul。状态变化是先从状态 3 到状态 9，然后回到状态 3，再进到状态 4。 第 9 步，移进一个 *。根据栈里的元素，迁移路径是 1-\u003e2-\u003e3-\u003e4-\u003e5。 第 10 步，移进 Int，进入状态 9。 第 11 步，根据“pri-\u003eInt”规约到 pri，先退回到状态 5，接着根据 pri 进入状态 6。 第 12 步，根据“mul-\u003emul*pri”规约到 mul，从而退回到状态 4。 第 13 步，根据“add-\u003eadd+mul”规约到 add，从而退回到状态 2。 从状态 2 再根据“start-\u003eadd”再规约一步，就变成了 start，回到状态 1，解析完成。现在我们已经对整个算法的整个执行过程建立了直觉认知。如果想深入掌握 LR 算法，我建议你把这种推导过程多做几遍，自然会了然于胸。建立了直觉认知以后，接下来，我们再把 LR 算法的类型和实现细节讨论一下。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:22:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"LR 解析器的类型和实现 LR 算法根据能力的强弱和实现的复杂程度，可以分成多个级别，分别是 LR(0)、SLR(k)（即简单 LR）、LALR(k)（Look ahead LR）和 LR(k)，其中 k 表示要在 Token 队列里预读 k 个 Token。 我来讲解一下这四种类型算法的特点，便于你选择和使用。 LR(0) 不需要预看右边的 Token，仅仅根据左边的栈就能准确进行反向推导。比如，前面 DFA 中的状态 8 只有一个 Item：“mul-\u003epri.”。如果处在这个状态，那接下来操作是规约。假设存在另一个状态，它也只有一个 Item，点符号不在末尾，比如“mul-\u003emul.*pri”，那接下来的操作就是移进，把下一个输入放到栈里。但实际使用的语法规则很少有这么简单的。所以 LR(0) 的表达能力太弱，能处理的语法规则有限，不太有实用价值。就像在前面的例子中，如果我们不往下预读一个 Token，仅仅利用左边工作区的信息，是找不到正确的句柄的。比如，在状态 7 中，我们可以做两个操作： 对于第一个 Item，“add-\u003emul.”，需要做一个规约操作。对于第二个 Item，“mul-\u003emul.*pri”，实际上需要做一个移进操作。 这里发生的冲突，就叫做“移进 / 规约”冲突（Shift/Reduce Conflict）。意思是，又可以做移进，又可以做规约，到底做哪个？对于状态 7 来说，到底做哪个操作，实际上取决于右边的 Token。 SLR（Simple LR）是在 LR(0) 的基础上做了增强。对于状态 7 的这种情况，我们要加一个判断条件：右边下一个输入的 Token，是不是在 add 的 Follow 集合中。因为只有这样，做规约才有意义。在例子中，add 的 Follow 集合是{+ ) $}。如果不在这个范围内，那么做规约肯定是不合法的。因为 Follow 集合的意思，就是哪些 Token 可以出现在某个非终结符后面。所以，如果在状态 7 中，下一个 Token 是 *，它不在 add 的 Follow 集合中，那么我们就只剩了一个可行的选择，就是移进。这样就不存在两个选择，也不存在冲突。实际上，就我们本讲所用的示例语法而言，SLR 就足够了，但是对于另一些更复杂的语法，采用 SLR 仍然会产生冲突，比如： start -\u003e exp exp -\u003e lvalue = rvalue exp -\u003e rvalue lvalue -\u003e Id lvalue -\u003e *rvalue rvalue -\u003e lvalue 这个语法说的是关于左值和右值的情况，我们曾在语义分析的时候说过。在这个语法里，右值只能出现在赋值符号右边。在状态 2，如果下一个输入是“=”，那么做移进和规约都是可以的。因为“=”在 rvalue 的 Follow 集合中。 怎么来处理这种冲突呢？仅仅根据 Follow 集合来判断是否 Reduce，不太严谨。因为在上图状态 2 的情况下，即使后面跟着的是“=”，我们仍然不能做规约。因为你一规约，就成了一个右值，但它在等号的左边，显然是跟我们的语法定义冲突的。办法是 Follow 集合拆了，把它的每个成员都变成 Item 的一部分。这样我们就能做更细致的判断。如下图所示，这样细化以后，我们发现在状态 2 中，只有下一个输入是“$”的时候，才能做规约。这就是 LR(1) 算法的原理，它更加强大。 但 LR(1) 算法也有一个缺点，就是 DFA 可能会很大。在语法分析阶段，DFA 的大小会随着语法规则的数量呈指数级上升，一个典型的语言的 DFA 状态可能达到上千个，这会使语法分析的性能很差，从而也丧失了实用性。LALR(k) 是基于这个缺点做的改进。它用了一些技巧，能让状态数量变得比较少，但处理能力没有太大的损失。YACC 和 Bison 这两个工具，就是基于 LALR(1) 算法的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:22:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 今天，我们讲了自底向上的 LR 算法的原理，包括移进 - 规约，如何寻找正确的句柄，如果基于 NFA 和 DFA 决定如何做移进和规约。LR 算法是公认的比较难学的一个算法。好在我们已经在前两讲给它做了技术上的铺垫了，包括 NFA 和 DFA，First 和 Follow 集合。这节课我们重点在于建立直观理解，特别是如何依据栈里的信息做正确的反推。这个直觉认知很重要，建立这个直觉的最好办法，就是像本节课一样，根据实例来画图、推导。这样，在你真正动手写算法的时候，就胸有成竹了！到今天为止，我们已经把前端技术中的关键算法都讲完了。不过我还是想强调一下，如果想真正掌握这些算法，必须动手实现一下才行，勤动手才是王道。 lab/16-18（算法篇的示例代码）：码云 GitHub LLParser.java（LL 算法的语法解析器）：码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:22:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现一门脚本语言 答疑 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:23:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"19 | 对于左递归的语法，为什么我的推导不是左递归的？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:24:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"问题一：对于左递归的语法，为什么我的推导不是左递归的？** 这个问题本身反映了，进行递归下降分析的时候，如何保持清晰的思路，值得讲一讲。在03 讲，我们刚开始接触到语法分析，也刚开始接触递归下降算法。这时，我介绍了左递归的概念，但你可能在实际推导的过程中，觉得不是左递归，比如用下面这个语法，来推导“2+3”这个简单的表达式： //简化的左递归文法 add-\u003eInt add-\u003eadd + Int 你可能会拿第一个产生式做推导：add-\u003e2成功返回 因为没有采用第二条产生式，所以不会触发递归调用。但这里的问题是，“2+3”是一个加法表达式，2 也是一个合法的加法表达式，但仅仅解析出 2 是不行的，我们必须完整地解析出“2+3”来。在17 讲，我提到，任何自顶向下的算法，都是在一个大的图里找到一条搜索路径的过程。最后的结果，是经过多次推导，生成跟输入的 Token 串相同的结果，解析完毕以后，所有 Token 也耗光。 如果只匹配上 2，那就证明这条搜索路径是错误的，我们必须尝试另一种可能性，也就是第二个产生式。要找到正确的搜索路径，在递归下降算法或者 LL 算法时，我们都是采用“贪婪”策略，这个策略在16 讲关于正则表达式时讲过。也就是要匹配尽量多的 Token 才可以。就算是换成右递归的文法，也不能采用第一个产生式。因为解析完 Int 以后，接下来的 Token 是 + 号，还可以尝试用第二个产生式，那我们就要启动贪婪策略，用第二个，而不是第一个。 //简化的右递归文法 add-\u003eInt add-\u003eInt + add 以上是第一种情况。不过有的同学说：“我运用第二个产生式也能匹配成功，根据‘add-\u003eadd + int’这个产生式，先拿第一个 add 去匹配 2，再去匹配 + 号和 3 不就行了吗？”这是另一种引起困扰的情况，也是我在 17 讲必须说一下广度优先算法的一个原因。因为这位同学的推导过程，是典型的广度优先。add 非终结符，先广度优先地拆成两条路径：第一条路径匹配不成功；第二条路径进一步进行广度优先的探索，于是成功解析： 但我们在 17 讲也说过了，广度优先算法的性能很低，在这个简单的例子中还看不出来，但如果是稍微复杂一点儿的语法和表达式，计算量就指数级上升。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:24:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"问题二：二元表达式的结合性的实现。 @nil：最终通过循环来消除递归带来的二元预算符的结合性问题？能否直接在递归中消除结合性问题？04 讲的这个问题在递归下降算法中是个难点，反映了理论上的算法用于工程实践时，会遇到的具体问题和解决方案，所以也值得探讨。因为递归下降算法是自顶向下、最左推导的。对于 AST 来说，父节点总是先于子节点来生成。因此，使用下面这个消除了左递归的加法文法来尝试解析“2+3+4+5”这个表达式： add -\u003e Int add' add' -\u003e + Int add' | ε 得到的 AST 应该是这样的： 这个 AST 会觉得有点儿怪，毕竟它把加法操作分成了 add 和 add’这两种操作。针对 add’这样一个节点，我们可以定义为把 Int 子节点和 add’子节点做加法，但这样就一共要做四次计算，1 个 add 计算，3 个 add’计算。并且，因为是右递归，所以计算顺序是右结合的。如果我们想改成左结合，可以尝试改变之前的约定，就是父节点先于子节点生成，把 AST 强行拧成这个样子： 可以看出，这样强拧的过程，已经违背了 add 和 add’产生式的规则。同时，用 add 和 add’这两个节点才能表达加法运算，还是跟我们日常的习惯相违背。与之相对的，Antlr 的写法，就很符合我们日常习惯。它是根据 \u003c assoc=left\u003e 这样的额外信息，决定解析时如何生成 AST 的结构： add : Int |\u003cassoc=left\u003e add + add ； 我们文稿中的示例算法，跟这个思路类似，也是不改变加法运算的含义，但会根据结合性改变 AST 节点的父子结构。这种改变，等价于我们在解析加法表达式时，不是用的最左推导，而是最右推导。 所以，我们可以看出：单纯的运用递归下降算法或 LL 算法，我们是无法支持左结合的，一定要做一些特殊的处理。而 LR 算法就不需要这些特殊处理，仅仅通过文法的设计，就能支持好结合性，这可能是很多人推崇 LR 算法的原因吧。另一方面，工程上运用良好的语法解析方法，不需要是纯粹的某一种单一的算法，增加一些辅助手段会让它更有效。比如 Antlr 的内部实现可以自动选择预读 1 个或更多个 Token。必要的话还会启动回溯机制。这样做的好处，是对语法编写的要求降低，更加照顾程序员的工作效率。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:24:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"问题三 ：二义性文法为什么也能正常解析？ @windpiaoxue： stmt -\u003e if expr stmt | if expr stmt else stmt | other 我测试了一下，Antlr 使用上面这个规则可以正确地处理悬挂 else 的问题， Antlr 在处理这种二义性问题的时候，是依据什么来处理的？ 针对07 讲中关于二义性文法的问题也有普遍意义，其实原因我在 07 讲里已经说了。我们实现一个算法的时候，是有确定的顺序来匹配的，所以，即使是二义性文法，在某种算法下也可以正常解析，也就是生成正确的 AST。如果我们采取深度优先的自顶向下的算法，在使用这两个产生式时： stmt -\u003e if expr stmt stmt -\u003e if expr stmt else stmt 我们就像问题一中讲加法运算时提到的那样，采用“贪婪”的算法，总是试图匹配更多的 Token。所以，只要有 else，它就会去匹配 else，所以 else 总是会跟最近的 if 组成一对。但采用这个文法的时候，如果不是用贪婪策略来解析，就可能会导致 if 和 else 错配。而严格的非二义性文法要求得比较高，它要求是算法无关的，也就是无论采用哪种推导顺序，形成的 AST 是一样的。 这里的关键点，在于把“文法”和“算法”这两件事区分开，文法是二义的，用某个具体算法却不一定是二义的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:24:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"问题四：“语法”和“文法”有什么区别和联系？ @鸠摩智：请问语法和文法有什么区别和联系？ 这是一个术语的问题，确实要理清楚，你也可能会有这种疑问。文法（Grammar），是形式语言（Formal Language）的一个术语。所以也有 Formal Grammar 这样的说法。这里的文法是定义清晰的规则，比如，我们的词法规则、语法规则和属性规则，都是用形式文法来定义的。我们的课程里讲解了正则文法 (Regular Grammar)、上下文无关文法 (Context-free Grammar) 等不同的文法规则，用来描述词法和语法。语法分析中的语法（Syntax），主要是描述词是怎么组成句子的，一个语言的语法规则，通常指的是这个 Syntax。问题是，Grammar 这个词，在中文很多应用场景中也叫做语法。这是会引起混淆的地方。我们在使用的时候要小心一点儿就行了。比如，我做了一个规则文件，里面都是一些词法规则（Lexer Grammar），我会说，这是一个词法规则文件，或者词法文法文件。这个时候，把它说成是一个语法规则文件，就有点儿含义模糊。因为这里面并没有语法规则（Syntax Grammar）。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:24:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"案例总结 在前端部分，我们伴随着文稿提供了丰富的示例程序，我相信代码是程序员之间沟通的最好手段。 第一批示例程序，是 lab/craft 目录下的。通过手工实现简单的词法分析和语法分析，获得第一手的感受，破除对于编译技术的神秘感。你会感觉到，如果要实现公式计算器甚至一个简单脚本，似乎也没那么难。 第二批示例程序，是基于 Antlr 工具的。使用这个工具，实现了两个目的：第一，让你借鉴成熟的规则文件，高效实现词法分析和语法分析功能。第二，在不必关注词法分析和语法分析的情况下，我们把更多的精力放在了语言特性设计、语义分析和运行期机制上。针对作用域、函数、闭包、面向对象等特性都提供了示例程序，最终实现出一门看上去挺威风的脚本语言。 第三批示例程序，则是完成了应用篇的两个题目。一个示范了如何通过解析 SQL 语句，实现分布式数据库的一个简单特性。另一个演示了如何来实现一个报表系统。通过两个实际案例将技术跟应用领域做了很好的连接，启发你按照类似的思路，去解决自己领域的问题。 第四批示例程序，是在算法篇，针对编译器前端的三组核心算法提供了示例。这些示例程序能够根据文法规则直接做词法分析和语法分析，不需要为每一组规则单独构造词法分析器和语法分析器，实际上相当于简化版本的 Lex（词法分析）、Antlr（LL 语法分析）和 YACC（LR 语法分析）。我给你的学习设计了多次迭代、循环提升认知的路径，从简单原理、现有工具和最佳实践、领域应用、算法逻辑等多个维度，给你全面的感受。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:24:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现一门编译型语言 原理 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:25:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"20 | 高效运行：编译器的后端技术 前 18 节课，我们主要探讨了编译器的前端技术，它的重点，是让编译器能够读懂程序。无结构的代码文本，经过前端的处理以后，就变成了 Token、AST 和语义属性、符号表等结构化的信息。基于这些信息，我们可以实现简单的脚本解释器，这也从另一个角度证明了我们的前端处理工作确实理解了程序代码，否则程序不可能正确执行嘛。实际上，学完前端技术以后，我们已经能做很多事情了，比如让软件有自定义功能，就像我们在15 讲中提到的报表系统，这时，不需要涉及编译器后端技术。 但很多情况下，我们需要继续把程序编译成机器能读懂的代码，并高效运行。这时，我们就面临了三个问题：1. 我们必须了解计算机运行一个程序的原理（也就是运行期机制），只有这样，才知道如何生成这样的程序。2. 要能利用前端生成的 AST 和属性信息，将其正确翻译成目标代码。3. 需要对程序做尽可能多的优化，比如让程序执行效率更高，占空间更少等等。 弄清这三个问题，是顺利完成编译器后端工作的关键，本节课，我会让你对程序运行机制、生成代码和优化代码有个直观的了解，然后再在接下来的课程中，将这些问题逐一击破。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:26:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"弄清程序的运行机制 总的来说，编译器后端要解决的问题是：现在给你一台计算机，你怎么生成一个可以运行的程序，然后还能让这个程序在计算机上正确和高效地运行？我画了一个模型： 基本上，我们需要面对的是两个硬件：一个是 CPU，它能接受机器指令和数据，并进行计算。它里面有寄存器、高速缓存和运算单元，充分利用寄存器和高速缓存会让系统的性能大大提升。另一个是内存。我们要在内存里保存编译好的代码和数据，还要设计一套机制，让程序最高效地利用这些内存。 通常情况下，我们的程序要受某个操作系统的管理，所以也要符合操作系统的一些约定。但有时候我们的程序也可能直接跑在硬件上，单片机和很多物联网设备采用这样的结构，甚至一些服务端系统，也可以不跑在操作系统上。你可以看出，编译器后端技术跟计算机体系结构的关系很密切。我们必须清楚地理解计算机程序是怎么运行的，有了这个基础，才能探讨如何编译生成这样的程序。所以，我会在下一节课，也就是 21 讲，将运行期的机制讲清楚，比如**内存空间如何划分和组织；程序是如何启动、跳转和退出的；执行过程中指令和数据如何传递到 CPU；整个过程中需要如何跟操作系统配合，**等等。也有的时候，我们的面对的机器是虚拟机，Java 的运行环境就是一个虚拟机（JVM），那我们需要就了解这个虚拟机的特点，以便生成可以在这个虚拟机上运行的代码，比如 Java 的字节码。同时，字节码有时仍然需要编译成机器码。在对运行期机制有了一定的了解之后，我们就有底气来进行下一步了，生成符合运行期机制的代码。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:26:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"生成代码 编译器后端的最终结果，就是生成目标代码。如果目标是在计算机上直接运行，就像 C 语言程序那样，那这个目标代码指的是汇编代码。而如果运行目标是 Java 虚拟机，那这个目标代码就是指 JVM 的字节码。基于我们在编译器前端所生成的成果，我们其实可以直接生成汇编代码，在后面的课程中，我会带你做一个这样的尝试。你可能惧怕汇编代码，觉得它肯定很难，能写汇编的人一定很牛。在我看来，这是一个偏见，因为汇编代码并不难写，为什么呢？其实汇编没有类型，也没有那么多的语法结构，它要做的通常就是把数据拷贝到寄存器，处理一下，再保存回内存。所以，从汇编语言的特性看，就决定了它不可能复杂到哪儿去。 你如果问问硬件工程师就知道了，因为他们经常拿汇编语言操作寄存器、调用中断，也没多难。但另一方面，正是因为汇编的基础机制太简单，而且不太安全，用它编写程序的效率太低，所以现在直接用汇编写的程序，都是处理很小、很单一的问题，我们不会再像阿波罗登月计划那样，用汇编写整个系统，这个项目的代码最近已经开源了，如果现在用高级语言去做这项工作，会容易得多，还可以像现在的汽车自动驾驶系统一样实现更多的功能。所以，在 22 和 23 讲，我会带你从 AST 直接翻译成汇编代码，并编译成可执行文件，这样你就会看到这个过程没有你想象的那么困难，你对汇编代码的恐惧感，也会就此消失了。 当然，写汇编跟使用高级语言有很多不同，其中一点就是要关心 CPU 和内存这样具体的硬件。比如，你需要了解不同的 CPU 指令集的差别，你还需要知道 CPU 是 64 位的还是 32 位的，有几个寄存器，每个寄存器可以用于什么指令，等等。但这样导致的问题是，每种语言，针对每种不同的硬件，都要生成不同的汇编代码。你想想看，一般我们设计一门语言要支持尽可能多的硬件平台，这样的工作量是不是很庞大？所以，为了降低后端工作量，提高软件复用度，就需要引入中间代码（Intermediate Representation，IR）的机制，它是独立于具体硬件的一种代码格式。各个语言的前端可以先翻译成 IR，然后再从 IR 翻译成不同硬件架构的汇编代码。如果有 n 个前端语言，m 个后端架构，本来需要做 m*n 个翻译程序，现在只需要 m+n 个了。这就大大降低了总体的工作量。 甚至，很多语言主要做好前端就行了，后端可以尽量重用已有的库和工具，这也是现在推出新语言越来越快的原因之一。像 Rust 就充分利用了 LLVM，GCC 的各种语言，如 C、C++、Object C 等，也是充分共享了后端技术。IR 可以有多种格式，在第 24 讲，我们会介绍三地址代码、静态单赋值码等不同的 IR。比如，“x + y * z”翻译成三地址代码是下面的样子，每行代码最多涉及三个地址，其中 t1 和 t2 是临时变量： t1 := y * z t2 := x + t1 Java 语言生成的字节码也是一种 IR，我们还会介绍 LLVM 的 IR，并且基于 LLVM 这个工具来加速我们后端的开发。其实，IR 这个词直译成中文，是“中间表示方式”的意思，不一定非是像汇编代码那样的一条条的指令。所以，AST 其实也可以看做一种 IR。我们在前端部分实现的脚本语言，就是基于 AST 这个 IR 来运行的。每种 IR 的目的和用途是不一样的： AST 主要用于前端的工作。Java 的字节码，是设计用来在虚拟机上运行的。LLVM 的中间代码，主要是用于做代码翻译和编译优化的。…… 总的来说，我们可以把各种语言翻译成中间代码，再针对每一种目标架构，通过一个程序将中间代码翻译成相应的汇编代码就可以了。然而事情真的这么简单吗？答案是否定的，因为我们还必须对代码进行优化。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:26:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"代码分析和优化 生成正确的、能够执行的代码比较简单，可这样的代码执行效率很低，因为直接翻译生成的代码往往不够简洁，**比如会生成大量的临时变量，指令数量也较多。**因为翻译程序首先照顾的是正确性，很难同时兼顾是否足够优化，这是一方面。另一方面，由于高级语言本身的限制和程序员的编程习惯，也会导致代码不够优化，不能充分发挥计算机的性能。所以我们一定要对代码做优化。程序员在比较各种语言的时候，一定会比较它们的性能差异。一个语言的性能太差，就会影响它的使用和普及。实际上，就算是现在常见的脚本语言，如 Python 和 JavaScript，也做了很多后端优化的工作，包括编译成字节码、支持即时编译等，这些都是为了进一步提高性能。从谷歌支持的开源项目 V8 开始，JavaScript 的性能获得了巨大的提高，这才导致了 JavaScript 再一次的繁荣，包括支持体验更好的前端应用和基于 Node.js 的后端应用。 优化工作又分为“独立于机器的优化”和“依赖于机器的优化”两种。 **独立于机器的优化，是基于 IR 进行的。**它可以通过对代码的分析，用更加高效的代码代替原来的代码。比如下面这段代码中的 foo() 函数，里面有多个地方可以优化。甚至，我们连整个对 foo() 函数的调用，也可以省略，因为 foo() 的值一定是 101。这些优化工作在编译期都可以去做。 int foo(){ int a = 10*10; //这里在编译时可以直接计算出100这个值 int b = 20; //这个变量没有用到，可以在代码中删除 if (a\u003e0){ //因为a一定大于0，所以判断条件和else语句都可以去掉 return a+1; //这里可以在编译器就计算出是101 } else{ return a-1; } } int a = foo(); //这里可以直接地换成 a=101; 上面的代码，通过优化，可以消除很多冗余的逻辑。这就好比你正在旅行，先从北京飞到了上海，然后又飞到厦门，最后飞回北京。然后你朋友问你现在在哪时，你告诉他在北京。那么他虽然知道你在北京，但并没有意识到你已经在几个城市折腾了一圈，因为他只关心你现在在哪儿，并不关心你的中间过程。 我们在给 a 赋值的时候，只需要知道这个值是 101 就行了。完全不需要在运行时去兜一大圈来计算。计算机代码里有很多这种需要优化的情形。我们在 27 和 28 讲会介绍多种优化技术，比如局部优化和全局优化，常数折叠、拷贝传播、删除公共子表达式等，其中数据流分析方法比较重要，会重点介绍。 依赖于机器的优化，则是依赖于硬件的特征。现代的计算机硬件设计了很多特性，以便提供更高的处理能力，比如并行计算能力，多层次内存结构（使用多个级别的高速缓存）等等。编译器要能够充分利用硬件提供的性能，比如 ： 寄存器优化。对于频繁访问的变量，最好放在寄存器中，并且尽量最大限度地利用寄存器，不让其中一些空着，有不少算法是解决这个问题的，教材上一般提到的是染色算法；充分利用高速缓存。高速缓存的访问速度可以比内存快几十倍上百倍，所以我们要尽量利用高速缓存。比如，某段代码操作的数据，在内存里尽量放在一起，这样 CPU 读入数据时，会一起都放到高速缓存中，不用一遍一遍地重新到内存取。并行性。现代计算机都有多个内核，可以并行计算。我们的编译器要尽可能把充分利用多个内核的计算能力。 这在编译技术中是一个专门的领域。流水线。CPU 在处理不同的指令的时候，需要等待的时间周期是不一样的，在等待某些指令做完的过程中其实还可以执行其他指令。就比如在星巴克买咖啡，交了钱就可以去等了，收银员可以先去处理下一个顾客，而不是要等到前一个顾客拿到咖啡才开始处理下一个顾客。指令选择。有的时候，CPU 完成一个功能，有多个指令可供选择。而针对某个特定的需求，采用 A 指令可能比 B 指令效率高百倍。比如 X86 架构的 CPU 提供 SIMD(Single Instruction Multiple Data) 功能，也就是一条指令可以处理多条数据，而不是像传统指令那样一条指令只能处理一条数据。在内存计算领域，SIMD 也可以大大提升性能，我们在第 30 讲的应用篇，会针对 SIMD 做一个实验。其他优化。比如可以针对专用的 AI 芯片和 GPU 做优化，提供 AI 计算能力，等等。 可以看出来，做好依赖于机器的优化要对目标机器的体系结构有清晰的理解，如果能做好这些工作，那么开发一些系统级的软件也会更加得心应手。实际上，数据库系统、大数据系统等等，都是要融合编译技术的。总结起来，在编译器中需要对代码进行的优化非常多。因此，这部分工作也是编译过程中耗时最长、最体现某个编译器的功力的一类工作，所以更值得引起你的重视。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:26:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我们对编译器的后端技术做了概述。你了解到要做好后端工作，必须熟悉计算机体系结构和程序的运行时机制；还要从前端生成中间代码，然后基于中间代码生成针对不同平台的目标代码；最后，需要对代码做各种优化工作，包括独立于机器的优化和依赖于机器的优化。刚接触编译技术的时候，你可能会把视线停留在前端技术上，以为能做 Lexer、Parser 就是懂编译了。实际上，词法分析和语法分析比较成熟，有成熟的工具来支撑。相对来说，后端的工作量更大，挑战更多，研究的热点也更多。比如，人工智能领域又出现了一些专用的 AI 芯片和指令集，就需要去适配。 编译器的后端，要把高级语言翻译成计算机能够理解的目标语言。它跟前端相比，关注点是不同的。前端关注的是正确反映了代码含义的静态结构，而后端关注的是让代码良好运行的动态结构。它们之间的差别，从我讲解“作用域”和“生存期”两个概念时就能看出来。作用域是前端的概念，而生存期是后端的概念。其实在前面的课程中，我们已经涉及了少量的后端技术的概念，比如生存期、栈桢，因为我们要让脚本语言运行起来。但这个运行环境比较简单，脚本的执行也是简单的基于 AST，所以性能是比较低的。但在后端部分，我们会实现一门静态编译型的语言，因此会对对运行期机制做更加深入的解读和实现。如果能把后端技术学好，你对计算机底层运行机制的理解会更上一层楼，也会成为一名底子更加扎实的软件工程师。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:26:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"21 | 运行时机制：突破现象看本质，透过语法看运行时 编译器的任务，是要生成能够在计算机上运行的代码，但要生成代码，我们必须对程序的运行环境和运行机制有比较透彻的了解。你要知道，大型的、复杂一点儿的系统，比如像淘宝一样的电商系统、搜索引擎系统等等，都存在一些技术任务，是需要你深入了解底层机制才能解决的。比如淘宝的基础技术团队就曾经贡献过，Java 虚拟机即时编译功能中的一个补丁。这反映出掌握底层技术能力的重要性，所以，如果你想进阶成为这个层次的工程师，不能只学学上层的语法，而是要把计算机语言从上层的语法到底层的运行机制都了解透彻。本节课，我会对计算机程序如何运行，做一个解密，话题分成两个部分： 了解程序运行的环境，包括 CPU、内存和操作系统，探知它们跟程序到底有什么关系。2. 了解程序运行的过程。比如，一个程序是怎么跑起来的，代码是怎样执行和跳转的，又是如何管理内存的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:27:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"程序运行的环境程序 运行的过程中，主要是跟两个硬件（CPU 和内存）以及一个软件（操作系统）打交道。 本质上，我们的程序只关心 CPU 和内存这两个硬件。你可能说：“不对啊，计算机还有其他硬件，比如显示器和硬盘啊。”但对我们的程序来说，操作这些硬件，也只是执行某些特定的驱动代码，跟执行其他代码并没有什么差异。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:27:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"关注 CPU 和内存 CPU 的内部有很多组成部分，对于本课程来说，我们重点关注的是寄存器以及高速缓存，它们跟程序的执行机制和优化密切相关。 寄存器是 CPU 指令在进行计算的时候，临时数据存储的地方。CPU 指令一般都会用到寄存器，比如，典型的一个加法计算（c=a+b）的过程是这样的：指令 1（mov）：从内存取 a 的值放到寄存器中；指令 2（add）：再把内存中 b 的值取出来与这个寄存器中的值相加，仍然保存在寄存器中；指令 3（mov）：最后再把寄存器中的数据写回内存中 c 的地址。 寄存器的速度也很快，所以能用寄存器就别用内存。尽量充分利用寄存器，是编译器做优化的内容之一。而高速缓存可以弥补 CPU 的处理速度和内存访问速度之间的差距。所以，我们的指令在内存读一个数据的时候，它不是老老实实地只读进当前指令所需要的数据，而是把跟这个数据相邻的一组数据都读进高速缓存了。这就相当于外卖小哥送餐的时候，不会为每一单来回跑一趟，而是一次取一批，如果这一批外卖恰好都是同一个写字楼里的，那小哥的送餐效率就会很高。 内存和高速缓存的速度差异差不多是两个数量级，也就是一百倍。比如，高速缓存的读取时间可能是 0.5ns，而内存的访问时间可能是 50ns。不同硬件的参数可能有差异，但总体来说是几十倍到上百倍的差异。你写程序时，尽量把某个操作所需的数据都放在内存中的连续区域中，不要零零散散地到处放，这样有利于充分利用高速缓存。这种优化思路，叫做数据的局部性。 这里提一句，在写系统级的程序时，你要对各种 IO 的时间有基本的概念，比如高速缓存、内存、磁盘、网络的 IO 大致都是什么数量级的。因为这都影响到系统的整体性能，也影响到你如何做程序优化。如果你需要对程序做更多的优化，还需要了解更多的 CPU 运行机制，包括流水线机制、并行机制等等，这里就不展开了。 讲完 CPU 之后，还有内存这个硬件。程序在运行时，操作系统会给它分配一块虚拟的内存空间，让它在运行期可以使用。我们目前使用的都是 64 位的机器，你可以用一个 64 位的长整型来表示内存地址，它能够表示的所有地址，我们叫做寻址空间。64 位机器的寻址空间就有 2 的 64 次方那么大，也就是有很多很多个 TB（Terabyte），大到你的程序根本用不完。不过，操作系统一般会给予一定的限制，不会给你这么大的寻址空间，比如给到 100 来个 G，这对一般的程序，也足够用了。在存在操作系统的情况下，程序逻辑上可使用的内存一般大于实际的物理内存。程序在使用内存的时候，操作系统会把程序使用的逻辑地址映射到真实的物理内存地址。有的物理内存区域会映射进多个进程的地址空间。 对于不太常用的内存数据，操作系统会写到磁盘上，以便腾出更多可用的物理内存。当然，也存在没有操作系统的情况，这个时候你的程序所使用的内存就是物理内存，我们必须自己做好内存的管理。 **对于这个内存，该怎么用呢？**本质上来说，你想怎么用就怎么用，并没有什么特别的限制。一个编译器的作者，可以决定在哪儿放代码，在哪儿放数据，当然了，别的作者也可能采用其他的策略。实际上，C 语言和 Java 虚拟机对内存的管理和使用策略就是不同的。尽管如此，大多数语言还是会采用一些通用的内存管理模式。以 C 语言为例，会把内存划分为代码区、静态数据区、栈和堆。 一般来讲，代码区是在最低的地址区域，然后是静态数据区，然后是堆。而栈传统上是从高地址向低地址延伸，栈的最顶部有一块区域，用来保存环境变量。 代码区（也叫文本段）存放编译完成以后的机器码。这个内存区域是只读的，不会再修改，但也不绝对。现代语言的运行时已经越来越动态化，除了保存机器码，还可以存放中间代码，并且还可以在运行时把中间代码编译成机器码，写入代码区。 **静态数据区保存程序中全局的变量和常量。**它的地址在编译期就是确定的，在生成的代码里直接使用这个地址就可以访问它们，它们的生存期是从程序启动一直到程序结束。它又可以细分为 Data 和 BSS 两个段。Data 段中的变量是在编译期就初始化好的，直接从程序装在进内存。BSS 段中是那些没有声明初始化值的变量，都会被初始化成 0。 **堆适合管理生存期较长的一些数据，这些数据在退出作用域以后也不会消失。**比如，我们在某个方法里创建了一个对象并返回，并希望代表这个对象的数据在退出函数后仍然可以访问。 **而栈适合保存生存期比较短的数据，比如函数和方法里的本地变量。**它们在进入某个作用域的时候申请内存，退出这个作用域的时候就可以释放掉。 讲完了 CPU 和内存之后，我们再来看看跟程序打交道的操作系统。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:27:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"2. 程序和操作系统的关系 程序跟操作系统的关系比较微妙：一方面我们的程序可以编译成不需要操作系统也能运行，就像一些物联网应用那样，完全跑在裸设备上。另一方面，有了操作系统的帮助，可以为程序提供便利，比如可以使用超过物理内存的存储空间，操作系统负责进行虚拟内存的管理。 在存在操作系统的情况下，因为很多进程共享计算机资源，所以就要遵循一些约定。这就仿佛办公室是所有同事共享的，那么大家就都要遵守一些约定，如果一个人大声喧哗，就会影响到其他人。 程序需要遵守的约定包括：程序文件的二进制格式约定，这样操作系统才能程序正确地加载进来，并为同一个程序的多个进程共享代码区。在使用寄存器和栈的时候也要遵守一些约定，便于操作系统在不同的进程之间切换的时候、在做系统调用的时候，做好上下文的保护。 所以，我们编译程序的时候，要知道需要遵守哪些约定。因为就算是使用同样的 CPU，针对不同的操作系统，编译的结果也是非常不同的。好了，我们了解了程序运行时的硬件和操作系统环境。接下来，我们看看程序运行时，是怎么跟它们互动的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:27:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"程序运行的过程 你天天运行程序，可对于程序运行的细节，真的清楚吗？ 程序运行的细节首先，可运行的程序一般是由操作系统加载到内存的，并且定位到代码区里程序的入口开始执行。比如，C 语言的 main 函数的第一行代码。每次加载一条代码，程序都会顺序执行，碰到跳转语句，才会跳到另一个地址执行。CPU 里有一个指令寄存器，里面保存了下一条指令的地址。 假设我们运行这样一段代码编译后形成的程序： int main(){ int a = 1; foo(3); bar(); } int foo(int c){ int b = 2; return b+c; } int bar(){ return foo(4) + 1; } 我们首先激活（Activate）main() 函数，main() 函数又激活 foo() 函数，然后又激活 bar() 函数，bar() 函数还会激活 foo() 函数，其中 foo() 函数被两次以不同的路径激活。 我们把每次调用一个函数的过程，叫做一次活动（Activation）。每个活动都对应一个活动记录（Activation Record），这个活动记录里有这个函数运行所需要的信息，比如参数、返回值、本地变量等。目前我们用栈来管理内存，所以可以把活动记录等价于栈桢。栈桢是活动记录的实现方式，我们可以自由设计活动记录或栈桢的结构，下图是一个常见的设计： 返回值：一般放在最顶上，这样它的地址是固定的。foo() 函数返回以后，它的调用者可以到这里来取到返回值。在实际情况中，我们会优先通过寄存器来传递返回值，比通过内存传递性能更高。参数：在调用 foo 函数时，把参数写到这个地址里。同样，我们也可以通过寄存器来传递，而不是内存。控制链接：就是上一级栈桢的地址。如果用到了上一级作用域中的变量，就可以顺着这个链接找到上一级栈桢，并找到变量的值。返回地址：foo 函数执行完毕以后，继续执行哪条指令。同样，我们可以用寄存器来保存这个信息。本地变量：foo 函数的本地变量 b 的存储空间。寄存器信息：我们还经常在栈桢里保存寄存器的数据。如果在 foo 函数里要使用某个寄存器，可能需要先把它的值保存下来，防止破坏了别的代码保存在这里的数据。这种约定叫做被调用者责任，也就是使用寄存器的人要保护好寄存器里原有的信息。某个函数如果使用了某个寄存器，但它又要调用别的函数，为了防止别的函数把自己放在寄存器中的数据覆盖掉，要自己保存在栈桢中。这种约定叫做调用者责任。 你可以看到，每个栈桢的长度是不一样的。用到的参数和本地变量多，栈桢就要长一点。但是，栈桢的长度和结构是在编译期就能完全确定的。这样就便于我们计算地址的偏移量，获取栈桢里某个数据。总的来说，栈桢的设计很自由。但是，你要考虑不同语言编译形成的模块要能够链接在一起，所以还是要遵守一些公共的约定的，否则，你写的函数，别人就没办法调用了。在08 讲，我提到过栈桢，这次我们用了更加贴近具体实现的描述：栈桢就是一块确定的内存，变量就是这块内存里的地址。在下一讲，我会带你动手实现我们的栈桢。 从全局角度看整个运行过程了解了栈桢的实现之后，我们再来看一个更大的场景，从全局的角度看看整个运行过程中都发生了什么。 代码区里存储了一些代码，main 函数、bar 函数和 foo 函数各自有一段连续的区域来存储代码，我用了一些汇编指令来表示这些代码（实际运行时这里其实是机器码）。假设我们执行到 foo 函数中的一段指令，来计算“b+c”的值，并返回。这里用到了 mov、add、jmp 这三个指令。mov 是把某个值从一个地方拷贝到另一个地方，add 是往某个地方加一个值，jmp 是改变代码执行的顺序，跳转到另一个地方去执行（汇编命令的细节，我们下节再讲，你现在简单了解一下就行了）。 mov b的地址 寄存器1 add c的地址 寄存器1 mov 寄存器1 foo的返回值地址 jmp 返回地址 //或ret指令 执行完这几个指令以后，foo 的返回值位置就写入了 6，并跳转到 bar 函数中执行 foo 之后的代码。这时，foo 的栈桢就没用了，新的栈顶是 bar 的栈桢的顶部。理论上讲，操作系统这时可以把 foo 的栈桢所占的内存收回了。比如，可以映射到另一个程序的寻址空间，让另一个程序使用。但是在这个例子中你会看到，即使返回了 bar 函数，我们仍要访问栈顶之外的一个内存地址，也就是返回值的地址。所以，目前的调用约定都规定，程序的栈顶之外，仍然会有一小块内存（比如 128K）是可以由程序访问的，比如我们可以拿来存储返回值。这一小段内存操作系统并不会回收。我们目前只讲了栈，堆的使用也类似，只不过是要手工进行申请和释放，比栈要多一些维护工作。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:27:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我带你了解了程序运行的环境和过程，我们的程序主要跟 CPU、内存，以及操作系统打交道。你需要了解的重点如下：CPU 上运行程序的指令，运行过程中要用到寄存器、高速缓存来提高指令和数据的存取效率。内存可以划分成不同的区域保存代码、静态数据，并用栈和堆来存放运行时产生的动态数据。操作系统会把物理的内存映射成进程的寻址空间，同一份代码会被映射进多个进程的内存空间，操作系统的公共库也会被映射进进程的内存空间，操作系统还会自动维护栈。 程序在运行时顺序执行代码，可以根据跳转指令来跳转；栈被划分成栈桢，栈桢的设计有一定的自由度，但通常也要遵守一些约定；栈桢的大小和结构在编译时就能决定；在运行时，栈桢作为活动记录，不停地被动态创建和释放。 以上这些内容就是一个程序运行时的秘密。你再面对代码时，脑海里就会想象出它是怎样跟 CPU、内存和操作系统打交道的了。而且有了这些背景知识，你也可以让编译器生成代码，按照本节课所说的模式运行了！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:27:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"22 | 生成汇编代码（一）：汇编语言其实不难学 敲黑板：课程用的是 GNU 汇编器，macOS 和 Linux 已内置，本文的汇编语言的写法是 GNU 汇编器规定的写法。Windows 系统可安装 MinGW 或 Linux 虚拟机。 对于静态编译型语言，比如 C 语言和 Go 语言，编译器后端的任务就是生成汇编代码，然后再由汇编器生成机器码，生成的文件叫目标文件，最后再使用链接器就能生成可执行文件或库文件了。 就算像 JavaScript 这样的解释执行的语言，也要在运行时利用类似的机制生成机器码，以便调高执行的速度。Java 的字节码，在运行时通常也会通过 JIT 机制编译成机器码。而汇编语言是完成这些工作的基础。 对你来说，掌握汇编语言是十分有益的，因为哪怕掌握一小点儿汇编技能，就能应用到某项工作中，比如，在 C 语言里嵌入汇编，实现某个特殊功能；或者读懂某些底层类库或驱动程序的代码，因为它可能是用汇编写的。本节课，我先带你了解一下汇编语言，来个破冰之旅。然后在接下来的课程中再带你基于 AST 手工生成汇编代码，破除你对汇编代码的恐惧，了解编译期后端生成汇编代码的原理。以后，当你看到高级语言的代码，以及 IR 时，就可以想象出来它对应的汇编代码是什么样子，实现从上层到底层认知的贯通。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:28:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"了解汇编语言 机器语言都是 0101 的二进制的数据，不适合我们阅读。而汇编语言，简单来说，是可读性更好的机器语言，基本上每条指令都可以直接翻译成一条机器码。跟你日常使用的高级语言相比，汇编语言的语法特别简单，但它要跟硬件（CPU 和内存）打交道，我们来体会一下。计算机的处理器有很多不同的架构，比如 x86-64、ARM、Power 等，每种处理器的指令集都不相同，那也就意味着汇编语言不同。我们目前用的电脑，CPU 一般是 x86-64 架构，是 64 位机。（如不做特别说明，本课程都是以 x86-64 架构作为例子的）。说了半天，汇编代码长什么样子呢？我用 C 语言写的例子来生成一下汇编代码。 #include \u003cstdio.h\u003e int main(int argc, char* argv[]){ printf(\"Hello %s!\\n\", \"Richard\"); return 0; } 在 macOS 中输入下面的命令，其中的 -S 参数就是告诉编译器把源代码编译成汇编代码，而 -O2 参数告诉编译器进行 2 级优化，这样生成的汇编代码会短一些： clang -S -O2 hello.c -o hello.s 或者： gcc -S -O2 hello.c -o hello.s 生成的汇编代码是下面的样子： .section __TEXT,__text,regular,pure_instructions .build_version macos, 10, 14 sdk_version 10, 14 .globl _main ## -- Begin function main .p2align 4, 0x90 _main: ## @main .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq %rsp, %rbp .cfi_def_cfa_register %rbp leaq L_.str(%rip), %rdi leaq L_.str.1(%rip), %rsi xorl %eax, %eax callq _printf xorl %eax, %eax popq %rbp retq .cfi_endproc ## -- End function .section __TEXT,__cstring,cstring_literals L_.str: ## @.str .asciz \"Hello %s!\\n\" L_.str.1: ## @.str.1 .asciz \"Richard\" .subsections_via_symbols 你如果再打下面的命令，就会把这段汇编代码编译成可执行文件（在 macOS 或 Linux 执行 as 命令，就是调用了 GNU 的汇编器）： as hello.s -o hello.o //用汇编器编译成目标文件 gcc hello.o -o hello //链接成可执行文件 ./hello //运行程序 以上面的代码为例，来看一下汇编语言的组成元素。这是汇编语言入门的基础，也是重点内容，在阅读时，你不需要死记硬背，而是要灵活掌握，比如 CPU 的指令特别多，我们记住常用的就行了，不太常用的可以去查手册。 汇编语言的组成元素 这段代码里有指令、伪指令、标签和注释四种元素，每个元素单独占一行。指令（instruction）是直接由 CPU 进行处理的命令，例如： pushq %rbp movq %rsp, %rbp 其中，开头的一个单词是助记符（mnemonic），后面跟着的是操作数（operand），有多个操作数时以逗号分隔。第二行代码的意思是把数据从这里（源）拷贝到那里（目的），这跟“请倒杯咖啡给我”这样的自然语句是一样的，先是动词（倒），然后是动词的作用对象（咖啡），再就是目的地（给我）。 伪指令以“.”开头，末尾没有冒号“：”。 .section __TEXT,__text,regular,pure_instructions .globl _main .asciz \"Hello %s!\\n\" 伪指令是是辅助性的，汇编器在生成目标文件时会用到这些信息，但伪指令不是真正的 CPU 指令，就是写给汇编器的。每种汇编器的伪指令也不同，要查阅相应的手册。标签以冒号“:”结尾，用于对伪指令生成的数据或指令做标记。例如 L_.str: 标签是对一个字符串做了标记。其他代码可以访问标签，例如跳转到这个标签所标记的指令。 L_.str: ## @.str .asciz \"Hello %s!\\n\" 标签很有用，它可以代表一段代码或者常量的地址（也就是在代码区或静态数据区中的位置）。可一开始，我们没法知道这个地址的具体值，必须生成目标文件后，才能算出来。所以，标签会简化汇编代码的编写。 **第四种元素，注释，以“#”号开头，这跟 C 语言中以 // 表示注释语句是一样的。**因为指令是汇编代码的主要部分，所以我们再把与指令有关的知识点展开讲解一下。 详细了解指令这个元素在代码中，助记符“movq”“xorl”中的“mov”和“xor”是指令，而“q”和“l”叫做后缀，表示操作数的位数。后缀一共有 b, w, l, q 四种，分别代表 8 位、16 位、32 位和 64 位。 比如，movq 中的 q 代表操作数是 8 个字节，也就是 64 位的。movq 就是把 8 字节从一个地方拷贝到另一个地方，而 movl 则是拷贝 4 个字节。而在指令中使用操作数，可以使用四种格式，它们分别是：立即数、寄存器、直接内存访问和间接内存访问。立即数以 $ 开头， 比如 $40。（下面这行代码是把 40 这个数字拷贝到 %eax 寄存器）。 movl $40, %eax 除此之外，我们在指令中最常见到的就是对寄存器的访问，GNU 的汇编器规定寄存器一定要以 % 开头。 直接内存访问：当我们在代码中看到操作数是一个数字时，它其实指的是内存地址。不要误以为它是一个数字，因为数字立即数必须以 $ 开头。另外，汇编代码里的标签，也会被翻译成直接内存访问的地址。比如“callq _printf”中的“_printf”是一个函数入口的地址。汇编器帮我们计算出程序装载在内存时，每个字面量和过程的地址。间接内存访问：带有括号，比如（%rbp），它是指 %rbp 寄存器的值所指向的地址。 间接内存访问的完整形式是：偏移量（基址，索引值，字节数）这样的格式。其地址是：基址 + 索引值 * 字节数 + 偏移量举例来说：8(%rbp)，是比 %rbp 寄存器的值加 8。-8(%rbp)，是比 %rbp 寄存器的值减 8。（%rbp, %eax, 4）的值，等于 %rbp + %eax*4。这个地址格式相当于访问 C 语言中的数组中的元素，数组元素是 32 位的整数，其索引值是 %eax，而数组的起始位置是 %rbp。其中字节数只能取 1,2,4,8 四个值。 你现在应该对指令的格式有所了解了，接下来，我们再学几个常用的指令： mov 指令 mov 寄存器|内存|立即数, 寄存器|内存 这个指令最常用到，用于在寄存器或内存之间传递数据，或者把立即数加载到内存或寄存器。mov 指令的第一个参数是源，可以是寄存器、内存或立即数。第二个参数是目的地，可以是寄存器或内存。 lea 指令，lea 是“load effective address”的意思，装载有效地址。 lea 源，目的 比如前面例子代码中的 leaq 指令，是把字符串的地址加载到 %rdi 寄存器。 leaq L_.str(%rip), %rdi add 指令是做加法运算，它可以采取下面的格式： add 立即数， 寄存器 add 寄存器， 寄存器 add 内存， 寄存器 add 立即数， 内存 add 寄存器， 内存 比如，典型的 c=a+b 这样一个算术运算可能是这样的： movl -4(%rbp), %eax #把%rbp-4的值拷贝到%eax addl -8(%rbp), %eax #把%rbp-8地址的值加到%eax上 movl %eax, -12(%rbp) #把%eax的值写到内存地址%rbp-12 这三行代码，分别是操作 a、b、c 三个变量的地址。它们的地址分别比 %rbp 的值减 4、减 8、减 12，因此 a、b、c 三个变量每个都是 4 个字节长，也就是 32 位，它们是紧挨着存放的，并且是从高地址向低地址延伸的，这是栈的特征。除了 add 以外，其他算术运算的指令： 与栈有关的操作： 跳转类： 过程调用： 比较操作： 以上我列举的指令，是你在编写汇编代码时，经常会用到的，比较重要，会满足你编写简单汇编程序的需求，所以你需要重点关注。x86-64 是复杂指令集的处理器，有非常多的指令，差不多有上千条，全部记住是比较难的。更好的办法，是记住主要的指令，其他指令在使用时去查Intel 公司的手册，在这里我就不举例了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:28:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"x86-64 架构的寄存器 在汇编代码中，我们经常要使用各种以 % 开头的寄存器的符号。初学者阅读这些代码时，通常会有一些疑问：有几个寄存器可以用？我可以随便用它们吗？使用不当会不会造成错误？等等。所以，有必要让你熟悉一下这些寄存器，了解它们的使用方法。 x86-64 架构的 CPU 里有很多寄存器，我们在代码里最常用的是 16 个 64 位的通用寄存器，分别是：%rax，%rbx，%rcx，%rdx，%rsi，%rdi，%rbp，%rsp， %r8，%r9，%r10，%r11，%r12，%r13，%r14，%r15。 这些寄存器在历史上有各自的用途，比如，rax 中的“a”，是 Accumulator(累加器) 的意思，这个寄存器是累加寄存器。但随着技术的发展，这些寄存器基本上都成为了通用的寄存器，不限于某种特定的用途。但是，为了方便软件的编写，我们还是做了一些约定，给这些寄存器划分了用途。针对 x86-64 架构有多个调用约定（Calling Convention），包括微软的 x64 调用约定（Windows 系统）、System V AMD64 ABI（Unix 和 Linux 系统）等，下面的内容属于后者： %rax 除了其他用途外，通常在函数返回的时候，把返回值放在这里。%rsp 作为栈指针寄存器，指向栈顶。%rdi，%rsi，%rdx，%rcx，%r8，%r9 给函数传整型参数，依次对应第 1 参数到第 6 参数。超过 6 个参数怎么办？放在栈桢里，我们21 讲已经讲过了。如果程序要使用 %rbx，%rbp，%r12，%r13，%r14，%r15 这几个寄存器，是由被调用者（Callee）负责保护的，也就是写到栈里，在返回的时候要恢复这些寄存器中原来的内容。其他寄存器的内容，则是由调用者（Caller）负责保护，如果不想这些寄存器中的内容被破坏，那么要自己保护起来。 上面这些寄存器的名字都是 64 位的名字，对于每个寄存器，我们还可以只使用它的一部分，并且另起一个名字。比如对于 %rax，如果使用它的前 32 位，就叫做 %eax，前 16 位叫 %ax，前 8 位（0 到 7 位）叫 %al，8 到 15 位叫 %ah。 其他的寄存器也有这样的使用方式，当你在汇编代码中看到这些名称的时候，你就知道其实它们有可能在物理上是同一个寄存器。 除了通用寄存器以外，有可能的话，还要了解下面的寄存器和它们的用途，我们写汇编代码时也经常跟它们发生关联：8 个 80 位的 x87 寄存器，用于做浮点计算；8 个 64 位的 MMX 寄存器，用于 MMX 指令（即多媒体指令），这 8 个跟 x87 寄存器在物理上是相同的寄存器。在传递浮点数参数的时候，要用 mmx 寄存器。16 个 128 位的 SSE 寄存器，用于 SSE 指令。我们将在应用篇里使用 SSE 指令，讲解 SIMD 的概念。指令寄存器，rip，保存指令地址。CPU 总是根据这个寄存器来读取指令。flags（64 位：rflags, 32 位：eflags）寄存器：每个位用来标识一个状态。比如，它们会用于比较和跳转的指令，比如 if 语句翻译成的汇编代码，就会用它们来保存 if 条件的计算结果。 总的来说，我们的汇编代码处处要跟寄存器打交道，正确和高效使用寄存器，是编译期后端的重要任务之一。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:28:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我讲解了汇编语言的一些基础知识，由于汇编语言的特点，涉及的知识点和细节比较多，在这个过程中，你无需死记硬背，只需要掌握几个重点内容：1. 汇编语言是由指令、标签、伪指令和注释构成的。其中主要内容都是指令。指令包含一个该指令的助记符和操作数。操作数可以使用直接数、寄存器，以及用两种方式访问内存地址。2. 汇编指令中会用到一些通用寄存器。这些寄存器除了用于计算以外，还可以根据调用约定帮助传递参数和返回值。使用寄存器时，要区分由调用者还是被调用者负责保护寄存器中原来的内容。 另外，我们还要注意按照一定的规则维护和使用栈桢，这个知识点会在后面的加餐中展开来讲一个例子。鉴于你可能是第一次使用汇编语言，所以我提供两个建议，让你快速上手汇编语言： 你可以用 C 语言写一些示例代码，然后用编译器生成汇编代码，看看能否看懂。2. 模仿文稿中的例子，自己改写并运行你自己的汇编程序，这个过程中，你会发现真的没那么难。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:28:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"加餐 | 汇编代码编程与栈帧管理 在22 讲中，我们侧重讲解了汇编语言的基础知识，包括构成元素、汇编指令和汇编语言中常用的寄存器。学习完基础知识之后，你要做的就是多加练习，和汇编语言“混熟”。小窍门是查看编译器所生成的汇编代码，跟着学习体会。不过，可能你是初次使用汇编语言，对很多知识点还会存在疑问，比如： 在汇编语言里调用函数（过程）时，传参和返回值是怎么实现的呢？21 讲中运行期机制所讲的栈帧，如何通过汇编语言实现？条件语句和循环语句如何实现？…… 为此，我策划了一期加餐，针对性地讲解这样几个实际场景，希望帮你加深对汇编语言的理解。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:29:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"示例 1：过程调用和栈帧 这个例子涉及了一个过程调用（相当于 C 语言的函数调用）。过程调用是汇编程序中的基础结构，它涉及到栈帧的管理、参数的传递这两个很重要的知识点。假设我们要写一个汇编程序，实现下面 C 语言的功能： /*function-call1.c */ #include \u003cstdio.h\u003e int fun1(int a, int b){ int c = 10; return a+b+c; } int main(int argc, char *argv[]){ printf(\"fun1: %d\\n\", fun1(1,2)); return 0; } fun1 函数接受两个整型的参数：a 和 b，来看看这两个参数是怎样被传递过去的，手写的汇编代码如下： # function-call1-craft.s 函数调用和参数传递 # 文本段,纯代码 .section __TEXT,__text,regular,pure_instructions _fun1: # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来 movq %rsp, %rbp # 把调用者的栈帧顶部地址,设置为本栈帧的底部 subq $4, %rsp # 扩展栈 movl $10, -4(%rbp) # 变量c赋值为10，也可以写成 movl $10, (%rsp) # 做加法 movl %edi, %eax # 第一个参数放进%eax addl %esi, %eax # 把第二个参数加到%eax,%eax同时也是存放返回值的寄存器 addl -4(%rbp), %eax # 加上c的值 addq $4, %rsp # 缩小栈 # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 retq # 返回 .globl _main # .global伪指令让_main函数外部可见 _main: ## @main # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来 movq %rsp, %rbp # 把调用者的栈帧顶部地址,设置为本栈帧的底部 # 设置第一个和第二个参数,分别为1和2 movl $1, %edi movl $2, %esi callq _fun1 # 调用函数 # 为pritf设置参数 leaq L_.str(%rip), %rdi # 第一个参数是字符串的地址 movl %eax, %esi # 第二个参数是前一个参数的返回值 callq _printf # 调用函数 # 设置返回值。这句也常用 xorl %esi, %esi 这样的指令,都是置为零 movl $0, %eax # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 retq # 返回 # 文本段,保存字符串字面量 .section __TEXT,__cstring,cstring_literals L_.str: ## @.str .asciz \"Hello World! :%d \\n\" 需要注意，手写的代码跟编译器生成的可能有所不同，但功能是等价的，代码里有详细的注释，你肯定能看明白。借用这个例子，我们讲一下栈的管理。在示例代码的两个函数里，有这样的固定结构： # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来 movq %rsp, %rbp # 把调用者的栈帧顶部地址，设置为本栈帧的底部 ... # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 在 C 语言生成的代码中，一般用 %rbp 寄存器指向栈帧的底部，而 %rsp 则指向栈帧的顶部。栈主要是通过 push 和 pop 这对指令来管理的：push 把操作数压到栈里，并让 %rsp 指向新的栈顶，pop 把栈顶数据取出来，同时调整 %rsp 指向新的栈顶。在进入函数的时候，用 pushq %rbp 指令把调用者的栈帧地址存起来（根据调用约定保护起来），而把调用者的栈顶地址设置成自己的栈底地址，它等价于下面两条指令，你可以不用 push 指令，而是运行下面两条指令： subq $8, %rsp #把%rsp的值减8，也就是栈增长8个字节，从高地址向低地址增长 movq %rbp, (%rsp) #把%rbp的值写到当前栈顶指示的内存位置 而在退出函数前，调用了 popq %rbp 指令。它恢复了之前保存的栈指针的地址，等价于下面两条指令： movq (%rsp), %rbp #把栈顶位置的值恢复回%rbp，这是之前保存在栈里的值。 addq $8, %rsp #把%rsp的值加8，也就是栈减少8个字节 上述过程画成一张直观的图，表示如下： 上面每句指令执行以后，我们看看 %rbp 和 %rsp 值的变化： 再来看看使用局部变量的时候会发生什么： subq $4, %rsp # 扩展栈 movl $10, -4(%rbp) # 变量c赋值为10，也可以写成 movl $10, (%rsp) ... addq $4, %rsp # 缩小栈 我们通过减少 %rsp 的值，来扩展栈，然后在扩展出来的 4 个字节的位置上写入整数，这就是变量 c 的值。在返回函数前，我们通过 addq $4, %rsp 再把栈缩小。这个过程如下图所示： 在这个例子中，我们通过移动 %rsp 指针来改变帧的大小。%rbp 和 %rsp 之间的空间就是当前栈帧。而过程调用和退出过程，分别使用 call 指令和 ret 指令。“callq _fun1”是调用 _fun1 过程，这个指令相当于下面两句代码，它用到了栈来保存返回地址： pushq %rip # 保存下一条指令的地址，用于函数返回继续执行 jmp _fun1 # 跳转到函数_fun1 _fun1 函数用 ret 指令返回，它相当于： popq %rip #恢复指令指针寄存器 jmp %rip 上一讲，我提到，在 X86-64 架构下，新的规范让程序可以访问栈顶之外 128 字节的内存，所以，我们甚至不需要通过改变 %rsp 来分配栈空间，而是直接用栈顶之外的空间。上面的示例程序，你可以用 as 命令生成可执行程序，运行一下看看，然后试着做一下修改，逐步熟悉汇编程序的编写思路。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:29:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"示例 2：同时使用寄存器和栈来传参 上一个示例中，函数传参只使用了两个参数，这时是通过两个寄存器传递参数的。这次，我们使用 8 个参数，来看看通过寄存器和栈传参这两种不同的机制。在 X86-64 架构下，有很多的寄存器，所以程序调用约定中规定尽量通过寄存器来传递参数，而且，只要参数不超过 6 个，都可以通过寄存器来传参，使用的寄存器如下： 超过 6 个的参数的话，我们要再加上栈来传参： 根据程序调用约定的规定，参数 1～6 是放在寄存器里的，参数 7 和 8 是放到栈里的，先放参数 8，再放参数 7。在 23 讲，我会带你为下面的一段 playscript 程序生成汇编代码： //asm.play int fun1(int x1, int x2, int x3, int x4, int x5, int x6, int x7, int x8){ int c = 10; return x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + c; } println(\"fun1:\" + fun1(1,2,3,4,5,6,7,8)); 现在，我们可以按照调用约定，先手工编写一段实现相同功能的汇编代码： # function-call2-craft.s 函数调用和参数传递 # 文本段,纯代码 .section __TEXT,__text,regular,pure_instructions _fun1: # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来 movq %rsp, %rbp # 把调用者的栈帧顶部地址,设置为本栈帧的底部 movl $10, -4(%rbp) # 变量c赋值为10,也可以写成 movl $10, (%rsp) # 做加法 movl %edi, %eax # 第一个参数放进%eax addl %esi, %eax # 加参数2 addl %edx, %eax # 加参数3 addl %ecx, %eax # 加参数4 addl %r8d, %eax # 加参数5 addl %r9d, %eax # 加参数6 addl 16(%rbp), %eax # 加参数7 addl 24(%rbp), %eax # 加参数8 addl -4(%rbp), %eax # 加上c的值 # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 retq # 返回 .globl _main # .global伪指令让_main函数外部可见 _main: ## @main # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来 movq %rsp, %rbp # 把调用者的栈帧顶部地址,设置为本栈帧的底部 subq $16, %rsp # 这里是为了让栈帧16字节对齐，实际使用可以更少 # 设置参数 movl $1, %edi # 参数1 movl $2, %esi # 参数2 movl $3, %edx # 参数3 movl $4, %ecx # 参数4 movl $5, %r8d # 参数5 movl $6, %r9d # 参数6 movl $7, (%rsp) # 参数7 movl $8, 8(%rsp) # 参数8 callq _fun1 # 调用函数 # 为pritf设置参数 leaq L_.str(%rip), %rdi # 第一个参数是字符串的地址 movl %eax, %esi # 第二个参数是前一个参数的返回值 callq _printf # 调用函数 # 设置返回值。这句也常用 xorl %esi, %esi 这样的指令,都是置为零 movl $0, %eax addq $16, %rsp # 缩小栈 # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 retq # 返回 # 文本段,保存字符串字面量 .section __TEXT,__cstring,cstring_literals L_.str: ## @.str .asciz \"fun1 :%d \\n\" 用 as 命令，把这段汇编代码生成可执行文件，运行后会输出结果：“fun1: 46”。 as functio-call2-craft.s -o function-call2 ./function-call2 这段程序虽然有点儿长，但思路很清晰，比如，每个函数（过程）都有固定的结构。7～10 行，我叫做序曲，是设置栈帧的指针；25~26 行，我叫做尾声，是恢复栈底指针并返回；13~22 行是做一些计算，还要为本地变量在栈里分配一些空间。我建议你读代码的时候，对照着每行代码的注释，弄清楚这条代码所做的操作，以及相关的寄存器和内存中值的变化，脑海里有栈帧和寄存器的直观的结构，就很容易理解清楚这段代码了。除了函数调用以外，我们在编程时经常使用循环语句和 if 语句，它们转换成汇编是什么样子呢？我们来研究一下，首先看看 while 循环语句。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:29:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"示例 3：循环语句的汇编码解析 看看下面这个 C 语言的语句： c void fun1(int a){ while (a \u003c 10){ a++; } } 我们要使用\"gcc -S ifstmt.c -o ifstmt.s\"命令，把它转换成汇编语句（注意不要带优化参数）： .section __TEXT,__text,regular,pure_instructions .macosx_version_min 10, 15 .globl _fun1 ## -- Begin function fun1 .p2align 4, 0x90 _fun1: ## @fun1 .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq %rsp, %rbp .cfi_def_cfa_register %rbp movl %edi, -4(%rbp) #把参数a放到栈里 LBB0_1: ## =\u003eThis Inner Loop Header: Depth=1 cmpl $10, -4(%rbp) #比较参数1和立即数10,设置eflags寄存器 jge LBB0_3 #如果大于等于，则跳转到LBB0_3基本块 ## %bb.2: ## in Loop: Header=BB0_1 Depth=1 movl -4(%rbp), %eax #这2行，是给a加1 addl $1, %eax movl %eax, -4(%rbp) jmp LBB0_1 LBB0_3: popq %rbp retq .cfi_endproc ## -- End function .subsections_via_symbols 这段代码的 15、16、21 行是关键，我解释一下：第 15 行，用 cmpl 指令，将 %edi 寄存器中的参数 1（即 C 代码中的参数 a）和立即数 10 做比较，比较的结果会设置 EFLAGS 寄存器中的相关位。EFLAGS 中有很多位，下图是Intel 公司手册中对各个位的解释，有的指令会影响这些位的设置，比如 cmp 指令，有的指令会从中读取信息，比如 16 行的 jge 指令： 第 16 行，jge 指令。jge 是“jump if greater or equal”的缩写，也就是当大于或等于的时候就跳转。大于等于是从哪知道的呢？就是根据 EFLAGS 中的某些位计算出来的。第 21 行，跳转到循环的开始。 在这个示例中，我们看到了 jmp（无条件跳转指令）和 jge（条件跳转指令）两个跳转指令。条件跳转指令很多，它们分别是基于 EFLAGS 的状态位做不同的计算，判断是否满足跳转条件，看看下面这张表格： 表格中的跳转指令，是基于有符号的整数进行判断的，对于无符号整数、浮点数，还有很多其他的跳转指令。现在你应该体会到，汇编指令为什么这么多了。好在其助记符都是有规律的，可以看做英文缩写，所以还比较容易理解其含义。 另外我再强调一下，刚刚我让你生成汇编时，不要带优化参数，那是因为优化算法很“聪明”，它知道这个循环语句对函数最终的计算结果没什么用，就优化掉了。后面学优化算法时，你会理解这种优化机制。不过这样做，也会有一个不好的影响，就是代码不够优化。比如这段代码把参数 1 拷贝到了栈里，在栈里做运算，而不是直接基于寄存器做运算，这样性能会低很多，这是没有做寄存器优化的结果。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:29:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"示例 4：if 语句的汇编码解析 循环语句看过了，if 语句如何用汇编代码实现呢？看看下面这段代码： int fun1(int a){ if (a \u003e 10){ return 4; } else{ return 8; } } 把上面的 C 语言代码转换成汇编代码如下： .section __TEXT,__text,regular,pure_instructions .macosx_version_min 10, 15 .globl _fun1 ## -- Begin function fun1 .p2align 4, 0x90 _fun1: ## @fun1 .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq %rsp, %rbp .cfi_def_cfa_register %rbp movl %edi, -8(%rbp) cmpl $10, -8(%rbp) #将参数a与10做比较 jle LBB0_2 #小于等于的话就调转到LBB0_2基本块 ## %bb.1: movl $4, -4(%rbp) #否则就给a赋值为4 jmp LBB0_3 LBB0_2: movl $8, -4(%rbp) #给a赋值为8 LBB0_3: movl -4(%rbp), %eax #设置返回值 popq %rbp retq .cfi_endproc ## -- End function .subsections_via_symbols 了解了条件跳转指令以后，再理解上面的代码容易了很多。还是先做比较，设置 EFLAGS 中的位，然后做跳转。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:29:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"示例 5：浮点数的使用 之前我们用的例子都是采用整数，现在使用浮点数来做运算，看看会有什么不同。看看下面这段代码： c float fun1(float a, float b){ float c = 2.0; return a + b + c; } 使用 -O2 参数，把 C 语言的程序编译成汇编代码如下： s .section __TEXT,__text,regular,pure_instructions .macosx_version_min 10, 15 .section __TEXT,__literal4,4byte_literals .p2align 2 ## -- Begin function fun1 LCPI0_0: .long 1073741824 ## float 2 常量 .section __TEXT,__text,regular,pure_instructions .globl _fun1 .p2align 4, 0x90 _fun1: ## @fun1 .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq %rsp, %rbp .cfi_def_cfa_register %rbp addss %xmm1, %xmm0 #浮点数传参用XMM寄存器，加法用addss指令 addss LCPI0_0(%rip), %xmm0 #把常量2.0加到xmm0上，xmm0保存返回值 popq %rbp retq .cfi_endproc ## -- End function .subsections_via_symbols 这个代码的结构你应该熟悉了，栈帧的管理方式都是一样的，都要维护 %rbp 和 %rsp。不一样的地方，有几个地方：传参。给函数传递浮点型参数，是要使用 XMM 寄存器。指令。浮点数的加法运算，使用的是 addss 指令，它用于对单精度的标量浮点数做加法计算，这是一个 SSE1 指令。SSE1 是一组指令，主要是对单精度浮点数 (比如 C 或 Java 语言中的 float) 进行运算的，而 SSE2 则包含了一些双精度浮点数（比如 C 或 Java 语言中的 double）的运算指令。返回值。整型返回值是放在 %eax 寄存器中，而浮点数返回值是放在 xmm0 寄存器中的。调用者可以从这里取出来使用。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:29:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 利用本节课的加餐，我带你把编程中常见的一些场景，所对应的汇编代码做了一些分析。你需要记住的要点如下：函数调用时，会使用寄存器传参，超过 6 个参数时，还要再加上栈，这都是遵守了调用约定。通过 push、pop 指令来使用栈，栈与 %rbp 和 %rsp 这两个指针有关。你可以图形化地记住栈的增长和回缩的过程。需要注意的是，是从高地址向低地址走，所以访问栈里的变量，都是基于 %rbp 来减某个值。使用 %rbp 前，要先保护起来，别破坏了调用者放在里面的值。循环语句和 if 语句的秘密在于比较指令和有条件跳转指令，它们都用到了 EFLAGS 寄存器。浮点数的计算要用到 MMX 寄存器，指令也有所不同。 通过这次加餐，你会更加直观地了解汇编语言，接下来的课程中，我会带你尝试通过翻译 AST 自动生成这些汇编代码，让你直观理解编译器生成汇编码的过程。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:29:6","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"23 | 生成汇编代码（二）：把脚本编译成可执行文件 学完两节课之后，对于后端编译过程，你可能还会产生一些疑问，比如：1. 大致知道汇编程序怎么写，却不知道如何从 AST 生成汇编代码，中间有什么挑战。2. 编译成汇编代码之后需要做什么，才能生成可执行文件。本节课，我会带你真正动手，基于 AST 把 playscript 翻译成正确的汇编代码，并将汇编代码编译成可执行程序。通过这样一个过程，可以实现从编译器前端到后端的完整贯通，帮你对编译器后端工作建立比较清晰的认识。这样一来，你在日常工作中进行大型项目的编译管理的时候，或者需要重用别人的类库的时候，思路会更加清晰。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:30:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"从 playscript 生成汇编代码 先来看看如何从 playscript 生成汇编代码。我会带你把 playscript 的几个有代表性的功能，而不是全部的功能翻译成汇编代码，一来工作量少一些，二来方便做代码优化。这几个有代表性的功能如下： 支持函数调用和传参（这个功能可以回顾加餐）。2. 支持整数的加法运算（在这个过程中要充分利用寄存器提高性能）。3. 支持变量声明和初始化。 具体来说，要能够把下面的示例程序正确生成汇编代码： //asm.play int fun1(int x1, int x2, int x3, int x4, int x5, int x6, int x7, int x8){ int c = 10; return x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + c; } println(\"fun1:\" + fun1(1,2,3,4,5,6,7,8)); 在加餐中，我提供了一段手写的汇编代码，功能等价于这段 playscript 代码，并讲述了如何在多于 6 个参数的情况下传参，观察栈帧的变化过程，你可以看看下面的图片和代码，回忆一下： s # function-call2-craft.s 函数调用和参数传递 # 文本段,纯代码 .section __TEXT,__text,regular,pure_instructions _fun1: # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来 movq %rsp, %rbp # 把调用者的栈帧顶部地址,设置为本栈帧的底部 movl $10, -4(%rbp) # 变量c赋值为10,也可以写成 movl $10, (%rsp) # 做加法 movl %edi, %eax # 第一个参数放进%eax addl %esi, %eax # 加参数2 addl %edx, %eax # 加参数3 addl %ecx, %eax # 加参数4 addl %r8d, %eax # 加参数5 addl %r9d, %eax # 加参数6 addl 16(%rbp), %eax # 加参数7 addl 24(%rbp), %eax # 加参数8 addl -4(%rbp), %eax # 加上c的值 # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 retq # 返回 .globl _main # .global伪指令让_main函数外部可见 _main: ## @main # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来 movq %rsp, %rbp # 把调用者的栈帧顶部地址,设置为本栈帧的底部 subq $16, %rsp # 这里是为了让栈帧16字节对齐，实际使用可以更少 # 设置参数 movl $1, %edi # 参数1 movl $2, %esi # 参数2 movl $3, %edx # 参数3 movl $4, %ecx # 参数4 movl $5, %r8d # 参数5 movl $6, %r9d # 参数6 movl $7, (%rsp) # 参数7 movl $8, 8(%rsp) # 参数8 callq _fun1 # 调用函数 # 为pritf设置参数 leaq L_.str(%rip), %rdi # 第一个参数是字符串的地址 movl %eax, %esi # 第二个参数是前一个参数的返回值 callq _printf # 调用函数 # 设置返回值。这句也常用 xorl %esi, %esi 这样的指令,都是置为零 movl $0, %eax addq $16, %rsp # 缩小栈 # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 retq # 返回 # 文本段,保存字符串字面量 .section __TEXT,__cstring,cstring_literals L_.str: ## @.str .asciz \"fun1 :%d \\n\" 接下来，我们动手写程序，从 AST 翻译成汇编代码（相关代码在 playscript-java 项目的AsmGen.java类里）。我们实现加法运算的翻译过程如下： case PlayScriptParser.ADD: //为加法运算申请一个临时的存储位置，可以是寄存器和栈 address = allocForExpression(ctx); bodyAsm.append(\"\\tmovl\\t\").append(left).append(\", \").append(address).append(\"\\n\"); //把左边节点拷贝到存储空间 bodyAsm.append(\"\\taddl\\t\").append(right).append(\", \").append(address).append(\"\\n\"); //把右边节点加上去 break; 这段代码的含义是：我们通过 allocForExpression() 方法，为每次加法运算申请一个临时空间（可以是寄存器，也可以是栈里的一个地址），用来存放加法操作的结果。接着，用 mov 指令把加号左边的值拷贝到这个临时空间，再用 add 指令加上右边的值。生成汇编代码的过程，基本上就是基于 AST 拼接字符串，其中 bodyAsm 变量是一个 StringBuffer 对象，我们可以用 StringBuffer 的 toString() 方法获得最后的汇编代码。按照上面的逻辑，针对“x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + c”这个表达式，形成的汇编代码如下： # 过程体 movl $10, -4(%rbp) movl %edi, %eax //x1 addl %esi, %eax //+x2 movl %eax, %ebx addl %edx, %ebx //+x3 movl %ebx, %r10d addl %ecx, %r10d //+x4 movl %r10d, %r11d addl %r8d, %r11d //+x5 movl %r11d, %r12d addl %r9d, %r12d //+x6 movl %r12d, %r13d addl 16(%rbp), %r13d //+x7 movl %r13d, %r14d addl 24(%rbp), %r14d //+x8 movl %r14d, %r15d addl -4(%rbp), %r15d //+c，本地变量 看出这个代码有什么问题了吗？我们每次执行加法运算的时候，都要占用一个新的寄存器。比如，x1+x2 使用了 %eax，再加 x3 时使用了 %ebx，按照这样的速度，寄存器很快就用完了，使用效率显然不高。所以必须要做代码优化。如果只是简单机械地翻译代码，相当于产生了大量的临时变量，每个临时变量都占用了空间： t1 := x1 + x2; t2 := t1 + x3; t3 := t2 + x4; ... 进行代码优化可以让不再使用的存储位置（t1，t2，t3…）能够复用，从而减少临时变量，也减少代码行数，优化后的申请临时存储空间的方法如下： //复用前序表达式的存储位置 if (ctx.bop != null \u0026\u0026 ctx.expression().size() \u003e= 2) { ExpressionContext left = ctx.expression(0); String leftAddress = tempVars.get(left); if (leftAddress!= null){ tempVars.put(ctx, leftAddress); //当前节点也跟这个地址关联起来 return leftAddress; } } 这段代码的意思是：对于每次加法运算，都要申请一个寄存器，如果加号左边的节点已经在某个寄存器中，那就直接复用这个寄存器，就不要用新的了。调整以后，生成的汇编代码就跟手写的一样了。而且，我们至始至终只用了 %eax 一个寄存器，代码数量也减少了一半，优化效果明显： # 过程体 movl $10, -4(%rbp) movl %edi, %eax addl %esi, %eax addl %edx, %eax addl %ecx, %eax addl %r8d, %eax addl %r9d, %eax addl 16(%rbp), %eax addl 24(%rbp), %eax addl -4(%rbp), %eax # 返回值 # 返回值在之前的计算中,已经存入%eax 对代码如何使用寄存器进行充分优化，是编译器后端一项必须要做的工作。这里只用了很粗糙的方法，不具备实用价值，后面可以学习更好的优化算法。弄清楚了加法运算的代码翻译逻辑，我们再看看 AsmGen.java 中的generate()方法和generateProcedure()方法，看看汇编代码完整的生成逻辑是怎样的。这样可以帮助你弄清楚整体脉络和所有的细节，比如函数的标签是怎么生成的，序曲和尾声是怎么加上去的，本地变量的地址是如何计算的，等等。 public String generate() { StringBuffer sb = new StringBuffer(); // 1.代码段的头 sb.append(\"\\t.section __TEXT,__text,regular,pure_in","date":"2022-07-18 23:55:09","objectID":"/cp_base/:30:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"通过 C 语言调用 playscript 模块 我们在编程的时候，经常调用一些公共的库实现一些功能，这些库可能是别的语言写的，但我们仍然可以调用。我们也可以实现 playscript 与其他语言的功能共享，在示例程序中实现很简单，微调一下生成的汇编代码，使用“.global _fun1”伪指令让 _fun1 过程变成全局的，这样其他语言写的程序就可以调用这个 _fun1 过程，实现功能的重用。 s # convention-fun1.s 测试调用约定，_fun1将在外部被调用 # 文本段,纯代码 .section __TEXT,__text,regular,pure_instructions .globl _fun1 # .global伪指令让_fun1函数外部可见 _fun1: # 函数调用的序曲,设置栈指针 pushq %rbp # 把调用者的栈帧底部地址保存起来 movq %rsp, %rbp # 把调用者的栈帧顶部地址,设置为本栈帧的底部 movl $10, -4(%rbp) # 变量c赋值为10,也可以写成 movl $10, (%rsp) # 做加法 movl %edi, %eax # 第一个参数放进%eax addl %esi, %eax # 加参数2 addl %edx, %eax # 加参数3 addl %ecx, %eax # 加参数4 addl %r8d, %eax # 加参数5 addl %r9d, %eax # 加参数6 addl 16(%rbp), %eax # 加参数7 addl 24(%rbp), %eax # 加参数8 addl -4(%rbp), %eax # 加上c的值 # 函数调用的尾声,恢复栈指针为原来的值 popq %rbp # 恢复调用者栈帧的底部数值 retq # 返回 接下来再写一个 C 语言的函数来调用 fun1()，其中的 extern 关键字，说明有一个 fun1() 函数是在另一个模块里实现的： /** * convention-main.c 测试调用约定。调用一个外部函数fun1 */ #include \u003cstdio.h\u003e //声明一个外部函数，在链接时会在其他模块中找到 extern int fun1(int x1, int x2, int x3, int x4, int x5, int x6, int x7, int x8); int main(int argc, char *argv[]) { printf(\"fun1: %d \\n\", fun1(1,2,3,4,5,6,7,8)); return 0; } 然后在命令行敲下面两个命令： # 编译汇编程序 as convention-fun1.s -o convention-fun1.o # 编译C程序 gcc convention-main.c convention-fun1.o -o convention 第一个命令，把 playscript 生成的汇编代码编译成一个二进制目标文件。第二个命令在编译 C 程序的时候，同时也带上这个二进制文件，那么编译器就会找到 fun1() 函数的定义，并链接到一起。最后生成的可执行文件能够顺利运行。 这里面，我需要解释一下链接过程，它有助于你在二进制文件层面上加深对编译过程的理解。其实，高级语言和汇编语言都容易阅读。而二进制文件，则是对计算机友好的，便于运行。汇编器可以把每一个汇编文件都编译生成一个二进制的目标文件，或者叫做一个模块。而链接器则把这些模块组装成一个整体。但在 C 语言生成的那个模块中，调用 fun1() 函数时，它没有办法知道 fun1() 函数的准确地址，因为这个地址必须是整个文件都组装完毕以后才能计算出来。所以，汇编器把这个任务推迟，交给链接器去解决。 这就好比你去饭店排队吃饭，首先要拿个号（函数的标签），但不知道具体坐哪桌。等叫到你的号的时候（链接过程），服务员才会给你安排一个确定的桌子（函数的地址）。既然我们已经从文本世界进入了二进制的世界，那么我们可以再加深一下对可执行文件结构的理解。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:30:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"理解可执行文件 我们编译一个程序，最后的结果是生成可运行的二进制文件。其实，生成汇编代码以后，我们就可以认为编译器的任务完成了。后面的工作，其实是由汇编器和链接器完成的。但我们也可以把整个过程都看做编译过程，了解二进制文件的结构，也为我们完整地了解整个编译过程划上了句号。 当然了，对二进制文件格式的理解，也是做大型项目编译管理、二进制代码分析等工作的基础，很有意义。 对于每个操作系统，我们对于可执行程序的格式要求是不一样的。比如，在 Linux 下，目标文件、共享对象文件、二进制文件，都是采用 ELF 格式。实际上，这些二进制文件的格式跟加载到内存中的程序的格式是很相似的。这样有什么好处呢？它可以迅速被操作系统读取，并加载到内存中去，加载速度越快，也就相当于程序的启动速度越快。同内存中的布局一样，在 ELF 格式中，代码和数据也是分开的。这样做的好处是，程序的代码部分，可以在多个进程中共享，不需要在内存里放多份。放一份，然后映射到每个进程的代码区就行了。而数据部分，则是每个进程都不一样的，所以要为每个进程加载一份。 这样讲的话，你就理解了可执行文件、目标文件等二进制文件的原理了，具体的细节，可以查阅相关的文档和手册。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:30:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 这节课，我们实现了从 AST 到汇编代码，汇编代码到可执行文件的完整过程。现在，你应该对后端工作的实质建立起了直接的认识。我建议你抓住几个关键点：首先，从 AST 生成汇编代码，可以通过比较机械的翻译来完成，我们举了加法运算的例子。阅读示例程序，你也可以看看函数调用、参数传递等等的实现过程。总体来说，这个过程并不难。第二，这种机械地翻译生成的代码，一定是不够优化的。我们已经看到了加法运算不够优化的情况，所以一定要增加一个优化的过程。第三，在生成汇编的过程中，最需要注意的就是要遵守调用约定。这就需要了解调用约定的很多细节。只要遵守调用约定，不同语言生成的二进制目标文件也可以链接在一起，形成最后的可执行文件。现在我已经带你完成了编译器后端的第一轮认知迭代，并且直接操刀汇编代码，破除你对汇编的恐惧心。在之后的课程中，我们会进入第二轮迭代：中间代码和代码优化。 示例代码我放在文末，供你参考。AsmGen.java（将 AST 翻译成汇编代码） 码云 GitHub asm.play（用于生成汇编码的 playscript 脚本） 码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:30:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"24 | 中间代码：兼容不同的语言和硬件 前几节课，我带你尝试不通过 IR，直接生成汇编代码，这是为了帮你快速破冰，建立直觉。在这个过程中，你也遇到了一些挑战，比如：你要对生成的代码进行优化，才有可能更好地使用寄存器和内存，同时也能减少代码量；另外，针对不同的 CPU 和操作系统，你需要调整生成汇编代码的逻辑。这些实际体验，都进一步验证了20 讲中，IR 的作用：我们能基于 IR 对接不同语言的前端，也能对接不同的硬件架构，还能做很多的优化。既然 IR 有这些作用，那你可能会问，IR 都是什么样子的呢？有什么特点？如何生成 IR 呢？本节课，我就带你了解 IR 的特点，认识常见的三地址代码，学会如何把高级语言的代码翻译成 IR。然后，我还会特别介绍 LLVM 的 IR，以便后面使用 LLVM 这个工具。首先，来看看 IR 的特征。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:31:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"介于中间的语言 IR 的意思是中间表达方式，它在高级语言和汇编语言的中间，这意味着，它的特征也是处于二者之间的。与高级语言相比，IR 丢弃了大部分高级语言的语法特征和语义特征，比如循环语句、if 语句、作用域、面向对象等等，它更像高层次的汇编语言；而相比真正的汇编语言，它又不会有那么多琐碎的、与具体硬件相关的细节。相信你在学习汇编语言的时候，会发现汇编语言的细节特别多。比如，你要知道很多指令的名字和用法，还要记住很多不同的寄存器。在 22 讲，我提到，如果你想完整地掌握 x86-64 架构，还需要接触很多指令集，以及调用约定的细节、内存使用的细节等等（参见 Intel 的手册）。 仅仅拿指令的数量来说，据有人统计，Intel 指令的助记符有 981 个之多！都记住怎么可能啊。所以说，汇编语言并不难，而是麻烦。IR 不会像 x86-64 汇编语言那么繁琐，但它却包含了足够的细节信息，能方便我们实现优化算法，以及生成针对目标机器的汇编代码。另外，我在 20 讲提到，IR 有很多种类（AST 也是一种 IR），每种 IR 都有不同的特点和用途，有的编译器，甚至要用到几种不同的 IR。我们在后端部分所讲的 IR，目的是方便执行各种优化算法，并有利于生成汇编。这种 IR，可以看做是一种高层次的汇编语言，主要体现在： 它可以使用寄存器，但寄存器的数量没有限制；控制结构也跟汇编语言比较像，比如有跳转语句，分成多个程序块，用标签来标识程序块等；使用相当于汇编指令的操作码。这些操作码可以一对一地翻译成汇编代码，但有时一个操作码会对应多个汇编指令。 下面来看看一个典型 IR：三地址代码，简称 TAC。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:31:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"认识典型的 IR：三地址代码（TAC） 下面是一种常见的 IR 的格式，它叫做三地址代码（Three Address Code, TAC），它的优点是很简洁，所以适合用来讨论算法： x := y op z //二元操作 x := op y //一元操作 每条三地址代码最多有三个地址，其中两个是源地址（比如第一行代码的 y 和 z），一个是目的地址（也就是 x），每条代码最多有一个操作（op）。我来举几个例子，带你熟悉一下三地址代码，这样，你能掌握三地址代码的特点，从高级语言的代码转换生成三地址代码。 基本的算术运算： int a, b, c, d; a = b + c * d; TAC： t1 := c * d a := b + t1 t1 是新产生的临时变量。当源代码的表达式中包含一个以上的操作符时，就需要引入临时变量，并把原来的一条代码拆成多条代码。 布尔值的计算： int a, b; bool x, y; x = a * 2 \u003c b; y = a + 3 == b; TAC： t1 := a * 2; x := t1 \u003c b; t2 := a + 3; y := t2 == b; 布尔值实际上是用整数表示的，0 代表 false，非 0 值代表 true。 条件语句： int a, b c; if (a \u003c b ) c = b; else c = a; c = c * 2; TAC： t1 := a \u003c b; IfZ t1 Goto L1; c := a; Goto L2; L1: c := b; L2: c := c * 2; IfZ 是检查后面的操作数是否是 0，“Z”就是“Zero”的意思。这里使用了标签和 Goto 语句来进行指令的跳转（Goto 相当于 x86-64 的汇编指令 jmp）。 循环语句： int a, b; while (a \u003c b){ a = a + 1; } a = a + b; TAC： L1: t1 := a \u003c b; IfZ t1 Goto L2; a := a + 1; Goto L1; L2: a := a + b; 三地址代码的规则相当简单，我们可以通过比较简单的转换规则，就能从 AST 生成 TAC。在课程中，三地址代码主要用来描述优化算法，因为它比较简洁易读，操作（指令）的类型很少，书写方式也符合我们的日常习惯。不过，我并不用它来生成汇编代码，因为它含有的细节信息还是比较少，比如，整数是 16 位的、32 位的还是 64 位的？目标机器的架构和操作系统是什么？生成二进制文件的布局是怎样的等等？我会用 LLVM 的 IR 来承担生成汇编的任务，因为它有能力描述与目标机器（CPU、操作系统）相关的更加具体的信息，准确地生成目标代码，从而真正能够用于生产环境。在讲这个问题之前，我想先延伸一下，讲讲另外几种 IR 的格式，主要想帮你开拓思维，如果你的项目需求，恰好能用这种 IR 实现，到时不妨拿来用一下： 首先是四元式。它是与三地址代码等价的另一种表达方式，格式是：（OP，arg1，arg2，result）所以，“a := b + c” 就等价于（+，b，c，a）。另一种常用的格式是逆波兰表达式。它把操作符放到后面，所以也叫做后缀表达式。“b + c”对应的逆波兰表达式是“b c +”；而“a = b + c”对应的逆波兰表达式是“a b c + =”。 逆波兰表达式特别适合用栈来做计算。比如计算“b c +”，先从栈里弹出加号，知道要做加法操作，然后从栈里弹出两个操作数，执行加法运算即可。这个计算过程，跟深度优先的遍历 AST 是等价的。所以，采用逆波兰表达式，有可能让你用一个很简单的方式就实现公式计算功能，如果你编写带有公式功能的软件时可以考虑使用它。而且，从 AST 生成逆波兰表达式也非常容易。 三地址代码主要是学习算法的工具，或者用于实现比较简单的后端，要实现工业级的后端，充分发挥硬件的性能，你还要学习 LLVM 的 IR。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:31:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"认识 LLVM 汇编码 LLVM 汇编码（LLVM Assembly），是 LLVM 的 IR。有的时候，我们就简单地称呼它为 LLVM 语言，因此我们可以把用 LLVM 汇编码书写的一个程序文件叫做 LLVM 程序。 我会在下一讲，详细讲解 LLVM 这个开源项目。本节课作为铺垫，告诉我们在使用 LLVM 之前，要先了解它的核心——IR。 首先，LLVM 汇编码是采用静态单赋值代码形式的。在三地址代码上再加一些限制，就能得到另一种重要的代码，即静态单赋值代码（Static Single Assignment, SSA ），在静态单赋值代码中，一个变量只能被赋值一次，来看个例子。“y = x1 + x2 + x3 + x4”的普通三地址代码如下： y := x1 + x2; y := y + x3; y := y + x4; 其中，y 被赋值了三次，如果写成 SSA 的形式，就只能写成下面的样子： t1 := x1 + x2; t2 := t1 + x3; y := t2 + x4; 为什么要费力写成这种形式呢，还要为此多添加 t1 和 t2 两个临时变量？原因是 SSA 的形式，体现了精确的“使用 - 定义”关系。每个变量很确定地只会被定义一次，然后可以多次使用。这种特点使得基于 SSA 更容易做数据流分析，而数据流分析又是很多代码优化技术的基础，所以，几乎所有语言的编译器、解释器或虚拟机中都使用了 SSA，因为有利于做代码优化。而 LLVM 的 IR，也是采用 SSA 的形式，也是因为 SSA 方便做代码优化。其次，LLVM IR 比起三地址代码，有更多的细节信息。比如整型变量的字长、内存对齐方式等等，所以使用 LLVM IR 能够更准确地翻译成汇编码。看看下面这段 C 语言代码： int fun1(int a, int b){ int c = 10; return a + b + c; } 对应的 LLLM 汇编码如下（这是我在 macOS 上生成的）： ; ModuleID = 'fun1.c' source_filename = \"fun1.c\" target datalayout = \"e-m:o-i64:64-f80:128-n8:16:32:64-S128\" target triple = \"x86_64-apple-macosx10.14.0\" ; Function Attrs: noinline nounwind optnone ssp uwtable define i32 @fun1(i32, i32) #0 { %3 = alloca i32, align 4 //为3个变量申请空间 %4 = alloca i32, align 4 %5 = alloca i32, align 4 store i32 %0, i32* %3, align 4 //参数1赋值给变量1 store i32 %1, i32* %4, align 4 //参数2赋值给变量2 store i32 10, i32* %5, align 4 //常量10赋值给变量3 %6 = load i32, i32* %3, align 4 // %7 = load i32, i32* %4, align 4 %8 = add nsw i32 %6, %7 %9 = load i32, i32* %5, align 4 %10 = add nsw i32 %8, %9 ret i32 %10 } attributes #0 = { noinline nounwind optnone ssp uwtable \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"less-precise-fpmad\"=\"false\" \"no-frame-pointer-elim\"=\"true\" \"no-frame-pointer-elim-non-leaf\" \"no-infs-fp-math\"=\"false\" \"no-jump-tables\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"penryn\" \"target-features\"=\"+cx16,+fxsr,+mmx,+sahf,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" } !llvm.module.flags = !{!0, !1, !2} !llvm.ident = !{!3} !0 = !{i32 2, !\"SDK Version\", [2 x i32] [i32 10, i32 14]} !1 = !{i32 1, !\"wchar_size\", i32 4} !2 = !{i32 7, !\"PIC Level\", i32 2} !3 = !{!\"Apple LLVM version 10.0.1 (clang-1001.0.46.4)\"} 这些代码看上去确实比三地址代码复杂，但还是比汇编精简多了，比如 LLVM IR 的指令数量连 x86-64 汇编的十分之一都不到。 我们来熟悉一下里面的元素：模块 LLVM 程序是由模块构成的，这个文件就是一个模块。模块里可以包括函数、全局变量和符号表中的条目。链接的时候，会把各个模块拼接到一起，形成可执行文件或库文件。在模块中，你可以定义目标数据布局（target datalayout）。例如，开头的小写“e”是低字节序（Little Endian）的意思，对于超过一个字节的数据来说，低位字节排放在内存的低地址端，高位字节排放在内存的高地址端。 target datalayout = \"e-m:o-i64:64-f80:128-n8:16:32:64-S128\" “target triple”用来定义模块的目标主机，它包括架构、厂商、操作系统三个部分。 target triple = \"x86_64-apple-macosx10.14.0\" 函数 在示例代码中有一个以 define 开头的函数的声明，还带着花括号。这有点儿像 C 语言的写法，比汇编用采取标签来表示一个函数的可读性更好。函数声明时可以带很多修饰成分，比如链接类型、调用约定等。如果不写，缺省的链接类型是 external 的，也就是可以像23 讲中做链接练习的那样，暴露出来被其他模块链接。调用约定也有很多种选择，缺省是“ccc”，也就是 C 语言的调用约定（C Calling Convention），而“swiftcc”则是 swift 语言的调用约定。这些信息都是生成汇编时所需要的。 示例中函数 fun1 还带有“#0”的属性值，定义了许多属性。这些也是生成汇编时所需要的。 标识符 分为全局的（Glocal）和本地的（Local）：全局标识符以 @开头，包括函数和全局变量，前面代码中的 @fun1 就是；本地标识符以 % 开头。有的标识符是有名字的，比如 @fun1 或 %a，有的是没有名字的，用数字表示就可以了，如 %1。 操作码 alloca、store、load、add、ret 这些，都是操作码。它们的含义是： 它们跟我们之前学到的汇编很相似。但是似乎函数体中的代码有点儿长。怎么一个简单的“a+b+c”就翻译成了 10 多行代码，还用到了那么多临时变量？不要担心，这只是完全没经过优化的格式，带上优化参数稍加优化以后，它就会被精简成下面的样子： define i32 @fun1(i32, i32) local_unnamed_addr #0 { %3 = add i32 %0, 10 %4 = add i32 %3, %1 ret i32 %4 } 类型系统 汇编是无类型的。如果你用 add 指令，它就认为你操作的是整数。而用 fadd（或 addss）指令，就认为你操作的是浮点数。这样会有类型不安全的风险，把整型当浮点数用了，造成的后果是计算结果完全错误。LLVM 汇编则带有一个类型系统。它能避免不安全的数据操作，并且有助于优化算法。这个类型系统包括基础数据类型、函数类型和 void 类型。 函数类型是包括对返回值和参数的定义，比如：i32 (i32)；void 类型不代表任何值，也没有长度。 全局变量和常量 在 LLVM 汇编中可以声明全局变量。全局变量所定义的内存，是在编译时就分配好了的，而不是在运行时，例如下面这句定义了一个全局变量 C： @c = global i32 100, align 4 你也可以声明常量，它的值在运行时不会被修改： @c = constant i32 100, align 4 元数据在代码中你还看到以“!”开头的一些句子，这些是元数据。这些元数据定义了一些额外的信息，提供给优化器和代码生成器使用。基本块函数中的代码会分成一个个的基本块，可以用标签（Label）来标记一个基本块。下面这段代码有 4 个基本块，其中第一个块有一个缺省的名字“entry”，也就是作为入口的基本块，这个基本块你不给它标签也可以。 define i32 @bb(i32) #0 { %2 = alloca i32, align 4 %3 = a","date":"2022-07-18 23:55:09","objectID":"/cp_base/:31:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 IR 是我们后续做代码优化、汇编代码生成的基础，在本节课中，我想让你明确的要点如下：1. 三地址代码是很常见的一种 IR，包含一个目的地址、一个操作符和至多两个源地址。它等价于四元式。我们在 27 讲和 28 讲中的优化算法，会用三地址代码来讲解，这样比较易于阅读。2.LLVM IR 的第一个特点是静态单赋值（SSA），也就是每个变量（地址）最多被赋值一次，它这种特性有利于运行代码优化算法；第二个特点是带有比较多的细节，方便我们做优化和生成高质量的汇编代码。通过本节课，你应该对于编译器后端中常常提到的 IR 建立了直观的认识，相信通过接下来的练习，你一定会消除对 IR 的陌生感，让它成为你得心应手的好工具！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:31:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"25 | 后端技术的重用：LLVM不仅仅让你高效 在编译器后端，做代码优化和为每个目标平台生成汇编代码，工作量是很大的。那么，有什么办法能降低这方面的工作量，提高我们的工作效率呢？答案就是利用现成的工具。 在前端部分，我就带你使用 Antlr 生成了词法分析器和语法分析器。那么在后端部分，我们也可以获得类似的帮助，比如利用 LLVM 和 GCC 这两个后端框架。相比前端的编译器工具，如 Lex（Flex）、Yacc（Bison）和 Antlr 等，对于后端工具，了解的人比较少，资料也更稀缺，如果你是初学者，那么上手的确有一些难度。不过我们已经用 20～24 讲，铺垫了必要的基础知识，也尝试了手写汇编代码，这些知识足够你学习和掌握后端工具了。本节课，我想先让你了解一些背景信息，所以会先概要地介绍一下 LLVM 和 GCC 这两个有代表性的框架的情况，这样，当我再更加详细地讲解 LLVM，带你实际使用一下它的时候，你接受起来就会更加容易了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:32:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"两个编译器后端框架：LLVM 和 GCC LLVM 是一个开源的编译器基础设施项目，主要聚焦于编译器的后端功能（代码生成、代码优化、JIT……）。它最早是美国伊利诺伊大学的一个研究性项目，核心主持人员是 Chris Lattner（克里斯·拉特纳）。LLVM 的出名是由于苹果公司全面采用了这个框架。苹果系统上的 C 语言、C++、Objective-C 的编译器 Clang 就是基于 LLVM 的，最新的 Swift 编程语言也是基于 LLVM，支撑了无数的移动应用和桌面应用。无独有偶，在 Android 平台上最新的开发语言 Kotlin，也支持基于 LLVM 编译成本地代码。另外，由 Mozilla 公司（Firefox 就是这个公司的产品）开发的系统级编程语言 RUST，也是基于 LLVM 开发的。还有一门相对小众的科学计算领域的语言，叫做 Julia，它既能像脚本语言一样灵活易用，又可以具有 C 语言一样的速度，在数据计算方面又有特别的优化，它的背后也有 LLVM 的支撑。 OpenGL 和一些图像处理领域也在用 LLVM，我还看到一个资料，说阿里云的工程师实现了一个 Cava 脚本语言，用于配合其搜索引擎系统 HA3。LLVM 的 logo，一只漂亮的龙： 还有，在人工智能领域炙手可热的 TensorFlow 框架，在后端也是用 LLVM 来编译。**它把机器学习的 IR 翻译成 LLVM 的 IR，然后再翻译成支持 CPU、GPU 和 TPU 的程序。**所以这样看起来，你所使用的很多语言和工具，背后都有 LLVM 的影子，只不过你可能没有留意罢了。所以在我看来，要了解编译器的后端技术，就不能不了解 LLVM。与 LLVM 起到类似作用的后端编译框架是 GCC（GNU Compiler Collection，GNU 编译器套件）。它支持了 GNU Linux 上的很多语言，例如 C、C++、Objective-C、Fortran、Go 语言和 Java 语言等。**其实，它最初只是一个 C 语言的编译器，后来把公共的后端功能也提炼了出来，形成了框架，支持多种前端语言和后端平台。**最近华为发布的方舟编译器，据说也是建立在 GCC 基础上的。 LLVM 和 GCC 很难比较优劣，因为这两个项目都取得了很大的成功。在本课程中，我们主要采用 LLVM，但其中学到的一些知识，比如 IR 的设计、代码优化算法、适配不同硬件的策略，在学习 GCC 或其他编译器后端的时候，也是有用的，从而大大提升学习效率。接下来，我们先来看看 LLVM 的构成和特点，让你对它有个宏观的认识。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:32:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"了解 LLVM 的特点 LLVM 能够支持多种语言的前端、多种后端 CPU 架构。在 LLVM 内部，使用类型化的和 SSA 特点的 IR 进行各种分析、优化和转换： LLVM 项目包含了很多组成部分：LLVM 核心（core）。就是上图中的优化和分析工具，还包括了为各种 CPU 生成目标代码的功能；这些库采用的是 LLVM IR，一个良好定义的中间语言，在上一讲，我们已经初步了解它了。Clang 前端（是基于 LLVM 的 C、C++、Objective-C 编译器）。LLDB（一个调试工具）。LLVM 版本的 C++ 标准类库。其他一些子项目。 我个人很喜欢 LLVM，想了想，主要有几点原因： 首先，LLVM 有良好的模块化设计和接口。以前的编译器后端技术很难复用，而 LLVM 具备定义了良好接口的库，方便使用者选择在什么时候，复用哪些后端功能。比如，针对代码优化，LLVM 提供了很多算法，语言的设计者可以自己选择合适的算法，或者实现自己特殊的算法，具有很好的灵活性。第二，LLVM 同时支持 JIT（即时编译）和 AOT（提前编译）两种模式。过去的语言要么是解释型的，要么编译后运行。习惯了使用解释型语言的程序员，很难习惯必须等待一段编译时间才能看到运行效果。很多科学工作者，习惯在一个 REPL 界面中一边写脚本，一边实时看到反馈。LLVM 既可以通过 JIT 技术支持解释执行，又可以完全编译后才执行，这对于语言的设计者很有吸引力。第三，有很多可以学习借鉴的项目。Swift、Rust、Julia 这些新生代的语言，实现了很多吸引人的特性，还有很多其他的开源项目，而我们可以研究、借鉴它们是如何充分利用 LLVM 的。第四，全过程优化的设计思想。LLVM 在设计上支持全过程的优化。Lattner 和 Adve 最早关于 LLVM 设计思想的文章《LLVM: 一个全生命周期分析和转换的编译框架》，就提出计算机语言可以在各个阶段进行优化，包括编译时、链接时、安装时，甚至是运行时。 以运行时优化为例，基于 LLVM 我们能够在运行时，收集一些性能相关的数据对代码编译优化，可以是实时优化的、动态修改内存中的机器码；也可以收集这些性能数据，然后做离线的优化，重新生成可执行文件，然后再加载执行，这一点非常吸引我，因为在现代计算环境下，每种功能的计算特点都不相同，确实需要针对不同的场景做不同的优化。下图展现了这个过程（图片来源《 LLVM: A Compilation Framework for Lifelong Program Analysis \u0026 Transformation》）： 我建议你读一读 Lattner 和 Adve 的这篇论文（另外强调一下，当你深入学习编译技术的时候，阅读领域内的论文就是必不可少的一项功课了）。第五，LLVM 的授权更友好。GNU 的很多软件都是采用 GPL 协议的，所以如果用 GCC 的后端工具来编写你的语言，你可能必须要按照 GPL 协议开源。而 LLVM 则更友好一些，你基于 LLVM 所做的工作，完全可以是闭源的软件产品。而我之所以说：“LLVM 不仅仅让你更高效”，就是因为上面它的这些特点。现在，你已经对 LLVM 的构成和特点有一定的了解了，接下来，我带你亲自动手操作和体验一下 LLVM 的功能，这样你就可以迅速消除对它的陌生感，快速上手了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:32:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"体验一下 LLVM 的功能 首先你需要安装一下 LLVM（参照官方网站上的相关介绍下载安装）。因为我使用的是 macOS，所以用 brew 就可以安装。 brew install llvm 因为 LLVM 里面带了一个版本的 Clang 和 C++ 的标准库，与本机原来的工具链可能会有冲突，所以 brew 安装的时候并没有在 /usr/local 下建立符号链接。你在用 LLVM 工具的时候，要配置好相关的环境变量。 # 可执行文件的路径 export PATH=\"/usr/local/opt/llvm/bin:$PATH\" # 让编译器能够找到LLVM export LDFLAGS=\"-L/usr/local/opt/llvm/lib\" export CPPFLAGS=\"-I/usr/local/opt/llvm/include” 安装完毕之后，我们使用一下 LLVM 自带的命令行工具，分几步体验一下 LLVM 的功能：1. 从 C 语言代码生成 IR；2. 优化 IR；3. 从文本格式的 IR 生成二进制的字节码；4. 把 IR 编译成汇编代码和可执行文件。 从 C 语言代码生成 IR 代码比较简单，上一讲中我们已经用到过一个 C 语言的示例代码： //fun1.c int fun1(int a, int b){ int c = 10; return a+b+c; } 用前端工具 Clang 就可以把它编译成 IR 代码： clang -emit-llvm -S fun1.c -o fun1.ll 其中，-emit-llvm 参数告诉 Clang 生成 LLVM 的汇编码，也就是 IR 代码（如果不带这个参数，就会生成针对目标机器的汇编码）所生成的 IR 我们上一讲也见过，你现在应该能够读懂它了。你可以多写几个不同的程序，看看生成的 IR 是什么样的，比如 if 语句、循环语句等等（这时你完成了第一步）： ; ModuleID = 'function-call1.c' source_filename = \"function-call1.c\" target datalayout = \"e-m:o-i64:64-f80:128-n8:16:32:64-S128\" target triple = \"x86_64-apple-macosx10.14.0\" ; Function Attrs: noinline nounwind optnone ssp uwtable define i32 @fun1(i32, i32) #0 { %3 = alloca i32, align 4 %4 = alloca i32, align 4 %5 = alloca i32, align 4 store i32 %0, i32* %3, align 4 store i32 %1, i32* %4, align 4 store i32 10, i32* %5, align 4 %6 = load i32, i32* %3, align 4 %7 = load i32, i32* %4, align 4 %8 = add nsw i32 %6, %7 %9 = load i32, i32* %5, align 4 %10 = add nsw i32 %8, %9 ret i32 %10 } attributes #0 = { noinline nounwind optnone ssp uwtable \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"less-precise-fpmad\"=\"false\" \"min-legal-vector-width\"=\"0\" \"no-frame-pointer-elim\"=\"true\" \"no-frame-pointer-elim-non-leaf\" \"no-infs-fp-math\"=\"false\" \"no-jump-tables\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"penryn\" \"target-features\"=\"+cx16,+fxsr,+mmx,+sahf,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" } !llvm.module.flags = !{!0, !1} !llvm.ident = !{!2} !0 = !{i32 1, !\"wchar_size\", i32 4} !1 = !{i32 7, !\"PIC Level\", i32 2} !2 = !{!\"clang version 8.0.0 (tags/RELEASE_800/final)\"} 上一讲我们提到过，可以对生成的 IR 做优化，让代码更短，你只要在上面的命令中加上 -O2 参数就可以了（这时你完成了第二步）： clang -emit-llvm -S -O2 fun1.c -o fun1.ll 这个时候，函数体的核心代码就变短了很多。这里面最重要的优化动作，是从原来使用内存（alloca 指令是在栈中分配空间，store 指令是往内存里写入值），优化到只使用寄存器（%0、%1 是参数，%3、%4 也是寄存器）。 define i32 @fun1(i32, i32) #0 { %3 = add nsw i32 %0, %1 %4 = add nsw i32 %3, 10 ret i32 %4 } 你还可以用 opt 命令来完成上面的优化，具体我们在 27、28 讲中讲优化算法的时候再细化。 另外，LLVM 的 IR 有两种格式。在示例代码中显示的，是它的文本格式，文件名一般以.ll 结尾。第二种格式是字节码（bitcode）格式，文件名以.bc 结尾。为什么要用两种格式呢？因为文本格式的文件便于程序员阅读，而字节码格式的是二进制文件，便于机器处理，比如即时编译和执行。生成字节码格式之后，所占空间会小很多，所以可以快速加载进内存，并转换为内存中的对象格式。而如果加载文本文件，则还需要一个解析的过程，才能变成内存中的格式，效率比较慢。调用 llvm-as 命令，我们可以把文本格式转换成字节码格式： llvm-as fun1.ll -o fun1.bc 我们也可以用 clang 直接生成字节码，这时不需要带 -S 参数，而是要用 -c 参数。 clang -emit-llvm -c fun1.c -o fun1.bc 因为.bc 文件是二进制文件，不能直接用文本编辑器查看，而要用 hexdump 命令查看（这时你完成了第三步）： hexdump -C fun1.bc LLVM 的一个优点，就是可以即时编译运行字节码，不一定非要编译生成汇编码和可执行文件才能运行（这一点有点儿像 Java 语言），这也让 LLVM 具有极高的灵活性，比如，可以在运行时根据收集的性能信息，改变优化策略，生成更高效的机器码。再进一步，我们可以把字节码编译成目标平台的汇编代码。我们使用的是 llc 命令，命令如下： llc fun1.bc -o fun1.s 用 clang 命令也能从字节码生成汇编代码，要注意带上 -S 参数就行了： clang -S fun1.bc -o fun1.s 到了这一步，我们已经得到了汇编代码，接着就可以进一步生成目标文件和可执行文件了。实际上，使用 LLVM 从源代码到生成可执行文件有两条可能的路径： 第一条路径，是把每个源文件分别编译成字节码文件，然后再编译成目标文件，最后链接成可执行文件。第二条路径，是先把编译好的字节码文件链接在一起，形成一个更大的字节码文件，然后对这个字节码文件进行进一步的优化，之后再生成目标文件和可执行文件。 第二条路径比第一条路径多了一个优化的步骤，第一条路径只对每个模块做了优化，没有做整体的优化。所以，如有可能，尽量采用第二条路径，这样能够生成更加优化的代码。现在你完成了第四步，对 LLVM 的命令行工具有了一定的了解。总结一下，我们用到的命令行工具包括：clang 前端、llvm-as、llc，其他命令还有 opt（代码优化）、llvm-dis（将字节码再反编译回 ll 文件）、llvm-link（链接）等，你可以看它们的 help 信息，并练习使用。在熟悉了命令行工具之后，我们就可以进一步在编程环境中使用 LLVM 了，不过在此之前，需要搭建一个开发环境。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:32:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"建立 C++ 开发环境来使用 LLVM LLVM 本身是用 C++ 开发的，所以最好采用 C++ 调用它的功能。当然，采用其他语言也有办法调用 LLVM：C 语言可以调用专门的 C 接口；像 Go、Rust、Python、Ocaml、甚至 Node.js 都有对 LLVM API 的绑定；如果使用 Java，也可以通过 JavaCPP（类似 JNI）技术调用 LLVM。 在课程中，我用 C++ 来做实现，因为这样能够最近距离地跟 LLVM 打交道。与此同时，我们前端工具采用的 Antlr，也能够支持 C++ 开发环境。所以，我为 playscript 建立了一个 C++ 的开发环境。开发工具方面：原则上只要一个编辑器加上工具链就行，但为了提高效率，有 IDE 的支持会更好（我用的是 JetBrains 的 Clion）。构建工具方面：目前 LLVM 本身用的是 CMake，而 Clion 刚好也采用 CMake，所以很方便。这里我想针对 CMake 多解释几句，因为越来越多的 C++ 项目都是用 CMake 来管理的，LLVM 以及 Antlr 的 C++ 版本也采用了 CMake，你最好对它有一定了解。 CMake 是一款优秀的工程构建工具，它类似于 Java 程序员们习惯使用的 Maven 工具。对于只包含少量文件或模块的 C 或 C++ 程序，你可以仅仅通过命令行带上一些参数就能编译。 不过，实际的项目都会比较复杂，往往会包含比较多的模块，存在比较复杂的依赖关系，编译过程也不是一步能完成的，要分成多步。这时候我们一般用 make 管理项目的构建过程，这就要学会写 make 文件。但手工写 make 文件工作量会比较大，而 CMake 就是在 make 的基础上再封装了一层，它能通过更简单的配置文件，帮我们生成 make 文件，帮助程序员提升效率。整个开发环境的搭建我在课程里就不多写了，你可以参见示例代码所附带的文档。文档里有比较清晰的说明，可以帮助你把环境搭建起来，并运行示例程序。另外，我知道你可能对 C++ 并不那么熟悉。但你应该学过 C 语言，所以示例代码还是能看懂的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:32:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，为了帮助你理解后端工具，我先概要介绍了后端工具的情况，接着着重介绍了 LLVM 的构成和特点，然后又带你熟悉了它的命令行工具，让你能够生成文本和字节码两种格式的 IR，并生成可执行文件，最后带你了解了 LLVM 的开发环境。本节课的内容比较好理解，因为侧重让你建立跟 LLVM 的熟悉感，没有什么复杂的算法和原理，而我想强调的是以下几点： 后端工具对于语言设计者很重要，我们必须学会善加利用；2.LLVM 有很好的模块化设计，支持即时编译（JIT）和提前编译（AOT），支持全过程的优化，并且具备友好的授权，值得我们好好掌握；3. 你要熟悉 LLVM 的命令行工具，这样可以上手做很多实验，加深对 LLVM 的了解。 最后，我想给你的建议是：一定要动手安装和使用 LLVM，写点代码测试它的功能。比如，写点儿 C、C++ 等语言的程序，并翻译成 IR，进一步熟悉 LLVM 的 IR。下一讲，我们就要进入它的内部，调用它的 API 来生成 IR 和运行了！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:32:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"26 | 生成IR：实现静态编译的语言 目前来讲，你已经初步了解了 LLVM 和它的 IR，也能够使用它的命令行工具。不过，我们还是要通过程序生成 LLVM 的 IR，这样才能复用 LLVM 的功能，从而实现一门完整的语言。不过，如果我们要像前面生成汇编语言那样，通过字符串拼接来生成 LLVM 的 IR，除了要了解 LLVM IR 的很多细节之外，代码一定比较啰嗦和复杂，因为字符串拼接不是结构化的方法，所以，最好用一个定义良好的数据结构来表示 IR。好在 LLVM 项目已经帮我们考虑到了这一点，它提供了代表 LLVM IR 的一组对象模型，我们只要生成这些对象，就相当于生成了 IR，这个难度就低多了。而且，LLVM 还提供了一个工具类，IRBuilder，我们可以利用它，进一步提升创建 LLVM IR 的对象模型的效率，让生成 IR 的过程变得更加简单！接下来，就让我们先来了解 LLVM IR 的对象模型。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:33:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"LLVM IR 的对象模型 LLVM 在内部有用 C++ 实现的对象模型，能够完整表示 LLVM IR，当我们把字节码读入内存时，LLVM 就会在内存中构建出这个模型。只有基于这个对象模型，我们才可以做进一步的工作，包括代码优化，实现即时编译和运行，以及静态编译生成目标文件。所以说，这个对象模型是 LLVM 运行时的核心。 IR 对象模型的头文件在include/llvm/IR目录下，其中最重要的类包括：Module（模块）Module 类聚合了一个模块中的所有数据，它可以包含多个函数。你可以通过 Model::iterator 来遍历模块中所有的函数。它也包含了一个模块的全局变量。Function（函数）Function 包含了与函数定义（definition）或声明（declaration）有关的所有对象。函数定义包含了函数体，而函数声明，则仅仅包含了函数的原型，它是在其他模块中定义的，在本模块中使用。 你可以通过 getArgumentList() 方法来获得函数参数的列表，也可以遍历函数体中的所有基本块，这些基本块会形成一个 CFG（控制流图）。 //函数声明，没有函数体。这个函数是在其他模块中定义的，在本模块中使用 declare void @foo(i32) //函数定义，包含函数体 define i32 @fun3(i32 %a) { %calltmp1 = call void @foo(i32 %a) //调用外部函数 ret i32 10 } BasicBlock（基本块）BasicBlock 封装了一系列的 LLVM 指令，你可以借助 bigin()/end() 模式遍历这些指令，还可以通过 getTerminator() 方法获得最后一条指令（也就是终结指令）。你还可以用到几个辅助方法在 CFG 中导航，比如获得某个基本块的前序基本块。 Instruction（指令）Instruction 类代表了 LLVM IR 的原子操作（也就是一条指令），你可以通过 getOpcode() 来获得它代表的操作码，它是一个 llvm::Instruction 枚举值，你可以通过 op_begin() 和 op_end() 方法对获得这个指令的操作数。 Value（值）Value 类代表一个值。在 LLVM 的内存 IR 中，如果一个类是从 Value 继承的，意味着它定义了一个值，其他方可以去使用。函数、基本块和指令都继承了 Value。 LLVMContext（上下文）这个类代表了 LLVM 做编译工作时的一个上下文，包含了编译工作中的一些全局数据，比如各个模块用到的常量和类型。 这些内容是 LLVM IR 对象模型的主要部分，我们生成 IR 的过程，就是跟这些类打交道，其他一些次要的类，你可以在阅读和编写代码的过程中逐渐熟悉起来。接下来，就让我们用程序来生成 LLVM 的 IR。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:33:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"尝试生成 LLVM IR 我刚刚提到的每个 LLVM IR 类，都可以通过程序来构建。那么，为下面这个 fun1() 函数生成 IR，应该怎么办呢？ int fun1(int a, int b){ return a+b; } 第一步，我们可以来生成一个 LLVM 模块，也就是顶层的 IR 对象。 Module *mod = new Module(\"fun1.ll\", TheModule); 第二步，我们继续在模块中定义函数 fun1，因为模块最主要的构成要素就是各个函数。不过在定义函数之前，要先定义函数的原型（或者叫函数的类型）。函数的类型，我们在前端讲过：如果两个函数的返回值相同，并且参数也相同，这两个函数的类型是相同的，这样就可以做函数指针或函数型变量的赋值。示例代码的函数原型是：返回值是 32 位整数，参数是两个 32 位整数。有了函数原型以后，就可以使用这个函数原型定义一个函数。我们还可以为每个参数设置一个名称，便于后面引用这个参数。 //函数原型 vector\u003cType *\u003e argTypes(2, Type::getInt32Ty(TheContext)); FunctionType *fun1Type = FunctionType::get(Type::getInt32Ty(TheContext), //返回值是整数 argTypes, //两个整型参数 false); //不是变长参数 //函数对象 Function *fun = Function::Create(fun1Type, Function::ExternalLinkage, //链接类型 \"fun2\", //函数名称 TheModule.get()); //所在模块 //设置参数名称 string argNames[2] = {\"a\", \"b\"}; unsigned i = 0; for (auto \u0026arg : fun-\u003eargs()){ arg.setName(argNames[i++]); } 这里你需要注意，代码中是如何使用变量类型的。所有的基础类型都是提前定义好的，可以通过 Type 类的 getXXXTy() 方法获得（我们使用的是 Int32 类型，你还可以获得其他类型）。 第三步，创建一个基本块。这个函数只有一个基本块，你可以把它命名为“entry”，也可以不给它命名。在创建了基本块之后，我们用了一个辅助类 IRBuilder，设置了一个插入点，后序生成的指令会插入到这个基本块中（IRBuilder 是 LLVM 为了简化 IR 生成过程所提供的一个辅助类）。 //创建一个基本块 BasicBlock *BB = BasicBlock::Create(TheContext,//上下文 \"\", //基本块名称 fun); //所在函数 Builder.SetInsertPoint(BB); //设置指令的插入点 第四步，生成\"a+b\"表达式所对应的 IR，插入到基本块中。a 和 b 都是函数 fun 的参数，我们把它取出来，分别赋值给 L 和 R（L 和 R 是 Value）。然后用 IRBuilder 的 CreateAdd() 方法，生成一条 add 指令。这个指令的计算结果存放在 addtemp 中。 //把参数变量存到NamedValues里面备用 NamedValues.clear(); for (auto \u0026Arg : fun-\u003eargs()) NamedValues[Arg.getName()] = \u0026Arg; //做加法 Value *L = NamedValues[\"a\"]; Value *R = NamedValues[\"b\"]; Value *addtmp = Builder.CreateAdd(L, R); 第五步，利用刚才获得的 addtmp 创建一个返回值。 //返回值 Builder.CreateRet(addtmp); 最后一步，检查这个函数的正确性。这相当于是做语义检查，比如，基本块的最后一个语句就必须是一个正确的返回指令。 //验证函数的正确性 verifyFunction(*fun); 完整的代码我也提供给你，放在codegen_fun1()里了，你可以看一下。我们可以调用这个方法，然后打印输出生成的 IR： Function *fun1 = codegen_fun1(); //在模块中生成Function对象 TheModule-\u003eprint(errs(), nullptr); //在终端输出IR 生成的 IR 如下： ; ModuleID = 'llvmdemo' source_filename = \"llvmdemo\" define i32 @fun1(i32 %a, i32 %b) { %1 = add i32 %a, %b ret i32 %1 } 这个例子简单，过程直观，只有一个加法运算，而我建议你在这个过程中注意每个 IR 对象都是怎样被创建的，在大脑中想象出整个对象结构。为了熟悉更多的 API，接下来，我再带你生成一个稍微复杂一点儿的，带有 if 语句的 IR。然后来看一看，函数中包含多个基本块的情况。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:33:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"支持 if 语句 具体说，我们要为下面的一个函数生成 IR（函数有一个参数 a，当 a 大于 2 的时候，返回 2；否则返回 3）。 int fun_ifstmt(int a) if (a \u003e 2) return 2; else return 3； } 这样的一个函数，需要包含 4 个基本块：入口基本块、Then 基本块、Else 基本块和 Merge 基本块。控制流图（CFG）是先分开，再合并，像下面这样： 在入口基本块中，我们要计算“a\u003e2”的值，并根据这个值，分别跳转到 ThenBB 和 ElseBB。这里，我们用到了 IRBuilder 的 CreateICmpUGE() 方法（UGE 的意思，是”不大于等于“，也就是小于）。这个指令的返回值是一个 1 位的整型，也就是 int1。 //计算a\u003e2 Value * L = NamedValues[\"a\"]; Value * R = ConstantInt::get(TheContext, APInt(32, 2, true)); Value * cond = Builder.CreateICmpUGE(L, R, \"cmptmp\"); 接下来，我们创建另外 3 个基本块，并用 IRBuilder 的 CreateCondBr() 方法创建条件跳转指令：当 cond 是 1 的时候，跳转到 ThenBB，0 的时候跳转到 ElseBB。 BasicBlock *ThenBB =BasicBlock::Create(TheContext, \"then\", fun); BasicBlock *ElseBB = BasicBlock::Create(TheContext, \"else\"); BasicBlock *MergeBB = BasicBlock::Create(TheContext, \"ifcont\"); Builder.CreateCondBr(cond, ThenBB, ElseBB); 如果你细心的话，可能会发现，在创建 ThenBB 的时候，指定了其所在函数是 fun，而其他两个基本块没有指定。这是因为，我们接下来就要为 ThenBB 生成指令，所以先加到 fun 中。之后，再顺序添加 ElseBB 和 MergeBB 到 fun 中。 //ThenBB Builder.SetInsertPoint(ThenBB); Value *ThenV = ConstantInt::get(TheContext, APInt(32, 2, true)); Builder.CreateBr(MergeBB); //ElseBB fun-\u003egetBasicBlockList().push_back(ElseBB); //把基本块加入到函数中 Builder.SetInsertPoint(ElseBB); Value *ElseV = ConstantInt::get(TheContext, APInt(32, 3, true)); Builder.CreateBr(MergeBB); 在 ThenBB 和 ElseBB 这两个基本块的代码中，我们分别计算出了两个值：ThenV 和 ElseV。它们都可能是最后的返回值，但具体采用哪个，还要看实际运行时，控制流走的是 ThenBB 还是 ElseBB。这就需要用到 phi 指令，它完成了根据控制流来选择合适的值的任务。 //MergeBB fun-\u003egetBasicBlockList().push_back(MergeBB); Builder.SetInsertPoint(MergeBB); //PHI节点：整型，两个候选值 PHINode *PN = Builder.CreatePHI(Type::getInt32Ty(TheContext), 2); PN-\u003eaddIncoming(ThenV, ThenBB); //前序基本块是ThenBB时，采用ThenV PN-\u003eaddIncoming(ElseV, ElseBB); //前序基本块是ElseBB时，采用ElseV //返回值 Builder.CreateRet(PN); 从上面这段代码中你能看出，在 if 语句中，phi 指令是关键。因为当程序的控制流经过多个基本块，每个基本块都可能改变某个值的时候，通过 phi 指令可以知道运行时实际走的是哪条路径，从而获得正确的值。最后生成的 IR 如下，其中的 phi 指令指出，如果前序基本块是 then，取值为 2，是 else 的时候取值为 3。 define i32 @fun_ifstmt(i32 %a) { %cmptmp = icmp uge i32 %a, 2 br i1 %cmptmp, label %then, label %else then: ; preds = %0 br label %ifcont else: ; preds = %0 br label %ifcont ifcont: ; preds = %else, %then %1 = phi i32 [ 2, %then ], [ 3, %else ] ret i32 %1 } 其实循环语句也跟 if 语句差不多，因为它们都是要涉及到多个基本块，要用到 phi 指令，所以一旦你会写 if 语句，肯定就会写循环语句的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:33:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"支持本地变量 在写程序的时候，本地变量是必不可少的一个元素，所以，我们趁热打铁，把刚才的示例程序变化一下，用本地变量 b 保存 ThenBB 和 ElseBB 中计算的值，借此学习一下 LLVM IR 是如何支持本地变量的。改变后的示例程序如下： int fun_localvar(int a) int b = 0; if (a \u003e 2) b = 2; else b = 3; return b; } 其中，函数有一个参数 a，一个本地变量 b：如果 a 大于 2，那么给 b 赋值 2；否则，给 b 赋值 3。最后的返回值是 b。现在挑战来了，在这段代码中，b 被声明了一次，赋值了 3 次。我们知道，LLVM IR 采用的是 SSA 形式，也就是每个变量只允许被赋值一次，那么对于多次赋值的情况，我们该如何生成 IR 呢？其实，LLVM 规定了对寄存器只能做单次赋值，而对内存中的变量，是可以多次赋值的。对于“int b = 0;”，我们用下面几条语句生成 IR： //本地变量b AllocaInst *b = Builder.CreateAlloca(Type::getInt32Ty(TheContext), nullptr, \"b\"); Value* initValue = ConstantInt::get(TheContext, APInt(32, 0, true)); Builder.CreateStore(initValue, b); 上面这段代码的含义是：首先用 CreateAlloca() 方法，在栈中申请一块内存，用于保存一个 32 位的整型，接着，用 CreateStore() 方法生成一条 store 指令，给 b 赋予初始值。上面几句生成的 IR 如下： %b = alloca i32 store i32 0, i32* %b 接着，我们可以在 ThenBB 和 ElseBB 中，分别对内存中的 b 赋值： //ThenBB Builder.SetInsertPoint(ThenBB); Value *ThenV = ConstantInt::get(TheContext, APInt(32, 2, true)); Builder.CreateStore(ThenV, b); Builder.CreateBr(MergeBB); //ElseBB fun-\u003egetBasicBlockList().push_back(ElseBB); Builder.SetInsertPoint(ElseBB); Value *ElseV = ConstantInt::get(TheContext, APInt(32, 3, true)); Builder.CreateStore(ElseV, b); Builder.CreateBr(MergeBB); 最后，在 MergeBB 中，我们只需要返回 b 就可以了： //MergeBB fun-\u003egetBasicBlockList().push_back(MergeBB); Builder.SetInsertPoint(MergeBB); //返回值 Builder.CreateRet(b); 最后生成的 IR 如下： define i32 @fun_ifstmt.1(i32 %a) { %b = alloca i32 store i32 0, i32* %b %cmptmp = icmp uge i32 %a, 2 br i1 %cmptmp, label %then, label %else then: ; preds = %0 store i32 2, i32* %b br label %ifcont else: ; preds = %0 store i32 3, i32* %b br label %ifcont ifcont: ; preds = %else, %then ret i32* %b } 当然，使用内存保存临时变量的性能比较低，但我们可以很容易通过优化算法，把上述代码从使用内存的版本，优化成使用寄存器的版本。通过上面几个示例，现在你已经学会了生成基本的 IR，包括能够支持本地变量、加法运算、if 语句。那么这样生成的 IR 能否正常工作呢？我们需要把这些 IR 编译和运行一下才知道。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:33:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"编译并运行程序 现在已经能够在内存中建立 LLVM 的 IR 对象了，包括模块、函数、基本块和各种指令。LLVM 可以即时编译并执行这个 IR 模型。我们先创建一个不带参数的 __main() 函数作为入口。同时，我会借这个例子延伸讲一下函数的调用。我们在前面声明了函数 fun1，现在在 __main() 函数中演示如何调用它。 Function * codegen_main(){ //创建main函数 FunctionType *mainType = FunctionType::get(Type::getInt32Ty(TheContext), false); Function *main = Function::Create(mainType, Function::ExternalLinkage, \"__main\", TheModule.get()); //创建一个基本块 BasicBlock *BB = BasicBlock::Create(TheContext, \"\", main); Builder.SetInsertPoint(BB); //设置参数的值 int argValues[2] = {2, 3}; std::vector\u003cValue *\u003e ArgsV; for (unsigned i = 0; i\u003c2; ++i) { Value * value = ConstantInt::get(TheContext, APInt(32,argValues[i],true)); ArgsV.push_back(value); if (!ArgsV.back()) return nullptr; } //调用函数fun1 Function *callee = TheModule-\u003egetFunction(\"fun1\"); Value * rtn = Builder.CreateCall(callee, ArgsV, \"calltmp\"); //返回值 Builder.CreateRet(rtn); return main; } 调用函数时，我们首先从模块中查找出名称为 fun1 的函数，准备好参数值，然后通过 IRBuilder 的 CreateCall() 方法来生成函数调用指令。最后生成的 IR 如下： define i32 @__main() { %calltmp = call i32 @fun1(i32 2, i32 3) ret i32 %calltmp3 } 接下来，我们调用即时编译的引擎来运行 __main 函数（与 JIT 引擎有关的代码，放到了 DemoJIT.h 中，你现在可以暂时不关心它的细节，留到以后再去了解）。使用这个 JIT 引擎，我们需要做几件事情： 初始化与目标硬件平台有关的设置。 InitializeNativeTarget(); InitializeNativeTargetAsmPrinter(); InitializeNativeTargetAsmParser(); 把创建的模型加入到 JIT 引擎中，找到 __main() 函数的地址（整个过程跟 C 语言中使用函数指针来执行一个函数没有太大区别）。 auto H = TheJIT-\u003eaddModule(std::move(TheModule)); //查找__main函数 auto main = TheJIT-\u003efindSymbol(\"__main\"); //获得函数指针 int32_t (*FP)() = (int32_t (*)())(intptr_t)cantFail(main.getAddress()); //执行函数 int rtn = FP(); //打印执行结果 fprintf(stderr, \"__main: %d\\n\", rtn); 程序可以成功执行，并打印 __main 函数的返回值。既然已经演示了如何调用函数，在这里，我给你揭示 LLVM 的一个惊人的特性：我们可以在 LLVM IR 里，调用本地编写的函数，比如编写一个 foo() 函数，用来打印输出一些信息： void foo(int a){ printf(\"in foo: %d\\n\",a); } 然后我们就可以在 __main 里直接调用这个 foo 函数，就像调用 fun1 函数一样： //调用一个外部函数foo vector\u003cType *\u003e argTypes(1, Type::getInt32Ty(TheContext)); FunctionType *fooType = FunctionType::get(Type::getVoidTy(TheContext), argTypes, false); Function *foo = Function::Create(fooType, Function::ExternalLinkage, \"foo\", TheModule.get()); std::vector\u003cValue *\u003e ArgsV2; ArgsV2.push_back(rtn); if (!ArgsV2.back()) return nullptr; Builder.CreateCall(foo, ArgsV2, \"calltmp2\"); 注意，我们在这里只对 foo 函数做了声明，并没有定义它的函数体，这时 LLVM 会在外部寻找 foo 的定义，它会找到用 C++ 编写的 foo 函数，然后调用并执行；如果 foo 函数在另一个目标文件中，它也可以找到。刚才讲的是即时编译和运行，你也可以生成目标文件，然后再去链接和执行。生成目标文件的代码参见emitObject()方法，基本上就是打开一个文件，然后写入生成的二进制目标代码。针对目标机器生成目标代码的大量工作，就用这么简单的几行代码就实现了，是不是帮了你的大忙了？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:33:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我们我们完成了从生成 IR 到编译执行的完整过程，同时，也初步熟悉了 LLVM 的接口。当然了，完全熟悉 LLVM 的接口还需要多做练习，掌握更多的细节。就本节课而言，我希望你掌握的重点如下：LLVM 用一套对象模型在内存中表示 IR，包括模块、函数、基本块和指令，你可以通过 API 来生成这些对象。这些对象一旦生成，就可以编译和执行。对于 if 语句和循环语句，需要生成多个基本块，并通过跳转指令形成正确的控制流图（CFG）。当存在多个前序节点可能改变某个变量的值的时候，使用 phi 指令来确定正确的值。存储在内存中的本地变量，可以多次赋值。LLVM 能够把外部函数和 IR 模型中的函数等价对待。 另外，为了降低学习难度，本节课，我没有做从 AST 翻译成 IR 的工作，而是针对一个目标功能（比如一个 C 语言的函数），硬编码调用 API 来生成 IR。你理解各种功能是如何生成 IR 以后，再从 AST 来翻译，就更加容易了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:33:6","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"27 | 代码优化：为什么你的代码比他的更高效？ 在使用 LLVM 的过程中，你应该觉察到了，优化之后和优化之前的代码相差很大。代码优化之后，数量变少了，性能也更高了。而针对这个看起来很神秘的代码优化，我想问你一些问题：代码优化的目标是什么？除了性能上的优化，还有什么优化？代码优化可以在多大的范围内执行？是在一个函数内，还是可以针对整个应用程序？常见的代码优化场景有哪些？这些问题是代码优化的基本问题，很重要，我会用两节课的时间带你了解和掌握。当然了，代码优化是编译器后端的两大工作之一（另一个是代码生成），弄懂它，你就掌握了一大块后端技术。而学习代码优化的原理，然后通过 LLVM 实践一下，这样原理与实践相结合，会帮你早日弄懂代码优化。接下来，我带你概要地了解一下代码优化的目标、对象、范围和策略等内容。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:34:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"了解代码优化的目标、对象、范围和策略 代码优化的目标 代码优化的目标，是优化程序对计算机资源的使用。我们平常最关心的就是 CPU 资源，最大效率地利用 CPU 资源可以提高程序的性能。代码优化有时候还会有其他目标，比如代码大小、内存占用大小、磁盘访问次数、网络通讯次数等等。 代码优化的对象 从代码优化的对象看，大多数的代码优化都是在 IR 上做的，而不是在前一阶段的 AST 和后一阶段汇编代码上进行的，为什么呢？ 其实，在 AST 上也能做一些优化，比如在讲前端内容的时候，我们曾经会把一些不必要的 AST 层次削减掉（例如 add-\u003emul-\u003epri-\u003eInt，每个父节点只有一个子节点，可以直接简化为一个 Int 节点），但它抽象层次太高，含有的硬件架构信息太少，难以执行很多优化算法。 在汇编代码上进行优化会让算法跟机器相关，当换一个目标机器的时候，还要重新编写优化代码。所以，在 IR 上是最合适的，它能尽量做到机器独立，同时又暴露出很多的优化机会。 代码优化的范围 从优化的范围看，分为本地优化、全局优化和过程间优化。 优化通常针对一组指令，最常用也是最重要的指令组，就是基本块。基本块的特点是：每个基本块只能从入口进入，从最后一条指令退出，每条指令都会被顺序执行。因着这个特点，我们在做某些优化时会比较方便。比如，针对下面的基本块，我们可以很安全地把第 3 行的“y:=t+x”改成“y:= 3 * x”，因为 t 的赋值一定是在 y 的前面： BB1: t:=2 * x y:=t + x Goto BB2 这种针对基本块的优化，我们叫做本地优化（Local Optimization）。那么另一个问题来了：我们能否把第二行的“t:=2 * x”也优化删掉呢？这取决于是否有别的代码会引用 t。所以，我们需要进行更大范围的分析，才能决定是否把第二行优化掉。超越基本块的范围进行分析，我们需要用到控制流图（Control Flow Graph，CFG）。CFG 是一种有向图，它体现了基本块之前的指令流转关系。如果从 BB1 的最后一条指令是跳转到 BB2，那么从 BB1 到 BB2 就有一条边。一个函数（或过程）里如果包含多个基本块，可以表达为一个 CFG。 如果通过分析 CFG，我们发现 t 在其他地方没有被使用，就可以把第二行删掉。这种针对一个函数、基于 CFG 的优化，叫做全局优化（Global Optimization）。比全局优化更大范围的优化，叫做过程间优化（Inter-procedural Optimization），它能跨越函数的边界，对多个函数之间的关系进行优化，而不是仅针对一个函数做优化。 代码优化的策略 最后，你不需要每次都把代码优化做彻底，因为做代码优化本身也需要消耗计算机的资源。所以，你需要权衡代码优化带来的好处和优化本身的开支这两个方面，然后确定做多少优化。比如，在浏览器里加载 JavaScript 的时候，JavaScript 引擎一定会对 JavaScript 做优化，但如果优化消耗的时间太长，界面的响应会变慢，反倒影响用户使用页面的体验，所以 JavaScript 引擎做优化时要掌握合适的度或调整优化时机。接下来，我带你认识一些常见的代码优化的场景，这样可以让你对代码优化的认识更加直观，然后我们也可以将这部分知识作为后面讨论算法的基础。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:34:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"一些优化的场景 代数优化（Algebraic Optimazation） 代数优化是最简单的一种优化，当操作符是代数运算的时候，你可以根据学过的数学知识进行优化。比如“x:=x+0 ”这行代码，操作前后 x 没有任何变化，所以这样的代码可以删掉；又比如“x:=x0” 可以简化成“x:=0”；对某些机器来说，移位运算的速度比乘法的快，那么“x:=x8”可以优化成“x:=x«3”。 常数折叠（Constant Folding） 它是指，对常数的运算可以在编译时计算，比如 “x:= 20 * 3 ”可以优化成“x:=60”。另外，在 if 条件中，如果条件是一个常量，那就可以确定地取某个分支。比如：“If 2\u003e0 Goto BB2” 可以简化成“Goto BB2”就好了。 删除不可达的基本块 有些代码永远不可能被激活。比如在条件编译的场景中，我们会写这样的程序：“if(DEBUG) {…}”。如果编译时，DEBUG 是一个常量 false，那这个代码块就没必要编译了。 删除公共子表达式（Common Subexpression Elimination） 下面这两行代码，x 和 y 右边的形式是一样的，如果这两行代码之间，a 和 b 的值没有发生变化（比如采用 SSA 形式），那么 x 和 y 的值一定是一样的。 x := a + b y := a + b 那我们就可以让 y 等于 x，从而减少了一次“a+b”的计算，这种优化叫做删除公共子表达式。 x := a + b y := x 拷贝传播（Copy Propagation）和常数传播（Constant Propagation）下面的示例代码中，第三行可以被替换成“z:= 2 * x”， 因为 y 的值就等于 x，这叫做拷贝传播。 x := a + b y := x z := 2 * y 如果 y := 10，常数 10 也可以传播下去，把最后一行替换成 z:= 2 * 10，这叫做常数传播。再做一次常数折叠，就变成 z:=20 了。 死代码删除（Ded code elimination）在上面的拷贝传播中，如果没有其他地方使用 y 变量了，那么第二行就是死代码，就可以删除掉，这种优化叫做死代码删除。 最后我强调一下，一个优化可能导致另一个优化，比如，拷贝传播导致 y 不再被使用，我们又可以进行死代码删除的优化。所以，一般进行多次优化、多次扫描。了解了优化的场景之后，你能直观地知道代码优化到底做了什么事情，不过知其然还要知其所以然，你还需要了解这些优化都是怎么实现的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:34:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"如何做本地优化 上面这些优化场景，可以用于本地优化、全局优化和过程间优化。这节课我们先看看如何做本地优化，因为它相对简单，学习难度较低，下节课再接着讨论全局优化。假设下面的代码是一个基本块（省略最后的终结指令）： a := b c := a + b c := b d := a + b e := a + b 为了优化它们，我们的方法是计算一个“可用表达式（available expression）”的集合。可用表达式，是指存在一个变量，保存着某个表达式的值。 我们从上到下顺序计算这个集合： 一开始是空集。2. 经过第一行代码后，集合里增加了“a:=b”；3. 经过第二行代码后，增加了“c:=a+b”。4. 注意，在经过第三行代码以后，由于变量 c 的定义变了，所以“c:=a+b”不再可用，而是换成了“c:=b”。 你能看到，代码“e:=a+b”，和集合中的“d:=a+b”等号右边部分是相同的，所以我们首先可以删除公共子表达式，优化成“e:=d”。变成下面这样： 然后，我们可以做一下拷贝传播，利用“a:=b”，把表达式中的多个 a 都替换成 b。 到目前为止，a 都被替换成了 b，对 e 的计算也简化了，优化后的代码变成了下面这样： a := b c := b + b c := b d := b + b e := d 观察一下这段代码，它似乎还存在可优化的空间，比如，会存在死代码，而我们可以将其删除。假设，在后序的基本块中，b 和 c 仍然会被使用，但其他变量就不会再被用到了。那么，上面这 5 行代码哪行能被删除呢？这时，我们要做另一个分析：活跃性分析（Liveness Analysis）。我们说一个变量是活的，意思是它的值在改变前，会被其他代码读取。（对于 SSA 格式的 IR，变量定义出来之后就不会再改变，所以你只要看后面的代码有没有使用这个变量的就可以了）我们会分析每个变量的活跃性，把死的变量删掉。怎么做呢？我们这次还是要借助一个集合，不过这个集合是从后向前，倒序计算的。 一开始集合里的元素是{b, c}，这是初始值，表示 b 和 c 会被后面的代码使用，所以它们是活的。扫描过“e := d”后，因为用到了 d，所以 d 是活的，结果是{b, c, d}。再扫描“d := b + b”，用到了 b，但集合里已经有 b 了；这里给 d 赋值了，已经满足了后面代码对 d 的要求，所以可以从集合里去掉 d 了，结果是{b，c}。再扫描“c := b”，从集合里去掉 c，结果是{b}。继续扫描，一直到第一行，最后的集合仍然是{b}。 现在，基于这个集合，我们就可以做死代码删除了。当给一个变量赋值时，它后面跟着的集合没有这个变量，说明它不被需要，就可以删掉了。图中标橙色的三行，都是死代码，都可以删掉。 删掉以后，只剩下了两行代码。注意，由于“ e := d”被删掉了，导致 d 也不再被需要，变成了死变量。 把变量 d 删掉以后，就剩下了一行代码“c := b”了。 到此为止，我们完成了整个的优化过程，5 行代码优化成了 1 行代码，成果是很显著的！ 我来带你总结一下这个优化过程： 我们首先做一个正向扫描，进行可用表达式分析，建立可用表达式的集合，然后参照这个集合替换公共子表达式，以及做拷贝传播。接着，我们做一个反向扫描，进行活跃性分析，建立活变量的集合，识别出死变量，并依据它删除给死变量赋值的代码。上述优化可能需要做不止一遍，才能得到最后的结果。 这样看来，优化并不难吧？当然了，目前我们做的优化是基于一段顺序执行的代码，没有跳转，都是属于一个基本块的，属于本地优化。直观地理解了本地优化之后，我们可以把这种理解用更加形式化的方式表达出来，这样，你可以理解得更加透彻。本地优化中，可用表达式分析和活跃性分析，都可以看做是由下面 4 个元素构成的： D（方向）。是朝前还是朝后遍历。V（值）。代码的每一个地方都要计算出一个值。可用表达式分析和活跃性分析的值是一个集合，也有些分析的值并不是集合，在下一讲你会看到这样的例子。F（转换函数，对 V 进行转换）。比如，在做可用表达式分析的时候，遇到了“c := b”时，可用表达式的集合从{a := b, c := a + b}转换成了{a := b, c := b}。这里遵守的转换规则是：因为变量 c 被重新赋值了，那么就从集合里，把变量 c 原来的定义去掉，并把带有 c 的表达式都去掉，因为过去的 c 已经失效了，然后，把变量 c 新的定义加进去。I（初始值，是算法开始时 V 的取值）。做可用表达式分析的时候，初始值是空集。在做活跃性分析的时候，初始值是后面代码中还会访问的变量，也就是活变量。 这样形式化以后，我们就可以按照这个模型来统一理解各种本地优化算法。接下来，我们来体验和熟悉一下 LLVM 的优化功能。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:34:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"用 LLVM 来演示优化功能 在25 讲中，我们曾经用 Clang 命令带上 O2 参数来生成优化的 IR： clang -emit-llvm -S -O2 fun1.c -o fun1-O2.ll 实际上，LLVM 还有一个单独的命令 opt，来做代码优化。缺省情况下，它的输入和输出都是.bc 文件，所以我们还要在.bc 和.ll 两种格式之间做转换。 clang -emit-llvm -S fun1.c -o fun1.ll //生成LLVM IR llc fun1.ll -o fun1.bc //编译成字节码 opt -O2 fun1.bc -o fun1-O2.bc //做O2级的优化 llvm-dis fun1-O2.bc -o fun1-O2.ll //将字节码反编译成文本格式 其中要注意的一点，是要把第一行命令生成的 fun1.ll 文件中的“optnone”这个属性去掉，因为这个它的意思是不要代码优化。我们还可以简化上述操作，给 opt 命令带上 -S 参数，直接对.ll 文件进行优化： opt -S -O2 fun1.ll -o fun1-O2.ll 另外，我解释一下 -O2 参数：-O2 代表的是二级优化，LLVM 中定义了多个优化级别，基本上数字越大，所做的优化就越多。我们可以不使用笼统的优化级别，而是指定采用某个特别的优化算法，比如 mem2reg 算法，会把对内存的访问优化成尽量访问寄存器。 opt -S -mem2reg fun1.ll -o fun1-O2.ll 用 opt –help 命令，可以查看 opt 命令所支持的所有优化算法。对于常数折叠，在调用 API 生成 IR 的时候，LLVM 缺省就会去做这个优化。比如下面这段代码，是返回 2+3 的值，但生成 IR 的时候直接变成了 5，因为这种优化比较简单，不需要做复杂的分析： Function * codegen_const_folding(){ //创建函数 FunctionType *funType = FunctionType::get(Type::getInt32Ty(TheContext), false); Function *fun = Function::Create(funType, Function::ExternalLinkage, \"const_folding\", TheModule.get()); //创建一个基本块 BasicBlock *BB = BasicBlock::Create(TheContext, \"\", fun); Builder.SetInsertPoint(BB); Value * tmp1 = ConstantInt::get(TheContext, APInt(32, 2, true)); Value * tmp2 = ConstantInt::get(TheContext, APInt(32, 3, true)); Value * tmp3 = Builder.CreateAdd(tmp1, tmp2); Builder.CreateRet(tmp3); return fun; } 生成的 IR 如下： define i32 @const_folding() { ret i32 5 } 你需要注意，很多优化算法，都是要基于寄存器变量来做，所以，我们通常都会先做一下 -mem2reg 优化。在 LLVM 中，做优化算法很方便，因为它采用的是 SSA 格式。具体来说，LLVM 中定义了 Value 和 User 两个接口，它们体现了 LLVM IR 最强大的特性，即静态单赋值中的定义 - 使用链，这种定义 - 使用关系会被用到优化算法中。 在26 讲中，我们已经讲过了 Value 类。如果一个类是从 Value 继承的，意味着它定义了一个值。另一个类是 User 类，函数和指令也是 User 类的子类，也就是说，在函数和指令中，可以使用别的地方定义的值。 这两个类是怎么帮助到优化算法中的呢？在 User 中，可以访问所有它用到的 Value，比如一个加法指令（%c = add nsw i32 %a, %b）用到了 a 和 b 这两个变量。而在 Value 中，可以访问所有使用这个值的 User，比如给 c 赋值的这条指令。所以，你可以遍历一个 Value 的所有 User，把它替换成另一个 Value，这就是拷贝传播。 接下来，我们看看如何用程序实现 IR 的优化。在 LLVM 内部，优化工作是通过一个个的 Pass（遍）来实现的，它支持三种类型的 Pass：一种是分析型的 Pass（Analysis Passes），只是做分析，产生一些分析结果用于后序操作。一些是做代码转换的（Transform Passes），比如做公共子表达式删除。还有一类 pass 是工具型的，比如对模块做正确性验证。你可以查阅 LLVM 所支持的各种 Pass。 下面的代码创建了一个 PassManager，并添加了两个优化 Pass： // 创建一个PassManager TheFPM = std::make_unique\u003clegacy::FunctionPassManager\u003e(TheModule.get()); // 窥孔优化和一些位计算优化 TheFPM-\u003eadd(createInstructionCombiningPass()); // 表达式重关联 TheFPM-\u003eadd(createReassociatePass()); TheFPM-\u003edoInitialization(); 之后，再简单地调用 PassManager 的 run() 方法，就可以对代码进行优化： TheFPM-\u003erun(*fun); 你可以查看本讲附带的代码，尝试自己编写一些示例程序，查看优化前和优化后的效果。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:34:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我带你学习了代码优化的原理，然后通过 LLVM 实践了一下，演示了优化功能，我希望你能记住几个关键点：1. 代码优化分为本地优化、全局优化和过程间优化三个范围。有些优化对于这三个范围都是适用的，但也有一些优化算法是全局优化和过程间优化专有的。2. 可用表达式分析和活跃性分析是本地优化时的两个关键算法。这些算法都是由扫描方向、值、转换函数和初始值这四个要素构成的。3.LLVM 用 pass 来做优化，你可以通过命令行或程序来使用这些 Pass。你也可以编写自己的 Pass。最后，我建议你多编写一些测试代码，并用 opt 命令去查看它的优化效果，在这个过程中增加对代码优化的感性认识。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:34:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"28 | 数据流分析：你写的程序，它更懂 上一讲，我提到了删除公共子表达式、拷贝传播等本地优化能做的工作，其实，这几个工作也可以在全局优化中进行。只不过，全局优化中的算法，不会像在本地优化中一样，只针对一个基本块。而是更复杂一些，因为要覆盖多个基本块。这些基本块构成了一个 CFG，代码在运行时有多种可能的执行路径，这会造成多路径下，值的计算问题，比如活跃变量集合的计算。当然了，还有些优化只能在全局优化中做，在本地优化中做不了，比如： 代码移动（code motion）能够将代码从一个基本块挪到另一个基本块，比如从循环内部挪到循环外部，来减少不必要的计算。部分冗余删除（Partial Redundancy Elimination），它能把一个基本块都删掉。 总之，全局优化比本地优化能做的工作更多，分析算法也更复杂，因为 CFG 中可能存在多条执行路径。不过，我们可以在上一节课提到的本地优化的算法思路上，解决掉多路径情况下，V 值的计算问题。而这种基于 CFG 做优化分析的方法框架，就叫做数据流分析。本节课，我会把全局优化的算法思路讲解清楚，借此引入数据流分析的完整框架。而且在解决多路径情况下，V 值的计算问题时，我还会带你学习一个数学工具：半格理论。这样，你会对基于数据流分析的代码优化思路建立清晰的认识，从而有能力根据需要编写自己的优化算法。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:35:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"数据流分析的场景：活跃性分析 上一讲，我已经讲了本地优化时的活跃性分析，那时，情况比较简单，你不需要考虑多路径问题。而在做全局优化时，情况就要复杂一些：代码不是在一个基本块里简单地顺序执行，而可能经过控制流图（CFG）中的多条路径。我们来看一个例子（例子由 if 语句形成了两条分支语句）： 基于这个 CFG，我们可以做全局的活跃性分析，从最底下的基本块开始，倒着向前计算活跃变量的集合（也就是从基本块 5 倒着向基本块 1 计算）。这里需要注意，对基本块 1 进行计算的时候，它的输入是基本块 2 的输出，也就是{a, b, c}，和基本块 3 的输出，也就是{a, c}，计算结果是这两个集合的并集{a, b, c}。也就是说，基本块 1 的后序基本块，有可能用到这三个变量。这里就是与本地优化不同的地方，我们要基于多条路径来计算。 基于这个分析图，我们马上发现 y 变量可以被删掉（因为它前面的活变量集合{x}不包括 y，也就是不被后面的代码所使用），并且影响到了活跃变量的集合。 删掉 y 变量以后，再继续优化一轮，会发现 d 也可以删掉。 d 删掉以后，2 号基本块里面已经没有代码了，也可以被删掉，最后的 CFG 是下面这样： 到目前为止，我们发现：全局优化总体来说跟本地优化很相似，唯一的不同，就是要基于多个分支计算集合的内容（也就是 V 值）。在进入基本块 1 时，2 和 3 两个分支相遇（meet），我们取了 2 和 3V 值的并集。这就是数据流分析的基本特征，你可以记住这个例子，建立直观印象。 但是，上面这个 CFG 还是比较简单的，因为它没有循环，属于有向无环图。这种图的特点是：针对图中的每一个节点，我们总能找到它的前序节点和后序节点，所以我们只需要按照顺序计算就好了。但是如果加上了环路，就不那么简单了，来看一看下面这张图： 基本块 4 有两个后序节点，分别是 5 和 1，所以要计算 4 的活跃变量，就需要知道 5 和 1 的输出是什么。5 的输出好说，但 1 的呢？还没计算出来呢。因为要计算 1，就要依赖 2 和 3，从而间接地又依赖了 4。这样一来，1 和 4 是循环依赖的。再进一步探究的话，你发现其实 1、2、3、4 四个节点之间，都是循环依赖的。所以说，一旦在 CFG 中引入循环回路，严格的前后计算顺序就不存在了。那你要怎么办呢？ 其实，我们不是第一次面对这个处境了。在前端部分，我们计算 First 和 Follow 集合的时候，就会遇到循环依赖的情况，只不过那时候没有像这样展开，细细地分析。不过，你可以回顾一下17 讲和18 讲，那个时候你是用什么算法来破解僵局的呢？是不动点法。在这里，我们还是要运用不动点法，具体操作是：给每个基本块的 V 值都分配初始值，也就是空集合。 然后对所有节点进行多次计算，直到所有集合都稳定为止。第一遍的时候，我们按照 5-4-3-2-1 的顺序计算（实际上，采取任何顺序都可以），计算结果如下： 如果现在计算就结束，我们实际上可以把基本块 2 中的 d 变量删掉。但如果我们再按照 5-4-3-2-1 的顺序计算一遍，就会往集合里增加一些新的元素（在图中标的是橙色）。这是因为，在计算基本块 4 的时候，基本块 1 的输出{b, c, d}也会变成 4 的输入。这时，我们发现，进入基本块 2 时，活变量集合里是含有 d 的，所以 d 是不能删除的。 你再仔细看看，这个 d 是哪里需要的呢？是基本块 3 需要的：它会跟 1 去要，1 会跟 4 要，4 跟 2 要。所以，再次证明，1、2、3、4 四个节点是互相依赖的。我们再来看一下，对于活变量集合的计算，当两个分支相遇的情况下，最终的结果我们取了两个分支的并集。 在上一讲，我们说一个本地优化分析包含四个元素：方向（D）、值（V）、转换函数（F）和初始值（I）。在做全局优化的时候，我们需要再多加一个元素，就是两个分支相遇的时候，要做一个运算，计算他们相交的值，这个运算我们可以用大写的希腊字母Λ（lambda）表示。包含了 D、V、F、I 和Λ的分析框架，就叫做数据流分析。那么Λ怎么计算呢？研究者们用了一个数学工具，叫做“半格”（Semilattice），帮助做Λ运算。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:35:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"直观地理解半格理论 如果要从数学理论角度完全把“半格”这个概念说清楚，需要依次介绍清楚“格”（Lattice）、“半格”（Semilattice）和“偏序集”（Partially Ordered Set）等概念。我想这个可以作为爱好数学的同学的一个研究题目，或者去向离散数学的老师求教。在我们的课程里，我只是通过举例子，让你对它有直观的认识。首先，半格是一种偏序集。偏序集就是集合中只有部分成员能够互相比较大小。举例来说会比较直观。在做全局活跃性分析的时候，{a, b, c}和{a, c}相遇，产生的新值是{a, b, c}。我们形式化地写成{a, b, c} Λ {a, c} = {a, b, c}。 这时候我们说{a, b, c}是可以跟{a, c}比较大小的。那么哪个大哪个小呢？如果 XΛY=X，我们说 X\u003c=Y。 所以，{a, b, c}是比较小的，{a, c}是比较大的。当然，{a, b, c}也可以跟{a, b}比较大小，但它没有办法跟{c, d}比较大小。所以把包含了{ {a, b, c},{a, c},{a, b},{c, d} }这样的一个集合，叫做偏序集，它们中只有部分成员之间可以比较大小。哪些成员可以比较呢？就是下面的半格图中，可以通过有方向的线连起来的。半格可以画成图形，理解起来更直观，假设我们的程序只有 a, b, c 三个变量，那么这个半格画成图形是这样的： 沿着上面图中的线，两个值是可以比较大小的，按箭头的方向依次减少：{}\u003e{a}\u003e{a, b}\u003e {a, b, c}。如果两个值之间没有一条路径，那么它们之间就是不能比较大小的，就像{a}和{b}就不能比较大小。对于这个半格，我们把{}（空集）叫做 Top，Top 大于所有的其他的值。而{a, b, c}叫做 Bottom，它是最小的值。在做活跃性分析时，我们的Λ运算是计算两个值的最大下界（Greatest Lower Bound）。怎么讲呢？就是比两个原始值都小的值中，取最大的那个。{a}和{b}的最大下界是{a, b}，{a, b, c} 和{a, c}的最大下界就是{a, b, c} 。 如果一个偏序集中，任意两个元素都有最大下界，那么这个偏序集就叫做交半格（Meet Semilattice）。 与此相对应的，如果集合中的每个元素都有最小上界（Least Upper Bound），那么这个偏序集叫做并半格（Join Semilattice）。如果一个偏序集既是交半格，又是并半格，我们说这个偏序集是一个格，示例的这个偏序集就是一个格。 你可能会奇怪，为什么要引入这么复杂的一套数学工具呢？不就是集合运算吗？两个分支相遇，就计算它们的并集，不就可以了吗？事情没那么简单。因为并不是所有的分析，其 V 值都是一个集合，就算是集合，相交时的运算也不一定是求并集，而有可能是求交集。我们通过另一个案例来分析一下非集合的半格运算：常数传播。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:35:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"数据流分析的场景：常数传播 常数传播，就是如果知道某个变量的值是个常数，那么就把用到这个变量的表达式，都用常数去替换。看看下面的例子，在基本块 4 中，a 的值能否用一个常数替代？ 答案是不能。到达基本块 4 的两条路径，一条 a=3，另一条 a=4。我们不知道在实际运行的时候，会从哪条路径过来，所以这个时候 a 的取值是不确定的，基本块 4 中的 a 无法用常数替换。 那么，运用数据流分析的框架怎么来做常数传播分析呢？在这种情况下，V 不再是一个集合，而是 a 可能取的常数值，但 a 有可能不是一个常数啊，所以我们再定义一个特殊的值：Top（T）。除了 T 之外，我们再引入一个与 T 对应的特殊值：Bottom（它的含义是，某个语句永远不会被执行）。总结起来，常数传播时，V 的取值可能是 3 个： 常数 c Top：意思是 a 的值不是一个常数 Bottom：某个语句不会被执行。 这些值是怎么排序的呢？最大的是 Top，中间各个常数之间是无法比较的，Bottom 是最小的。 接下来，我们看看如何计算多个 V 值相交的值。我们再把计算过程形式化一下。在这个分析中，当我们经过每个语句的时候，V 值都可能发生变化，我们用下面两个函数来代表不同地方的 V 值：C(a, s, in)。表示在语句 s 之前 a 的取值，比如，C(a, b:=a+2, in) = 3。C(a, s, out)。表示在语句 s 之后 a 的取值，比如，C(a, a:=4, out) = 4。 如果 s 的前序有 i 条可能的路径，那么多个输出和一个输入“C(a, si, out) 和 C(a, s, in)”的关系，可以制定一系列规则： 如果有一条输入路径是 Top，或者说 C(a, si, out) 是 Top，那么结果 C(a, s, in) 就是 Top。2. 如果输入中有两个不同的常数，比如 3 和 4，那么结果也是 Top（我们的示例就是这种情况）。3. 如果所有的输入都是相同的常数或 Bottom，那么结果就是该常数。如果所有路径 a 的值都是 3，那么这里就可以安全地认为 a 的值是 3。那些 Bottom 路径不影响，因为整条路径不会执行。4. 如果所有的输入都是 Bottom，那么结果也是 Bottom。 上面的这 4 个规则，就是一套半格的计算规则。在这里，我们也可以总结一下它的转换规则，也就是 F，考虑一下某个 Statement 的 in 值和 out 值的关系，也就是经过该 Statement 以后，V 值会有啥变化： 如果输入是 Bottom，那么输出也是 Bottom。也就是这条路径不会经过。2. 如果该 Statement 就是“ a := 常数”，那么输出就是该常数。3. 如果该 Statement 是 a 赋予的一个比较复杂的表达式，而不是常数，那么输出就是 Top。4. 如果该 Statement 不是对 a 赋值的，那么 V 值保持不变。 好了，转换函数 F 也搞清楚了。初始值 I 是什么呢？是 Top，因为一开始的时候，a 还没有赋值，所以不会是常数；方向 D 是什么呢？D 是向下。这个时候，D、V、F、I 和Λ5 个元素都清楚了，我们就可以写算法实现了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:35:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我们基于全局优化分析的任务，介绍了数据流分析这个框架，并且介绍了半格这个数学工具。我希望你在本讲记住几个要点：全局分析比本地分析多处理的部分就是 CFG，因为有了多条执行分支，所以要计算分支相遇时的值，当 CFG 存在环路的时候，要用不动点法来计算出所有的 V 值。数据流分析框架包含方向（D）、值（V）、转换函数（F）、初始值（I）和交运算（Λ）5 个元素，只要分析清楚这 5 个元素，就可以按照固定的套路来编写分析程序。对于半格理论，关键是要知道如何比较偏序集中元素的大小，理解了这个核心概念，那么求最大下界、最小上界这些也就没有问题了。 数据流分析也是一个容易让学习者撞墙的知识点，特别是再加上“半格”这样的数学术语的时候。不过，我们通过全局活跃性分析和全局常数传播的示例，对“半格”的抽象数学概念建立了直觉的理解。遇到全局分析的任务，你也应该能够比照这两个示例，设计出完整的数据流分析的算法了。不过我建议你，还是要按照上一讲中对 LLVM 优化功能的介绍，多做几个例子实验一下。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:35:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"29 | 目标代码的生成和优化（一）：如何适应各种硬件架构？ 在编译器的后端，我们要能够针对不同的计算机硬件，生成优化的代码。在23 讲，我曾带你试着生成过汇编代码，但当时生成汇编代码的逻辑是比较幼稚的，一个正式的编译器后端，代码生成部分需要考虑得更加严密才可以。那么具体要考虑哪些问题呢？其实主要有三点： 指令的选择。同样一个功能，可以用不同的指令或指令序列来完成，而我们需要选择比较优化的方案。寄存器分配。每款 CPU 的寄存器都是有限的，我们要有效地利用它。指令重排序。计算执行的次序会影响所生成的代码的效率。在不影响运行结果的情况下，我们要通过代码重排序获得更高的效率。 我会用两节课的时间，带你对这三点问题建立直观认识，然后，我还会介绍 LLVM 的实现策略。这样一来，你会对目标代码的生成，建立比较清晰的顶层认知，甚至可以尝试去实现自己的算法。 接下来，我们针对第一个问题，聊一聊为什么需要选择指令，以及如何选择指令。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:36:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"选择正确的指令 你可能会问：我们为什么非要关注指令的选择呢？我来做个假设。如果我们不考虑目标代码的性能，可以按照非常机械的方式翻译代码。比如，我们可以制定一个代码翻译的模板，把形如“a := b + c”的代码都翻译成下面的汇编代码： mov b, r0 //把b装入寄存器r0 add c, r0 //把c加到r0上 mov r0, a //把r0存入a 那么，下面两句代码： a := b + c d := a + e 将被机械地翻译成： mov b, r0 add c, r0 mov r0, a mov a, r0 add e, r0 mov r0, d 你可以从上面这段代码中看到，第 4 行其实是多余的，因为 r0 的值就是 a，不用再装载一遍了。另外，如果后面的代码不会用到 a（也就是说 a 只是个临时变量），那么第 3 行也是多余的。这种算法很幼稚，正确性没有问题，但代码量太大，代价太高。所以我们最好用聪明一点儿的算法来生成更加优化的代码。这是我们要做指令选择的原因之一。做指令选择的第二个原因是，实现同一种功能可以使用多种指令，特别是 CISC 指令集（可替代的选择很多，但各自有适用的场景）。对于某个 CPU 来说，完成同样的任务可以采用不同的指令。比如，实现“a := a + 1”，可以生成三条代码： mov a, r0 add $1, r0 mov r0, a 也可以直接用一行代码，采用 inc 指令，而我们要看看用哪种方法总体代价最低： inc a 第二个例子，把 r0 寄存器置为 0，也可以有多个方法： mov $0, r0 //赋值为立即数0 xor r0, r0 //异或操作 sub r0, r0 //用自身的值去减 ... 再比如，a * 7 可以用 a«3 - a 实现：首先移位 3 位，相当于乘 8，然后再减去一次 a，就相当于乘以 7。虽然用了两条指令，但是，可能消耗的总的时钟周期更少。 在这里我想再次强调一下，无论是为了生成更简短的代码，还是从多种可能的指令中选择最优的，我们确实需要关注指令的选择。那么，我们做指令选择的思路是什么呢？目前最成熟的算法都是基于树覆盖的方法，我通过一个例子带你了解一下，什么是树覆盖算法。a[i] = b 这个表达式的意思是，给数组 a 的第 i 个元素赋值为 b。假设 a 和 b 都是栈里的本地变量，i 是放在寄存器 ri 中。这个表达式可以用一个 AST 表示。 你可能觉得这棵树看着像 AST，但又不大像，那是因为里面有 mem 节点（意思是存入内存）、mov 节点、栈指针 (fp)。它可以算作低级（low-level）AST，是一种 IR 的表达方式，有时被称为结构化 IR。这个 AST 里面包含了丰富的运行时的细节信息，相当于把 LLVM 的 IR 用树结构来表示了。你可以把一个基本块的指令都画成这样的树状结构。基于这棵树，我们可以翻译成汇编代码： load M[fp+a], r1 //取出数组开头的地址，放入r1，fp是栈桢的指针，a是地址的偏移量 addi 4, r2 //把4加载到r2 mul ri, r2 //把ri的值乘到r2上，即i*4，即数组元素的偏移量，每个元素4字节 add r2, r1 //把r2加到r1上，也就是算出a[i]的地址 load M[fp+b], r2 //把b的值加载到r2寄存器 store r2, M[r1] //把r2写入地址为r1的内存 在这里，我用了一种假想的汇编代码，跟 LLVM IR 有点儿像，但更简化、易读： 注意，我们生成的汇编代码还是比较精简的。如果采用比较幼稚的方法，逐个树节点进行翻译，代码会很多，你可以手工翻译试试看。用树覆盖的方法可以大大减少代码量，其中用橙色的线包围的部分被形象地叫做一个瓦片 (tiling)，那些包含了操作符的瓦片，就可以转化成一条指令。每个瓦片可以覆盖多个节点，所以生成的指令比较少。 那我们是用什么来做瓦片的呢？原来，每条机器指令，都会对应 IR 的一些模式（Pattern），可以表示成一些小的树，而这些小树就可以当作瓦片： 我们的算法可以遍历 AST，遇到上面的模式，就可以生成对应的指令。以 load 指令为例，它有几个模式：任意一个节点加上一个常量就行，这相当于汇编语言中的间接地址访问；或者 mem 下直接就是一个常量就行，这相当于是直接地址访问。最后，地址值还可以由下级子节点计算出来。 所以，从一棵 AST 生成代码的过程，就是用上面这些小树去匹配一棵大树，并把整个大树覆盖的过程，所以叫做树覆盖算法。2、4、5、6、8、9 这几个节点依次生成汇编代码。要注意的是，覆盖方式可能会有多个，比如下面这个覆盖方式，相比之前的结果，它在 8 和 9 两个瓦片上是有区别的： 生成的汇编代码最后两句也不同： load M[fp+a], r1 //取出数组开头的地址，放入r1，fp是栈桢的指针，a是地址的偏移量 addi 4, r2 //把4加载到r2 mul ri, r2 //把ri的值乘到r2上，即i*4，即数组元素的偏移量，每个元素4字节 add r2, r1 //把r2加到r1上，也就是算出a[i]的地址 addi fp+b, r2 //把fp+b的值加载到r2寄存器 movm M[r2], M[r1] //把地址为r2到值拷贝到地址为r1内存里 你可以体会一下，这两个覆盖方式的差别：对于瓦片 8 中的加法运算，一个当做了间接地址的计算，一个就是当成加法；对于根节点的操作，一个翻译成从 store，把寄存器中的 b 的值写入到内存。一个翻译成 movm 指令，直接在内存之间拷贝值。至于这两种翻译方法哪种更好，比较总体的性能哪个更高就行了。 到目前为止，你已经直观地了解了为什么要进行指令选择，以及最常用的树覆盖方法了。当然了，树覆盖算法有很多，比如 Maximal Munch 算法、动态规划算法、树文法等，LLVM 也有自己的算法。 简单地说一下 Maximal Munch 算法。Maximal Munch 直译成中文，是每次尽量咬一大口的意思。具体来说，就是从树根开始，每次挑一个能覆盖最多节点的瓦片，这样就形成几棵子树。对每棵子树也都用相同的策略，这样会使得生成的指令是最少的。注意，指令的顺序要反过来，按照深度优先的策略，先是叶子，再是树根。这个算法是 Optimal 的算法。Optimal 被翻译成最佳，我不太赞正这种翻译方法，翻译成“较优”会比较合适，它指的是在局部，相邻的两个瓦片不可能连接成代价更低的瓦片。覆盖算法除了 Optimal 的还有 Optimum 的，Optimum 是全局最优化的状态，就是代码总体的代价是最低的。关于其他算法的细节在本节课就不展开了，因为根据我的经验，在学指令选择时，最重要的还是建立图形化的、直观的理解，理解什么是瓦片，如何覆盖会得到最优的结果。接下来，我们继续探讨开篇提到的第二个问题：寄存器分配。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:36:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"分配寄存器 寄存器优化的任务是：最大程度地利用寄存器，但不要超过寄存器总数量的限制。 因为我们生成 IR 时，是不知道目标机器的信息的，也就不知道目标机器到底有几个寄存器可以用，所以我们在 IR 中可以使用无限个临时变量，每个临时变量都代表一个寄存器。现在既然要生成针对目标机器的代码，也就知道这些信息了，那么就要把原来的 IR 改写一下，以便使用寄存器时不超标。那么寄存器优化的原理是什么呢？我用一个例子带你了解一下。下图左边的 IR 中，a、d、f 这三个临时变量不会同时出现。假设 a 和 d 在这个代码块之后成了死变量，那么这三个变量可以共用同一个寄存器，就像右边显示的那样： 实际上，这三行代码是对“b + c + e + 10”这个表达式的翻译，所以 a 和 d 都是在转换为 IR 时引入的中间变量，用完就不用了。这和在 23 讲，我们把 8 个参数以及一个本地变量相加时，只用了一个寄存器来一直保存累加结果，是一样的。所以，通过这个例子，你可以直观地理解寄存器共享的原则：如果存在两个临时变量 a 和 b，它们在整个程序执行过程中，最多只有一个变量是活跃的，那么这两个变量可以共享同一个寄存器。在27和28 讲中，你已经学过了如何做变量的活跃性分析，所以你可以很容易分析出，在任何一个程序点，活跃变量的集合。然后，你再看一下，哪些变量从来没有出现在同一个集合中就行。看看下面的这个图： 上图中，凡是出现在同一个花括号里的变量，都不能共享寄存器，因为它们在某个时刻是同时活跃的。那 a 到 f，哪些变量从来没碰到过呢？我们再画一个图来寻找一下。下图中，每个临时变量作为一个节点，如果两个变量同时存在过，就画一条边。这样形成的图，叫做寄存器干扰图 (Register Interference Graph, RIG)。在这张图里，凡是没有连线的两个变量，就可以分配到同一个寄存器，例如，a 和 b，b 和 d，a 和 d，b 和 e，a 和 e。 那么问题来了：针对这个程序，我们一共需要几个寄存器？怎么分配呢？一个比较常用的算法是图染色算法：只要两个节点之间有连线，节点就染成不同的颜色。最后所需要的最少颜色，就是所需要的寄存器的数量。我画了两个染色方案，都是需要 4 种颜色： 不过我们是手工染色的，那么如何用算法来染色呢？假如一共有 4 个寄存器，我们想用算法知道寄存器是否够用？应该如何染色？染色算法很简单。如果想知道 k 个寄存器够不够用，你只需要找到一个少于 k 条边的节点，把它从图中去掉。接着再找下一个少于 k 条边的节点，再去掉。如果最后整个图都被删掉了，那么这个图一定可以用 k 种颜色来染色。 为什么呢？因为如果一个图（蓝色边的）是能用 k 种颜色染色的，那么再加上一个节点，它的边的数量少于 k 个，比如是 n，那么这个大一点儿的图（橙色边的）还是可以用 k 种颜色染色的。道理很简单，因为加进来的节点的边数少于 k 个，所以一定能找到一个颜色，与这个点的 n 个邻居都不相同。所以，我们把刚才一个个去掉节点的顺序反过来，把一个个节点依次加到图上，每加上一个，就找一个它的邻居没有用的颜色来染色就行了。整个方法简单易行。但是，如果所需要寄存器比实际寄存器的数量多，该怎么办呢？当然是用栈了。这个问题就是寄存器溢出（Register Spilling），溢出到栈里去，我在21 讲关于运行时机制时提到过，像本地变量、参数、返回值等，都尽量用寄存器，如果寄存器不够用，那就放到栈里。另外再说一下，无论放在寄存器里，还是栈里，都是活动记录的组成部分，所以活动记录这个概念比栈桢更广义。还是拿上面的例子来说，如果只有 3 个寄存器，那么要计算一下 3 个寄存器够不够用。我们先把 a 和 b 从图中去掉： 这时你发现，剩下的 4 个节点，每个节点都有 3 个邻居。所以，3 个寄存器肯定不够用，必须要溢出一个去。我们可以选择让 f 保存在栈里，把 f 去掉以后，剩下的 c，d，e 可以用 3 种颜色成功染色。这就结束了吗？当然没有。f 虽然被保存到了栈里，但每次使用它的时候，都要 load 到一个临时变量，也就是寄存器中。每次保存 f，也都要用一个临时变量写入到内存。所以，我们要把原来的代码修改一下，把每个使用 f 的地方，都加上一条 load 或 save 指令，以便在使用 f 的时候把 f 放到寄存器，用完后再写回内存。修改后的 CFG 如下： 因为原来有 4 个地方用到了 f，所以我们引入了 f1 到 f4 四个临时变量。这样的话，总的临时变量反而变多了，从 6 个到了 9 个。不过没关系，虽然临时变量更多了，但这几个临时变量的生存期都很短，图里带有 f 的活跃变量集合，比之前少多了。所以，即使有 9 个临时变量，也能用三种颜色染色，如下图所示： 最后，在选择把哪个变量溢出的时候，你实际上是要有所选择的。你最好选择使用次数最少的变量。在程序内循环中的变量，就最好不要溢出，因为每次循环都会用到它们，还是放在寄存器里性能更高。目前为止，代码生成中的第二项重要工作，分配寄存器就概要地讲完了。我留给你一段时间消化本节课的内容，在下一讲，我会接着讲指令重排序和 LLVM 的实现。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:36:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 目标代码生成过程中有三个关键知识点：指令选择、寄存器分配和指令重排序，本节课，我讲了前两个，期望能帮你理解这两个问题的实质，让你对指令选择和寄存器分配这两个问题建立直观理解。这样你再去研究不同的算法时，脑海里会有这两个概念的顶层的、图形化的认识，事半功倍。与此同时，本节课我希望你记住几个要点如下：相同的 IR 可以由不同的机器指令序列来实现。你要理解瓦片为什么长那个样子，并且在大脑里建立用瓦片覆盖一棵 AST 的直观印象，最好具备多种覆盖方式，从而把这个问题由抽象变得具象。寄存器分配是编译器必须要做的一项工作，它把可以使用无限多寄存器的 IR，变成了满足物理寄存器数量的 IR，超出的要溢出到内存中保管。染色算法是其中一个可行的算法。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:36:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"30 | 目标代码的生成和优化（二）：如何适应各种硬件架构？ 前一讲，我带你了解了指令选择和寄存器分配，本节课我们继续讲解目标代码生成的，第三个需要考虑的因素：指令重排序（Instruction Scheduling）。我们可以通过重新排列指令，让代码的整体执行效率加快。那你可能会问了：就算重新排序了，每一条指令还是要执行啊？怎么就会变快了呢？别着急，本节课我就带你探究其中的原理和算法，来了解这个问题。而且，我还会带你了解 LLVM 是怎么把指令选择、寄存器分配、指令重排序这三项工作组织成一个完整流程，完成目标代码生成的任务的。这样，你会对编译器后端的代码生成过程形成完整的认知，为正式做一些后端工作打下良好的基础。首先，我们来看看指令重排序的问题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:37:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"指令重排序 如果你有上面的疑问，其实是很正常的。因为我们通常会把 CPU 看做一个整体，把 CPU 执行指令的过程想象成，依此检票进站的过程，改变不同乘客的次序，并不会加快检票的速度。所以，我们会自然而然地认为改变顺序并不会改变总时间。但当我们进入 CPU 内部，会看到 CPU 是由多个功能部件构成的。下图是 Ice Lake 微架构的 CPU 的内部构成（从Intel 公司的技术手册中获取）： 在这个结构中，一条指令执行时，要依次用到多个功能部件，分成多个阶段，虽然每条指令是顺序执行的，但每个部件的工作完成以后，就可以服务于下一条指令，从而达到并行执行的效果。这种结构叫做流水线（pipeline）结构。我举例子说明一下，比如典型的 RISC 指令在执行过程会分成前后共 5 个阶段。 IF：获取指令；ID（或 RF）：指令解码和获取寄存器的值；EX：执行指令；ME（或 MEM）：内存访问（如果指令不涉及内存访问，这个阶段可以省略）；WB：写回寄存器。 对于 CISC 指令，CPU 的流水线会根据指令的不同，分成更多个阶段，比如 7 个、10 个甚至更多。在执行指令的阶段，不同的指令也会由不同的单元负责，我们可以把这些单元叫做执行单元，比如，Intel 的 Ice Lake 架构的 CPU 有下面这些执行单元： 其他执行单元还有：BM、Vec ALU、Vec SHFT、Vec Add、Vec Mul、Shuffle 等。因为 CPU 内部存在着多个功能单元，所以在同一时刻，不同的功能单元其实可以服务于不同的指令，看看下面这个图； 这样的话，多条指令实质上是并行执行的，从而减少了总的执行时间，这种并行叫做指令级并行： 如果没有这种并行结构，或者由于指令之间存在依赖关系，无法并行，那么执行周期就会大大加长： 我们来看一个实际的例子。为了举例子方便，我们做个假设：假设 load 和 store 指令需要 3 个时钟周期来读写数据，add 指令需要 1 个时钟周期，mul 指令需要 2 个时钟周期。图中橙色的编号是原来的指令顺序，绿色的数字是每条指令开始时的时钟周期，你把每条指令的时钟周期累计一下就能算出来。最后一条指令开始的时钟周期是 20，该条指令运行需要 3 个时钟周期，所以在第 22 个时钟周期执行完所有的指令。右边是重新排序后的指令，一共花了 13 个时钟周期。这个优化力度还是很大的！ 仔细看一下左边前两条指令，这两条指令的意思是：先加载数据到寄存器，然后做一个加法。但加载需要 3 个时钟周期，所以 add 指令无法执行，只能干等着。右列的前三条都是 load 指令，它们之间没有数据依赖关系，我们可以每个时钟周期启动一个，到了第四个时钟周期，每一条指令的数据已经加载完毕，所以就可以执行加法运算了。我们可以把右边的内容画成下面的样子，你能看到，很多指令在时钟周期上是重叠的，这就是指令级并行的特点。 当然了，不是所有的指令都可以并行，最后的 3 条指令就是顺序执行的，导致无法并行的原因有几个： 数据依赖约束 如果后一条指令要用到前一条指令的结果，那必须排在它后面，比如下面两条指令：add 和 mul。对于第二条指令来说，除了获取指令的阶段（IF）可以和第一条指令并行以外，其他阶段需要等第一条指令的结果写入 r1，第二条指令才可以使用 r1 的值继续运行。 add r2, r1 mul r3, r1 功能部件约束 如果只有一个乘法计算器，那么一次只能执行一条乘法运算。 指令流出约束 指令流出部件一次流出 n 条指令。 寄存器约束 寄存器数量有限，指令并行时使用的寄存器不可以超标。 后三者也可以合并成为一类，称作资源约束。在数据依赖约束中，如果有因为使用同一个存储位置，而导致不能并行的，可以用重命名变量的方式消除，这类约束被叫做伪约束。而先写再写，以及先读后写是伪约束的两种呈现方式： 先写再写：如果指令 A 写一个寄存器或内存位置，B 也写同一个位置，就不能改变 A 和 B 的执行顺序，不过我们可以修改程序，让 A 和 B 写不同的位置。先读后写：如果 A 必须在 B 写某个位置之前读某个位置，那么不能改变 A 和 B 的执行顺序。除非能够通过重命名让它们使用不同的位置。 以上就是指令重排序的原理，掌握这个原理你就明白为什么重排序可以提升性能了，不过明白原理之后，我们还有能够用算法实现出来才行。用算法排序的关键点，是要找出代码之间的数据依赖关系。下图展现了示例中各行代码之间的数据依赖，可以叫做数据的依赖图（dependence graph）。它的边代表了值的流动，比如 a 行加载了一个数据到 r1，b 行利用 r1 来做计算，所以 b 行依赖 a 行，这个图也可以叫做优先图（precedence graph），因为 a 比 b 优先，b 比 d 优先。 我们可以给图中的每个节点再加上两个属性，利用这两个属性，就可以对指令进行排序了：一是操作类型，因为这涉及它所需要的功能单元。二是时延属性，也就是每条指令所需的时钟周期。 图中的 a、c、e、g 是叶子，它们没有依赖任何其他的节点，所以尽量排在前面。b、d、f、h 必须出现在各自所依赖的节点后面。而根节点 i，总是要排在最后面。根据时延属性，我们计算出了每个节点的累计时延（每个节点的累计时延等于父节点的累计时延加上本节点的时延）。其中 a-b-d-f-h-i 路径是关键路径，代码执行的最少时间就是这条路径所花的时钟周期之和。 因为 a 在关键路径上，所以首先考虑把 a 节点排在第 1 行。 剩下的树中，c-d-f-h-i 变成了关键路径，因为 c 的累计时延最大。c 节点可以排在第 2 行。 b 和 e 的累计时延都是最长的，但由于 b 必须在 a 执行完毕后，才会开始执行，所以最好等够 a 的 3 个时钟周期，否则还是会空等，所以先不考虑 b，而是把 e 放到第 3 行。 继续按照这个方式排，最后可以获得 a-c-e-b-d-g-f-h-i 的指令序列。不过这个代码其实还可以继续优化：也就是发现并消除其中的伪约束。c 和 e 都向 r2 里写了值，而 d 使用的是 c 写入的值。如果修改变量名称，比如让 e 使用 r3 寄存器，我们就可以去掉 e 跟 d，以及 e 与 c 之间伪约束，让 e 就可以排在 c 和 d 之前。同理，也可以让 g 使用 r4 寄存器，使得 g 可以排在 e 和 f 的前面。当然了，在这个示例中，这种改变并没有减少总的时间消耗，因为关键路径上的依赖没有变化，它们都使用了 r1 寄存器。但在别的情况下，就有可能带来更大的优化。 我们刚才其实是采用了一种最常见的算法，List Scheduling 算法，大致分为 4 步： 把变量重命名来消除伪约束（可选步骤）。2. 创建依赖图。3. 为每行代码计算优先值（计算方法可以有很多，比如我们示例中基于最长时延的方法就是一种）。4. 迭代处理代码并排序。 除了 List Scheduling 算法以外，还有其他的算法，这里我就不展开了。不过，讲到算法时，我们需要考虑算法的复杂度。前一讲讲算法时，我没有提这个问题，是想在这里集中讲一下。这两节课中，关于指令选择、寄存器分配和指令重排序的算法，其难度（时间复杂度）都是“NP- 完全”的。“NP- 完全”是什么意思呢？也就是这类问题找不到一个随规模（代码行数）计算量增长比较慢的算法（多项式时间算法）来找到最优解。反之，有可能计算量会随着代码行数呈指数级上升。因此，编译原理中的一些难度最高的算法，都在代码生成这一环。当然了，找最优解太难，我们可以退而求其次，找一个次优解。就比如我们用地图软件导航的时候，没必要要求导航路径每次都是找到最短的。这时，就会有比较简单的算法，计算量不会随规模增长太快，但结果还比较理想。我们这两讲的算法都是这个性质的。到目前为止，我带你了解了目标代码生成的三大考虑因素：指令选择、寄存器分配和指令重排序。现在，我们来看看目标代码生成，在 LLVM 中是如何实现的，这样，你能从概念过渡到实操，从而把知识点掌握得更加扎实。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:37:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"LLVM 的实现 LLVM 的后端需要多个处理步骤来生成目标代码： 图中橙色的部分是重要的步骤，它本身包含了多个 Pass，所以也叫做超级 Pass。图中蓝框的 Pass，是用来做一些额外的优化处理（关于 LLVM 的 Pass 机制，我在 27 讲提到过，如果你忘记了，可以回顾一下）。接下来，我来讲解一下 LLVM 生成目标代码的关键步骤。 指令选择 LLVM 的指令选择算法是基于 DAG（有向无环图）的树模式匹配，与前一讲基于 AST 的算法有一些区别，但总思路是一致的（具体算法描述可以参见这篇论文）。这个算法是 Near-Optimal（接近 Optimal）的，能够在线性的时间内完成指令的选择，并且它特别关注产生的代码的尺寸，要求尺寸足够小。DAG 是融合了公共子表达式的 AST，也是一种结构化的 IR。下面两行代码对应的 AST 和 DAG 分如图所示，你能看到，DAG 把 a=5 这棵子树给融合了： a = 5 b = (2 + a）+ (a * 3) LLVM 把内存中的 IR 模型，转化成了一个体现了某个目标平台特征的 SelectionDAG，用于做指令选择。每个基本块转化成一个 DAG，DAG 中的节点通常代表指令，边代表指令之间的数据流动。在这个阶段之后，LLVM 会把 DAG 中的 LLVM IR 节点，全部转换成目标机器的节点，代表目标机器的指令，而不是 LLVM 的指令。 指令排序（寄存器分配之前）基于前一步的处理结果，我们要对指令进行排序。但因为 DAG 不能反映没有依赖关系的节点之间的排序，所以 LLVM 要先把 DAG 转换成一种三地址模式，这个格式叫做 MachineInstr。这个阶段会把指令排序，并尽量发挥指令级并行的能力。 寄存器分配 接下来做寄存器的分配。LLVM 的 IR 支持无限多的寄存器，在这个环节要分配到实际的寄存器上，分配不下的就溢出到内存。 指令排序（寄存器分配之后） 分配完寄存器之后，LLVM 会再做一次指令排序。因为寄存器分配，会指定确定的寄存器，而访问不同的寄存器的时钟周期，可能是不同的。对于溢出到内存中的变量，也增加了一些指令在内存和寄存器之间传输数据。利用这些信息，LLVM 可以进一步优化指令的排序。 代码输出 做完上面的所有工作后，就可以输出目标代码了。 LLVM 在这一步把 MachineInstr 格式转换为 MCInst 格式，因为后者更有利于汇编器和链接器输出汇编代码或二进制目标代码。 在这里，我想延伸一下，和你探讨一个问题：如果现在有一个新的 CPU 架构，要实现一个崭新的后端，来支持各种语言，应该怎么做。在我国大力促进芯片研发的背景下，这是一个值得探讨的问题，新芯片需要编译器的支持才可以呀。你要实现各种指令选择的算法、寄存器分配的算法、指令排序的算法来反映这款 CPU 的特点。对于这个难度颇高的任务，LLVM 的 TableGen 模块会给你提供很大的帮助。这个模块能够帮助你为某个新的 CPU 架构快速生成后端。你可以用一系列配置文件定义你的 CPU 架构的特征，比如寄存器的数量、指令集等等。一旦你定义了这些信息，TableGen 模块就能根据这些配置信息，生成各种算法，如指令选择器、指令排序器、一些优化算法等等。这就像编译器前段工具可以帮你生成词法分析器，和语法分析器一样，能够大大降低开发一个新后端的工作量，所以说，把 LLVM 研究透彻，会有助于你在这样的重大项目中发挥重要作用。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:37:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我讲解了目标代码生成的第三个主题：指令重排序。要理解这个主题，你首先要知道 CPU 内部是分成多个功能部件的，要知道一条指令的执行过程中，指令获取、解码、执行、访问数据都是如何发生的，这样你会知道指令级并行的原理。其次，从算法角度，你要知道 List Scheduling 的步骤，掌握基于最大时延的优先级计算策略。有了这个基础之后，你可以进一步地研究其他算法。 我想强调的是，指令选择、寄存器分配、指令重排序这三个领域的算法，都是“NP- 完全”的，所以寻找优化的算法，是这个领域最富有挑战的任务。要研究清楚这些算法，你需要阅读相关的资料，比如本讲推荐的论文和其他该领域的经典论文。另外，我建议你阅读 CPU 厂商的手册，因为只有手册才会提供相关 CPU 的具体信息，解答你对技术细节的一些疑惑。比如网上曾经有人提问说：为什么 mov 指令要用到 ALU 部件？这个其实看一下手册就知道了。最后，我带你了解了 LLVM 是如何做这些后端工作的，这样可以加深你对代码生成这部分知识的了解。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:37:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现一门编译型语言 应用 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:38:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"31 | 内存计算：对海量数据做计算，到底可以有多快？ 内存计算是近十几年来，在数据库和大数据领域的一个热点。随着内存越来越便宜，CPU 的架构越来越先进，整个数据库都可以放在内存中，并通过 SIMD 和并行计算技术，来提升数据处理的性能。我问你一个问题：做 1.6 亿条数据的汇总计算，需要花费多少时间呢？几秒？几十秒？还是几分钟？如果你经常使用数据库，肯定会知道，我们不会在数据库的一张表中保存上亿条的数据，因为处理速度会很慢。但今天，我会带你采用内存计算技术，提高海量数据处理工作的性能。与此同时，我还会介绍 SIMD 指令、高速缓存和局部性、动态优化等知识点。这些知识点与编译器后端技术息息相关，掌握这些内容，会对你从事基础软件研发工作，有很大的帮助。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:39:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"了解 SIMD 本节课所采用的 CPU，支持一类叫做 SIMD（Single Instruction Multiple Data）的指令，它的字面意思是：单条指令能处理多个数据。相应的，你可以把每次只处理一个数据的指令，叫做 SISD（Single Instruction Single Data）。SISD 使用普通的寄存器进行操作，比如加法： addl $10, %eax 这行代码是把一个 32 位的整型数字，加到 %eax 寄存器上（在 x86-64 架构下，这个寄存器一共有 64 位，但这个指令只用它的低 32 位，高 32 位是闲置的）。这种一次只处理一个数据的计算，叫做标量计算；一次可以同时处理多个数据的计算，叫做矢量计算。它在一个寄存器里可以并排摆下 4 个、8 个甚至更多标量，构成一个矢量。图中 ymm 寄存器是 256 位的，可以支持同时做 4 个 64 位数的计算（xmm 寄存器是它的低 128 位）。 如果不做 64 位整数，而做 32 位整数计算，一次能计算 8 个，如果做单字节（8 位）数字的计算，一次可以算 32 个！ 1997 年，Intel 公司推出了奔腾处理器，带有 MMX 指令集，意思是多媒体扩展。当时，让计算机能够播放多媒体（比如播放视频），是一个巨大的进步。但播放视频需要大量的浮点计算，依靠原来 CPU 的浮点运算功能并不够。所以，Intel 公司就引入了 MMX 指令集，和容量更大的寄存器来支持一条指令，同时计算多个数据，这是在 PC 上最早的 SIMD 指令集。后来，SIMD 又继续发展，陆续产生了 SSE（流式 SIMD 扩展）、AVX（高级矢量扩展）指令集，处理能力越来越强大。 2017 年，Intel 公司发布了一款至强处理器，支持 AVX-512 指令（也就是它的一个寄存器有 512 位）。每次能处理 8 个 64 位整数，或 16 个 32 位整数，或者 32 个双精度数、64 个单精度数。你想想，一条指令顶 64 条指令，几十倍的性能提升，是不是很厉害！那么你的电脑是否支持 SIMD 指令？又支持哪些指令集呢？在命令行终端，打下面的命令，你可以查看 CPU 所支持的指令集。 sysctl -a | grep features | grep cpu //macOs操作系统 cat /proc/cpuinfo //Linux操作系统 现在，想必你已经知道了 SIMD 指令的强大之处了。而它的实际作用主要有以下几点：SIMD 有助于多媒体的处理，比如在电脑上流畅地播放视频，或者开视频会议；在游戏领域，图形渲染主要靠 GPU，但如果你没有强大的 GPU，还是要靠 CPU 的 SIMD 指令来帮忙；在商业领域，数据库系统会采用 SIMD 来快速处理海量的数据；人工智能领域，机器学习需要消耗大量的计算量，SIMD 指令可以提升机器学习的速度。你平常写的程序，编译器也会优化成，尽量使用 SIMD 指令来提高性能。 所以，我们所用到的程序，其实天天在都在执行 SIMD 指令。 接下来，我来演示一下如何使用 SIMD 指令，与传统的数据处理技术做性能上的对比，并探讨如何在编译器中生成 SIMD 指令，这样你可以针对自己的项目充分发挥 SIMD 指令的优势。Intel 公司为 SIMD 指令提供了一个标准的库，可以生成 SIMD 的汇编指令。我们写一个简单的程序（参考simd1.c）来对两组数据做加法运算，每组 8 个整数： c #include \u003cstdio.h\u003e #include \"immintrin.h\" void sum(){ //初始化两个矢量 ，8个32位整数 __m256i a=_mm256_set_epi32(20,30,40,60,342,34523,474,123); __m256i b=_mm256_set_epi32(234,234,456,78,2345,213,76,88); //矢量加法 __m256i sum=_mm256_add_epi32(a, b); //打印每个值 int32_t* s = (int32_t*)\u0026sum; for (int i = 0; i\u003c 8; i++){ printf(\"s[%d] : %d\\n\", i, s[i]); } } 把矢量加法运算翻译成汇编语言的话，采用的指令是 vpaddd（其中的 p 是 pack 的意思，对一组数据操作）。寄存器的名字是 ymm（y 开头意思是 256 位的）。 vpaddd %ymm0, %ymm1, %ymm0 在这个示例中，我们构建了两个矢量数据，这个计算很简单。接下来，我们挑战一个有难度的题目：把 1.6 亿个 64 位的整数做加法！1.6 亿个 64 位整数要占据大约 1.2G 的内存，你要把这 1.2G 的数据全部汇总一遍！要实现这个功能，你首先要申请一块 1.2G 大小的内存，并且要是 32 位对齐的（因为后面加载数据到寄存器的指令需要内存对齐，这样加载速度更快）。 unsigned totalNums = 160000000; //申请一块32位对齐的内存。 //注意：aligned_alloc函数C11标准才支持 int64_t * nums = aligned_alloc(32, totalNums * sizeof(int64_t)); //初始化sum值 __m256i sum=_mm256_setzero_si256(); __m256i * vectorptr = (__m256i *) nums; for (int i = 0; i \u003c totalNums/4; i++) { //从内存加载256位进来 __m256i a = _mm256_load_si256(vectorptr+i); //矢量加法 sum=_mm256_add_epi64(sum,a); } 完整的代码见simd2.c。最后，要用下面的命令，编译成可执行文件（-mavx2 参数是告诉编译器，要使用 CPU 的 AVX2 特性）： gcc -mavx2 simd2.c -o simd2 或 clang -mavx2 simd2.c -o simd2 你可以运行一下，看看用了多少时间。我的 MacBook Pro 大约用了 0.15 秒。注意，这还是只用了一个内核做计算的情况。我提供的 simd3.c 示例程序，是计算 1.6 亿个双精度浮点数，所用的时间也差不多，都是亚秒级。而计算速度之所以这么快，主要有两个原因： 采用了 SIMD；高速缓存和数据局部性所带来的帮助。 我们先把 SIMD 讨论完，然后再讨论高速缓存和数据局部性。矢量化功能可以一个指令当好几个用，但刚才编写的 SIMD 示例代码使用了特别的库，这些库函数本身就是用嵌入式的汇编指令写的，所以，相当于我们直接使用了 SIMD 的指令。如果我们不调用这几个库，直接做加减乘除运算，能否获得 SIMD 的好处呢？也可以。不过要靠编译器的帮助，所以，接下来来看看 LLVM 是怎样帮我们使用 SIMD 指令的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:39:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"LLVM 的自动矢量化功能（Auto-Vectorization） 各个编译器都在自动矢量化功能上下了功夫，以 LLVM 为例，它支持循环的矢量化（Loop Vectorizer）和 SLP 矢量化功能。循环的矢量化很容易理解。如果我们处理一个很大的数组，肯定是顺序读取内存的，就如loop1()函数的代码： c int loop1(int totalNums, int * nums){ int sum = 0; for (int i = 0; i\u003c totalNums; i++){ sum += nums[i]; } return sum; } 不过，如果你用不同的参数去生成汇编代码，结果会不一样： clang -S loop.c -o loop-scalar.s这是最常规的汇编代码，老老实实地用 add 指令和 %eax 寄存器做加法。 clang -S -O2 loop.c -o loop-O2.s它在使用 paddd 指令和 xmm 寄存器，这已经在使用 SIMD 指令了。 clang -S -O2 -fno-vectorize loop.c -o loop-O2-scalar.s这次带上了 -O2 参数，要求编译器做优化，但又带上了 -fno-vectorize 参数，要求编译器不要通过矢量化做优化。那么生成的代码会是这个样子： addl (%rsi,%rdx,4), %eax addl 4(%rsi,%rdx,4), %eax addl 8(%rsi,%rdx,4), %eax addl 12(%rsi,%rdx,4), %eax addl 16(%rsi,%rdx,4), %eax addl 20(%rsi,%rdx,4), %eax addl 24(%rsi,%rdx,4), %eax addl 28(%rsi,%rdx,4), %eax 也就是它一次循环就做了 8 次加法计算，减少了循环的次数，也更容易利用高速缓存，来提高数据读入的效率，所以会导致性能上的优化。 clang -S -O2 -mavx2 loop.c -o loop-avx2.s这次带上 -mavx2 参数，编译器就会使用 AVX2 指令来做矢量化，你查看代码会看到对 vpaddd 指令和 ymm 寄存器的使用。 其实，在 simd2.c 中，我们有一段循环语句，对标量数字进行加总。这段代码在缺省的情况下，也会被编译器矢量化（你可以看看汇编代码simd2-O2-avx2.s确认一下）。在做自动矢量化的时候，编译器要避免一些潜在的问题，看看loop2()函数的代码： c void loop2(int totalNums, int * nums1, int * nums2){ for (int i = 0; i\u003c totalNums; i++){ nums2[i] += nums1[i]; } } 代码中的 nums1 和 nums2 是两个指针，指向内存中的两个整数数组的位置。但我们从代码里看不出 nums1 和 nums2 是否有重叠，一旦它们有重叠的话，矢量化的计算结果会出错。所以，编译程序会生成矢量和标量两个版本的目标代码，在运行时检测 nums1 和 nums2 是否重叠，从而判断是否跳转到矢量化的计算代码。从这里你也可以看出：写编译器真的要有工匠精神，要把各种可能性都想到。实际上，在编译器里有很多这样的实现。你可以将循环次数改为一个常量，看一下loop3()函数，它所生成的汇编代码会根据常量的值做优化，甚至完全不做循环： c int loop3(int * nums){ int sum = 0; for (int i = 0; i\u003c 160; i++){ sum += nums[i]; } return sum; } 除了循环的矢量化器，LLVM 还有一个 SLP 矢量化器，它能在做全局优化时，寻找可被矢量化的代码来做转换。比如下面的代码，对 A[0]和 A[1]的操作非常相似，可以考虑按照矢量的方式来计算： c void foo(int a1, int a2, int b1, int b2, int *A) { A[0] = a1*(a1 + b1)/b1 + 50*b1/a1; A[1] = a2*(a2 + b2)/b2 + 50*b2/a2; } 所以，LLVM 确实在自动矢量化方面做了大量工作。在你设计一个新的编译器的时候，可以充分利用这些已有的成果。否则，在每个优化算法上，你都需要投入大量的精力，还不一定能做得足够稳定。到目前为止，我们针对 SIMD 和矢量化谈得足够多了。2011 年左右，我第一次做内存计算方面的编程时，被如此快的处理速度吓了一跳。因为如果你经常操作数据库，肯定会知道从数据库里做 1.6 亿个数据的汇总是什么概念。一般来说，一张表有上亿条数据之前，我们就已经要做分拆了。大多数情况下，表中的数据要比 1.6 亿低一个数量级，就算是这样，你对一个有着一两千万行数据表做统计，仍然要花费不少的时间。而毫不费力地进行海量数据的计算，就是内存计算的魅力。当然了，这里面有高速缓存和局部性的帮助。所以，我们继续讨论一下，跟内存计算有关的第二个问题：高速缓存和局部性。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:39:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"高速缓存和局部性 我们知道，计算机的存储是分成多个级别的：速度最快的是寄存器，通常在寄存器之间复制数据只需要 1 个时钟周期。其次是高速缓存，它根据速度和容量分为多个层级，读取所花费的时间从几个时钟周期到几十个时钟周期不等。内存则要用上百到几百个时钟周期。 在图中的存储层次结构中，越往下，存取速度越慢，但是却可以有更大的容量，从寄存器的 K 级，到高速缓存的 M 级，到内存的 G 级，到磁盘的 T 级（灰色标的数据是 Intel 公司的Ice Lake架构的 CPU 的数据）。一般的计算机指令 1 到几个时钟周期就可以执行完毕。所以，如果等待内存中读取，获得数据的话，CPU 的性能可能只能发挥出 1%。不过由于高速缓存的存在，读取数据的平均时间会缩短到几个时钟周期，这样 CPU 的能力可以充分发挥出来。所以，我在讲程序的运行时环境的时候，让你关注 CPU 上两个重要的部件：一个是寄存器，另一个就是高速缓存。 在代码里，我们会用到寄存器，并且还会用专门的寄存器分配的算法来优化寄存器。可是对于高速缓存，我们没有办法直接控制。因为当你用 mov 指令从内存中，加载数据到寄存器时，或者用 add 指令把内存中的一个数据，加到寄存器中，一个已有的值上面时，CPU 会自动控制是从内存里取，还是在高速缓存中取，并控制高速缓存的刷新。那我们有什么办法呢？答案是提高程序的局部性（locality），这个局部性又分为两个： 一是时间局部性。一个数据一旦被加载到高速缓存甚至寄存器，我们后序的代码都能集中访问这个数据，别等着这个数据失效了再访问，那就又需要从低级别的存储中加载一次。第二个是空间局部性。当我们访问了一条数据之后，很可能马上访问跟这个数据挨着的其他数据。CPU 在一次读入数据的时候，会把相邻的数据都加载到高速缓存，这样会增加后面代码在高速缓存中命中的概率。 提高局部性这件事情，更多的是程序员的责任，编译器能做的事情不多。不过，有一种编译优化技术，叫做循环互换优化（loop interchange optimization）可以让程序更好地利用高速缓存和寄存器。下面的例子中有内循环和外循环，内循环次数较少，外循环次数很大。如果内循环里的临时变量比较多，需要占用寄存器和高速缓存，那么 i 就可能被挤掉，等下一次用到 i 的时候，需要重新从低一级的存储中获取，从而造成性能的降低： for(i=0; i\u003c1000000; i++) for(j=0; j\u003c10; j++){ a[i] *= b[i] ... } 编译器可以把内外层循环交换，这样就提高了局部性： for(j=0; i\u003c10; j++) for(i= 0; i\u003c1000000; i++){ a[i] *= b[i] ... } 不过，在大多数情况下，i 和 j 循环的次数不是一个常量，而是一个变量，在编译时不知道内层循环次数更多还是外层循环。这样的话，可能就需要生成两套代码，在运行时根据情况决定跳转到哪个代码块去执行，这样会导致目标代码的膨胀。如果不想让代码膨胀，又能获得优化的目标代码，你可以尝试在运行时做动态的优化（也就是动态编译），这也是 LLVM 的设计目标之一。因为在静态编译期，我们确实没办法知道运行时的信息，从而也没有办法生成最优化的目标代码。作为一名优秀的程序员，你有责任让程序保持更好的局部性。比如，假设你要设计一个内存数据库，并且经常做汇总计算，那么你会把每个字段的数据按行存储在一起，还是按列存储？当然是后者，因为这样才具备更好的数据局部性。最后，除了 SIMD 和数据局部性，促成内存计算这个领域发展的还有两个因素： 多内核并行计算。现在的 CPU 内核越来越多，特别是用于服务器的 CPU。多路 CPU 几十上百个内核，能够让单机处理能力再次提升几十，甚至上百倍。内存越来越便宜。在服务器上配置几十个 G 的内存已经是常规配置，配置上 T 的内存，也不罕见。这使得大量与数据处理有关的工作，可以基于内存，而不是磁盘。除了要更新数据，几乎可以不访问相对速度很低的磁盘。 在这些因素的共同作用下，内存计算的使用越来越普遍。在你的项目里，你可以考虑采用这个技术，来加速海量数据的处理。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:39:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我带你了解了内存计算的特点，以及与编译技术的关系，我希望你能记住几点：SIMD 是一种指令级并行技术，它能够矢量化地一次计算多条数据，从而提升计算性能，在计算密集型的需求中，比如多媒体处理、海量数据处理、人工智能、游戏等领域，你可以考虑充分利用 SIMD 技术。充分保持程序的局部性，能够更好地利用计算机的高速缓存，从而提高程序的性能。SIMD，加上数据局部性，和多个 CPU 内核的并行处理能力，再加上低价的海量的内存，推动了内存计算技术的普及，它能够同时满足计算密集，和海量数据的需求。有时候，我们必须在运行期，根据一些数据来做优化，生成更优的目标代码，在编译期不可能做到尽善尽美。 我想强调的是，熟悉编译器的后端技术将会有利于你参与基础平台的研发。如果你想设计一款内存数据库产品，一款大数据产品，或者其他产品，将计算机的底层架构知识，和编译技术结合起来，会让你有机会发挥更大的作用！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:39:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"32 | 字节码生成：为什么Spring技术很强大？ Java 程序员几乎都了解 Spring。它的 IoC（依赖反转）和 AOP（面向切面编程）功能非常强大、易用。而它背后的字节码生成技术（在运行时，根据需要修改和生成 Java 字节码的技术）就是一项重要的支撑技术。Java 字节码能够在 JVM（Java 虚拟机）上解释执行，或即时编译执行。其实，除了 Java，JVM 上的 Groovy、Kotlin、Closure、Scala 等很多语言，也都需要生成字节码。另外，playscript 也可以生成字节码，从而在 JVM 上高效地运行！而且，字节码生成技术很有用。你可以用它将高级语言编译成字节码，还可以向原来的代码中注入新代码，来实现对性能的监测等功能。目前，我就有一个实际项目的需求。我们的一个产品，需要一个规则引擎，解析自定义的 DSL，进行规则的计算。这个规则引擎处理的数据量比较大，所以它的性能越高越好。因此，如果把 DSL 编译成字节码就最理想了。既然字节码生成技术有很强的实用价值，那么本节课，我就带你掌握它。我会先带你了解 Java 的虚拟机和字节码的指令，然后借助 ASM 这个工具，生成字节码，最后，再实现从 AST 编译成字节码。通过这样一个过程，你会加深对 Java 虚拟机的了解，掌握字节码生成技术，从而更加了解 Spring 的运行机制，甚至有能力编写这样的工具！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:40:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"Java 虚拟机和字节码 字节码是一种二进制格式的中间代码，它不是物理机器的目标代码，而是运行在 Java 虚拟机上，可以被解释执行和即时编译执行。在讲后端技术时，我强调的都是，如何生成直接在计算机上运行的二进制代码，这比较符合 C、C++、Go 等静态编译型语言。但如果想要解释执行，除了直接解释执行 AST 以外，我没有讲其他解释执行技术。而目前更常见的解释执行的语言，是采用虚拟机，其中最典型的就是 JVM，它能够解释执行 Java 字节码。 而虚拟机的设计又有两种技术：一是基于栈的虚拟机；二是基于寄存器的虚拟机。 标准的 JVM 是基于栈的虚拟机（后面简称“栈机”）。每一个线程都有一个 JVM 栈，每次调用一个方法都会生成一个栈桢，来支持这个方法的运行。栈桢里面又包含了本地变量数组（包括方法的参数和本地变量）、操作数栈和这个方法所用到的常数。这种栈桢的设计跟之前我们学过 C 语言的栈桢的结构，其实有很大的相似性，你可以通过21 讲回顾一下。 栈机是基于操作数栈做计算的。以“2+3”的计算为例，只要把它转化成逆波兰表达式，“2 3 +”，然后按照顺序执行就可以了。也就是：先把 2 入栈，再把 3 入栈，再执行加法指令，这时，要从栈里弹出 2 个操作数做加法计算，再把结果压入栈。 你可以看出，栈机的加法指令，是不需要带操作数的，就是简单的“iadd”就行，这跟你之前学过的 IR 都不一样。为什么呢？因为操作数都在栈里，加法操作需要 2 个操作数，从栈里弹出 2 个元素就行了。也就是说，指令的操作数是由栈确定的，我们不需要为每个操作数显式地指定存储位置，所以指令可以比较短，这是栈机的一个优点。接下来，我们聊聊字节码的特点。字节码是什么样子的呢？我编写了一个简单的类MyClass.java，其中的 foo() 方法实现了一个简单的加法计算，你可以看看它对应的字节码是怎样的： java public class MyClass { public int foo(int a){ return a + 3; } } 在命令行终端敲入下面两行命令，生成文本格式的字节码文件： javac MyClass.java javap -v MyClass \u003e MyClass.bc 打开 MyClass.bc 文件，你会看到下面的内容片段： java public int foo(int); Code: 0: iload_1 //把下标为1的本地变量入栈 1: iconst_3 //把常数3入栈 2: iadd //执行加法操作 3: ireturn //返回 其中，foo() 方法一共有四条指令，前三条指令是计算一个加法表达式 a+3。这完全是按照逆波兰表达式的顺序来执行的：先把一个本地变量入栈，再把常数 3 入栈，再执行加法运算。如果你细心的话，应该会发现：把参数 a 入栈的第一条指令，用的下标是 1，而不是 0。这是因为，每个方法的第一个参数（下标为 0）是当前对象实例的引用（this）。我提供了字节码中，一些常用的指令，增加你对字节码特点的直观认识，完整的指令集可以参见JVM 的规格书： 其中，每个指令都是 8 位的，占一个字节，而且 iload_0、iconst_0 这种指令，甚至把操作数（变量的下标、常数的值）压缩进了操作码里，可以看出，字节码的设计很注重节省空间。根据这些指令所对应的操作码的数值，MyClass.bc 文件中，你所看到的那四行代码，变成二进制格式，就是下面的样子： 你可以用“hexdump MyClass.class”显示字节码文件的内容，从中可以发现这个片段（就是橙色框里的内容）： 现在，你已经初步了解了基于栈的虚拟机，与此对应的是基于寄存器的虚拟机。这类虚拟机的运行机制跟机器码的运行机制是差不多的，它的指令要显式地指出操作数的位置（寄存器或内存地址）。它的优势是：可以更充分地利用寄存器来保存中间值，从而可以进行更多的优化。例如，当存在公共子表达式时，这个表达式的计算结果可以保存在某个寄存器中，另一个用到该公共子表达式的指令，就可以直接访问这个寄存器，不用再计算了。在栈机里是做不到这样的优化的，所以基于寄存器的虚拟机，性能可以更高。而它的典型代表，是 Google 公司为 Android 开发的 Dalvik 虚拟机和 Lua 语言的虚拟机。 这里你需要注意，栈机并不是不用寄存器，实际上，操作数栈是可以基于寄存器实现的，寄存器放不下的再溢出到内存里。只不过栈机的每条指令，只能操作栈顶部的几个操作数，所以也就没有办法访问其它寄存器，实现更多的优化。现在，你应该对虚拟机以及字节码有了一定的了解了。那么，如何借助工具生成字节码呢？你可能会问了：为什么不纯手工生成字节码呢？当然可以，只不过借助工具会更快一些。就像你生成 LLVM 的 IR 时，也曾获得了 LLVM 的 API 的帮助。所以，接下来我会带你认识 ASM 这个工具，并借助它为我们生成字节码。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:40:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"字节码生成工具 ASM 其实，有很多工具会帮我们生成字节码，比如 Apache BCEL、Javassist 等，选择 ASM 是因为它的性能比较高，并且它还被 Spring 等著名软件所采用。ASM是一个开源的字节码生成工具。Grovvy 语言就是用它来生成字节码的，它还能解析 Java 编译后生成的字节码，从而进行修改。ASM 解析字节码的过程，有点像 XML 的解析器解析 XML 的过程：先解析类，再解析类的成员，比如类的成员变量（Field）、类的方法（Mothod）。在方法里，又可以解析出一行行的指令。 你需要掌握两个核心的类的用法：ClassReader，用来解析字节码。ClassWriter，用来生成字节码。 这两个类如果配合起来用，就可以一边读入，做一定修改后再写出，从而实现对原来代码的修改。我们先试验一下，用 ClassWriter 生成字节码，看看能不能生成一个跟前面示例代码中的 MyClass 一样的类（我们可以称呼这个类为 MyClass2），里面也有一个一模一样的 foo 函数。相关代码参考genMyClass2()方法，这里只拿出其中一段看一下： //////创建foo方法 MethodVisitor mv = cw.visitMethod(Opcodes.ACC_PUBLIC, \"foo\", \"(I)I\", //括号中的是参数类型，括号后面的是返回值类型 null, null); //添加参数a mv.visitParameter(\"a\", Opcodes.ACC_PUBLIC); mv.visitVarInsn(Opcodes.ILOAD, 1); //iload_1 mv.visitInsn(Opcodes.ICONST_3); //iconst_3 mv.visitInsn(Opcodes.IADD); //iadd mv.visitInsn(Opcodes.IRETURN); //ireturn //设置操作数栈最大的帧数，以及最大的本地变量数 mv.visitMaxs(2,2); //结束方法 mv.visitEnd(); 从这个示例代码中，你会看到两个特点：ClassWriter 有 visitClass、visitMethod 这样的方法，以及 ClassVisitor、MethodVistor 这样的类。这是因为 ClassWriter 用了 visitor 模式来编程。你每一次调用 visitXXX 方法，就会创建相应的字节码对象，就像 LLVM 形成内存中的 IR 对象一样。foo() 方法里的指令，跟我们前面看到的字节码指令是一样的。 执行这个程序，就会生成 MyClass2.class 文件。把 MyClass2.class 变成可读的文本格式之后，你可以看到它跟 MyClass 的字节码内容几乎是一样的，只有类名称不同。当然了，你还可以写一个程序调用 MyClass2，验证一下它是否能够正常工作。发现了吗？只要熟悉 Java 的字节码指令，在 ASM 的帮助下，你可以很方便地生成字节码！想要了解更多 ASM 的用法，可以参考它的一个技术指南。既然你已经能生成字节码了，那么不如趁热打铁，把编译器前端生成的 AST 编译成字节码，在 JVM 上运行？因为这样，你就能从前端到后端，完整地实现一门基于 JVM 的语言了！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:40:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"将 AST 编译成字节码 基于 AST 生成 JVM 的字节码的逻辑还是比较简单的，比生成针对物理机器的目标代码要简单得多，为什么这么说呢？主要有以下几个原因：首先，你不用太关心指令选择的问题。针对 AST 中的每个运算，基本上都有唯一的字节码指令对应，你直白地翻译就可以了，不需要用到树覆盖这样的算法。你也不需要关心寄存器的分配，因为 JVM 是使用操作数栈的；指令重排序也不用考虑，因为指令的顺序是确定的，按照逆波兰表达式的顺序就可以了；优化算法，你暂时也不用考虑。 按照这个思路，你可以在 playscript-java 中增加一个ByteCodeGen的类，针对少量的语言特性做一下字节码的生成。最后，我们再增加一点代码，能够加载并执行所生成的字节码。运行下面的命令，可以把bytecode.play示例代码编译并运行。 java play.PlayScript -bc bytecode.play 当然了，我们只实现了 playscript 的少量特性，不过，如果在这个基础上继续完善，你就可以逐步实现一门完整的，基于 JVM 的语言了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:40:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"Spring 与字节码生成技术 我在开篇提到，Java 程序员大部分都会使用 Spring。Spring 的 IoC（依赖反转）和 AOP（面向切面编程）特性几乎是 Java 程序员在面试时必被问到的问题，了解 Spring 和字节码生成技术的关系，能让你在面试时更轻松。Spring 的 AOP 是基于代理（proxy）的机制实现的。在调用某个对象的方法之前，要先经过代理，在代理这儿，可以进行安全检查、记日志、支持事务等额外的功能。 Spring 采用的代理技术有两个：一个是 Java 的动态代理（dynamic proxy）技术；一个是采用 cglib 自动生成代理，cglib 采用了 asm 来生成字节码。 Java 的动态代理技术，只支持某个类所实现的接口中的方法。如果一个类不是某个接口的实现，那么 Spring 就必须用到 cglib，从而用到字节码生成技术来生成代理对象的字节码。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:40:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我主要带你了解了字节码生成技术。字节码生成技术是 Java 程序员非常熟悉的 Spring 框架背后所依赖的核心技术之一。如果想要掌握这个技术，你需要对 Java 虚拟机的运行原理、字节码的格式，以及常见指令有所了解。我想强调的重点如下：运行程序的虚拟机有两种设计：一个是基于栈的；一个是基于寄存器的。 基于栈的虚拟机不用显式地管理操作数的地址，因此指令会比较短，指令生成也比较容易。而基于寄存器的虚拟机，则能更好地利用寄存器资源，也能对代码进行更多的优化。你要能够在大脑中图形化地想象出栈机运行的过程，从而对它的原理理解得更清晰。ASM 是一个字节码操纵框架，它能帮你修改和生成字节码，如果你有这方面的需求，可以采用这样的工具。 相信有了前几课的基础，你再接触一种新的后端技术时，学习速度会变得很快。学完这节课之后，你可能会觉得：字节码就是另一种 IR，而且比 LLVM 的 IR 简单多了。如果你有这个感受，那么你已经在脑海里，建立了相关的知识体系，达到了举一反三的效果。在这里，我也建议 Java 程序员，多多了解 JVM 的运行机制，和 Java 字节码，这样会更好地把握 Java 语言的底层机制，从而更利于自己职业生涯的发展。 示例代码链接，我放在文末，供你参考。GenClass.java（用 asm 工具生成字节码）： 码云 GitHub MyClass.java（一个简单的 java 类）： 码云 GitHub MyClass.bc（文本格式的字节码）： 码云 GitHub ByteCodeGen.java（基于 AST 生成字节码）： 码云 GitHub bytecode.play（示例用的 playscript 脚本）： 码云 GitHub ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:40:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现一门编译型语言 拓展 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:41:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"33 | 垃圾收集：能否不停下整个世界？ 对于内存的管理，我们已经了解了栈和栈桢，在编译器和操作系统的配合下，栈里的内存可以实现自动管理。不过，如果你熟悉 C 和 C++，那么肯定熟悉在堆中申请内存，也知道要小心维护所申请的内存，否则很容易引起内存泄漏或奇怪的 Bug。其实，现代计算机语言大多数都带有自动内存管理功能，也就是垃圾收集（GC）。程序可以使用堆中的内存，但我们没必要手工去释放。垃圾收集器可以知道哪些内存是垃圾，然后归还给操作系统。那么这里会有几个问题，也是本节课关注的重点： 自动内存管理有哪些不同的策略？这些策略各自有什么优缺点？为什么垃圾收集会造成系统停顿？工程师们又为什么特别在意这一点？相信学完这节课之后，你对垃圾收集的机制理解得会更加深刻，从而在使用 Java、Go 等带有垃圾收集功能的语言时，可以更好地提升回收效率，减少停顿，提高程序的运行效率。当然，想要达到这个目的，你首先需要了解什么是内存垃圾，如何发现哪些内存是没用的？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:42:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"什么是内存垃圾 内存垃圾是一些保存在堆里的对象，但从程序里已经无法访问。 在堆中申请一块内存时（比如 Java 中的对象实例），我们会用一个变量指向这块内存。这个变量可能是：全局变量、常量、栈里的变量、寄存器里的变量。我们把这些变量叫做 GC 根节点。它指向的对象中，可能还包含指向其他对象的指针。但是，如果给变量赋予一个新的地址，或者当栈桢弹出，该栈桢的变量全部失效，这时，变量所指向的内存就无用了（如图中的灰色块）。 另外，如果 A 对象有一个成员变量指向 C 对象，那么如果 A 不可达，C 也会不可达，也就失效了。但 D 对象除了被 A 引用，还被 B 引用，仍然是可达的。 所以，所有可达的内存就不是垃圾，而计算可达性，重点在于知道哪些是根节点。在一个活动记录（栈桢）里，有些位置放的是指向堆中内存的指针，有的位置不是，比如，可能存放的是返回地址，或者是一个整数值。如果我们能够知道活动记录的布局，就可以找出所有的指针，然后就能计算寻找垃圾内存。 现在，你应该知道了内存垃圾的特点了，接下来，只要用算法找出哪些内存是不可达的，就能进行垃圾收集了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:42:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"标记和清除（Mark and Sweep） 标记和清除算法是最为经典的垃圾收集算法，它分为标记阶段和清除阶段。在标记阶段中，GC 跟踪所有可达的对象并做标记。每个对象上有一个标记位，一开始置为 0，如果发现这个对象是可达的，就置为 1。这个过程其实就是图的遍历算法，我们把这个算法细化一下，写成伪代码如下： 把所有的根节点加入todo列表 只要todo列表不为空，就循环处理： 从todo列表里移走一个变量v 如果v的标记为0，那么 把v的标记置为1 假设v1...vn是v中包含的指针 那么把v1...vn加入todo列表(去除重复成员) 下面的示例图中，x 和 y 是 GC 根节点，标记完毕以后，A、C 和 D 是可达的，B、E 和 F 是可收集的（我用不同的颜色做了标注）。 在清除阶段中，GC 遍历所有从堆里申请的对象，把标记为 0 的对象收回，把标记为 1 的内存重新置为 0，等待下次垃圾收集再做标记。这个算法虽然看上去简单清晰，但存在一个潜在的问题。在标记阶段，也就是遍历图的时候，必须要有一个列表作为辅助的数据结构，来保存所有待检查的对象。但这个列表要多大，只有运行时才清楚，所以没有办法提前预留出一块内存，用于清除算法。而一旦开始垃圾收集，那说明系统的内存已经比较紧张了，所以剩下的内存是否够这个辅助的数据结构用，是不确定的。可能你会说：那我可以改成递归算法，递归地查找下级节点并做标记。这是不行的，因为每次递归调用都会增加一个栈桢，来保存递归调用的参数等信息，内存消耗有可能更大。不过，方法总比问题多，针对算法的内存占用问题，你可以用指针逆转（pointer reversal）来解决。这个技术的思想是：把算法所需要的辅助数据，记录在内存对象自身的存储空间。具体做法是：顺着指针方向从 A 到达 B 时，我们把从 A 到 B 的指针逆转过来，改成从 B 到 A。把 B 以及 B 的子节点标记完以后，再顺着这个指针找到回去的路，回到 A，然后再把指针逆转回来。整个标记过程的直观示意图如下： 关于这个技术，你需要注意其中一个技术细节：内存对象中，可能没有空间来存一个指针信息。比如下图中，B 对象原来就有一个变量，用来保存指向 C 的指针。现在用这个变量的位置保存逆转指针，来指向 A 就行了。但到 C 的时候，发现 C 没有空间来存逆转到 B 的指针。 这时，借助寄存器就可以了。在设置从 B 到 A 的指针之前，要把 B 和 C 的地址，临时保存在寄存器里，避免地址的丢失。进入 C 以后，如果 C 没有存指针的空间，就证明 C 是个叶子节点，这时，用寄存器里保存的地址返回给 B 就行了。采用标记和清除算法，你会记住所有收集了的内存（通常是存在一个双向列表里），在下次申请内存的时候，可以从中寻找大小合适的内存块。不过，这会导致一个问题：随着我们多次申请和释放内存，内存会变得碎片化。所以，在申请内存的时候，要寻找合适的内存块，算法会有点儿复杂。而且就算你努力去寻找，当申请稍微大一点儿的内存时，也会失败。 为了避免内存碎片，你可以采用变化后的算法，标记 - 整理算法：在做完标记以后，做一下内存的整理，让存活的对象都移动到一边，消除掉内存碎片。 除此之外，停止和拷贝算法，也能够避免内存碎片化。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:42:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"停止和拷贝（Stop and Copy） 采用这个算法后，内存被分成两块：一块是旧空间，用于分配内存。一块是新空间，用于垃圾收集。 停止和拷贝算法也可以叫做复制式收集（Coping Collection）。 你需要保持一个堆指针，指向自由空间开始的位置。申请内存时，把堆指针往右移动就行了，比标记 - 清除算法申请内存更简单。这里需要注意，旧空间里有一些对象可能已经不可达了（图中的灰色块），但你不用管。当旧空间变满时，就把所有可达的对象，拷贝到新空间，并且把新旧空间互换。这时，新空间里所有对象整齐排列，没有内存碎片。 停止 - 拷贝算法被认为是最快的垃圾收集算法，有两点原因：分配内存比较简单，只需要移动堆指针就可以了。垃圾收集的代价也比较低，因为它只拷贝可达的对象。当垃圾对象所占比例较高的时候，这种算法的优势就更大。 不过，停止 - 拷贝算法还有缺陷：有些语言不允许修改指针地址。在拷贝内存之后，你需要修改所有指向这块内存的指针。像 C、C++ 这样的语言，因为内存地址是对编程者可见的，所以没法采用停止和拷贝算法。始终有一半内存是闲置的，所以内存利用率不高。最后，它一次垃圾收集的工作量比较大，会导致系统停顿时间比较长，对于一些关键系统来说，这种较长时间的停顿是不可接受的。但这两个算法都是基础的算法，它们可以被组合进更复杂的算法中，比如分代和增量的算法中，就能避免这个问题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:42:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"引用计数（Reference Counting） 引用计数支持增量的垃圾收集，可以避免较长时间的停顿。 它的原理是：在每个对象中，保存引用本对象的指针数量，每次做赋值操作时，都要修改这个引用计数。如果 x 和 y 分别指向 A 和 B，当执行“x=y”这样的赋值语句时，要把 A 的引用计数减少，把 B 的引用计数增加。如果某个对象的引用计数变成了 0，那就可以把它收集掉。 所以，引用计数算法非常容易实现，只需要在赋值时修改引用计数就可以了。不过，引用计数方法也有缺陷：首先，是不能收集循环引用的结构。比如图中的 A、B、C 和 D 的引用计数都是 1，但它们只是互相引用，没有其他变量指向它们。而循环引用在面向对象编程里很常见，比如一棵树的结构中，父节点保存了子节点的引用，子节点也保存了父节点的引用，这会让整棵树都没有办法被收集。 如果你有 C++ 工作经验，应该思考过，怎么自动管理内存。有一个思路是：实现智能指针，对指针的引用做计数。这种思路也有循环引用的问题，所以要用其他算法辅助，来解决这个问题。其次，在每次赋值时，都要修改引用计数，开销大。何况修改引用计数涉及写内存的操作，而写内存是比较慢的，会导致性能的降低。其实，这三个算法都是比较单一的算法，实际上，它们可以作为更复杂、更实用算法的组成部分，比如分代收集算法。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:42:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"分代收集（Generational Collection） 分代收集算法在商业级的产品里很普及，比如 Java 和 Go 语言的 GC。 它的核心思想是：在程序中，往往新创建的对象会很快死去，比如，你在一个方法中，使用临时变量指向一些新创建的对象，这些对象大多数在退出方法时，就没用了。根据这个原理，垃圾收集器将注意力集中在比较“年轻”的数据上，因为它们成为垃圾的概率比较高。我们把堆划分成若干“代”（Generation）：G0 是最新代，G1 就要老一些。不过 GC 根节点的计算有一个小小的区别：在收集 G0 时，根节点除了全局变量、栈和寄存器中的变量外，还要包含老一代的对象中指向 G0 的指针（下图中橙色的线，都是指向 G0 中对象的）。 所以，一个重要的问题是：记住 G1、G2…中的根节点。但如果每次都去搜一遍，相当于遍历所有世代，效率很低。所以，要采用效率高一点儿的算法，比如记忆表法。这个算法是指：如果 A 对象的 x 属性被设置成了 B 对象，那么就要把 A 对象加入一个向量里（记忆表），记住这个对象曾经被更新过。在垃圾收集时，要扫描这张表，寻找指向 G0 的老对象。因为这个算法要记的对象太多，记忆表会变得很大，不太划算。不过我们可以把内存划为 2 的 k 次方大小的一个个卡片，如果卡片上的对象被赋值，那么就把这张卡片标记一下，这叫做卡片标记法。如果你熟悉操作系统，会马上发现，这种卡片和操作系统内存管理时的分页比较相似。所以你可以由操作系统帮忙记录哪页被写入数据了，这种方法叫做页标记法。 解决了根节点的问题之后，我们就可以对 G0 进行收集了。在 G0 被收集了多次以后，对 G1、G2 也可以进行收集。这里你需要注意，G0 比较适合复制式收集算法，因为大部分对象会被收集掉，剩下来的不多；而老年代的对象生存周期比较长，拷贝的话代价太大，比较适合标记 - 清除算法，或者标记 - 整理算法。Java 的 GC 就采用了分代收集，现在，你再去看介绍 Java 垃圾收集的资料，会容易多了。在带你了解了一些常见的垃圾收集算法之后，我想和你讨论一下：能否不停下整个世界？这个标题里的痛点问题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:42:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"增量收集和并发收集（Incremental Collection, Concurrent Collection） 垃圾收集算法在运行时，通常会把程序停下。因为在垃圾收集的过程中，如果程序继续运行，程序可能会出错。这种停下整个程序的现象，被形象地称作“停下整个世界（STW）”。可是让程序停下来，会导致系统卡顿，用户的体验感会很不好。一些对实时性要求比较高的系统，根本不可能忍受这种停顿。 所以，在自动内存管理领域的一个研究的重点，就是如何缩短这种停顿时间。以 Go 语言为例，它的停顿时间从早期的几十毫秒，已经降低到了几毫秒。甚至有一些激进的算法，力图实现不用停顿。增量收集和并发收集算法，就是在这方面的有益探索。增量收集可以每次只完成部分收集工作，没必要一次把活干完，从而减少停顿。并发收集就是在不影响程序执行的情况下，并发地执行垃圾收集工作。 为了讨论增量和并发收集算法，我们定义两个角色：一个是收集器（Collector），负责垃圾收集；一个是变异器（Mutator），其实就是程序本身，它会造成可达对象的改变。然后，用三色标记（tricolor marking）的方法，来表示算法中，不同的内存对象的处理阶段： 白色表示，算法还没有访问的对象。灰色表示，这个节点已经被访问过，但子节点还没有被访问过。黑色节点表示，这个节点已经访问过，子节点也已经被访问过了。 用三色标记法来分析的话，你会发现前面的算法有两个特点：1. 不会有黑色对象指向白色对象，因为黑色对象都已经被扫描完毕了。2. 每一个灰色对象都处于收集器的待处理工作区中，比如在标记 - 清除算法的 todo 列表中。 再进一步分析后，我们发现，只要保证这两个特点一直成立，那么收集器和变异器就可以一起工作，互不干扰，从而实现增量收集或并发收集。因为算法可以不断扫描灰色对象，加入到黑色区域。这样整个算法就可以增量式地运行下去。 现在我们的重点，就变成了保证上面两个特点一直成立。比如，如果变异器要在一个黑色对象 a 里存储一个指针 b，把 a 涂成灰色，或者把 b 涂成灰色，都会保持上面两条的成立。或者当变异器要读取一个白色指针 a 的时候，就把它涂成灰色，这样的话也不会违背上面两条。 不同的算法会采取不同的策略，但无论采取哪种算法，收集器和变异器都是通过下面三种机制来协作： 读屏障（read barrier 或 load barrier）。在 load 指令（从内存到寄存器）之后立即执行的一小段代码，用于维护垃圾收集所需的数据。包括把内存对象涂成正确的颜色，并保证所有灰色对象都在算法的工作区里。写屏障（write barrier 或 store barrier）。在 store 指令（从寄存器到内存）之前执行的一小段代码，也要为垃圾收集做点儿工作。安全点（safepoint）。安全点是代码中的一些点，在这些点上，指针的值是可以安全地修改的。有时，你修改指针的值是有问题的，比如正在做一个大的数组的拷贝，拷到一半，你把数组的地址改了，这就有问题。所以安全点一般都在方法调用、循环跳转、异常跳转等地方。 概要地总结一下：要想实现增量或并发的垃圾收集，就要保证与垃圾收集有关数据的正确性，所以，需要读屏障、写屏障两个机制。另外，还要保证垃圾收集不会导致程序出错，所以需要安全点机制。要实现这三个机制，需要编译器的帮助。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:43:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"LLVM 对垃圾收集的支持 总的来说，垃圾收集器是一门语言，运行期的一部分，不是编译器的职责。所以，LLVM 并没有为我们提供垃圾收集器。但是，要想让垃圾收集器发挥功能，必须要编译器配合，LLVM 能够支持： 在代码中创建安全点，只有在这些点上才可以执行 GC。计算栈图（Stack Map）。在安全点上，栈桢中的指针会被识别出来，作为 GC 根节点被 GC 所使用。提供写屏障和读屏障的支持，用于支持增量和并发收集。 LLVM 能为当前所有常见的 GC 算法提供支持，包括我们本讲提到的所有算法，你写 GC 的时候，一定要跟 LLVM 配合，才能让 GC 顺利发挥作用。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:43:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 垃圾收集是高级语言的重要特征，我们针对垃圾收集，探讨了它的原理和常见的算法，我希望你记住以下几点： 内存垃圾是从根节点不能到达的对象。标记 - 清除算法中，你要记住不占额外的内存来做标记的技巧，也就是指针逆转。停止 - 拷贝算法比较适合活对象比例比较低的情况，因为只需要拷贝少量对象。引用计数的方法比较简单，但不能处理循环引用的情况，所以可以考虑跟其他算法配合。分代收集算法非常有效，关键在于计算老一代中的根节点。增量收集和并发收集是当前的前沿，因为它能解决垃圾收集中最大的痛点，时延问题LLVM 给垃圾收集提供安全点、栈图、读写屏障方面的支持，GC 要跟编译器配合才能很好的工作。 总之，垃圾收集是一项很前沿的技术，如果你有兴趣在这方面做些工作，有一些开源的 GC 可以参考。不过，就算不从事 GC 的编写，仅仅了解原理，也会有助于你更好地使用自己的语言，比如把 Java 和 Go 语言做好调优。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:43:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"34 | 运行时优化：即时编译的原理和作用 前面所讲的编译过程，都存在一个明确的编译期，编译成可执行文件后，再执行，这种编译方式叫做提前编译（AOT）。 与之对应的，另一个编译方式是即时编译（JIT），也就是，在需要运行某段代码的时候，再去编译。其实，Java、JavaScript 等语言，都是通过即时编译来提高性能的。那么问题来了： 什么时候用 AOT，什么时候用 JIT 呢？在讲运行期原理时，我提到程序编译后，会生成二进制的可执行文件，加载到内存以后，目标代码会放到代码区，然后开始执行。那么即时编译时，对应的过程是什么？目标代码会存放到哪里呢？在什么情况下，我们可以利用即时编译技术，获得运行时的优化效果，又该如何实现呢？ 本节课，我会带你掌握，即时编译技术的特点，和它的实现机理，并通过一个实际的案例，探讨如何充分利用即时编译技术，让系统获得更好的优化。这样一来，你对即时编译技术的理解会更透彻，也会更清楚怎样利用即时编译技术，来优化自己的软件。首先，来了解一下，即时编译的特点和原理。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:44:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"了解即时编译的特点及原理 根据计算机程序运行的机制，我们把，不需要编译成机器码的执行方式，叫做解释执行。解释执行，通常都会基于虚拟机来实现，比如，基于栈的虚拟机，和基于寄存器的虚拟机（在32 讲中，我带你了解过）。与解释执行对应的，是编译成目标代码，直接在 CPU 上运行。而根据编译时机的不同，又分为 AOT 和 JIT。那么，JIT 的特点，和使用场景是什么呢？ 一般来说，一个稍微大点儿的程序，静态编译一次，花费的时间很长，而这个等待时间是很煎熬的。如果采用 JIT 机制，你的程序就可以像，解释执行的程序一样，马上运行，得到反馈结果。其次，JIT 能保证代码的可移植性。在某些场景下，我们没法提前知道，程序运行的目标机器，所以，也就没有办法提前编译。Java 语言，先编译成字节码，到了具体运行的平台上，再即时编译成，目标代码来运行，这种机制，使 Java 程序具有很好的可移植性。再比如，很多程序会用到 GPU 的功能，加速图像的渲染，但针对不同的 GPU，需要编译成不同的目标代码，这里也会用到即时编译功能。最后，JIT 是编译成机器码的，在这个过程中，可以进行深度的优化，因此程序的性能要比解释执行高很多。 这样看来，JIT 非常有优势。而从实际应用来看，原来一些解释执行的语言，后来也采用 JIT 技术，优化其运行机制，在保留即时运行、可移植的优点的同时，又提升了运行效率，JavaScript 就是其中的典型代表。基于谷歌推出的 V8 开源项目，JavaScript 的性能获得了极大的提升，使基于 Web 的前端应用的体验，越来越好，这其中就有 JIT 的功劳。而且据我了解，R 语言也加入了 JIT 功能，从而提升了性能；Python 的数据计算模块 numba 也采用了 JIT。在了解 JIT 的特点，和使用场景之后，你有必要对 JIT 和 AOT 在技术上的不同之处有所了解，以便掌握 JIT 的技术原理。静态编译的时候，首先要把程序，编译成二进制目标文件，再链接形成可执行文件，然后加载到内存中运行。JIT 也需要编译成二进制的目标代码，但是目标代码的加载和链接过程，就不太一样了。 首先说说目标代码的加载。在静态编译的情况下，应用程序会被操作系统加载，目标代码被放到了代码区。从安全角度出发，操作系统给每个内存区域，设置了不同的权限，代码区具备可执行权限，所以我们才能运行程序。在 JIT 的情况下，我们需要为这种动态生成的目标代码，申请内存，并给内存设置可执行权限。我写个实际的 C 语言程序，让你直观地理解一下这个过程。我们在一个数组里，存一段小小的机器码，只有 9 个字节。这段代码的功能，相当于一个 C 语言函数的功能，也就是把输入的参数加上 2，并返回。 /* * 机器码，对应下面函数的功能： * int foo(int a){ * return a + 2; * } */ uint8_t machine_code[] = { 0x55, 0x48, 0x89, 0xe5, 0x8d, 0x47, 0x02, 0x5d, 0xc3 }; 你可能问了：你怎么知道这个函数，对应的机器码是这 9 个字节呢？这不难，你把 foo.c 编译成目标文件，然后查看里面的机器码就行了。 clang -c -O2 foo.c -o foo.o 或者 gcc -c -O2 foo.c -o foo.o objdump -d foo.o objdump 命令，能够反编译一个目标文件，并把机器码，和对应的汇编码都显示出来： 另外，用“hexdump foo.o”命令显示这个二进制文件的内容，也能找到这 9 个字节（图中橙色框中的内容）。 这里多说一句，如果你喜欢深入钻研的话，那么我建议你研究一下，如何从汇编代码生成机器码（实际上就是研究汇编器的原理）。比如，第一行汇编指令“pushq %rbp”，为什么对应的机器码，只有一个字节？如果指令一个字节，操作数一个字节，应该两个字节才对啊？其实，你阅读 Intel 的手册之后，就会知道这个机器码为什么这么设计。因为它要尽量节省空间，所以实际上，很多指令和操作码，会被压缩进，一个字节中去表示。在 32 讲中，研究字节码的设计时，你应该发现了这个规律。这些设计思路都是相通的，如果你要设计自己的字节码，也可以借鉴这些思想。说回我们的话题，既然已经有了机器码，那我们接着再做下面几步： 用 mmap 申请 9 个字节的一块内存。用这个函数（不是 malloc 函数）的好处是，可以指定内存的权限。我们先让它的权限是可读可写的。然后用 memcp 函数，把刚才那 9 个字节的数组，拷贝到，所申请的内存块中。用 mprotect 函数，把内存的权限，修改为可读和可执行。再接着，用一个 int(*)(int) 型的函数指针，指向这块内存的起始地址，也就是说，该函数有一个 int 型参数，返回值也是 int。最后，通过这个函数指针，调用这段机器码，比如 fun(1)。你打印它的值，看看是否符合预期。 完整的代码在jit.cpp里。借这个例子，你可能会知道，通过内存溢出来攻击计算机是怎么回事了。因为只要一块内存是可执行的，你又通过程序写了一些代码进去，就可以攻击系统。是不是有点儿黑客的感觉？所以在 jit.cpp 里，我们其实很小心地，把内存地址的写权限去掉了。 如果你愿意深究，我建议你，再看一眼 objdump 打印的汇编码。你会发现，其中开头为 0、1 和 7 的三行是没有用的。根据你之前学过的汇编知识，你应该知道，这三行实际是保存栈指针、设置新的栈指针的。但这个例子中，都是用寄存器来操作的，没用到栈，所以这三行代码对应的机器码可以省掉。最后，只用 4 个字节的机器码也能完成同样的功能： //省略了三行汇编代码的机器码： uint8_t machine_code1[] = { 0x8d, 0x47, 0x02, 0xc3 }; 现在，你应该清楚了，动态生成的代码，是如何加载到内存，然后执行了吧？不过，刚刚这个函数比较简单，只做了一点儿算术计算。通常情况下，你的程序会比较复杂，往往在一个函数里，要调用另一个函数。比如，需要在 foo 函数里，调用 bar 函数。这个 bar 函数可能是你自己写的，也可能是一个库函数。执行的时候，需要能从 foo 函数，跳转到 bar 函数的地址，执行完毕以后再跳转回来。那么，你要如何确定 bar 函数的地址呢？ 这就涉及目标代码的链接问题了。原来，编译器生成的二进制对象，都是可重定位的。在静态编译的时候，链接程序最重要的工作，就是重定位（Relocatable），各个符号的地址，包括全局变量、常量的地址和函数的地址，这样，你就可以访问这些变量，或者跳转到函数的入口。JIT 没有静态链接的过程，但是，也可以运用同样的思路，解决地址解析的问题。你编写的程序里的所有全局变量，和函数，都会被放到一个符号表里，在符号表里记录下它们的地址。这样，引用它们的函数就知道正确的地址了。 更进一步，你写的函数，不仅可以引用你自己编写的，程序中的对象，还可以访问共享库中的对象。比如，很多程序，都会共享 libc 库中的标准功能，这个库的源代码超过了几百万行，像打印输出这样的基础功能，都可以用这个库来实现。这时候，你可以用到动态链接技术。动态链接技术运用得很普遍，它是在应用程序加载的时候，来做地址的重定位。动态链接，通常会采用“位置无关代码（PIC）”的技术，使动态库，映射进每个应用程序的空间时，其地址看上去都不同。这样一来，可以让动态库被很多应用共享，从而节省内存空间，而且可以提升安全性。因为固定的地址，有利于恶意的程序，去攻击共享库中的代码，从而危及整个系统。 到目前为止，你已经了解了实现 JIT 的两个关键技术：让代码动态加载和执行。访问自己写的程序和共享库中的对象。 它们是 JIT 的核心。至于代码优化和目标代码生成，与静态编译是一样的。了解这些内容之后，你应该更加理解 Java、JavaScript 等语言，即时编译运行的过程了吧？当然，LLVM 对即时编译提供了很好的支持，它大致的机制是这样的： 我们编写的任何模块 (Module)，都以内存 IR 的形式存在，LLVM 会把模块中的符号，都统一保存到符号表中。当程序要调用模块的方法时，这个模块就会被即时编译，形成可重定位的目标对象，并被调用执行。动态链接库中的方法（如 printf）也能够被重定位并调用。 在第一次编译时，你可以让 LLVM，仅运行少量的优化算法，这样编译速度比较快，马上就可以运行。而对于被多次调用的函数，你可以让 LLVM 执行更多的优化算法，生成更优化版本的目标代码。而运行时所收集的某些信息，可以作为某些优化算法的输入，像 Java 和 JavaScript 都带有这种多次生成目标代码的功能。带你了解 JIT 的原理之后，接下来，我再通过一个案例，让你对 JIT 的作用有更加直观的认识。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:44:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"用 JIT 提升系统性能 著名的开源数据库软件，PostgreSQL，你可能听说过。它的历史比 MySQL 久，功能也比 MySQL 多一些。在最近的版本中，它添加了基于 LLVM 的，即时编译功能，性能大大提高。看一下下面的 SQL 语句： select count(*) from table_name where (x + y) \u003e 100 这个语句的意思是：针对某个表，统计一下字段 x 和 y 的和大于 100 的记录有多少条。这个 SQL 在运行时，需要遍历所有的行，并对每一行，计算“(x + y) \u003e 100”这个表达式的值。如果有 1000 万行，这个表达式就要被执行 1000 万次。PostgreSQL 的团队发现，直接基于 AST 或者某种 IR，解释执行这个表达式的话，所用的时间，占到了处理一行记录所需时间的 56%。而基于 LLVM 实现 JIT 以后，所用的时间只占到 6%，性能整整提高了一倍。在这里，我联系31 讲内存计算的内容，带你拓展一下。上面的需求，是典型的基于列进行汇总计算的需求。如果对代码进行向量化处理，再保证数据的局部性，针对这个需求，性能还可以提升很多倍。 再说回来。除了针对表达式的计算进行优化，PostgreSQL 的团队还利用 LLVM 的 JIT 功能，实现了其他的优化。比如，编译 SQL 执行计划的时间，缩短了 5.5 倍；创建 b 树索引的时间，降低了 5%~19%。那么 32 讲中，我提到，将一个规则引擎，编译成字节码，这样在处理大量数据时，可以提高性能。这是因为，JVM 也会针对字节码做即时编译。道理是一样的。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:44:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 对现代编程语言来说，编译期和运行期的界限，越来越模糊了，解释型语言和编译型语言的区别，也越来越模糊了。即时编译技术可以生成，最满足你需求的目标代码。那么通过今天的内容，我强调这样几点：1. 为了实现 JIT 功能，你可以动态申请内存，加载目标代码到内存，并赋予内存可执行的权限。在这个过程中，你要注意安全因素。比如，向内存写完代码以后，要取消写权限。2. 可重定位的目标代码，加上动态链接技术，让 JIT 产生的代码可以互相调用，以及调用共享库的功能。3.JIT 技术可以让数据库这类基础软件，获得性能上的提升，如果你有志参与研发这类软件，掌握 JIT 技术会给你加分很多。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:44:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"35 | 案例总结与热点问题答疑：后端部分真的比前端部分难吗？ 本节课，我会继续剖析一些，你们提出的，有代表性的问题（以后端问题为主），主要包括以下几个方面：后端技术部分真的比前端技术部分难吗？怎样更好地理解栈和栈桢（有几个同学提出的问题很好，有必要在这里探究一下）？这样，你对栈桢的理解会更加扎实。有关数据流分析框架。数据流分析是后端技术的几个重点之一，需要再细化一下。关于 Java 的两个知识点：泛型和反射。我会从编译技术的角度讲一讲。 接下来，进入第一个问题：后端技术真的难吗？正确的学习路径是什么？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:45:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"后端技术真的难吗？该怎么学？ 有同学觉得，一进到后端，难度马上加大了，你是不是也有这样的感觉？我承认，前端部分和后端部分确实不太相同。前端部分偏纯逻辑，你只要把算法琢磨透就行了。而后端部分，开始用到计算机组成原理的知识，要考虑 CPU、寄存器、内存和指令集，甚至还要深入到 CPU 内部，去看它的流水线结构，以便理解指令排序。当然，我们还要说清楚与操作系统的关系，操作系统是如何加载代码并运行的，如何帮你管理内存等等。另外，还涉及 ABI 和调用约定，NP 完全的算法等等。看上去复杂了很多。 虽然比较复杂，但我认为，这并不意味着后端更难，只意味着知识点更多。可这些知识，往往你熟悉了就不难了。比如，@风同学见到了汇编代码说：总算遇到了自己熟悉的内容了，不用天天看 Java 代码了。 我觉得，从算法的角度出发，后端部分的算法，至少没比前端的语法分析算法难。而且有些知识点，别的课程里应该讲过，如果你从以下三个方面多多积累，会更容易掌握后端内容： 计算机组成原理：CPU 的运行原理、汇编指令等等。数据结构和算法，特别是与树和图有关的算法：如果你之前了解过，与图有关的算法，了解旅行商问题，那么会发现，指令选择等算法似曾相识。自然会理解，我提到某些算法是 NP 完全的，是什么意思。操作系统：大部分情况下，程序是在操作系统中运行的，所以，要搞清楚我们编译的程序是如何跟操作系统互动的。 @沉淀的梦想就对这些内容，发表过感触：感觉学编译原理，真的能够帮助我们贯通整个计算机科学，涉及到的东西好多。确实如他所说，那么我也希望《编译原理之美》这门课，能促使你去学习另外几门基础课，把基础夯实。 后端技术的另一个特点，是它比较偏工程性，不像前端部分有很强的理论性，对于每个问题有清晰的答案。而后端技术部分，往往对同一个问题有多种解决思路和算法，不一定有统一的答案，甚至算法和术语的名称都不统一。 后端技术的工程性特点，还体现在它会涉及很多技术细节，这些细节信息往往在教科书上是找不到的，必须去查厂商（比如 Intel）的手册，有时要到社区里问，有时要看论文，甚至有时候要看源代码。 总的来说，如何学好后端，我的建议主要有三个方面：学习关联的基础课程，比如《数据结构与算法》，互相印证；理解编译原理工程性的特点，接受术语、算法等信息的不一致，并从多渠道获得前沿信息，比如源代码、厂商的手册等等。注重实操，亲自动手。比如，你在学优化算法时，即使没时间写算法，也要尽可能用 LLVM 的算法做做实验。 按照上面三条建议，你应该可以充分掌握后端技术了。当然，如果你只是想做一个概要的了解，那么阅读文稿也会有不错的收获，因为我已经把主线梳理出来了，能避免你摸不着头脑，不知如何入手。接下来，我们进入第二个问题：再次审视一下栈桢。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:45:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"再次认识栈桢 @刘强同学问：操作系统在栈的管理中到底起不起作用？这是操作系统方面的知识点，但可以跟编译技术中栈的管理联系在一起看。我们应用程序能够访问很大的地址空间，但操作系统不会慷慨地，一下子分配很多真实的物理内存。操作系统会把内存分成很多页，一页一页地按需分配给应用程序。那么什么时候分配呢？ 当应用访问自己内存空间中的一个地址，但实际上没有对应的物理内存时，就会导致 CPU 产生一个 PageFault（在 Intel 手册中可以查到），这是一种异常（Exception）。对异常的处理跟中断的处理很相似，会调用注册好的一个操作系统的例程，在内核态运行，来处理这个异常。这时候，操作系统就会实际分配物理内存。之后，回到用户态，继续执行你的程序，比如，一个 push 指令等等。整个过程对应用程序是透明的，其实背后有 CPU 和操作系统的参与。 @风提出了关于栈桢的第二个问题：看到汇编代码里管理栈桢的时候，用了 rbp 和 rsp 两个寄存器。是不是有点儿浪费？一个寄存器就够了啊。确实是这样，用这种写法是习惯形成的，其实可以省略。而我在34 讲里，用到的那个 foo 函数，根本没有使用栈，仅仅用寄存器就完成了工作。这时，可以把下面三行指令全部省掉： pushq %rbp movq %rsp, %rbp popq %rbp 从而让产生的机器码少 5 个字节。最重要的是，还省掉两次内存读写操作（相比对寄存器的操作，对内存的操作是很费时间的）。实际上，如果你用 GCC 编译的话，可以使用 -fomit-frame-pointer 参数来优化，会产生同样的效果，也就是不再使用 rbp。在访问栈中的地址时，会采用 4(%rsp)、8(%rsp) 的方式，在 rsp 的基础上加某个值，来访问内存。 @沉淀的梦想提出了第三个问题：栈顶（也就是 rsp 的值）为什么要 16 字节对齐？这其实是一个调用约定。是在 GCC 发展的过程中，形成的一个事实上的标准。不过，它也有一些好处，比如内存对齐后，某些指令读取数据的速度会更快，这会让你产生一个清晰的印象，每次用到栈桢，至少要占 16 个字节，也就是 4 个 32 位的整数的空间。那么，如果把一些尾递归转化为循环来执行，确实会降低系统的开销，包括内存开销和保存前一个桢的 bsp、返回地址、寄存器的运行时间开销。而 @不的问了第四个问题： 为什么要设计成区分调用者、被调用者保护的寄存器，统一由被调用者或者调用者保护，有什么问题么？这个问题是关于保护寄存器的，我没有仔细去研究它的根源。不过我想，这种策略是最经济的。 如果全部都是调用者保护，那么你调用的对象不会破坏你的寄存器的话，你也要保护起来，那就增加了成本；如果全部都是被调用者保护，也是一样的逻辑。如果调用者用了很少几个寄存器，被调用者却要保护很多，也不划算。所以最优的方法，其实是比较中庸主义的，两边各负责保护一部分，不过，我觉得这可以用概率的方法做比较严谨的证明。 关于栈桢，我最后再补充一点。有的教材用活动记录这个术语，有的教材叫做栈桢。你要知道这两个概念的联系和区别。活动记录是比较抽象的概念，它可以表现为多种实际的实现方式。在我们的课程中，栈桢加上函数调用中所使用的寄存器，就相当于一个活动记录。讲完栈桢之后，再来说说与数据流分析框架有关的问题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:45:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"细化数据流分析框架 数据流分析本身，理解起来并不难，就算不引入半格这个数学工具，你也完全可以理解。对于数据流分析方法，不同的文献也有不同的描述，有的说是 3 个要素，有的说是 4 个要素。而我在文稿里说的是 5 个要素：方向（D）、值（V）、转换函数（F）、相遇运算（meet operation, Λ）和初始值（I）。你只要把这几个问题弄清楚，就可以了。引入半格理论，主要是进一步规范相遇运算，这也是近些年研究界的一个倾向。用数学做形式化地描述虽然简洁清晰，但会不小心提升学习门槛。如果你只是为了写算法，完全可以不理半格理论，但如果为了方便看这方面算法的论文，了解半格理论会更好。 首先，半格是一种偏序集。偏序集里，某些元素是可以比较大小的。但怎么比较大小呢？其实，有时是人为定的，比如，{a, b}和{a, b, c}的大小，就是人为定的。那么，既然能比较大小，就有上界（Upper Bound）和下界（Lower Bound）的说法。给定偏序集 P 的一个子集 A，如果 A 中的每个元素 a，都小于等于一个值 x（x 属于 P），那么 x 就是 A 的一个上界。反过来，你也知道什么是下界。 半格是偏序集中，一种特殊的类型，它要求偏序集中，每个非空有限的子集，要么有最小上界（并半格，join-semilattice），要么有最大下界（交半格，meet-semilattice）。其实，如果你把一个偏序集排序的含义反过来，它就会从交半格转换成并半格，或者并半格转换成交半格。我们还定义了两个特殊值：Top、Bottom。在不同的文献里，Top 和 Bottom 有时刚好是反着的，那是因为排序的方向是反着的。 因为交半格和并半格是可以相互转化的，所以有的研究者采用的框架，就只用交半格。交半格中，集合{x, y}的最大下界，就记做 x Λ y。在做活跃性分析的时候，我们就规定{a, b} \u003e {a, b, c}就行了，这样就是个交半格。如果按照这个规矩，我在28 讲中举的那个常数传播的例子，应该把大小反过来，也做成个交半格。文稿中的写法，实际是个并半格，不过也不影响写算法。这样讲，你更容易理解了吧？现在你再看到不同文献里，关于数据流分析中的偏序集、半格的时候，应该可以明白是怎么回事了。最后，我再讲讲关于 Java 的两个知识点：泛型和反射。这也是一些同学关注的问题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:45:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"Java 的两个知识点：泛型和反射 泛型机制大大方便了我们编写某些程序，不用一次次做强制类型转换和检查了。比如，我们要用一个 String 类型的 List，就声明为： List\u003cString\u003e myList； 这样，你从 myList 中访问一个元素，获取的自然就是一个 String 对象，而不是基类 Object 对象。而增加泛型这个机制其实很简单。它只是在编译期增加了类型检查的机制，运行期没有任何改变。List和 List运行的字节码都是完全相同的。那么反射机制呢？它使我们能够在运行期，通过字符串形式的类名和方法名，来创建类，并调用方法。这其实绕过了编译期的检查机制，而是在运行期操纵对象： //获取Class Class\u003c?\u003e clazz = Class.forName(\"MyClass\"); //动态创建实例 Object obj = clazz.newInstance(); //获取add方法的引用 Method method = clazz.getMethod(\"add\",int.class,int.class); //调用add方法 Object result = method.invoke(obj,1,4); 这样能带来很多灵活性，方便你写一些框架，或者写 IDE。从编译技术的角度看，实现反射很容易。因为在32 讲中，你已经了解了字节码的结构。当时，我比较侧重讲指令，其实你还会看到它前面的，完整的符号表（也就是记录了类名、方法名等信息）。正因为有这些信息，所以反编译工具能够从字节码重新生成 Java 的源文件。所以，虽然在运行时，Java 类已经编译成字节码了，但我们仍然可以列出它所有的方法，可以实例化它，可以执行它的方法（因为可以查到方法的入口地址）。所以你看，一旦你掌握了底层机制，理解上层的一些特性就很容易了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:45:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 编译器的后端技术部分也告一段落了。我们用 16 讲的篇幅，涵盖了运行时机制、汇编语言基础知识、中间代码、优化算法、目标代码生成、垃圾收集、即时编译等知识点，还针对内存计算和 Java 的字节生码成做了两个练习，中间还一直穿插介绍 LLVM 这个工具。我之前就提到，实现一个编译器，后端的工作量会很大，现在你应该有所体会。在这里，我也想强调，后端技术的工程性比较强，每本书所采用的术语和算法等信息，都不尽相同。在我们的课程中，我给你梳理了一条，比较清晰的脉络，你可以沿着这条脉络，逐步深化，不断获得自己的感悟，早日修炼成后端技术的高手！在答疑篇的最后，我总结了一些案例，供你参考。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:45:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"案例总结 第一批示例程序，与汇编代码有关，包括手写的汇编代码，以及从 playscript 生成汇编代码的程序。这部分内容，主要是打破你对汇编代码的畏惧心，知道它虽然细节很多，但并不难。在讲解后端技术部分时，我总是在提汇编代码，在 34 讲，我甚至写了一个黑客级的小程序，直接操作机器码。我希望经历了这些过程之后，你能对汇编代码亲切起来，产生可以掌握它的信心。第二批示例程序，是基于 LLVM 工具生成 IR 的示例代码。掌握 LLVM 的 IR，熟悉调用 LLVM 的 API 编程，能让你在写完前端以后，以最短的时间，拥有所有后端的功能。通过 LLVM，你也会更加具体的体会，代码优化等功能。第三批示例程序，是内存计算和字节码生成，这两个应用题目。通过这两个应用题目，你会体会到两点： 编译器后端技术对于从事一些基础软件的开发很有用；虽然课程没有过多讲解 Java 技术，只通过一个应用篇去使用 Java 的字节码，但你会发现，我们对后端技术的基本知识，比如对中间代码的理解，都可以马上应用到 Java 语言上，得到举一反三的感觉。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:45:6","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"面向未来的编程语言 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:46:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"36 | 当前技术的发展趋势以及其对编译技术的影响 在 IT 领域，技术一直在飞速的进步，而每次进步，都会带来新的业态和新的发展机遇。退回到 10 年前，移动互联网刚兴起不久，谁也没想到它会催生现在这么多的业态。而云计算还在酝酿期，腾讯和百度的创始人都觉得它走不远，现在竟然这么普及。退回到 20 年前，互联网刚兴起，上网都要拨号。互联网的几个巨头，像阿里巴巴、百度、腾讯、新浪，还有网易，都是在那个时代展露头角的。毫不夸张地说，如果你在那个时代搞技术，懂 Web 编程的话，那绝对是人人争抢的“香饽饽”，毕竟那时，Web 编程是前沿技术，懂这个领域的人，凤毛麟角。退回到 30 年前，微软等公司才刚开始展露头角，雷军、求伯君等老一代程序员也正在发力，WPS 的第一个版本运行在 DOS 操作系统上。我还记得，95 年的时候，我在大学的阶梯教室里，看了比尔盖茨曾发表的，关于未来技术方向的演讲。当时，他预测了未来的科技成果，比如移动智能设备，听上去像天方夜谭，但现在移动互联网、人工智能和 5G 的发展，早已超出了他当时的想象。那么你有理由相信，未来 10 年、20 年、30 年，会发生同样天翻地覆的变化。这种变化所造成的的影响，你我哪怕大开“脑洞”都无法预料。而你在这种趋势下，所能做的就是，把握当下，并为未来的职业生涯做好准备。这是一件认真且严肃的事情，值得你用心对待。 当然，洞悉未来很难，但你可以根据当前了解到的信息，捕捉一些发展趋势，看看这些发展趋势，让编译技术的发展方向有了哪些不同，跟你又有什么关系。本节课，我想与你分享 3 个方面的技术发展趋势，以及它们对编译技术的影响： 人工智能，以及如何让编程和编译技术变得更智能？云计算，以及是否需要云原生的语言？前端技术，以及能否出现统一各个平台的大前端技术？ 期望这些内容，能让你看到一些不同的思考视角，获得一些新的信息。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:47:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"趋势 1：让编程更智能 人工智能是当前发展最迅速的技术之一了。这几年，它的发展速度超过了人们的预期。那么你知道，它对编译技术和计算机语言的影响是什么吗？ 首先，它需要编译器能够支撑，机器学习对庞大计算力的需求，同时兼容越来越多新的硬件架构。由于机器学习的过程需要大量的计算，仅仅采用 CPU 效率很低，所以 GPU 和 TPU 等新的硬件架构得到了迅速的发展。对于编译技术来说，首要的任务，是要充分发挥这些新硬件的能力；因为 AI 的算法要能跑在各种后端架构上，包括 CPU、GPU 和 TPU，也包括仍然要采用 SIMD 等技术，所以后端技术就会变得比较复杂。同时，前端也有不同的技术框架，比如谷歌的 TensorFlow、Facebooke 的 PyTorch 等。那么编译器怎样更好地支持多种前端和多种后端呢？根据在24 讲学到的知识，你应该会想到要借助中间代码。所以，MLIR 应运而生。这里要注意，ML 是 Multi-Level（多层次）的意思，而不是 Machine Learning 的缩写。我还想告诉你，MLIR 的作者，也是 LLVM 的核心作者、Swift 语言的发明人，Chris Lattner（他目前在谷歌 TensorFlow 项目中）。而当你看到 MLIR 的格式，也许会觉得跟 LLVM 的 IR 很像，那么你其实可以用更短的学习周期来掌握这个 IR。 其次，AI 还可能让现有的编译技术发生较大的改变。实际上，把 AI 和编译技术更好地结合，是已经持续研究了 20 年左右的，一个研究方向。不过，没有很大的发展。因为之前，人工智能技术的进步不像这几年这么快。近几年，随着人工智能技术快速进步，在人脸识别、自动驾驶等各个领域产生了相当实用的成果，人们对人工智能可能给编译技术带来的改变，产生了更大的兴趣。这给了研究者们研究的动力，他们开始从各个角度探索变革的可能性。比如说，在后端技术部分，很多算法都是 NP 完全的。这就是说，如果你用穷举的方法找出最优解，成本非常高。这个时候，就会用启发式（heuristic）的算法，也就是凭借直观或经验构造的算法，能够在可接受的花费下找出一个可行的解。那么采用人工智能技术，通过大数据的训练，有可能找出更好的启发式算法，供我们选择。这是人工智能和编译技术结合的一个方向。Milepost GCC 项目早在 2009 年就发布了，它是一款开源的，人工智能编译器。它能够通过自动学习来确定去优化哪些代码，以便让程序的性能更高。据 IBM 的测试数据，某些嵌入式软件的性能因此提升了 18%。 另一个讨论度比较高的方向就是人工智能编程（或者叫自动编程）。从某种意义上看，从计算机诞生到现在，我们编写程序的方式一直没有太大的进步。最早，是通过在纸带或卡片上打孔，来写程序；后来产生了汇编语言和高级语言。但是，写程序的本质没有变化，我们只是在用更高级的方式打孔。 讽刺的是，在计算机语言的帮助下，很多领域都出现了非常好的工具，比如 CAD 完全改变了建筑设计行业。但计算机行业本身用的工具仍然是比较原始的，还是在一个编辑器中，用文本的方式输入代码。而人工智能技术可能让我们习惯已久的编程模式发生改变。比如，现在的编译器只是检查错误并生成代码，带有 AI 功能的编译器呢，有可能不仅检查出比较明显的错误，甚至还会对你的编码方式提出建议。假设你用一个循环去做某个数组的计算，带有 AI 功能的编译器会告诉你，用函数式编程做向量计算性能更高，并提供一键式替换功能。 这里延伸一下，有可能，未来写程序的方式都会改变。微软收购 GitHub 以后，运用大量的代码作为训练数据，正在改进 IDE，提供智能提示功能。而这还只是开始。目前，AI 其实已经能帮你做 UI 的设计：你画一个草图，AI 给你生成对应的 Web 页面。而且在 AI 辅助设计领域，算法还能根据你的需要，帮你生成平面或三维的设计方案。我能想象，未来，你告诉 AI 给你生成一个电商 APP，它就能给你生成出来。你再告诉它做什么样的修改，它都会立即修改。在未来，应用开发中，最令人头疼的需求变化的问题，在 AI 看来根本不是事。那么，如果这个前景是真实的，对于你而言，需要掌握什么技能呢？ 我建议你了解，编译技术和人工智能这两个领域的知识。那些计算机的基础知识会一直有用，你可以参与到编程范式迁移，这样一个伟大的进程中。现有程序开发中的那些简单枯燥，又不需要多少创造力的工作，也就是大家通常所说的“搬砖”工作，可能会被 AI 代替。而我猜测，未来的机会可能会留给两类人： 一类是具备更加深入的计算机基础技能，能应对未来挑战的，计算机技术人才，他们为新的计算基础设施的发展演化，贡献自己的力量。另一类是应用领域的专家和人才。他们通过更富有创造力的工作，利用新的编程技术实现各种应用。编写应用程序的重点，可能不再是写代码，而是通过人工智能，训练出能够反映领域特点的模型。 当然，向自动编程转移的过程肯定是逐步实现的：AI 先是能帮一些小忙，解放我们一部分工作量，比如辅助做界面设计、智能提示；接着是能够自动生成某些小的、常用的模块；最后是能够生成和管理复杂的系统。 总而言之，AI 技术给编译技术，和编程模式带来了各种可能性，而你会见证这种转变。除此之外，云计算技术的普及和深化，也可能给编译技术和编程模式带来改变。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:47:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"趋势 2：云原生的开发语言 云计算技术现在的普及度很广，所有应用的后端部分，缺省情况下都是跑在云平台上的，云就是应用的运行环境。在课程里，我带你了解过程序的运行环境。那时，我们的关注点，还是在一个单机的环境上，包括 CPU 和内存这些硬件，以及操作系统这个软件，弄清楚程序跟它们互动的关系。比如，操作系统能够加载程序，能够帮程序管理内存，能够为程序提供一些系统功能（把数据写到磁盘上等等）。 然而，在云计算时代，云就是应用的运行环境。一个应用程序不是仅仅加载到一台物理机上，而是要能够根据需要，加载很多实例到很多机器上，实现横向扩展。当然了，云也给应用程序提供各种系统功能，比如云存储功能，它就像一台单独的服务器，会给程序提供读写磁盘的能力一样。 除此之外，在单机环境下，传统的应用程序，是通过函数或方法，来调用另一个模块的功能，函数调用的层次体现为栈里一个个栈桢的叠加，编译器要能够形成正确的栈桢，实现自动的内存管理。而在云环境下，软件模块以服务的形式存在，也就是说，一个模块通过 RESTful 接口或各种 RPC 协议，调用另外的模块的功能。程序还需要处理通讯失败的情况，甚至要在调用多个微服务时，保证分布式事务特性。而我们却没从编译技术的角度，帮助程序员减轻这个负担。 导致的结果是：现在后端的程序特别复杂。你要写很多代码，来处理 RPC、消息、分布式事务、数据库分片等逻辑，还要跟各种库、框架、通讯协议等等打交道。更糟糕的是，这些技术性的逻辑跟应用逻辑，混杂在一起，让系统的复杂度迅速提高，开发成本迅速提升，维护难度也增加。很多试图采用微服务架构的项目因此受到挫折，甚至回到单一应用的架构。 所以，一个有意义的问题是：能否在语言设计的时候，就充分利用云的基础设施，实现云原生（Cloud Native）的应用？也就是说，我们的应用，能够透明地利用好云计算的能力，并能兼容各种不同厂商的云计算平台，就像传统应用程序，能够编译成，不同操作系统的可执行文件一样。 好消息是，云计算的服务商在不断地升级技术，希望能帮助应用程序，更好地利用云计算的能力。而无服务器（Serverless）架构就是最新的成果之一。采用无服务器架构，你的程序都不需要知道容器的存在，也不需要关心虚拟机和物理机器，你只需要写一个个的函数，来完成功能就可以了。至于这个函数所需要的计算能力、存储能力，想要多少就有多少。但是，云计算厂商提供的服务和接口缺少标准化，当你把大量应用都部署到某个云平台的时候，可能会被厂商锁定。如果有一门面向云原生应用的编程语言，和相应的开发平台，能帮助人们简化云应用的开发，同时又具备跨不同云平台的能力，那就最理想了。 实际上，已经有几个创业项目在做这个方向做探索了，比如 Ballerina、Pulumi和Dark，你可以看一下。当然了，云计算和编程结合起来，就是另一个有趣的话题：云编程。我会在下一讲，与你进一步讨论这个话题。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:47:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"趋势 3：大前端技术栈 上面所讲的云计算，针对的是后端编程，而与此对应的，是前端编程工作。后端工作的特点，是越来越云化，让工程师们头疼的问题，是处理分布式计算环境下，软件系统的复杂性。当然，前端的挑战也不少。 我们开发一款应用，通常需要支持 Web、IOS 和 Android 三种平台，有时候，甚至需要提供 Windows 和 macOS 的桌面客户端。不同的平台会需要不同的技术栈，从而导致一款应用的开发成本很高，这也是前端工程师们不太满意的地方。所以，前端工程师们一直希望能用一套技术栈，搞定多个平台。比如，尝试用 Web 开发的技术栈完成 Android、IOS 和桌面应用的开发。React Native、Electron 等框架是这个方面的有益探索；Flutter 项目也做了一些更大胆的尝试。Flutter 采用 Dart 开发语言，可以在 Android 和 IOS 上生成高质量的原生界面，甚至还可以支持 macOS、Windows 和 Linux 上的原生界面。另外，它还能编译成 Web 应用。所以，本质上，你可以用同一套代码，给移动端、桌面端和 Web 端开发 UI。 你可以把这种技术思路叫做大前端：同一套代码，支持多个平台。从 Flutter 框架中，你可以看出编译技术起到的作用。首先，Dart 语言也是基于虚拟机的，编译方式支持 AOT 和 JIT，能够运行在移动端和桌面端，能够调用本地操作系统的功能。对于 Web 应用则编译成 JavaScript、CSS 和 HTML。这个项目的创新力度已经超过了 React Native 这些项目，工程师们已经不满足于，在现有的语言（JavaScript）基础上编写框架，而是用一门新的语言去整合多个技术栈。当然，提到前端技术，就不能不提 Web Assembly（WASM）。WASM 是一种二进制的字节码，也就是一种新的 IR，能够在浏览器里运行。相比 JavaScript，它有以下特点： 静态类型；性能更高；支持 C/C++/Rust 等各种语言生成 WASM，LLVM 也给了 WASM 很好的支持；字节码尺寸比较少，减少了下载时间；因为提前编译成字节码，因此相比 JavaScript 减少了代码解析的时间。 由于这些特点，WASM 可以在浏览器里，更高效地运行，比如可以支持更复杂的游戏效果。我猜想，未来可能出现，基于浏览器的、性能堪比本地应用的字处理软件、电子表格软件。基于云的文档软件（比如 Google Doc）会得到再一次升级，使用者也将获得更好的体验。此外，WASM 还允许除了 JavaScript 之外的语言，来编写 Web 应用。这些语言可以像 JVM 上的语言一样，生成字节码，并且只要有运行 WASM 的虚拟机，它们就具备一样的可移植性。而且，WASM 不仅可以运行在前端，还可以运行在后端。就像 JavaScript 语言被 Node.js 项目，用于开发后端服务一样，现在 Node.js 项目也可以调用 WASM 模块。还有一些更激进的项目，正在开发高效运行 WASM 的虚拟机，比如wasmer 项目。wasmer 虚拟机可以使用 LLVM 进行编译和优化，从而能够提供更高的性能。 讨论到这里，你有什么感受？C/C++ 语言写的程序，以 WASM 的形式运行在浏览器里，或者运行在后端的虚拟机里，通过即时编译运行。完全颠覆了你对这两门语言的传统印象吧？这就是编译技术与时俱进的一个体现。其实，学过《编译原理之美》这门课程以后，我也期望你有信心，做一款 WASM 的虚拟机，并基于它，做一个类似 Node.js 的后端服务平台。因为这并没有太大的技术难度，你只要做到稳定好用，花费很多心血就是了。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:47:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 为了拓展你的视野，我带你探讨了三个技术的发展趋势，以及它们对编译技术和编程方式所带来的影响。我希望，在学完本节课之后，你能有以下收获：人工智能有可能提升现有的编译技术框架，并带来自动编程等，编程模式的重大变化。应用程序的运行环境，不能仅仅考虑单机，还要考虑云这个更大的环境。因此，新一代的编程语言和开发平台，可能会让开发云原生的应用更加简单。在应用开发的前端技术方面，如果要想支持多种平台，可能还需要通过编译技术来获得大的突破。 当然，编译技术还有很多其他的研究方向，比如更好地支持并行计算、支持物联网和低功耗场景，支持区块链，甚至支持一些同学感兴趣的，未来的量子计算机，等等。不过，在我看来，我在文中提到的这三个趋势，跟你的关系是最为密切的。因为你现在或多或少地都在接触 AI、云和前端技术。我希望今天的内容能帮你开拓思路，为迎接未来的技术趋势做好准备，并且能够更好地利用编译技术，增强自身的竞争力。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:47:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"37 | 云编程：云计算会如何改变编程模式？ 上一讲中，我分享了当前 3 个技术发展趋势，以及其对编译技术的影响。今天我们把其中的云计算和编程模式、编译技术的之间的关系、前景再展开探讨一下。总的来说，现在编写程序是越来越云化了，所以，我们简单地称作云编程就好了。 关于云编程，有很多有趣的问题：1. 编程本身是否也能上云？在云上编程会跟本地开发有什么不同？2. 如何编写云应用，来充分发挥云平台的能力？分为哪些不同的模式？3. 为什么编写云应用那么复杂？如何降低这些复杂度？云原生应用的开发平台，能否解决这些问题？本节课，我就带你深入讨论这些问题，希望借此帮助你对编程和云计算技术的关系做一个梳理，促使你更好地利用云计算技术。首先，来看看如何实现云上编程。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:48:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"实现云上编程 90 年代初，我在大学学习编程，宿舍几个人合买了一台 386 电脑。那个时候，我记得自己不太喜欢微软提供的 MFC 编程框架，这和 386 电脑没有浮点运算器，编译起来比较慢有关，编译一次使用 MFC 框架的，C++ 程序的时间，足够我看一页报纸的了。 喜欢编程的人，为了获得流畅的性能，电脑配置总是很高，虽然这足以满足 C/C++ 时代的编程需要，但进入 Java 时代后，因为应用结构越来越复杂，工程师们有时需要在笔记本或桌面电脑上，安装各种复杂的中间件，甚至还要安装数据库软件，这时，电脑的配置即便再高，也很难安装和配置好这么复杂的环境。那么到了云计算时代，挑战就更大了，比如，你能想象在电脑上安装 Hadoop 等软件，来做大数据功能的开发吗？ 其实，编写一个小的应用还好，但现在的应用越来越复杂，所需的服务端资源越来越多。以我最近参与的一个项目为例，这个项目是采用微服务架构的一个企业应用，要想实现可扩展的性能、更好的功能复用，就要用到数据库、消息队列、容器服务、RPC 服务、分布式事务服务、API 服务等等很多基础设施，在自己的电脑上配置所有这些环境，是不大可能的。 因此，工程师们已经习惯于，在云上搭建开发和测试环境，这样，可以随需获取各种云端资源。因为编程跟云的关系越发紧密，有些开发工具已经跟云平台有了一定的整合，方便开发者按需获取云端资源。比如，微软的 Visual Studio 支持直接使用 Azure 云上的资源。再进一步，IDE 本身也可以云化，我们可以把它叫做“云 IDE”。你的电脑只负责代码编辑的工作，代码本身放在云上，编译过程以及所需的类库也放在云上。Visual Studio Code 就具备 UI 和服务端分离的能力。还有一些服务商提供基于浏览器的 IDE，也是实现了前后端的分离。 我认为，未来的 IDE 可能会越来越云化，因为云 IDE 有很多优势，能给你带来很多好处。 易于管理的编程环境 编程环境完全配置在云上，不用在本地配置各种依赖项。这一点，会给编程教育这个领域，提供很大的帮助。因为，学习编程的人能够根据需要，打开不同的编程环境，立即投入学习。反之，如果要先做很多复杂的配置才能开始学习，学习热情就会减退，一些人也就因此止步了。其实，在软件开发团队中，你经常会看到这样一个现象：新加入项目组的成员，要花很长的时间，才能把开发环境搭建起来。因为他们需要安装各种软件，开通各种账号等等。那么，如果是基于云 IDE 开发的，这些麻烦都可以省掉。 支持跨平台编程 有些编程所需要的环境，在本地很难配置，在云中开发就很简单。比如，可以用 Windows 电脑为 Linux 平台开发程序，甚至你可以在云上，为你的无人机开发程序，并下载到无人机上。在为手机编程时，比较复杂的一项工作是，适配各种不同型号的手机。这时，你只需要通过云 IDE，整合同样基于云的移动应用测试环境，就可以在成百上千种型号的手机上测试你的应用了。 更强的计算能力 有些软件的编译非常消耗 CPU，比如，完整编译 LLVM 可能需要一两个小时，而充分利用服务器的资源可以让编译速度更快。如果你从事 AI 方面的开发，体会会更深，AI 需要大量的算力，并且 GPU 和 TPU 都很昂贵，我们很难自己去搭建这样的开发环境。而基于云开发，你可以按需使用云上的 GPU、TPU 和 CPU 的计算能力。 有利于开发过程的管理 开发活动集中到云上以后，会有利于各种管理工作。比如，很多软件项目是外包开发的，那么你可以想象，基于云编程的平台，甲乙双方的项目管理者，都可以获得更多关于开发过程的大数据，也更容易做好源代码的保护。 更好的团队协作 越来越多的人已经习惯在网上编写文档，平心而论，线上文档工具并没有本地的 Office 软件功能强大，是什么因素让我们更加偏爱线上文档工具呢？就是它的协作功能。团队中的成员可以同时编辑一个文档，还可以方便地将这个文档在团队中分享。 而我比较希望见到这样的场景，那就是，程序员们可以基于同一个代码文件，进行点评和交互式的修改，这相当于基于云的结对编程，对于加强团队的知识分享、提升软件质量都会有好处。基于上述几点，我个人猜测：编程这项工作，会越来越与云紧密结合。这样一来，不仅仅能方便地调取云端的资源，越来越多的编程环境也会迁移到云上。既然提到了在云上编程的方式，那么接下来，我们从编译技术的视角，来探讨一下，如何编写能充分运用云计算强大威力的应用，这样，你会对云计算有一个更加全面的认知。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:48:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"如何编写云应用？ 学习编译原理，你可能会有一个感受，那就是编程可以在不同的抽象层次上进行。也就是说，你可以通过抽象，把底层复杂的技术细节转换成上层简单的语义。程序员最早是直接编写机器码，指令和寄存器都要直接用 0101 来表示。后来，冯·诺依曼的一个学生，发明了用助记符的方法（也就是汇编语言）简化机器码的编写。用汇编语言编程的时候，你仍然要使用指令和寄存器，但可以通过名称来引用，比如34 讲中，用 pushq %rbp 这样的汇编指令来表示机器码 0x55。这就增加了一个抽象层次，用名称代替了指令和寄存器的编码。而高级语言出现后，我们不再直接访问寄存器，而是使用变量、过程和作用域，抽象程度进一步增加。 总结起来，就是我们使用的语言抽象程度越来越高，每一次抽象对下一层的复杂性做了屏蔽，因此使用起来越来越友好。而编译技术，则帮你一层层地还原这个抽象过程，重新转换成复杂的底层实现。云计算的发展过程跟编译技术也很类似。云计算服务商们希望通过层层的抽象，来屏蔽底层的复杂性，让云计算变得更易用。而且，通常来说，在较低的抽象层次上，你可以有更大的掌控力，而在更高的抽象层次上，则会获得更好的方便性。 虚拟机是人们最早使用云资源的方式，一台物理服务器可以分割成多个虚拟机。在需要的时候，可以创建同一个虚拟机镜像的多个实例，形成集群。因为虚拟机包含了一套完整的操作系统，所以占据空间比较大，启动一个实例的速度比较慢。我们一般是通过编写脚本来管理软件的部署，每种软件的安装部署方式都不相同，系统管理的负担比较重。 最近几年，容器技术变得流行起来。容器技术可以用更轻量级的方式，分配和管理计算资源。一台物理服务器可以运行几十、上百个容器，启动新容器的速度也比虚拟机快了很多。跟虚拟机模式相比，容器部署和管理软件模块的方式标准化了，我们通过 Kubernetes 这样的软件，编写配置文件来管理容器。从编译原理的角度出发，这些配置文件就是容器管理的 DSL，它用标准化的方式，取代了原来对软件配置进行管理的各种脚本。 无服务器（Serverless）架构，或者叫做 FaaS（Function as a Service），做了进一步的抽象。你只要把一个个功能写成函数，就能被平台调用，来完成 Web 服务、消息队列处理等工作。这些函数可能是运行在容器中的，通过 Kubernetes 管理的，并且按照一定的架构来协调各种服务功能。但这些技术细节都不需要你关心，你会因此丧失一些掌控力，比如，你不能自己去生成很多个线程做并行计算。不过，也因为需要你关心的技术细节变少了，编程效率会提高很多。 上面三个层次，每一级都比上一级的抽象层次更高。就像编译技术中，高级语言比汇编语言简单一样，使用无服务架构要比直接使用虚拟机和容器更简单、更方便。但即使到了 FaaS 这个层次，编写一个云应用仍然不是一件简单的事情，你还是要面临很多复杂性，比如，处理应用程序与大容量数据库的关系，实现跨公有云和私有云的应用等等。那么能否再进一步抽象并简化云应用的开发？是否能通过针对云原生应用的编程平台，来实现这个目标呢？为了探究这个问题，我们需要进一步审视一下，现在云编程仍然有哪些，需要被新的抽象层次消除掉的复杂性。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:48:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"对云原生编程平台的需求：能否解决云应用的复杂性？ 在《人月神话》里，作者把复杂性分为两种：一种叫做本质复杂性（Essential Complexity），指的是你要解决的问题本身的复杂性，是无法避免的。一种叫做附属复杂性（Accidental Complexity），是指我们在解决本质问题时，所采用的解决方案而引入的复杂性。在我们现在的系统中，90% 的工作量都是用来解决附属复杂性的。 我经常会被问到这样的问题：做一个电商系统，成本是多少？而我给出的回答是：可能几千块，也可能很多亿。如果你理解我的答案，那意味着比较理解当前软件编程的复杂性问题。因为软件系统的复杂性会随着规模急剧上升。像阿里那样的电商系统，需要成千上万位工程师来维护。它在双 11 的时候，一天的成交量要达到几千亿，接受几亿用户的访问，在性能、可靠性、安全性、数据一致性等维度，都面临巨大的挑战。最重要的是，复杂性不是线性叠加的，可能是相乘的。比如，当一个软件服务 1 万个用户的时候，增加一个功能可能需要 100 人天的话；针对服务于 1 百万用户的系统，增加同样的功能，可能需要几千到上万人天。同样的，如果功能不变，只是用户规模增加，你同样要花费很多人天来修改系统。那么你可以看出，整体的复杂性是多个因素相乘的结果，而不是简单相加。 这跟云计算的初衷是相悖的。云计算最早承诺，当我们需要更多计算资源的时候，简单增加一下就行了。然而，现有软件的架构，其实离这个目标还很远。那有没有可能把这些复杂性解耦，使得复杂性的增长变成线性或多项式级别（这里是借助算法复杂性的理论）的呢？我再带你细化地看一下附属复杂性的一些构成，以便加深你对造成复杂性的根源的理解。 基础设施的复杂性 编写一个简单的程序，你只需要写写业务逻辑、处理少量数据，采用很简单的架构就行了。但是编写大型应用，你必须关心软件运行的基础设施，比如，你是用虚拟机还是容器？你还要关心很多技术构成部分，比如 Kubernetes、队列、负载均衡器、网络、防火墙、服务发现、系统监控、安全、数据库、分片、各种优化，等等。这些基础设施产生的复杂性，要花费你很多时间。像无服务器架构这样的技术，已经能够帮你屏蔽部分的复杂性，但还不够，仍然有很多复杂性因素需要找到解决方案。举个例子。大多数商业应用都要很小心地处理跟数据库的关系，因为一旦数据出错（比如电商平台上的商品价格出错），就意味着重大的商业损失。你要根据应用需求设计数据库结构；要根据容量设计数据库分片的方案；要根据数据分析的需求设计数据仓库方案，以及对应的 ETL 程序。一个经常出现的情况是，数据处理的逻辑分布在几个微服务中，要让它们对数据的修改满足事务特征，所以你要在代码里添加与分布式事务有关的逻辑。那么，能否由云原生的开发平台来自动处理所有这些事情？我们只需要做业务对象（比如订单）的逻辑设计，把上述所有技术细节都隐藏起来呢？ 部署复杂性 大型软件从编写代码，到部署，再到生产环境运行，是一个复杂的过程。源代码可能有多个分支，需要进行合并；需要能够正确地编译；编译后的成果，要打包成可部署的对象，比如容器镜像；要对需要发布的模块进行测试，确保不会因为这次发布而造成很多 bug；要对数据库的结构、基础数据等做必要的修改；新版本的软件上线，有时候不是全部上线，而是先让一部分用户使用，然后再针对所有用户；如果上线的特性出现问题，需要能够回滚到原来的版本。 是不是很复杂？那么，这样的复杂性，是否也可以由云原生的开发平台隐藏起来呢？ API 的复杂性 我们在写云应用的时候，需要通过 API 来调用别的服务。你需要处理与之相关的各种问题，包括 API 访问的权限、访问次数的限制、错误处理、不同的 RPC 协议和调用约定，以及相同的功能在不同的云平台上使用不同的 API。 那么我的问题是：能否让 API 调用跟普通语言的函数调用一样简单，让开发平台来处理上述复杂性呢？回答上面 3 个问题，并不简单。但是，根据计算机语言的发展规律，我们总是会想办法建立更高的抽象层次，把复杂性隐藏在下层。就像高级语言隐藏了寄存器和内存管理的复杂性一样。 这样看来，解决云计算的复杂性，要求新的编程语言从更高的一个抽象层次上，做编译、转换和优化。我们只需要编写业务逻辑就可以了，当应用规模扩大时，真的只需要增加计算资源就行了；当应用需求变化时，也只需要修改业务逻辑，而不会引起技术细节上的很多工作量。能解决这些问题的软件，就是云原生的编程语言及其基础设施。而现在的技术进步已经提供了很好的基础，容器技术、无服务器架构、处理大数据的 Map/Reduce 架构等，为云原生的编程语言准备好了素材。 我相信，在很多应用领域，我们其实可以降低对掌控力的要求，从而获取更大的方便性的。比如，对于大多数企业应用来说（比如 ERP、CRM 等），进行的都是以业务数据为核心的处理，也就是以数据库为核心的处理。这些应用都具备相对一致的模式，通过更高的抽象层次，去除各种附属复杂性是有可能的。像这样的针对数据库编程的特定领域的云原生编程平台，会比较容易成功。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:48:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我带你围绕“云编程”这个话题，剖析了云计算技术怎样和编程结合。我希望以下几个观点会对你有所启发：1. 编程环境会逐渐跟云平台结合起来，不仅仅是调用云上的资源，还可能实现编程环境本身的云化。2. 编译技术能够在不同的抽象层次上，处理计算问题，云计算技术也有类似的不同级别的抽象层次。一般来说，抽象层次越高，对技术细节的掌控力就越低，但是获得的便利性就越高。3. 附属复杂性会让成本和工作量呈指数级上升，云原生编程平台的核心任务是去除附属复杂性。而我对于在特定领域，成功应用云原生编程平台，持乐观态度。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:48:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"38 | 元编程：一边写程序，一边写语言 今天，我再带你讨论一个很有趣的话题：元编程。把这个话题放在这一篇的压轴位置，也暗示了这个话题的重要性。我估计很多同学会觉得元编程（Meta Programming）很神秘。编程，你不陌生，但什么是元编程呢？ 元编程是这样一种技术：你可以让计算机程序来操纵程序，也就是说，用程序修改或生成程序。另一种说法是，具有元编程能力的语言，能够把程序当做数据来处理，从而让程序产生程序。而元编程也有传统编程所不具备的好处：比如，可以用更简单的编码来实现某个功能，以及可以按需产生、完成某个功能的代码，从而让系统更有灵活性。 某种意义上，元编程让程序员拥有了语言设计者的一些权力。是不是很酷？你甚至可以说，普通程序员自己写程序，文艺程序员让程序写程序。那么本节课，我会带你通过实际的例子，详细地来理解什么是元编程，然后探讨带有元编程能力的语言的特性，以及与编译技术的关系。通过这样的讨论，我希望你能理解元编程的思维，并利用编译技术和元编程的思维，提升自己的编程水平。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:49:0","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"从 Lisp 语言了解元编程 说起元编程，追溯源头，应该追到 Lisp 语言。这门语言其实没有复杂的语法结构，仅有的语法结构就是一个个函数嵌套的调用，就像下面的表达式，其中“+”和“*”也是函数，并不是其他语言中的操作符： (+ 2 (* 3 5)) //对2和3求和，这里+是一个函数，并不是操作符 你会发现，如果解析 Lisp 语言形成 AST，是特别简单的事情，基本上括号嵌套的结构，就是 AST 的树状结构（其实，你让 Antlr 打印输出 AST 的时候，它缺省就是按照 Lisp 的格式输出的，括号嵌套括号）。这也是 Lisp 容易支持元编程的根本原因，你实际上可以通过程序来生成，或修改 AST。我采用了 Common Lisp 的一个实现，叫做 SBCL。在 macOS 下，你可以用“brew install sbcl”来安装它；而在 Windows 平台，你需要到 sbcl.org 去下载安装。在命令行输入 sbcl，就可以进入它的 REPL，你可以试着输入刚才的代码运行一下。在 Lisp 中，你可以把 (+ 2 (* 3 5)) 看做一段代码，也可以看做是一个列表数据。所以，你可以生成这样一组数据，然后作为代码执行。这就是 Lisp 的宏功能。 我们通过一个例子来看一下，宏跟普通的函数有什么不同。下面两段代码分别是用 Java 和 Common Lisp 写的，都是求一组数据的最大值。 Java 版本： public static int max(int[] numbers) { int rtn = numbers[0]; for (int i = 1;i \u003c numbers.length; i++){ if (numbers[i] \u003e rtn) rtn = numbers[i]; } return rtn; } Common Lisp 版本： (defun mymax1 (list) (let ((rtn (first list))) ;让rtn等于list的第一个元素 (do ((i 1 (1+ i))) ;做一个循环，让i从1开始，每次加1 ((\u003e= i (length list)) rtn) ;循环终止条件：i\u003e=list的长度 (when (\u003e (nth i list) rtn) ;如果list的第i个元素 \u003e rtn (setf rtn (nth i list)))))) ;让rtn等于list的第i个元素 那么，如果写一个函数，去求一组数据的最小值，你该怎么做呢？采用普通的编程方法，你会重写一个函数，里面大部分代码都跟求最大值的代码一样，只是把其中的一个“\u003e”改为”\u003c\"。这样的话，代码佷冗余。那么，能不能实现代码复用呢？这一点，用普通的编程方法是做不到的，你需要利用元编程技术。我们用 Lisp 的宏来实现一下： (defmacro maxmin(list pred) `(let ((rtn (first ,list))) (do ((i 1 (1+ i))) ((\u003e= i (length ,list)) rtn) (when (,pred (nth i ,list) rtn) (setf rtn (nth i ,list)))))) (defun mymax2 (list) (maxmin list \u003e)) (defun mymin2 (list) (maxmin list \u003c)) 在宏中，到底使用“\u003e” 还是使用“\u003c”，是可以作为参数传入的。你可以看一下函数 mymax2 和 mymin2 的定义。这样，宏展开后，就形成了不同的代码。你可以敲入下面的命令，显示一下宏展开后的效果（跟我们前面定义的 mymax1 函数是完全一样的）。 (macroexpand-1 '(maxmin list \u003e)) 在 Lisp 运行时，会先进行宏展开，然后再编译或解释执行所生成的代码。通过这个例子，你是否理解了“用程序写程序”的含义呢？这种元编程技术用好了以后，会让代码特别精简，产生很多神奇的效果。初步了解了元编程的含义之后，你可能会问，我们毕竟不熟悉 Lisp 语言，目前那些常见的语言有没有元编程机制呢？我们又该如何加以利用呢？ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:49:1","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"不同语言的元编程机制 首先，我们回到元编程的定义上来。比较狭义的定义认为，一门语言要像 Lisp 那样，要能够把程序当做数据来操作，这样才算是具备元编程的能力。但是，你学过编译原理就知道，在 CPU 眼里，程序本来就是数据。 我们在34 讲，曾经直接把二进制机器码放到内存，然后作为函数调用执行。有一位同学在评论区留言说，这看上去就是把程序当数据处理。在32 讲中，我们也曾生成字节码，并动态加载进 JVM 中运行。这也是把程序当数据处理。实际上，整个课程，都是在把程序当做数据来处理。你先把文本形式的代码变成 Token，再变成 AST，然后是 IR，最后是汇编代码和机器代码。所以，有的研究者认为，编写编译器、汇编器、解释器、链接器、加载器、调试器，实际上都是在做元编程的工作，你可以参考一下这篇文章。 从这里，你应该得到一个启示：学习汇编技术以后，你应该有更强的自信，去发掘你所采用的语言的元编程能力，从而去实现一些高级的功能。当然了，通常我们说某个语言的元编程能力，要求并不高，没必要都去实现一个编译器（当然，如果必须要实现，你还是能做到的），而是利用语言本身的特性来操纵程序。这又分为两个级别： 如果一门语言写的程序，能够报告它自身的信息，这叫做自省（introspection）。如果能够再进一步，操纵它自身，那就更高级一些，这叫做反射（reflection）。 那么你常见的语言，都具备哪些元编程能力呢？ JavaScript 从代码的可操纵性来看，JavaScript 是很灵活的，可以给高水平的程序员，留下充分发挥的空间。JavaScript 的对象就跟一个字典差不多，你可以随时给它添加或修改某个属性，你也可以通过拼接字符串，形成一段 JavaScript 代码，然后再用 eval() 解释执行。JavaScript 还提供了一个 Reflect 对象，帮你更方便地操纵对象。实际上，JavaScript 被认为是继承了 Lisp 衣钵的几门语言之一，因为 JavaScript 的对象确实就是个可以随意修改的数据结构。这也难怪有人用 JavaScript，实现了很多优秀的框架，比如 React 和 Vue。 Java 从元编程的定义来看，Java 的反射机制就算是一种元编程机制。你可以查询一个对象的属性和方法，你也可以用程序按需生成对象和方法来处理某些问题。我们32 讲中的字节码生成技术，也是 Java 可以采用的元编程技术。你再配合上注解机制或者配置文件，就能实现类似 Spring 的功能。可以说，Spring 是采用了元编程技术的典范。 Clojure Clojure 语言是在 JVM 上，运行的一个现代版本的 Lisp 语言，所以它也继承了 Lisp 的元编程机制。 Ruby 喜欢 Ruby 语言的人很多，一个重要原因在于 Ruby 的元编程能力。而 Ruby 也声称自己继承了 Lisp 语言的精髓。其实，它的元编程能力表现在，能够在运行时，随时修改对象的属性和方法。虽然实现方式不一样，但原理和 JavaScript 其实是很像的。元编程技术使 Ruby 语言能够以很简单的方式快速实现功能，但因为 Ruby 过于动态，所以编译优化比较困难，性能比较差。Twitter 最早是基于 Ruby 写的，但后来由于性能原因改成了 Java。同样是动态性很强的语言，JavaScript 在浏览器里使用普遍，厂商们做了大量的投入进行优化，因此，JavaScript 在大部分情况下的性能，比 Ruby 高很多，有的测试用例会高 50 倍以上。所以近几年，Ruby 的流行度在下降。这也侧面说明了编译器后端技术的重要性。 C++ 语言 C++ 语言也有元编程功能，最主要的就是模板（Template）技术。C++ 标准库里的很多工具，都是用模板技术来写的，这部分功能叫做 STL（Standard Template Library），其中常用的是 vector、map、set 这些集合类。它们的特点是，都能保存各种类型的数据。看上去像是 Java 的泛型，如 vector\u003c T \u003e，但 C++ 和 Java 的实现机制是非常不同的。我们在35 讲中曾经提到 Java 的泛型，指出 Java 的泛型只是做了类型检查，实际上保存的都是 Object 对象的引用，List\u003c Integer \u003e 和 List\u003c String \u003e 对应的字节码是相同的。C++ 的模板则不一样。它像 Lisp 的宏一样，能够在编译期展开，生成 C++ 代码再编译。vector\u003c double \u003e 和 vector\u003c long \u003e 所生成的源代码是不同的，编译后的目标代码，当然也是不同的。常见语言的元编程特性，你现在已经有所了解了。但是，关于是否应该用元编程的方法写程序，以及如何利用元编程方法，却存在一些争议。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:49:2","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"是否该使用元编程技术？ 我们看到，很多支持元编程技术的语言，都声称继承了 Lisp 的设计思想。Lisp 语言也一致被认为是编程高手应该去使用的语言。可有一个悖论是，Lisp 语言至今也还很小众。Lisp 语言的倡导者之一，Paul Graham，在互联网发展的早期，曾经用 Lisp 编写了一个互联网软件 Viaweb，后来被 Yahoo 收购。但 Yahoo 收购以后，就用 C++ 重新改写了。问题是：如果 Lisp 这么优秀，为什么会被替换掉呢？ 所以，一方面，Lisp 受到很多极客的推崇，比如自由软件的领袖 Richard Stallman 就是 Lisp 的倡导者，他写的 Emacs 编辑器就采用了 Lisp 来自动实现很多功能。另一方面，Lisp 却没有成为被大多数程序员所接受的语言。这该怎么解释呢？难道普通程序员不聪明，以至于没有办法掌握宏？进一步说，我们应该怎样看待元编程这种酷炫的技术呢？该不该用 Lisp 的宏那样的机制来编程呢？程序员的圈子里，争论这个问题，争论了很多年。我比较赞同的一个看法是这样的：首先，像 Lisp 宏这样的元编程是很有用的，你可以用宏写出非常巧妙的库和框架，来给普通的程序员来用。但一个人写的宏对另外的人来说，确实是比较难懂、难维护的。从软件开发管理的角度看，难以维护的宏不是好事情。 所以，我的结论是：首先，元编程还是比较高级的程序员的工作，就像比较高级的程序员才能写编译器一样。元编程其实比写编译器简单，但还是比一般的编程要难。第二，如果你要用到元编程技术，最好所提供的软件是容易学习、维护良好的，就像 React、Vue 和 Spring 那样。这样，其他程序员只需要使用就行了，不必承担维护的职责。 其实，我们学编译技术也是一样的。你不能指望公司或者项目组的每个人，都用编译技术写一个 DSL 或者写一个工具。毕竟维护这样的代码有一定的门槛，使用这些工具的人也有一定的学习成本。我曾经看到社区里有工程师抱怨，某国外大的互联网公司里面 DSL 泛滥，新加入的成员学习成本很高。所以，一个 DSL 也好、一套类库也好，必须提供的价值远远大于学习成本，才能被广泛接受。为了降低使用者的学习成本，框架、工具的接口设计应该非常友好。怎样才算是友好呢？我们可以借鉴特定领域语言（DSL）的思路。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:49:3","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"发明自己的特定领域语言（DSL） 框架和工具的设计者，为了解决某一个特定领域的问题，需要仔细设计自己的接口。好的接口设计是对领域问题的抽象，并通过这种抽象屏蔽了底层的技术细节。这跟上一讲我们提到语言设计的抽象原则是一样的。这样的面向领域的、设计良好的接口，很多情况下都表现为 DSL，例如 React 的 JSX、Spring 的配置文件或注解。DSL 既然叫做语言，那么就应该具备语言设计的特征：通过简单的上层语义，屏蔽下层的技术细节，降低认知成本。我很早以前就在 BPM 领域工作。像 JBPM 这样的开源软件，都提供了一个定义流程的模板，也就是 DSL。这种 DSL 的优点是：你只需要了解与业务流程这个领域有关的知识，就可以定义一个流程，不需要知道流程实现的细节，学习成本很低。 15 讲的报表工具的例子，也提供了一个报表模板的参考设计，这也是一个 DSL。使用这个 DSL 的人也不需要了解报表实现的细节，也是符合抽象原则的。我们在日常工作中，还会发现很多这样的需求。你会想，如果有一门专门干这个事情的 DSL 就好了。比如，前两年我参与过一个儿童教育项目，教师需要一些带有动画的课件。如果要让一个卡通人物动起来，动画设计人员需要做很多繁琐的工作。当时就想，如果有一个语言，能够驱动这些卡通人物，让它做什么动作就做什么动作，屏蔽底层的技术复杂性，那么那些老师们就可以自己做动画了，充分发挥自己的创造力，而不需要求助于专门的技术人员。当然，要实现这种 DSL，有时候可以借助语言自带的元编程能力，就像 React 用 JavaScript 就能实现自己的 DSL。但如果 DSL 的难度比较高，那还是要实现一个编译器，这可能就是终极的元编程技能了吧！ ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:49:4","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["School courses"],"content":"课程小结 本节课，我带你了解了元编程这个话题，并把它跟编译原理联系在一起，做了一些讨论。学习编译原理的人，某种意义上都是语言的设计者。而元编程，也是让程序员具有语言设计者的能力。所以，你可以利用自己关于编译的知识，来深入掌握自己所采用的语言的元编程能力。 我希望你能记住几个要点：元编程是指用程序操纵程序的能力，也就是用程序修改或者生成程序。也有人用另外的表述方式，认为具有元编程能力的语言，能够把程序当做数据来处理，典型的代表是 Lisp 语言。编译技术的本质就是把程序当做数据处理，所以你可以用编译技术的视角考察各种语言是如何实现元编程的。采用元编程技术，要保证所实现的软件是容易学习、维护良好的。好的 DSL 能够抽象出领域的特点，不需要使用者关心下层的技术细节。DSL 可以用元编程技术实现，也可以用我们本课程的编译技术实现。 ","date":"2022-07-18 23:55:09","objectID":"/cp_base/:49:5","tags":["compilation principle"],"title":"Cp_base","uri":"/cp_base/"},{"categories":["Advanced learning"],"content":"趣谈网络协议 刘超 2018-05-14 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:0:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"开篇词 | 想成为技术牛人？先搞定网络协议！ 工作 15 年，我在 EMC 做过类似 GFS 的分布式存储开发，做过基于 Lucene 的搜索引擎，做过 Hadoop 的运维；在 HP 和华为做过 OpenStack 的开发、实施和解决方案；还创业倒腾过 Mesos 容器平台，后来在网易做 Kubernetes。 集群规模一大，我们首先想到的就是网络互通的问题；应用吞吐量压不上去，我们首先想到的也是网络互通的问题。不客气地讲，很多情况下，只要搞定了网络，一个大型系统也就搞定了一半。所以，要成为技术牛人，搞定大系统，一定要过网络这一关，而网络协议在网络中占有举足轻重的地位。 第一，我会从身边经常见到的事情出发，用故事来讲解各种网络协议，然后慢慢扩展到不熟悉的领域。 第二，我会用贴近场景的方式来讲解网络协议，将各个层次的关系串起来，而非孤立地讲解某个概念。 第三，我会在讲解完各个层次的网络协议之后，着重剖析如何在当下热门领域使用这些协议，比如云计算、容器和微服务。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:1:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"通信协议综述 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:2:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第1讲 | 为什么要学习网络协议？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:3:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"协议三要素 当然，这种协议还是更接近人类语言，机器不能直接读懂，需要进行翻译，翻译的工作教给编译器，也就是程序员常说的 compile。这个过程比较复杂，其中的编译原理非常复杂，我在这里不进行详述。 但是可以看得出，计算机语言作为程序员控制一台计算机工作的协议，具备了协议的三要素。 语法，就是这一段内容要符合一定的规则和格式。例如，括号要成对，结束要使用分号等。语义，就是这一段内容要代表某种意义。例如数字减去数字是有意义的，数字减去文本一般来说就没有意义。顺序，就是先干啥，后干啥。例如，可以先加上某个数值，然后再减去某个数值。 HTTP/1.1 200 OK Date: Tue, 27 Mar 2018 16:50:26 GMT Content-Type: text/html;charset=UTF-8 Content-Language: zh-CN \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003cbase href=\"https://pages.kaola.com/\" /\u003e \u003cmeta charset=\"utf-8\"/\u003e \u003ctitle\u003e网易考拉3周年主会场\u003c/title\u003e 这符合协议的三要素吗？我带你来看一下。 首先，符合语法，也就是说，只有按照上面那个格式来，浏览器才认。例如，上来是状态，然后是首部，然后是内容。第二，符合语义，就是要按照约定的意思来。例如，状态 200，表述的意思是网页成功返回。如果不成功，就是我们常见的“404”。第三，符合顺序，你一点浏览器，就是发送出一个 HTTP 请求，然后才有上面那一串 HTTP 返回的东西。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:3:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"我们常用的网络协议有哪些？ DNS、HTTP、HTTPS 所在的层我们称为应用层。经过应用层封装后，浏览器会将应用层的包交给下一层去完成，通过 socket 编程来实现。下一层是传输层。传输层有两种协议，一种是无连接的协议 UDP，一种是面向连接的协议 TCP。对于支付来讲，往往使用 TCP 协议。所谓的面向连接就是，TCP 会保证这个包能够到达目的地。如果不能到达，就会重新发送，直至到达。 TCP 协议里面会有两个端口，一个是浏览器监听的端口，一个是电商的服务器监听的端口。操作系统往往通过端口来判断，它得到的包应该给哪个进程。 传输层封装完毕后，浏览器会将包交给操作系统的网络层。网络层的协议是 IP 协议。在 IP 协议里面会有源 IP 地址，即浏览器所在机器的 IP 地址和目标 IP 地址，也即电商网站所在服务器的 IP 地址。 操作系统既然知道了目标 IP 地址，就开始想如何根据这个门牌号找到目标机器。操作系统往往会判断，这个目标 IP 地址是本地人，还是外地人。如果是本地人，从门牌号就能看出来，但是显然电商网站不在本地，而在遥远的地方。 操作系统知道要离开本地去远方。虽然不知道远方在何处，但是可以这样类比一下：如果去国外要去海关，去外地就要去网关。而操作系统启动的时候，就会被 DHCP 协议配置 IP 地址，以及默认的网关的 IP 地址 192.168.1.1。 操作系统如何将 IP 地址发给网关呢？在本地通信基本靠吼，于是操作系统大吼一声，谁是 192.168.1.1 啊？网关会回答它，我就是，我的本地地址在村东头。这个本地地址就是 MAC 地址，而大吼的那一声是 ARP 协议。 于是操作系统将 IP 包交给了下一层，也就是 MAC 层。网卡再将包发出去。由于这个包里面是有 MAC 地址的，因而它能够到达网关。 网关收到包之后，会根据自己的知识，判断下一步应该怎么走。网关往往是一个路由器，到某个 IP 地址应该怎么走，这个叫作路由表。 路由器有点像玄奘西行路过的一个个国家的一个个城关。每个城关都连着两个国家，每个国家相当于一个局域网，在每个国家内部，都可以使用本地的地址 MAC 进行通信。 一旦跨越城关，就需要拿出 IP 头来，里面写着贫僧来自东土大唐（就是源 IP 地址），欲往西天拜佛求经（指的是目标 IP 地址）。路过宝地，借宿一晚，明日启程，请问接下来该怎么走啊？ 城关往往是知道这些“知识”的，因为城关和临近的城关也会经常沟通。到哪里应该怎么走，这种沟通的协议称为路由协议，常用的有 OSPF 和 BGP。 城关与城关之间是一个国家，当网络包知道了下一步去哪个城关，还是要使用国家内部的 MAC 地址，通过下一个城关的 MAC 地址，找到下一个城关，然后再问下一步的路怎么走，一直到走出最后一个城关。 最后一个城关知道这个网络包要去的地方。于是，对着这个国家吼一声，谁是目标 IP 啊？目标服务器就会回复一个 MAC 地址。网络包过关后，通过这个 MAC 地址就能找到目标服务器。 目标服务器发现 MAC 地址对上了，取下 MAC 头来，发送给操作系统的网络层。发现 IP 也对上了，就取下 IP 头。IP 头里会写上一层封装的是 TCP 协议，然后将其交给传输层，即 TCP 层。 在这一层里，对于收到的每个包，都会有一个回复的包说明收到了。这个回复的包绝非这次下单请求的结果，例如购物是否成功，扣了多少钱等，而仅仅是 TCP 层的一个说明，即收到之后的回复。当然这个回复，会沿着刚才来的方向走回去，报个平安。 如果过一段时间还是没到，发送端的 TCP 层会重新发送这个包，还是上面的过程，直到有一天收到平安到达的回复。**这个重试绝非你的浏览器重新将下单这个动作重新请求一次。**对于浏览器来讲，就发送了一次下单请求，TCP 层不断自己闷头重试。除非 TCP 这一层出了问题，例如连接断了，才轮到浏览器的应用层重新发送下单请求。 当网络包平安到达 TCP 层之后，TCP 头中有目标端口号，通过这个端口号，可以找到电商网站的进程正在监听这个端口号，假设一个 Tomcat，将这个包发给电商网站。 电商网站的进程得到 HTTP 请求的内容，知道了要买东西，买多少。往往一个电商网站最初接待请求的这个 Tomcat 只是个接待员，负责统筹处理这个请求，而不是所有的事情都自己做。例如，这个接待员要告诉专门管理订单的进程，登记要买某个商品，买多少，要告诉管理库存的进程，库存要减少多少，要告诉支付的进程，应该付多少钱，等等。如何告诉相关的进程呢？往往通过 RPC 调用，即远程过程调用的方式来实现。远程过程调用就是当告诉管理订单进程的时候，接待员不用关心中间的网络互连问题，会由 RPC 框架统一处理。RPC 框架有很多种，有基于 HTTP 协议放在 HTTP 的报文里面的，有直接封装在 TCP 报文里面的。当接待员发现相应的部门都处理完毕，就回复一个 HTTPS 的包，告知下单成功。这个 HTTPS 的包，会像来的时候一样，经过千难万险到达你的个人电脑，最终进入浏览器，显示支付成功。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:4:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 看到了吧，一个简简单单的下单过程，中间牵扯到这么多的协议。而管理一大片机器，更是一件特别有技术含量的事情。除此之外，像最近比较火的云计算、容器、微服务等技术，也都需要借助各种协议，来达成大规模机器之间的合作。我在这里列一下之后要讲的网络协议，之后我会按照从底层到上层的顺序来讲述。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:4:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第2讲 | 网络分层的真实含义是什么？ 当你听到什么二层设备、三层设备、四层 LB 和七层 LB 中层的时候，是否有点一头雾水，不知道这些所谓的层，对应的各种协议具体要做什么“工作”？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:5:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"这四个问题你真的懂了吗？ 那么第一个问题来了。TCP 在进行三次握手的时候，IP 层和 MAC 层对应都有什么操作呢？ 那么第二个问题来了。A 知道自己的下一个中转站是 B，那从 A 发出来的包，应该把 B 的 IP 地址放在哪里呢？B 知道自己的下一个中转站是 C，从 B 发出来的包，应该把 C 的 IP 地址放在哪里呢？如果放在 IP 协议中的目标地址，那包到了中转站，怎么知道最终的目的地址是 D 呢？ 我再问你一个问题。你一定经常听说二层设备、三层设备。二层设备处理的通常是 MAC 层的东西。那我发送一个 HTTP 的包，是在第七层工作的，那是不是不需要经过二层设备？或者即便经过了，二层设备也不处理呢？或者换一种问法，二层设备处理的包里，有没有 HTTP 层的内容呢？ 最终，我想问你一个综合的问题。从你的电脑，通过 SSH 登录到公有云主机里面，都需要经历哪些过程？或者说你打开一个电商网站，都需要经历哪些过程？说得越详细越好。 上面的这些问题，有的在这一节就会有一个解释，有的则会贯穿我们整个课程。好在后面一节中我会举一个贯穿的例子，将很多层的细节讲过后，你很容易就能把这些知识点串起来。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:5:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"网络为什么要分层？ 这里我们先探讨第一个问题，网络为什么要分层？因为，是个复杂的程序都要分层 复杂的程序都要分层，这是程序设计的要求。比如，复杂的电商还会分数据库层、缓存层、Compose 层、Controller 层和接入层，每一层专注做本层的事情。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:5:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"程序是如何工作的？ 我们可以简单地想象“你”这个程序的工作过程。 当一个网络包从一个网口经过的时候，你看到了，首先先看看要不要请进来，处理一把。有的网口配置了混杂模式，凡是经过的，全部拿进来。拿进来以后，就要交给一段程序来处理。于是，你调用 process_layer2(buffer)。当然，这是一个假的函数。但是你明白其中的意思，知道肯定是有这么个函数的。那这个函数是干什么的呢？从 Buffer 中，摘掉二层的头，看一看，应该根据头里面的内容做什么操作。 假设你发现这个包的 MAC 地址和你的相符，那说明就是发给你的，于是需要调用 process_layer3(buffer)。这个时候，Buffer 里面往往就没有二层的头了，因为已经在上一个函数的处理过程中拿掉了，或者将开始的偏移量移动了一下。在这个函数里面，摘掉三层的头，看看到底是发送给自己的，还是希望自己转发出去的。如何判断呢？如果 IP 地址不是自己的，那就应该转发出去；如果 IP 地址是自己的，那就是发给自己的。根据 IP 头里面的标示，拿掉三层的头，进行下一层的处理，到底是调用 process_tcp(buffer) 呢，还是调用 process_udp(buffer) 呢？假设这个地址是 TCP 的，则会调用 process_tcp(buffer)。这时候，Buffer 里面没有三层的头，就需要查看四层的头，看这是一个发起，还是一个应答，又或者是一个正常的数据包，然后分别由不同的逻辑进行处理。如果是发起或者应答，接下来可能要发送一个回复包；如果是一个正常的数据包，就需要交给上层了。交给谁呢？是不是有 process_http(buffer) 函数呢？ 没有的，如果你是一个网络包处理程序，你不需要有 process_http(buffer)，而是应该交给应用去处理。交给哪个应用呢？在四层的头里面有端口号，不同的应用监听不同的端口号。如果发现浏览器应用在监听这个端口，那你发给浏览器就行了。至于浏览器怎么处理，和你没有关系。浏览器自然是解析 HTML，显示出页面来。电脑的主人看到页面很开心，就点了鼠标。点击鼠标的动作被浏览器捕获。浏览器知道，又要发起另一个 HTTP 请求了，于是使用端口号，将请求发给了你。 你应该调用 send_tcp(buffer)。不用说，Buffer 里面就是 HTTP 请求的内容。这个函数里面加一个 TCP 的头，记录下源端口号。浏览器会给你目的端口号，一般为 80 端口。然后调用 send_layer3(buffer)。Buffer 里面已经有了 HTTP 的头和内容，以及 TCP 的头。在这个函数里面加一个 IP 的头，记录下源 IP 的地址和目标 IP 的地址。然后调用 send_layer2(buffer)。Buffer 里面已经有了 HTTP 的头和内容、TCP 的头，以及 IP 的头。这个函数里面要加一下 MAC 的头，记录下源 MAC 地址，得到的就是本机器的 MAC 地址和目标的 MAC 地址。不过，这个还要看当前知道不知道，知道就直接加上；不知道的话，就要通过一定的协议处理过程，找到 MAC 地址。反正要填一个，不能空着。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:5:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"揭秘层与层之间的关系 所有不能表示出层层封装含义的比喻，都是不恰当的。 那 TCP 在三次握手的时候，IP 层和 MAC 层在做什么呢？当然是 TCP 发送每一个消息，都会带着 IP 层和 MAC 层了。因为，TCP 每发送一个消息，IP 层和 MAC 层的所有机制都要运行一遍。而你只看到 TCP 三次握手了，其实，IP 层和 MAC 层为此也忙活好久了。 这里要记住一点：只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层。 所以，对 TCP 协议来说，三次握手也好，重试也好，只要想发出去包，就要有 IP 层和 MAC 层，不然是发不出去的。 经常有人会问这样一个问题，我都知道那台机器的 IP 地址了，直接发给他消息呗，要 MAC 地址干啥？这里的关键就是，没有 MAC 地址消息是发不出去的。 所以如果一个 HTTP 协议的包跑在网络上，它一定是完整的。无论这个包经过哪些设备，它都是完整的。 所谓的二层设备、三层设备，都是这些设备上跑的程序不同而已。一个 HTTP 协议的包经过一个二层设备，二层设备收进去的是整个网络包。这里面 HTTP、TCP、 IP、 MAC 都有。什么叫二层设备呀，就是只把 MAC 头摘下来，看看到底是丢弃、转发，还是自己留着。那什么叫三层设备呢？就是把 MAC 头摘下来之后，再把 IP 头摘下来，看看到底是丢弃、转发，还是自己留着。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:5:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第3讲 | ifconfig：最熟悉又陌生的命令行 你知道 ifconfig 和 ip addr 的区别吗？这是一个有关 net-tools 和 iproute2 的“历史”故事，你刚来到第三节，暂时不用了解这么细，但这也是一个常考的知识点。 想象一下，你登录进入一个被裁剪过的非常小的 Linux 系统中，发现既没有 ifconfig 命令，也没有 ip addr 命令，你是不是感觉这个系统压根儿没法用？这个时候，你可以自行安装 net-tools 和 iproute2 这两个工具。当然，大多数时候这两个命令是系统自带的。 安装好后，我们来运行一下 ip addr。不出意外，应该会输出下面的内容。 root@test:~# ip addr 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff inet 10.100.122.2/24 brd 10.100.122.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::f816:3eff:fec7:7975/64 scope link valid_lft forever preferred_lft forever 这个命令显示了这台机器上所有的网卡。大部分的网卡都会有一个 IP 地址，当然，这不是必须的。在后面的分享中，我们会遇到没有 IP 地址的情况。 IP 地址是一个网卡在网络世界的通讯地址，相当于我们现实世界的门牌号码。既然是门牌号码，不能大家都一样，不然就会起冲突。比方说，假如大家都叫六单元 1001 号，那快递就找不到地方了。所以，有时候咱们的电脑弹出网络地址冲突，出现上不去网的情况，多半是 IP 地址冲突了。 如上输出的结果，10.100.122.2 就是一个 IP 地址。这个地址被点分隔为四个部分，每个部分 8 个 bit，所以 IP 地址总共是 32 位。这样产生的 IP 地址的数量很快就不够用了。因为当时设计 IP 地址的时候，哪知道今天会有这么多的计算机啊！因为不够用，于是就有了 IPv6，也就是上面输出结果里面 inet6 fe80::f816:3eff:fec7:7975/64。这个有 128 位，现在看来是够了，但是未来的事情谁知道呢？ 本来 32 位的 IP 地址就不够，还被分成了 5 类。现在想想，当时分配地址的时候，真是太奢侈了。 在网络地址中，至少在当时设计的时候，对于 A、B、 C 类主要分两部分，前面一部分是网络号，后面一部分是主机号。这很好理解，大家都是六单元 1001 号，我是小区 A 的六单元 1001 号，而你是小区 B 的六单元 1001 号。下面这个表格，详细地展示了 A、B、C 三类地址所能包含的主机的数量。在后文中，我也会多次借助这个表格来讲解。 这里面有个尴尬的事情，就是 C 类地址能包含的最大主机数量实在太少了，只有 254 个。当时设计的时候恐怕没想到，现在估计一个网吧都不够用吧。而 B 类地址能包含的最大主机数量又太多了。6 万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:6:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"无类型域间选路（CIDR） 于是有了一个折中的方式叫作无类型域间选路，简称 CIDR。这种方式打破了原来设计的几类地址的做法，将 32 位的 IP 地址一分为二，前面是网络号，后面是主机号。从哪里分呢？你如果注意观察的话可以看到，10.100.122.2/24，这个 IP 地址中有一个斜杠，斜杠后面有个数字 24。这种地址表示形式，就是 CIDR。后面 24 的意思是，32 位中，前 24 位是网络号，后 8 位是主机号。 伴随着 CIDR 存在的，一个是广播地址，10.100.122.255。如果发送这个地址，所有 10.100.122 网络里面的机器都可以收到。另一个是子网掩码，255.255.255.0。 将子网掩码和 IP 地址进行 AND 计算。前面三个 255，转成二进制都是 1。1 和任何数值取 AND，都是原来数值，因而前三个数不变，为 10.100.122。后面一个 0，转换成二进制是 0，0 和任何数值取 AND，都是 0，因而最后一个数变为 0，合起来就是 10.100.122.0。这就是网络号。将子网掩码和 IP 地址按位计算 AND，就可得到网络号。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:6:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"公有 IP 地址和私有 IP 地址 在日常的工作中，几乎不用划分 A 类、B 类或者 C 类，所以时间长了，很多人就忘记了这个分类，而只记得 CIDR。但是有一点还是要注意的，就是公有 IP 地址和私有 IP 地址。 我们继续看上面的表格。表格最右列是私有 IP 地址段。平时我们看到的数据中心里，办公室、家里或学校的 IP 地址，一般都是私有 IP 地址段。因为这些地址允许组织内部的 IT 人员自己管理、自己分配，而且可以重复。因此，你学校的某个私有 IP 地址段和我学校的可以是一样的。这就像每个小区有自己的楼编号和门牌号，你们小区可以叫 6 栋，我们小区也叫 6 栋，没有任何问题。但是一旦出了小区，就需要使用公有 IP 地址。就像人民路 888 号，是国家统一分配的，不能两个小区都叫人民路 888 号。公有 IP 地址有个组织统一分配，你需要去买。如果你搭建一个网站，给你学校的人使用，让你们学校的 IT 人员给你一个 IP 地址就行。但是假如你要做一个类似网易 163 这样的网站，就需要有公有 IP 地址，这样全世界的人才能访问。表格中的 192.168.0.x 是最常用的私有 IP 地址。你家里有 Wi-Fi，对应就会有一个 IP 地址。一般你家里地上网设备不会超过 256 个，所以 /24 基本就够了。有时候我们也能见到 /16 的 CIDR，这两种是最常见的，也是最容易理解的。 不需要将十进制转换为二进制 32 位，就能明显看出 192.168.0 是网络号，后面是主机号。而整个网络里面的第一个地址 192.168.0.1，往往就是你这个私有网络的出口地址。例如，你家里的电脑连接 Wi-Fi，Wi-Fi 路由器的地址就是 192.168.0.1，而 192.168.0.255 就是广播地址。一旦发送这个地址，整个 192.168.0 网络里面的所有机器都能收到。但是也不总都是这样的情况。因此，其他情况往往就会很难理解，还容易出错。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:6:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"举例：一个容易“犯错”的 CIDR 我们来看 16.158.165.91/22 这个 CIDR。求一下这个网络的第一个地址、子网掩码和广播地址。你要是上来就写 16.158.165.1，那就大错特错了。 /22 不是 8 的整数倍，不好办，只能先变成二进制来看。16.158 的部分不会动，它占了前 16 位。中间的 165，变为二进制为10100101。除了前面的 16 位，还剩 6 位。所以，这 8 位中前 6 位是网络号，16.158.\u003c101001\u003e，而 \u003c01\u003e.91 是机器号。第一个地址是 16.158.\u003c101001\u003e\u003c00\u003e.1，即 16.158.164.1。子网掩码是 255.255.\u003c111111\u003e\u003c00\u003e.0，即 255.255.252.0。广播地址为 16.158.\u003c101001\u003e\u003c11\u003e.255，即 16.158.167.255。 这五类地址中，还有一类 D 类是组播地址。使用这一类地址，属于某个组的机器都能收到。这有点类似在公司里面大家都加入了一个邮件组。发送邮件，加入这个组的都能收到。组播地址在后面讲述 VXLAN 协议的时候会提到。 讲了这么多，才讲了上面的输出结果中很小的一部分，是不是觉得原来并没有真的理解 ip addr 呢？我们接着来分析。在 IP 地址的后面有个 scope，对于 eth0 这张网卡来讲，是 global，说明这张网卡是可以对外的，可以接收来自各个地方的包。对于 lo 来讲，是 host，说明这张网卡仅仅可以供本机相互通信。lo 全称是 loopback，又称环回接口，往往会被分配到 127.0.0.1 这个地址。这个地址用于本机通信，经过内核处理后直接返回，不会在任何网络中出现。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:6:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"MAC 地址 在 IP 地址的上一行是 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff，这个被称为 MAC 地址，是一个网卡的物理地址，用十六进制，6 个 byte 表示。MAC 地址是一个很容易让人“误解”的地址。因为 MAC 地址号称全局唯一，不会有两个网卡有相同的 MAC 地址，而且网卡自生产出来，就带着这个地址。很多人看到这里就会想，既然这样，整个互联网的通信，全部用 MAC 地址好了，只要知道了对方的 MAC 地址，就可以把信息传过去。 这样当然是不行的。 一个网络包要从一个地方传到另一个地方，除了要有确定的地址，还需要有定位功能。 而有门牌号码属性的 IP 地址，才是有远程定位功能的。 MAC 地址更像是身份证，是一个唯一的标识。它的唯一性设计是为了组网的时候，不同的网卡放在一个网络里面的时候，可以不用担心冲突。从硬件角度，保证不同的网卡有不同的标识。 MAC 地址是有一定定位功能的，只不过范围非常有限。你可以根据 IP 地址，找到杭州市网商路 599 号 B 楼 6 层，但是依然找不到我，你就可以靠吼了，大声喊身份证 XXXX 的是哪位？我听到了，我就会站起来说，是我啊。但是如果你在上海，到处喊身份证 XXXX 的是哪位，我不在现场，当然不会回答，因为我在杭州不在上海。所以，MAC 地址的通信范围比较小，局限在一个子网里面。例如，从 192.168.0.2/24 访问 192.168.0.3/24 是可以用 MAC 地址的。一旦跨子网，即从 192.168.0.2/24 到 192.168.1.2/24，MAC 地址就不行了，需要 IP 地址起作用了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:6:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"网络设备的状态标识 解析完了 MAC 地址，我们再来看 \u003c BROADCAST,MULTICAST,UP,LOWER_UP\u003e 是干什么的？这个叫做 net_device flags，网络设备的状态标识。 UP 表示网卡处于启动的状态；BROADCAST 表示这个网卡有广播地址，可以发送广播包；MULTICAST 表示网卡可以发送多播包；LOWER_UP 表示 L1 是启动的，也即网线插着呢。MTU1500 是指什么意思呢？是哪一层的概念呢？最大传输单元 MTU 为 1500，这是以太网的默认值。 上一节，我们讲过网络包是层层封装的。MTU 是二层 MAC 层的概念。MAC 层有 MAC 的头，以太网规定正文部分不允许超过 1500 个字节。正文里面有 IP 的头、TCP 的头、HTTP 的头。如果放不下，就需要分片来传输。 qdisc pfifo_fast 是什么意思呢？qdisc 全称是 queueing discipline，中文叫排队规则。内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的 qdisc（排队规则）把数据包加入队列。 最简单的 qdisc 是 pfifo，它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。pfifo_fast 稍微复杂一些，它的队列包括三个波段（band）。在每个波段里面，使用先进先出规则。 三个波段（band）的优先级也不相同。band 0 的优先级最高，band 2 的最低。如果 band 0 里面有数据包，系统就不会处理 band 1 里面的数据包，band 1 和 band 2 之间也是一样。数据包是按照服务类型（Type of Service，TOS）被分配到三个波段（band）里面的。TOS 是 IP 头里面的一个字段，代表了当前的包是高优先级的，还是低优先级的。队列是个好东西，后面我们讲云计算中的网络的时候，会有很多用户共享一个网络出口的情况，这个时候如何排队，每个队列有多粗，队列处理速度应该怎么提升，我都会详细为你讲解。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:6:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 怎么样，看起来很简单的一个命令，里面学问很大吧？通过这一节，希望你能记住以下的知识点，后面都能用得上：IP 是地址，有定位功能；MAC 是身份证，无定位功能；CIDR 可以用来判断是不是本地人；IP 分公有的 IP 和私有的 IP。后面的章节中我会谈到“出国门”，就与这个有关。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:6:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第4讲 | DHCP与PXE：IP是怎么来的，又是怎么没的？ 如果需要和其他机器通讯，我们就需要一个通讯地址，我们需要给网卡配置这么一个地址。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:7:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何配置 IP 地址？ 那如何配置呢？如果有相关的知识和积累，你可以用命令行自己配置一个地址。可以使用 ifconfig，也可以使用 ip addr。设置好了以后，用这两个命令，将网卡 up 一下，就可以开始工作了。 使用 net-tools： $ sudo ifconfig eth1 10.0.0.1/24 $ sudo ifconfig eth1 up 使用 iproute2： $ sudo ip addr add 10.0.0.1/24 dev eth1 $ sudo ip link set up eth1 你可能会问了，自己配置这个自由度太大了吧，我是不是配置什么都可以？如果配置一个和谁都不搭边的地址呢？例如，旁边的机器都是 192.168.1.x，我非得配置一个 16.158.23.6，会出现什么现象呢？不会出现任何现象，就是包发不出去呗。为什么发不出去呢？我来举例说明。192.168.1.6 就在你这台机器的旁边，甚至是在同一个交换机上，而你把机器的地址设为了 16.158.23.6。在这台机器上，你企图去 ping192.168.1.6，你觉得只要将包发出去，同一个交换机的另一台机器马上就能收到，对不对？可是 Linux 系统不是这样的，它没你想的那么智能。你用肉眼看到那台机器就在旁边，它则需要根据自己的逻辑进行处理。 还记得我们在第二节说过的原则吗？只要是在网络上跑的包，都是完整的，可以有下层没上层，绝对不可能有上层没下层。 所以，你看着它有自己的源 IP 地址 16.158.23.6，也有目标 IP 地址 192.168.1.6，但是包发不出去，这是因为 MAC 层还没填。自己的 MAC 地址自己知道，这个容易。但是目标 MAC 填什么呢？是不是填 192.168.1.6 这台机器的 MAC 地址呢？当然不是。Linux 首先会判断，要去的这个地址和我是一个网段的吗，或者和我的一个网卡是同一网段的吗？只有是一个网段的，它才会发送 ARP 请求，获取 MAC 地址。如果发现不是呢？ Linux 默认的逻辑是，如果这是一个跨网段的调用，它便不会直接将包发送到网络上，而是企图将包发送到网关。 如果你配置了网关的话，Linux 会获取网关的 MAC 地址，然后将包发出去。对于 192.168.1.6 这台机器来讲，虽然路过它家门的这个包，目标 IP 是它，但是无奈 MAC 地址不是它的，所以它的网卡是不会把包收进去的。 如果没有配置网关呢？那包压根就发不出去。 如果将网关配置为 192.168.1.6 呢？不可能，Linux 不会让你配置成功的，因为网关要和当前的网络至少一个网卡是同一个网段的，怎么可能 16.158.23.6 的网关是 192.168.1.6 呢？ 所以，当你需要手动配置一台机器的网络 IP 时，一定要好好问问你的网络管理员。如果在机房里面，要去网络管理员那里申请，让他给你分配一段正确的 IP 地址。当然，真正配置的时候，一定不是直接用命令配置的，而是放在一个配置文件里面。不同系统的配置文件格式不同，但是无非就是 CIDR、子网掩码、广播地址和网关地址。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:7:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"动态主机配置协议（DHCP） 原来配置 IP 有这么多门道儿啊。你可能会问了，配置了 IP 之后一般不能变的，配置一个服务端的机器还可以，但是如果是客户端的机器呢？我抱着一台笔记本电脑在公司里走来走去，或者白天来晚上走，每次使用都要配置 IP 地址，那可怎么办？还有人事、行政等非技术人员，如果公司所有的电脑都需要 IT 人员配置，肯定忙不过来啊。 因此，我们需要有一个自动配置的协议，也就是动态主机配置协议（Dynamic Host Configuration Protocol），简称 DHCP。有了这个协议，网络管理员就轻松多了。他只需要配置一段共享的 IP 地址。每一台新接入的机器都通过 DHCP 协议，来这个共享的 IP 地址里申请，然后自动配置好就可以了。等人走了，或者用完了，还回去，这样其他的机器也能用。 所以说，如果是数据中心里面的服务器，IP 一旦配置好，基本不会变，这就相当于买房自己装修。DHCP 的方式就相当于租房。你不用装修，都是帮你配置好的。你暂时用一下，用完退租就可以了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:7:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"解析 DHCP 的工作方式 当一台机器新加入一个网络的时候，肯定一脸懵，啥情况都不知道，只知道自己的 MAC 地址。怎么办？先吼一句，我来啦，有人吗？这时候的沟通基本靠“吼”。这一步，我们称为 DHCP Discover。 新来的机器使用 IP 地址 0.0.0.0 发送了一个广播包，目的 IP 地址为 255.255.255.255。广播包封装了 UDP，UDP 封装了 BOOTP。其实 DHCP 是 BOOTP 的增强版，但是如果你去抓包的话，很可能看到的名称还是 BOOTP 协议。在这个广播包里面，新人大声喊：我是新来的（Boot request），我的 MAC 地址是这个，我还没有 IP，谁能给租给我个 IP 地址！格式就像这样： 如果一个网络管理员在网络里面配置了 DHCP Server 的话，他就相当于这些 IP 的管理员。他立刻能知道来了一个“新人”。这个时候，我们可以体会 MAC 地址唯一的重要性了。当一台机器带着自己的 MAC 地址加入一个网络的时候，MAC 是它唯一的身份，如果连这个都重复了，就没办法配置了。只有 MAC 唯一，IP 管理员才能知道这是一个新人，需要租给它一个 IP 地址，这个过程我们称为 DHCP Offer。同时，DHCP Server 为此客户保留为它提供的 IP 地址，从而不会为其他 DHCP 客户分配此 IP 地址。DHCP Offer 的格式就像这样，里面有给新人分配的地址。 DHCP Server 仍然使用广播地址作为目的地址，因为，此时请求分配 IP 的新人还没有自己的 IP。DHCP Server 回复说，我分配了一个可用的 IP 给你，你看如何？除此之外，服务器还发送了子网掩码、网关和 IP 地址租用期等信息。新来的机器很开心，它的“吼”得到了回复，并且有人愿意租给它一个 IP 地址了，这意味着它可以在网络上立足了。当然更令人开心的是，如果有多个 DHCP Server，这台新机器会收到多个 IP 地址，简直受宠若惊。它会选择其中一个 DHCP Offer，一般是最先到达的那个，并且会向网络发送一个 DHCP Request 广播数据包，包中包含客户端的 MAC 地址、接受的租约中的 IP 地址、提供此租约的 DHCP 服务器地址等，并告诉所有 DHCP Server 它将接受哪一台服务器提供的 IP 地址，告诉其他 DHCP 服务器，谢谢你们的接纳，并请求撤销它们提供的 IP 地址，以便提供给下一个 IP 租用请求者。 此时，由于还没有得到 DHCP Server 的最后确认，客户端仍然使用 0.0.0.0 为源 IP 地址、255.255.255.255 为目标地址进行广播。在 BOOTP 里面，接受某个 DHCP Server 的分配的 IP。当 DHCP Server 接收到客户机的 DHCP request 之后，会广播返回给客户机一个 DHCP ACK 消息包，表明已经接受客户机的选择，并将这一 IP 地址的合法租用信息和其他的配置信息都放入该广播包，发给客户机，欢迎它加入网络大家庭。 最终租约达成的时候，还是需要广播一下，让大家都知道。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:7:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"IP 地址的收回和续租 既然是租房子，就是有租期的。租期到了，管理员就要将 IP 收回。如果不用的话，收回就收回了。就像你租房子一样，如果还要续租的话，不能到了时间再续租，而是要提前一段时间给房东说。DHCP 也是这样。客户机会在租期过去 50% 的时候，直接向为其提供 IP 地址的 DHCP Server 发送 DHCP request 消息包。客户机接收到该服务器回应的 DHCP ACK 消息包，会根据包中所提供的新的租期以及其他已经更新的 TCP/IP 参数，更新自己的配置。这样，IP 租用更新就完成了。好了，一切看起来完美。DHCP 协议大部分人都知道，但是其实里面隐藏着一个细节，很多人可能不会去注意。接下来，我就讲一个有意思的事情：网络管理员不仅能自动分配 IP 地址，还能帮你自动安装操作系统！ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:7:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"预启动执行环境（PXE） 普通的笔记本电脑，一般不会有这种需求。因为你拿到电脑时，就已经有操作系统了，即便你自己重装操作系统，也不是很麻烦的事情。但是，在数据中心里就不一样了。数据中心里面的管理员可能一下子就拿到几百台空的机器，一个个安装操作系统，会累死的。所以管理员希望的不仅仅是自动分配 IP 地址，还要自动安装系统。装好系统之后自动分配 IP 地址，直接启动就能用了，这样当然最好了！ 这事儿其实仔细一想，还是挺有难度的。安装操作系统，应该有个光盘吧。数据中心里不能用光盘吧，想了一个办法就是，可以将光盘里面要安装的操作系统放在一个服务器上，让客户端去下载。但是客户端放在哪里呢？它怎么知道去哪个服务器上下载呢？客户端总得安装在一个操作系统上呀，可是这个客户端本来就是用来安装操作系统的呀？ 其实，这个过程和操作系统启动的过程有点儿像。首先，启动 BIOS。这是一个特别小的小系统，只能干特别小的一件事情。其实就是读取硬盘的 MBR 启动扇区，将 GRUB 启动起来；然后将权力交给 GRUB，GRUB 加载内核、加载作为根文件系统的 initramfs 文件；然后将权力交给内核；最后内核启动，初始化整个操作系统。 那我们安装操作系统的过程，只能插在 BIOS 启动之后了。因为没安装系统之前，连启动扇区都没有。因而这个过程叫做预启动执行环境（Pre-boot Execution Environment），简称 PXE。 PXE 协议分为客户端和服务器端，由于还没有操作系统，只能先把客户端放在 BIOS 里面。当计算机启动时，BIOS 把 PXE 客户端调入内存里面，就可以连接到服务端做一些操作了。 首先，PXE 客户端自己也需要有个 IP 地址。因为 PXE 的客户端启动起来，就可以发送一个 DHCP 的请求，让 DHCP Server 给它分配一个地址。PXE 客户端有了自己的地址，那它怎么知道 PXE 服务器在哪里呢？对于其他的协议，都好办，要有人告诉他。例如，告诉浏览器要访问的 IP 地址，或者在配置中告诉它；例如，微服务之间的相互调用。但是 PXE 客户端启动的时候，啥都没有。好在 DHCP Server 除了分配 IP 地址以外，还可以做一些其他的事情。这里有一个 DHCP Server 的一个样例配置： ddns-update-style interim; ignore client-updates; allow booting; allow bootp; subnet 192.168.1.0 netmask 255.255.255.0 { option routers 192.168.1.1; option subnet-mask 255.255.255.0; option time-offset -18000; default-lease-time 21600; max-lease-time 43200; range dynamic-bootp 192.168.1.240 192.168.1.250; filename \"pxelinux.0\"; next-server 192.168.1.180; } 按照上面的原理，默认的 DHCP Server 是需要配置的，无非是我们配置 IP 的时候所需要的 IP 地址段、子网掩码、网关地址、租期等。如果想使用 PXE，则需要配置 next-server，指向 PXE 服务器的地址，另外要配置初始启动文件 filename。这样 PXE 客户端启动之后，发送 DHCP 请求之后，除了能得到一个 IP 地址，还可以知道 PXE 服务器在哪里，也可以知道如何从 PXE 服务器上下载某个文件，去初始化操作系统。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:7:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"解析 PXE 的工作过程 接下来我们来详细看一下 PXE 的工作过程。 首先，启动 PXE 客户端。第一步是通过 DHCP 协议告诉 DHCP Server，我刚来，一穷二白，啥都没有。DHCP Server 便租给它一个 IP 地址，同时也给它 PXE 服务器的地址、启动文件 pxelinux.0。 其次，PXE 客户端知道要去 PXE 服务器下载这个文件后，就可以初始化机器。于是便开始下载，下载的时候使用的是 TFTP 协议。所以 PXE 服务器上，往往还需要有一个 TFTP 服务器。PXE 客户端向 TFTP 服务器请求下载这个文件，TFTP 服务器说好啊，于是就将这个文件传给它。 然后，PXE 客户端收到这个文件后，就开始执行这个文件。这个文件会指示 PXE 客户端，向 TFTP 服务器请求计算机的配置信息 pxelinux.cfg。TFTP 服务器会给 PXE 客户端一个配置文件，里面会说内核在哪里、initramfs 在哪里。PXE 客户端会请求这些文件。 最后，启动 Linux 内核。一旦启动了操作系统，以后就啥都好办了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:7:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了。我来总结一下今天的内容：DHCP 协议主要是用来给客户租用 IP 地址，和房产中介很像，要商谈、签约、续租，广播还不能“抢单”；DHCP 协议能给客户推荐“装修队”PXE，能够安装操作系统，这个在云计算领域大有用处。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:7:7","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"从第二层到第三层 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:8:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第5讲 | 从物理层到MAC层：如何在宿舍里自己组网玩联机游戏？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:9:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第一层（物理层） 物理层能折腾啥？现在的同学可能想不到，我们当时去学校配电脑的地方买网线，卖网线的师傅都会问，你的网线是要电脑连电脑啊，还是电脑连网口啊？我们要的是电脑连电脑。这种方式就是一根网线，有两个头。一头插在一台电脑的网卡上，另一头插在另一台电脑的网卡上。但是在当时，普通的网线这样是通不了的，所以水晶头要做交叉线，用的就是所谓的 1－3、2－6 交叉接法。水晶头的第 1、2 和第 3、6 脚，它们分别起着收、发信号的作用。将一端的 1 号和 3 号线、2 号和 6 号线互换一下位置，就能够在物理层实现一端发送的信号，另一端能收到。当然电脑连电脑，除了网线要交叉，还需要配置这两台电脑的 IP 地址、子网掩码和默认网关。这三个概念上一节详细描述过了。要想两台电脑能够通信，这三项必须配置成为一个网络，可以一个是 192.168.0.1/24，另一个是 192.168.0.2/24，否则是不通的。这里我想问你一个问题，两台电脑之间的网络包，包含 MAC 层吗？当然包含，要完整。IP 层要封装了 MAC 层才能将包放入物理层。到此为止，两台电脑已经构成了一个最小的局域网，也即 LAN。可以玩联机局域网游戏啦！ 等到第三个哥们也买了一台电脑，怎么把三台电脑连在一起呢？先别说交换机，当时交换机也贵。有一个叫做 Hub 的东西，也就是集线器。这种设备有多个口，可以将宿舍里的多台电脑连接起来。但是，和交换机不同，集线器没有大脑，它完全在物理层工作。它会将自己收到的每一个字节，都复制到其他端口上去。这是第一层物理层联通的方案。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:9:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第二层（数据链路层） 你可能已经发现问题了。Hub 采取的是广播的模式，如果每一台电脑发出的包，宿舍的每个电脑都能收到，那就麻烦了。这就需要解决几个问题：这个包是发给谁的？谁应该接收？大家都在发，会不会产生混乱？有没有谁先发、谁后发的规则？如果发送的时候出现了错误，怎么办？ 这几个问题，都是第二层，数据链路层，也即 MAC 层要解决的问题。MAC 的全称是 Medium Access Control，即媒体访问控制。控制什么呢？其实就是控制在往媒体上发数据的时候，谁先发、谁后发的问题。防止发生混乱。这解决的是第二个问题。这个问题中的规则，学名叫多路访问。有很多算法可以解决这个问题。就像车管所管束马路上跑的车，能想的办法都想过了。 比如接下来这三种方式：方式一：分多个车道。每个车一个车道，你走你的，我走我的。这在计算机网络里叫作信道划分；方式二：今天单号出行，明天双号出行，轮着来。这在计算机网络里叫作轮流协议；方式三：不管三七二十一，有事儿先出门，发现特堵，就回去。错过高峰再出。我们叫作随机接入协议。著名的以太网，用的就是这个方式。 解决了第二个问题，就是解决了媒体接入控制的问题，MAC 的问题也就解决好了。这和 MAC 地址没什么关系。接下来要解决第一个问题：发给谁，谁接收？这里用到一个物理地址，叫作链路层地址。但是因为第二层主要解决媒体接入控制的问题，所以它常被称为MAC 地址。解决第一个问题就牵扯到第二层的网络包格式。对于以太网，第二层的最开始，就是目标的 MAC 地址和源的 MAC 地址。 接下来是类型，大部分的类型是 IP 数据包，然后 IP 里面包含 TCP、UDP，以及 HTTP 等，这都是里层封装的事情。有了这个目标 MAC 地址，数据包在链路上广播，MAC 的网卡才能发现，这个包是给它的。MAC 的网卡把包收进来，然后打开 IP 包，发现 IP 地址也是自己的，再打开 TCP 包，发现端口是自己，也就是 80，而 nginx 就是监听 80。于是将请求提交给 nginx，nginx 返回一个网页。然后将网页需要发回请求的机器。然后层层封装，最后到 MAC 层。因为来的时候有源 MAC 地址，返回的时候，源 MAC 就变成了目标 MAC，再返给请求的机器。对于以太网，第二层的最后面是 CRC，也就是循环冗余检测。通过 XOR 异或的算法，来计算整个包是否在发送的过程中出现了错误，主要解决第三个问题。这里还有一个没有解决的问题，当源机器知道目标机器的时候，可以将目标地址放入包里面，如果不知道呢？一个广播的网络里面接入了 N 台机器，我怎么知道每个 MAC 地址是谁呢？这就是 ARP 协议，也就是已知 IP 地址，求 MAC 地址的协议。 在一个局域网里面，当知道了 IP 地址，不知道 MAC 怎么办呢？靠“吼”。 广而告之，发送一个广播包，谁是这个 IP 谁来回答。具体询问和回答的报文就像下面这样： 为了避免每次都用 ARP 请求，机器本地也会进行 ARP 缓存。当然机器会不断地上线下线，IP 也可能会变，所以 ARP 的 MAC 地址缓存过一段时间就会过期。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:9:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"局域网 好了，至此我们宿舍四个电脑就组成了一个局域网。用 Hub 连接起来，就可以玩局域网版的《魔兽争霸》了。 打开游戏，进入“局域网选项”，选择一张地图，点击“创建游戏”，就可以进入这张地图的房间中。等同一个局域网里的其他小伙伴加入后，游戏就可以开始了。这种组网的方法，对一个宿舍来说没有问题，但是一旦机器数目增多，问题就出现了。因为 Hub 是广播的，不管某个接口是否需要，所有的 Bit 都会被发送出去，然后让主机来判断是不是需要。这种方式路上的车少就没问题，车一多，产生冲突的概率就提高了。而且把不需要的包转发过去，纯属浪费。看来 Hub 这种不管三七二十一都转发的设备是不行了，需要点儿智能的。因为每个口都只连接一台电脑，这台电脑又不怎么换 IP 和 MAC 地址，只要记住这台电脑的 MAC 地址，如果目标 MAC 地址不是这台电脑的，这个口就不用转发了。 谁能知道目标 MAC 地址是否就是连接某个口的电脑的 MAC 地址呢？这就需要一个能把 MAC 头拿下来，检查一下目标 MAC 地址，然后根据策略转发的设备，按第二节课中讲过的，这个设备显然是个二层设备，我们称为交换机。交换机怎么知道每个口的电脑的 MAC 地址呢？这需要交换机会学习。 一台 MAC1 电脑将一个包发送给另一台 MAC2 电脑，当这个包到达交换机的时候，一开始交换机也不知道 MAC2 的电脑在哪个口，所以没办法，它只能将包转发给除了来的那个口之外的其他所有的口。但是，这个时候，交换机会干一件非常聪明的事情，就是交换机会记住，MAC1 是来自一个明确的口。以后有包的目的地址是 MAC1 的，直接发送到这个口就可以了。 当交换机作为一个关卡一样，过了一段时间之后，就有了整个网络的一个结构了，这个时候，基本上不用广播了，全部可以准确转发。当然，每个机器的 IP 地址会变，所在的口也会变，因而交换机上的学习的结果，我们称为转发表，是有一个过期时间的。 有了交换机，一般来说，你接个几十台、上百台机器打游戏，应该没啥问题。你可以组个战队了。能上网了，就可以玩网游了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:9:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，今天的内容差不多了，我们来总结一下，有三个重点需要你记住：第一，MAC 层是用来解决多路访问的堵车问题的；第二，ARP 是通过吼的方式来寻找目标 MAC 地址的，吼完之后记住一段时间，这个叫作缓存；第三，交换机是有 MAC 地址学习能力的，学完了它就知道谁在哪儿了，不用广播了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:9:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第6讲 | 交换机与VLAN：办公室太复杂，我要回学校 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"拓扑结构是怎么形成的？ 我们常见到的办公室大多是一排排的桌子，每个桌子都有网口，一排十几个座位就有十几个网口，一个楼层就会有几十个甚至上百个网口。如果算上所有楼层，这个场景自然比你宿舍里的复杂多了。具体哪里复杂呢？我来给你具体讲解。 首先，这个时候，一个交换机肯定不够用，需要多台交换机，交换机之间连接起来，就形成一个稍微复杂的拓扑结构。我们先来看两台交换机的情形。两台交换机连接着三个局域网，每个局域网上都有多台机器。如果机器 1 只知道机器 4 的 IP 地址，当它想要访问机器 4，把包发出去的时候，它必须要知道机器 4 的 MAC 地址。 于是机器 1 发起广播，机器 2 收到这个广播，但是这不是找它的，所以没它什么事。交换机 A 一开始是不知道任何拓扑信息的，在它收到这个广播后，采取的策略是，除了广播包来的方向外，它还要转发给其他所有的网口。于是机器 3 也收到广播信息了，但是这和它也没什么关系。当然，交换机 B 也是能够收到广播信息的，但是这时候它也是不知道任何拓扑信息的，因而也是进行广播的策略，将包转发到局域网三。这个时候，机器 4 和机器 5 都收到了广播信息。机器 4 主动响应说，这是找我的，这是我的 MAC 地址。于是一个 ARP 请求就成功完成了。在上面的过程中，交换机 A 和交换机 B 都是能够学习到这样的信息：机器 1 是在左边这个网口的。当了解到这些拓扑信息之后，情况就好转起来。当机器 2 要访问机器 1 的时候，机器 2 并不知道机器 1 的 MAC 地址，所以机器 2 会发起一个 ARP 请求。这个广播消息会到达机器 1，也同时会到达交换机 A。这个时候交换机 A 已经知道机器 1 是不可能在右边的网口的，所以这个广播信息就不会广播到局域网二和局域网三。当机器 3 要访问机器 1 的时候，也需要发起一个广播的 ARP 请求。这个时候交换机 A 和交换机 B 都能够收到这个广播请求。交换机 A 当然知道主机 A 是在左边这个网口的，所以会把广播消息转发到局域网一。同时，交换机 B 收到这个广播消息之后，由于它知道机器 1 是不在右边这个网口的，所以不会将消息广播到局域网三。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何解决常见的环路问题？ 这样看起来，两台交换机工作得非常好。随着办公室越来越大，交换机数目肯定越来越多。当整个拓扑结构复杂了，这么多网线，绕过来绕过去，不可避免地会出现一些意料不到的情况。其中常见的问题就是环路问题。例如这个图，当两个交换机将两个局域网同时连接起来的时候。你可能会觉得，这样反而有了高可用性。但是却不幸地出现了环路。出现了环路会有什么结果呢？ 我们来想象一下机器 1 访问机器 2 的过程。一开始，机器 1 并不知道机器 2 的 MAC 地址，所以它需要发起一个 ARP 的广播。广播到达机器 2，机器 2 会把 MAC 地址返回来，看起来没有这两个交换机什么事情。但是问题来了，这两个交换机还是都能够收到广播包的。交换机 A 一开始是不知道机器 2 在哪个局域网的，所以它会把广播消息放到局域网二，在局域网二广播的时候，交换机 B 右边这个网口也是能够收到广播消息的。交换机 B 会将这个广播信息发送到局域网一。局域网一的这个广播消息，又会到达交换机 A 左边的这个接口。交换机 A 这个时候还是不知道机器 2 在哪个局域网，于是将广播包又转发到局域网二。左转左转左转，好像是个圈哦。可能有人会说，当两台交换机都能够逐渐学习到拓扑结构之后，是不是就可以了？ 别想了，压根儿学不会的。机器 1 的广播包到达交换机 A 和交换机 B 的时候，本来两个交换机都学会了机器 1 是在局域网一的，但是当交换机 A 将包广播到局域网二之后，交换机 B 右边的网口收到了来自交换机 A 的广播包。根据学习机制，这彻底损坏了交换机 B 的三观，刚才机器 1 还在左边的网口呢，怎么又出现在右边的网口呢？哦，那肯定是机器 1 换位置了，于是就误会了，交换机 B 就学会了，机器 1 是从右边这个网口来的，把刚才学习的那一条清理掉。同理，交换机 A 右边的网口，也能收到交换机 B 转发过来的广播包，同样也误会了，于是也学会了，机器 1 从右边的网口来，不是从左边的网口来。然而当广播包从左边的局域网一广播的时候，两个交换机再次刷新三观，原来机器 1 是在左边的，过一会儿，又发现不对，是在右边的，过一会，又发现不对，是在左边的。这还是一个包转来转去，每台机器都会发广播包，交换机转发也会复制广播包，当广播包越来越多的时候，按照上一节讲过一个共享道路的算法，也就是路会越来越堵，最后谁也别想走。所以，必须有一个方法解决环路的问题，怎么破除环路呢？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"STP 协议中那些难以理解的概念 在数据结构中，有一个方法叫做最小生成树。有环的我们常称为图。将图中的环破了，就生成了树。在计算机网络中，生成树的算法叫作 STP，全称 Spanning Tree Protocol。STP 协议比较复杂，一开始很难看懂，但是其实这是一场血雨腥风的武林比武或者华山论剑，最终决出五岳盟主的方式。 在 STP 协议里面有很多概念，译名就非常拗口，但是我一作比喻，你很容易就明白了。Root Bridge，也就是根交换机。这个比较容易理解，可以比喻为“掌门”交换机，是某棵树的老大，是掌门，最大的大哥。Designated Bridges，有的翻译为指定交换机。这个比较难理解，可以想像成一个“小弟”，对于树来说，就是一棵树的树枝。所谓“指定”的意思是，我拜谁做大哥，其他交换机通过这个交换机到达根交换机，也就相当于拜他做了大哥。这里注意是树枝，不是叶子，因为叶子往往是主机。Bridge Protocol Data Units （BPDU） ，网桥协议数据单元。可以比喻为“相互比较实力”的协议。行走江湖，比的就是武功，拼的就是实力。当两个交换机碰见的时候，也就是相连的时候，就需要互相比一比内力了。BPDU 只有掌门能发，已经隶属于某个掌门的交换机只能传达掌门的指示。Priority Vector，优先级向量。可以比喻为实力 （值越小越牛）。实力是啥？就是一组 ID 数目，[Root Bridge ID, Root Path Cost, Bridge ID, and Port ID]。为什么这样设计呢？这是因为要看怎么来比实力。先看 Root Bridge ID。拿出老大的 ID 看看，发现掌门一样，那就是师兄弟；再比 Root Path Cost，也即我距离我的老大的距离，也就是拿和掌门关系比，看同一个门派内谁和老大关系铁；最后比 Bridge ID，比我自己的 ID，拿自己的本事比。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"STP 的工作过程是怎样的？ 接下来，我们来看 STP 的工作过程。一开始，江湖纷争，异常混乱。大家都觉得自己是掌门，谁也不服谁。于是，所有的交换机都认为自己是掌门，每个网桥都被分配了一个 ID。这个 ID 里有管理员分配的优先级，当然网络管理员知道哪些交换机贵，哪些交换机好，就会给它们分配高的优先级。这种交换机生下来武功就很高，起步就是乔峰。 既然都是掌门，互相都连着网线，就互相发送 BPDU 来比功夫呗。这一比就发现，有人是岳不群，有人是封不平，赢的接着当掌门，输的就只好做小弟了。当掌门的还会继续发 BPDU，而输的人就没有机会了。它们只有在收到掌门发的 BPDU 的时候，转发一下，表示服从命令。 数字表示优先级。就像这个图，5 和 6 碰见了，6 的优先级低，所以乖乖做小弟。于是一个小门派形成，5 是掌门，6 是小弟。其他诸如 1-7、2-8、3-4 这样的小门派，也诞生了。于是江湖出现了很多小的门派，小的门派，接着合并。合并的过程会出现以下四种情形，我分别来介绍。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"情形一：掌门遇到掌门 当 5 碰到了 1，掌门碰见掌门，1 觉得自己是掌门，5 也刚刚跟别人 PK 完成为掌门。这俩掌门比较功夫，最终 1 胜出。于是输掉的掌门 5 就会率领所有的小弟归顺。结果就是 1 成为大掌门。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"情形二：同门相遇 同门相遇可以是掌门与自己的小弟相遇，这说明存在“环”了。这个小弟已经通过其他门路拜在你门下，结果你还不认识，就 PK 了一把。结果掌门发现这个小弟功夫不错，不应该级别这么低，就把它招到门下亲自带，那这个小弟就相当于升职了。我们再来看，假如 1 和 6 相遇。6 原来就拜在 1 的门下，只不过 6 的上司是 5，5 的上司是 1。1 发现，6 距离我才只有 2，比从 5 这里过来的 5（=4+1）近多了，那 6 就直接汇报给我吧。于是，5 和 6 分别汇报给 1。 同门相遇还可以是小弟相遇。这个时候就要比较谁和掌门的关系近，当然近的当大哥。刚才 5 和 6 同时汇报给 1 了，后来 5 和 6 在比较功夫的时候发现，5 你直接汇报给 1 距离是 4，如果 5 汇报给 6 再汇报给 1，距离只有 2+1=3，所以 5 干脆拜 6 为上司。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"情形三：掌门与其他帮派小弟相遇 小弟拿本帮掌门和这个掌门比较，赢了，这个掌门拜入门来。输了，会拜入新掌门，并且逐渐拉拢和自己连接的兄弟，一起弃暗投明。 例如，2 和 7 相遇，虽然 7 是小弟，2 是掌门。就个人武功而言，2 比 7 强，但是 7 的掌门是 1，比 2 牛，所以没办法，2 要拜入 7 的门派，并且连同自己的小弟都一起拜入。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:7","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"情形四：不同门小弟相遇 各自拿掌门比较，输了的拜入赢的门派，并且逐渐将与自己连接的兄弟弃暗投明。 例如，5 和 4 相遇。虽然 4 的武功好于 5，但是 5 的掌门是 1，比 4 牛，于是 4 拜入 5 的门派。后来当 3 和 4 相遇的时候，3 发现 4 已经叛变了，4 说我现在老大是 1，比你牛，要不你也来吧，于是 3 也拜入 1。最终，生成一棵树，武林一统，天下太平。但是天下大势，分久必合，合久必分，天下统一久了，也会有相应的问题。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:8","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何解决广播问题和安全问题？ 毕竟机器多了，交换机也多了，就算交换机比 Hub 智能一些，但是还是难免有广播的问题，一大波机器，相关的部门、不相关的部门，广播一大堆，性能就下来了。就像一家公司，创业的时候，一二十个人，坐在一个会议室，有事情大家讨论一下，非常方便。但是如果变成了 50 个人，全在一个会议室里面吵吵，就会乱得不得了。你们公司有不同的部门，有的部门需要保密的，比如人事部门，肯定要讨论升职加薪的事儿。由于在同一个广播域里面，很多包都会在一个局域网里面飘啊飘，碰到了一个会抓包的程序员，就能抓到这些包，如果没有加密，就能看到这些敏感信息了。还是上面的例子，50 个人在一个会议室里面七嘴八舌地讨论，其中有两个 HR，那他们讨论的问题，肯定被其他人偷偷听走了。 那咋办，分部门，分会议室呗。那我们就来看看怎么分。有两种分的方法，一个是物理隔离。每个部门设一个单独的会议室，对应到网络方面，就是每个部门有单独的交换机，配置单独的子网，这样部门之间的沟通就需要路由器了。路由器咱们还没讲到，以后再说。这样的问题在于，有的部门人多，有的部门人少。人少的部门慢慢人会变多，人多的部门也可能人越变越少。如果每个部门有单独的交换机，口多了浪费，少了又不够用。另外一种方式是虚拟隔离，就是用我们常说的 VLAN，或者叫虚拟局域网。使用 VLAN，一个交换机上会连属于多个局域网的机器，那交换机怎么区分哪个机器属于哪个局域网呢？ 我们只需要在原来的二层的头上加一个 TAG，里面有一个 VLAN ID，一共 12 位。为什么是 12 位呢？因为 12 位可以划分 4096 个 VLAN。这样是不是还不够啊。现在的情况证明，目前云计算厂商里面绝对不止 4096 个用户。当然每个用户需要一个 VLAN 了啊，怎么办呢，这个我们在后面的章节再说。如果我们买的交换机是支持 VLAN 的，当这个交换机把二层的头取下来的时候，就能够识别这个 VLAN ID。这样只有相同 VLAN 的包，才会互相转发，不同 VLAN 的包，是看不到的。这样广播问题和安全问题就都能够解决了。 我们可以设置交换机每个口所属的 VLAN。如果某个口坐的是程序员，他们属于 VLAN 10；如果某个口坐的是人事，他们属于 VLAN 20；如果某个口坐的是财务，他们属于 VLAN 30。这样，财务发的包，交换机只会转发到 VLAN 30 的口上。程序员啊，你就监听 VLAN 10 吧，里面除了代码，啥都没有。而且对于交换机来讲，每个 VLAN 的口都是可以重新设置的。一个财务走了，把他所在座位的口从 VLAN 30 移除掉，来了一个程序员，坐在财务的位置上，就把这个口设置为 VLAN 10，十分灵活。有人会问交换机之间怎么连接呢？将两个交换机连接起来的口应该设置成什么 VLAN 呢？对于支持 VLAN 的交换机，有一种口叫作 Trunk 口。它可以转发属于任何 VLAN 的口。交换机之间可以通过这种口相互连接。好了，解决这么多交换机连接在一起的问题，办公室的问题似乎搞定了。然而这只是一般复杂的场景，因为你能接触到的网络，到目前为止，不管是你的台式机，还是笔记本所连接的网络，对于带宽、高可用等都要求不高。就算出了问题，一会儿上不了网，也不会有什么大事。我们在宿舍、学校或者办公室，经常会访问一些网站，这些网站似乎永远不会“挂掉”。那是因为这些网站都生活在一个叫做数据中心的地方，那里的网络世界更加复杂。在后面的章节，我会为你详细讲解。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:9","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这节就到这里，我们这里来总结一下：当交换机的数目越来越多的时候，会遭遇环路问题，让网络包迷路，这就需要使用 STP 协议，通过华山论剑比武的方式，将有环路的图变成没有环路的树，从而解决环路问题。交换机数目多会面临隔离问题，可以通过 VLAN 形成虚拟局域网，从而解决广播问题和安全问题。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:10:10","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第7讲 | ICMP与ping：投石问路的侦察兵 无论是在宿舍，还是在办公室，或者运维一个数据中心，我们常常会遇到网络不通的问题。那台机器明明就在那里，你甚至都可以通过机器的终端连上去看。它看着好好的，可是就是连不上去，究竟是哪里出了问题呢？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:11:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"ICMP 协议的格式 一般情况下，你会想到 ping 一下。那你知道 ping 是如何工作的吗？ping 是基于 ICMP 协议工作的。ICMP 全称 Internet Control Message Protocol，就是互联网控制报文协议。这里面的关键词是“控制”，那具体是怎么控制的呢？网络包在异常复杂的网络环境中传输时，常常会遇到各种各样的问题。当遇到问题的时候，总不能“死个不明不白”，要传出消息来，报告情况，这样才可以调整传输策略。这就相当于我们经常看到的电视剧里，古代行军的时候，为将为帅者需要通过侦察兵、哨探或传令兵等人肉的方式来掌握情况，控制整个战局。ICMP 报文是封装在 IP 包里面的。因为传输指令的时候，肯定需要源地址和目标地址。它本身非常简单。因为作为侦查兵，要轻装上阵，不能携带大量的包袱。 ICMP 报文有很多的类型，不同的类型有不同的代码。最常用的类型是主动请求为 8，主动请求的应答为 0。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:11:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"查询报文类型 我们经常在电视剧里听到这样的话：主帅说，来人哪！前方战事如何，快去派人打探，一有情况，立即通报！这种是主帅发起的，主动查看敌情，对应 ICMP 的查询报文类型。例如，常用的 ping 就是查询报文，是一种主动请求，并且获得主动应答的 ICMP 协议。所以，ping 发的包也是符合 ICMP 协议格式的，只不过它在后面增加了自己的格式。对 ping 的主动请求，进行网络抓包，称为 ICMP ECHO REQUEST。同理主动请求的回复，称为ICMP ECHO REPLY。比起原生的 ICMP，这里面多了两个字段，一个是标识符。这个很好理解，你派出去两队侦查兵，一队是侦查战况的，一队是去查找水源的，要有个标识才能区分。另一个是序号，你派出去的侦查兵，都要编个号。如果派出去 10 个，回来 10 个，就说明前方战况不错；如果派出去 10 个，回来 2 个，说明情况可能不妙。在选项数据中，ping 还会存放发送请求的时间值，来计算往返时间，说明路程的长短。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:11:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"差错报文类型 当然也有另外一种方式，就是差错报文。主帅骑马走着走着，突然来了一匹快马，上面的小兵气喘吁吁的：报告主公，不好啦！张将军遭遇埋伏，全军覆没啦！这种是异常情况发起的，来报告发生了不好的事情，对应 ICMP 的差错报文类型。我举几个 ICMP 差错报文的例子：终点不可达为 3，源抑制为 4，超时为 11，重定向为 5。这些都是什么意思呢？我给你具体解释一下。第一种是终点不可达。小兵：报告主公，您让把粮草送到张将军那里，结果没有送到。如果你是主公，你肯定会问，为啥送不到？具体的原因在代码中表示就是，网络不可达代码为 0，主机不可达代码为 1，协议不可达代码为 2，端口不可达代码为 3，需要进行分片但设置了不分片位代码为 4。具体的场景就像这样： 网络不可达：主公，找不到地方呀？主机不可达：主公，找到地方没这个人呀？协议不可达：主公，找到地方，找到人，口号没对上，人家天王盖地虎，我说 12345！端口不可达：主公，找到地方，找到人，对了口号，事儿没对上，我去送粮草，人家说他们在等救兵。需要进行分片但设置了不分片位：主公，走到一半，山路狭窄，想换小车，但是您的将令，严禁换小车，就没办法送到了。 第二种是源站抑制，也就是让源站放慢发送速度。小兵：报告主公，您粮草送的太多了吃不完。第三种是时间超时，也就是超过网络包的生存时间还是没到。小兵：报告主公，送粮草的人，自己把粮草吃完了，还没找到地方，已经饿死啦。第四种是路由重定向，也就是让下次发给另一个路由器。小兵：报告主公，上次送粮草的人本来只要走一站地铁，非得从五环绕，下次别这样了啊。 差错报文的结构相对复杂一些。除了前面还是 IP，ICMP 的前 8 字节不变，后面则跟上出错的那个 IP 包的 IP 头和 IP 正文的前 8 个字节。而且这类侦查兵特别恪尽职守，不但自己返回来报信，还把一部分遗物也带回来。 侦察兵：报告主公，张将军已经战死沙场，这是张将军的印信和佩剑。主公：神马？张将军是怎么死的（可以查看 ICMP 的前 8 字节）？没错，这是张将军的剑，是他的剑（IP 数据包的头及正文前 8 字节）。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:11:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"ping：查询报文类型的使用 接下来，我们重点来看 ping 的发送和接收过程。 假定主机 A 的 IP 地址是 192.168.1.1，主机 B 的 IP 地址是 192.168.1.2，它们都在同一个子网。那当你在主机 A 上运行“ping 192.168.1.2”后，会发生什么呢?ping 命令执行的时候，源主机首先会构建一个 ICMP 请求数据包，ICMP 数据包内包含多个字段。最重要的是两个，第一个是类型字段，对于请求数据包而言该字段为 8；另外一个是顺序号，主要用于区分连续 ping 的时候发出的多个数据包。每发出一个请求数据包，顺序号会自动加 1。为了能够计算往返时间 RTT，它会在报文的数据部分插入发送时间。然后，由 ICMP 协议将这个数据包连同地址 192.168.1.2 一起交给 IP 层。IP 层将以 192.168.1.2 作为目的地址，本机 IP 地址作为源地址，加上一些其他控制信息，构建一个 IP 数据包。接下来，需要加入 MAC 头。如果在本节 ARP 映射表中查找出 IP 地址 192.168.1.2 所对应的 MAC 地址，则可以直接使用；如果没有，则需要发送 ARP 协议查询 MAC 地址，获得 MAC 地址后，由数据链路层构建一个数据帧，目的地址是 IP 层传过来的 MAC 地址，源地址则是本机的 MAC 地址；还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送出去。主机 B 收到这个数据帧后，先检查它的目的 MAC 地址，并和本机的 MAC 地址对比，如符合，则接收，否则就丢弃。接收后检查该数据帧，将 IP 数据包从帧中提取出来，交给本机的 IP 层。同样，IP 层检查后，将有用的信息提取后交给 ICMP 协议。主机 B 会构建一个 ICMP 应答包，应答数据包的类型字段为 0，顺序号为接收到的请求数据包中的顺序号，然后再发送出去给主机 A。 在规定的时候间内，源主机如果没有接到 ICMP 的应答包，则说明目标主机不可达；如果接收到了 ICMP 应答包，则说明目标主机可达。此时，源主机会检查，用当前时刻减去该数据包最初从源主机上发出的时刻，就是 ICMP 数据包的时间延迟。当然这只是最简单的，同一个局域网里面的情况。如果跨网段的话，还会涉及网关的转发、路由器的转发等等。但是对于 ICMP 的头来讲，是没什么影响的。会影响的是根据目标 IP 地址，选择路由的下一跳，还有每经过一个路由器到达一个新的局域网，需要换 MAC 头里面的 MAC 地址。这个过程后面几节会详细描述，这里暂时不多说。如果在自己的可控范围之内，当遇到网络不通的问题的时候，除了直接 ping 目标的 IP 地址之外，还应该有一个清晰的网络拓扑图。并且从理论上来讲，应该要清楚地知道一个网络包从源地址到目标地址都需要经过哪些设备，然后逐个 ping 中间的这些设备或者机器。如果可能的话，在这些关键点，通过 tcpdump -i eth0 icmp，查看包有没有到达某个点，回复的包到达了哪个点，可以更加容易推断出错的位置。经常会遇到一个问题，如果不在我们的控制范围内，很多中间设备都是禁止 ping 的，但是 ping 不通不代表网络不通。这个时候就要使用 telnet，通过其他协议来测试网络是否通，这个就不在本篇的讲述范围了。说了这么多，你应该可以看出 ping 这个程序是使用了 ICMP 里面的 ECHO REQUEST 和 ECHO REPLY 类型的。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:11:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"Traceroute：差错报文类型的使用 那其他的类型呢？是不是只有真正遇到错误的时候，才能收到呢？那也不是，有一个程序 Traceroute，是个“大骗子”。它会使用 ICMP 的规则，故意制造一些能够产生错误的场景。 所以，Traceroute 的第一个作用就是故意设置特殊的 TTL，来追踪去往目的地时沿途经过的路由器。Traceroute 的参数指向某个目的 IP 地址，它会发送一个 UDP 的数据包。将 TTL 设置成 1，也就是说一旦遇到一个路由器或者一个关卡，就表示它“牺牲”了。如果中间的路由器不止一个，当然碰到第一个就“牺牲”。于是，返回一个 ICMP 包，也就是网络差错包，类型是时间超时。那大军前行就带一顿饭，试一试走多远会被饿死，然后找个哨探回来报告，那我就知道大军只带一顿饭能走多远了。接下来，将 TTL 设置为 2。第一关过了，第二关就“牺牲”了，那我就知道第二关有多远。如此反复，直到到达目的主机。这样，Traceroute 就拿到了所有的路由器 IP。当然，有的路由器压根不会回这个 ICMP。这也是 Traceroute 一个公网的地址，看不到中间路由的原因。 怎么知道 UDP 有没有到达目的主机呢？Traceroute 程序会发送一份 UDP 数据报给目的主机，但它会选择一个不可能的值作为 UDP 端口号（大于 30000）。当该数据报到达时，将使目的主机的 UDP 模块产生一份“端口不可达”错误 ICMP 报文。如果数据报没有到达，则可能是超时。这就相当于故意派人去西天如来那里去请一本《道德经》，结果人家信佛不信道，消息就会被打出来。被打的消息传回来，你就知道西天是能够到达的。为什么不去取《心经》呢？因为 UDP 是无连接的。也就是说这人一派出去，你就得不到任何音信。你无法区别到底是半路走丢了，还是真的信佛遁入空门了，只有让人家打出来，你才会得到消息。Traceroute 还有一个作用是故意设置不分片，从而确定路径的 MTU。要做的工作首先是发送分组，并设置“不分片”标志。发送的第一个分组的长度正好与出口 MTU 相等。如果中间遇到窄的关口会被卡住，会发送 ICMP 网络差错包，类型为“需要进行分片但设置了不分片位”。其实，这是人家故意的好吧，每次收到 ICMP“不能分片”差错时就减小分组的长度，直到到达目标主机。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:11:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节内容差不多了，我来总结一下：ICMP 相当于网络世界的侦察兵。我讲了两种类型的 ICMP 报文，一种是主动探查的查询报文，一种异常报告的差错报文；ping 使用查询报文，Traceroute 使用差错报文。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:11:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第8讲 | 世界这么大，我想出网关：欧洲十国游与玄奘西行 前几节，我主要跟你讲了宿舍里和办公室里用到的网络协议。你已经有了一些基础，是时候去外网逛逛了！ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:12:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"怎么在宿舍上网？ 还记得咱们在宿舍的时候买了台交换机，几台机器组了一个局域网打游戏吗？可惜啊，只能打局域网的游戏，不能上网啊！盼啊盼啊，终于盼到大二，允许宿舍开通网络了。学校给每个宿舍的网口分配了一个 IP 地址。这个 IP 是校园网的 IP，完全由网管部门控制。宿舍网的 IP 地址多为 192.168.1.x。校园网的 IP 地址，假设是 10.10.x.x。这个时候，你要在宿舍上网，有两个办法：第一个办法，让你们宿舍长再买一个网卡。这个时候，你们宿舍长的电脑里就有两张网卡。一张网卡的线插到你们宿舍的交换机上，另一张网卡的线插到校园网的网口。而且，这张新的网卡的 IP 地址要按照学校网管部门分配的配置，不然上不了网。这种情况下，如果你们宿舍的人要上网，就需要一直开着宿舍长的电脑。第二个办法，你们共同出钱买个家庭路由器（反正当时我们买不起）。家庭路由器会有内网网口和外网网口。把外网网口的线插到校园网的网口上，将这个外网网口配置成和网管部的一样。内网网口连上你们宿舍的所有的电脑。这种情况下，如果你们宿舍的人要上网，就需要一直开着路由器。 这两种方法其实是一样的。只不过第一种方式，让你的宿舍长的电脑，变成一个有多个口的路由器而已。而你买的家庭路由器，里面也跑着程序，和你宿舍长电脑里的功能一样，只不过是一个嵌入式的系统。当你的宿舍长能够上网之后，接下来，就是其他人的电脑怎么上网的问题。这就需要配置你们的网卡。当然 DHCP 是可以默认配置的。在进行网卡配置的时候，除了 IP 地址，还需要配置一个Gateway 的东西，这个就是网关。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:12:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"你了解 MAC 头和 IP 头的细节吗？ 一旦配置了 IP 地址和网关，往往就能够指定目标地址进行访问了。由于在跨网关访问的时候，牵扯到 MAC 地址和 IP 地址的变化，这里有必要详细描述一下 MAC 头和 IP 头的细节。 在 MAC 头里面，先是目标 MAC 地址，然后是源 MAC 地址，然后有一个协议类型，用来说明里面是 IP 协议。IP 头里面的版本号，目前主流的还是 IPv4，服务类型 TOS 在第三节讲 ip addr 命令的时候讲过，TTL 在第 7 节讲 ICMP 协议的时候讲过。另外，还有 8 位标识协议。这里到了下一层的协议，也就是，是 TCP 还是 UDP。最重要的就是源 IP 和目标 IP。先是源 IP 地址，然后是目标 IP 地址。在任何一台机器上，当要访问另一个 IP 地址的时候，都会先判断，这个目标 IP 地址，和当前机器的 IP 地址，是否在同一个网段。怎么判断同一个网段呢？需要 CIDR 和子网掩码，这个在第三节的时候也讲过了。 如果是同一个网段，例如，你访问你旁边的兄弟的电脑，那就没网关什么事情，直接将源地址和目标地址放入 IP 头中，然后通过 ARP 获得 MAC 地址，将源 MAC 和目的 MAC 放入 MAC 头中，发出去就可以了。如果不是同一网段，例如，你要访问你们校园网里面的 BBS，该怎么办？这就需要发往默认网关 Gateway。Gateway 的地址一定是和源 IP 地址是一个网段的。往往不是第一个，就是第二个。例如 192.168.1.0/24 这个网段，Gateway 往往会是 192.168.1.1/24 或者 192.168.1.2/24。 如何发往默认网关呢？网关不是和源 IP 地址是一个网段的么？这个过程就和发往同一个网段的其他机器是一样的：将源地址和目标 IP 地址放入 IP 头中，通过 ARP 获得网关的 MAC 地址，将源 MAC 和网关的 MAC 放入 MAC 头中，发送出去。网关所在的端口，例如 192.168.1.1/24 将网络包收进来，然后接下来怎么做，就完全看网关的了。网关往往是一个路由器，是一个三层转发的设备。啥叫三层设备？前面也说过了，就是把 MAC 头和 IP 头都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。在你的宿舍里面，网关就是你宿舍长的电脑。一个路由器往往有多个网口，如果是一台服务器做这个事情，则就有多个网卡，其中一个网卡是和源 IP 同网段的。很多情况下，人们把网关就叫做路由器。其实不完全准确，而另一种比喻更加恰当：路由器是一台设备，它有五个网口或者网卡，相当于有五只手，分别连着五个局域网。每只手的 IP 地址都和局域网的 IP 地址相同的网段，每只手都是它握住的那个局域网的网关。任何一个想发往其他局域网的包，都会到达其中一只手，被拿进来，拿下 MAC 头和 IP 头，看看，根据自己的路由算法，选择另一只手，加上 IP 头和 MAC 头，然后扔出去。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:12:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"静态路由是什么？ 这个时候，问题来了，该选择哪一只手？IP 头和 MAC 头加什么内容，哪些变、哪些不变呢？这个问题比较复杂，大致可以分为两类，一个是静态路由，一个是动态路由。动态路由下一节我们详细地讲。这一节我们先说静态路由。静态路由，其实就是在路由器上，配置一条一条规则。这些规则包括：想访问 BBS 站（它肯定有个网段），从 2 号口出去，下一跳是 IP2；想访问教学视频站（它也有个自己的网段），从 3 号口出去，下一跳是 IP3，然后保存在路由器里。每当要选择从哪只手抛出去的时候，就一条一条的匹配规则，找到符合的规则，就按规则中设置的那样，从某个口抛出去，找下一跳 IPX。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:12:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"IP 头和 MAC 头哪些变、哪些不变？ 对于 IP 头和 MAC 头哪些变、哪些不变的问题，可以分两种类型。我把它们称为“欧洲十国游”型和“玄奘西行”型。之前我说过，MAC 地址是一个局域网内才有效的地址。因而，MAC 地址只要过网关，就必定会改变，因为已经换了局域网。两者主要的区别在于 IP 地址是否改变。不改变 IP 地址的网关，我们称为转发网关；改变 IP 地址的网关，我们称为NAT 网关。 “欧洲十国游”型结合这个图，我们先来看“欧洲十国游”型。 服务器 A 要访问服务器 B。首先，服务器 A 会思考，192.168.4.101 和我不是一个网段的，因而需要先发给网关。那网关是谁呢？已经静态配置好了，网关是 192.168.1.1。网关的 MAC 地址是多少呢？发送 ARP 获取网关的 MAC 地址，然后发送包。包的内容是这样的：源 MAC：服务器 A 的 MAC目标 MAC：192.168.1.1 这个网口的 MAC源 IP：192.168.1.101目标 IP：192.168.4.101 包到达 192.168.1.1 这个网口，发现 MAC 一致，将包收进来，开始思考往哪里转发。在路由器 A 中配置了静态路由之后，要想访问 192.168.4.0/24，要从 192.168.56.1 这个口出去，下一跳为 192.168.56.2。于是，路由器 A 思考的时候，匹配上了这条路由，要从 192.168.56.1 这个口发出去，发给 192.168.56.2，那 192.168.56.2 的 MAC 地址是多少呢？路由器 A 发送 ARP 获取 192.168.56.2 的 MAC 地址，然后发送包。包的内容是这样的： 源 MAC：192.168.56.1 的 MAC 地址目标 MAC：192.168.56.2 的 MAC 地址源 IP：192.168.1.101目标 IP：192.168.4.101 包到达 192.168.56.2 这个网口，发现 MAC 一致，将包收进来，开始思考往哪里转发。在路由器 B 中配置了静态路由，要想访问 192.168.4.0/24，要从 192.168.4.1 这个口出去，没有下一跳了。因为我右手这个网卡，就是这个网段的，我是最后一跳了。于是，路由器 B 思考的时候，匹配上了这条路由，要从 192.168.4.1 这个口发出去，发给 192.168.4.101。那 192.168.4.101 的 MAC 地址是多少呢？路由器 B 发送 ARP 获取 192.168.4.101 的 MAC 地址，然后发送包。包的内容是这样的： 源 MAC：192.168.4.1 的 MAC 地址目标 MAC：192.168.4.101 的 MAC 地址源 IP：192.168.1.101目标 IP：192.168.4.101 包到达服务器 B，MAC 地址匹配，将包收进来。通过这个过程可以看出，每到一个新的局域网，MAC 都是要变的，但是 IP 地址都不变。在 IP 头里面，不会保存任何网关的 IP 地址。所谓的下一跳是，某个 IP 要将这个 IP 地址转换为 MAC 放入 MAC 头。之所以将这种模式比喻称为欧洲十国游，是因为在整个过程中，IP 头里面的地址都是不变的。IP 地址在三个局域网都可见，在三个局域网之间的网段都不会冲突。在三个网段之间传输包，IP 头不改变。这就像在欧洲各国之间旅游，一个签证就能搞定。 “玄奘西行”型 我们再来看“玄奘西行”型。这里遇见的第一个问题是，局域网之间没有商量过，各定各的网段，因而 IP 段冲突了。最左面大唐的地址是 192.168.1.101，最右面印度的地址也是 192.168.1.101，如果单从 IP 地址上看，简直是自己访问自己，其实是大唐的 192.168.1.101 要访问印度的 192.168.1.101。 怎么解决这个问题呢？既然局域网之间没有商量过，你们各管各的，那到国际上，也即中间的局域网里面，就需要使用另外的地址。就像出国，不能用咱们自己的身份证，而要改用护照一样，玄奘西游也要拿着专门取经的通关文牒，而不能用自己国家的身份证。首先，目标服务器 B 在国际上要有一个国际的身份，我们给它一个 192.168.56.2。在网关 B 上，我们记下来，国际身份 192.168.56.2 对应国内身份 192.168.1.101。凡是要访问 192.168.56.2，都转成 192.168.1.101。于是，源服务器 A 要访问目标服务器 B，要指定的目标地址为 192.168.56.2。这是它的国际身份。服务器 A 想，192.168.56.2 和我不是一个网段的，因而需要发给网关，网关是谁？已经静态配置好了，网关是 192.168.1.1，网关的 MAC 地址是多少？发送 ARP 获取网关的 MAC 地址，然后发送包。包的内容是这样的： 源 MAC：服务器 A 的 MAC目标 MAC：192.168.1.1 这个网口的 MAC源 IP：192.168.1.101目标 IP：192.168.56.2 包到达 192.168.1.1 这个网口，发现 MAC 一致，将包收进来，开始思考往哪里转发。在路由器 A 中配置了静态路由：要想访问 192.168.56.2/24，要从 192.168.56.1 这个口出去，没有下一跳了，因为我右手这个网卡，就是这个网段的，我是最后一跳了。于是，路由器 A 思考的时候，匹配上了这条路由，要从 192.168.56.1 这个口发出去，发给 192.168.56.2。那 192.168.56.2 的 MAC 地址是多少呢？路由器 A 发送 ARP 获取 192.168.56.2 的 MAC 地址。当网络包发送到中间的局域网的时候，服务器 A 也需要有个国际身份，因而在国际上，源 IP 地址也不能用 192.168.1.101，需要改成 192.168.56.1。发送包的内容是这样的： 源 MAC：192.168.56.1 的 MAC 地址目标 MAC：192.168.56.2 的 MAC 地址源 IP：192.168.56.1目标 IP：192.168.56.2 包到达 192.168.56.2 这个网口，发现 MAC 一致，将包收进来，开始思考往哪里转发。路由器 B 是一个 NAT 网关，它上面配置了，要访问国际身份 192.168.56.2 对应国内身份 192.168.1.101，于是改为访问 192.168.1.101。在路由器 B 中配置了静态路由：要想访问 192.168.1.0/24，要从 192.168.1.1 这个口出去，没有下一跳了，因为我右手这个网卡，就是这个网段的，我是最后一跳了。于是，路由器 B 思考的时候，匹配上了这条路由，要从 192.168.1.1 这个口发出去，发给 192.168.1.101。那 192.168.1.101 的 MAC 地址是多少呢？路由器 B 发送 ARP 获取 192.168.1.101 的 MAC 地址，然后发送包。内容是这样的： 源 MAC：192.168.1.1 的 MAC 地址目标 MAC：192.168.1.101 的 MAC 地址源 IP：192.168.56.1目标 IP：192.168.1.101 包到达服务器 B，MAC 地址匹配，将包收进来。从服务器 B 接收的包可以看出，源 IP 为服务器 A 的国际身份，因而发送返回包的时候，也发给这个国际身份，由路由器 A 做 NAT，转换为国内身份。从这个过程可以看出，IP 地址也会变。这个过程用英文说就是 Network Address Translation，简称 NAT。 其实这第二种方式我们经常见，现在大家每家都有家用路由器，家里的网段都是 192.168.1.x，所以你肯定访问不了你邻居家的这个私网的 IP 地址的。所以，当我们家里的包发出去的时候，都被家用路由器 NAT 成为了运营商的地址了。很多办公室访问外网的时候，也是被 NAT 过的，因为不可能办公室里面的 IP 也是公网可见的，公网地址实在是太贵了，所以一般就是整个办公室共用一个到两个出口 IP 地址。你可以通过 https://www.whatismyip.com/ 查看自己的出口 IP 地址。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:12:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节内容差不多了，我来总结一下：如果离开本局域网，就需要经过网关，网关是路由器的一个网口；路由器是一个三层设备，里面有如何寻找下一跳的规则；经过路由器之后 MAC 头要变，如果 IP 不变，相当于不换护照的欧洲旅游，如果 IP 变，相当于换护照的玄奘西行。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:12:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第9讲 | 路由协议：西出网关无故人，敢问路在何方 俗话说得好，在家千日好，出门一日难。网络包一旦出了网关，就像玄奘西行一样踏上了江湖漂泊的路。上一节我们描述的是一个相对简单的情形。出了网关之后，只有一条路可以走。但是，网络世界复杂得多，一旦出了网关，会面临着很多路由器，有很多条道路可以选。如何选择一个更快速的道路求取真经呢？这里面还有很多门道可以讲。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:13:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何配置路由？ 通过上一节的内容，你应该已经知道，路由器就是一台网络设备，它有多张网卡。当一个入口的网络包送到路由器时，它会根据一个本地的转发信息库，来决定如何正确地转发流量。这个转发信息库通常被称为路由表。一张路由表中会有多条路由规则。每一条规则至少包含这三项信息。 目的网络：这个包想去哪儿？出口设备：将包从哪个口扔出去？下一跳网关：下一个路由器的地址。 通过 route 命令和 ip route 命令都可以进行查询或者配置。例如，我们设置 ip route add 10.176.48.0/20 via 10.173.32.1 dev eth0，就说明要去 10.176.48.0/20 这个目标网络，要从 eth0 端口出去，经过 10.173.32.1。上一节的例子中，网关上的路由策略就是按照这三项配置信息进行配置的。这种配置方式的一个核心思想是：根据目的 IP 地址来配置路由。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:13:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何配置策略路由？ 当然，在真实的复杂的网络环境中，除了可以根据目的 ip 地址配置路由外，还可以根据多个参数来配置路由，这就称为策略路由。可以配置多个路由表，可以根据源 IP 地址、入口设备、TOS 等选择路由表，然后在路由表中查找路由。这样可以使得来自不同来源的包走不同的路由。例如，我们设置： ip rule add from 192.168.1.0/24 table 10 ip rule add from 192.168.2.0/24 table 20 表示从 192.168.1.10/24 这个网段来的，使用 table 10 中的路由表，而从 192.168.2.0/24 网段来的，使用 table20 的路由表。在一条路由规则中，也可以走多条路径。例如，在下面的路由规则中： ip route add default scope global nexthop via 100.100.100.1 weight 1 nexthop via 200.200.200.1 weight 2 下一跳有两个地方，分别是 100.100.100.1 和 200.200.200.1，权重分别为 1 比 2。在什么情况下会用到如此复杂的配置呢？我来举一个现实中的例子。我是房东，家里从运营商那儿拉了两根网线。这两根网线分别属于两个运行商。一个带宽大一些，一个带宽小一些。这个时候，我就不能买普通的家用路由器了，得买个高级点的，可以接两个外网的。家里的网络呢，就是普通的家用网段 192.168.1.x/24。家里有两个租户，分别把线连到路由器上。IP 地址为 192.168.1.101/24 和 192.168.1.102/24，网关都是 192.168.1.1/24，网关在路由器上。 就像上一节说的一样，家里的网段是私有网段，出去的包需要 NAT 成公网的 IP 地址，因而路由器是一个 NAT 路由器。两个运营商都要为这个网关配置一个公网的 IP 地址。如果你去查看你们家路由器里的网段，基本就是我图中画的样子。 运行商里面也有一个 IP 地址，在运营商网络里面的网关。不同的运营商方法不一样，有的是 /32 的，也即一个一对一连接。例如，运营商 1 给路由器分配的地址是 183.134.189.34/32，而运营商网络里面的网关是 183.134.188.1/32。有的是 /30 的，也就是分了一个特别小的网段。运营商 2 给路由器分配的地址是 60.190.27.190/30，运营商网络里面的网关是 60.190.27.189/30。根据这个网络拓扑图，可以将路由配置成这样： $ ip route list table main 60.190.27.189/30 dev eth3 proto kernel scope link src 60.190.27.190 183.134.188.1 dev eth2 proto kernel scope link src 183.134.189.34 192.168.1.0/24 dev eth1 proto kernel scope link src 192.168.1.1 127.0.0.0/8 dev lo scope link default via 183.134.188.1 dev eth2 当路由这样配置的时候，就告诉这个路由器如下的规则：如果去运营商二，就走 eth3；如果去运营商一呢，就走 eth2；如果访问内网，就走 eth1；如果所有的规则都匹配不上，默认走运营商一，也即走快的网络。 但是问题来了，租户 A 不想多付钱，他说我就上上网页，从不看电影，凭什么收我同样贵的网费啊？没关系，咱有技术可以解决。下面我添加一个 Table，名字叫 chao。 # echo 200 chao \u003e\u003e /etc/iproute2/rt_tables 添加一条规则： # ip rule add from 192.168.1.101 table chao # ip rule ls 0: from all lookup local 32765: from 10.0.0.10 lookup chao 32766: from all lookup main 32767: from all lookup default 设定规则为：从 192.168.1.101 来的包都查看个 chao 这个新的路由表。在 chao 路由表中添加规则： # ip route add default via 60.190.27.189 dev eth3 table chao # ip route flush cache 默认的路由走慢的，谁让你不付钱。上面说的都是静态的路由，一般来说网络环境简单的时候，在自己的可控范围之内，自己捣鼓还是可以的。但是有时候网络环境复杂并且多变，如果总是用静态路由，一旦网络结构发生变化，让网络管理员手工修改路由太复杂了，因而需要动态路由算法。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:13:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"动态路由算法 使用动态路由路由器，可以根据路由协议算法生成动态路由表，随网络运行状况的变化而变化。那路由算法是什么样的呢？我们可以想象唐僧西天取经，需要解决两大问题，一个是在每个国家如何找到正确的路，去换通关文牒、吃饭、休息；一个是在国家之间，野外行走的时候，如何找到正确的路、水源的问题。 无论是一个国家内部，还是国家之间，我们都可以将复杂的路径，抽象为一种叫作图的数据结构。至于唐僧西行取经，肯定想走的路越少越好，道路越短越好，因而这就转化成为如何在途中找到最短路径的问题。咱们在大学里面学习计算机网络与数据结构的时候，知道求最短路径常用的有两种方法，一种是 Bellman-Ford 算法，一种是 Dijkstra 算法。在计算机网络中基本也是用这两种方法计算的。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:13:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"1. 距离矢量路由算法 第一大类的算法称为距离矢量路由（distance vector routing）。它是基于 Bellman-Ford 算法的。这种算法的基本思路是，每个路由器都保存一个路由表，包含多行，每行对应网络中的一个路由器，每一行包含两部分信息，一个是要到目标路由器，从那条线出去，另一个是到目标路由器的距离。由此可以看出，每个路由器都是知道全局信息的。那这个信息如何更新呢？每个路由器都知道自己和邻居之间的距离，每过几秒，每个路由器都将自己所知的到达所有的路由器的距离告知邻居，每个路由器也能从邻居那里得到相似的信息。每个路由器根据新收集的信息，计算和其他路由器的距离，比如自己的一个邻居距离目标路由器的距离是 M，而自己距离邻居是 x，则自己距离目标路由器是 x+M。这个算法比较简单，但是还是有问题。 第一个问题就是好消息传得快，坏消息传得慢。 如果有个路由器加入了这个网络，它的邻居就能很快发现它，然后将消息广播出去。要不了多久，整个网络就都知道了。但是一旦一个路由器挂了，挂的消息是没有广播的。当每个路由器发现原来的道路到不了这个路由器的时候，感觉不到它已经挂了，而是试图通过其他的路径访问，直到试过了所有的路径，才发现这个路由器是真的挂了。我再举个例子。 原来的网络包括两个节点，B 和 C。A 加入了网络，它的邻居 B 很快就发现 A 启动起来了。于是它将自己和 A 的距离设为 1，同样 C 也发现 A 起来了，将自己和 A 的距离设置为 2。但是如果 A 挂掉，情况就不妙了。B 本来和 A 是邻居，发现连不上 A 了，但是 C 还是能够连上，只不过距离远了点，是 2，于是将自己的距离设置为 3。殊不知 C 的距离 2 其实是基于原来自己的距离为 1 计算出来的。C 发现自己也连不上 A，并且发现 B 设置为 3，于是自己改成距离 4。依次类推，数越来越大，直到超过一个阈值，我们才能判定 A 真的挂了。这个道理有点像有人走丢了。当你突然发现找不到这个人了。于是你去学校问，是不是在他姨家呀？找到他姨家，他姨说，是不是在他舅舅家呀？他舅舅说，是不是在他姥姥家呀？他姥姥说，是不是在学校呀？总归要问一圈，或者是超过一定的时间，大家才会认为这个人的确走丢了。如果这个人其实只是去见了一个谁都不认识的网友去了，当这个人回来的时候，只要他随便见到其中的一个亲戚，这个亲戚就会拉着他到他的家长那里，说你赶紧回家，你妈都找你一天了。 这种算法的第二个问题是，每次发送的时候，要发送整个全局路由表。网络大了，谁也受不了，所以最早的路由协议 RIP 就是这个算法。它适用于小型网络（小于 15 跳）。当网络规模都小的时候，没有问题。现在一个数据中心内部路由器数目就很多，因而不适用了。所以上面的两个问题，限制了距离矢量路由的网络规模。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:13:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"2. 链路状态路由算法 第二大类算法是链路状态路由（link state routing），基于 Dijkstra 算法。这种算法的基本思路是：当一个路由器启动的时候，首先是发现邻居，向邻居 say hello，邻居都回复。然后计算和邻居的距离，发送一个 echo，要求马上返回，除以二就是距离。然后将自己和邻居之间的链路状态包广播出去，发送到整个网络的每个路由器。这样每个路由器都能够收到它和邻居之间的关系的信息。因而，每个路由器都能在自己本地构建一个完整的图，然后针对这个图使用 Dijkstra 算法，找到两点之间的最短路径。不像距离距离矢量路由协议那样，更新时发送整个路由表。链路状态路由协议只广播更新的或改变的网络拓扑，这使得更新信息更小，节省了带宽和 CPU 利用率。而且一旦一个路由器挂了，它的邻居都会广播这个消息，可以使得坏消息迅速收敛。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:13:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"动态路由协议 1. 基于链路状态路由算法的 OSPF OSPF（Open Shortest Path First，开放式最短路径优先）就是这样一个基于链路状态路由协议，广泛应用在数据中心中的协议。由于主要用在数据中心内部，用于路由决策，因而称为内部网关协议（Interior Gateway Protocol，简称 IGP）。内部网关协议的重点就是找到最短的路径。在一个组织内部，路径最短往往最优。当然有时候 OSPF 可以发现多个最短的路径，可以在这多个路径中进行负载均衡，这常常被称为等价路由。 这一点非常重要。有了等价路由，到一个地方去可以有相同的两个路线，可以分摊流量，还可以当一条路不通的时候，走另外一条路。这个在后面我们讲数据中心的网络的时候，一般应用的接入层会有负载均衡 LVS。它可以和 OSPF 一起，实现高吞吐量的接入层设计。有了内网的路由协议，在一个国家内，唐僧可以想怎么走怎么走了，两条路选一条也行。2. 基于距离矢量路由算法的 BGP 2. 基于距离矢量路由算法的 BGP 但是外网的路由协议，也即国家之间的，又有所不同。我们称为外网路由协议（Border Gateway Protocol，简称 BGP）。在一个国家内部，有路当然选近的走。但是国家之间，不光远近的问题，还有政策的问题。例如，唐僧去西天取经，有的路近。但是路过的国家看不惯僧人，见了僧人就抓。例如灭法国，连光头都要抓。这样的情况即便路近，也最好绕远点走。对于网络包同样，每个数据中心都设置自己的 Policy。例如，哪些外部的 IP 可以让内部知晓，哪些内部的 IP 可以让外部知晓，哪些可以通过，哪些不能通过。这就好比，虽然从我家里到目的地最近，但是不能谁都能从我家走啊！ 在网络世界，这一个个国家成为自治系统 AS（Autonomous System）。自治系统分几种类型。Stub AS：对外只有一个连接。这类 AS 不会传输其他 AS 的包。例如，个人或者小公司的网络。Multihomed AS：可能有多个连接连到其他的 AS，但是大多拒绝帮其他的 AS 传输包。例如一些大公司的网络。Transit AS：有多个连接连到其他的 AS，并且可以帮助其他的 AS 传输包。例如主干网。 每个自治系统都有边界路由器，通过它和外面的世界建立联系。 BGP 又分为两类，eBGP 和 iBGP。自治系统间，边界路由器之间使用 eBGP 广播路由。内部网络也需要访问其他的自治系统。边界路由器如何将 BGP 学习到的路由导入到内部网络呢？就是通过运行 iBGP，使得内部的路由器能够找到到达外网目的地的最好的边界路由器。BGP 协议使用的算法是路径矢量路由协议（path-vector protocol）。它是距离矢量路由协议的升级版。前面说了距离矢量路由协议的缺点。其中一个是收敛慢。在 BGP 里面，除了下一跳 hop 之外，还包括了自治系统 AS 的路径，从而可以避免坏消息传得慢的问题，也即上面所描述的，B 知道 C 原来能够到达 A，是因为通过自己，一旦自己都到达不了 A 了，就不用假设 C 还能到达 A 了。另外，在路径中将一个自治系统看成一个整体，不区分自治系统内部的路由器，这样自治系统的数目是非常有限的。就像大家都能记住出去玩，从中国出发先到韩国然后到日本，只要不计算细到具体哪一站，就算是发送全局信息，也是没有问题的。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:13:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我来做个总结：路由分静态路由和动态路由，静态路由可以配置复杂的策略路由，控制转发策略；动态路由主流算法有两种，距离矢量算法和链路状态算法。基于两种算法产生两种协议，BGP 协议和 OSPF 协议。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:13:7","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"最重要的传输层 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:14:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第10讲 | UDP协议：因性善而简单，难免碰到“城会玩” 讲完了 IP 层以后，接下来我们开始讲传输层。传输层里比较重要的两个协议，一个是 TCP，一个是 UDP。对于不从事底层开发的人员来讲，或者对于开发应用的人来讲，最常用的就是这两个协议。由于面试的时候，这两个协议经常会被放在一起问，因而我在讲的时候，也会结合着来讲。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:15:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"TCP 和 UDP 有哪些区别？ 一般面试的时候我问这两个协议的区别，大部分人会回答，TCP 是面向连接的，UDP 是面向无连接的。什么叫面向连接，什么叫无连接呢？在互通之前，面向连接的协议会先建立连接。例如，TCP 会三次握手，而 UDP 不会。为什么要建立连接呢？你 TCP 三次握手，我 UDP 也可以发三个包玩玩，有什么区别吗？ 所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。例如，TCP 提供可靠交付。通过 TCP 连接传输的数据，无差错、不丢失、不重复、并且按序到达。我们都知道 IP 包是没有任何可靠性保证的，一旦发出去，就像西天取经，走丢了、被妖怪吃了，都只能随它去。但是 TCP 号称能做到那个连接维护的程序做的事情，这个下两节我会详细描述。而 UDP 继承了 IP 包的特性，不保证不丢失，不保证按顺序到达。 再如，TCP 是面向字节流的。发送的时候发的是一个流，没头没尾。IP 包可不是一个流，而是一个个的 IP 包。之所以变成了流，这也是 TCP 自己的状态维护做的事情。而 UDP 继承了 IP 的特性，基于数据报的，一个一个地发，一个一个地收。还有 TCP 是可以有拥塞控制的。它意识到包丢弃了或者网络的环境不好了，就会根据情况调整自己的行为，看看是不是发快了，要不要发慢点。UDP 就不会，应用让我发，我就发，管它洪水滔天。因而 TCP 其实是一个有状态服务，通俗地讲就是有脑子的，里面精确地记着发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点儿都不行。而 UDP 则是无状态服务。通俗地说是没脑子的，天真无邪的，发出去就发出去了。 我们可以这样比喻，如果 MAC 层定义了本地局域网的传输行为，IP 层定义了整个网络端到端的传输行为，这两层基本定义了这样的基因：网络传输是以包为单位的，二层叫帧，网络层叫包，传输层叫段。我们笼统地称为包。包单独传输，自行选路，在不同的设备封装解封装，不保证到达。基于这个基因，生下来的孩子 UDP 完全继承了这些特性，几乎没有自己的思想。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:15:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"UDP 包头是什么样的？ 我们来看一下 UDP 包头。前面章节我已经讲过包的传输过程，这里不再赘述。当我发送的 UDP 包到达目标机器后，发现 MAC 地址匹配，于是就取下来，将剩下的包传给处理 IP 层的代码。把 IP 头取下来，发现目标 IP 匹配，接下来呢？这里面的数据包是给谁呢？ 发送的时候，我知道我发的是一个 UDP 的包，收到的那台机器咋知道的呢？所以在 IP 头里面有个 8 位协议，这里会存放，数据里面到底是 TCP 还是 UDP，当然这里是 UDP。于是，如果我们知道 UDP 头的格式，就能从数据里面，将它解析出来。解析出来以后呢？数据给谁处理呢？处理完传输层的事情，内核的事情基本就干完了，里面的数据应该交给应用程序自己去处理，可是一台机器上跑着这么多的应用程序，应该给谁呢？无论应用程序写的使用 TCP 传数据，还是 UDP 传数据，都要监听一个端口。正是这个端口，用来区分应用程序，要不说端口不能冲突呢。两个应用监听一个端口，到时候包给谁呀？所以，按理说，无论是 TCP 还是 UDP 包头里面应该有端口号，根据端口号，将数据交给相应的应用程序。 当我们看到 UDP 包头的时候，发现的确有端口号，有源端口号和目标端口号。因为是两端通信嘛，这很好理解。但是你还会发现，UDP 除了端口号，再没有其他的了。和下两节要讲的 TCP 头比起来，这个简直简单得一塌糊涂啊！ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:15:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"UDP 的三大特点 UDP 就像小孩子一样，有以下这些特点：第一，沟通简单，不需要一肚子花花肠子（大量的数据结构、处理逻辑、包头字段）。前提是它相信网络世界是美好的，秉承性善论，相信网络通路默认就是很容易送达的，不容易被丢弃的。第二，轻信他人。它不会建立连接，虽然有端口号，但是监听在这个地方，谁都可以传给他数据，他也可以传给任何人数据，甚至可以同时传给多个人数据。第三，愣头青，做事不懂权变。不知道什么时候该坚持，什么时候该退让。它不会根据网络的情况进行发包的拥塞控制，无论网络丢包丢成啥样了，它该怎么发还怎么发。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:15:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"UDP 的三大使用场景 基于 UDP 这种“小孩子”的特点，我们可以考虑在以下的场景中使用。 第一，需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用。这很好理解，就像如果你是领导，你会让你们组刚毕业的小朋友去做一些没有那么难的项目，打一些没有那么难的客户，或者做一些失败了也能忍受的实验性项目。我们在第四节讲的 DHCP 就是基于 UDP 协议的。一般的获取 IP 地址都是内网请求，而且一次获取不到 IP 又没事，过一会儿还有机会。我们讲过 PXE 可以在启动的时候自动安装操作系统，操作系统镜像的下载使用的 TFTP，这个也是基于 UDP 协议的。在还没有操作系统的时候，客户端拥有的资源很少，不适合维护一个复杂的状态机，而且因为是内网，一般也没啥问题。 第二，不需要一对一沟通，建立连接，而是可以广播的应用。咱们小时候人都很简单，大家在班级里面，谁成绩好，谁写作好，应该表扬谁惩罚谁，谁得几个小红花都是当着全班的面讲的，公平公正公开。长大了人心复杂了，薪水、奖金要背靠背，和员工一对一沟通。UDP 的不面向连接的功能，可以使得可以承载广播或者多播的协议。DHCP 就是一种广播的形式，就是基于 UDP 协议的，而广播包的格式前面说过了。对于多播，我们在讲 IP 地址的时候，讲过一个 D 类地址，也即组播地址，使用这个地址，可以将包组播给一批机器。当一台机器上的某个进程想监听某个组播地址的时候，需要发送 IGMP 包，所在网络的路由器就能收到这个包，知道有个机器上有个进程在监听这个组播地址。当路由器收到这个组播地址的时候，会将包转发给这台机器，这样就实现了跨路由器的组播。在后面云中网络部分，有一个协议 VXLAN，也是需要用到组播，也是基于 UDP 协议的。 第三，需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候。记得曾国藩建立湘军的时候，专门招出生牛犊不怕虎的新兵，而不用那些“老油条”的八旗兵，就是因为八旗兵经历的事情多，遇到敌军不敢舍死忘生。同理，UDP 简单、处理速度快，不像 TCP 那样，操这么多的心，各种重传啊，保证顺序啊，前面的不收到，后面的没法处理啊。不然等这些事情做完了，时延早就上去了。而 TCP 在网络不好出现丢包的时候，拥塞控制策略会主动的退缩，降低发送速度，这就相当于本来环境就差，还自断臂膀，用户本来就卡，这下更卡了。当前很多应用都是要求低时延的，它们可不想用 TCP 如此复杂的机制，而是想根据自己的场景，实现自己的可靠和连接保证。例如，如果应用自己觉得，有的包丢了就丢了，没必要重传了，就可以算了，有的比较重要，则应用自己重传，而不依赖于 TCP。有的前面的包没到，后面的包到了，那就先给客户展示后面的嘛，干嘛非得等到齐了呢？如果网络不好，丢了包，那不能退缩啊，要尽快传啊，速度不能降下来啊，要挤占带宽，抢在客户失去耐心之前到达。 由于 UDP 十分简单，基本啥都没做，也就给了应用“城会玩”的机会。就像在和平年代，每个人应该有独立的思考和行为，应该可靠并且礼让；但是如果在战争年代，往往不太需要过于独立的思考，而需要士兵简单服从命令就可以了。曾国藩说哪支部队需要诱敌牺牲，也就牺牲了，相当于包丢了就丢了。两军狭路相逢的时候，曾国藩说上，没有带宽也要上，这才给了曾国藩运筹帷幄，城会玩的机会。同理如果你实现的应用需要有自己的连接策略，可靠保证，时延要求，使用 UDP，然后再应用层实现这些是再好不过了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:15:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"基于 UDP 的“城会玩”的五个例子 我列举几种“城会玩”的例子。 “城会玩”一：网页或者 APP 的访问 原来访问网页和手机 APP 都是基于 HTTP 协议的。HTTP 协议是基于 TCP 的，建立连接都需要多次交互，对于时延比较大的目前主流的移动互联网来讲，建立一次连接需要的时间会比较长，然而既然是移动中，TCP 可能还会断了重连，也是很耗时的。而且目前的 HTTP 协议，往往采取多个数据通道共享一个连接的情况，这样本来为了加快传输速度，但是 TCP 的严格顺序策略使得哪怕共享通道，前一个不来，后一个和前一个即便没关系，也要等着，时延也会加大。而 QUIC（全称 Quick UDP Internet Connections，快速 UDP 互联网连接）是 Google 提出的一种基于 UDP 改进的通信协议，其目的是降低网络通信的延迟，提供更好的用户互动体验。QUIC 在应用层上，会自己实现快速连接建立、减少重传时延，自适应拥塞控制，是应用层“城会玩”的代表。这一节主要是讲 UDP，QUIC 我们放到应用层去讲。 “城会玩”二：流媒体的协议 现在直播比较火，直播协议多使用 RTMP，这个协议我们后面的章节也会讲，而这个 RTMP 协议也是基于 TCP 的。TCP 的严格顺序传输要保证前一个收到了，下一个才能确认，如果前一个收不到，下一个就算包已经收到了，在缓存里面，也需要等着。对于直播来讲，这显然是不合适的，因为老的视频帧丢了其实也就丢了，就算再传过来用户也不在意了，他们要看新的了，如果老是没来就等着，卡顿了，新的也看不了，那就会丢失客户，所以直播，实时性比较比较重要，宁可丢包，也不要卡顿的。另外，对于丢包，其实对于视频播放来讲，有的包可以丢，有的包不能丢，因为视频的连续帧里面，有的帧重要，有的不重要，如果必须要丢包，隔几个帧丢一个，其实看视频的人不会感知，但是如果连续丢帧，就会感知了，因而在网络不好的情况下，应用希望选择性的丢帧。还有就是当网络不好的时候，TCP 协议会主动降低发送速度，这对本来当时就卡的看视频来讲是要命的，应该应用层马上重传，而不是主动让步。因而，很多直播应用，都基于 UDP 实现了自己的视频传输协议。 “城会玩”三：实时游戏 游戏有一个特点，就是实时性比较高。快一秒你干掉别人，慢一秒你被别人爆头，所以很多职业玩家会买非常专业的鼠标和键盘，争分夺秒。因而，实时游戏中客户端和服务端要建立长连接，来保证实时传输。但是游戏玩家很多，服务器却不多。由于维护 TCP 连接需要在内核维护一些数据结构，因而一台机器能够支撑的 TCP 连接数目是有限的，然后 UDP 由于是没有连接的，在异步 IO 机制引入之前，常常是应对海量客户端连接的策略。另外还是 TCP 的强顺序问题，对战的游戏，对网络的要求很简单，玩家通过客户端发送给服务器鼠标和键盘行走的位置，服务器会处理每个用户发送过来的所有场景，处理完再返回给客户端，客户端解析响应，渲染最新的场景展示给玩家。如果出现一个数据包丢失，所有事情都需要停下来等待这个数据包重发。客户端会出现等待接收数据，然而玩家并不关心过期的数据，激战中卡 1 秒，等能动了都已经死了。游戏对实时要求较为严格的情况下，采用自定义的可靠 UDP 协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。 “城会玩”四：IoT 物联网 一方面，物联网领域终端资源少，很可能只是个内存非常小的嵌入式系统，而维护 TCP 协议代价太大；另一方面，物联网对实时性要求也很高，而 TCP 还是因为上面的那些原因导致时延大。Google 旗下的 Nest 建立 Thread Group，推出了物联网通信协议 Thread，就是基于 UDP 协议的。 “城会玩”五：移动通信领域 在 4G 网络里，移动流量上网的数据面对的协议 GTP-U 是基于 UDP 的。因为移动网络协议比较复杂，而 GTP 协议本身就包含复杂的手机上线下线的通信协议。如果基于 TCP，TCP 的机制就显得非常多余，这部分协议我会在后面的章节单独讲解。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:15:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这节就到这里了，我们来总结一下：如果将 TCP 比作成熟的社会人，UDP 则是头脑简单的小朋友。TCP 复杂，UDP 简单；TCP 维护连接，UDP 谁都相信；TCP 会坚持知进退；UDP 愣头青一个，勇往直前；UDP 虽然简单，但它有简单的用法。它可以用在环境简单、需要多播、应用层自己控制传输的地方。例如 DHCP、VXLAN、QUIC 等。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:15:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第11讲 | TCP协议（上）：因性恶而复杂，先恶后善反轻松 上一节，我们讲的 UDP，基本上包括了传输层所必须的端口字段。它就像我们小时候一样简单，相信“网之初，性本善，不丢包，不乱序”。后来呢，我们都慢慢长大，了解了社会的残酷，变得复杂而成熟，就像 TCP 协议一样。它之所以这么复杂，那是因为它秉承的是“性恶论”。它天然认为网络环境是恶劣的，丢包、乱序、重传，拥塞都是常有的事情，一言不合就可能送达不了，因而要从算法层面来保证可靠性。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:16:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"TCP 包头格式 我们先来看 TCP 头的格式。从这个图上可以看出，它比 UDP 复杂得多。 首先，源端口号和目标端口号是不可少的，这一点和 UDP 是一样的。如果没有这两个端口号。数据就不知道应该发给哪个应用。接下来是包的序号。为什么要给包编号呢？当然是为了解决乱序的问题。不编好号怎么确认哪个应该先来，哪个应该后到呢。编号是为了解决乱序问题。既然是社会老司机，做事当然要稳重，一件件来，面临再复杂的情况，也临危不乱。还应该有的就是确认序号。发出去的包应该有确认，要不然我怎么知道对方有没有收到呢？如果没有收到就应该重新发送，直到送达。这个可以解决不丢包的问题。作为老司机，做事当然要靠谱，答应了就要做到，暂时做不到也要有个回复。TCP 是靠谱的协议，但是这不能说明它面临的网络环境好。从 IP 层面来讲，如果网络状况的确那么差，是没有任何可靠性保证的，而作为 IP 的上一层 TCP 也无能为力，唯一能做的就是更加努力，不断重传，通过各种算法保证。也就是说，对于 TCP 来讲，IP 层你丢不丢包，我管不着，但是我在我的层面上，会努力保证可靠性。这有点像如果你在北京，和客户约十点见面，那么你应该清楚堵车是常态，你干预不了，也控制不了，你唯一能做的就是早走。打车不行就改乘地铁，尽力不失约。接下来有一些状态位。例如 SYN 是发起一个连接，ACK 是回复，RST 是重新连接，FIN 是结束连接等。TCP 是面向连接的，因而双方要维护连接的状态，这些带状态位的包的发送，会引起双方的状态变更。不像小时候，随便一个不认识的小朋友都能玩在一起，人大了，就变得礼貌，优雅而警觉，人与人遇到会互相热情的寒暄，离开会不舍地道别，但是人与人之间的信任会经过多次交互才能建立。还有一个重要的就是窗口大小。TCP 要做流量控制，通信双方各声明一个窗口，标识自己当前能够的处理能力，别发送的太快，撑死我，也别发的太慢，饿死我。作为老司机，做事情要有分寸，待人要把握尺度，既能适当提出自己的要求，又不强人所难。除了做流量控制以外，TCP 还会做拥塞控制，对于真正的通路堵车不堵车，它无能为力，唯一能做的就是控制自己，也即控制发送的速度。不能改变世界，就改变自己嘛。作为老司机，要会自我控制，知进退，知道什么时候应该坚持，什么时候应该让步。 通过对 TCP 头的解析，我们知道要掌握 TCP 协议，重点应该关注以下几个问题：顺序问题 ，稳重不乱；丢包问题，承诺靠谱；连接维护，有始有终；流量控制，把握分寸；拥塞控制，知进知退。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:16:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"TCP 的三次握手 所有的问题，首先都要先建立一个连接，所以我们先来看连接维护问题。TCP 的连接建立，我们常常称为三次握手。 A：您好，我是 A。B：您好 A，我是 B。A：您好 B。 我们也常称为“请求 -\u003e 应答 -\u003e 应答之应答”的三个回合。这个看起来简单，其实里面还是有很多的学问，很多的细节。 首先，为什么要三次，而不是两次？按说两个人打招呼，一来一回就可以了啊？为了可靠，为什么不是四次？我们还是假设这个通路是非常不可靠的，A 要发起一个连接，当发了第一个请求杳无音信的时候，会有很多的可能性，比如第一个请求包丢了，再如没有丢，但是绕了弯路，超时了，还有 B 没有响应，不想和我连接。A 不能确认结果，于是再发，再发。终于，有一个请求包到了 B，但是请求包到了 B 的这个事情，目前 A 还是不知道的，A 还有可能再发。 B 收到了请求包，就知道了 A 的存在，并且知道 A 要和它建立连接。如果 B 不乐意建立连接，则 A 会重试一阵后放弃，连接建立失败，没有问题；如果 B 是乐意建立连接的，则会发送应答包给 A。当然对于 B 来说，这个应答包也是一入网络深似海，不知道能不能到达 A。这个时候 B 自然不能认为连接是建立好了，因为应答包仍然会丢，会绕弯路，或者 A 已经挂了都有可能。 而且这个时候 B 还能碰到一个诡异的现象就是，A 和 B 原来建立了连接，做了简单通信后，结束了连接。还记得吗？A 建立连接的时候，请求包重复发了几次，有的请求包绕了一大圈又回来了，B 会认为这也是一个正常的的请求的话，因此建立了连接，可以想象，这个连接不会进行下去，也没有个终结的时候，纯属单相思了。因而两次握手肯定不行。B 发送的应答可能会发送多次，但是只要一次到达 A，A 就认为连接已经建立了，因为对于 A 来讲，他的消息有去有回。A 会给 B 发送应答之应答，而 B 也在等这个消息，才能确认连接的建立，只有等到了这个消息，对于 B 来讲，才算它的消息有去有回。 当然 A 发给 B 的应答之应答也会丢，也会绕路，甚至 B 挂了。按理来说，还应该有个应答之应答之应答，这样下去就没底了。所以四次握手是可以的，四十次都可以，关键四百次也不能保证就真的可靠了。只要双方的消息都有去有回，就基本可以了。好在大部分情况下，A 和 B 建立了连接之后，A 会马上发送数据的，一旦 A 发送数据，则很多问题都得到了解决。例如 A 发给 B 的应答丢了，当 A 后续发送的数据到达的时候，B 可以认为这个连接已经建立，或者 B 压根就挂了，A 发送的数据，会报错，说 B 不可达，A 就知道 B 出事情了。当然你可以说 A 比较坏，就是不发数据，建立连接后空着。我们在程序设计的时候，可以要求开启 keepalive 机制，即使没有真实的数据包，也有探活包。 另外，你作为服务端 B 的程序设计者，对于 A 这种长时间不发包的客户端，可以主动关闭，从而空出资源来给其他客户端使用。 三次握手除了双方建立连接外，主要还是为了沟通一件事情，就是 TCP 包的序号的问题。A 要告诉 B，我这面发起的包的序号起始是从哪个号开始的，B 同样也要告诉 A，B 发起的包的序号起始是从哪个号开始的。为什么序号不能都从 1 开始呢？因为这样往往会出现冲突。 例如，A 连上 B 之后，发送了 1、2、3 三个包，但是发送 3 的时候，中间丢了，或者绕路了，于是重新发送，后来 A 掉线了，重新连上 B 后，序号又从 1 开始，然后发送 2，但是压根没想发送 3，但是上次绕路的那个 3 又回来了，发给了 B，B 自然认为，这就是下一个包，于是发生了错误。因而，每个连接都要有不同的序号。这个序号的起始序号是随着时间变化的，可以看成一个 32 位的计数器，每 4 微秒加一，如果计算一下，如果到重复，需要 4 个多小时，那个绕路的包早就死翘翘了，因为我们都知道 IP 包头里面有个 TTL，也即生存时间。好了，双方终于建立了信任，建立了连接。前面也说过，为了维护这个连接，双方都要维护一个状态机，在连接建立的过程中，双方的状态变化时序图就像这样。 一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN 状态。然后客户端主动发起连接 SYN，之后处于 SYN-SENT 状态。服务端收到发起的连接，返回 SYN，并且 ACK 客户端的 SYN，之后处于 SYN-RCVD 状态。客户端收到服务端发送的 SYN 和 ACK 之后，发送 ACK 的 ACK，之后处于 ESTABLISHED 状态，因为它一发一收成功了。服务端收到 ACK 的 ACK 之后，处于 ESTABLISHED 状态，因为它也一发一收了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:16:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"TCP 四次挥手 好了，说完了连接，接下来说一说“拜拜”，好说好散。这常被称为四次挥手。 A：B 啊，我不想玩了。B：哦，你不想玩了啊，我知道了。 这个时候，还只是 A 不想玩了，也即 A 不会再发送数据，但是 B 能不能在 ACK 的时候，直接关闭呢？当然不可以了，很有可能 A 是发完了最后的数据就准备不玩了，但是 B 还没做完自己的事情，还是可以发送数据的，所以称为半关闭的状态。 这个时候 A 可以选择不再接收数据了，也可以选择最后再接收一段数据，等待 B 也主动关闭。 B：A 啊，好吧，我也不玩了，拜拜。A：好的，拜拜。 这样整个连接就关闭了。但是这个过程有没有异常情况呢？当然有，上面是和平分手的场面。A 开始说“不玩了”，B 说“知道了”，这个回合，是没什么问题的，因为在此之前，双方还处于合作的状态，如果 A 说“不玩了”，没有收到回复，则 A 会重新发送“不玩了”。但是这个回合结束之后，就有可能出现异常情况了，因为已经有一方率先撕破脸。一种情况是，A 说完“不玩了”之后，直接跑路，是会有问题的，因为 B 还没有发起结束，而如果 A 跑路，B 就算发起结束，也得不到回答，B 就不知道该怎么办了。另一种情况是，A 说完“不玩了”，B 直接跑路，也是有问题的，因为 A 不知道 B 是还有事情要处理，还是过一会儿会发送结束。 那怎么解决这些问题呢？TCP 协议专门设计了几个状态来处理这些问题。我们来看断开连接的时候的状态时序图。 断开的时候，我们可以看到，当 A 说“不玩了”，就进入 FIN_WAIT_1 的状态，B 收到“A 不玩”的消息后，发送知道了，就进入 CLOSE_WAIT 的状态。A 收到“B 说知道了”，就进入 FIN_WAIT_2 的状态，如果这个时候 B 直接跑路，则 A 将永远在这个状态。TCP 协议里面并没有对这个状态的处理，但是 Linux 有，可以调整 tcp_fin_timeout 这个参数，设置一个超时时间。 如果 B 没有跑路，发送了“B 也不玩了”的请求到达 A 时，A 发送“知道 B 也不玩了”的 ACK 后，从 FIN_WAIT_2 状态结束，按说 A 可以跑路了，但是最后的这个 ACK 万一 B 收不到呢？则 B 会重新发一个“B 不玩了”，这个时候 A 已经跑路了的话，B 就再也收不到 ACK 了，因而 TCP 协议要求 A 最后等待一段时间 TIME_WAIT，这个时间要足够长，长到如果 B 没收到 ACK 的话，“B 说不玩了”会重发的，A 会重新发一个 ACK 并且足够时间到达 B。A 直接跑路还有一个问题是，A 的端口就直接空出来了，但是 B 不知道，B 原来发过的很多包很可能还在路上，如果 A 的端口被一个新的应用占用了，这个新的应用会收到上个连接中 B 发过来的包，虽然序列号是重新生成的，但是这里要上一个双保险，防止产生混乱，因而也需要等足够长的时间，等到原来 B 发送的所有的包都死翘翘，再空出端口来。 等待的时间设为 2MSL，MSL 是 Maximum Segment Lifetime，报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 TTL 域，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。协议规定 MSL 为 2 分钟，实际应用中常用的是 30 秒，1 分钟和 2 分钟等。还有一个异常情况就是，B 超过了 2MSL 的时间，依然没有收到它发的 FIN 的 ACK，怎么办呢？按照 TCP 的原理，B 当然还会重发 FIN，这个时候 A 再收到这个包之后，A 就表示，我已经在这里等了这么长时间了，已经仁至义尽了，之后的我就都不认了，于是就直接发送 RST，B 就知道 A 早就跑了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:16:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"TCP 状态机 将连接建立和连接断开的两个时序状态图综合起来，就是这个著名的 TCP 的状态机。学习的时候比较建议将这个状态机和时序状态机对照着看，不然容易晕。 在这个图中，加黑加粗的部分，是上面说到的主要流程，其中阿拉伯数字的序号，是连接过程中的顺序，而大写中文数字的序号，是连接断开过程中的顺序。加粗的实线是客户端 A 的状态变迁，加粗的虚线是服务端 B 的状态变迁。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:16:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我来做一个总结：TCP 包头很复杂，但是主要关注五个问题，顺序问题，丢包问题，连接维护，流量控制，拥塞控制；连接的建立是经过三次握手，断开的时候四次挥手，一定要掌握的我画的那个状态图。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:16:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第12讲 | TCP协议（下）：西行必定多妖孽，恒心智慧消磨难 我们前面说到玄奘西行，要出网关。既然出了网关，那就是在公网上传输数据，公网往往是不可靠的，因而需要很多的机制去保证传输的可靠性，这里面需要恒心，也即各种重传的策略，还需要有智慧，也就是说，这里面包含着大量的算法。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:17:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何做个靠谱的人？ TCP 想成为一个成熟稳重的人，成为一个靠谱的人。那一个人怎么样才算靠谱呢？咱们工作中经常就有这样的场景，比如你交代给下属一个事情以后，下属到底能不能做到，做到什么程度，什么时候能够交付，往往就会有应答，有回复。这样，处理事情的过程中，一旦有异常，你也可以尽快知道，而不是交代完之后就石沉大海，过了一个月再问，他说，啊我不记得了。对应到网络协议上，就是客户端每发送的一个包，服务器端都应该有个回复，如果服务器端超过一定的时间没有回复，客户端就会重新发送这个包，直到有回复。 这个发送应答的过程是什么样呢？可以是上一个收到了应答，再发送下一个。这种模式有点像两个人直接打电话，你一句，我一句。但是这种方式的缺点是效率比较低。如果一方在电话那头处理的时间比较长，这一头就要干等着，双方都没办法干其他事情。咱们在日常工作中也不是这样的，不能你交代你的下属办一件事情，就一直打着电话看着他做，而是应该他按照你的安排，先将事情记录下来，办完一件回复一件。在他办事情的过程中，你还可以同时交代新的事情，这样双方就并行了。 如果使⽤这种模式，其实需要你和你的下属就不能靠脑⼦了，⽽是要都准备⼀个本⼦，你每交代下属⼀个事情，双方的本子都要记录⼀下。当你的下属做完⼀件事情，就回复你，做完了，你就在你的本⼦上将这个事情划去。同时你的本⼦上每件事情都有时限，如果超过了时限下属还没有回复，你就要主动重新交代⼀下：上次那件事情，你还没回复我，咋样啦？既然多件事情可以一起处理，那就需要给每个事情编个号，防止弄错了。例如，程序员平时看任务的时候，都会看 JIRA 的 ID，而不是每次都要描述一下具体的事情。在大部分情况下，对于事情的处理是按照顺序来的，先来的先处理，这就给应答和汇报工作带来了方便。等开周会的时候，每个程序员都可以将 JIRA ID 的列表拉出来，说以上的都做完了，⽽不⽤⼀个个说。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:17:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何实现一个靠谱的协议？ TCP 协议使用的也是同样的模式。为了保证顺序性，每一个包都有一个 ID。在建立连接的时候，会商定起始的 ID 是什么，然后按照 ID 一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个一个来的，而是会应答某个之前的 ID，表示都收到了，这种模式称为累计确认或者累计应答（cumulative acknowledgment）。为了记录所有发送的包和接收的包，TCP 也需要发送端和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的 ID 一个个排列，根据处理的情况分成四个部分。 第一部分：发送了并且已经确认的。这部分就是你交代下属的，并且也做完了的，应该划掉的。第二部分：发送了并且尚未确认的。这部分是你交代下属的，但是还没做完的，需要等待做完的回复之后，才能划掉。第三部分：没有发送，但是已经等待发送的。这部分是你还没有交代给下属，但是马上就要交代的。第四部分：没有发送，并且暂时还不会发送的。这部分是你还没有交代给下属，而且暂时还不会交代给下属的。 这里面为什么要区分第三部分和第四部分呢？没交代的，一下子全交代了不就完了吗？这就是我们上一节提到的十个词口诀里的“流量控制，把握分寸”。作为项目管理人员，你应该根据以往的工作情况和这个员工反馈的能力、抗压力等，先在心中估测一下，这个人一天能做多少工作。如果工作布置少了，就会不饱和；如果工作布置多了，他就会做不完；如果你使劲逼迫，人家可能就要辞职了。到底一个员工能够同时处理多少事情呢？在 TCP 里，接收端会给发送端报一个窗口的大小，叫 Advertised window。这个窗口的大小应该等于上面的第二部分加上第三部分，就是已经交代了没做完的加上马上要交代的。超过这个窗口的，接收端做不过来，就不能发送了。于是，发送端需要保持下面的数据结构。 LastByteAcked：第一部分和第二部分的分界线LastByteSent：第二部分和第三部分的分界线LastByteAcked + AdvertisedWindow：第三部分和第四部分的分界线 对于接收端来讲，它的缓存里记录的内容要简单一些。第一部分：接受并且确认过的。也就是我领导交代给我，并且我做完的。第二部分：还没接收，但是马上就能接收的。也即是我自己的能够接受的最大工作量。第三部分：还没接收，也没法接收的。也即超过工作量的部分，实在做不完。 对应的数据结构就像这样。 MaxRcvBuffer：最大缓存的量；LastByteRead 之后是已经接收了，但是还没被应用层读取的；NextByteExpected 是第一部分和第二部分的分界线。 第二部分的窗口有多大呢？NextByteExpected 和 LastByteRead 的差其实是还没被应用层读取的部分占用掉的 MaxRcvBuffer 的量，我们定义为 A。AdvertisedWindow 其实是 MaxRcvBuffer 减去 A。也就是：AdvertisedWindow=MaxRcvBuffer-((NextByteExpected-1)-LastByteRead)。 那第二部分和第三部分的分界线在哪里呢？NextByteExpected 加 AdvertisedWindow 就是第二部分和第三部分的分界线，其实也就是 LastByteRead 加上 MaxRcvBuffer。其中第二部分里面，由于受到的包可能不是顺序的，会出现空档，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:17:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"顺序问题与丢包问题 接下来我们结合一个例子来看。还是刚才的图，在发送端来看，1、2、3 已经发送并确认；4、5、6、7、8、9 都是发送了还没确认；10、11、12 是还没发出的；13、14、15 是接收方没有空间，不准备发的。在接收端来看，1、2、3、4、5 是已经完成 ACK，但是没读取的；6、7 是等待接收的；8、9 是已经接收，但是没有 ACK 的。发送端和接收端当前的状态如下： 1、2、3 没有问题，双方达成了一致。4、5 接收方说 ACK 了，但是发送方还没收到，有可能丢了，有可能在路上。6、7、8、9 肯定都发了，但是 8、9 已经到了，但是 6、7 没到，出现了乱序，缓存着但是没办法 ACK。 根据这个例子，我们可以知道，顺序问题和丢包问题都有可能发生，所以我们先来看确认与重发的机制。假设 4 的确认到了，不幸的是，5 的 ACK 丢了，6、7 的数据包丢了，这该怎么办呢？ 一种方法就是超时重试，也即对每一个发送了，但是没有 ACK 的包，都有设一个定时器，超过了一定的时间，就重新尝试。但是这个超时的时间如何评估呢？这个时间不宜过短，时间必须大于往返时间 RTT，否则会引起不必要的重传。也不宜过长，这样超时时间变长，访问就变慢了。估计往返时间，需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个值，而且这个值还是要不断变化的，因为网络状况不断地变化。除了采样 RTT，还要采样 RTT 的波动范围，计算出一个估计的超时时间。由于重传时间是不断变化的，我们称为自适应重传算法（Adaptive Retransmission Algorithm）。如果过一段时间，5、6、7 都超时了，就会重新发送。接收方发现 5 原来接收过，于是丢弃 5；6 收到了，发送 ACK，要求下一个是 7，7 不幸又丢了。当 7 再次超时的时候，有需要重传的时候，TCP 的策略是超时间隔加倍。每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？有一个可以快速重传的机制，当接收方收到一个序号大于下一个所期望的报文段时，就会检测到数据流中的一个间隔，于是它就会发送冗余的 ACK，仍然 ACK 的是期望接收的报文段。而当客户端收到三个冗余的 ACK 后，就会在定时器过期之前，重传丢失的报文段。 例如，接收方发现 6 收到了，8 也收到了，但是 7 还没来，那肯定是丢了，于是发送 6 的 ACK，要求下一个是 7。接下来，收到后续的包，仍然发送 6 的 ACK，要求下一个是 7。当客户端收到 3 个重复 ACK，就会发现 7 的确丢了，不等超时，马上重发。还有一种方式称为 Selective Acknowledgment （SACK）。这种方式需要在 TCP 头里加一个 SACK 的东西，可以将缓存的地图发送给发送方。例如可以发送 ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是 7 丢了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:17:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"流量控制问题 我们再来看流量控制机制，在对于包的确认中，同时会携带一个窗口的大小。我们先假设窗口不变的情况，窗口始终为 9。4 的确认来的时候，会右移一个，这个时候第 13 个包也可以发送了。 这个时候，假设发送端发送过猛，会将第三部分的 10、11、12、13 全部发送完毕，之后就停止发送了，未发送可发送部分为 0。 当对于包 5 的确认到达的时候，在客户端相当于窗口再滑动了一格，这个时候，才可以有更多的包可以发送了，例如第 14 个包才可以发送。 如果接收方实在处理的太慢，导致缓存中没有空间了，可以通过确认信息修改窗口的大小，甚至可以设置为 0，则发送方将暂时停止发送。我们假设一个极端情况，接收端的应用一直不读取缓存中的数据，当数据包 6 确认后，窗口大小就不能再是 9 了，就要缩小一个变为 8。 这个新的窗口 8 通过 6 的确认消息到达发送端的时候，你会发现窗口没有平行右移，而是仅仅左面的边右移了，窗口的大小从 9 改成了 8。 如果接收端还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，直到为 0。 当这个窗口通过包 14 的确认到达发送端的时候，发送端的窗口也调整为 0，停止发送。 如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。这就是我们常说的流量控制。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:17:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"拥塞控制问题 最后，我们看一下拥塞控制的问题，也是通过窗口的大小来控制的，前面的滑动窗口 rwnd 是怕发送方把接收方缓存塞满，而拥塞窗口 cwnd，是怕把网络塞满。这里有一个公式 LastByteSent - LastByteAcked \u003c= min {cwnd, rwnd} ，是拥塞窗口和滑动窗口共同控制发送的速度。 那发送方怎么判断网络是不是慢呢？这其实是个挺难的事情，因为对于 TCP 协议来讲，他压根不知道整个网络路径都会经历什么，对他来讲就是一个黑盒。TCP 发送包常被比喻为往一个水管里面灌水，而 TCP 的拥塞控制就是在不堵塞，不丢包的情况下，尽量发挥带宽。水管有粗细，网络有带宽，也即每秒钟能够发送多少数据；水管有长度，端到端有时延。在理想状态下，水管里面水的量 = 水管粗细 x 水管长度。对于到网络上，通道的容量 = 带宽 × 往返延迟。如果我们设置发送窗口，使得发送但未确认的包为为通道的容量，就能够撑满整个管道。 如图所示，假设往返时间为 8s，去 4s，回 4s，每秒发送一个包，每个包 1024byte。已经过去了 8s，则 8 个包都发出去了，其中前 4 个包已经到达接收端，但是 ACK 还没有返回，不能算发送成功。5-8 后四个包还在路上，还没被接收。这个时候，整个管道正好撑满，在发送端，已发送未确认的为 8 个包，正好等于带宽，也即每秒发送 1 个包，乘以来回时间 8s。如果我们在这个基础上再调大窗口，使得单位时间内更多的包可以发送，会出现什么现象呢？ 我们来想，原来发送一个包，从一端到达另一端，假设一共经过四个设备，每个设备处理一个包时间耗费 1s，所以到达另一端需要耗费 4s，如果发送的更加快速，则单位时间内，会有更多的包到达这些中间设备，这些设备还是只能每秒处理一个包的话，多出来的包就会被丢弃，这是我们不想看到的。这个时候，我们可以想其他的办法，例如这个四个设备本来每秒处理一个包，但是我们在这些设备上加缓存，处理不过来的在队列里面排着，这样包就不会丢失，但是缺点是会增加时延，这个缓存的包，4s 肯定到达不了接收端了，如果时延达到一定程度，就会超时重传，也是我们不想看到的。于是 TCP 的拥塞控制主要来避免两种现象，包丢失和超时重传。一旦出现了这些现象就说明，发送速度太快了，要慢一点。但是一开始我怎么知道速度多快呢，我怎么知道应该把窗口调整到多大呢？ 如果我们通过漏斗往瓶子里灌水，我们就知道，不能一桶水一下子倒进去，肯定会溅出来，要一开始慢慢的倒，然后发现总能够倒进去，就可以越倒越快。这叫作慢启动。一条 TCP 连接开始，cwnd 设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd 加一，于是一次能够发送两个；当这两个的确认到来的时候，每个确认 cwnd 加一，两个确认 cwnd 加二，于是一次能够发送四个；当这四个的确认到来的时候，每个确认 cwnd 加一，四个确认 cwnd 加四，于是一次能够发送八个。可以看出这是指数性的增长。 涨到什么时候是个头呢？有一个值 ssthresh 为 65535 个字节，当超过这个值的时候，就要小心一点了，不能倒这么快了，可能快满了，再慢下来。每收到一个确认后，cwnd 增加 1/cwnd，我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加 1/8，八个确认一共 cwnd 增加 1，于是一次能够发送九个，变成了线性增长。但是线性增长还是增长，还是越来越多，直到有一天，水满则溢，出现了拥塞，这时候一般就会一下子降低倒水的速度，等待溢出的水慢慢渗下去。拥塞的一种表现形式是丢包，需要超时重传，这个时候，将 sshresh 设为 cwnd/2，将 cwnd 设为 1，重新开始慢启动。这真是一旦超时重传，马上回到解放前。但是这种方式太激进了，将一个高速的传输速度一下子停了下来，会造成网络卡顿。 前面我们讲过快速重传算法。当接收端发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，cwnd 减半为 cwnd/2，然后 sshthresh = cwnd，当三个包返回的时候，cwnd = sshthresh + 3，也就是没有一夜回到解放前，而是还在比较高的值，呈线性增长。 就像前面说的一样，正是这种知进退，使得时延很重要的情况下，反而降低了速度。但是如果你仔细想一下，TCP 的拥塞控制主要来避免的两个现象都是有问题的。 第一个问题是丢包并不代表着通道满了，也可能是管子本来就漏水。例如公网上带宽不满也会丢包，这个时候就认为拥塞了，退缩了，其实是不对的。第二个问题是 TCP 的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实 TCP 只要填满管道就可以了，不应该接着填，直到连缓存也填满。 为了优化这两个问题，后来有了 TCP BBR 拥塞算法。它企图找到一个平衡点，就是通过不断地加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:17:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节我们就到这里，总结一下：顺序问题、丢包问题、流量控制都是通过滑动窗口来解决的，这其实就相当于你领导和你的工作备忘录，布置过的工作要有编号，干完了有反馈，活不能派太多，也不能太少；拥塞控制是通过拥塞窗口来解决的，相当于往管道里面倒水，快了容易溢出，慢了浪费带宽，要摸着石头过河，找到最优值。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:17:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第13讲 | 套接字Socket：Talk is cheap, show me the code 前面讲完了 TCP 和 UDP 协议，还没有上手过，这一节咱们讲讲基于 TCP 和 UDP 协议的 Socket 编程。在讲 TCP 和 UDP 协议的时候，我们分客户端和服务端，在写程序的时候，我们也同样这样分。Socket 这个名字很有意思，可以作插口或者插槽讲。虽然我们是写软件程序，但是你可以想象为弄一根网线，一头插在客户端，一头插在服务端，然后进行通信。所以在通信之前，双方都要建立一个 Socket。在建立 Socket 的时候，应该设置什么参数呢？Socket 编程进行的是端到端的通信，往往意识不到中间经过多少局域网，多少路由器，因而能够设置的参数，也只能是端到端协议之上网络层和传输层的。在网络层，Socket 函数需要指定到底是 IPv4 还是 IPv6，分别对应设置为 AF_INET 和 AF_INET6。另外，还要指定到底是 TCP 还是 UDP。还记得咱们前面讲过的，TCP 协议是基于数据流的，所以设置为 SOCK_STREAM，而 UDP 是基于数据报的，因而设置为 SOCK_DGRAM。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:18:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"基于 TCP 协议的 Socket 程序函数调用过程 两端创建了 Socket 之后，接下来的过程中，TCP 和 UDP 稍有不同，我们先来看 TCP。TCP 的服务端要先监听一个端口，一般是先调用 bind 函数，给这个 Socket 赋予一个 IP 地址和端口。为什么需要端口呢？要知道，你写的是一个应用程序，当一个网络包来的时候，内核要通过 TCP 头里面的这个端口，来找到你这个应用程序，把包给你。为什么要 IP 地址呢？有时候，一台机器会有多个网卡，也就会有多个 IP 地址，你可以选择监听所有的网卡，也可以选择监听一个网卡，这样，只有发给这个网卡的包，才会给你。 当服务端有了 IP 和端口号，就可以调用 listen 函数进行监听。在 TCP 的状态图里面，有一个 listen 状态，当调用这个函数之后，服务端就进入了这个状态，这个时候客户端就可以发起连接了。在内核中，为每个 Socket 维护两个队列。一个是已经建立了连接的队列，这时候连接三次握手已经完毕，处于 established 状态；一个是还没有完全建立连接的队列，这个时候三次握手还没完成，处于 syn_rcvd 的状态。 接下来，服务端调用 accept 函数，拿出一个已经完成的连接进行处理。如果还没有完成，就要等着。在服务端等待的时候，客户端可以通过 connect 函数发起连接。先在参数中指明要连接的 IP 地址和端口号，然后开始发起三次握手。内核会给客户端分配一个临时的端口。一旦握手成功，服务端的 accept 就会返回另一个 Socket。这是一个经常考的知识点，就是监听的 Socket 和真正用来传数据的 Socket 是两个，一个叫作监听 Socket，一个叫作已连接 Socket。连接建立成功之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。这个图就是基于 TCP 协议的 Socket 程序函数调用过程。 说 TCP 的 Socket 就是一个文件流，是非常准确的。因为，Socket 在 Linux 中就是以文件的形式存在的。除此之外，还存在文件描述符。写入和读出，也是通过文件描述符。在内核中，Socket 是一个文件，那对应就有文件描述符。每一个进程都有一个数据结构 task_struct，里面指向一个文件描述符数组，来列出这个进程打开的所有文件的文件描述符。文件描述符是一个整数，是这个数组的下标。这个数组中的内容是一个指针，指向内核中所有打开的文件的列表。既然是一个文件，就会有一个 inode，只不过 Socket 对应的 inode 不像真正的文件系统一样，保存在硬盘上的，而是在内存中的。在这个 inode 中，指向了 Socket 在内核中的 Socket 结构。在这个结构里面，主要的是两个队列，一个是发送队列，一个是接收队列。在这两个队列里面保存的是一个缓存 sk_buff。这个缓存里面能够看到完整的包的结构。看到这个，是不是能和前面讲过的收发包的场景联系起来了？整个数据结构我也画了一张图。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:18:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"基于 UDP 协议的 Socket 程序函数调用过程 对于 UDP 来讲，过程有些不一样。UDP 是没有连接的，所以不需要三次握手，也就不需要调用 listen 和 connect，但是，UDP 的交互仍然需要 IP 和端口号，因而也需要 bind。UDP 是没有维护连接状态的，因而不需要每对连接建立一组 Socket，而是只要有一个 Socket，就能够和多个客户端通信。也正是因为没有连接状态，每次通信的时候，都调用 sendto 和 recvfrom，都可以传入 IP 地址和端口。这个图的内容就是基于 UDP 协议的 Socket 程序函数调用过程。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:18:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"服务器如何接更多的项目？ 会了这几个基本的 Socket 函数之后，你就可以轻松地写一个网络交互的程序了。就像上面的过程一样，在建立连接后，进行一个 while 循环。客户端发了收，服务端收了发。当然这只是万里长征的第一步，因为如果使用这种方法，基本上只能一对一沟通。如果你是一个服务器，同时只能服务一个客户，肯定是不行的。这就相当于老板成立一个公司，只有自己一个人，自己亲自上来服务客户，只能干完了一家再干下一家，这样赚不来多少钱。那作为老板你就要想了，我最多能接多少项目呢？当然是越多越好。我们先来算一下理论值，也就是最大连接数，系统会用一个四元组来标识一个 TCP 连接 {本机IP, 本机端口, 对端IP, 对端端口} 服务器通常固定在某个本地端口上监听，等待客户端的连接请求。因此，服务端端 TCP 连接四元组中只有对端 IP, 也就是客户端的 IP 和对端的端口，也即客户端的端口是可变的，因此，最大 TCP 连接数 = 客户端 IP 数×客户端端口数。对 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是服务端单机最大 TCP 连接数，约为 2 的 48 次方。当然，服务端最大并发 TCP 连接数远不能达到理论上限。首先主要是文件描述符限制，按照上面的原理，Socket 都是文件，所以首先要通过 ulimit 配置文件描述符的数目；另一个限制是内存，按上面的数据结构，每个 TCP 连接都要占用一定内存，操作系统是有限的。所以，作为老板，在资源有限的情况下，要想接更多的项目，就需要降低每个项目消耗的资源数目。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:18:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"方式一：将项目外包给其他公司（多进程方式） 这就相当于你是一个代理，在那里监听来的请求。一旦建立了一个连接，就会有一个已连接 Socket，这时候你可以创建一个子进程，然后将基于已连接 Socket 的交互交给这个新的子进程来做。就像来了一个新的项目，但是项目不一定是你自己做，可以再注册一家子公司，招点人，然后把项目转包给这家子公司做，以后对接就交给这家子公司了，你又可以去接新的项目了。这里有一个问题是，如何创建子公司，并如何将项目移交给子公司呢？在 Linux 下，创建子进程使用 fork 函数。通过名字可以看出，这是在父进程的基础上完全拷贝一个子进程。在 Linux 内核中，会复制文件描述符的列表，也会复制内存空间，还会复制一条记录当前执行到了哪一行程序的进程。显然，复制的时候在调用 fork，复制完毕之后，父进程和子进程都会记录当前刚刚执行完 fork。这两个进程刚复制完的时候，几乎一模一样，只是根据 fork 的返回值来区分到底是父进程，还是子进程。如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。进程复制过程我画在这里。 因为复制了文件描述符列表，而文件描述符都是指向整个内核统一的打开文件列表的，因而父进程刚才因为 accept 创建的已连接 Socket 也是一个文件描述符，同样也会被子进程获得。接下来，子进程就可以通过这个已连接 Socket 和客户端进行互通了，当通信完毕之后，就可以退出进程，那父进程如何知道子进程干完了项目，要退出呢？还记得 fork 返回的时候，如果是整数就是父进程吗？这个整数就是子进程的 ID，父进程可以通过这个 ID 查看子进程是否完成项目，是否需要退出。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:18:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"方式二：将项目转包给独立的项目组（多线程方式） 上面这种方式你应该也能发现问题，如果每次接一个项目，都申请一个新公司，然后干完了，就注销掉这个公司，实在是太麻烦了。毕竟一个新公司要有新公司的资产，有新的办公家具，每次都买了再卖，不划算。于是你应该想到了，我们可以使用线程。相比于进程来讲，这样要轻量级的多。如果创建进程相当于成立新公司，购买新办公家具，而创建线程，就相当于在同一个公司成立项目组。一个项目做完了，那这个项目组就可以解散，组成另外的项目组，办公家具可以共用。在 Linux 下，通过 pthread_create 创建一个线程，也是调用 do_fork。不同的是，虽然新的线程在 task 列表会新创建一项，但是很多资源，例如文件描述符列表、进程空间，还是共享的，只不过多了一个引用而已。 新的线程也可以通过已连接 Socket 处理请求，从而达到并发处理的目的。上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程。一台机器无法创建很多进程或者线程。有个 C10K，它的意思是一台机器要维护 1 万个连接，就要创建 1 万个进程或者线程，那么操作系统是无法承受的。如果维持 1 亿用户在线需要 10 万台服务器，成本也太高了。其实 C10K 问题就是，你接项目接的太多了，如果每个项目都成立单独的项目组，就要招聘 10 万人，你肯定养不起，那怎么办呢？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:18:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"方式三：一个项目组支撑多个项目（IO 多路复用，一个线程维护多个 Socket） 当然，一个项目组可以看多个项目了。这个时候，每个项目组都应该有个项目进度墙，将自己组看的项目列在那里，然后每天通过项目墙看每个项目的进度，一旦某个项目有了进展，就派人去盯一下。由于 Socket 是文件描述符，因而某个线程盯的所有的 Socket，都放在一个文件描述符集合 fd_set 中，这就是项目进度墙，然后调用 select 函数来监听文件描述符集合是否有变化。一旦有变化，就会依次查看每个文件描述符。那些发生变化的文件描述符在 fd_set 对应的位都设为 1，表示 Socket 可读或者可写，从而可以进行读写操作，然后再调用 select，接着盯着下一轮的变化。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:18:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"方式四：一个项目组支撑多个项目（IO 多路复用，从“派人盯着”到“有事通知”） 上面 select 函数还是有问题的，因为每次 Socket 所在的文件描述符集合中有 Socket 发生变化的时候，都需要通过轮询的方式，也就是需要将全部项目都过一遍的方式来查看进度，这大大影响了一个项目组能够支撑的最大的项目数量。因而使用 select，能够同时盯的项目数量由 FD_SETSIZE 限制。如果改成事件通知的方式，情况就会好很多，项目组不需要通过轮询挨个盯着这些项目，而是当项目进度发生变化的时候，主动通知项目组，然后项目组再根据项目进展情况做相应的操作。能完成这件事情的函数叫 epoll，它在内核中的实现不是通过轮询的方式，而是通过注册 callback 函数的方式，当某个文件描述符发送变化的时候，就会主动通知。 如图所示，假设进程打开了 Socket m, n, x 等多个文件描述符，现在需要通过 epoll 来监听是否这些 Socket 都有事件发生。其中 epoll_create 创建一个 epoll 对象，也是一个文件，也对应一个文件描述符，同样也对应着打开文件列表中的一项。在这项里面有一个红黑树，在红黑树里，要保存这个 epoll 要监听的所有 Socket。当 epoll_ctl 添加一个 Socket 的时候，其实是加入这个红黑树，同时红黑树里面的节点指向一个结构，将这个结构挂在被监听的 Socket 的事件列表中。当一个 Socket 来了一个事件的时候，可以从这个列表中得到 epoll 对象，并调用 call back 通知它。这种通知方式使得监听的 Socket 数据增加的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了。上限就为系统定义的、进程打开的最大文件描述符个数。因而，epoll 被称为解决 C10K 问题的利器。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:18:7","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下：你需要记住 TCP 和 UDP 的 Socket 的编程中，客户端和服务端都需要调用哪些函数；写一个能够支撑大量连接的高并发的服务端不容易，需要多进程、多线程，而 epoll 机制能解决 C10K 问题。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:18:8","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"最常用的应用层 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:19:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第14讲 | HTTP协议：看个新闻原来这么麻烦 前面讲述完传输层，接下来开始讲应用层的协议。从哪里开始讲呢，就从咱们最常用的 HTTP 协议开始。HTTP 协议，几乎是每个人上网用的第一个协议，同时也是很容易被人忽略的协议。既然说看新闻，咱们就先登录 http://www.163.com 。http://www.163.com 是个 URL，叫作统一资源定位符。之所以叫统一，是因为它是有格式的。HTTP 称为协议，www.163.com 是一个域名，表示互联网上的一个位置。有的 URL 会有更详细的位置标识，例如 http://www.163.com/index.html 。正是因为这个东西是统一的，所以当你把这样一个字符串输入到浏览器的框里的时候，浏览器才知道如何进行统一处理。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:20:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"HTTP 请求的准备 浏览器会将 www.163.com 这个域名发送给 DNS 服务器，让它解析为 IP 地址。有关 DNS 的过程，其实非常复杂，这个在后面专门介绍 DNS 的时候，我会详细描述，这里我们先不管，反正它会被解析成为 IP 地址。那接下来是发送 HTTP 请求吗？不是的，HTTP 是基于 TCP 协议的，当然是要先建立 TCP 连接了，怎么建立呢？还记得第 11 节讲过的三次握手吗？目前使用的 HTTP 协议大部分都是 1.1。在 1.1 的协议里面，默认是开启了 Keep-Alive 的，这样建立的 TCP 连接，就可以在多次请求中复用。学习了 TCP 之后，你应该知道，TCP 的三次握手和四次挥手，还是挺费劲的。如果好不容易建立了连接，然后就做了一点儿事情就结束了，有点儿浪费人力和物力。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:20:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"HTTP 请求的构建 建立了连接以后，浏览器就要发送 HTTP 的请求。请求的格式就像这样。 HTTP 的报文大概分为三大部分。第一部分是请求行，第二部分是请求的首部，第三部分才是请求的正文实体。 第一部分：请求行在请求行中，URL 就是 http://www.163.com ，版本为 HTTP 1.1。这里要说一下的，就是方法。方法有几种类型。对于访问网页来讲，最常用的类型就是 GET。顾名思义，GET 就是去服务器获取一些资源。对于访问网页来讲，要获取的资源往往是一个页面。其实也有很多其他的格式，比如说返回一个 JSON 字符串，到底要返回什么，是由服务器端的实现决定的。例如，在云计算中，如果我们的服务器端要提供一个基于 HTTP 协议的 API，获取所有云主机的列表，这就会使用 GET 方法得到，返回的可能是一个 JSON 字符串。字符串里面是一个列表，列表里面是一项的云主机的信息。另外一种类型叫做 POST。它需要主动告诉服务端一些信息，而非获取。要告诉服务端什么呢？一般会放在正文里面。正文可以有各种各样的格式。常见的格式也是 JSON。例如，我们下一节要讲的支付场景，客户端就需要把“我是谁？我要支付多少？我要买啥？”告诉服务器，这就需要通过 POST 方法。再如，在云计算里，如果我们的服务器端，要提供一个基于 HTTP 协议的创建云主机的 API，也会用到 POST 方法。这个时候往往需要将“我要创建多大的云主机？多少 CPU 多少内存？多大硬盘？”这些信息放在 JSON 字符串里面，通过 POST 的方法告诉服务器端。还有一种类型叫 PUT，就是向指定资源位置上传最新内容。但是，HTTP 的服务器往往是不允许上传文件的，所以 PUT 和 POST 就都变成了要传给服务器东西的方法。在实际使用过程中，这两者还会有稍许的区别。POST 往往是用来创建一个资源的，而 PUT 往往是用来修改一个资源的。例如，云主机已经创建好了，我想对这个云主机打一个标签，说明这个云主机是生产环境的，另外一个云主机是测试环境的。那怎么修改这个标签呢？往往就是用 PUT 方法。再有一种常见的就是 DELETE。这个顾名思义就是用来删除资源的。例如，我们要删除一个云主机，就会调用 DELETE 方法。 第二部分：首部字段请求行下面就是我们的首部字段。首部是 key value，通过冒号分隔。这里面，往往保存了一些非常重要的字段。例如，Accept-Charset，表示客户端可以接受的字符集。防止传过来的是另外的字符集，从而导致出现乱码。再如，Content-Type 是指正文的格式。例如，我们进行 POST 的请求，如果正文是 JSON，那么我们就应该将这个值设置为 JSON。这里需要重点说一下的就是缓存。为啥要使用缓存呢？那是因为一个非常大的页面有很多东西。例如，我浏览一个商品的详情，里面有这个商品的价格、库存、展示图片、使用手册等等。商品的展示图片会保持较长时间不变，而库存会根据用户购买的情况经常改变。如果图片非常大，而库存数非常小，如果我们每次要更新数据的时候都要刷新整个页面，对于服务器的压力就会很大。对于这种高并发场景下的系统，在真正的业务逻辑之前，都需要有个接入层，将这些静态资源的请求拦在最外面。这个架构的图就像这样。 其中 DNS、CDN 我在后面的章节会讲。和这一节关系比较大的就是 Nginx 这一层，它如何处理 HTTP 协议呢？对于静态资源，有 Vanish 缓存层。当缓存过期的时候，才会访问真正的 Tomcat 应用集群。在 HTTP 头里面，Cache-control 是用来控制缓存的。当客户端发送的请求中包含 max-age 指令时，如果判定缓存层中，资源的缓存时间数值比指定时间的数值小，那么客户端可以接受缓存的资源；当指定 max-age 值为 0，那么缓存层通常需要将请求转发给应用集群。另外，If-Modified-Since 也是一个关于缓存的。也就是说，如果服务器的资源在某个时间之后更新了，那么客户端就应该下载最新的资源；如果没有更新，服务端会返回“304 Not Modified”的响应，那客户端就不用下载了，也会节省带宽。到此为止，我们仅仅是拼凑起了 HTTP 请求的报文格式，接下来，浏览器会把它交给下一层传输层。怎么交给传输层呢？其实也无非是用 Socket 这些东西，只不过用的浏览器里，这些程序不需要你自己写，有人已经帮你写好了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:20:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"HTTP 请求的发送 HTTP 协议是基于 TCP 协议的，所以它使用面向连接的方式发送请求，通过 stream 二进制流的方式传给对方。当然，到了 TCP 层，它会把二进制流变成一个个报文段发送给服务器。在发送给每个报文段的时候，都需要对方有一个回应 ACK，来保证报文可靠地到达了对方。如果没有回应，那么 TCP 这一层会进行重新传输，直到可以到达。同一个包有可能被传了好多次，但是 HTTP 这一层不需要知道这一点，因为是 TCP 这一层在埋头苦干。TCP 层发送每一个报文的时候，都需要加上自己的地址（即源地址）和它想要去的地方（即目标地址），将这两个信息放到 IP 头里面，交给 IP 层进行传输。IP 层需要查看目标地址和自己是否是在同一个局域网。如果是，就发送 ARP 协议来请求这个目标地址对应的 MAC 地址，然后将源 MAC 和目标 MAC 放入 MAC 头，发送出去即可；如果不在同一个局域网，就需要发送到网关，还要需要发送 ARP 协议，来获取网关的 MAC 地址，然后将源 MAC 和网关 MAC 放入 MAC 头，发送出去。网关收到包发现 MAC 符合，取出目标 IP 地址，根据路由协议找到下一跳的路由器，获取下一跳路由器的 MAC 地址，将包发给下一跳路由器。 这样路由器一跳一跳终于到达目标的局域网。这个时候，最后一跳的路由器能够发现，目标地址就在自己的某一个出口的局域网上。于是，在这个局域网上发送 ARP，获得这个目标地址的 MAC 地址，将包发出去。目标的机器发现 MAC 地址符合，就将包收起来；发现 IP 地址符合，根据 IP 头中协议项，知道自己上一层是 TCP 协议，于是解析 TCP 的头，里面有序列号，需要看一看这个序列包是不是我要的，如果是就放入缓存中然后返回一个 ACK，如果不是就丢弃。TCP 头里面还有端口号，HTTP 的服务器正在监听这个端口号。于是，目标机器自然知道是 HTTP 服务器这个进程想要这个包，于是将包发给 HTTP 服务器。HTTP 服务器的进程看到，原来这个请求是要访问一个网页，于是就把这个网页发给客户端。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:20:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"HTTP 返回的构建 HTTP 的返回报文也是有一定格式的。这也是基于 HTTP 1.1 的。 状态码会反映 HTTP 请求的结果。“200”意味着大吉大利；而我们最不想见的，就是“404”，也就是“服务端无法响应这个请求”。然后，短语会大概说一下原因。接下来是返回首部的 key value。这里面，Retry-After 表示，告诉客户端应该在多长时间以后再次尝试一下。“503 错误”是说“服务暂时不再和这个值配合使用”。在返回的头部里面也会有 Content-Type，表示返回的是 HTML，还是 JSON。构造好了返回的 HTTP 报文，接下来就是把这个报文发送出去。还是交给 Socket 去发送，还是交给 TCP 层，让 TCP 层将返回的 HTML，也分成一个个小的段，并且保证每个段都可靠到达。这些段加上 TCP 头后会交给 IP 层，然后把刚才的发送过程反向走一遍。虽然两次不一定走相同的路径，但是逻辑过程是一样的，一直到达客户端。客户端发现 MAC 地址符合、IP 地址符合，于是就会交给 TCP 层。根据序列号看是不是自己要的报文段，如果是，则会根据 TCP 头中的端口号，发给相应的进程。这个进程就是浏览器，浏览器作为客户端也在监听某个端口。当浏览器拿到了 HTTP 的报文。发现返回“200”，一切正常，于是就从正文中将 HTML 拿出来。HTML 是一个标准的网页格式。浏览器只要根据这个格式，展示出一个绚丽多彩的网页。这就是一个正常的 HTTP 请求和返回的完整过程。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:20:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"HTTP 2.0 当然 HTTP 协议也在不断的进化过程中，在 HTTP1.1 基础上便有了 HTTP 2.0。HTTP 1.1 在应用层以纯文本的形式进行通信。每次通信都要带完整的 HTTP 的头，而且不考虑 pipeline 模式的话，每次的过程总是像上面描述的那样一去一回。这样在实时性、并发性上都存在问题。为了解决这些问题，HTTP 2.0 会对 HTTP 的头进行一定的压缩，将原来每次都要携带的大量 key value 在两端建立一个索引表，对相同的头只发送索引表中的索引。另外，HTTP 2.0 协议将一个 TCP 的连接中，切分成多个流，每个流都有自己的 ID，而且流可以是客户端发往服务端，也可以是服务端发往客户端。它其实只是一个虚拟的通道。流是有优先级的。 HTTP 2.0 还将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。常见的帧有 Header 帧，用于传输 Header 内容，并且会开启一个新的流。再就是 Data 帧，用来传输正文实体。多个 Data 帧属于同一个流。通过这两种机制，HTTP 2.0 的客户端可以将多个请求分到不同的流中，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据每个帧首部的流标识符重新组装，并且可以根据优先级，决定优先处理哪个流的数据。 我们来举一个例子。假设我们的一个页面要发送三个独立的请求，一个获取 css，一个获取 js，一个获取图片 jpg。如果使用 HTTP 1.1 就是串行的，但是如果使用 HTTP 2.0，就可以在一个连接里，客户端和服务端都可以同时发送多个请求或回应，而且不用按照顺序一对一对应。 HTTP 2.0 其实是将三个请求变成三个流，将数据分成帧，乱序发送到一个 TCP 连接中。 HTTP 2.0 成功解决了 HTTP 1.1 的队首阻塞问题，同时，也不需要通过 HTTP 1.x 的 pipeline 机制用多条 TCP 连接来实现并行请求与响应；减少了 TCP 连接数对服务器性能的影响，同时将页面的多个数据 css、js、 jpg 等通过一个数据链接进行传输，能够加快页面组件的传输速度。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:20:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"QUIC 协议的“城会玩” HTTP 2.0 虽然大大增加了并发性，但还是有问题的。因为 HTTP 2.0 也是基于 TCP 协议的，TCP 协议在处理包时是有严格顺序的。当其中一个数据包遇到问题，TCP 连接需要等待这个包完成重传之后才能继续进行。虽然 HTTP 2.0 通过多个 stream，使得逻辑上一个 TCP 连接上的并行内容，进行多路数据的传输，然而这中间并没有关联的数据。一前一后，前面 stream 2 的帧没有收到，后面 stream 1 的帧也会因此阻塞。于是，就又到了从 TCP 切换到 UDP，进行“城会玩”的时候了。这就是 Google 的 QUIC 协议，接下来我们来看它是如何“城会玩”的。 机制一：自定义连接机制 我们都知道，一条 TCP 连接是由四元组标识的，分别是源 IP、源端口、目的 IP、目的端口。一旦一个元素发生变化时，就需要断开重连，重新连接。在移动互联情况下，当手机信号不稳定或者在 WIFI 和 移动网络切换时，都会导致重连，从而进行再次的三次握手，导致一定的时延。这在 TCP 是没有办法的，但是基于 UDP，就可以在 QUIC 自己的逻辑里面维护连接的机制，不再以四元组标识，而是以一个 64 位的随机数作为 ID 来标识，而且 UDP 是无连接的，所以当 IP 或者端口变化的时候，只要 ID 不变，就不需要重新建立连接。 机制二：自定义重传机制 前面我们讲过，TCP 为了保证可靠性，通过使用序号和应答机制，来解决顺序问题和丢包问题。任何一个序号的包发过去，都要在一定的时间内得到应答，否则一旦超时，就会重发这个序号的包。那怎么样才算超时呢？还记得我们提过的自适应重传算法吗？这个超时是通过采样往返时间 RTT 不断调整的。其实，在 TCP 里面超时的采样存在不准确的问题。例如，发送一个包，序号为 100，发现没有返回，于是再发送一个 100，过一阵返回一个 ACK101。这个时候客户端知道这个包肯定收到了，但是往返时间是多少呢？是 ACK 到达的时间减去后一个 100 发送的时间，还是减去前一个 100 发送的时间呢？事实是，第一种算法把时间算短了，第二种算法把时间算长了。QUIC 也有个序列号，是递增的。任何一个序列号的包只发送一次，下次就要加一了。例如，发送一个包，序号是 100，发现没有返回；再次发送的时候，序号就是 101 了；如果返回的 ACK 100，就是对第一个包的响应。如果返回 ACK 101 就是对第二个包的响应，RTT 计算相对准确。但是这里有一个问题，就是怎么知道包 100 和包 101 发送的是同样的内容呢？QUIC 定义了一个 offset 概念。QUIC 既然是面向连接的，也就像 TCP 一样，是一个数据流，发送的数据在这个数据流里面有个偏移量 offset，可以通过 offset 查看数据发送到了哪里，这样只要这个 offset 的包没有来，就要重发；如果来了，按照 offset 拼接，还是能够拼成一个流。 机制三：无阻塞的多路复用 有了自定义的连接和重传机制，我们就可以解决上面 HTTP 2.0 的多路复用问题。同 HTTP 2.0 一样，同一条 QUIC 连接上可以创建多个 stream，来发送多个 HTTP 请求。但是，QUIC 是基于 UDP 的，一个连接上的多个 stream 之间没有依赖。这样，假如 stream2 丢了一个 UDP 包，后面跟着 stream3 的一个 UDP 包，虽然 stream2 的那个包需要重传，但是 stream3 的包无需等待，就可以发给用户。 机制四：自定义流量控制 TCP 的流量控制是通过滑动窗口协议。QUIC 的流量控制也是通过 window_update，来告诉对端它可以接受的字节数。但是 QUIC 的窗口是适应自己的多路复用机制的，不但在一个连接上控制窗口，还在一个连接中的每个 stream 控制窗口。还记得吗？在 TCP 协议中，接收端的窗口的起始点是下一个要接收并且 ACK 的包，即便后来的包都到了，放在缓存里面，窗口也不能右移，因为 TCP 的 ACK 机制是基于序列号的累计应答，一旦 ACK 了一个序列号，就说明前面的都到了，所以只要前面的没到，后面的到了也不能 ACK，就会导致后面的到了，也有可能超时重传，浪费带宽。QUIC 的 ACK 是基于 offset 的，每个 offset 的包来了，进了缓存，就可以应答，应答后就不会重发，中间的空档会等待到来或者重发即可，而窗口的起始位置为当前收到的最大 offset，从这个 offset 到当前的 stream 所能容纳的最大缓存，是真正的窗口大小。显然，这样更加准确。 另外，还有整个连接的窗口，需要对于所有的 stream 的窗口做一个统计。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:20:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，今天就讲到这里，我们来总结一下：HTTP 协议虽然很常用，也很复杂，重点记住 GET、POST、 PUT、DELETE 这几个方法，以及重要的首部字段；HTTP 2.0 通过头压缩、分帧、二进制编码、多路复用等技术提升性能；QUIC 协议通过基于 UDP 自定义的类似 TCP 的连接、重试、多路复用、流量控制技术，进一步提升性能。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:20:7","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第15讲 | HTTPS协议：点外卖的过程原来这么复杂 用 HTTP 协议，看个新闻还没有问题，但是换到更加严肃的场景中，就存在很多的安全风险。例如，你要下单做一次支付，如果还是使用普通的 HTTP 协议，那你很可能会被黑客盯上。你发送一个请求，说我要点个外卖，但是这个网络包被截获了，于是在服务器回复你之前，黑客先假装自己就是外卖网站，然后给你回复一个假的消息说：“好啊好啊，来来来，银行卡号、密码拿来。”如果这时候你真把银行卡密码发给它，那你就真的上套了。那怎么解决这个问题呢？当然一般的思路就是加密。加密分为两种方式一种是对称加密，一种是非对称加密。 在对称加密算法中，加密和解密使用的密钥是相同的。也就是说，加密和解密使用的是同一个密钥。因此，对称加密算法要保证安全性的话，密钥要做好保密。只能让使用的人知道，不能对外公开。在非对称加密算法中，加密使用的密钥和解密使用的密钥是不相同的。一把是作为公开的公钥，另一把是作为谁都不能给的私钥。公钥加密的信息，只有私钥才能解密。私钥加密的信息，只有公钥才能解密。因为对称加密算法相比非对称加密算法来说，效率要高得多，性能也好，所以交互的场景下多用对称加密。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:21:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"对称加密 假设你和外卖网站约定了一个密钥，你发送请求的时候用这个密钥进行加密，外卖网站用同样的密钥进行解密。这样就算中间的黑客截获了你的请求，但是它没有密钥，还是破解不了。这看起来很完美，但是中间有个问题，你们两个怎么来约定这个密钥呢？如果这个密钥在互联网上传输，也是很有可能让黑客截获的。黑客一旦截获这个秘钥，它可以佯作不知，静静地等着你们两个交互。这时候你们之间互通的任何消息，它都能截获并且查看，就等你把银行卡账号和密码发出来。我们在谍战剧里面经常看到这样的场景，就是特工破译的密码会有个密码本，截获无线电台，通过密码本就能将原文破解出来。怎么把密码本给对方呢？只能通过线下传输。比如，你和外卖网站偷偷约定时间地点，它给你一个纸条，上面写着你们两个的密钥，然后说以后就用这个密钥在互联网上定外卖了。当然你们接头的时候，也会先约定一个口号，什么“天王盖地虎”之类的，口号对上了，才能把纸条给它。但是，“天王盖地虎”同样也是对称加密密钥，同样存在如何把“天王盖地虎”约定成口号的问题。而且在谍战剧中一对一接头可能还可以，在互联网应用中，客户太多，这样是不行的。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:21:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"非对称加密 所以，只要是对称加密，就会永远在这个死循环里出不来，这个时候，就需要非对称加密介入进来。非对称加密的私钥放在外卖网站这里，不会在互联网上传输，这样就能保证这个密钥的私密性。但是，对应私钥的公钥，是可以在互联网上随意传播的，只要外卖网站把这个公钥给你，你们就可以愉快地互通了。比如说你用公钥加密，说“我要定外卖”，黑客在中间就算截获了这个报文，因为它没有私钥也是解不开的，所以这个报文可以顺利到达外卖网站，外卖网站用私钥把这个报文解出来，然后回复，“那给我银行卡和支付密码吧”。先别太乐观，这里还是有问题的。回复的这句话，是外卖网站拿私钥加密的，互联网上人人都可以把它打开，当然包括黑客。那外卖网站可以拿公钥加密吗？当然不能，因为它自己的私钥只有它自己知道，谁也解不开。另外，这个过程还有一个问题，黑客也可以模拟发送“我要定外卖”这个过程的，因为它也有外卖网站的公钥。为了解决这个问题，看来一对公钥私钥是不够的，客户端也需要有自己的公钥和私钥，并且客户端要把自己的公钥，给外卖网站。这样，客户端给外卖网站发送的时候，用外卖网站的公钥加密。而外卖网站给客户端发送消息的时候，使用客户端的公钥。这样就算有黑客企图模拟客户端获取一些信息，或者半路截获回复信息，但是由于它没有私钥，这些信息它还是打不开。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:21:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"数字证书 不对称加密也会有同样的问题，如何将不对称加密的公钥给对方呢？一种是放在一个公网的地址上，让对方下载；另一种就是在建立连接的时候，传给对方。这两种方法有相同的问题，那就是，作为一个普通网民，你怎么鉴别别人给你的公钥是对的。会不会有人冒充外卖网站，发给你一个它的公钥。接下来，你和它所有的互通，看起来都是没有任何问题的。毕竟每个人都可以创建自己的公钥和私钥。例如，我自己搭建了一个网站 cliu8site，可以通过这个命令先创建私钥。 openssl genrsa -out cliu8siteprivate.key 1024 然后，再根据这个私钥，创建对应的公钥。 openssl rsa -in cliu8siteprivate.key -pubout -outcliu8sitepublic.pem 这个时候就需要权威部门的介入了，就像每个人都可以打印自己的简历，说自己是谁，但是有公安局盖章的，就只有户口本，这个才能证明你是你。这个由权威部门颁发的称为证书（Certificate）。证书里面有什么呢？当然应该有公钥，这是最重要的；还有证书的所有者，就像户口本上有你的姓名和身份证号，说明这个户口本是你的；另外还有证书的发布机构和证书的有效期，这个有点像身份证上的机构是哪个区公安局，有效期到多少年。这个证书是怎么生成的呢？会不会有人假冒权威机构颁发证书呢？就像有假身份证、假户口本一样。生成证书需要发起一个证书请求，然后将这个请求发给一个权威机构去认证，这个权威机构我们称为 CA（ Certificate Authority）。证书请求可以通过这个命令生成。 openssl req -key cliu8siteprivate.key -new -out cliu8sitecertificate.req 将这个请求发给权威机构，权威机构会给这个证书卡一个章，我们称为签名算法。问题又来了，那怎么签名才能保证是真的权威机构签名的呢？当然只有用只掌握在权威机构手里的东西签名了才行，这就是 CA 的私钥。签名算法大概是这样工作的：一般是对信息做一个 Hash 计算，得到一个 Hash 值，这个过程是不可逆的，也就是说无法通过 Hash 值得出原来的信息内容。在把信息发送出去时，把这个 Hash 值加密后，作为一个签名和信息一起发出去。权威机构给证书签名的命令是这样的。 openssl x509 -req -in cliu8sitecertificate.req -CA cacertificate.pem -CAkey caprivate.key -out cliu8sitecertificate.pem 这个命令会返回 Signature ok，而 cliu8sitecertificate.pem 就是签过名的证书。CA 用自己的私钥给外卖网站的公钥签名，就相当于给外卖网站背书，形成了外卖网站的证书。我们来查看这个证书的内容。 openssl x509 -in cliu8sitecertificate.pem -noout -text 这里面有个 Issuer，也即证书是谁颁发的；Subject，就是证书颁发给谁；Validity 是证书期限；Public-key 是公钥内容；Signature Algorithm 是签名算法。这下好了，你不会从外卖网站上得到一个公钥，而是会得到一个证书，这个证书有个发布机构 CA，你只要得到这个发布机构 CA 的公钥，去解密外卖网站证书的签名，如果解密成功了，Hash 也对的上，就说明这个外卖网站的公钥没有啥问题。你有没有发现，又有新问题了。要想验证证书，需要 CA 的公钥，问题是，你怎么确定 CA 的公钥就是对的呢？所以，CA 的公钥也需要更牛的 CA 给它签名，然后形成 CA 的证书。要想知道某个 CA 的证书是否可靠，要看 CA 的上级证书的公钥，能不能解开这个 CA 的签名。就像你不相信区公安局，可以打电话问市公安局，让市公安局确认区公安局的合法性。这样层层上去，直到全球皆知的几个著名大 CA，称为 root CA，做最后的背书。通过这种层层授信背书的方式，从而保证了非对称加密模式的正常运转。 除此之外，还有一种证书，称为 Self-Signed Certificate，就是自己给自己签名。这个给人一种“我就是我，你爱信不信”的感觉。这里我就不多说了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:21:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"HTTPS 的工作模式 我们可以知道，非对称加密在性能上不如对称加密，那是否能将两者结合起来呢？例如，公钥私钥主要用于传输对称加密的秘钥，而真正的双方大数据量的通信都是通过对称加密进行的。当然是可以的。这就是 HTTPS 协议的总体思路。 当你登录一个外卖网站的时候，由于是 HTTPS，客户端会发送 Client Hello 消息到服务器，以明文传输 TLS 版本信息、加密套件候选列表、压缩算法候选列表等信息。另外，还会有一个随机数，在协商对称密钥的时候使用。这就类似在说：“您好，我想定外卖，但你要保密我吃的是什么。这是我的加密套路，再给你个随机数，你留着。”然后，外卖网站返回 Server Hello 消息, 告诉客户端，服务器选择使用的协议版本、加密套件、压缩算法等，还有一个随机数，用于后续的密钥协商。这就类似在说：“您好，保密没问题，你的加密套路还挺多，咱们就按套路 2 来吧，我这里也有个随机数，你也留着。”然后，外卖网站会给你一个服务器端的证书，然后说：“Server Hello Done，我这里就这些信息了。”你当然不相信这个证书，于是你从自己信任的 CA 仓库中，拿 CA 的证书里面的公钥去解密外卖网站的证书。如果能够成功，则说明外卖网站是可信的。这个过程中，你可能会不断往上追溯 CA、CA 的 CA、CA 的 CA 的 CA，反正直到一个授信的 CA，就可以了。证书验证完毕之后，觉得这个外卖网站可信，于是客户端计算产生随机数字 Pre-master，发送 Client Key Exchange，用证书中的公钥加密，再发送给服务器，服务器可以通过私钥解密出来。到目前为止，无论是客户端还是服务器，都有了三个随机数，分别是：自己的、对端的，以及刚生成的 Pre-Master 随机数。通过这三个随机数，可以在客户端和服务器产生相同的对称密钥。有了对称密钥，客户端就可以说：“Change Cipher Spec，咱们以后都采用协商的通信密钥和加密算法进行加密通信了。”然后发送一个 Encrypted Handshake Message，将已经商定好的参数等，采用协商密钥进行加密，发送给服务器用于数据与握手验证。同样，服务器也可以发送 Change Cipher Spec，说：“没问题，咱们以后都采用协商的通信密钥和加密算法进行加密通信了”，并且也发送 Encrypted Handshake Message 的消息试试。当双方握手结束之后，就可以通过对称密钥进行加密传输了。这个过程除了加密解密之外，其他的过程和 HTTP 是一样的，过程也非常复杂。上面的过程只包含了 HTTPS 的单向认证，也即客户客户端验证服务端的证书，是大部分的场景，也可以在更加严格安全要求的情况下，启用双向认证，双方互相验证证书。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:21:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"重放与篡改 其实，这里还有一些没有解决的问题，例如重放和篡改的问题。没错，有了加密和解密，黑客截获了包也打不开了，但是它可以发送 N 次。这个往往通过 Timestamp 和 Nonce 随机数联合起来，然后做一个不可逆的签名来保证。Nonce 随机数保证唯一，或者 Timestamp 和 Nonce 合起来保证唯一，同样的，请求只接受一次，于是服务器多次收到相同的 Timestamp 和 Nonce，则视为无效即可。如果有人想篡改 Timestamp 和 Nonce，还有签名保证不可篡改性，如果改了用签名算法解出来，就对不上了，可以丢弃了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:21:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下。加密分对称加密和非对称加密。对称加密效率高，但是解决不了密钥传输问题；非对称加密可以解决这个问题，但是效率不高。 非对称加密需要通过证书和权威机构来验证公钥的合法性。HTTPS 是综合了对称加密和非对称加密算法的 HTTP 协议。既保证传输安全，也保证传输效率。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:21:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第16讲 | 流媒体协议：如何在直播里看到美女帅哥？ 最近直播比较火，很多人都喜欢看直播，那一个直播系统里面都有哪些组成部分，都使用了什么协议呢？无论是直播还是点播，其实都是对于视频数据的传输。一提到视频，大家都爱看，但是一提到视频技术，大家都头疼，因为名词实在是太多了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:22:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"三个名词系列 我这里列三个名词系列，你先大致有个印象。名词系列一：AVI、MPEG、RMVB、MP4、MOV、FLV、WebM、WMV、ASF、MKV。例如 RMVB 和 MP4，看着是不是很熟悉？名词系列二：H.261、 H.262、H.263、H.264、H.265。这个是不是就没怎么听过了？别着急，你先记住，要重点关注 H.264。名词系列三：MPEG-1、MPEG-2、MPEG-4、MPEG-7。MPEG 好像听说过，但是后面的数字是怎么回事？是不是又熟悉又陌生？ 这里，我想问你个问题，视频是什么？我说，其实就是快速播放一连串连续的图片。每一张图片，我们称为一帧。只要每秒钟帧的数据足够多，也即播放得足够快。比如每秒 30 帧，以人的眼睛的敏感程度，是看不出这是一张张独立的图片的，这就是我们常说的帧率（FPS）。每一张图片，都是由像素组成的，假设为 1024*768（这个像素数不算多）。每个像素由 RGB 组成，每个 8 位，共 24 位。我们来算一下，每秒钟的视频有多大？ 30 帧 × 1024 × 768 × 24 = 566,231,040Bits = 70,778,880Bytes如果一分钟呢？4,246,732,800Bytes，已经是 4 个 G 了。是不是不算不知道，一算吓一跳？这个数据量实在是太大，根本没办法存储和传输。如果这样存储，你的硬盘很快就满了；如果这样传输，那多少带宽也不够用啊！怎么办呢？人们想到了编码，就是看如何用尽量少的 Bit 数保存视频，使播放的时候画面看起来仍然很精美。编码是一个压缩的过程。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:22:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"视频和图片的压缩过程有什么特点？ 之所以能够对视频流中的图片进行压缩，因为视频和图片有这样一些特点。空间冗余：图像的相邻像素之间有较强的相关性，一张图片相邻像素往往是渐变的，不是突变的，没必要每个像素都完整地保存，可以隔几个保存一个，中间的用算法计算出来。时间冗余：视频序列的相邻图像之间内容相似。一个视频中连续出现的图片也不是突变的，可以根据已有的图片进行预测和推断。视觉冗余：人的视觉系统对某些细节不敏感，因此不会每一个细节都注意到，可以允许丢失一些数据。编码冗余：不同像素值出现的概率不同，概率高的用的字节少，概率低的用的字节多，类似霍夫曼编码（Huffman Coding）的思路。 总之，用于编码的算法非常复杂，而且多种多样，但是编码过程其实都是类似的。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:22:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"视频编码的两大流派 能不能形成一定的标准呢？要不然开发视频播放的人得累死了。当然能，我这里就给你介绍，视频编码的两大流派。流派一：ITU（International Telecommunications Union）的 VCEG（Video Coding Experts Group），这个称为国际电联下的 VCEG。既然是电信，可想而知，他们最初做视频编码，主要侧重传输。名词系列二，就是这个组织制定的标准。流派二：ISO（International Standards Organization）的 MPEG（Moving Picture Experts Group），这个是 ISO 旗下的 MPEG，本来是做视频存储的。例如，编码后保存在 VCD 和 DVD 中。当然后来也慢慢侧重视频传输了。名词系列三，就是这个组织制定的标准。 后来，ITU-T（国际电信联盟电信标准化部门，ITU Telecommunication Standardization Sector）与 MPEG 联合制定了 H.264/MPEG-4 AVC，这才是我们这一节要重点关注的。经过编码之后，生动活泼的一帧一帧的图像，就变成了一串串让人看不懂的二进制，这个二进制可以放在一个文件里面，按照一定的格式保存起来，这就是名词系列一。其实这些就是视频保存成文件的格式。例如，前几个字节是什么意义，后几个字节是什么意义，然后是数据，数据中保存的就是编码好的结果。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:22:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何在直播里看到帅哥美女？ 当然，这个二进制也可以通过某种网络协议进行封装，放在互联网上传输，这个时候就可以进行网络直播了。网络协议将编码好的视频流，从主播端推送到服务器，在服务器上有个运行了同样协议的服务端来接收这些网络包，从而得到里面的视频流，这个过程称为接流。服务端接到视频流之后，可以对视频流进行一定的处理，例如转码，也即从一个编码格式，转成另一种格式。因为观众使用的客户端千差万别，要保证他们都能看到直播。流处理完毕之后，就可以等待观众的客户端来请求这些视频流。观众的客户端请求的过程称为拉流。 如果有非常多的观众，同时看一个视频直播，那都从一个服务器上拉流，压力太大了，因而需要一个视频的分发网络，将视频预先加载到就近的边缘节点，这样大部分观众看的视频，是从边缘节点拉取的，就能降低服务器的压力。当观众的客户端将视频流拉下来之后，就需要进行解码，也即通过上述过程的逆过程，将一串串看不懂的二进制，再转变成一帧帧生动的图片，在客户端播放出来，这样你就能看到美女帅哥啦。整个直播过程，可以用这个的图来描述。 接下来，我们依次来看一下每个过程。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:22:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"编码：如何将丰富多彩的图片变成二进制流？ 虽然我们说视频是一张张图片的序列，但是如果每张图片都完整，就太大了，因而会将视频序列分成三种帧。I 帧，也称关键帧。里面是完整的图片，只需要本帧数据，就可以完成解码。P 帧，前向预测编码帧。P 帧表示的是这一帧跟之前的一个关键帧（或 P 帧）的差别，解码时需要用之前缓存的画面，叠加上和本帧定义的差别，生成最终画面。B 帧，双向预测内插编码帧。B 帧记录的是本帧与前后帧的差别。要解码 B 帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的数据与本帧数据的叠加，取得最终的画面。 可以看出，I 帧最完整，B 帧压缩率最高，而压缩后帧的序列，应该是在 IBBP 的间隔出现的。这就是通过时序进行编码。 在一帧中，分成多个片，每个片中分成多个宏块，每个宏块分成多个子块，这样将一张大的图分解成一个个小块，可以方便进行空间上的编码。尽管时空非常立体地组成了一个序列，但是总归还是要压缩成一个二进制流。这个流是有结构的，是一个个的网络提取层单元（NALU，Network Abstraction Layer Unit）。变成这种格式就是为了传输，因为网络上的传输，默认的是一个个的包，因而这里也就分成了一个个的单元。 每一个 NALU 首先是一个起始标识符，用于标识 NALU 之间的间隔；然后是 NALU 的头，里面主要配置了 NALU 的类型；最终 Payload 里面是 NALU 承载的数据。在 NALU 头里面，主要的内容是类型 NAL Type。0x07 表示 SPS，是序列参数集， 包括一个图像序列的所有信息，如图像尺寸、视频格式等。0x08 表示 PPS，是图像参数集，包括一个图像的所有分片的所有相关信息，包括图像类型、序列号等。 在传输视频流之前，必须要传输这两类参数，不然无法解码。为了保证容错性，每一个 I 帧前面，都会传一遍这两个参数集合。如果 NALU Header 里面的表示类型是 SPS 或者 PPS，则 Payload 中就是真正的参数集的内容。如果类型是帧，则 Payload 中才是正的视频数据，当然也是一帧一帧存放的，前面说了，一帧的内容还是挺多的，因而每一个 NALU 里面保存的是一片。对于每一片，到底是 I 帧，还是 P 帧，还是 B 帧，在片结构里面也有个 Header，这里面有个类型，然后是片的内容。这样，整个格式就出来了，一个视频，可以拆分成一系列的帧，每一帧拆分成一系列的片，每一片都放在一个 NALU 里面，NALU 之间都是通过特殊的起始标识符分隔，在每一个 I 帧的第一片前面，要插入单独保存 SPS 和 PPS 的 NALU，最终形成一个长长的 NALU 序列。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:22:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"推流：如何把数据流打包传输到对端？ 那这个格式是不是就能够直接在网上传输到对端，开始直播了呢？其实还不是，还需要将这个二进制的流打包成网络包进行发送，这里我们使用 RTMP 协议。这就进入了第二个过程，推流。RTMP 是基于 TCP 的，因而肯定需要双方建立一个 TCP 的连接。在有 TCP 的连接的基础上，还需要建立一个 RTMP 的连接，也即在程序里面，你需要调用 RTMP 类库的 Connect 函数，显示创建一个连接。RTMP 为什么需要建立一个单独的连接呢？ 因为它们需要商量一些事情，保证以后的传输能正常进行。主要就是两个事情，一个是版本号，如果客户端、服务器的版本号不一致，则不能工作。另一个就是时间戳，视频播放中，时间是很重要的，后面的数据流互通的时候，经常要带上时间戳的差值，因而一开始双方就要知道对方的时间戳。未来沟通这些事情，需要发送六条消息：客户端发送 C0、C1、 C2，服务器发送 S0、 S1、 S2。首先，客户端发送 C0 表示自己的版本号，不必等对方的回复，然后发送 C1 表示自己的时间戳。服务器只有在收到 C0 的时候，才能返回 S0，表明自己的版本号，如果版本不匹配，可以断开连接。服务器发送完 S0 后，也不用等什么，就直接发送自己的时间戳 S1。客户端收到 S1 的时候，发一个知道了对方时间戳的 ACK C2。同理服务器收到 C1 的时候，发一个知道了对方时间戳的 ACK S2。于是，握手完成。 握手之后，双方需要互相传递一些控制信息，例如 Chunk 块的大小、窗口大小等。真正传输数据的时候，还是需要创建一个流 Stream，然后通过这个 Stream 来推流 publish。推流的过程，就是将 NALU 放在 Message 里面发送，这个也称为 RTMP Packet 包。Message 的格式就像这样。 发送的时候，去掉 NALU 的起始标识符。因为这部分对于 RTMP 协议来讲没有用。接下来，将 SPS 和 PPS 参数集封装成一个 RTMP 包发送，然后发送一个个片的 NALU。RTMP 在收发数据的时候并不是以 Message 为单位的，而是把 Message 拆分成 Chunk 发送，而且必须在一个 Chunk 发送完成之后，才能开始发送下一个 Chunk。每个 Chunk 中都带有 Message ID，表示属于哪个 Message，接收端也会按照这个 ID 将 Chunk 组装成 Message。前面连接的时候，设置的 Chunk 块大小就是指这个 Chunk。将大的消息变为小的块再发送，可以在低带宽的情况下，减少网络拥塞。 这有一个分块的例子，你可以看一下。假设一个视频的消息长度为 307，但是 Chunk 大小约定为 128，于是会拆分为三个 Chunk。第一个 Chunk 的 Type＝0，表示 Chunk 头是完整的；头里面 Timestamp 为 1000，总长度 Length 为 307，类型为 9，是个视频，Stream ID 为 12346，正文部分承担 128 个字节的 Data。第二个 Chunk 也要发送 128 个字节，Chunk 头由于和第一个 Chunk 一样，因此采用 Chunk Type＝3，表示头一样就不再发送了。第三个 Chunk 要发送的 Data 的长度为 307-128-128=51 个字节，还是采用 Type＝3。 就这样数据就源源不断到达流媒体服务器，整个过程就像这样。 这个时候，大量观看直播的观众就可以通过 RTMP 协议从流媒体服务器上拉取，但是这么多的用户量，都去同一个地方拉取，服务器压力会很大，而且用户分布在全国甚至全球，如果都去统一的一个地方下载，也会时延比较长，需要有分发网络。分发网络分为中心和边缘两层。边缘层服务器部署在全国各地及横跨各大运营商里，和用户距离很近。中心层是流媒体服务集群，负责内容的转发。智能负载均衡系统，根据用户的地理位置信息，就近选择边缘服务器，为用户提供推 / 拉流服务。中心层也负责转码服务，例如，把 RTMP 协议的码流转换为 HLS 码流。 这套机制在后面的 DNS、HTTPDNS、CDN 的章节会更有详细的描述。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:22:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"拉流：观众的客户端如何看到视频？ 接下来，我们再来看观众的客户端通过 RTMP 拉流的过程。 先读到的是 H.264 的解码参数，例如 SPS 和 PPS，然后对收到的 NALU 组成的一个个帧，进行解码，交给播发器播放，一个绚丽多彩的视频画面就出来了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:22:7","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，今天的内容就到这里了，我们来总结一下： 视频名词比较多，编码两大流派达成了一致，都是通过时间、空间的各种算法来压缩数据；压缩好的数据，为了传输组成一系列 NALU，按照帧和片依次排列；排列好的 NALU，在网络传输的时候，要按照 RTMP 包的格式进行包装，RTMP 的包会拆分成 Chunk 进行传输；推送到流媒体集群的视频流经过转码和分发，可以被客户端通过 RTMP 协议拉取，然后组合为 NALU，解码成视频格式进行播放。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:22:8","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第17讲 | P2P协议：我下小电影，99%急死你 如果你想下载一个电影，一般会通过什么方式呢？当然，最简单的方式就是通过 HTTP 进行下载。但是相信你有过这样的体验，通过浏览器下载的时候，只要文件稍微大点，下载的速度就奇慢无比。还有种下载文件的方式，就是通过 FTP，也即文件传输协议。FTP 采用两个 TCP 连接来传输一个文件。 控制连接：服务器以被动的方式，打开众所周知用于 FTP 的端口 21，客户端则主动发起连接。该连接将命令从客户端传给服务器，并传回服务器的应答。常用的命令有：list——获取文件目录；reter——取一个文件；store——存一个文件。数据连接：每当一个文件在客户端与服务器之间传输时，就创建一个数据连接。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:23:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"FTP 的两种工作模式 每传输一个文件，都要建立一个全新的数据连接。FTP 有两种工作模式，分别是主动模式（PORT）和被动模式（PASV），这些都是站在 FTP 服务器的角度来说的。主动模式下，客户端随机打开一个大于 1024 的端口 N，向服务器的命令端口 21 发起连接，同时开放 N+1 端口监听，并向服务器发出 “port N+1” 命令，由服务器从自己的数据端口 20，主动连接到客户端指定的数据端口 N+1。被动模式下，当开启一个 FTP 连接时，客户端打开两个任意的本地端口 N（大于 1024）和 N+1。第一个端口连接服务器的 21 端口，提交 PASV 命令。然后，服务器会开启一个任意的端口 P（大于 1024），返回“227 entering passive mode”消息，里面有 FTP 服务器开放的用来进行数据传输的端口。客户端收到消息取得端口号之后，会通过 N+1 号端口连接服务器的端口 P，然后在两个端口之间进行数据传输。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:23:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"P2P 是什么？ 但是无论是 HTTP 的方式，还是 FTP 的方式，都有一个比较大的缺点，就是难以解决单一服务器的带宽压力， 因为它们使用的都是传统的客户端服务器的方式。后来，一种创新的、称为 P2P 的方式流行起来。P2P 就是 peer-to-peer。资源开始并不集中地存储在某些设备上，而是分散地存储在多台设备上。这些设备我们姑且称为 peer。 想要下载一个文件的时候，你只要得到那些已经存在了文件的 peer，并和这些 peer 之间，建立点对点的连接，而不需要到中心服务器上，就可以就近下载文件。一旦下载了文件，你也就成为 peer 中的一员，你旁边的那些机器，也可能会选择从你这里下载文件，所以当你使用 P2P 软件的时候，例如 BitTorrent，往往能够看到，既有下载流量，也有上传的流量，也即你自己也加入了这个 P2P 的网络，自己从别人那里下载，同时也提供给其他人下载。可以想象，这种方式，参与的人越多，下载速度越快，一切完美。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:23:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"种子（.torrent）文件 但是有一个问题，当你想下载一个文件的时候，怎么知道哪些 peer 有这个文件呢？这就用到种子啦，也即咱们比较熟悉的.torrent 文件。.torrent 文件由两部分组成，分别是：announce（tracker URL）和文件信息。文件信息里面有这些内容。 info 区：这里指定的是该种子有几个文件、文件有多长、目录结构，以及目录和文件的名字。Name 字段：指定顶层目录名字。每个段的大小：BitTorrent（简称 BT）协议把一个文件分成很多个小段，然后分段下载。段哈希值：将整个种子中，每个段的 SHA-1 哈希值拼在一起。 下载时，BT 客户端首先解析.torrent 文件，得到 tracker 地址，然后连接 tracker 服务器。tracker 服务器回应下载者的请求，将其他下载者（包括发布者）的 IP 提供给下载者。下载者再连接其他下载者，根据.torrent 文件，两者分别对方告知自己已经有的块，然后交换对方没有的数据。此时不需要其他服务器参与，并分散了单个线路上的数据流量，因此减轻了服务器的负担。下载者每得到一个块，需要算出下载块的 Hash 验证码，并与.torrent 文件中的对比。如果一样，则说明块正确，不一样则需要重新下载这个块。这种规定是为了解决下载内容的准确性问题。从这个过程也可以看出，这种方式特别依赖 tracker。tracker 需要收集下载者信息的服务器，并将此信息提供给其他下载者，使下载者们相互连接起来，传输数据。虽然下载的过程是非中心化的，但是加入这个 P2P 网络的时候，都需要借助 tracker 中心服务器，这个服务器是用来登记有哪些用户在请求哪些资源。所以，这种工作方式有一个弊端，一旦 tracker 服务器出现故障或者线路遭到屏蔽，BT 工具就无法正常工作了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:23:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"去中心化网络（DHT） 那能不能彻底非中心化呢？于是，后来就有了一种叫作 DHT（Distributed Hash Table）的去中心化网络。每个加入这个 DHT 网络的人，都要负责存储这个网络里的资源信息和其他成员的联系信息，相当于所有人一起构成了一个庞大的分布式存储数据库。有一种著名的 DHT 协议，叫 Kademlia 协议。这个和区块链的概念一样，很抽象，我来详细讲一下这个协议。任何一个 BitTorrent 启动之后，它都有两个角色。一个是 peer，监听一个 TCP 端口，用来上传和下载文件，这个角色表明，我这里有某个文件。另一个角色 DHT node，监听一个 UDP 的端口，通过这个角色，这个节点加入了一个 DHT 的网络。 在 DHT 网络里面，每一个 DHT node 都有一个 ID。这个 ID 是一个很长的串。每个 DHT node 都有责任掌握一些知识，也就是文件索引，也即它应该知道某些文件是保存在哪些节点上。它只需要有这些知识就可以了，而它自己本身不一定就是保存这个文件的节点。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:23:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"哈希值 当然，每个 DHT node 不会有全局的知识，也即不知道所有的文件保存在哪里，它只需要知道一部分。那应该知道哪一部分呢？这就需要用哈希算法计算出来。每个文件可以计算出一个哈希值，而 DHT node 的 ID 是和哈希值相同长度的串。DHT 算法是这样规定的：如果一个文件计算出一个哈希值，则和这个哈希值一样的那个 DHT node，就有责任知道从哪里下载这个文件，即便它自己没保存这个文件。当然不一定这么巧，总能找到和哈希值一模一样的，有可能一模一样的 DHT node 也下线了，所以 DHT 算法还规定：除了一模一样的那个 DHT node 应该知道，ID 和这个哈希值非常接近的 N 个 DHT node 也应该知道。什么叫和哈希值接近呢？例如只修改了最后一位，就很接近；修改了倒数 2 位，也不远；修改了倒数 3 位，也可以接受。总之，凑齐了规定的 N 这个数就行。刚才那个图里，文件 1 通过哈希运算，得到匹配 ID 的 DHT node 为 node C，当然还会有其他的，我这里没有画出来。所以，node C 有责任知道文件 1 的存放地址，虽然 node C 本身没有存放文件 1。同理，文件 2 通过哈希运算，得到匹配 ID 的 DHT node 为 node E，但是 node D 和 E 的 ID 值很近，所以 node D 也知道。当然，文件 2 本身没有必要一定在 node D 和 E 里，但是碰巧这里就在 E 那有一份。 接下来一个新的节点 node new 上线了。如果想下载文件 1，它首先要加入 DHT 网络，如何加入呢？在这种模式下，种子.torrent 文件里面就不再是 tracker 的地址了，而是一个 list 的 node 的地址，而所有这些 node 都是已经在 DHT 网络里面的。当然随着时间的推移，很可能有退出的，有下线的，但是我们假设，不会所有的都联系不上，总有一个能联系上。node new 只要在种子里面找到一个 DHT node，就加入了网络。node new 会计算文件 1 的哈希值，并根据这个哈希值了解到，和这个哈希值匹配，或者很接近的 node 上知道如何下载这个文件，例如计算出来的哈希值就是 node C。但是 node new 不知道怎么联系上 node C，因为种子里面的 node 列表里面很可能没有 node C，但是它可以问，DHT 网络特别像一个社交网络，node new 只有去它能联系上的 node 问，你们知道不知道 node C 的联系方式呀？在 DHT 网络中，每个 node 都保存了一定的联系方式，但是肯定没有 node 的所有联系方式。DHT 网络中，节点之间通过互相通信，也会交流联系方式，也会删除联系方式。和人们的方式一样，你有你的朋友圈，你的朋友有它的朋友圈，你们互相加微信，就互相认识了，过一段时间不联系，就删除朋友关系。 有个理论是，社交网络中，任何两个人直接的距离不超过六度，也即你想联系比尔盖茨，也就六个人就能够联系到了。所以，node new 想联系 node C，就去万能的朋友圈去问，并且求转发，朋友再问朋友，很快就能找到。如果找不到 C，也能找到和 C 的 ID 很像的节点，它们也知道如何下载文件 1。在 node C 上，告诉 node new，下载文件 1，要去 B、D、 F，于是 node new 选择和 node B 进行 peer 连接，开始下载，它一旦开始下载，自己本地也有文件 1 了，于是 node new 告诉 node C 以及和 node C 的 ID 很像的那些节点，我也有文件 1 了，可以加入那个文件拥有者列表了。但是你会发现 node new 上没有文件索引，但是根据哈希算法，一定会有某些文件的哈希值是和 node new 的 ID 匹配上的。在 DHT 网络中，会有节点告诉它，你既然加入了咱们这个网络，你也有责任知道某些文件的下载地址。 好了，一切都分布式了。这里面遗留几个细节的问题。 DHT node ID 以及文件哈希是个什么东西？节点 ID 是一个随机选择的 160bits（20 字节）空间，文件的哈希也使用这样的 160bits 空间。所谓 ID 相似，具体到什么程度算相似？在 Kademlia 网络中，距离是通过异或（XOR）计算的。我们就不以 160bits 举例了。我们以 5 位来举例。01010 与 01000 的距离，就是两个 ID 之间的异或值，为 00010，也即为 2。 01010 与 00010 的距离为 01000，也即为 8,。01010 与 00011 的距离为 01001，也即 8+1=9 。以此类推，高位不同的，表示距离更远一些；低位不同的，表示距离更近一些，总的距离为所有的不同的位的距离之和。这个距离不能比喻为地理位置，因为在 Kademlia 网络中，位置近不算近，ID 近才算近，所以我把这个距离比喻为社交距离，也即在朋友圈中的距离，或者社交网络中的距离。这个和你住的位置没有关系，和人的经历关系比较大。还是以 5 位 ID 来举例，就像在领英中，排第一位的表示最近一份工作在哪里，第二位的表示上一份工作在哪里，然后第三位的是上上份工作，第四位的是研究生在哪里读，第五位的表示大学在哪里读。如果你是一个猎头，在上面找候选人，当然最近的那份工作是最重要的。而对于工作经历越丰富的候选人，大学在哪里读的反而越不重要。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:23:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"DHT 网络中的朋友圈是怎么维护的？ 就像人一样，虽然我们常联系人的只有少数，但是朋友圈里肯定是远近都有。DHT 网络的朋友圈也是一样，远近都有，并且按距离分层。假设某个节点的 ID 为 01010，如果一个节点的 ID，前面所有位数都与它相同，只有最后 1 位不同。这样的节点只有 1 个，为 01011。与基础节点的异或值为 00001，即距离为 1；对于 01010 而言，这样的节点归为“k-bucket 1”。如果一个节点的 ID，前面所有位数都相同，从倒数第 2 位开始不同，这样的节点只有 2 个，即 01000 和 01001，与基础节点的异或值为 00010 和 00011，即距离范围为 2 和 3；对于 01010 而言，这样的节点归为“k-bucket 2”。如果一个节点的 ID，前面所有位数相同，从倒数第 i 位开始不同，这样的节点只有 2^(i-1) 个，与基础节点的距离范围为[2^(i-1), 2^i)；对于 01010 而言，这样的节点归为“k-bucket i”。 最终到从倒数 160 位就开始都不同。你会发现，差距越大，陌生人越多，但是朋友圈不能都放下，所以每一层都只放 K 个，这是参数可以配置。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:23:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"DHT 网络是如何查找朋友的？ 假设，node A 的 ID 为 00110，要找 node B ID 为 10000，异或距离为 10110，距离范围在[2^4, 2^5)，所以这个目标节点可能在“k-bucket 5”中，这就说明 B 的 ID 与 A 的 ID 从第 5 位开始不同，所以 B 可能在“k-bucket 5”中。然后，A 看看自己的 k-bucket 5 有没有 B。如果有，太好了，找到你了；如果没有，在 k-bucket 5 里随便找一个 C。因为是二进制，C、B 都和 A 的第 5 位不同，那么 C 的 ID 第 5 位肯定与 B 相同，即它与 B 的距离会小于 2^4，相当于比 A、B 之间的距离缩短了一半以上。再请求 C，在它自己的通讯录里，按同样的查找方式找一下 B。如果 C 知道 B，就告诉 A；如果 C 也不知道 B，那 C 按同样的搜索方法，可以在自己的通讯录里找到一个离 B 更近的 D 朋友（D、B 之间距离小于 2^3），把 D 推荐给 A，A 请求 D 进行下一步查找。Kademlia 的这种查询机制，是通过折半查找的方式来收缩范围，对于总的节点数目为 N，最多只需要查询 log2(N) 次，就能够找到。 例如，图中这个最差的情况。 A 和 B 每一位都不一样，所以相差 31，A 找到的朋友 C，不巧正好在中间。和 A 的距离是 16，和 B 距离为 15，于是 C 去自己朋友圈找的时候，不巧找到 D，正好又在中间，距离 C 为 8，距离 B 为 7。于是 D 去自己朋友圈找的时候，不巧找到 E，正好又在中间，距离 D 为 4，距离 B 为 3，E 在朋友圈找到 F，距离 E 为 2，距离 B 为 1，最终在 F 的朋友圈距离 1 的地方找到 B。当然这是最最不巧的情况，每次找到的朋友都不远不近，正好在中间。如果碰巧了，在 A 的朋友圈里面有 G，距离 B 只有 3，然后在 G 的朋友圈里面一下子就找到了 B，两次就找到了。 在 DHT 网络中，朋友之间怎么沟通呢？ Kademlia 算法中，每个节点只有 4 个指令。PING：测试一个节点是否在线，还活着没，相当于打个电话，看还能打通不。STORE：要求一个节点存储一份数据，既然加入了组织，有义务保存一份数据。FIND_NODE：根据节点 ID 查找一个节点，就是给一个 160 位的 ID，通过上面朋友圈的方式找到那个节点。FIND_VALUE：根据 KEY 查找一个数据，实则上跟 FIND_NODE 非常类似。KEY 就是文件对应的 160 位的 ID，就是要找到保存了文件的节点。 DHT 网络中，朋友圈如何更新呢？ 每个 bucket 里的节点，都按最后一次接触的时间倒序排列，这就相当于，朋友圈里面最近联系过的人往往是最熟的。每次执行四个指令中的任意一个都会触发更新。当一个节点与自己接触时，检查它是否已经在 k-bucket 中，也就是说是否已经在朋友圈。如果在，那么将它挪到 k-bucket 列表的最底，也就是最新的位置，刚联系过，就置顶一下，方便以后多联系；如果不在，新的联系人要不要加到通讯录里面呢？假设通讯录已满的情况，PING 一下列表最上面，也即最旧的一个节点。如果 PING 通了，将旧节点挪到列表最底，并丢弃新节点，老朋友还是留一下；如果 PING 不通，删除旧节点，并将新节点加入列表，这人联系不上了，删了吧。 这个机制保证了任意节点加入和离开都不影响整体网络。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:23:7","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，今天的讲解就到这里了，我们总结一下：下载一个文件可以使用 HTTP 或 FTP，这两种都是集中下载的方式，而 P2P 则换了一种思路，采取非中心化下载的方式；P2P 也是有两种，一种是依赖于 tracker 的，也即元数据集中，文件数据分散；另一种是基于分布式的哈希算法，元数据和文件数据全部分散。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:23:8","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"陌生的数据中心 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:24:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第18讲 | DNS协议：网络世界的地址簿 前面我们讲了平时常见的看新闻、支付、直播、下载等场景，现在网站的数目非常多，常用的网站就有二三十个，如果全部用 IP 地址进行访问，恐怕很难记住。于是，就需要一个地址簿，根据名称，就可以查看具体的地址。例如，我要去西湖边的“外婆家”，这就是名称，然后通过地址簿，查看到底是哪条路多少号。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:25:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"DNS 服务器 在网络世界，也是这样的。你肯定记得住网站的名称，但是很难记住网站的 IP 地址，因而也需要一个地址簿，就是 DNS 服务器。由此可见，DNS 在日常生活中多么重要。每个人上网，都需要访问它，但是同时，这对它来讲也是非常大的挑战。一旦它出了故障，整个互联网都将瘫痪。另外，上网的人分布在全世界各地，如果大家都去同一个地方访问某一台服务器，时延将会非常大。因而，DNS 服务器，一定要设置成高可用、高并发和分布式的。于是，就有了这样树状的层次结构。 根 DNS 服务器 ：返回顶级域 DNS 服务器的 IP 地址 顶级域 DNS 服务器：返回权威 DNS 服务器的 IP 地址 权威 DNS 服务器 ：返回相应主机的 IP 地址 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:25:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"DNS 解析流程 为了提高 DNS 的解析性能，很多网络都会就近部署 DNS 缓存服务器。于是，就有了以下的 DNS 解析流程。 电脑客户端会发出一个 DNS 请求，问 www.163.com 的 IP 是啥啊，并发给本地域名服务器 (本地 DNS)。那本地域名服务器 (本地 DNS) 是什么呢？如果是通过 DHCP 配置，本地 DNS 由你的网络服务商（ISP），如电信、移动等自动分配，它通常就在你网络服务商的某个机房。本地 DNS 收到来自客户端的请求。你可以想象这台服务器上缓存了一张域名与之对应 IP 地址的大表格。如果能找到 www.163.com，它就直接返回 IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大，能告诉我 www.163.com 的 IP 地址吗？”根域名服务器是最高层次的，全球共有 13 套。它不直接用于域名解析，但能指明一条道路。根 DNS 收到来自本地 DNS 的请求，发现后缀是 .com，说：“哦，www.163.com 啊，这个域名是由.com 区域管理，我给你它的顶级域名服务器的地址，你去问问它吧。”本地 DNS 转向问顶级域名服务器：“老二，你能告诉我 www.163.com 的 IP 地址吗？”顶级域名服务器就是大名鼎鼎的比如 .com、.net、 .org 这些一级域名，它负责管理二级域名，比如 163.com，所以它能提供一条更清晰的方向。顶级域名服务器说：“我给你负责 www.163.com 区域的权威 DNS 服务器的地址，你去问它应该能问到。”本地 DNS 转向问权威 DNS 服务器：“您好，www.163.com 对应的 IP 是啥呀？”163.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。权威 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。 至此，我们完成了 DNS 的解析过程。现在总结一下，整个过程我画成了一个图。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:25:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"负载均衡 站在客户端角度，这是一次 DNS 递归查询过程。因为本地 DNS 全权为它效劳，它只要坐等结果即可。在这个过程中，DNS 除了可以通过名称映射为 IP 地址，它还可以做另外一件事，就是负载均衡。还是以访问“外婆家”为例，还是我们开头的“外婆家”，但是，它可能有很多地址，因为它在杭州可以有很多家。所以，如果一个人想去吃“外婆家”，他可以就近找一家店，而不用大家都去同一家，这就是负载均衡。 DNS 首先可以做内部负载均衡。例如，一个应用要访问数据库，在这个应用里面应该配置这个数据库的 IP 地址，还是应该配置这个数据库的域名呢？显然应该配置域名，因为一旦这个数据库，因为某种原因，换到了另外一台机器上，而如果有多个应用都配置了这台数据库的话，一换 IP 地址，就需要将这些应用全部修改一遍。但是如果配置了域名，则只要在 DNS 服务器里，将域名映射为新的 IP 地址，这个工作就完成了，大大简化了运维。 在这个基础上，我们可以再进一步。例如，某个应用要访问另外一个应用，如果配置另外一个应用的 IP 地址，那么这个访问就是一对一的。但是当被访问的应用撑不住的时候，我们其实可以部署多个。但是，访问它的应用，如何在多个之间进行负载均衡？只要配置成为域名就可以了。在域名解析的时候，我们只要配置策略，这次返回第一个 IP，下次返回第二个 IP，就可以实现负载均衡了。 另外一个更加重要的是，DNS 还可以做全局负载均衡。 为了保证我们的应用高可用，往往会部署在多个机房，每个地方都会有自己的 IP 地址。当用户访问某个域名的时候，这个 IP 地址可以轮询访问多个数据中心。如果一个数据中心因为某种原因挂了，只要在 DNS 服务器里面，将这个数据中心对应的 IP 地址删除，就可以实现一定的高可用。另外，我们肯定希望北京的用户访问北京的数据中心，上海的用户访问上海的数据中心，这样，客户体验就会非常好，访问速度就会超快。这就是全局负载均衡的概念。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:25:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"示例：DNS 访问数据中心中对象存储上的静态资源 我们通过 DNS 访问数据中心中对象存储上的静态资源为例，看一看整个过程。假设全国有多个数据中心，托管在多个运营商，每个数据中心三个可用区（Available Zone）。对象存储通过跨可用区部署，实现高可用性。在每个数据中心中，都至少部署两个内部负载均衡器，内部负载均衡器后面对接多个对象存储的前置服务器（Proxy-server）。 当一个客户端要访问 object.yourcompany.com 的时候，需要将域名转换为 IP 地址进行访问，所以它要请求本地 DNS 解析器。本地 DNS 解析器先查看看本地的缓存是否有这个记录。如果有则直接使用，因为上面的过程太复杂了，如果每次都要递归解析，就太麻烦了。如果本地无缓存，则需要请求本地的 DNS 服务器。本地的 DNS 服务器一般部署在你的数据中心或者你所在的运营商的网络中，本地 DNS 服务器也需要看本地是否有缓存，如果有则返回，因为它也不想把上面的递归过程再走一遍。至 7. 如果本地没有，本地 DNS 才需要递归地从根 DNS 服务器，查到.com 的顶级域名服务器，最终查到 yourcompany.com 的权威 DNS 服务器，给本地 DNS 服务器，权威 DNS 服务器按说会返回真实要访问的 IP 地址。 对于不需要做全局负载均衡的简单应用来讲，yourcompany.com 的权威 DNS 服务器可以直接将 object.yourcompany.com 这个域名解析为一个或者多个 IP 地址，然后客户端可以通过多个 IP 地址，进行简单的轮询，实现简单的负载均衡。但是对于复杂的应用，尤其是跨地域跨运营商的大型应用，则需要更加复杂的全局负载均衡机制，因而需要专门的设备或者服务器来做这件事情，这就是全局负载均衡器（GSLB，Global Server Load Balance）。在 yourcompany.com 的 DNS 服务器中，一般是通过配置 CNAME 的方式，给 object.yourcompany.com 起一个别名，例如 object.vip.yourcomany.com，然后告诉本地 DNS 服务器，让它请求 GSLB 解析这个域名，GSLB 就可以在解析这个域名的过程中，通过自己的策略实现负载均衡。图中画了两层的 GSLB，是因为分运营商和地域。我们希望不同运营商的客户，可以访问相同运营商机房中的资源，这样不跨运营商访问，有利于提高吞吐量，减少时延。 第一层 GSLB，通过查看请求它的本地 DNS 服务器所在的运营商，就知道用户所在的运营商。假设是移动，通过 CNAME 的方式，通过另一个别名 object.yd.yourcompany.com，告诉本地 DNS 服务器去请求第二层的 GSLB。第二层 GSLB，通过查看请求它的本地 DNS 服务器所在的地址，就知道用户所在的地理位置，然后将距离用户位置比较近的 Region 里面，六个内部负载均衡（SLB，Server Load Balancer）的地址，返回给本地 DNS 服务器。本地 DNS 服务器将结果返回给本地 DNS 解析器。本地 DNS 解析器将结果缓存后，返回给客户端。客户端开始访问属于相同运营商的距离较近的 Region 1 中的对象存储，当然客户端得到了六个 IP 地址，它可以通过负载均衡的方式，随机或者轮询选择一个可用区进行访问。对象存储一般会有三个备份，从而可以实现对存储读写的负载均衡。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:25:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这节内容就到这里了，我们来总结一下：DNS 是网络世界的地址簿，可以通过域名查地址，因为域名服务器是按照树状结构组织的，因而域名查找是使用递归的方法，并通过缓存的方式增强性能；在域名和 IP 的映射过程中，给了应用基于域名做负载均衡的机会，可以是简单的负载均衡，也可以根据地址和运营商做全局的负载均衡。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:25:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第19讲 | HttpDNS：网络世界的地址簿也会指错路 上一节我们知道了 DNS 的两项功能，第一是根据名称查到具体的地址，另外一个是可以针对多个地址做负载均衡，而且可以在多个地址中选择一个距离你近的地方访问。然而有时候这个地址簿也经常给你指错路，明明距离你 500 米就有个吃饭的地方，非要把你推荐到 5 公里外。为什么会出现这样的情况呢？还记得吗？当我们发出请求解析 DNS 的时候，首先，会先连接到运营商本地的 DNS 服务器，由这个服务器帮我们去整棵 DNS 树上进行解析，然后将解析的结果返回给客户端。但是本地的 DNS 服务器，作为一个本地导游，往往有自己的“小心思”。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:26:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"传统 DNS 存在哪些问题？ 1. 域名缓存问题 它可以在本地做一个缓存，也就是说，不是每一个请求，它都会去访问权威 DNS 服务器，而是访问过一次就把结果缓存到自己本地，当其他人来问的时候，直接就返回这个缓存数据。这就相当于导游去过一个饭店，自己脑子记住了地址，当有一个游客问的时候，他就凭记忆回答了，不用再去查地址簿。这样经常存在的一个问题是，人家那个饭店明明都已经搬了，结果作为导游，他并没有刷新这个缓存，结果你辛辛苦苦到了这个地点，发现饭店已经变成了服装店，你是不是会非常失望？另外，有的运营商会把一些静态页面，缓存到本运营商的服务器内，这样用户请求的时候，就不用跨运营商进行访问，这样既加快了速度，也减少了运营商之间流量计算的成本。在域名解析的时候，不会将用户导向真正的网站，而是指向这个缓存的服务器。 很多情况下是看不出问题的，但是当页面更新，用户会访问到老的页面，问题就出来了。例如，你听说一个餐馆推出了一个新菜，你想去尝一下。结果导游告诉你，在这里吃也是一样的。有的游客会觉得没问题，但是对于想尝试新菜的人来说，如果导游说带你去，但其实并没有吃到新菜，你是不是也会非常失望呢？再就是本地的缓存，往往使得全局负载均衡失败，因为上次进行缓存的时候，缓存中的地址不一定是这次访问离客户最近的地方，如果把这个地址返回给客户，那肯定就会绕远路。就像上一次客户要吃西湖醋鱼的事，导游知道西湖边有一家，因为当时游客就在西湖边，可是，下一次客户在灵隐寺，想吃西湖醋鱼的时候，导游还指向西湖边的那一家，那这就绕得太远了。 2. 域名转发问题 缓存问题还是说本地域名解析服务，还是会去权威 DNS 服务器中查找，只不过不是每次都要查找。可以说这还是大导游、大中介。还有一些小导游、小中介，有了请求之后，直接转发给其他运营商去做解析，自己只是外包了出去。这样的问题是，如果是 A 运营商的客户，访问自己运营商的 DNS 服务器，如果 A 运营商去权威 DNS 服务器查询的话，权威 DNS 服务器知道你是 A 运营商的，就返回给一个部署在 A 运营商的网站地址，这样针对相同运营商的访问，速度就会快很多。但是 A 运营商偷懒，将解析的请求转发给 B 运营商，B 运营商去权威 DNS 服务器查询的话，权威服务器会误认为，你是 B 运营商的，那就返回给你一个在 B 运营商的网站地址吧，结果客户的每次访问都要跨运营商，速度就会很慢。 3. 出口 NAT 问题 前面讲述网关的时候，我们知道，出口的时候，很多机房都会配置 NAT，也即网络地址转换，使得从这个网关出去的包，都换成新的 IP 地址，当然请求返回的时候，在这个网关，再将 IP 地址转换回去，所以对于访问来说是没有任何问题。但是一旦做了网络地址的转换，权威的 DNS 服务器，就没办法通过这个地址，来判断客户到底是来自哪个运营商，而且极有可能因为转换过后的地址，误判运营商，导致跨运营商的访问。 4. 域名更新问题 本地 DNS 服务器是由不同地区、不同运营商独立部署的。对域名解析缓存的处理上，实现策略也有区别，有的会偷懒，忽略域名解析结果的 TTL 时间限制，在权威 DNS 服务器解析变更的时候，解析结果在全网生效的周期非常漫长。但是有的时候，在 DNS 的切换中，场景对生效时间要求比较高。例如双机房部署的时候，跨机房的负载均衡和容灾多使用 DNS 来做。当一个机房出问题之后，需要修改权威 DNS，将域名指向新的 IP 地址，但是如果更新太慢，那很多用户都会出现访问异常。这就像，有的导游比较勤快、敬业，时时刻刻关注酒店、餐馆、交通的变化，问他的时候，往往会得到最新情况。有的导游懒一些，8 年前背的导游词就没换过，问他的时候，指的路往往就是错的。 5. 解析延迟问题 从上一节的 DNS 查询过程来看，DNS 的查询过程需要递归遍历多个 DNS 服务器，才能获得最终的解析结果，这会带来一定的时延，甚至会解析超时。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:26:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"HttpDNS 的工作模式 既然 DNS 解析中有这么多问题，那怎么办呢？难不成退回到直接用 IP 地址？这样显然不合适，所以就有了 HttpDNS。HttpDNS 其实就是，不走传统的 DNS 解析，而是自己搭建基于 HTTP 协议的 DNS 服务器集群，分布在多个地点和多个运营商。当客户端需要 DNS 解析的时候，直接通过 HTTP 协议进行请求这个服务器集群，得到就近的地址。这就相当于每家基于 HTTP 协议，自己实现自己的域名解析，自己做一个自己的地址簿，而不使用统一的地址簿。但是默认的域名解析都是走 DNS 的，因而使用 HttpDNS 需要绕过默认的 DNS 路径，就不能使用默认的客户端。使用 HttpDNS 的，往往是手机应用，需要在手机端嵌入支持 HttpDNS 的客户端 SDK。通过自己的 HttpDNS 服务器和自己的 SDK，实现了从依赖本地导游，到自己上网查询做旅游攻略，进行自由行，爱怎么玩怎么玩。这样就能够避免依赖导游，而导游又不专业，你还不能把他怎么样的尴尬。 下面我来解析一下 HttpDNS 的工作模式。在客户端的 SDK 里动态请求服务端，获取 HttpDNS 服务器的 IP 列表，缓存到本地。随着不断地解析域名，SDK 也会在本地缓存 DNS 域名解析的结果。当手机应用要访问一个地址的时候，首先看是否有本地的缓存，如果有就直接返回。这个缓存和本地 DNS 的缓存不一样的是，这个是手机应用自己做的，而非整个运营商统一做的。如何更新、何时更新，手机应用的客户端可以和服务器协调来做这件事情。如果本地没有，就需要请求 HttpDNS 的服务器，在本地 HttpDNS 服务器的 IP 列表中，选择一个发出 HTTP 的请求，会返回一个要访问的网站的 IP 列表。请求的方式是这样的。 curl http://106.2.xxx.xxx/d?dn=c.m.163.com {\"dns\":[{\"host\":\"c.m.163.com\",\"ips\":[\"223.252.199.12\"],\"ttl\":300,\"http2\":0}],\"client\":{\"ip\":\"106.2.81.50\",\"line\":269692944}} 手机客户端自然知道手机在哪个运营商、哪个地址。由于是直接的 HTTP 通信，HttpDNS 服务器能够准确知道这些信息，因而可以做精准的全局负载均衡。 当然，当所有这些都不工作的时候，可以切换到传统的 LocalDNS 来解析，慢也比访问不到好。那 HttpDNS 是如何解决上面的问题的呢？其实归结起来就是两大问题。一是解析速度和更新速度的平衡问题，二是智能调度的问题，对应的解决方案是 HttpDNS 的缓存设计和调度设计。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:26:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"HttpDNS 的缓存设计 解析 DNS 过程复杂，通信次数多，对解析速度造成很大影响。为了加快解析，因而有了缓存，但是这又会产生缓存更新速度不及时的问题。最要命的是，这两个方面都掌握在别人手中，也即本地 DNS 服务器手中，它不会为你定制，你作为客户端干着急没办法。而 HttpDNS 就是将解析速度和更新速度全部掌控在自己手中。一方面，解析的过程，不需要本地 DNS 服务递归的调用一大圈，一个 HTTP 的请求直接搞定，要实时更新的时候，马上就能起作用；另一方面为了提高解析速度，本地也有缓存，缓存是在客户端 SDK 维护的，过期时间、更新时间，都可以自己控制。HttpDNS 的缓存设计策略也是咱们做应用架构中常用的缓存设计模式，也即分为客户端、缓存、数据源三层。 对于应用架构来讲，就是应用、缓存、数据库。常见的是 Tomcat、Redis、MySQL。对于 HttpDNS 来讲，就是手机客户端、DNS 缓存、HttpDNS 服务器。 只要是缓存模式，就存在缓存的过期、更新、不一致的问题，解决思路也是很像的。例如 DNS 缓存在内存中，也可以持久化到存储上，从而 APP 重启之后，能够尽快从存储中加载上次累积的经常访问的网站的解析结果，就不需要每次都全部解析一遍，再变成缓存。这有点像 Redis 是基于内存的缓存，但是同样提供持久化的能力，使得重启或者主备切换的时候，数据不会完全丢失。SDK 中的缓存会严格按照缓存过期时间，如果缓存没有命中，或者已经过期，而且客户端不允许使用过期的记录，则会发起一次解析，保障记录是更新的。解析可以同步进行，也就是直接调用 HttpDNS 的接口，返回最新的记录，更新缓存；也可以异步进行，添加一个解析任务到后台，由后台任务调用 HttpDNS 的接口。同步更新的优点是实时性好，缺点是如果有多个请求都发现过期的时候，同时会请求 HttpDNS 多次，其实是一种浪费。同步更新的方式对应到应用架构中缓存的 Cache-Aside 机制，也即先读缓存，不命中读数据库，同时将结果写入缓存。 异步更新的优点是，可以将多个请求都发现过期的情况，合并为一个对于 HttpDNS 的请求任务，只执行一次，减少 HttpDNS 的压力。同时可以在即将过期的时候，就创建一个任务进行预加载，防止过期之后再刷新，称为预加载。它的缺点是当前请求拿到过期数据的时候，如果客户端允许使用过期数据，需要冒一次风险。如果过期的数据还能请求，就没问题；如果不能请求，则失败一次，等下次缓存更新后，再请求方能成功。 异步更新的机制对应到应用架构中缓存的 Refresh-Ahead 机制，即业务仅仅访问缓存，当过期的时候定期刷新。在著名的应用缓存 Guava Cache 中，有个 RefreshAfterWrite 机制，对于并发情况下，多个缓存访问不命中从而引发并发回源的情况，可以采取只有一个请求回源的模式。在应用架构的缓存中，也常常用数据预热或者预加载的机制。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:26:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"HttpDNS 的调度设计 由于客户端嵌入了 SDK，因而就不会因为本地 DNS 的各种缓存、转发、NAT，让权威 DNS 服务器误会客户端所在的位置和运营商，而可以拿到第一手资料。 在客户端，可以知道手机是哪个国家、哪个运营商、哪个省，甚至哪个市，HttpDNS 服务端可以根据这些信息，选择最佳的服务节点访问。如果有多个节点，还会考虑错误率、请求时间、服务器压力、网络状况等，进行综合选择，而非仅仅考虑地理位置。当有一个节点宕机或者性能下降的时候，可以尽快进行切换。要做到这一点，需要客户端使用 HttpDNS 返回的 IP 访问业务应用。客户端的 SDK 会收集网络请求数据，如错误率、请求时间等网络请求质量数据，并发送到统计后台，进行分析、聚合，以此查看不同的 IP 的服务质量。 在服务端，应用可以通过调用 HttpDNS 的管理接口，配置不同服务质量的优先级、权重。HttpDNS 会根据这些策略综合地理位置和线路状况算出一个排序，优先访问当前那些优质的、时延低的 IP 地址。HttpDNS 通过智能调度之后返回的结果，也会缓存在客户端。为了不让缓存使得调度失真，客户端可以根据不同的移动网络运营商 WIFI 的 SSID 来分维度缓存。不同的运营商或者 WIFI 解析出来的结果会不同。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:26:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这节就到这里了，我们来总结一下，你需要记住这两个重点：传统的 DNS 有很多问题，例如解析慢、更新不及时。因为缓存、转发、NAT 问题导致客户端误会自己所在的位置和运营商，从而影响流量的调度。HttpDNS 通过客户端 SDK 和服务端，通过 HTTP 直接调用解析 DNS 的方式，绕过了传统 DNS 的这些缺点，实现了智能的调度。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:26:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第20讲 | CDN：你去小卖部取过快递么？ 上一节，我们看到了网站的一般访问模式。当一个用户想访问一个网站的时候，指定这个网站的域名，DNS 就会将这个域名解析为地址，然后用户请求这个地址，返回一个网页。就像你要买个东西，首先要查找商店的位置，然后去商店里面找到自己想要的东西，最后拿着东西回家。 那这里面还有没有可以优化的地方呢？例如你去电商网站下单买个东西，这个东西一定要从电商总部的中心仓库送过来吗？原来基本是这样的，每一单都是单独配送，所以你可能要很久才能收到你的宝贝。但是后来电商网站的物流系统学聪明了，他们在全国各地建立了很多仓库，而不是只有总部的中心仓库才可以发货。电商网站根据统计大概知道，北京、上海、广州、深圳、杭州等地，每天能够卖出去多少书籍、卫生纸、包、电器等存放期比较长的物品。这些物品用不着从中心仓库发出，所以平时就可以将它们分布在各地仓库里，客户一下单，就近的仓库发出，第二天就可以收到了。这样，用户体验大大提高。当然，这里面也有个难点就是，生鲜这类东西保质期太短，如果提前都备好货，但是没有人下单，那肯定就坏了。这个问题，我后文再说。 我们先说，我们的网站访问可以借鉴“就近配送”这个思路。全球有这么多的数据中心，无论在哪里上网，临近不远的地方基本上都有数据中心。是不是可以在这些数据中心里部署几台机器，形成一个缓存的集群来缓存部分数据，那么用户访问数据的时候，就可以就近访问了呢？当然是可以的。这些分布在各个地方的各个数据中心的节点，就称为边缘节点。由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中，这样就会在边缘节点之上。有区域节点，规模就要更大，缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。 这就是 CDN 分发系统的架构。CDN 系统的缓存，也是一层一层的，能不访问后端真正的源，就不打扰它。这也是电商网站物流系统的思路，北京局找不到，找华北局，华北局找不到，再找北方局。有了这个分发系统之后，接下来，客户端如何找到相应的边缘节点进行访问呢？还记得我们讲过的基于 DNS 的全局负载均衡吗？这个负载均衡主要用来选择一个就近的同样运营商的服务器进行访问。你会发现，CDN 分发网络也是一个分布在多个区域、多个运营商的分布式系统，也可以用相同的思路选择最合适的边缘节点。 在没有 CDN 的情况下，用户向浏览器输入 www.web.com 这个域名，客户端访问本地 DNS 服务器的时候，如果本地 DNS 服务器有缓存，则返回网站的地址；如果没有，递归查询到网站的权威 DNS 服务器，这个权威 DNS 服务器是负责 web.com 的，它会返回网站的 IP 地址。本地 DNS 服务器缓存下 IP 地址，将 IP 地址返回，然后客户端直接访问这个 IP 地址，就访问到了这个网站。然而有了 CDN 之后，情况发生了变化。在 web.com 这个权威 DNS 服务器上，会设置一个 CNAME 别名，指向另外一个域名 www.web.cdn.com，返回给本地 DNS 服务器。当本地 DNS 服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的就不是 web.com 的权威 DNS 服务器了，而是 web.cdn.com 的权威 DNS 服务器，这是 CDN 自己的权威 DNS 服务器。在这个服务器上，还是会设置一个 CNAME，指向另外一个域名，也即 CDN 网络的全局负载均衡器。接下来，本地 DNS 服务器去请求 CDN 的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，选择的依据包括： 根据用户 IP 地址，判断哪一台服务器距用户最近；用户所处的运营商；根据用户所请求的 URL 中携带的内容名称，判断哪一台服务器上有用户所需的内容；查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。 基于以上这些条件，进行综合分析之后，全局负载均衡器会返回一台缓存服务器的 IP 地址。本地 DNS 服务器缓存这个 IP 地址，然后将 IP 返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。 CDN 可以进行缓存的内容有很多种。保质期长的日用品比较容易缓存，因为不容易过期，对应到就像电商仓库系统里，就是静态页面、图片等，因为这些东西也不怎么变，所以适合缓存。 还记得这个接入层缓存的架构吗？在进入数据中心的时候，我们希望通过最外层接入层的缓存，将大部分静态资源的访问拦在边缘。而 CDN 则更进一步，将这些静态资源缓存到离用户更近的数据中心。越接近客户，访问性能越好，时延越低。但是静态内容中，有一种特殊的内容，也大量使用了 CDN，这个就是前面讲过的流媒体。 CDN 支持流媒体协议，例如前面讲过的 RTMP 协议。在很多情况下，这相当于一个代理，从上一级缓存读取内容，转发给用户。由于流媒体往往是连续的，因而可以进行预先缓存的策略，也可以预先推送到用户的客户端。对于静态页面来讲，内容的分发往往采取拉取的方式，也即当发现未命中的时候，再去上一级进行拉取。但是，流媒体数据量大，如果出现回源，压力会比较大，所以往往采取主动推送的模式，将热点数据主动推送到边缘节点。 对于流媒体来讲，很多 CDN 还提供预处理服务，也即文件在分发之前，经过一定的处理。例如将视频转换为不同的码流，以适应不同的网络带宽的用户需求；再如对视频进行分片，降低存储压力，也使得客户端可以选择使用不同的码率加载不同的分片。这就是我们常见的，“我要看超清、标清、流畅等”。对于流媒体 CDN 来讲，有个关键的问题是防盗链问题。因为视频是要花大价钱买版权的，为了挣点钱，收点广告费，如果流媒体被其他的网站盗走，在人家的网站播放，那损失可就大了。 最常用也最简单的方法就是 HTTP 头的 referer 字段， 当浏览器发送请求的时候，一般会带上 referer，告诉服务器是从哪个页面链接过来的，服务器基于此可以获得一些信息用于处理。如果 refer 信息不是来自本站，就阻止访问或者跳到其它链接。 referer 的机制相对比较容易破解，所以还需要配合其他的机制。一种常用的机制是时间戳防盗链。使用 CDN 的管理员可以在配置界面上，和 CDN 厂商约定一个加密字符串。客户端取出当前的时间戳，要访问的资源及其路径，连同加密字符串进行签名算法得到一个字符串，然后生成一个下载链接，带上这个签名字符串和截止时间戳去访问 CDN。 在 CDN 服务端，根据取出过期时间，和当前 CDN 节点时间进行比较，确认请求是否过期。然后 CDN 服务端有了资源及路径，时间戳，以及约定的加密字符串，根据相同的签名算法计算签名，如果匹配则一致，访问合法，才会将资源返回给客户。然而比如在电商仓库中，我在前面提过，有关生鲜的缓存就是非常麻烦的事情，这对应着就是动态的数据，比较难以缓存。怎么办呢？现在也有动态 CDN，主要有两种模式。 一种为生鲜超市模式，也即边缘计算的模式。既然数据是动态生成的，所以数据的逻辑计算和存储，也相应的放在边缘的节点。其中定时从源数据那里同步存储的数据，然后在边缘进行计算得到结果。就像对生鲜的烹饪是动态的，没办法事先做好缓存，因而将生鲜超市放在你家旁边，既能够送货上门，也能够现场烹饪，也是边缘计算的一种体现。另一种是冷链运输模式，也即路径优化的模式。数据不是在边缘计算生成的，而是在源站生成的，但是数据的下发则可以通过 CDN 的网络，对路径进行优化。因为 CDN 节点较多，能够找到离源站很近的边缘节点，也能找到离用户很近的边缘节点。中间的链路完全由 CDN 来规划，选择一个更加可靠的路径，使用类似专线的方式进行访问。 对于常用的 TCP 连接，在公网上传输的时候经常会丢数据，导致 TCP 的窗口始终很小，发送速度上不去。根据前面的 TCP 流量控制和拥塞控制的原理，在 CDN 加速网络中可以调整 TCP 的参数，使得 TCP 可以更加激进地传输数据。可以通过多个请求复用一个连接，保证每次动态请求到达时。连接都已经建立了，不必临时三次握手或者建立过多的连接，增加服务器的压力。另外，可以通过对传输数据进行压缩，增加传输效率。所有这些手段就像冷链运输，整个物流优化了，全程冷冻高速运输。不管生鲜是从你旁边的超市送到你家的，还是从产地送的，保证到你家是新鲜的。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:27:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这节就到这里了。咱们来总结一下，你记住这两个重点就好。CDN 和电商系统的分布式仓储系统一样，分为中心节点、区域节点、边缘节点，而数据缓存在离用户最近的位置。CDN 最擅长的是缓存静态数据，除此之外还可以缓存流媒体数据，这时候要注意使用防盗链。它也支持动态数据的缓存，一种是边缘计算的生鲜超市模式，另一种是链路优化的冷链运输模式。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:27:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第21讲 | 数据中心：我是开发商，自己拿地盖别墅 无论你是看新闻、下订单、看视频、下载文件，最终访问的目的地都在数据中心里面。我们前面学了这么多的网络协议和网络相关的知识，你是不是很好奇，数据中心究竟长啥样呢？数据中心是一个大杂烩，几乎要用到前面学过的所有知识。前面讲办公室网络的时候，我们知道办公室里面有很多台电脑。如果要访问外网，需要经过一个叫网关的东西，而网关往往是一个路由器。数据中心里面也有一大堆的电脑，但是它和咱们办公室里面的笔记本或者台式机不一样。数据中心里面是服务器。服务器被放在一个个叫作机架（Rack）的架子上面。数据中心的入口和出口也是路由器，由于在数据中心的边界，就像在一个国家的边境，称为边界路由器（Border Router）。为了高可用，边界路由器会有多个。一般家里只会连接一个运营商的网络，而为了高可用，为了当一个运营商出问题的时候，还可以通过另外一个运营商来提供服务，所以数据中心的边界路由器会连接多个运营商网络。既然是路由器，就需要跑路由协议，数据中心往往就是路由协议中的自治区域（AS）。数据中心里面的机器要想访问外面的网站，数据中心里面也是有对外提供服务的机器，都可以通过 BGP 协议，获取内外互通的路由信息。这就是我们常听到的多线 BGP 的概念。如果数据中心非常简单，没几台机器，那就像家里或者宿舍一样，所有的服务器都直接连到路由器上就可以了。但是数据中心里面往往有非常多的机器，当塞满一机架的时候，需要有交换机将这些服务器连接起来，可以互相通信。这些交换机往往是放在机架顶端的，所以经常称为 TOR（Top Of Rack）交换机。这一层的交换机常常称为接入层（Access Layer）。注意这个接入层和原来讲过的应用的接入层不是一个概念。 当一个机架放不下的时候，就需要多个机架，还需要有交换机将多个机架连接在一起。这些交换机对性能的要求更高，带宽也更大。这些交换机称为汇聚层交换机（Aggregation Layer）。数据中心里面的每一个连接都是需要考虑高可用的。这里首先要考虑的是，如果一台机器只有一个网卡，上面连着一个网线，接入到 TOR 交换机上。如果网卡坏了，或者不小心网线掉了，机器就上不去了。所以，需要至少两个网卡、两个网线插到 TOR 交换机上，但是两个网卡要工作得像一张网卡一样，这就是常说的网卡绑定（bond）。这就需要服务器和交换机都支持一种协议 LACP（Link Aggregation Control Protocol）。它们互相通信，将多个网卡聚合称为一个网卡，多个网线聚合成一个网线，在网线之间可以进行负载均衡，也可以为了高可用作准备。 网卡有了高可用保证，但交换机还有问题。如果一个机架只有一个交换机，它挂了，那整个机架都不能上网了。因而 TOR 交换机也需要高可用，同理接入层和汇聚层的连接也需要高可用性，也不能单线连着。最传统的方法是，部署两个接入交换机、两个汇聚交换机。服务器和两个接入交换机都连接，接入交换机和两个汇聚都连接，当然这样会形成环，所以需要启用 STP 协议，去除环，但是这样两个汇聚就只能一主一备了。STP 协议里我们学过，只有一条路会起作用。 交换机有一种技术叫作堆叠，所以另一种方法是，将多个交换机形成一个逻辑的交换机，服务器通过多根线分配连到多个接入层交换机上，而接入层交换机多根线分别连接到多个交换机上，并且通过堆叠的私有协议，形成双活的连接方式。 由于对带宽要求更大，而且挂了影响也更大，所以两个堆叠可能就不够了，可以就会有更多的，比如四个堆叠为一个逻辑的交换机。汇聚层将大量的计算节点相互连接在一起，形成一个集群。在这个集群里面，服务器之间通过二层互通，这个区域常称为一个 POD（Point Of Delivery），有时候也称为一个可用区（Available Zone）。当节点数目再多的时候，一个可用区放不下，需要将多个可用区连在一起，连接多个可用区的交换机称为核心交换机。 核心交换机吞吐量更大，高可用要求更高，肯定需要堆叠，但是往往仅仅堆叠，不足以满足吞吐量，因而还是需要部署多组核心交换机。核心和汇聚交换机之间为了高可用，也是全互连模式的。这个时候还存在一个问题，出现环路怎么办？一种方式是，不同的可用区在不同的二层网络，需要分配不同的网段。汇聚和核心之间通过三层网络互通的，二层都不在一个广播域里面，不会存在二层环路的问题。三层有环是没有问题的，只要通过路由协议选择最佳的路径就可以了。那为啥二层不能有环路，而三层可以呢？你可以回忆一下二层环路的情况。 如图，核心层和汇聚层之间通过内部的路由协议 OSPF，找到最佳的路径进行访问，而且还可以通过 ECMP 等价路由，在多个路径之间进行负载均衡和高可用。但是随着数据中心里面的机器越来越多，尤其是有了云计算、大数据，集群规模非常大，而且都要求在一个二层网络里面。这就需要二层互连从汇聚层上升为核心层，也即在核心以下，全部是二层互连，全部在一个广播域里面，这就是常说的大二层。 如果大二层横向流量不大，核心交换机数目不多，可以做堆叠，但是如果横向流量很大，仅仅堆叠满足不了，就需要部署多组核心交换机，而且要和汇聚层进行全互连。由于堆叠只解决一个核心交换机组内的无环问题，而组之间全互连，还需要其他机制进行解决。如果是 STP，那部署多组核心无法扩大横向流量的能力，因为还是只有一组起作用。于是大二层就引入了 TRILL（Transparent Interconnection of Lots of Link），即多链接透明互联协议。它的基本思想是，二层环有问题，三层环没有问题，那就把三层的路由能力模拟在二层实现。运行 TRILL 协议的交换机称为 RBridge，是具有路由转发特性的网桥设备，只不过这个路由是根据 MAC 地址来的，不是根据 IP 来的。Rbridage 之间通过链路状态协议运作。记得这个路由协议吗？通过它可以学习整个大二层的拓扑，知道访问哪个 MAC 应该从哪个网桥走；还可以计算最短的路径，也可以通过等价的路由进行负载均衡和高可用性。 TRILL 协议在原来的 MAC 头外面加上自己的头，以及外层的 MAC 头。TRILL 头里面的 Ingress RBridge，有点像 IP 头里面的源 IP 地址，Egress RBridge 是目标 IP 地址，这两个地址是端到端的，在中间路由的时候，不会发生改变。而外层的 MAC，可以有下一跳的 Bridge，就像路由的下一跳，也是通过 MAC 地址来呈现的一样。如图中所示的过程，有一个包要从主机 A 发送到主机 B，中间要经过 RBridge 1、RBridge 2、RBridge X 等等，直到 RBridge 3。在 RBridge 2 收到的包里面，分内外两层，内层就是传统的主机 A 和主机 B 的 MAC 地址以及内层的 VLAN。在外层首先加上一个 TRILL 头，里面描述这个包从 RBridge 1 进来的，要从 RBridge 3 出去，并且像三层的 IP 地址一样有跳数。然后再外面，目的 MAC 是 RBridge 2，源 MAC 是 RBridge 1，以及外层的 VLAN。当 RBridge 2 收到这个包之后，首先看 MAC 是否是自己的 MAC，如果是，要看自己是不是 Egress RBridge，也即是不是最后一跳；如果不是，查看跳数是不是大于 0，然后通过类似路由查找的方式找到下一跳 RBridge X，然后将包发出去。RBridge 2 发出去的包，内层的信息是不变的，外层的 TRILL 头里面。同样，描述这个包从 RBridge 1 进来的，要从 RBridge 3 出去，但是跳数要减 1。外层的目标 MAC 变成 RBridge X，源 MAC 变成 RBridge 2。 如此一直转发，直到 RBridge 3，将外层解出来，发送内层的包给主机 B。这个过程是不是和 IP 路由很像？对于大二层的广播包，也需要通过分发树的技术来实现。我们知道 STP 是将一个有环的图，通过去掉边形成一棵树，而分发树是一个有环的图形成多棵树，不同的树有不同的 VLAN，有的广播包从 VLAN A 广播，有的从 VLAN B 广播，实现负载均衡和高可用。 核心交换机之外，就是边界路由器了。至此从服务器到数据中心边界的层次情况已经清楚了。在核心交换上面，往往会挂一些安全设备，例如入侵检测、DDoS 防护等等。这是整个数据中心的屏障，防止来自外来的攻击。核心交换机上往往还有负载均衡器，原理前面的章节已经说过了。在有的数据中心里面，对于存储设备，还会有一个存储网络，用来连接 SAN 和 NAS。但是对于新的云计算来讲，往往不使用传统的 SAN 和 NAS，而使用部署在 x86 机器上的软件定义存储，这样存储也是服务器了，而且可以和计算节点融合在一个机架上，从而更加有效率，也就没有了单独的存储网络了。于是整个数据中心的网络如下图所示。 这是一个典型的三层网络结构。这里的三层不是指 IP 层，而是指接入层、汇聚层、核心层三层。这种模式非常有利于外部流量请求到内部应用。这个类型的流量，是从外到内或者从内到外，对应到上面那张图里，就是从上到下，从下到上，上北下南，所以称为南北流量。但是随着云计算和大数据的发展，节点之间的交互越来越多，例如大数据计算经常要在不同的节点将数据拷贝来拷贝去，这样需要经过交换机，使得数据从左到右，从右到左，左西右东，所以称为东西流量。 为了解决东西流量的问题，演进出了叶脊网络（Spine/Leaf）。叶子交换机（leaf），直接连接物理服务器。L2/L3 网络的分","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:28:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，复杂的数据中心就讲到这里了。我们来总结一下，你需要记住这三个重点。数据中心分为三层。服务器连接到接入层，然后是汇聚层，再然后是核心层，最外面是边界路由器和安全设备。数据中心的所有链路都需要高可用性。服务器需要绑定网卡，交换机需要堆叠，三层设备可以通过等价路由，二层设备可以通过 TRILL 协议。随着云和大数据的发展，东西流量相对于南北流量越来越重要，因而演化为叶脊网络结构。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:28:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第22讲 | VPN：朝中有人好做官 前面我们讲到了数据中心，里面很复杂，但是有的公司有多个数据中心，需要将多个数据中心连接起来，或者需要办公室和数据中心连接起来。这该怎么办呢？第一种方式是走公网，但是公网太不安全，你的隐私可能会被别人偷窥。第二种方式是租用专线的方式把它们连起来，这是土豪的做法，需要花很多钱。第三种方式是用 VPN 来连接，这种方法比较折中，安全又不贵。 VPN，全名 Virtual Private Network，虚拟专用网，就是利用开放的公众网络，建立专用数据传输通道，将远程的分支机构、移动办公人员等连接起来。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:29:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"VPN 是如何工作的？ VPN 通过隧道技术在公众网络上仿真一条点到点的专线，是通过利用一种协议来传输另外一种协议的技术，这里面涉及三种协议：乘客协议、隧道协议和承载协议。我们以 IPsec 协议为例来说明。 你知道如何通过自驾进行海南游吗？这其中，你的车怎么通过琼州海峡呢？这里用到轮渡，其实这就用到隧道协议。在广州这边开车是有“协议”的，例如靠右行驶、红灯停、绿灯行，这个就相当于“被封装”的乘客协议。当然在海南那面，开车也是同样的协议。这就相当于需要连接在一起的一个公司的两个分部。但是在海上坐船航行，也有它的协议，例如要看灯塔、要按航道航行等。这就是外层的承载协议。 那我的车如何从广州到海南呢？这就需要你遵循开车的协议，将车开上轮渡，所有通过轮渡的车都关在船舱里面，按照既定的规则排列好，这就是隧道协议。在大海上，你的车是关在船舱里面的，就像在隧道里面一样，这个时候内部的乘客协议，也即驾驶协议没啥用处，只需要船遵从外层的承载协议，到达海南就可以了。到达之后，外部承载协议的任务就结束了，打开船舱，将车开出来，就相当于取下承载协议和隧道协议的头。接下来，在海南该怎么开车，就怎么开车，还是内部的乘客协议起作用。在最前面的时候说了，直接使用公网太不安全，所以接下来我们来看一种十分安全的 VPN，IPsec VPN。这是基于 IP 协议的安全隧道协议，为了保证在公网上面信息的安全，因而采取了一定的机制保证安全性。 机制一：私密性，防止信息泄露给未经授权的个人，通过加密把数据从明文变成无法读懂的密文，从而确保数据的私密性。前面讲 HTTPS 的时候，说过加密可以分为对称加密和非对称加密。对称加密速度快一些。而 VPN 一旦建立，需要传输大量数据，因而我们采取对称加密。但是同样，对称加密还是存在加密密钥如何传输的问题，这里需要用到因特网密钥交换（IKE，Internet Key Exchange）协议。机制二：完整性，数据没有被非法篡改，通过对数据进行 hash 运算，产生类似于指纹的数据摘要，以保证数据的完整性。机制三：真实性，数据确实是由特定的对端发出，通过身份认证可以保证数据的真实性。 那如何保证对方就是真正的那个人呢？第一种方法就是预共享密钥，也就是双方事先商量好一个暗号，比如“天王盖地虎，宝塔镇河妖”，对上了，就说明是对的。另外一种方法就是用数字签名来验证。咋签名呢？当然是使用私钥进行签名，私钥只有我自己有，所以如果对方能用我的数字证书里面的公钥解开，就说明我是我。 基于以上三个特性，组成了 IPsec VPN 的协议簇。这个协议簇内容比较丰富。 在这个协议簇里面，有两种协议，这两种协议的区别在于封装网络包的格式不一样。一种协议称为 AH（Authentication Header），只能进行数据摘要 ，不能实现数据加密。还有一种 ESP（Encapsulating Security Payload），能够进行数据加密和数据摘要。 在这个协议簇里面，还有两类算法，分别是加密算法和摘要算法。这个协议簇还包含两大组件，一个用于 VPN 的双方要进行对称密钥的交换的 IKE 组件，另一个是 VPN 的双方要对连接进行维护的 SA（Security Association）组件。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:29:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"IPsec VPN 的建立过程 下面来看 IPsec VPN 的建立过程，这个过程分两个阶段。第一个阶段，建立 IKE 自己的 SA。这个 SA 用来维护一个通过身份认证和安全保护的通道，为第二个阶段提供服务。在这个阶段，通过 DH（Diffie-Hellman）算法计算出一个对称密钥 K。DH 算法是一个比较巧妙的算法。客户端和服务端约定两个公开的质数 p 和 q，然后客户端随机产生一个数 a 作为自己的私钥，服务端随机产生一个 b 作为自己的私钥，客户端可以根据 p、q 和 a 计算出公钥 A，服务端根据 p、q 和 b 计算出公钥 B，然后双方交换公钥 A 和 B。到此客户端和服务端可以根据已有的信息，各自独立算出相同的结果 K，就是对称密钥。但是这个过程，对称密钥从来没有在通道上传输过，只传输了生成密钥的材料，通过这些材料，截获的人是无法算出的。 有了这个对称密钥 K，接下来是第二个阶段，建立 IPsec SA。在这个 SA 里面，双方会生成一个随机的对称密钥 M，由 K 加密传给对方，然后使用 M 进行双方接下来通信的数据。对称密钥 M 是有过期时间的，会过一段时间，重新生成一次，从而防止被破解。IPsec SA 里面有以下内容：SPI（Security Parameter Index），用于标识不同的连接；双方商量好的加密算法、哈希算法和封装模式；生存周期，超过这个周期，就需要重新生成一个 IPsec SA，重新生成对称密钥。 当 IPsec 建立好，接下来就可以开始打包封装传输了。 左面是原始的 IP 包，在 IP 头里面，会指定上一层的协议为 TCP。ESP 要对 IP 包进行封装，因而 IP 头里面的上一层协议为 ESP。在 ESP 的正文里面，ESP 的头部有双方商讨好的 SPI，以及这次传输的序列号。接下来全部是加密的内容。可以通过对称密钥进行解密，解密后在正文的最后，指明了里面的协议是什么。如果是 IP，则需要先解析 IP 头，然后解析 TCP 头，这是从隧道出来后解封装的过程。有了 IPsec VPN 之后，客户端发送的明文的 IP 包，都会被加上 ESP 头和 IP 头，在公网上传输，由于加密，可以保证不被窃取，到了对端后，去掉 ESP 的头，进行解密。 这种点对点的基于 IP 的 VPN，能满足互通的要求，但是速度往往比较慢，这是由底层 IP 协议的特性决定的。IP 不是面向连接的，是尽力而为的协议，每个 IP 包自由选择路径，到每一个路由器，都自己去找下一跳，丢了就丢了，是靠上一层 TCP 的重发来保证可靠性。 因为 IP 网络从设计的时候，就认为是不可靠的，所以即使同一个连接，也可能选择不同的道路，这样的好处是，一条道路崩溃的时候，总有其他的路可以走。当然，带来的代价就是，不断的路由查找，效率比较差。和 IP 对应的另一种技术称为 ATM。这种协议和 IP 协议的不同在于，它是面向连接的。你可以说 TCP 也是面向连接的啊。这两个不同，ATM 和 IP 是一个层次的，和 TCP 不是一个层次的。另外，TCP 所谓的面向连接，是不停地重试来保证成功，其实下层的 IP 还是不面向连接的，丢了就丢了。ATM 是传输之前先建立一个连接，形成一个虚拟的通路，一旦连接建立了，所有的包都按照相同的路径走，不会分头行事。 好处是不需要每次都查路由表的，虚拟路径已经建立，打上了标签，后续的包傻傻的跟着走就是了，不用像 IP 包一样，每个包都思考下一步怎么走，都按相同的路径走，这样效率会高很多。但是一旦虚拟路径上的某个路由器坏了，则这个连接就断了，什么也发不过去了，因为其他的包还会按照原来的路径走，都掉坑里了，它们不会选择其他的路径走。ATM 技术虽然没有成功，但其屏弃了繁琐的路由查找，改为简单快速的标签交换，将具有全局意义的路由表改为只有本地意义的标签表，这些都可以大大提高一台路由器的转发功力。有没有一种方式将两者的优点结合起来呢？这就是多协议标签交换（MPLS，Multi-Protocol Label Switching）。MPLS 的格式如图所示，在原始的 IP 头之外，多了 MPLS 的头，里面可以打标签。 在二层头里面，有类型字段，0x0800 表示 IP，0x8847 表示 MPLS Label。在 MPLS 头里面，首先是标签值占 20 位，接着是 3 位实验位，再接下来是 1 位栈底标志位，表示当前标签是否位于栈底了。这样就允许多个标签被编码到同一个数据包中，形成标签栈。最后是 8 位 TTL 存活时间字段，如果标签数据包的出发 TTL 值为 0，那么该数据包在网络中的生命期被认为已经过期了。有了标签，还需要设备认这个标签，并且能够根据这个标签转发，这种能够转发标签的路由器称为标签交换路由器（LSR，Label Switching Router）。这种路由器会有两个表格，一个就是传统的 FIB，也即路由表，另一个就是 LFIB，标签转发表。有了这两个表，既可以进行普通的路由转发，也可以进行基于标签的转发。 有了标签转发表，转发的过程如图所示，就不用每次都进行普通路由的查找了。这里我们区分 MPLS 区域和非 MPLS 区域。在 MPLS 区域中间，使用标签进行转发，非 MPLS 区域，使用普通路由转发，在边缘节点上，需要有能力将对于普通路由的转发，变成对于标签的转发。例如图中要访问 114.1.1.1，在边界上查找普通路由，发现马上要进入 MPLS 区域了，进去了对应标签 1，于是在 IP 头外面加一个标签 1，在区域里面，标签 1 要变成标签 3，标签 3 到达出口边缘，将标签去掉，按照路由发出。这样一个通过标签转换而建立的路径称为 LSP，标签交换路径。在一条 LSP 上，沿数据包传送的方向，相邻的 LSR 分别叫上游 LSR（upstream LSR）和下游 LSR（downstream LSR）。有了标签，转发是很简单的事，但是如何生成标签，却是 MPLS 中最难修炼的部分。在 MPLS 秘笈中，这部分被称为 LDP（Label Distribution Protocol），是一个动态的生成标签的协议。其实 LDP 与 IP 帮派中的路由协议十分相像，通过 LSR 的交互，互相告知去哪里应该打哪个标签，称为标签分发，往往是从下游开始的。 如果有一个边缘节点发现自己的路由表中出现了新的目的地址，它就要给别人说，我能到达一条新的路径了。如果此边缘节点存在上游 LSR，并且尚有可供分配的标签，则该节点为新的路径分配标签，并向上游发出标签映射消息，其中包含分配的标签等信息。收到标签映射消息的 LSR 记录相应的标签映射信息，在其标签转发表中增加相应的条目。此 LSR 为它的上游 LSR 分配标签，并继续向上游 LSR 发送标签映射消息。当入口 LSR 收到标签映射消息时，在标签转发表中增加相应的条目。这时，就完成了 LSP 的建立。有了标签，转发轻松多了，但是这个和 VPN 什么关系呢？可以想象，如果我们 VPN 通道里面包的转发，都是通过标签的方式进行，效率就会高很多。所以要想个办法把 MPLS 应用于 VPN。 在 MPLS VPN 中，网络中的路由器分成以下几类：PE（Provider Edge）：运营商网络与客户网络相连的边缘网络设备；CE（Customer Edge）：客户网络与 PE 相连接的边缘设备；P（Provider）：这里特指运营商网络中除 PE 之外的其他运营商网络设备。 为什么要这样分呢？因为我们发现，在运营商网络里面，也即 P Router 之间，使用标签是没有问题的，因为都在运营商的管控之下，对于网段，路由都可以自己控制。但是一旦客户要接入这个网络，就复杂得多。首先是客户地址重复的问题。客户所使用的大多数都是私网的地址 (192.168.X.X;10.X.X.X;172.X.X.X)，而且很多情况下都会与其它的客户重复。比如，机构 A 和机构 B 都使用了 192.168.101.0/24 网段的地址，这就发生了地址空间重叠（Overlapping Address Spaces）。首先困惑的是 BGP 协议，既然 VPN 将两个数据中心连起来，应该看起来像一个数据中心一样，那么如何到达另一端需要通过 BGP 将路由广播过去，传统 BGP 无法正确处理地址空间重叠的 VPN 的路由。假设机构 A 和机构 B 都使用了 192.168.101.0/24 网段的地址，并各自发布了一条去往此网段的路由，BGP 将只会选择其中一条路由，从而导致去往另一个 VPN 的路由丢失。所以 PE 路由器之间使用特殊的 MP-BGP 来发布 VPN 路由，在相互沟通的消息中，在一般 32 位 IPv4 的地址之前加上一个客户标示的区分符用于客户地址的区分，这种称为 VPN-IPv4 地址族，这样 PE 路由器会收到如下的消息，机构 A 的 192.168.101.0/24 应该往这面走，机构 B 的 192.168.101.0/24 则应该去另外一个方向。另外困惑的是路由表，当两个客户的 IP 包到达 PE 的时候，PE 就困惑了，因为网段是重复的。如何区分哪些路由是属于哪些客户 VPN 内的？如何保证 VPN 业务路由与普通路由不相互干扰？在 PE 上，可以通过 VRF（VPN Routing\u0026Forwarding Instance）建立每个客户一个路由表，与其它 VPN 客户路由和普通路由相互区分。可以理解为专属于客户的小路由器。远端 PE 通过 MP-BGP 协议把业务路由放到近端 PE，近端 PE 根据不","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:29:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下：VPN 可以将一个机构的多个数据中心通过隧道的方式连接起来，让机构感觉在一个数据中心里面，就像自驾游通过琼州海峡一样；完全基于软件的 IPsec VPN 可以保证私密性、完整性、真实性、简单便宜，但是性能稍微差一些；MPLS-VPN 综合和 IP 转发模式和 ATM 的标签转发模式的优势，性能较好，但是需要从运营商购买。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:29:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第23讲 | 移动网络：去巴塞罗那，手机也上不了脸书 前面讲的都是电脑上网的场景，那使用手机上网有什么不同呢？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:30:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"移动网络的发展历程 你一定知道手机上网有 2G、3G、4G 的说法，究竟这都是什么意思呢？有一个通俗的说法就是：用 2G 看 txt，用 3G 看 jpg，用 4G 看 avi。 2G 网络 手机本来是用来打电话的，不是用来上网的，所以原来在 2G 时代，上网使用的不是 IP 网络，而是电话网络，走模拟信号，专业名称为公共交换电话网（PSTN，Public Switched Telephone Network）。那手机不连网线，也不连电话线，它是怎么上网的呢？ 手机是通过收发无线信号来通信的，专业名称是 Mobile Station，简称 MS，需要嵌入 SIM。手机是客户端，而无线信号的服务端，就是基站子系统（BSS，Base Station SubsystemBSS）。至于什么是基站，你可以回想一下，你在爬山的时候，是不是看到过信号塔？我们平时城市里面的基站比较隐蔽，不容易看到，所以只有在山里才会注意到。正是这个信号塔，通过无线信号，让你的手机可以进行通信。但是你要知道一点，无论无线通信如何无线，最终还是要连接到有线的网络里。前面讲数据中心的时候我也讲过，电商的应用是放在数据中心的，数据中心的电脑都是插着网线的。 因而，基站子系统分两部分，一部分对外提供无线通信，叫作基站收发信台（BTS，Base Transceiver Station），另一部分对内连接有线网络，叫作基站控制器（BSC，Base Station Controller）。基站收发信台通过无线收到数据后，转发给基站控制器。这部分属于无线的部分，统称为无线接入网（RAN，Radio Access Network）。基站控制器通过有线网络，连接到提供手机业务的运营商的数据中心，这部分称为核心网（CN，Core Network）。核心网还没有真的进入互联网，这部分还是主要提供手机业务，是手机业务的有线部分。 首先接待基站来的数据的是移动业务交换中心（MSC，Mobile Service Switching Center），它是进入核心网的入口，但是它不会让你直接连接到互联网上。因为在让你的手机真正进入互联网之前，提供手机业务的运营商，需要认证是不是合法的手机接入。你别自己造了一张手机卡，就连接上来。鉴权中心（AUC，Authentication Center）和设备识别寄存器（EIR，Equipment Identity Register）主要是负责安全性的。另外，需要看你是本地的号，还是外地的号，这个牵扯到计费的问题，异地收费还是很贵的。访问位置寄存器（VLR，Visit Location Register）是看你目前在的地方，归属位置寄存器（HLR，Home Location Register）是看你的号码归属地。 当你的手机卡既合法又有钱的时候，才允许你上网，这个时候需要一个网关，连接核心网和真正的互联网。网关移动交换中心（GMSC ，Gateway Mobile Switching Center）就是干这个的，然后是真正的互连网。在 2G 时代，还是电话网络 PSTN。数据中心里面的这些模块统称为网络子系统（NSS，Network and Switching Subsystem）。 因而 2G 时代的上网如图所示，我们总结一下，有这几个核心点： 手机通过无线信号连接基站；基站一面朝前接无线，一面朝后接核心网；核心网一面朝前接到基站请求，一是判断你是否合法，二是判断你是不是本地号，还有没有钱，一面通过网关连接电话网络。 2.5G 网络 后来从 2G 到了 2.5G，也即在原来电路交换的基础上，加入了分组交换业务，支持 Packet 的转发，从而支持 IP 网络。在上述网络的基础上，基站一面朝前接无线，一面朝后接核心网。在朝后的组件中，多了一个分组控制单元（PCU，Packet Control Unit），用以提供分组交换通道。在核心网里面，有个朝前的接待员（SGSN，Service GPRS Supported Node）和朝后连接 IP 网络的网关型 GPRS 支持节点（GGSN，Gateway GPRS Supported Node）。 3G 网络 到了 3G 时代，主要是无线通信技术有了改进，大大增加了无线的带宽。以 W-CDMA 为例，理论最高 2M 的下行速度，因而基站改变了，一面朝外的是 Node B，一面朝内连接核心网的是无线网络控制器（RNC，Radio Network Controller）。核心网以及连接的 IP 网络没有什么变化。 4G 网络 然后就到了今天的 4G 网络，基站为 eNodeB，包含了原来 Node B 和 RNC 的功能，下行速度向百兆级别迈进。另外，核心网实现了控制面和数据面的分离，这个怎么理解呢？在前面的核心网里面，有接待员 MSC 或者 SGSN，你会发现检查是否合法是它负责，转发数据也是它负责，也即控制面和数据面是合二为一的，这样灵活性比较差，因为控制面主要是指令，多是小包，往往需要高的及时性；数据面主要是流量，多是大包，往往需要吞吐量。于是有了下面这个架构。 HSS 用于存储用户签约信息的数据库，其实就是你这个号码归属地是哪里的，以及一些认证信息。MME 是核心控制网元，是控制面的核心，当手机通过 eNodeB 连上的时候，MME 会根据 HSS 的信息，判断你是否合法。如果允许连上来，MME 不负责具体的数据的流量，而是 MME 会选择数据面的 SGW 和 PGW，然后告诉 eNodeB，我允许你连上来了，你连接它们吧。于是手机直接通过 eNodeB 连接 SGW，连上核心网，SGW 相当于数据面的接待员，并通过 PGW 连到 IP 网络。PGW 就是出口网关。在出口网关，有一个组件 PCRF，称为策略和计费控制单元，用来控制上网策略和流量的计费。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:30:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"4G 网络协议解析 我们来仔细看一下 4G 网络的协议，真的非常复杂。我们将几个关键组件放大来看。 控制面协议 其中虚线部分是控制面的协议。当一个手机想上网的时候，先要连接 eNodeB，并通过 S1-MME 接口，请求 MME 对这个手机进行认证和鉴权。S1-MME 协议栈如下图所示。 UE 就是你的手机，eNodeB 还是两面派，朝前对接无线网络，朝后对接核心网络，在控制面对接的是 MME。eNodeB 和 MME 之间的连接就是很正常的 IP 网络，但是这里面在 IP 层之上，却既不是 TCP，也不是 UDP，而是 SCTP。这也是传输层的协议，也是面向连接的，但是更加适合移动网络。 它继承了 TCP 较为完善的拥塞控制并改进 TCP 的一些不足之处。SCTP 的第一个特点是多宿主。一台机器可以有多个网卡，而对于 TCP 连接来讲，虽然服务端可以监听 0.0.0.0，也就是从哪个网卡来的连接都能接受，但是一旦建立了连接，就建立了四元组，也就选定了某个网卡。SCTP 引入了联合（association）的概念，将多个接口、多条路径放到一个联合中来。当检测到一条路径失效时，协议就会通过另外一条路径来发送通信数据。应用程序甚至都不必知道发生了故障、恢复，从而提供更高的可用性和可靠性。 SCTP 的第二个特点是将一个联合分成多个流。一个联合中的所有流都是独立的，但均与该联合相关。每个流都给定了一个流编号，它被编码到 SCTP 报文中，通过联合在网络上传送。在 TCP 的机制中，由于强制顺序，导致前一个不到达，后一个就得等待，SCTP 的多个流不会相互阻塞。SCTP 的第三个特点是四次握手，防止 SYN 攻击。在 TCP 中是三次握手，当服务端收到客户的 SYN 之后，返回一个 SYN-ACK 之前，就建立数据结构，并记录下状态，等待客户端发送 ACK 的 ACK。当恶意客户端使用虚假的源地址来伪造大量 SYN 报文时，服务端需要分配大量的资源，最终耗尽资源，无法处理新的请求。SCTP 可以通过四次握手引入 Cookie 的概念，来有效地防止这种攻击的产生。在 SCTP 中，客户机使用一个 INIT 报文发起一个连接。服务器使用一个 INIT-ACK 报文进行响应，其中就包括了 Cookie。然后客户端就使用一个 COOKIE-ECHO 报文进行响应，其中包含了服务器所发送的 Cookie。这个时候，服务器为这个连接分配资源，并通过向客户机发送一个 COOKIE-ACK 报文对其进行响应。 SCTP 的第四个特点是将消息分帧。TCP 是面向流的，也即发送的数据没头没尾，没有明显的界限。这对于发送数据没有问题，但是对于发送一个个消息类型的数据，就不太方便。有可能客户端写入 10 个字节，然后再写入 20 个字节。服务端不是读出 10 个字节的一个消息，再读出 20 个字节的一个消息，而有可能读入 25 个字节，再读入 5 个字节，需要业务层去组合成消息。SCTP 借鉴了 UDP 的机制，在数据传输中提供了消息分帧功能。当一端对一个套接字执行写操作时，可确保对等端读出的数据大小与此相同。SCTP 的第五个特点是断开连接是三次挥手。在 TCP 里面，断开连接是四次挥手，允许另一端处于半关闭的状态。SCTP 选择放弃这种状态，当一端关闭自己的套接字时，对等的两端全部需要关闭，将来任何一端都不允许再进行数据的移动了。当 MME 通过认证鉴权，同意这个手机上网的时候，需要建立一个数据面的数据通路。建立通路的过程还是控制面的事情，因而使用的是控制面的协议 GTP-C。 建设的数据通路分两段路，其实是两个隧道。一段是从 eNodeB 到 SGW，这个数据通路由 MME 通过 S1-MME 协议告诉 eNodeB，它是隧道的一端，通过 S11 告诉 SGW，它是隧道的另一端。第二端是从 SGW 到 PGW，SGW 通过 S11 协议知道自己是其中一端，并主动通过 S5 协议，告诉 PGW 它是隧道的另一端。GTP-C 协议是基于 UDP 的，这是UDP 的“城会玩”中的一个例子。如果看 GTP 头，我们可以看到，这里面有隧道的 ID，还有序列号。 通过序列号，不用 TCP，GTP-C 自己就可以实现可靠性，为每个输出信令消息分配一个依次递增的序列号，以确保信令消息的按序传递，并便于检测重复包。对于每个输出信令消息启动定时器，在定时器超时前未接收到响应消息则进行重发。 数据面协议 当两个隧道都打通，接在一起的时候，PGW 会给手机分配一个 IP 地址，这个 IP 地址是隧道内部的 IP 地址，可以类比为 IPsec 协议里面的 IP 地址。这个 IP 地址是归手机运营商管理的。然后，手机可以使用这个 IP 地址，连接 eNodeB，从 eNodeB 经过 S1-U 协议，通过第一段隧道到达 SGW，再从 SGW 经过 S8 协议，通过第二段隧道到达 PGW，然后通过 PGW 连接到互联网。数据面的协议都是通过 GTP-U，如图所示。 手机每发出的一个包，都由 GTP-U 隧道协议封装起来，格式如下。 和 IPsec 协议很类似，分为乘客协议、隧道协议、承载协议。其中乘客协议是手机发出来的包，IP 是手机的 IP，隧道协议里面有隧道 ID，不同的手机上线会建立不同的隧道，因而需要隧道 ID 来标识。承载协议的 IP 地址是 SGW 和 PGW 的 IP 地址。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:30:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"手机上网流程 接下来，我们来看一个手机开机之后上网的流程，这个过程称为 Attach。可以看出来，移动网络还是很复杂的。因为这个过程要建立很多的隧道，分配很多的隧道 ID，所以我画了一个图来详细说明这个过程。 手机开机以后，在附近寻找基站 eNodeB，找到后给 eNodeB 发送 Attach Request，说“我来啦，我要上网”。eNodeB 将请求发给 MME，说“有个手机要上网”。MME 去请求手机，一是认证，二是鉴权，还会请求 HSS 看看有没有钱，看看是在哪里上网。当 MME 通过了手机的认证之后，开始分配隧道，先告诉 SGW，说要创建一个会话（Create Session）。在这里面，会给 SGW 分配一个隧道 ID t1，并且请求 SGW 给自己也分配一个隧道 ID。SGW 转头向 PGW 请求建立一个会话，为 PGW 的控制面分配一个隧道 ID t2，也给 PGW 的数据面分配一个隧道 ID t3，并且请求 PGW 给自己的控制面和数据面分配隧道 ID。PGW 回复 SGW 说“创建会话成功”，使用自己的控制面隧道 ID t2，回复里面携带着给 SGW 控制面分配的隧道 ID t4 和控制面的隧道 ID t5，至此 SGW 和 PGW 直接的隧道建设完成。双方请求对方，都要带着对方给自己分配的隧道 ID，从而标志是这个手机的请求。接下来 SGW 回复 MME 说“创建会话成功”，使用自己的隧道 ID t1 访问 MME，回复里面有给 MME 分配隧道 ID t6，也有 SGW 给 eNodeB 分配的隧道 ID t7。当 MME 发现后面的隧道都建设成功之后，就告诉 eNodeB，“后面的隧道已经建设完毕，SGW 给你分配的隧道 ID 是 t7，你可以开始连上来了，但是你也要给 SGW 分配一个隧道 ID”。eNodeB 告诉 MME 自己给 SGW 分配一个隧道，ID 为 t8。MME 将 eNodeB 给 SGW 分配的隧道 ID t8 告知 SGW，从而前面的隧道也建设完毕。 这样，手机就可以通过建立的隧道成功上网了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:30:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"异地上网问题 接下来我们考虑异地上网的事情。为什么要分 SGW 和 PGW 呢，一个 GW 不可以吗？SGW 是你本地的运营商的设备，而 PGW 是你所属的运营商的设备。如果你在巴塞罗那，一下飞机，手机开机，周围搜寻到的肯定是巴塞罗那的 eNodeB。通过 MME 去查询国内运营商的 HSS，看你是否合法，是否还有钱。如果允许上网，你的手机和巴塞罗那的 SGW 会建立一个隧道，然后巴塞罗那的 SGW 和国内运营商的 PGW 建立一个隧道，然后通过国内运营商的 PGW 上网。 因此，判断你是否能上网的是国内运营商的 HSS，控制你上网策略的是国内运营商的 PCRF，给手机分配的 IP 地址也是国内运营商的 PGW 负责的，给手机分配的 IP 地址也是国内运营商里统计的。运营商由于是在 PGW 里面统计的，这样你的上网流量全部通过国内运营商即可，只不过巴塞罗那运营商也要和国内运营商进行流量结算。由于你的上网策略是由国内运营商在 PCRF 中控制的，因而你还是上不了脸书。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:30:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下：移动网络的发展历程从 2G 到 3G，再到 4G，逐渐从打电话的功能为主，向上网的功能为主转变；请记住 4G 网络的结构，有 eNodeB、MME、SGW、PGW 等，分控制面协议和数据面协议，你可以对照着结构，试着说出手机上网的流程；即便你在国外的运营商下上网，也是要通过国内运营商控制的，因而也上不了脸书。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:30:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"云计算中的网络 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:31:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第24讲 | 云中网络：自己拿地成本高，购买公寓更灵活 前面我们讲了，数据中心里面堆着一大片一大片的机器，用网络连接起来，机器数目一旦非常多，人们就发现，维护这么一大片机器还挺麻烦的，有好多不灵活的地方。采购不灵活：如果客户需要一台电脑，那就需要自己采购、上架、插网线、安装操作系统，周期非常长。一旦采购了，一用就 N 年，不能退货，哪怕业务不做了，机器还在数据中心里留着。运维不灵活：一旦需要扩容 CPU、内存、硬盘，都需要去机房手动弄，非常麻烦。规格不灵活：采购的机器往往动不动几百 G 的内存，而每个应用往往可能只需要 4 核 8G，所以很多应用混合部署在上面，端口各种冲突，容易相互影响。复用不灵活：一台机器，一旦一个用户不用了，给另外一个用户，那就需要重装操作系统。因为原来的操作系统可能遗留很多数据，非常麻烦。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:32:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"从物理机到虚拟机 为了解决这些问题，人们发明了一种叫虚拟机的东西，并基于它产生了云计算技术。其实在你的个人电脑上，就可以使用虚拟机。如果你对虚拟机没有什么概念，你可以下载一个桌面虚拟化的软件，自己动手尝试一下。它可以让你灵活地指定 CPU 的数目、内存的大小、硬盘的大小，可以有多个网卡，然后在一台笔记本电脑里面创建一台或者多台虚拟电脑。不用的时候，一点删除就没有了。在数据中心里面，也有一种类似的开源技术 qemu-kvm，能让你在一台巨大的物理机里面，掏出一台台小的机器。这套软件就能解决上面的问题：一点就能创建，一点就能销毁。你想要多大就有多大，每次创建的系统还都是新的。 我们常把物理机比喻为自己拿地盖房子，而虚拟机则相当于购买公寓，更加灵活方面，随时可买可卖。 那这个软件为什么能做到这些事儿呢？它用的是软件模拟硬件的方式。刚才说了，数据中心里面用的 qemu-kvm。从名字上来讲，emu 就是 Emulator（模拟器）的意思，主要会模拟 CPU、内存、网络、硬盘，使得虚拟机感觉自己在使用独立的设备，但是真正使用的时候，当然还是使用物理的设备。例如，多个虚拟机轮流使用物理 CPU，内存也是使用虚拟内存映射的方式，最终映射到物理内存上。硬盘在一块大的文件系统上创建一个 N 个 G 的文件，作为虚拟机的硬盘。简单比喻，虚拟化软件就像一个“骗子”，向上“骗”虚拟机里面的应用，让它们感觉独享资源，其实自己啥都没有，全部向下从物理机里面弄。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:32:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"虚拟网卡的原理 那网络是如何“骗”应用的呢？如何将虚拟机的网络和物理机的网络连接起来？ 首先，虚拟机要有一张网卡。对于 qemu-kvm 来说，这是通过 Linux 上的一种 TUN/TAP 技术来实现的。虚拟机是物理机上跑着的一个软件。这个软件可以像其他应用打开文件一样，打开一个称为 TUN/TAP 的 Char Dev（字符设备文件）。打开了这个字符设备文件之后，在物理机上就能看到一张虚拟 TAP 网卡。虚拟化软件作为“骗子”，会将打开的这个文件，在虚拟机里面虚拟出一张网卡，让虚拟机里面的应用觉得它们真有一张网卡。于是，所有的网络包都往这里发。当然，网络包会到虚拟化软件这里。它会将网络包转换成为文件流，写入字符设备，就像写一个文件一样。内核中 TUN/TAP 字符设备驱动会收到这个写入的文件流，交给 TUN/TAP 的虚拟网卡驱动。这个驱动将文件流再次转成网络包，交给 TCP/IP 协议栈，最终从虚拟 TAP 网卡发出来，成为标准的网络包。就这样，几经转手，数据终于从虚拟机里面，发到了虚拟机外面。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:32:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"虚拟网卡连接到云中 我们就这样有了虚拟 TAP 网卡。接下来就要看，这个卡怎么接入庞大的数据中心网络中。在接入之前，我们先来看，云计算中的网络都需要注意哪些点。 共享：尽管每个虚拟机都会有一个或者多个虚拟网卡，但是物理机上可能只有有限的网卡。那这么多虚拟网卡如何共享同一个出口？隔离：分两个方面，一个是安全隔离，两个虚拟机可能属于两个用户，那怎么保证一个用户的数据不被另一个用户窃听？一个是流量隔离，两个虚拟机，如果有一个疯狂下片，会不会导致另外一个上不了网？互通：分两个方面，一个是如果同一台机器上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？另一个是如果不同物理机上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？灵活：虚拟机和物理不同，会经常创建、删除，从一个机器漂移到另一台机器，有的互通、有的不通等等，灵活性比物理网络要好得多，需要能够灵活配置。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:32:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"共享与互通问题 这些问题，我们一个个来解决。首先，一台物理机上有多个虚拟机，有多个虚拟网卡，这些虚拟网卡如何连在一起，进行相互访问，并且可以访问外网呢？还记得我们在大学宿舍里做的事情吗？你可以想象你的物理机就是你们宿舍，虚拟机就是你的个人电脑，这些电脑应该怎么连接起来呢？当然应该买一个交换机。在物理机上，应该有一个虚拟的交换机，在 Linux 上有一个命令叫作 brctl，可以创建虚拟的网桥 brctl addbr br0。创建出来以后，将两个虚拟机的虚拟网卡，都连接到虚拟网桥 brctl addif br0 tap0 上，这样将两个虚拟机配置相同的子网网段，两台虚拟机就能够相互通信了。 那这些虚拟机如何连接外网呢？在桌面虚拟化软件上面，我们能看到以下选项。 这里面，host-only 的网络对应的，其实就是上面两个虚拟机连到一个 br0 虚拟网桥上，而且不考虑访问外部的场景，只要虚拟机之间能够相互访问就可以了。如果要访问外部，往往有两种方式。一种方式称为桥接。如果在桌面虚拟化软件上选择桥接网络，则在你的笔记本电脑上，就会形成下面的结构。 每个虚拟机都会有虚拟网卡，在你的笔记本电脑上，会发现多了几个网卡，其实是虚拟交换机。这个虚拟交换机将虚拟机连接在一起。在桥接模式下，物理网卡也连接到这个虚拟交换机上，物理网卡在桌面虚拟化软件上，在“界面名称”那里选定。如果使用桥接网络，当你登录虚拟机里看 IP 地址的时候会发现，你的虚拟机的地址和你的笔记本电脑的，以及你旁边的同事的电脑的网段是一个网段。这是为什么呢？这其实相当于将物理机和虚拟机放在同一个网桥上，相当于这个网桥上有三台机器，是一个网段的，全部打平了。我将图画成下面的样子你就好理解了。 在数据中心里面，采取的也是类似的技术，只不过都是 Linux，在每台机器上都创建网桥 br0，虚拟机的网卡都连到 br0 上，物理网卡也连到 br0 上，所有的 br0 都通过物理网卡出来连接到物理交换机上。 同样我们换一个角度看待这个拓扑图。同样是将网络打平，虚拟机会和你的物理网络具有相同的网段。 在这种方式下，不但解决了同一台机器的互通问题，也解决了跨物理机的互通问题，因为都在一个二层网络里面，彼此用相同的网段访问就可以了。但是当规模很大的时候，会存在问题。你还记得吗？在一个二层网络里面，最大的问题是广播。一个数据中心的物理机已经很多了，广播已经非常严重，需要通过 VLAN 进行划分。如果使用了虚拟机，假设一台物理机里面创建 10 台虚拟机，全部在一个二层网络里面，那广播就会很严重，所以除非是你的桌面虚拟机或者数据中心规模非常小，才可以使用这种相对简单的方式。另外一种方式称为 NAT。如果在桌面虚拟化软件中使用 NAT 模式，在你的笔记本电脑上会出现如下的网络结构。 在这种方式下，你登录到虚拟机里面查看 IP 地址，会发现虚拟机的网络是虚拟机的，物理机的网络是物理机的，两个不相同。虚拟机要想访问物理机的时候，需要将地址 NAT 成为物理机的地址。除此之外，它还会在你的笔记本电脑里内置一个 DHCP 服务器，为笔记本电脑上的虚拟机动态分配 IP 地址。因为虚拟机的网络自成体系，需要进行 IP 管理。为什么桥接方式不需要呢？因为桥接将网络打平了，虚拟机的 IP 地址应该由物理网络的 DHCP 服务器分配。在数据中心里面，也是使用类似的方式。这种方式更像是真的将你宿舍里面的情况，搬到一台物理机上来。 虚拟机是你的电脑，路由器和 DHCP Server 相当于家用路由器或者寝室长的电脑，物理网卡相当于你们宿舍的外网网口，用于访问互联网。所有电脑都通过内网网口连接到一个网桥 br0 上，虚拟机要想访问互联网，需要通过 br0 连到路由器上，然后通过路由器将请求 NAT 成为物理网络的地址，转发到物理网络。如果是你自己登录到物理机上做个简单配置，你可以简化一下。例如将虚拟机所在网络的网关的地址直接配置到 br0 上，不用 DHCP Server，手动配置每台虚拟机的 IP 地址，通过命令 iptables -t nat -A POSTROUTING -o ethX -j MASQUERADE，直接在物理网卡 ethX 上进行 NAT，所有从这个网卡出去的包都 NAT 成这个网卡的地址。通过设置 net.ipv4.ip_forward = 1，开启物理机的转发功能，直接做路由器，而不用单独的路由器，这样虚拟机就能直接上网了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:32:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"隔离问题 解决了互通的问题，接下来就是隔离的问题。如果一台机器上的两个虚拟机不属于同一个用户，怎么办呢？好在 brctl 创建的网桥也是支持 VLAN 功能的，可以设置两个虚拟机的 tag，这样在这个虚拟网桥上，两个虚拟机是不互通的。但是如何跨物理机互通，并且实现 VLAN 的隔离呢？由于 brctl 创建的网桥上面的 tag 是没办法在网桥之外的范围内起作用的，因此我们需要寻找其他的方式。有一个命令 vconfig，可以基于物理网卡 eth0 创建带 VLAN 的虚拟网卡，所有从这个虚拟网卡出去的包，都带这个 VLAN，如果这样，跨物理机的互通和隔离就可以通过这个网卡来实现。 首先为每个用户分配不同的 VLAN，例如有一个用户 VLAN 10，一个用户 VLAN 20。在一台物理机上，基于物理网卡，为每个用户用 vconfig 创建一个带 VLAN 的网卡。不同的用户使用不同的虚拟网桥，带 VLAN 的虚拟网卡也连接到虚拟网桥上。这样是否能保证两个用户的隔离性呢？不同的用户由于网桥不通，不能相互通信，一旦出了网桥，由于 VLAN 不同，也不会将包转发到另一个网桥上。另外，出了物理机，也是带着 VLAN ID 的。只要物理交换机也是支持 VLAN 的，到达另一台物理机的时候，VLAN ID 依然在，它只会将包转发给相同 VLAN 的网卡和网桥，所以跨物理机，不同的 VLAN 也不会相互通信。使用 brctl 创建出来的网桥功能是简单的，基于 VLAN 的虚拟网卡也能实现简单的隔离。但是这都不是大规模云平台能够满足的，一个是 VLAN 的隔离，数目太少。前面我们学过，VLAN ID 只有 4096 个，明显不够用。另外一点是这个配置不够灵活。谁和谁通，谁和谁不通，流量的隔离也没有实现，还有大量改进的空间。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:32:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下：云计算的关键技术是虚拟化，这里我们重点关注的是，虚拟网卡通过打开 TUN/TAP 字符设备的方式，将虚拟机内外连接起来；云中的网络重点关注四个方面，共享、隔离、互通、灵活。其中共享和互通有两种常用的方式，分别是桥接和 NAT，隔离可以通过 VLAN 的方式。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:32:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第25讲 | 软件定义网络：共享基础设施的小区物业管理办法 上一节我们说到，使用原生的 VLAN 和 Linux 网桥的方式来进行云平台的管理，但是这样在灵活性、隔离性方面都显得不足，而且整个网络缺少统一的视图、统一的管理。可以这样比喻，云计算就像大家一起住公寓，要共享小区里面的基础设施，其中网络就相当于小区里面的电梯、楼道、路、大门等，大家都走，往往会常出现问题，尤其在上班高峰期，出门的人太多，对小区的物业管理就带来了挑战。物业可以派自己的物业管理人员，到每个单元的楼梯那里，将电梯的上下行速度调快一点，可以派人将隔离健身区、景色区的栅栏门暂时打开，让大家可以横穿小区，直接上地铁，还可以派人将多个小区出入口，改成出口多、入口少等等。等过了十点半，上班高峰过去，再派人都改回来。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:33:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"软件定义网络（SDN） 这种模式就像传统的网络设备和普通的 Linux 网桥的模式，配置整个云平台的网络通路，你需要登录到这台机器上配置这个，再登录到另外一个设备配置那个，才能成功。如果物业管理人员有一套智能的控制系统，在物业监控室里就能看到小区里每个单元、每个电梯的人流情况，然后在监控室里面，只要通过远程控制的方式，拨弄一个手柄，电梯的速度就调整了，栅栏门就打开了，某个入口就改出口了。这就是软件定义网络（SDN）。它主要有以下三个特点。 控制与转发分离：转发平面就是一个个虚拟或者物理的网络设备，就像小区里面的一条条路。控制平面就是统一的控制中心，就像小区物业的监控室。它们原来是一起的，物业管理员要从监控室出来，到路上去管理设备，现在是分离的，路就是走人的，控制都在监控室。控制平面与转发平面之间的开放接口：控制器向上提供接口，被应用层调用，就像总控室提供按钮，让物业管理员使用。控制器向下调用接口，来控制网络设备，就像总控室会远程控制电梯的速度。这里经常使用两个名词，前面这个接口称为北向接口，后面这个接口称为南向接口，上北下南嘛。逻辑上的集中控制：逻辑上集中的控制平面可以控制多个转发面设备，也就是控制整个物理网络，因而可以获得全局的网络状态视图，并根据该全局网络状态视图实现对网络的优化控制，就像物业管理员在监控室能够看到整个小区的情况，并根据情况优化出入方案。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:33:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"OpenFlow 和 OpenvSwitch SDN 有很多种实现方式，我们来看一种开源的实现方式。OpenFlow 是 SDN 控制器和网络设备之间互通的南向接口协议，OpenvSwitch 用于创建软件的虚拟交换机。OpenvSwitch 是支持 OpenFlow 协议的，当然也有一些硬件交换机也支持 OpenFlow 协议。它们都可以被统一的 SDN 控制器管理，从而实现物理机和虚拟机的网络连通。 SDN 控制器是如何通过 OpenFlow 协议控制网络的呢？ 在 OpenvSwitch 里面，有一个流表规则，任何通过这个交换机的包，都会经过这些规则进行处理，从而接收、转发、放弃。那流表长啥样呢？其实就是一个个表格，每个表格好多行，每行都是一条规则。每条规则都有优先级，先看高优先级的规则，再看低优先级的规则。 对于每一条规则，要看是否满足匹配条件。这些条件包括，从哪个端口进来的，网络包头里面有什么等等。满足了条件的网络包，就要执行一个动作，对这个网络包进行处理。可以修改包头里的内容，可以跳到任何一个表格，可以转发到某个网口出去，也可以丢弃。通过这些表格，可以对收到的网络包随意处理。 具体都能做什么处理呢？通过上面的表格可以看出，简直是想怎么处理怎么处理，可以覆盖 TCP/IP 协议栈的四层。 对于物理层：匹配规则包括从哪个口进来；执行动作包括从哪个口出去。 对于 MAC 层：匹配规则包括：源 MAC 地址是多少？（dl_src），目标 MAC 是多少？（dl_dst），所属 vlan 是多少？（dl_vlan）；执行动作包括：修改源 MAC（mod_dl_src），修改目标 MAC（mod_dl_dst），修改 VLAN（mod_vlan_vid），删除 VLAN（strip_vlan），MAC 地址学习（learn）。 对于网络层：匹配规则包括：源 IP 地址是多少？(nw_src)，目标 IP 是多少？（nw_dst）。执行动作包括：修改源 IP 地址（mod_nw_src），修改目标 IP 地址（mod_nw_dst）。 对于传输层：匹配规则包括：源端口是多少？（tp_src），目标端口是多少？（tp_dst）。执行动作包括：修改源端口（mod_tp_src），修改目标端口（mod_tp_dst）。 总而言之，对于 OpenvSwitch 来讲，网络包到了我手里，就是一个 Buffer，我想怎么改怎么改，想发到哪个端口就发送到哪个端口。OpenvSwitch 有本地的命令行可以进行配置，能够实验咱们前面讲过的一些功能。我们可以通过 OpenvSwitch 的命令创建一个虚拟交换机。然后可以将多个虚拟端口 port 添加到这个虚拟交换机上。比如说下面这个 add-br 命令，就是创建虚拟交换机的。 ovs-vsctl add-br br0 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:33:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"实验一：用 OpenvSwitch 实现 VLAN 的功能 下面我们实验一下通过 OpenvSwitch 实现 VLAN 的功能，在 OpenvSwitch 中端口 port 分两种，分别叫做 access port 和 trunk port。第一类是 access port： 这个端口可以配置一个 tag，其实就是一个 VLAN ID，从这个端口进来的包都会被打上这个 tag；如果网络包本身带有某个 VLAN ID 并且等于这个 tag，则这个包就会从这个 port 发出去；从 access port 发出的包就会把 VLAN ID 去掉。 第二类是 trunk port：这个 port 是不配置任何 tag 的，配置叫 trunks 的参数；如果 trunks 为空，则所有的 VLAN 都 trunk，也就意味着对于所有的 VLAN 的包，无论本身带什么 VLAN ID，我还是让他携带着这个 VLAN ID，如果没有设置 VLAN，就属于 VLAN 0，全部允许通过；如果 trunks 不为空，则仅仅允许带着这些 VLAN ID 的包通过。 我们通过以下命令创建如下的环境： ovs-vsctl add-port br0 first_br ovs-vsctl add-port br0 second_br ovs-vsctl add-port br0 third_br ovs-vsctl set Port vnet0 tag=101 ovs-vsctl set Port vnet1 tag=102 ovs-vsctl set Port vnet2 tag=103 ovs-vsctl set Port first_br tag=103 ovs-vsctl clear Port second_br tag ovs-vsctl set Port third_br trunks=101,102 另外要配置禁止 MAC 地址学习。 ovs-vsctl set bridge br0 flood-vlans=101,102,103 这样就形成了如下的拓扑图，有三个虚拟机，有三个网卡，都连到一个叫 br0 的网桥上，并且他们被都打了不同的 VLAN tag。 创建好了环境以后，我们来做这个实验。首先，我们从 192.168.100.102 来 ping 192.168.100.103，然后用 tcpdump 进行抓包。由于 192.168.100.102 和 first_br 都配置了 tag103，也就是说他们都属于同一个 VLAN 103 的，因而这个 first_if 是能够收到包的。但是根据 access port 的规则，从 first_br 出来的包头是没有带 VLAN ID 的。由于 second_br 是 trunk port，所有的 VLAN 都会放行，因而 second_if 也是能收到包的，并且根据 trunk port 的规则，出来的包的包头里面是带有 VLAN ID 的。由于 third_br 仅仅配置了允许 VLAN 101 和 102 通过，不允许 103 通过，因而 third_if 他是收不到包的。 然后我们再尝试，从 192.168.100.100 来 ping 192.168.100.105。 因为 192.168.100.100 是配置了 VLAN 101 的，因为 second_br 是配置了 trunk 的，是全部放行的，所以说 second_if 是可以收到包的。那 third_br 是配置了可以放行 VLAN 101 和 102，所以说 third_if 是可以收到包的。当然 ping 不通，因为从 third_br 出来的包是带 VLAN 的，而 third_if 他本身不属于某个 VLAN，所以说他 ping 不通，但是能够收到包这里补充说明一下，收到包和 ping 不同不矛盾，要想 ping 的通，需要发送 ICMP 包，并且收到回复，而仅仅收到包，则不需要回复。这里正是这种情况，third_if 收到了这个包，但是发现 VLAN ID 匹配不上，就会把包丢了，不回复，也就 Ping 不通了。first_br 是属于 VLAN 103 的，因而 first_if 是收不到包的。second_if 是能够收到包的，而且可以看到包头里面是带 VLAN 101 的。third_if 也是能收到包的，而且包头里面也是带 VLAN I101 的。最后我们再尝试，从 192.168.100.101 来 ping 192.168.100.104，因为 192.168.100.101 是属于 VLAN 102 的， 因而 second_if 和 third_if 都因为配置了 trunk，是都可以收到包的。first_br 是属于 VLAN 103 的，他不属于 VLAN 102，所以 first_if 是收不到包的。second_br 能够收到包，并且包头里面是带 VLAN ID 102 的。third_if 也能收到包，并且包头里面也是带 VLAN ID 102 的。通过这个例子，我们可以看到，通过 OpenvSwitch，不用买一个支持 VLAN 的交换机，你也能学习 VLAN 的工作模式了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:33:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"实验二：用 OpenvSwitch 模拟网卡绑定，连接交换机 接下来，我们来做另一个实验。在前面，我们还说过，为了高可用，可以使用网卡绑定，连接到交换机，OpenvSwitch 也可以模拟这一点。 在 OpenvSwitch 里面，有个 bond_mode，可以设置为以下三个值：active-backup：一个连接是 active，其他的是 backup，只有当 active 失效的时候，backup 才顶上；balance-slb：流量按照源 MAC 和 output VLAN 进行负载均衡；balance-tcp：必须在支持 LACP 协议的情况下才可以，可根据 L2、L3、L4 进行负载均衡（L2、L3、L4 指的是网络协议 2、3、4 层） 我们搭建一个测试环境。这个测试环境是两台虚拟机连接到 br0 上，另外两台虚拟机连接到 br1 上，br0 和 br1 之间通过两条通路进行 bond（绑定）。形成如下的拓扑图。 我们使用下面的命令，建立 bond 连接。 ovs-vsctl add-bond br0 bond0 first_br second_br ovs-vsctl add-bond br1 bond1 first_if second_if ovs-vsctl set Port bond0 lacp=active ovs-vsctl set Port bond1 lacp=active 默认情况下 bond_mode 是 active-backup 模式，一开始 active 的是左面这条路，也即 first_br 和 first_if 这条路。这个时候如果我们从 192.168.100.100 来 ping 192.168.100.102，以及从 192.168.100.101 来 ping 192.168.100.103 的时候，我从 tcpdump 可以看到所有的包都是从 first_if 这条路通过。接下来，如果我们把 first_if 这个网卡设成 down 的模式，则包的走向就会改变，你会发现 second_if 这条路开始有流量了，对于 192.168.100.100 和 192.168.100.101 从应用层来讲，感觉似乎没有收到影响。如果我们通过以下命令，把 bond_mode 改为 balance-slb。然后我们同时在 192.168.100.100 来 ping 192.168.100.102，同时也在 192.168.100.101 来 ping 192.168.100.103，我们通过 tcpdump 会发现，包已经被分流了。 ovs-vsctl set Port bond0 bond_mode=balance-slb ovs-vsctl set Port bond1 bond_mode=balance-slb 通过这个例子，我们可以看到，通过 OpenvSwitch，你不用买两台支持 bond 的交换机，也能看到 bond 的效果。那 OpenvSwitch 是怎么做到这些的呢？我们来看 OpenvSwitch 的架构图。OpenvSwitch 包含很多的模块，在用户态有两个重要的进程，也有两个重要的命令行工具。 第一个进程是 OVSDB 进程。ovs-vsctl 命令行会和这个进程通信，去创建虚拟交换机，创建端口，将端口添加到虚拟交换机上，OVSDB 会将这些拓扑信息保存在一个本地的文件中。第二个进程是 vswitchd 进程。ovs-ofctl 命令行会和这个进程通信，去下发流表规则，规则里面会规定如何对网络包进行处理，vswitchd 会将流表放在用户态 Flow Table 中。 在内核态，OpenvSwitch 有内核模块 OpenvSwitch.ko，对应图中的 Datapath 部分。他会在网卡上注册一个函数，每当有网络包到达网卡的时候，这个函数就会被调用。在内核的这个函数里面，会拿到网络包，将各个层次的重要信息拿出来，例如： 在物理层，会拿到 in_port，即包是从哪个网口进来的。；在 MAC 层，会拿到源和目的 MAC 地址；在 IP 层，会拿到源和目的 IP 地址；在传输层，会拿到源和目的端口号。 在内核中，还有一个内核态 Flow Table。接下来内核态模块在这个内核态的流表中匹配规则，如果匹配上了，就执行相应的操作，比如修改包，或者转发，或者放弃。如果内核没有匹配上，这个时候就需要进入用户态，用户态和内核态之间通过 Linux 的一个机制叫 Netlink，来进行相互通信。内核通过 upcall，告知用户态进程 vswitchd，在用户态的 Flow Table 里面去匹配规则，这里面的规则是全量的流表规则，而内核态的 Flow Table 只是为了做快速处理，保留了部分规则，内核里面的规则过一段时间就会过期。当在用户态匹配到了流表规则之后，就在用户态执行操作，同时将这个匹配成功的流表通过 reinject 下发到内核，从而接下来的包都能在内核找到这个规则，来进行转发。这里调用 openflow 协议的，是本地的命令行工具。当然你也可以是远程的 SDN 控制器来进行控制，一个重要的 SDN 控制器是 OpenDaylight。下面这个图就是 OpenDaylight 中看到的拓扑图。是不是有种物业管理员在监控室里的感觉？ 我们可以通过在 OpenDaylight 里，将两个交换机之间配置通，也可以配置不通，还可以配置一个虚拟 IP 地址为 VIP，在不同的机器之间实现负载均衡等等，所有的策略都可以灵活配置。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:33:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何在云计算中使用 OpenvSwitch？ OpenvSwitch 这么牛，如何用在云计算中呢？ 我们还是讨论 VLAN 的场景。在没有 OpenvSwitch 的时候，如果一个新的用户要使用一个新的 VLAN，就需要创建一个属于新的 VLAN 的虚拟网卡，并且为这个租户创建一个单独的虚拟网桥，这样用户越来越多的时候，虚拟网卡和虚拟网桥会越来越多，管理就越来越复杂。另一个问题是虚拟机的 VLAN 和物理环境的 VLAN 是透传的，也即从一开始规划的时候，这两个就需要匹配起来，将物理环境和虚拟环境强绑定，这样本来就不灵活。而引入了 OpenvSwitch，状态就得到了改观。 首先，由于 OpenvSwitch 本身就是支持 VLAN 的，这样所有的虚拟机都可以放在一个网桥 br0 上，通过不同的用户配置不同的 tag，就能够实现隔离。例如上面的图左面的部分，用户 A 的虚拟机都在 br0 上，用户 B 的虚拟机都在 br1 上，有了 OpenvSwitch，就可以都放在 br0 上，只是设置了不同的 tag 就可以了。另外，还可以创建一个虚拟交换机 br1，将物理网络和虚拟网络进行隔离。物理网络有物理网络的 VLAN 规划，虚拟机在一台物理机上，所有的 VLAN 都可以从 1 开始。由于一台物理机上的虚拟机肯定不会超过 4096 个，所以 VLAN 在一台物理机上如果从 1 开始，肯定够用了。例如在图中右面部分的上面的那台物理机里面，用户 A 被分配的 tag 是 1，用户 B 被分配的 tag 是 2，而在下面的物理机里面，用户 A 被分配的 tag 是 7，用户 B 被分配的 tag 是 6。如果物理机之间的通信和隔离还是通过 VLAN 的话，需要将虚拟机的 VLAN 和物理环境的 VLAN 对应起来，但为了灵活性，不一定一致，这样可以实现分别管理物理机的网络和虚拟机的网络。好在 OpenvSwitch 可以对包的内容进行修改。例如通过匹配 dl_vlan，然后执行 mod_vlan_vid 来改变进进出出物理机的网络包。尽管租户多了，物理环境的 VLAN 还是不够用，但是有了 OpenvSwitch 的映射，将物理和虚拟解耦，从而可以让物理环境使用其他技术，而不影响虚拟机环境，这个我们后面再讲。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:33:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下：用 SDN 控制整个云里面的网络，就像小区保安从总控室管理整个物业是一样的，将控制面和数据面进行了分离；一种开源的虚拟交换机的实现 OpenvSwitch，它能对经过自己的包做任意修改，从而使得云对网络的控制十分灵活；将 OpenvSwitch 引入了云之后，可以使得配置简单而灵活，并且可以解耦物理网络和虚拟网络。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:33:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第26讲 | 云中的网络安全：虽然不是土豪，也需要基本安全和保障 在今天的内容开始之前，我先卖个关子。文章结尾，我会放一个超级彩蛋，所以，今天的内容你一定要看到最后哦！上一节我们看到，做一个小区物业维护一个大家共享的环境，还是挺不容易的。如果都是自觉遵守规则的住户那还好，如果遇上不自觉的住户就会很麻烦。就像公有云的环境，其实没有你想的那么纯净，各怀鬼胎的黑客到处都是。扫描你的端口呀，探测一下你启动的什么应用啊，看一看是否有各种漏洞啊。这就像小偷潜入小区后，这儿看看，那儿瞧瞧，窗户有没有关严了啊，窗帘有没有拉上啊，主人睡了没，是不是时机潜入室内啊，等等。假如你创建了一台虚拟机，里面明明跑了一个电商应用，这是你非常重要的一个应用，你会把它进行安全加固。这台虚拟机的操作系统里，不小心安装了另外一个后台应用，监听着一个端口，而你的警觉性没有这么高。虚拟机的这个端口是对着公网开放的，碰巧这个后台应用本身是有漏洞的，黑客就可以扫描到这个端口，然后通过这个后台应用的端口侵入你的机器，将你加固好的电商网站黑掉。这就像你买了一个五星级的防盗门，卡车都撞不开，但是厕所窗户的门把手是坏的，小偷从厕所里面就进来了。 所以对于公有云上的虚拟机，我的建议是仅仅开放需要的端口，而将其他的端口一概关闭。这个时候，你只要通过安全措施守护好这个唯一的入口就可以了。采用的方式常常是用 ACL（Access Control List，访问控制列表）来控制 IP 和端口。设置好了这些规则，只有指定的 IP 段能够访问指定的开放接口，就算有个有漏洞的后台进程在那里，也会被屏蔽，黑客进不来。在云平台上，这些规则的集合常称为安全组。那安全组怎么实现呢？我们来复习一下，当一个网络包进入一台机器的时候，都会做什么事情。首先拿下 MAC 头看看，是不是我的。如果是，则拿下 IP 头来。得到目标 IP 之后呢，就开始进行路由判断。在路由判断之前，这个节点我们称为 PREROUTING。如果发现 IP 是我的，包就应该是我的，就发给上面的传输层，这个节点叫作 INPUT。如果发现 IP 不是我的，就需要转发出去，这个节点称为 FORWARD。如果是我的，上层处理完毕后，一般会返回一个处理结果，这个处理结果会发出去，这个节点称为 OUTPUT，无论是 FORWARD 还是 OUTPUT，都是路由判断之后发生的，最后一个节点是 POSTROUTING。整个过程如图所示。 整个包的处理过程还是原来的过程，只不过为什么要格外关注这五个节点呢？是因为在 Linux 内核中，有一个框架叫 Netfilter。它可以在这些节点插入 hook 函数。这些函数可以截获数据包，对数据包进行干预。例如做一定的修改，然后决策是否接着交给 TCP/IP 协议栈处理；或者可以交回给协议栈，那就是 ACCEPT；或者过滤掉，不再传输，就是 DROP；还有就是 QUEUE，发送给某个用户态进程处理。这个比较难理解，经常用在内部负载均衡，就是过来的数据一会儿传给目标地址 1，一会儿传给目标地址 2，而且目标地址的个数和权重都可能变。协议栈往往处理不了这么复杂的逻辑，需要写一个函数接管这个数据，实现自己的逻辑。有了这个 Netfilter 框架就太好了，你可以在 IP 转发的过程中，随时干预这个过程，只要你能实现这些 hook 函数。一个著名的实现，就是内核模块 ip_tables。它在这五个节点上埋下函数，从而可以根据规则进行包的处理。按功能可分为四大类：连接跟踪（conntrack）、数据包的过滤（filter）、网络地址转换（nat）和数据包的修改（mangle）。其中连接跟踪是基础功能，被其他功能所依赖。其他三个可以实现包的过滤、修改和网络地址转换。在用户态，还有一个你肯定知道的客户端程序 iptables，用命令行来干预内核的规则。内核的功能对应 iptables 的命令行来讲，就是表和链的概念。 iptables 的表分为四种：raw–\u003emangle–\u003enat–\u003efilter。这四个优先级依次降低，raw 不常用，所以主要功能都在其他三种表里实现。每个表可以设置多个链。filter 表处理过滤功能，主要包含三个链： INPUT 链：过滤所有目标地址是本机的数据包；FORWARD 链：过滤所有路过本机的数据包；OUTPUT 链：过滤所有由本机产生的数据包。 nat 表主要是处理网络地址转换，可以进行 Snat（改变数据包的源地址）、Dnat（改变数据包的目标地址），包含三个链：PREROUTING 链：可以在数据包到达防火墙时改变目标地址；OUTPUT 链：可以改变本地产生的数据包的目标地址；POSTROUTING 链：在数据包离开防火墙时改变数据包的源地址。 mangle 表主要是修改数据包，包含：PREROUTING 链；INPUT 链；FORWARD 链；OUTPUT 链；POSTROUTING 链。 将 iptables 的表和链加入到上面的过程图中，就形成了下面的图和过程。 数据包进入的时候，先进 mangle 表的 PREROUTING 链。在这里可以根据需要，改变数据包头内容之后，进入 nat 表的 PREROUTING 链，在这里可以根据需要做 Dnat，也就是目标地址转换。进入路由判断，要判断是进入本地的还是转发的。如果是进入本地的，就进入 INPUT 链，之后按条件过滤限制进入。之后进入本机，再进入 OUTPUT 链，按条件过滤限制出去，离开本地。如果是转发就进入 FORWARD 链，根据条件过滤限制转发。之后进入 POSTROUTING 链，这里可以做 Snat，离开网络接口。 有了 iptables 命令，我们就可以在云中实现一定的安全策略。例如我们可以处理前面的偷窥事件。首先我们将所有的门都关闭。 iptables -t filter -A INPUT -s 0.0.0.0/0.0.0.0 -d X.X.X.X -j DROP -s 表示源 IP 地址段，-d 表示目标地址段，DROP 表示丢弃，也即无论从哪里来的，要想访问我这台机器，全部拒绝，谁也黑不进来。但是你发现坏了，ssh 也进不来了，都不能远程运维了，可以打开一下。 iptables -I INPUT -s 0.0.0.0/0.0.0.0 -d X.X.X.X -p tcp --dport 22 -j ACCEPT 如果这台机器是提供的是 web 服务，80 端口也应该打开，当然一旦打开，这个 80 端口就需要很好的防护，但是从规则角度还是要打开。 iptables -A INPUT -s 0.0.0.0/0.0.0.0 -d X.X.X.X -p tcp --dport 80 -j ACCEPT 这样就搞定了，其他的账户都封死，就一个防盗门可以进出，只要防盗门是五星级的，就比较安全了。这些规则都可以在虚拟机里，自己安装 iptables 自己配置。但是如果虚拟机数目非常多，都要配置，对于用户来讲就太麻烦了，能不能让云平台把这部分工作做掉呢？当然可以了。在云平台上，一般允许一个或者多个虚拟机属于某个安全组，而属于不同安全组的虚拟机之间的访问以及外网访问虚拟机，都需要通过安全组进行过滤。 例如图中，我们会创建一系列的网站，都是前端在 Tomcat 里面，对外开放 8080 端口。数据库使用 MySQL，开放 3306 端口。为了方便运维，我们创建两个安全组，将 Tomcat 所在的虚拟机放在安全组 A 里面。在安全组 A 里面，允许任意 IP 地址 0.0.0.0/0 访问 8080 端口，但是对于 ssh 的 22 端口，仅仅允许管理员网段 203.0.113.0/24 访问。我们将 MySQL 所在的虚拟机放在安全组 B 里面。在安全组 B 里面，仅仅允许来自安全组 A 的机器访问 3306 端口，但是对于 ssh 的 22 端口，同样允许管理员网段 203.0.113.0/24 访问。这些安全组规则都可以自动下发到每个在安全组里面的虚拟机上，从而控制一大批虚拟机的安全策略。这种批量下发是怎么做到的呢？你还记得这幅图吗？ 两个 VM 都通过 tap 网卡连接到一个网桥上，但是网桥是二层的，两个 VM 之间是可以随意互通的，因而需要有一个地方统一配置这些 iptables 规则。可以多加一个网桥，在这个网桥上配置 iptables 规则，将在用户在界面上配置的规则，放到这个网桥上。然后在每台机器上跑一个 Agent，将用户配置的安全组变成 iptables 规则，配置在这个网桥上。安全问题解决了，iptables 真强大！别忙，iptables 除了 filter，还有 nat 呢，这个功能也非常重要。前面的章节我们说过，在设计云平台的时候，我们想让虚拟机之间的网络和物理网络进行隔离，但是虚拟机毕竟还是要通过物理网和外界通信的，因而需要在出物理网的时候，做一次网络地址转换，也即 nat，这个就可以用 iptables 来做。我们学过，IP 头里面包含源 IP 地址和目标 IP 地址，这两种 IP 地址都可以转换成其他地址。转换源 IP 地址的，我们称为 Snat；转换目标 IP 地址的，我们称为 Dnat。 你有没有思考过这个问题，TCP 的访问都是一去一回的，而你在你家里连接 WiFi 的 IP 地址是一个私网 IP，192.168.1.x。当你通过你们家的路由器访问 163 网站之后，网站的返回结果如何能够到达你的笔记本电脑呢？肯定不能通过 192.168.1.x，这是个私网 IP，不具有公网上的定位能力，而且用这个网段的人很多，茫茫人海，","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:34:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就讲到这里了，我们来总结一下。云中的安全策略的常用方式是，使用 iptables 的规则，请记住它的五个阶段，PREROUTING、INPUT、FORWARD、OUTPUT、POSTROUTING。iptables 分为四种表，raw、mangle、nat、filter。其中安全策略主要在 filter 表中实现，而虚拟网络和物理网络地址的转换主要在 nat 表中实现。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:34:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第27讲 | 云中的网络QoS：邻居疯狂下电影，我该怎么办？ 在小区里面，是不是经常有住户不自觉就霸占公共通道，如果你找他理论，他的话就像一个相声《楼道曲》说的一样：“公用公用，你用我用，大家都用，我为什么不能用？”。除此之外，你租房子的时候，有没有碰到这样的情况：本来合租共享 WiFi，一个人狂下小电影，从而你网都上不去，是不是很懊恼？在云平台上，也有这种现象，好在有一种流量控制的技术，可以实现 QoS（Quality of Service），从而保障大多数用户的服务质量。对于控制一台机器的网络的 QoS，分两个方向，一个是入方向，一个是出方向。 其实我们能控制的只有出方向，通过 Shaping，将出的流量控制成自己想要的模样。而进入的方向是无法控制的，只能通过 Policy 将包丢弃。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:35:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"控制网络的 QoS 有哪些方式？ 在 Linux 下，可以通过 TC 控制网络的 QoS，主要就是通过队列的方式。 无类别排队规则 第一大类称为无类别排队规则（Classless Queuing Disciplines）。还记得我们讲ip addr的时候讲过的 pfifo_fast，这是一种不把网络包分类的技术。 pfifo_fast 分为三个先入先出的队列，称为三个 Band。根据网络包里面 TOS，看这个包到底应该进入哪个队列。TOS 总共四位，每一位表示的意思不同，总共十六种类型。通过命令行 tc qdisc show dev eth0，可以输出结果 priomap，也是十六个数字。在 0 到 2 之间，和 TOS 的十六种类型对应起来，表示不同的 TOS 对应的不同的队列。其中 Band 0 优先级最高，发送完毕后才轮到 Band 1 发送，最后才是 Band 2。另外一种无类别队列规则叫作随机公平队列（Stochastic Fair Queuing）。 会建立很多的 FIFO 的队列，TCP Session 会计算 hash 值，通过 hash 值分配到某个队列。在队列的另一端，网络包会通过轮询策略从各个队列中取出发送。这样不会有一个 Session 占据所有的流量。当然如果两个 Session 的 hash 是一样的，会共享一个队列，也有可能互相影响。hash 函数会经常改变，从而 session 不会总是相互影响。还有一种无类别队列规则称为令牌桶规则（TBF，Token Bucket Filte）。 所有的网络包排成队列进行发送，但不是到了队头就能发送，而是需要拿到令牌才能发送。令牌根据设定的速度生成，所以即便队列很长，也是按照一定的速度进行发送的。当没有包在队列中的时候，令牌还是以既定的速度生成，但是不是无限累积的，而是放满了桶为止。设置桶的大小为了避免下面的情况：当长时间没有网络包发送的时候，积累了大量的令牌，突然来了大量的网络包，每个都能得到令牌，造成瞬间流量大增。 基于类别的队列规则 另外一大类是基于类别的队列规则（Classful Queuing Disciplines），其中典型的为分层令牌桶规则（HTB， Hierarchical Token Bucket）。HTB 往往是一棵树，接下来我举个具体的例子，通过 TC 如何构建一棵 HTB 树来带你理解。 使用 TC 可以为某个网卡 eth0 创建一个 HTB 的队列规则，需要付给它一个句柄为（1:）。这是整棵树的根节点，接下来会有分支。例如图中有三个分支，句柄分别为（:10）、（:11）、（:12）。最后的参数 default 12，表示默认发送给 1:12，也即发送给第三个分支。 tc qdisc add dev eth0 root handle 1: htb default 12 对于这个网卡，需要规定发送的速度。一般有两个速度可以配置，一个是 rate，表示一般情况下的速度；一个是 ceil，表示最高情况下的速度。对于根节点来讲，这两个速度是一样的，于是创建一个 root class，速度为（rate=100kbps，ceil=100kbps）。 tc class add dev eth0 parent 1: classid 1:1 htb rate 100kbps ceil 100kbps 接下来要创建分支，也即创建几个子 class。每个子 class 统一有两个速度。三个分支分别为（rate=30kbps，ceil=100kbps）、（rate=10kbps，ceil=100kbps）、（rate=60kbps，ceil=100kbps）。 tc class add dev eth0 parent 1:1 classid 1:10 htb rate 30kbps ceil 100kbps tc class add dev eth0 parent 1:1 classid 1:11 htb rate 10kbps ceil 100kbps tc class add dev eth0 parent 1:1 classid 1:12 htb rate 60kbps ceil 100kbps 你会发现三个 rate 加起来，是整个网卡允许的最大速度。HTB 有个很好的特性，同一个 root class 下的子类可以相互借流量，如果不直接在队列规则下面创建一个 root class，而是直接创建三个 class，它们之间是不能相互借流量的。借流量的策略，可以使得当前不使用这个分支的流量的时候，可以借给另一个分支，从而不浪费带宽，使带宽发挥最大的作用。最后，创建叶子队列规则，分别为 fifo 和 sfq。 tc qdisc add dev eth0 parent 1:10 handle 20: pfifo limit 5 tc qdisc add dev eth0 parent 1:11 handle 30: pfifo limit 5 tc qdisc add dev eth0 parent 1:12 handle 40: sfq perturb 10 基于这个队列规则，我们还可以通过 TC 设定发送规则：从 1.2.3.4 来的，发送给 port 80 的包，从第一个分支 1:10 走；其他从 1.2.3.4 发送来的包从第二个分支 1:11 走；其他的走默认分支。 tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 match ip dport 80 0xffff flowid 1:10 tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 flowid 1:11 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:35:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何控制 QoS？ 我们讲过，使用 OpenvSwitch 将云中的网卡连通在一起，那如何控制 QoS 呢？就像我们上面说的一样，OpenvSwitch 支持两种： 对于进入的流量，可以设置策略 Ingress policy； ovs-vsctl set Interface tap0 ingress_policing_rate=100000 ovs-vsctl set Interface tap0 ingress_policing_burst=10000 对于发出的流量，可以设置 QoS 规则 Egress shaping，支持 HTB。我们构建一个拓扑图，来看看 OpenvSwitch 的 QoS 是如何工作的。 首先，在 port 上可以创建 QoS 规则，一个 QoS 规则可以有多个队列 Queue。 ovs-vsctl set port first_br qos=@newqos -- --id=@newqos create qos type=linux-htb other-config:max-rate=10000000 queues=0=@q0,1=@q1,2=@q2 -- --id=@q0 create queue other-config:min-rate=3000000 other-config:max-rate=10000000 -- --id=@q1 create queue other-config:min-rate=1000000 other-config:max-rate=10000000 -- --id=@q2 create queue other-config:min-rate=6000000 other-config:max-rate=10000000 上面的命令创建了一个 QoS 规则，对应三个 Queue。min-rate 就是上面的 rate，max-rate 就是上面的 ceil。通过交换机的网络包，要通过流表规则，匹配后进入不同的队列。然后我们就可以添加流表规则 Flow(first_br 是 br0 上的 port 5)。 ovs-ofctl add-flow br0 \"in_port=6 nw_src=192.168.100.100 actions=enqueue:5:0\" ovs-ofctl add-flow br0 \"in_port=7 nw_src=192.168.100.101 actions=enqueue:5:1\" ovs-ofctl add-flow br0 \"in_port=8 nw_src=192.168.100.102 actions=enqueue:5:2\" 接下来，我们单独测试从 192.168.100.100，192.168.100.101，192.168.100.102 到 192.168.100.103 的带宽的时候，每个都是能够打满带宽的。如果三个一起测试，一起狂发网络包，会发现是按照 3:1:6 的比例进行的，正是根据配置的队列的带宽比例分配的。如果 192.168.100.100 和 192.168.100.101 一起测试，发现带宽占用比例为 3:1，但是占满了总的流量，也即没有发包的 192.168.100.102 有 60% 的带宽被借用了。如果 192.168.100.100 和 192.168.100.102 一起测试，发现带宽占用比例为 1:2。如果 192.168.100.101 和 192.168.100.102 一起测试，发现带宽占用比例为 1:6。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:35:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就讲到这里了，我们来总结一下。云中的流量控制主要通过队列进行的，队列分为两大类：无类别队列规则和基于类别的队列规则。在云中网络 Openvswitch 中，主要使用的是分层令牌桶规则（HTB），将总的带宽在一棵树上按照配置的比例进行分配，并且在一个分支不用的时候，可以借给另外的分支，从而增强带宽利用率。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:35:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第28讲 | 云中网络的隔离GRE、VXLAN：虽然住一个小区，也要保护隐私 对于云平台中的隔离问题，前面咱们用的策略一直都是 VLAN，但是我们也说过这种策略的问题，VLAN 只有 12 位，共 4096 个。当时设计的时候，看起来是够了，但是现在绝对不够用，怎么办呢？一种方式是修改这个协议。这种方法往往不可行，因为当这个协议形成一定标准后，千千万万设备上跑的程序都要按这个规则来。现在说改就放，谁去挨个儿告诉这些程序呢？很显然，这是一项不可能的工程。另一种方式就是扩展，在原来包的格式的基础上扩展出一个头，里面包含足够用于区分租户的 ID，外层的包的格式尽量和传统的一样，依然兼容原来的格式。一旦遇到需要区分用户的地方，我们就用这个特殊的程序，来处理这个特殊的包的格式。这个概念很像咱们第 22 讲讲过的隧道理论，还记得自驾游通过摆渡轮到海南岛的那个故事吗？在那一节，我们说过，扩展的包头主要是用于加密的，而我们现在需要的包头是要能够区分用户的。底层的物理网络设备组成的网络我们称为 Underlay 网络，而用于虚拟机和云中的这些技术组成的网络称为 Overlay 网络，这是一种基于物理网络的虚拟化网络实现。这一节我们重点讲两个 Overlay 的网络技术。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:36:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"GRE 第一个技术是 GRE，全称 Generic Routing Encapsulation，它是一种 IP-over-IP 的隧道技术。它将 IP 包封装在 GRE 包里，外面加上 IP 头，在隧道的一端封装数据包，并在通路上进行传输，到另外一端的时候解封装。你可以认为 Tunnel 是一个虚拟的、点对点的连接。 从这个图中可以看到，在 GRE 头中，前 32 位是一定会有的，后面的都是可选的。在前 4 位标识位里面，有标识后面到底有没有可选项？这里面有个很重要的 key 字段，是一个 32 位的字段，里面存放的往往就是用于区分用户的 Tunnel ID。32 位，够任何云平台喝一壶的了！下面的格式类型专门用于网络虚拟化的 GRE 包头格式，称为 NVGRE，也给网络 ID 号 24 位，也完全够用了。除此之外，GRE 还需要有一个地方来封装和解封装 GRE 的包，这个地方往往是路由器或者有路由功能的 Linux 机器。使用 GRE 隧道，传输的过程就像下面这张图。这里面有两个网段、两个路由器，中间要通过 GRE 隧道进行通信。当隧道建立之后，会多出两个 Tunnel 端口，用于封包、解封包。 主机 A 在左边的网络，IP 地址为 192.168.1.102，它想要访问主机 B，主机 B 在右边的网络，IP 地址为 192.168.2.115。于是发送一个包，源地址为 192.168.1.102，目标地址为 192.168.2.115。因为要跨网段访问，于是根据默认的 default 路由表规则，要发给默认的网关 192.168.1.1，也即左边的路由器。根据路由表，从左边的路由器，去 192.168.2.0/24 这个网段，应该走一条 GRE 的隧道，从隧道一端的网卡 Tunnel0 进入隧道。在 Tunnel 隧道的端点进行包的封装，在内部的 IP 头之外加上 GRE 头。对于 NVGRE 来讲，是在 MAC 头之外加上 GRE 头，然后加上外部的 IP 地址，也即路由器的外网 IP 地址。源 IP 地址为 172.17.10.10，目标 IP 地址为 172.16.11.10，然后从 E1 的物理网卡发送到公共网络里。在公共网络里面，沿着路由器一跳一跳地走，全部都按照外部的公网 IP 地址进行。当网络包到达对端路由器的时候，也要到达对端的 Tunnel0，然后开始解封装，将外层的 IP 头取下来，然后根据里面的网络包，根据路由表，从 E3 口转发出去到达服务器 B。 从 GRE 的原理可以看出，GRE 通过隧道的方式，很好地解决了 VLAN ID 不足的问题。但是，GRE 技术本身还是存在一些不足之处。首先是 Tunnel 的数量问题。GRE 是一种点对点隧道，如果有三个网络，就需要在每两个网络之间建立一个隧道。如果网络数目增多，这样隧道的数目会呈指数性增长。 其次，GRE 不支持组播，因此一个网络中的一个虚机发出一个广播帧后，GRE 会将其广播到所有与该节点有隧道连接的节点。另外一个问题是目前还是有很多防火墙和三层网络设备无法解析 GRE，因此它们无法对 GRE 封装包做合适地过滤和负载均衡。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:36:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"VXLAN 第二种 Overlay 的技术称为 VXLAN。和三层外面再套三层的 GRE 不同，VXLAN 则是从二层外面就套了一个 VXLAN 的头，这里面包含的 VXLAN ID 为 24 位，也够用了。在 VXLAN 头外面还封装了 UDP、IP，以及外层的 MAC 头。 VXLAN 作为扩展性协议，也需要一个地方对 VXLAN 的包进行封装和解封装，实现这个功能的点称为 VTEP（VXLAN Tunnel Endpoint）。VTEP 相当于虚拟机网络的管家。每台物理机上都可以有一个 VTEP。每个虚拟机启动的时候，都需要向这个 VTEP 管家注册，每个 VTEP 都知道自己上面注册了多少个虚拟机。当虚拟机要跨 VTEP 进行通信的时候，需要通过 VTEP 代理进行，由 VTEP 进行包的封装和解封装。和 GRE 端到端的隧道不同，VXLAN 不是点对点的，而是支持通过组播的来定位目标机器的，而非一定是这一端发出，另一端接收。当一个 VTEP 启动的时候，它们都需要通过 IGMP 协议。加入一个组播组，就像加入一个邮件列表，或者加入一个微信群一样，所有发到这个邮件列表里面的邮件，或者发送到微信群里面的消息，大家都能收到。而当每个物理机上的虚拟机启动之后，VTEP 就知道，有一个新的 VM 上线了，它归我管。 如图，虚拟机 1、2、3 属于云中同一个用户的虚拟机，因而需要分配相同的 VXLAN ID=101。在云的界面上，就可以知道它们的 IP 地址，于是可以在虚拟机 1 上 ping 虚拟机 2。虚拟机 1 发现，它不知道虚拟机 2 的 MAC 地址，因而包没办法发出去，于是要发送 ARP 广播。 ARP 请求到达 VTEP1 的时候，VTEP1 知道，我这里有一台虚拟机，要访问一台不归我管的虚拟机，需要知道 MAC 地址，可是我不知道啊，这该咋办呢？VTEP1 想，我不是加入了一个微信群么？可以在里面 @all 一下，问问虚拟机 2 归谁管。于是 VTEP1 将 ARP 请求封装在 VXLAN 里面，组播出去。当然在群里面，VTEP2 和 VTEP3 都收到了消息，因而都会解开 VXLAN 包看，里面是一个 ARP。VTEP3 在本地广播了半天，没人回，都说虚拟机 2 不归自己管。VTEP2 在本地广播，虚拟机 2 回了，说虚拟机 2 归我管，MAC 地址是这个。通过这次通信，VTEP2 也学到了，虚拟机 1 归 VTEP1 管，以后要找虚拟机 1，去找 VTEP1 就可以了。 VTEP2 将 ARP 的回复封装在 VXLAN 里面，这次不用组播了，直接发回给 VTEP1。VTEP1 解开 VXLAN 的包，发现是 ARP 的回复，于是发给虚拟机 1。通过这次通信，VTEP1 也学到了，虚拟机 2 归 VTEP2 管，以后找虚拟机 2，去找 VTEP2 就可以了。虚拟机 1 的 ARP 得到了回复，知道了虚拟机 2 的 MAC 地址，于是就可以发送包了。 虚拟机 1 发给虚拟机 2 的包到达 VTEP1，它当然记得刚才学的东西，要找虚拟机 2，就去 VTEP2，于是将包封装在 VXLAN 里面，外层加上 VTEP1 和 VTEP2 的 IP 地址，发送出去。网络包到达 VTEP2 之后，VTEP2 解开 VXLAN 封装，将包转发给虚拟机 2。虚拟机 2 回复的包，到达 VTEP2 的时候，它当然也记得刚才学的东西，要找虚拟机 1，就去 VTEP1，于是将包封装在 VXLAN 里面，外层加上 VTEP1 和 VTEP2 的 IP 地址，也发送出去。网络包到达 VTEP1 之后，VTEP1 解开 VXLAN 封装，将包转发给虚拟机 1。 有了 GRE 和 VXLAN 技术，我们就可以解决云计算中 VLAN 的限制了。那如何将这个技术融入云平台呢？还记得将你宿舍里面的情况，所有东西都搬到一台物理机上那个故事吗？ 虚拟机是你的电脑，路由器和 DHCP Server 相当于家用路由器或者寝室长的电脑，外网网口访问互联网，所有的电脑都通过内网网口连接到一个交换机 br0 上，虚拟机要想访问互联网，需要通过 br0 连到路由器上，然后通过路由器将请求 NAT 后转发到公网。接下来的事情就惨了，你们宿舍闹矛盾了，你们要分成三个宿舍住，对应上面的图，你们寝室长，也即路由器单独在一台物理机上，其他的室友也即 VM 分别在两台物理机上。这下把一个完整的 br0 一刀三断，每个宿舍都是单独的一段。 可是只有你的寝室长有公网口可以上网，于是你偷偷在三个宿舍中间打了一个隧道，用网线通过隧道将三个宿舍的两个 br0 连接起来，让其他室友的电脑和你寝室长的电脑，看起来还是连到同一个 br0 上，其实中间是通过你隧道中的网线做了转发。为什么要多一个 br1 这个虚拟交换机呢？主要通过 br1 这一层将虚拟机之间的互联和物理机机之间的互联分成两层来设计，中间隧道可以有各种挖法，GRE、VXLAN 都可以。使用了 OpenvSwitch 之后，br0 可以使用 OpenvSwitch 的 Tunnel 功能和 Flow 功能。OpenvSwitch 支持三类隧道：GRE、VXLAN、IPsec_GRE。在使用 OpenvSwitch 的时候，虚拟交换机就相当于 GRE 和 VXLAN 封装的端点。我们模拟创建一个如下的网络拓扑结构，来看隧道应该如何工作。 三台物理机，每台上都有两台虚拟机，分别属于两个不同的用户，因而 VLAN tag 都得打地不一样，这样才不能相互通信。但是不同物理机上的相同用户，是可以通过隧道相互通信的，因而通过 GRE 隧道可以连接到一起。接下来，所有的 Flow Table 规则都设置在 br1 上，每个 br1 都有三个网卡，其中网卡 1 是对内的，网卡 2 和 3 是对外的。下面我们具体来看 Flow Table 的设计。 1.Table 0 是所有流量的入口，所有进入 br1 的流量，分为两种流量，一个是进入物理机的流量，一个是从物理机发出的流量。从 port 1 进来的，都是发出去的流量，全部由 Table 1 处理。 ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 in_port=1 actions=resubmit(,1)\" 从 port 2、3 进来的，都是进入物理机的流量，全部由 Table 3 处理。 ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 in_port=2 actions=resubmit(,3)\" ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 in_port=3 actions=resubmit(,3)\" 如果都没匹配上，就默认丢弃。 ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=0 actions=drop\" 2.Table 1 用于处理所有出去的网络包，分为两种情况，一种是单播，一种是多播。对于单播，由 Table 20 处理。 ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 table=1 dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20)\" 对于多播，由 Table 21 处理。 ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 table=1 dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,21)\" 3.Table 2 是紧接着 Table1 的，如果既不是单播，也不是多播，就默认丢弃。 ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=0 table=2 actions=drop\" 4.Table 3 用于处理所有进来的网络包，需要将隧道 Tunnel ID 转换为 VLAN ID。如果匹配不上 Tunnel ID，就默认丢弃。 ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=0 table=3 actions=drop\" 如果匹配上了 Tunnel ID，就转换为相应的 VLAN ID，然后跳到 Table 10。 ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 table=3 tun_id=0x1 actions=mod_vlan_vid:1,resubmit(,10)\" ovs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 table=3 tun_id=0x2 actions=mod_vlan_vid:2,resubmit(,10)\" 5.对于进来的包，Table 10 会进行 MAC 地址学习。这是一个二层交换机应该做的事情，学习完了之后，再从 port 1 发出","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:36:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下。要对不同用户的网络进行隔离，解决 VLAN 数目有限的问题，需要通过 Overlay 的方式，常用的有 GRE 和 VXLAN。GRE 是一种点对点的隧道模式，VXLAN 支持组播的隧道模式，它们都要在某个 Tunnel Endpoint 进行封装和解封装，来实现跨物理机的互通。OpenvSwitch 可以作为 Tunnel Endpoint，通过设置流表的规则，将虚拟机网络和物理机网络进行隔离、转换。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:36:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"容器技术中的网络 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:37:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第29讲 | 容器网络：来去自由的日子，不买公寓去合租 如果说虚拟机是买公寓，容器则相当于合租，有一定的隔离，但是隔离性没有那么好。云计算解决了基础资源层的弹性伸缩，却没有解决 PaaS 层应用随基础资源层弹性伸缩而带来的批量、快速部署问题。于是，容器应运而生。容器就是 Container，而 Container 的另一个意思是集装箱。其实容器的思想就是要变成软件交付的集装箱。集装箱的特点，一是打包，二是标准。 在没有集装箱的时代，假设要将货物从 A 运到 B，中间要经过三个码头、换三次船。每次都要将货物卸下船来，弄得乱七八糟，然后还要再搬上船重新整齐摆好。因此在没有集装箱的时候，每次换船，船员们都要在岸上待几天才能干完活。有了尺寸全部都一样的集装箱以后，可以把所有的货物都打包在一起，所以每次换船的时候，一个箱子整体搬过去就行了，小时级别就能完成，船员再也不用耗费很长时间了。这是集装箱的“打包”“标准”两大特点在生活中的应用。 那么容器如何对应用打包呢？学习集装箱，首先要有个封闭的环境，将货物封装起来，让货物之间互不干扰，互相隔离，这样装货卸货才方便。封闭的环境主要使用了两种技术，一种是看起来是隔离的技术，称为 namespace，也即每个 namespace 中的应用看到的是不同的 IP 地址、用户空间、程号等。另一种是用起来是隔离的技术，称为 cgroup，也即明明整台机器有很多的 CPU、内存，而一个应用只能用其中的一部分。有了这两项技术，就相当于我们焊好了集装箱。接下来的问题就是如何“将这个集装箱标准化”，并在哪艘船上都能运输。这里的标准首先就是镜像。 所谓镜像，就是将你焊好集装箱的那一刻，将集装箱的状态保存下来，就像孙悟空说：“定！”，集装箱里的状态就被定在了那一刻，然后将这一刻的状态保存成一系列文件。无论从哪里运行这个镜像，都能完整地还原当时的情况。 接下来我们就具体来看看，这两种网络方面的打包技术。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:38:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"命名空间（namespace） 我们首先来看网络 namespace。namespace 翻译过来就是命名空间。其实很多面向对象的程序设计语言里面，都有命名空间这个东西。大家一起写代码，难免会起相同的名词，编译就会冲突。而每个功能都有自己的命名空间，在不同的空间里面，类名相同，不会冲突。在 Linux 下也是这样的，很多的资源都是全局的。比如进程有全局的进程 ID，网络也有全局的路由表。但是，当一台 Linux 上跑多个进程的时候，如果我们觉得使用不同的路由策略，这些进程可能会冲突，那就需要将这个进程放在一个独立的 namespace 里面，这样就可以独立配置网络了。网络的 namespace 由 ip netns 命令操作。它可以创建、删除、查询 namespace。我们再来看将你们宿舍放进一台物理机的那个图。你们宿舍长的电脑是一台路由器，你现在应该知道怎么实现这个路由器吧？可以创建一个 Router 虚拟机来做这件事情，但是还有一个更加简单的办法，就是我在图里画的这条虚线，这个就是通过 namespace 实现的。 我们创建一个 routerns，于是一个独立的网络空间就产生了。你可以在里面尽情设置自己的规则。 ip netns add routerns 既然是路由器，肯定要能转发嘛，因而 forward 开关要打开。 ip netns exec routerns sysctl -w net.ipv4.ip_forward=1 exec 的意思就是进入这个网络空间做点事情。初始化一下 iptables，因为这里面要配置 NAT 规则。 ip netns exec routerns iptables-save -c ip netns exec routerns iptables-restore -c 路由器需要有一张网卡连到 br0 上，因而要创建一个网卡。 ovs-vsctl -- add-port br0 taprouter -- set Interface taprouter type=internal -- set Interface taprouter external-ids:iface-status=active -- set Interface taprouter external-ids:attached-mac=fa:16:3e:84:6e:cc 这个网络创建完了，但是是在 namespace 外面的，如何进去呢？可以通过这个命令： ip link set taprouter netns routerns 要给这个网卡配置一个 IP 地址，当然应该是虚拟机网络的网关地址。例如虚拟机私网网段为 192.168.1.0/24，网关的地址往往为 192.168.1.1。 ip netns exec routerns ip -4 addr add 192.168.1.1/24 brd 192.168.1.255 scope global dev taprouter 为了访问外网，还需要另一个网卡连在外网网桥 br-ex 上，并且塞在 namespace 里面。 ovs-vsctl -- add-port br-ex taprouterex -- set Interface taprouterex type=internal -- set Interface taprouterex external-ids:iface-status=active -- set Interface taprouterex external-ids:attached-mac=fa:16:3e:68:12:c0 ip link set taprouterex netns routerns 我们还需要为这个网卡分配一个地址，这个地址应该和物理外网网络在一个网段。假设物理外网为 16.158.1.0/24，可以分配一个外网地址 16.158.1.100/24。 ip netns exec routerns ip -4 addr add 16.158.1.100/24 brd 16.158.1.255 scope global dev taprouterex 接下来，既然是路由器，就需要配置路由表，路由表是这样的： ip netns exec routerns route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 16.158.1.1 0.0.0.0 UG 0 0 0 taprouterex 192.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 taprouter 16.158.1.0 0.0.0.0 255.255.255.0 U 0 0 0 taprouterex 路由表中的默认路由是去物理外网的，去 192.168.1.0/24 也即虚拟机私网，走下面的网卡，去 16.158.1.0/24 也即物理外网，走上面的网卡。我们在前面的章节讲过，如果要在虚拟机里面提供服务，提供给外网的客户端访问，客户端需要访问外网 IP3，会在外网网口 NAT 称为虚拟机私网 IP。这个 NAT 规则要在这个 namespace 里面配置。 ip netns exec routerns iptables -t nat -nvL Chain PREROUTING target prot opt in out source destination DNAT all -- * * 0.0.0.0/0 16.158.1.103 to:192.168.1.3 Chain POSTROUTING target prot opt in out source destination SNAT all -- * * 192.168.1.3 0.0.0.0/0 to:16.158.1.103 这里面有两个规则，一个是 SNAT，将虚拟机的私网 IP 192.168.1.3 NAT 成物理外网 IP 16.158.1.103。一个是 DNAT，将物理外网 IP 16.158.1.103 NAT 成虚拟机私网 IP 192.168.1.3。至此为止，基于网络 namespace 的路由器实现完毕。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:38:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"机制网络（cgroup） 我们再来看打包的另一个机制网络 cgroup。cgroup 全称 control groups，是 Linux 内核提供的一种可以限制、隔离进程使用的资源机制。cgroup 能控制哪些资源呢？它有很多子系统： CPU 子系统使用调度程序为进程控制 CPU 的访问；cpuset，如果是多核心的 CPU，这个子系统会为进程分配单独的 CPU 和内存；memory 子系统，设置进程的内存限制以及产生内存资源报告；blkio 子系统，设置限制每个块设备的输入输出控制；net_cls，这个子系统使用等级识别符（classid）标记网络数据包，可允许 Linux 流量控制程序（tc）识别从具体 cgroup 中生成的数据包。 我们这里最关心的是 net_cls，它可以和前面讲过的 TC 关联起来。cgroup 提供了一个虚拟文件系统，作为进行分组管理和各子系统设置的用户接口。要使用 cgroup，必须挂载 cgroup 文件系统，一般情况下都是挂载到 /sys/fs/cgroup 目录下。所以首先我们要挂载一个 net_cls 的文件系统。 mkdir /sys/fs/cgroup/net_cls mount -t cgroup -onet_cls net_cls /sys/fs/cgroup/net_cls 接下来我们要配置 TC 了。还记得咱们实验 TC 的时候那颗树吗？ 当时我们通过这个命令设定了规则：从 1.2.3.4 来的，发送给 port 80 的包，从 1:10 走；其他从 1.2.3.4 发送来的包从 1:11 走；其他的走默认。 tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 match ip dport 80 0xffff flowid 1:10 tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 flowid 1:11 这里是根据源 IP 来设定的，现在有了 cgroup，我们按照 cgroup 再来设定规则。 tc filter add dev eth0 protocol ip parent 1:0 prio 1 handle 1: cgroup 假设我们有两个用户 a 和 b，要对它们进行带宽限制。首先，我们要创建两个 net_cls。 mkdir /sys/fs/cgroup/net_cls/a mkdir /sys/fs/cgroup/net_cls/b 假设用户 a 启动的进程 ID 为 12345，把它放在 net_cls/a/tasks 文件中。同样假设用户 b 启动的进程 ID 为 12346，把它放在 net_cls/b/tasks 文件中。net_cls/a 目录下面，还有一个文件 net_cls.classid，我们放 flowid 1:10。net_cls/b 目录下面，也创建一个文件 net_cls.classid，我们放 flowid 1:11。这个数字怎么放呢？要转换成一个 0xAAAABBBB 的值，AAAA 对应 class 中冒号前面的数字，而 BBBB 对应后面的数字。 echo 0x00010010 \u003e /sys/fs/cgroup/net_cls/a/net_cls.classid echo 0x00010011 \u003e /sys/fs/cgroup/net_cls/b/net_cls.classid 这样用户 a 的进程发的包，会打上 1:10 这个标签；用户 b 的进程发的包，会打上 1:11 这个标签。然后 TC 根据这两个标签，让用户 a 的进程的包走左边的分支，用户 b 的进程的包走右边的分支。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:38:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"容器网络中如何融入物理网络？ 了解了容器背后的技术，接下来我们来看，容器网络究竟是如何融入物理网络的？如果你使用 docker run 运行一个容器，你应该能看到这样一个拓扑结构。 是不是和虚拟机很像？容器里面有张网卡，容器外有张网卡，容器外的网卡连到 docker0 网桥，通过这个网桥，容器直接实现相互访问。如果你用 brctl 查看 docker0 网桥，你会发现它上面连着一些网卡。其实这个网桥和第 24 讲，咱们自己用 brctl 创建的网桥没什么两样。那连接容器和网桥的那个网卡和虚拟机一样吗？在虚拟机场景下，有一个虚拟化软件，通过 TUN/TAP 设备虚拟一个网卡给虚拟机，但是容器场景下并没有虚拟化软件，这该怎么办呢？在 Linux 下，可以创建一对 veth pair 的网卡，从一边发送包，另一边就能收到。我们首先通过这个命令创建这么一对。 ip link add name veth1 mtu 1500 type veth peer name veth2 mtu 1500 其中一边可以打到 docker0 网桥上。 ip link set veth1 master testbr ip link set veth1 up 那另一端如何放到容器里呢？一个容器的启动会对应一个 namespace，我们要先找到这个 namespace。对于 docker 来讲，pid 就是 namespace 的名字，可以通过这个命令获取。 docker inspect '--format={ { .State.Pid }}' test 假设结果为 12065，这个就是 namespace 名字。默认 Docker 创建的网络 namespace 不在默认路径下 ，ip netns 看不到，所以需要 ln 软链接一下。链接完毕以后，我们就可以通过 ip netns 命令操作了。 rm -f /var/run/netns/12065 ln -s /proc/12065/ns/net /var/run/netns/12065 然后，我们就可以将另一端 veth2 塞到 namespace 里面。 ip link set veth2 netns 12065 然后，将容器内的网卡重命名。 ip netns exec 12065 ip link set veth2 name eth0 然后，给容器内网卡设置 ip 地址。 ip netns exec 12065 ip addr add 172.17.0.2/16 dev eth0 ip netns exec 12065 ip link set eth0 up 一台机器内部容器的互相访问没有问题了，那如何访问外网呢？你先想想看有没有思路？对，就是虚拟机里面的桥接模式和 NAT 模式。Docker 默认使用 NAT 模式。NAT 模式分为 SNAT 和 DNAT，如果是容器内部访问外部，就需要通过 SNAT。从容器内部的客户端访问外部网络中的服务器，我画了一张图。在虚拟机那一节，也有一张类似的图。 在宿主机上，有这么一条 iptables 规则： -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE 所有从容器内部发出来的包，都要做地址伪装，将源 IP 地址，转换为物理网卡的 IP 地址。如果有多个容器，所有的容器共享一个外网的 IP 地址，但是在 conntrack 表中，记录下这个出去的连接。当服务器返回结果的时候，到达物理机，会根据 conntrack 表中的规则，取出原来的私网 IP，通过 DNAT 将地址转换为私网 IP 地址，通过网桥 docker0 实现对内的访问。如果在容器内部属于一个服务，例如部署一个网站，提供给外部进行访问，需要通过 Docker 的端口映射技术，将容器内部的端口映射到物理机上来。例如容器内部监听 80 端口，可以通 Docker run 命令中的参数 -p 10080:80，将物理机上的 10080 端口和容器的 80 端口映射起来， 当外部的客户端访问这个网站的时候，通过访问物理机的 10080 端口，就能访问到容器内的 80 端口了。 Docker 有两种方式，一种是通过一个进程 docker-proxy 的方式，监听 10080，转换为 80 端口。 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 10080 -container-ip 172.17.0.2 -container-port 80 另外一种方式是通过 DNAT 方式，在 -A PREROUTING 阶段加一个规则，将到端口 10080 的 DNAT 称为容器的私有网络。 -A DOCKER -p tcp -m tcp --dport 10080 -j DNAT --to-destination 172.17.0.2:80 如此就可以实现容器和物理网络之间的互通了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:38:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下。容器是一种比虚拟机更加轻量级的隔离方式，主要通过 namespace 和 cgroup 技术进行资源的隔离，namespace 用于负责看起来隔离，cgroup 用于负责用起来隔离。容器网络连接到物理网络的方式和虚拟机很像，通过桥接的方式实现一台物理机上的容器进行相互访问，如果要访问外网，最简单的方式还是通过 NAT。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:38:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第30讲 | 容器网络之Flannel：每人一亩三分地 上一节我们讲了容器网络的模型，以及如何通过 NAT 的方式与物理网络进行互通。每一台物理机上面安装好了 Docker 以后，都会默认分配一个 172.17.0.0/16 的网段。一台机器上新创建的第一个容器，一般都会给 172.17.0.2 这个地址，当然一台机器这样玩玩倒也没啥问题。但是容器里面是要部署应用的，就像上一节讲过的一样，它既然是集装箱，里面就需要装载货物。如果这个应用是比较传统的单体应用，自己就一个进程，所有的代码逻辑都在这个进程里面，上面的模式没有任何问题，只要通过 NAT 就能访问进来。但是因为无法解决快速迭代和高并发的问题，单体应用越来越跟不上时代发展的需要了。你可以回想一下，无论是各种网络直播平台，还是共享单车，是不是都是很短时间内就要积累大量用户，否则就会错过风口。所以应用需要在很短的时间内快速迭代，不断调整，满足用户体验；还要在很短的时间内，具有支撑高并发请求的能力。单体应用作为个人英雄主义的时代已经过去了。如果所有的代码都在一个工程里面，开发的时候必然存在大量冲突，上线的时候，需要开大会进行协调，一个月上线一次就很不错了。而且所有的流量都让一个进程扛，怎么也扛不住啊！没办法，一个字：拆！拆开了，每个子模块独自变化，减少相互影响。拆开了，原来一个进程扛流量，现在多个进程一起扛。所以，微服务就是从个人英雄主义，变成集团军作战。容器作为集装箱，可以保证应用在不同的环境中快速迁移，提高迭代的效率。但是如果要形成容器集团军，还需要一个集团军作战的调度平台，这就是 Kubernetes。它可以灵活地将一个容器调度到任何一台机器上，并且当某个应用扛不住的时候，只要在 Kubernetes 上修改容器的副本数，一个应用马上就能变八个，而且都能提供服务。然而集团军作战有个重要的问题，就是通信。这里面包含两个问题，第一个是集团军的 A 部队如何实时地知道 B 部队的位置变化，第二个是两个部队之间如何相互通信。第一个问题位置变化，往往是通过一个称为注册中心的地方统一管理的，这个是应用自己做的。当一个应用启动的时候，将自己所在环境的 IP 地址和端口，注册到注册中心指挥部，这样其他的应用请求它的时候，到指挥部问一下它在哪里就好了。当某个应用发生了变化，例如一台机器挂了，容器要迁移到另一台机器，这个时候 IP 改变了，应用会重新注册，则其他的应用请求它的时候，还是能够从指挥部得到最新的位置。 接下来是如何相互通信的问题。NAT 这种模式，在多个主机的场景下，是存在很大问题的。在物理机 A 上的应用 A 看到的 IP 地址是容器 A 的，是 172.17.0.2，在物理机 B 上的应用 B 看到的 IP 地址是容器 B 的，不巧也是 172.17.0.2，当它们都注册到注册中心的时候，注册中心就是这个图里这样子。 这个时候，应用 A 要访问应用 B，当应用 A 从注册中心将应用 B 的 IP 地址读出来的时候，就彻底困惑了，这不是自己访问自己吗？怎么解决这个问题呢？一种办法是不去注册容器内的 IP 地址，而是注册所在物理机的 IP 地址，端口也要是物理机上映射的端口。 这样存在的问题是，应用是在容器里面的，它怎么知道物理机上的 IP 地址和端口呢？这明明是运维人员配置的，除非应用配合，读取容器平台的接口获得这个 IP 和端口。一方面，大部分分布式框架都是容器诞生之前就有了，它们不会适配这种场景；另一方面，让容器内的应用意识到容器外的环境，本来就是非常不好的设计。说好的集装箱，说好的随意迁移呢？难道要让集装箱内的货物意识到自己传的信息？而且本来 Tomcat 都是监听 8080 端口的，结果到了物理机上，就不能大家都用这个端口了，否则端口就冲突了，因而就需要随机分配端口，于是在注册中心就出现了各种各样奇怪的端口。无论是注册中心，还是调用方都会觉得很奇怪，而且不是默认的端口，很多情况下也容易出错。Kubernetes 作为集团军作战管理平台，提出指导意见，说网络模型要变平，但是没说怎么实现。于是业界就涌现了大量的方案，Flannel 就是其中之一。对于 IP 冲突的问题，如果每一个物理机都是网段 172.17.0.0/16，肯定会冲突啊，但是这个网段实在太大了，一台物理机上根本启动不了这么多的容器，所以能不能每台物理机在这个大网段里面，抠出一个小的网段，每个物理机网段都不同，自己看好自己的一亩三分地，谁也不和谁冲突。例如物理机 A 是网段 172.17.8.0/24，物理机 B 是网段 172.17.9.0/24，这样两台机器上启动的容器 IP 肯定不一样，而且就看 IP 地址，我们就一下子识别出，这个容器是本机的，还是远程的，如果是远程的，也能从网段一下子就识别出它归哪台物理机管，太方便了。 接下来的问题，就是物理机 A 上的容器如何访问到物理机 B 上的容器呢？你是不是想到了熟悉的场景？虚拟机也需要跨物理机互通，往往通过 Overlay 的方式，容器是不是也可以这样做呢？这里我要说 Flannel 使用 UDP 实现 Overlay 网络的方案。 在物理机 A 上的容器 A 里面，能看到的容器的 IP 地址是 172.17.8.2/24，里面设置了默认的路由规则 default via 172.17.8.1 dev eth0。如果容器 A 要访问 172.17.9.2，就会发往这个默认的网关 172.17.8.1。172.17.8.1 就是物理机上面 docker0 网桥的 IP 地址，这台物理机上的所有容器都是连接到这个网桥的。在物理机上面，查看路由策略，会有这样一条 172.17.0.0/24 via 172.17.0.0 dev flannel.1，也就是说发往 172.17.9.2 的网络包会被转发到 flannel.1 这个网卡。这个网卡是怎么出来的呢？在每台物理机上，都会跑一个 flanneld 进程，这个进程打开一个 /dev/net/tun 字符设备的时候，就出现了这个网卡。你有没有想起 qemu-kvm，打开这个字符设备的时候，物理机上也会出现一个网卡，所有发到这个网卡上的网络包会被 qemu-kvm 接收进来，变成二进制串。只不过接下来 qemu-kvm 会模拟一个虚拟机里面的网卡，将二进制的串变成网络包，发给虚拟机里面的网卡。但是 flanneld 不用这样做，所有发到 flannel.1 这个网卡的包都会被 flanneld 进程读进去，接下来 flanneld 要对网络包进行处理。 物理机 A 上的 flanneld 会将网络包封装在 UDP 包里面，然后外层加上物理机 A 和物理机 B 的 IP 地址，发送给物理机 B 上的 flanneld。为什么是 UDP 呢？因为不想在 flanneld 之间建立两两连接，而 UDP 没有连接的概念，任何一台机器都能发给另一台。物理机 B 上的 flanneld 收到包之后，解开 UDP 的包，将里面的网络包拿出来，从物理机 B 的 flannel.1 网卡发出去。在物理机 B 上，有路由规则 172.17.9.0/24 dev docker0 proto kernel scope link src 172.17.9.1。将包发给 docker0，docker0 将包转给容器 B。通信成功。上面的过程连通性没有问题，但是由于全部在用户态，所以性能差了一些。 跨物理机的连通性问题，在虚拟机那里有成熟的方案，就是 VXLAN，那能不能 Flannel 也用 VXLAN 呢？当然可以了。如果使用 VXLAN，就不需要打开一个 TUN 设备了，而是要建立一个 VXLAN 的 VTEP。如何建立呢？可以通过 netlink 通知内核建立一个 VTEP 的网卡 flannel.1。在我们讲 OpenvSwitch 的时候提过，netlink 是一种用户态和内核态通信的机制。当网络包从物理机 A 上的容器 A 发送给物理机 B 上的容器 B，在容器 A 里面通过默认路由到达物理机 A 上的 docker0 网卡，然后根据路由规则，在物理机 A 上，将包转发给 flannel.1。这个时候 flannel.1 就是一个 VXLAN 的 VTEP 了，它将网络包进行封装。内部的 MAC 地址这样写：源为物理机 A 的 flannel.1 的 MAC 地址，目标为物理机 B 的 flannel.1 的 MAC 地址，在外面加上 VXLAN 的头。外层的 IP 地址这样写：源为物理机 A 的 IP 地址，目标为物理机 B 的 IP 地址，外面加上物理机的 MAC 地址。这样就能通过 VXLAN 将包转发到另一台机器，从物理机 B 的 flannel.1 上解包，变成内部的网络包，通过物理机 B 上的路由转发到 docker0，然后转发到容器 B 里面。通信成功。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:39:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，今天的内容就到这里，我来总结一下。基于 NAT 的容器网络模型在微服务架构下有两个问题，一个是 IP 重叠，一个是端口冲突，需要通过 Overlay 网络的机制保持跨节点的连通性。Flannel 是跨节点容器网络方案之一，它提供的 Overlay 方案主要有两种方式，一种是 UDP 在用户态封装，一种是 VXLAN 在内核态封装，而 VXLAN 的性能更好一些。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:39:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第31讲 | 容器网络之Calico：为高效说出善意的谎言 上一节我们讲了 Flannel 如何解决容器跨主机互通的问题，这个解决方式其实和虚拟机的网络互通模式是差不多的，都是通过隧道。但是 Flannel 有一个非常好的模式，就是给不同的物理机设置不同网段，这一点和虚拟机的 Overlay 的模式完全不一样。在虚拟机的场景下，整个网段在所有的物理机之间都是可以“飘来飘去”的。网段不同，就给了我们做路由策略的可能。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:40:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"Calico 网络模型的设计思路 我们看图中的两台物理机。它们的物理网卡是同一个二层网络里面的。由于两台物理机的容器网段不同，我们完全可以将两台物理机配置成为路由器，并按照容器的网段配置路由表。 例如，在物理机 A 中，我们可以这样配置：要想访问网段 172.17.9.0/24，下一跳是 192.168.100.101，也即到物理机 B 上去。这样在容器 A 中访问容器 B，当包到达物理机 A 的时候，就能够匹配到这条路由规则，并将包发给下一跳的路由器，也即发给物理机 B。在物理机 B 上也有路由规则，要访问 172.17.9.0/24，从 docker0 的网卡进去即可。当容器 B 返回结果的时候，在物理机 B 上，可以做类似的配置：要想访问网段 172.17.8.0/24，下一跳是 192.168.100.100，也即到物理机 A 上去。当包到达物理机 B 的时候，能够匹配到这条路由规则，将包发给下一跳的路由器，也即发给物理机 A。在物理机 A 上也有路由规则，要访问 172.17.8.0/24，从 docker0 的网卡进去即可。 这就是 Calico 网络的大概思路，即不走 Overlay 网络，不引入另外的网络性能损耗，而是将转发全部用三层网络的路由转发来实现，只不过具体的实现和上面的过程稍有区别。首先，如果全部走三层的路由规则，没必要每台机器都用一个 docker0，从而浪费了一个 IP 地址，而是可以直接用路由转发到 veth pair 在物理机这一端的网卡。同样，在容器内，路由规则也可以这样设定：把容器外面的 veth pair 网卡算作默认网关，下一跳就是外面的物理机。于是，整个拓扑结构就变成了这个图中的样子。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:40:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"Calico 网络的转发细节 我们来看其中的一些细节。容器 A1 的 IP 地址为 172.17.8.2/32，这里注意，不是 /24，而是 /32，将容器 A1 作为一个单点的局域网了。容器 A1 里面的默认路由，Calico 配置得比较有技巧。 default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link 这个 IP 地址 169.254.1.1 是默认的网关，但是整个拓扑图中没有一张网卡是这个地址。那如何到达这个地址呢？前面我们讲网关的原理的时候说过，当一台机器要访问网关的时候，首先会通过 ARP 获得网关的 MAC 地址，然后将目标 MAC 变为网关的 MAC，而网关的 IP 地址不会在任何网络包头里面出现，也就是说，没有人在乎这个地址具体是什么，只要能找到对应的 MAC，响应 ARP 就可以了。ARP 本地有缓存，通过 ip neigh 命令可以查看。 169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee STALE 这个 MAC 地址是 Calico 硬塞进去的，但是没有关系，它能响应 ARP，于是发出的包的目标 MAC 就是这个 MAC 地址。在物理机 A 上查看所有网卡的 MAC 地址的时候，我们会发现 veth1 就是这个 MAC 地址。所以容器 A1 里发出的网络包，第一跳就是这个 veth1 这个网卡，也就到达了物理机 A 这个路由器。在物理机 A 上有三条路由规则，分别是去两个本机的容器的路由，以及去 172.17.9.0/24，下一跳为物理机 B。 172.17.8.2 dev veth1 scope link 172.17.8.3 dev veth2 scope link 172.17.9.0/24 via 192.168.100.101 dev eth0 proto bird onlink 同理，物理机 B 上也有三条路由规则，分别是去两个本机的容器的路由，以及去 172.17.8.0/24，下一跳为物理机 A。 172.17.9.2 dev veth1 scope link 172.17.9.3 dev veth2 scope link 172.17.8.0/24 via 192.168.100.100 dev eth0 proto bird onlink 如果你觉得这些规则过于复杂，我将刚才的拓扑图转换为这个更加容易理解的图。 在这里，物理机化身为路由器，通过路由器上的路由规则，将包转发到目的地。在这个过程中，没有隧道封装解封装，仅仅是单纯的路由转发，性能会好很多。但是，这种模式也有很多问题。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:40:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"Calico 的架构 路由配置组件 Felix 如果只有两台机器，每台机器只有两个容器，而且保持不变。我手动配置一下，倒也没啥问题。但是如果容器不断地创建、删除，节点不断地加入、退出，情况就会变得非常复杂。 就像图中，有三台物理机，两两之间都需要配置路由，每台物理机上对外的路由就有两条。如果有六台物理机，则每台物理机上对外的路由就有五条。新加入一个节点，需要通知每一台物理机添加一条路由。这还是在物理机之间，一台物理机上，每创建一个容器，也需要多配置一条指向这个容器的路由。如此复杂，肯定不能手动配置，需要每台物理机上有一个 agent，当创建和删除容器的时候，自动做这件事情。这个 agent 在 Calico 中称为 Felix。 路由广播组件 BGP Speaker 当 Felix 配置了路由之后，接下来的问题就是，如何将路由信息，也即将“如何到达我这个节点，访问我这个节点上的容器”这些信息，广播出去。能想起来吗？这其实就是路由协议啊！路由协议就是将“我能到哪里，如何能到我”的信息广播给全网传出去，从而客户端可以一跳一跳地访问目标地址的。路由协议有很多种，Calico 使用的是 BGP 协议。在 Calico 中，每个 Node 上运行一个软件 BIRD，作为 BGP 的客户端，或者叫作 BGP Speaker，将“如何到达我这个 Node，访问我这个 Node 上的容器”的路由信息广播出去。所有 Node 上的 BGP Speaker 都互相建立连接，就形成了全互连的情况，这样每当路由有所变化的时候，所有节点就都能够收到了。 安全策略组件 Calico 中还实现了灵活配置网络策略 Network Policy，可以灵活配置两个容器通或者不通。这个怎么实现呢？ 虚拟机中的安全组，是用 iptables 实现的。Calico 中也是用 iptables 实现的。这个图里的内容是 iptables 在内核处理网络包的过程中可以嵌入的处理点。Calico 也是在这些点上设置相应的规则。 当网络包进入物理机上的时候，进入 PREOUTING 规则，这里面有一个规则是 cali-fip-dnat，这是实现浮动 IP（Floating IP）的场景，主要将外网的 IP 地址 dnat 作为容器内的 IP 地址。在虚拟机场景下，路由器的网络 namespace 里面有一个外网网卡上，也设置过这样一个 DNAT 规则。接下来可以根据路由判断，是到本地的，还是要转发出去的。 如果是本地的，走 INPUT 规则，里面有个规则是 cali-wl-to-host，wl 的意思是 workload，也即容器，也即这是用来判断从容器发到物理机的网络包是否符合规则的。这里面内嵌一个规则 cali-from-wl-dispatch，也是匹配从容器来的包。如果有两个容器，则会有两个容器网卡，这里面内嵌有详细的规则“cali-fw-cali 网卡 1”和“cali-fw-cali 网卡 2”，fw 就是 from workload，也就是匹配从容器 1 来的网络包和从容器 2 来的网络包。如果是转发出去的，走 FORWARD 规则，里面有个规则 cali-FORWARD。这里面分两种情况，一种是从容器里面发出来，转发到外面的；另一种是从外面发进来，转发到容器里面的。第一种情况匹配的规则仍然是 cali-from-wl-dispatch，也即 from workload。第二种情况匹配的规则是 cali-to-wl-dispatch，也即 to workload。如果有两个容器，则会有两个容器网卡，在这里面内嵌有详细的规则“cali-tw-cali 网卡 1”和“cali-tw-cali 网卡 2”，tw 就是 to workload，也就是匹配发往容器 1 的网络包和发送到容器 2 的网络包。接下来是匹配 OUTPUT 规则，里面有 cali-OUTPUT。接下来是 POSTROUTING 规则，里面有一个规则是 cali-fip-snat，也即发出去的时候，将容器网络 IP 转换为浮动 IP 地址。在虚拟机场景下，路由器的网络 namespace 里面有一个外网网卡上，也设置过这样一个 SNAT 规则。至此为止，Calico 的所有组件基本凑齐。来看看我汇总的图。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:40:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"全连接复杂性与规模问题 这里面还存在问题，就是 BGP 全连接的复杂性问题。你看刚才的例子里只有六个节点，BGP 的互连已经如此复杂，如果节点数据再多，这种全互连的模式肯定不行，到时候都成蜘蛛网了。于是多出了一个组件 BGP Route Reflector，它也是用 BIRD 实现的。有了它，BGP Speaker 就不用全互连了，而是都直连它，它负责将全网的路由信息广播出去。可是问题来了，规模大了，大家都连它，它受得了吗？这个 BGP Router Reflector 会不会成为瓶颈呢？ 所以，肯定不能让一个 BGP Router Reflector 管理所有的路由分发，而是应该有多个 BGP Router Reflector，每个 BGP Router Reflector 管一部分。多大算一部分呢？咱们讲述数据中心的时候，说服务器都是放在机架上的，每个机架上最顶端有个 TOR 交换机。那将机架上的机器连在一起，这样一个机架是不是可以作为一个单元，让一个 BGP Router Reflector 来管理呢？如果要跨机架，如何进行通信呢？这就需要 BGP Router Reflector 也直接进行路由交换。它们之间的交换和一个机架之间的交换有什么关系吗？有没有觉得在这个场景下，一个机架就像一个数据中心，可以把它设置为一个 AS，而 BGP Router Reflector 有点儿像数据中心的边界路由器。在一个 AS 内部，也即服务器和 BGP Router Reflector 之间使用的是数据中心内部的路由协议 iBGP，BGP Router Reflector 之间使用的是数据中心之间的路由协议 eBGP。 这个图中，一个机架上有多台机器，每台机器上面启动多个容器，每台机器上都有可以到达这些容器的路由。每台机器上都启动一个 BGP Speaker，然后将这些路由规则上报到这个 Rack 上接入交换机的 BGP Route Reflector，将这些路由通过 iBGP 协议告知到接入交换机的三层路由功能。在接入交换机之间也建立 BGP 连接，相互告知路由，因而一个 Rack 里面的路由可以告知另一个 Rack。有多个核心或者汇聚交换机将接入交换机连接起来，如果核心和汇聚起二层互通的作用，则接入和接入之间之间交换路由即可。如果核心和汇聚交换机起三层路由的作用，则路由需要通过核心或者汇聚交换机进行告知。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:40:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"跨网段访问问题 上面的 Calico 模式还有一个问题，就是跨网段问题，这里的跨网段是指物理机跨网段。前面我们说的那些逻辑成立的条件，是我们假设物理机可以作为路由器进行使用。例如物理机 A 要告诉物理机 B，你要访问 172.17.8.0/24，下一跳是我 192.168.100.100；同理，物理机 B 要告诉物理机 A，你要访问 172.17.9.0/24，下一跳是我 192.168.100.101。之所以能够这样，是因为物理机 A 和物理机 B 是同一个网段的，是连接在同一个交换机上的。那如果物理机 A 和物理机 B 不是在同一个网段呢？ 例如，物理机 A 的网段是 192.168.100.100/24，物理机 B 的网段是 192.168.200.101/24，这样两台机器就不能通过二层交换机连接起来了，需要在中间放一台路由器，做一次路由转发，才能跨网段访问。本来物理机 A 要告诉物理机 B，你要访问 172.17.8.0/24，下一跳是我 192.168.100.100 的，但是中间多了一台路由器，下一跳不是我了，而是中间的这台路由器了，这台路由器的再下一跳，才是我。这样之前的逻辑就不成立了。我们看刚才那张图的下半部分。物理机 B 上的容器要访问物理机 A 上的容器，第一跳就是物理机 B，IP 为 192.168.200.101，第二跳是中间的物理路由器右面的网口，IP 为 192.168.200.1，第三跳才是物理机 A，IP 为 192.168.100.100。 这是咱们通过拓扑图看到的，关键问题是，在系统中物理机 A 如何告诉物理机 B，怎么让它才能到我这里？物理机 A 根本不可能知道从物理机 B 出来之后的下一跳是谁，况且现在只是中间隔着一个路由器这种简单的情况，如果隔着多个路由器呢？谁能把这一串的路径告诉物理机 B 呢？我们能想到的第一种方式是，让中间所有的路由器都来适配 Calico。本来它们互相告知路由，只互相告知物理机的，现在还要告知容器的网段。这在大部分情况下，是不可能的。第二种方式，还是在物理机 A 和物理机 B 之间打一个隧道，这个隧道有两个端点，在端点上进行封装，将容器的 IP 作为乘客协议放在隧道里面，而物理主机的 IP 放在外面作为承载协议。这样不管外层的 IP 通过传统的物理网络，走多少跳到达目标物理机，从隧道两端看起来，物理机 A 的下一跳就是物理机 B，这样前面的逻辑才能成立。这就是 Calico 的 IPIP 模式。使用了 IPIP 模式之后，在物理机 A 上，我们能看到这样的路由表： 172.17.8.2 dev veth1 scope link 172.17.8.3 dev veth2 scope link 172.17.9.0/24 via 192.168.200.101 dev tun0 proto bird onlink 这和原来模式的区别在于，下一跳不再是同一个网段的物理机 B 了，IP 为 192.168.200.101，并且不是从 eth0 跳，而是建立一个隧道的端点 tun0，从这里才是下一跳。如果我们在容器 A1 里面的 172.17.8.2，去 ping 容器 B1 里面的 172.17.9.2，首先会到物理机 A。在物理机 A 上根据上面的规则，会转发给 tun0，并在这里对包做封装： 内层源 IP 为 172.17.8.2；内层目标 IP 为 172.17.9.2；外层源 IP 为 192.168.100.100；外层目标 IP 为 192.168.200.101。 将这个包从 eth0 发出去，在物理网络上会使用外层的 IP 进行路由，最终到达物理机 B。在物理机 B 上，tun0 会解封装，将内层的源 IP 和目标 IP 拿出来，转发给相应的容器。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:40:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里，我们来总结一下。Calico 推荐使用物理机作为路由器的模式，这种模式没有虚拟化开销，性能比较高。Calico 的主要组件包括路由、iptables 的配置组件 Felix、路由广播组件 BGP Speaker，以及大规模场景下的 BGP Route Reflector。为解决跨网段的问题，Calico 还有一种 IPIP 模式，也即通过打隧道的方式，从隧道端点来看，将本来不是邻居的两台机器，变成相邻的机器。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:40:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"微服务相关协议 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:41:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第32讲 | RPC协议综述：远在天边，近在眼前 前面我们讲了容器网络如何实现跨主机互通，以及微服务之间的相互调用。 网络是打通了，那服务之间的互相调用，该怎么实现呢？你可能说，咱不是学过 Socket 吗。服务之间分调用方和被调用方，我们就建立一个 TCP 或者 UDP 的连接，不就可以通信了？ 你仔细想一下，这事儿没这么简单。我们就拿最简单的场景，客户端调用一个加法函数，将两个整数加起来，返回它们的和。如果放在本地调用，那是简单的不能再简单了，只要稍微学过一种编程语言，三下五除二就搞定了。但是一旦变成了远程调用，门槛一下子就上去了。首先你要会 Socket 编程，至少先要把咱们这门网络协议课学一下，然后再看 N 本砖头厚的 Socket 程序设计的书，学会咱们学过的几种 Socket 程序设计的模型。这就使得本来大学毕业就能干的一项工作，变成了一件五年工作经验都不一定干好的工作，而且，搞定了 Socket 程序设计，才是万里长征的第一步。后面还有很多问题呢！ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:42:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何解决这五个问题？ 问题一：如何规定远程调用的语法？ 客户端如何告诉服务端，我是一个加法，而另一个是乘法。我是用字符串“add”传给你，还是传给你一个整数，比如 1 表示加法，2 表示乘法？服务端该如何告诉客户端，我的这个加法，目前只能加整数，不能加小数，不能加字符串；而另一个加法“add1”，它能实现小数和整数的混合加法。那返回值是什么？正确的时候返回什么，错误的时候又返回什么？ 问题二：如果传递参数？ 我是先传两个整数，后传一个操作符“add”，还是先传操作符，再传两个整数？是不是像咱们数据结构里一样，如果都是 UDP，想要实现一个逆波兰表达式，放在一个报文里面还好，如果是 TCP，是一个流，在这个流里面，如何将两次调用进行分界？什么时候是头，什么时候是尾？把这次的参数和上次的参数混了起来，TCP 一端发送出去的数据，另外一端不一定能一下子全部读取出来。所以，怎么才算读完呢？ 问题三：如何表示数据？ 在这个简单的例子中，传递的就是一个固定长度的 int 值，这种情况还好，如果是变长的类型，是一个结构体，甚至是一个类，应该怎么办呢？如果是 int，不同的平台上长度也不同，该怎么办呢？ 在网络上传输超过一个 Byte 的类型，还有大端 Big Endian 和小端 Little Endian 的问题。假设我们要在 32 位四个 Byte 的一个空间存放整数 1，很显然只要一个 Byte 放 1，其他三个 Byte 放 0 就可以了。那问题是，最后一个 Byte 放 1 呢，还是第一个 Byte 放 1 呢？或者说 1 作为最低位，应该是放在 32 位的最后一个位置呢，还是放在第一个位置呢？最低位放在最后一个位置，叫作 Little Endian，最低位放在第一个位置，叫作 Big Endian。TCP/IP 协议栈是按照 Big Endian 来设计的，而 X86 机器多按照 Little Endian 来设计的，因而发出去的时候需要做一个转换。 问题四：如何知道一个服务端都实现了哪些远程调用？ 从哪个端口可以访问这个远程调用？假设服务端实现了多个远程调用，每个可能实现在不同的进程中，监听的端口也不一样，而且由于服务端都是自己实现的，不可能使用一个大家都公认的端口，而且有可能多个进程部署在一台机器上，大家需要抢占端口，为了防止冲突，往往使用随机端口，那客户端如何找到这些监听的端口呢？ 问题五：发生了错误、重传、丢包、性能等问题怎么办？ 本地调用没有这个问题，但是一旦到网络上，这些问题都需要处理，因为网络是不可靠的，虽然在同一个连接中，我们还可通过 TCP 协议保证丢包、重传的问题，但是如果服务器崩溃了又重启，当前连接断开了，TCP 就保证不了了，需要应用自己进行重新调用，重新传输会不会同样的操作做两遍，远程调用性能会不会受影响呢？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:42:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"协议约定问题 看到这么多问题，你是不是想起了我第一节讲过的这张图。 本地调用函数里有很多问题，比如词法分析、语法分析、语义分析等等，这些编译器本来都能帮你做了。但是在远程调用中，这些问题你都需要重新操心。很多公司的解决方法是，弄一个核心通信组，里面都是 Socket 编程的大牛，实现一个统一的库，让其他业务组的人来调用，业务的人不需要知道中间传输的细节。通信双方的语法、语义、格式、端口、错误处理等，都需要调用方和被调用方开会协商，双方达成一致。一旦有一方改变，要及时通知对方，否则通信就会有问题。可是不是每一个公司都有这种大牛团队，往往只有大公司才配得起，那有没有已经实现好的框架可以使用呢？当然有。一个大牛 Bruce Jay Nelson 写了一篇论文Implementing Remote Procedure Calls，定义了 RPC 的调用标准。后面所有 RPC 框架，都是按照这个标准模式来的。 当客户端的应用想发起一个远程调用时，它实际是通过本地调用本地调用方的 Stub。它负责将调用的接口、方法和参数，通过约定的协议规范进行编码，并通过本地的 RPCRuntime 进行传输，将调用网络包发送到服务器。服务器端的 RPCRuntime 收到请求后，交给提供方 Stub 进行解码，然后调用服务端的方法，服务端执行方法，返回结果，提供方 Stub 将返回结果编码后，发送给客户端，客户端的 RPCRuntime 收到结果，发给调用方 Stub 解码得到结果，返回给客户端。这里面分了三个层次，对于用户层和服务端，都像是本地调用一样，专注于业务逻辑的处理就可以了。对于 Stub 层，处理双方约定好的语法、语义、封装、解封装。对于 RPCRuntime，主要处理高性能的传输，以及网络的错误和异常。最早的 RPC 的一种实现方式称为 Sun RPC 或 ONC RPC。Sun 公司是第一个提供商业化 RPC 库和 RPC 编译器的公司。这个 RPC 框架是在 NFS 协议中使用的。 NFS（Network File System）就是网络文件系统。要使 NFS 成功运行，要启动两个服务端，一个是 mountd，用来挂载文件路径；一个是 nfsd，用来读写文件。NFS 可以在本地 mount 一个远程的目录到本地的一个目录，从而本地的用户在这个目录里面写入、读出任何文件的时候，其实操作的是远程另一台机器上的文件。操作远程和远程调用的思路是一样的，就像操作本地一样。所以 NFS 协议就是基于 RPC 实现的。当然无论是什么 RPC，底层都是 Socket 编程。 XDR（External Data Representation，外部数据表示法）是一个标准的数据压缩格式，可以表示基本的数据类型，也可以表示结构体。这里是几种基本的数据类型。 在 RPC 的调用过程中，所有的数据类型都要封装成类似的格式。而且 RPC 的调用和结果返回，也有严格的格式。XID 唯一标识一对请求和回复。请求为 0，回复为 1。RPC 有版本号，两端要匹配 RPC 协议的版本号。如果不匹配，就会返回 Deny，原因就是 RPC_MISMATCH。程序有编号。如果服务端找不到这个程序，就会返回 PROG_UNAVAIL。程序有版本号。如果程序的版本号不匹配，就会返回 PROG_MISMATCH。一个程序可以有多个方法，方法也有编号，如果找不到方法，就会返回 PROC_UNAVAIL。调用需要认证鉴权，如果不通过，则 Deny。最后是参数列表，如果参数无法解析，则返回 GABAGE_ARGS。 为了可以成功调用 RPC，在客户端和服务端实现 RPC 的时候，首先要定义一个双方都认可的程序、版本、方法、参数等。 如果还是上面的加法，则双方约定为一个协议定义文件，同理如果是 NFS、mount 和读写，也会有类似的定义。有了协议定义文件，ONC RPC 会提供一个工具，根据这个文件生成客户端和服务器端的 Stub 程序。 最下层的是 XDR 文件，用于编码和解码参数。这个文件是客户端和服务端共享的，因为只有双方一致才能成功通信。在客户端，会调用 clnt_create 创建一个连接，然后调用 add_1，这是一个 Stub 函数，感觉是在调用本地一样。其实是这个函数发起了一个 RPC 调用，通过调用 clnt_call 来调用 ONC RPC 的类库，来真正发送请求。调用的过程非常复杂，一会儿我详细说这个。当然服务端也有一个 Stub 程序，监听客户端的请求，当调用到达的时候，判断如果是 add，则调用真正的服务端逻辑，也即将两个数加起来。服务端将结果返回服务端的 Stub，这个 Stub 程序发送结果给客户端，客户端的 Stub 程序正在等待结果，当结果到达客户端 Stub，就将结果返回给客户端的应用程序，从而完成整个调用过程。有了这个 RPC 的框架，前面五个问题中的前三个“如何规定远程调用的语法？”“如何传递参数？”以及“如何表示数据？”基本解决了，这三个问题我们统称为协议约定问题。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:42:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"传输问题 但是错误、重传、丢包、性能等问题还没有解决，这些问题我们统称为传输问题。这个就不用 Stub 操心了，而是由 ONC RPC 的类库来实现。这是大牛们实现的，我们只要调用就可以了。 在这个类库中，为了解决传输问题，对于每一个客户端，都会创建一个传输管理层，而每一次 RPC 调用，都会是一个任务，在传输管理层，你可以看到熟悉的队列机制、拥塞窗口机制等。由于在网络传输的时候，经常需要等待，因而同步的方式往往效率比较低，因而也就有 Socket 的异步模型。为了能够异步处理，对于远程调用的处理，往往是通过状态机来实现的。只有当满足某个状态的时候，才进行下一步，如果不满足状态，不是在那里等，而是将资源留出来，用来处理其他的 RPC 调用。 从这个图可以看出，这个状态转换图还是很复杂的。首先，进入起始状态，查看 RPC 的传输层队列中有没有空闲的位置，可以处理新的 RPC 任务。如果没有，说明太忙了，或直接结束或重试。如果申请成功，就可以分配内存，获取服务的端口号，然后连接服务器。连接的过程要有一段时间，因而要等待连接的结果，会有连接失败，或直接结束或重试。如果连接成功，则开始发送 RPC 请求，然后等待获取 RPC 结果，这个过程也需要一定的时间；如果发送出错，可以重新发送；如果连接断了，可以重新连接；如果超时，可以重新传输；如果获取到结果，就可以解码，正常结束。这里处理了连接失败、重试、发送失败、超时、重试等场景。不是大牛真写不出来，因而实现一个 RPC 的框架，其实很有难度。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:42:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"服务发现问题 传输问题解决了，我们还遗留一个问题，就是问题四“如何找到 RPC 服务端的那个随机端口”。这个问题我们称为服务发现问题。在 ONC RPC 中，服务发现是通过 portmapper 实现的。 portmapper 会启动在一个众所周知的端口上，RPC 程序由于是用户自己写的，会监听在一个随机端口上，但是 RPC 程序启动的时候，会向 portmapper 注册。客户端要访问 RPC 服务端这个程序的时候，首先查询 portmapper，获取 RPC 服务端程序的随机端口，然后向这个随机端口建立连接，开始 RPC 调用。从图中可以看出，mount 命令的 RPC 调用，就是这样实现的。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:42:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里，我们来总结一下。远程调用看起来用 Socket 编程就可以了，其实是很复杂的，要解决协议约定问题、传输问题和服务发现问题。大牛 Bruce Jay Nelson 的论文、早期 ONC RPC 框架，以及 NFS 的实现，给出了解决这三大问题的示范性实现，也即协议约定要公用协议描述文件，并通过这个文件生成 Stub 程序；RPC 的传输一般需要一个状态机，需要另外一个进程专门做服务发现。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:42:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第33讲 | 基于XML的SOAP协议：不要说NBA，请说美国职业篮球联赛 上一节我们讲了 RPC 的经典模型和设计要点，并用最早期的 ONC RPC 为例子，详述了具体的实现。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:43:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"ONC RPC 存在哪些问题？ ONC RPC 将客户端要发送的参数，以及服务端要发送的回复，都压缩为一个二进制串，这样固然能够解决双方的协议约定问题，但是存在一定的不方便。首先，需要双方的压缩格式完全一致，一点都不能差。一旦有少许的差错，多一位，少一位或者错一位，都可能造成无法解压缩。当然，我们可以用传输层的可靠性以及加入校验值等方式，来减少传输过程中的差错。其次，协议修改不灵活。如果不是传输过程中造成的差错，而是客户端因为业务逻辑的改变，添加或者删除了字段，或者服务端添加或者删除了字段，而双方没有及时通知，或者线上系统没有及时升级，就会造成解压缩不成功。因而，当业务发生改变，需要多传输一些参数或者少传输一些参数的时候，都需要及时通知对方，并且根据约定好的协议文件重新生成双方的 Stub 程序。自然，这样灵活性比较差。如果仅仅是沟通的问题也还好解决，其实更难弄的还有版本的问题。比如在服务端提供一个服务，参数的格式是版本一的，已经有 50 个客户端在线上调用了。现在有一个客户端有个需求，要加一个字段，怎么办呢？这可是一个大工程，所有的客户端都要适配这个，需要重新写程序，加上这个字段，但是传输值是 0，不需要这个字段的客户端很“冤”，本来没我啥事儿，为啥让我也忙活？最后，ONC RPC 的设计明显是面向函数的，而非面向对象。而当前面向对象的业务逻辑设计与实现方式已经成为主流。这一切的根源就在于压缩。这就像平时我们爱用缩略语。如果是篮球爱好者，你直接说 NBA，他马上就知道什么意思，但是如果你给一个大妈说 NBA，她可能就不知所云。所以，这种 RPC 框架只能用于客户端和服务端全由一拨人开发的场景，或者至少客户端和服务端的开发人员要密切沟通，相互合作，有大量的共同语言，才能按照既定的协议顺畅地进行工作。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:43:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"XML 与 SOAP 但是，一般情况下，我们做一个服务，都是要提供给陌生人用的，你和客户不会经常沟通，也没有什么共同语言。就像你给别人介绍 NBA，你要说美国职业篮球赛，这样不管他是干啥的，都能听得懂。放到我们的场景中，对应的就是用文本类的方式进行传输。无论哪个客户端获得这个文本，都能够知道它的意义。一种常见的文本类格式是 XML。我们这里举个例子来看。 xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cgeek:purchaseOrder xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:geek=\"http://www.example.com/geek\"\u003e \u003corder\u003e \u003cdate\u003e2018-07-01\u003c/date\u003e \u003cclassName\u003e趣谈网络协议\u003c/className\u003e \u003cAuthor\u003e刘超\u003c/Author\u003e \u003cprice\u003e68\u003c/price\u003e \u003c/order\u003e \u003c/geek:purchaseOrder\u003e 我这里不准备详细讲述 XML 的语法规则，但是你相信我，看完下面的内容，即便你没有学过 XML，也能一看就懂，这段 XML 描述的是什么，不像全面的二进制，你看到的都是 010101，不知所云。有了这个，刚才我们说的那几个问题就都不是问题了。首先，格式没必要完全一致。比如如果我们把 price 和 author 换个位置，并不影响客户端和服务端解析这个文本，也根本不会误会，说这个作者的名字叫 68。如果有的客户端想增加一个字段，例如添加一个推荐人字段，只需要在上面的文件中加一行： \u003crecommended\u003e Gary \u003c/recommended\u003e 对于不需要这个字段的客户端，只要不解析这一行就是了。只要用简单的处理，就不会出现错误。另外，这种表述方式显然是描述一个订单对象的，是一种面向对象的、更加接近用户场景的表示方式。既然 XML 这么好，接下来我们来看看怎么把它用在 RPC 中。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:43:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"传输协议问题 我们先解决第一个，传输协议的问题。基于 XML 的最著名的通信协议就是 SOAP 了，全称简单对象访问协议（Simple Object Access Protocol）。它使用 XML 编写简单的请求和回复消息，并用 HTTP 协议进行传输。SOAP 将请求和回复放在一个信封里面，就像传递一个邮件一样。信封里面的信分抬头和正文。 POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/soap+xml; charset=utf-8 Content-Length: nnn xml \u003c?xml version=\"1.0\"?\u003e \u003csoap:Envelope xmlns:soap=\"http://www.w3.org/2001/12/soap-envelope\" soap:encodingStyle=\"http://www.w3.org/2001/12/soap-encoding\"\u003e \u003csoap:Header\u003e \u003cm:Trans xmlns:m=\"http://www.w3schools.com/transaction/\" soap:mustUnderstand=\"1\"\u003e1234 \u003c/m:Trans\u003e \u003c/soap:Header\u003e \u003csoap:Body xmlns:m=\"http://www.geektime.com/perchaseOrder\"\u003e \u003cm:purchaseOrder\"\u003e \u003corder\u003e \u003cdate\u003e2018-07-01\u003c/date\u003e \u003cclassName\u003e趣谈网络协议\u003c/className\u003e \u003cAuthor\u003e刘超\u003c/Author\u003e \u003cprice\u003e68\u003c/price\u003e \u003c/order\u003e \u003c/m:purchaseOrder\u003e \u003c/soap:Body\u003e \u003c/soap:Envelope\u003e HTTP 协议我们学过，这个请求使用 POST 方法，发送一个格式为 application/soap + xml 的 XML 正文给 www.geektime.com，从而下一个单，这个订单封装在 SOAP 的信封里面，并且表明这是一笔交易（transaction），而且订单的详情都已经写明了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:43:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"协议约定问题 接下来我们解决第二个问题，就是双方的协议约定是什么样的？因为服务开发出来是给陌生人用的，就像上面下单的那个 XML 文件，对于客户端来说，它如何知道应该拼装成上面的格式呢？这就需要对于服务进行描述，因为调用的人不认识你，所以没办法找到你，问你的服务应该如何调用。当然你可以写文档，然后放在官方网站上，但是你的文档不一定更新得那么及时，而且你也写的文档也不一定那么严谨，所以常常会有调试不成功的情况。因而，我们需要一种相对比较严谨的 Web 服务描述语言，WSDL（Web Service Description Languages）。它也是一个 XML 文件。在这个文件中，要定义一个类型 order，与上面的 XML 对应起来。 xml \u003cwsdl:types\u003e \u003cxsd:schema targetNamespace=\"http://www.example.org/geektime\"\u003e \u003cxsd:complexType name=\"order\"\u003e \u003cxsd:element name=\"date\" type=\"xsd:string\"\u003e\u003c/xsd:element\u003e \u003cxsd:element name=\"className\" type=\"xsd:string\"\u003e\u003c/xsd:element\u003e \u003cxsd:element name=\"Author\" type=\"xsd:string\"\u003e\u003c/xsd:element\u003e \u003cxsd:element name=\"price\" type=\"xsd:int\"\u003e\u003c/xsd:element\u003e \u003c/xsd:complexType\u003e \u003c/xsd:schema\u003e \u003c/wsdl:types\u003e 接下来，需要定义一个 message 的结构。 xml \u003cwsdl:message name=\"purchase\"\u003e \u003cwsdl:part name=\"purchaseOrder\" element=\"tns:order\"\u003e\u003c/wsdl:part\u003e \u003c/wsdl:message\u003e 接下来，应该暴露一个端口。 xml \u003cwsdl:portType name=\"PurchaseOrderService\"\u003e \u003cwsdl:operation name=\"purchase\"\u003e \u003cwsdl:input message=\"tns:purchase\"\u003e\u003c/wsdl:input\u003e \u003cwsdl:output message=\"......\"\u003e\u003c/wsdl:output\u003e \u003c/wsdl:operation\u003e \u003c/wsdl:portType\u003e 然后，我们来编写一个 binding，将上面定义的信息绑定到 SOAP 请求的 body 里面。 xml \u003cwsdl:binding name=\"purchaseOrderServiceSOAP\" type=\"tns:PurchaseOrderService\"\u003e \u003csoap:binding style=\"rpc\" transport=\"http://schemas.xmlsoap.org/soap/http\" /\u003e \u003cwsdl:operation name=\"purchase\"\u003e \u003cwsdl:input\u003e \u003csoap:body use=\"literal\" /\u003e \u003c/wsdl:input\u003e \u003cwsdl:output\u003e \u003csoap:body use=\"literal\" /\u003e \u003c/wsdl:output\u003e \u003c/wsdl:operation\u003e \u003c/wsdl:binding\u003e 最后，我们需要编写 service。 xml \u003cwsdl:service name=\"PurchaseOrderServiceImplService\"\u003e \u003cwsdl:port binding=\"tns:purchaseOrderServiceSOAP\" name=\"PurchaseOrderServiceImplPort\"\u003e \u003csoap:address location=\"http://www.geektime.com:8080/purchaseOrder\" /\u003e \u003c/wsdl:port\u003e \u003c/wsdl:service\u003e WSDL 还是有些复杂的，不过好在有工具可以生成。对于某个服务，哪怕是一个陌生人，都可以通过在服务地址后面加上“?wsdl”来获取到这个文件，但是这个文件还是比较复杂，比较难以看懂。不过好在也有工具可以根据 WSDL 生成客户端 Stub，让客户端通过 Stub 进行远程调用，就跟调用本地的方法一样。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:43:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"服务发现问题 最后解决第三个问题，服务发现问题。这里有一个 UDDI（Universal Description, Discovery, and Integration），也即统一描述、发现和集成协议。它其实是一个注册中心，服务提供方可以将上面的 WSDL 描述文件，发布到这个注册中心，注册完毕后，服务使用方可以查找到服务的描述，封装为本地的客户端进行调用。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:43:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下。原来的二进制 RPC 有很多缺点，格式要求严格，修改过于复杂，不面向对象，于是产生了基于文本的调用方式——基于 XML 的 SOAP。SOAP 有三大要素：协议约定用 WSDL、传输协议用 HTTP、服务发现用 UDDL。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:43:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第34讲 | 基于JSON的RESTful接口协议：我不关心过程，请给我结果 上一节我们讲了基于 XML 的 SOAP 协议，SOAP 的 S 是啥意思来着？是 Simple，但是好像一点儿都不简单啊！你会发现，对于 SOAP 来讲，无论 XML 中调用的是什么函数，多是通过 HTTP 的 POST 方法发送的。但是咱们原来学 HTTP 的时候，我们知道 HTTP 除了 POST，还有 PUT、DELETE、GET 等方法，这些也可以代表一个个动作，而且基本满足增、删、查、改的需求，比如增是 POST，删是 DELETE，查是 GET，改是 PUT。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:44:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"传输协议问题 对于 SOAP 来讲，比如我创建一个订单，用 POST，在 XML 里面写明动作是 CreateOrder；删除一个订单，还是用 POST，在 XML 里面写明了动作是 DeleteOrder。其实创建订单完全可以使用 POST 动作，然后在 XML 里面放一个订单的信息就可以了，而删除用 DELETE 动作，然后在 XML 里面放一个订单的 ID 就可以了。于是上面的那个 SOAP 就变成下面这个简单的模样。 POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/xml; charset=utf-8 Content-Length: nnn \u003c?xml version=\"1.0\"?\u003e \u003corder\u003e \u003cdate\u003e2018-07-01\u003c/date\u003e \u003cclassName\u003e趣谈网络协议\u003c/className\u003e \u003cAuthor\u003e刘超\u003c/Author\u003e \u003cprice\u003e68\u003c/price\u003e \u003c/order\u003e 而且 XML 的格式也可以改成另外一种简单的文本化的对象表示格式 JSON。 POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/json; charset=utf-8 Content-Length: nnn { \"order\": { \"date\": \"2018-07-01\", \"className\": \"趣谈网络协议\", \"Author\": \"刘超\", \"price\": \"68\" } } 经常写 Web 应用的应该已经发现，这就是 RESTful 格式的 API 的样子。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:44:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"协议约定问题 然而 RESTful 可不仅仅是指 API，而是一种架构风格，全称 Representational State Transfer，表述性状态转移，来自一篇重要的论文《架构风格与基于网络的软件架构设计》（Architectural Styles and the Design of Network-based Software Architectures）。这篇文章从深层次，更加抽象地论证了一个互联网应用应该有的设计要点，而这些设计要点，成为后来我们能看到的所有高并发应用设计都必须要考虑的问题，再加上 REST API 比较简单直接，所以后来几乎成为互联网应用的标准接口。因此，和 SOAP 不一样，REST 不是一种严格规定的标准，它其实是一种设计风格。如果按这种风格进行设计，RESTful 接口和 SOAP 接口都能做到，只不过后面的架构是 REST 倡导的，而 SOAP 相对比较关注前面的接口。而且由于能够通过 WSDL 生成客户端的 Stub，因而 SOAP 常常被用于类似传统的 RPC 方式，也即调用远端和调用本地是一样的。然而本地调用和远程跨网络调用毕竟不一样，这里的不一样还不仅仅是因为有网络而导致的客户端和服务端的分离，从而带来的网络性能问题。更重要的问题是，客户端和服务端谁来维护状态。所谓的状态就是对某个数据当前处理到什么程度了。 这里举几个例子，例如，我浏览到哪个目录了，我看到第几页了，我要买个东西，需要扣减一下库存，这些都是状态。本地调用其实没有人纠结这个问题，因为数据都在本地，谁处理都一样，而且一边处理了，另一边马上就能看到。当有了 RPC 之后，我们本来期望对上层透明，就像上一节说的“远在天边，尽在眼前”。于是使用 RPC 的时候，对于状态的问题也没有太多的考虑。就像 NFS 一样，客户端会告诉服务端，我要进入哪个目录，服务端必须要为某个客户端维护一个状态，就是当前这个客户端浏览到哪个目录了。例如，客户端输入 cd hello，服务端要在某个地方记住，上次浏览到 /root/liuchao 了，因而客户的这次输入，应该给它显示 /root/liuchao/hello 下面的文件列表。而如果有另一个客户端，同样输入 cd hello，服务端也在某个地方记住，上次浏览到 /var/lib，因而要给客户显示的是 /var/lib/hello。不光 NFS，如果浏览翻页，我们经常要实现函数 next()，在一个列表中取下一页，但是这就需要服务端记住，客户端 A 上次浏览到 20～30 页了，那它调用 next()，应该显示 30～40 页，而客户端 B 上次浏览到 100～110 页了，调用 next() 应该显示 110～120 页。上面的例子都是在 RPC 场景下，由服务端来维护状态，很多 SOAP 接口设计的时候，也常常按这种模式。这种模式原来没有问题，是因为客户端和服务端之间的比例没有失衡。因为一般不会同时有太多的客户端同时连上来，所以 NFS 还能把每个客户端的状态都记住。 公司内部使用的 ERP 系统，如果使用 SOAP 的方式实现，并且服务端为每个登录的用户维护浏览到报表那一页的状态，由于一个公司内部的人也不会太多，把 ERP 放在一个强大的物理机上，也能记得过来。但是互联网场景下，客户端和服务端就彻底失衡了。你可以想象“双十一”，多少人同时来购物，作为服务端，它能记得过来吗？当然不可能，只好多个服务端同时提供服务，大家分担一下。但是这就存在一个问题，服务端怎么把自己记住的客户端状态告诉另一个服务端呢？或者说，你让我给你分担工作，你也要把工作的前因后果给我说清楚啊！那服务端索性就要想了，既然这么多客户端，那大家就分分工吧。服务端就只记录资源的状态，例如文件的状态，报表的状态，库存的状态，而客户端自己维护自己的状态。比如，你访问到哪个目录了啊，报表的哪一页了啊，等等。这样对于 API 也有影响，也就是说，当客户端维护了自己的状态，就不能这样调用服务端了。例如客户端说，我想访问当前目录下的 hello 路径。服务端说，我怎么知道你的当前路径。所以客户端要先看看自己当前路径是 /root/liuchao，然后告诉服务端说，我想访问 /root/liuchao/hello 路径。再比如，客户端说我想访问下一页，服务端说，我怎么知道你当前访问到哪一页了。所以客户端要先看看自己访问到了 100～110 页，然后告诉服务器说，我想访问 110～120 页。 这就是服务端的无状态化。这样服务端就可以横向扩展了，一百个人一起服务，不用交接，每个人都能处理。所谓的无状态，其实是服务端维护资源的状态，客户端维护会话的状态。对于服务端来讲，只有资源的状态改变了，客户端才调用 POST、PUT、DELETE 方法来找我；如果资源的状态没变，只是客户端的状态变了，就不用告诉我了，对于我来说都是统一的 GET。虽然这只改进了 GET，但是已经带来了很大的进步。因为对于互联网应用，大多数是读多写少的。而且只要服务端的资源状态不变，就给了我们缓存的可能。例如可以将状态缓存到接入层，甚至缓存到 CDN 的边缘节点，这都是资源状态不变的好处。按照这种思路，对于 API 的设计，就慢慢变成了以资源为核心，而非以过程为核心。也就是说，客户端只要告诉服务端你想让资源状态最终变成什么样就可以了，而不用告诉我过程，不用告诉我动作。还是文件目录的例子。客户端应该访问哪个绝对路径，而非一个动作，我就要进入某个路径。再如，库存的调用，应该查看当前的库存数目，然后减去购买的数量，得到结果的库存数。这个时候应该设置为目标库存数（但是当前库存数要匹配），而非告知减去多少库存。 这种 API 的设计需要实现幂等，因为网络不稳定，就会经常出错，因而需要重试，但是一旦重试，就会存在幂等的问题，也就是同一个调用，多次调用的结果应该一样，不能一次支付调用，因为调用三次变成了支付三次。不能进入 cd a，做了三次，就变成了 cd a/a/a。也不能扣减库存，调用了三次，就扣减三次库存。当然按照这种设计模式，无论 RESTful API 还是 SOAP API 都可以将架构实现成无状态的，面向资源的、幂等的、横向扩展的、可缓存的。但是 SOAP 的 XML 正文中，是可以放任何动作的。例如 XML 里面可以写 \u003c ADD \u003e，\u003c MINUS \u003e 等。这就方便使用 SOAP 的人，将大量的动作放在 API 里面。RESTful 没这么复杂，也没给客户提供这么多的可能性，正文里的 JSON 基本描述的就是资源的状态，没办法描述动作，而且能够出发的动作只有 CRUD，也即 POST、GET、PUT、DELETE，也就是对于状态的改变。所以，从接口角度，就让你死了这条心。当然也有很多技巧的方法，在使用 RESTful API 的情况下，依然提供基于动作的有状态请求，这属于反模式了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:44:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"服务发现问题 对于 RESTful API 来讲，我们已经解决了传输协议的问题——基于 HTTP，协议约定问题——基于 JSON，最后要解决的是服务发现问题。有个著名的基于 RESTful API 的跨系统调用框架叫 Spring Cloud。在 Spring Cloud 中有一个组件叫 Eureka。传说，阿基米德在洗澡时发现浮力原理，高兴得来不及穿上裤子，跑到街上大喊：“Eureka（我找到了）！”所以 Eureka 是用来实现注册中心的，负责维护注册的服务列表。服务分服务提供方，它向 Eureka 做服务注册、续约和下线等操作，注册的主要数据包括服务名、机器 IP、端口号、域名等等。另外一方是服务消费方，向 Eureka 获取服务提供方的注册信息。为了实现负载均衡和容错，服务提供方可以注册多个。 当消费方要调用服务的时候，会从注册中心读出多个服务来，那怎么调用呢？当然是 RESTful 方式了。Spring Cloud 提供一个 RestTemplate 工具，用于将请求对象转换为 JSON，并发起 Rest 调用，RestTemplate 的调用也是分 POST、PUT、GET、 DELETE 的，当结果返回的时候，根据返回的 JSON 解析成对象。通过这样封装，调用起来也很方便。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:44:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下。SOAP 过于复杂，而且设计是面向动作的，因而往往因为架构问题导致并发量上不去。RESTful 不仅仅是一个 API，而且是一种架构模式，主要面向资源，提供无状态服务，有利于横向扩展应对高并发。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:44:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第35讲 | 二进制类RPC协议：还是叫NBA吧，总说全称多费劲 前面我们讲了两个常用文本类的 RPC 协议，对于陌生人之间的沟通，用 NBA、CBA 这样的缩略语，会使得协议约定非常不方便。在讲 CDN 和 DNS 的时候，我们讲过接入层的设计，对于静态资源或者动态资源静态化的部分都可以做缓存。但是对于下单、支付等交易场景，还是需要调用 API。对于微服务的架构，API 需要一个 API 网关统一的管理。API 网关有多种实现方式，用 Nginx 或者 OpenResty 结合 Lua 脚本是常用的方式。在上一节讲过的 Spring Cloud 体系中，有个组件 Zuul 也是干这个的。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:45:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"数据中心内部是如何相互调用的？ API 网关用来管理 API，但是 API 的实现一般在一个叫作 Controller 层的地方。这一层对外提供 API。由于是让陌生人访问的，我们能看到目前业界主流的，基本都是 RESTful 的 API，是面向大规模互联网应用的。 在 Controller 之内，就是咱们互联网应用的业务逻辑实现。上节讲 RESTful 的时候，说过业务逻辑的实现最好是无状态的，从而可以横向扩展，但是资源的状态还需要服务端去维护。资源的状态不应该维护在业务逻辑层，而是在最底层的持久化层，一般会使用分布式数据库和 ElasticSearch。这些服务端的状态，例如订单、库存、商品等，都是重中之重，都需要持久化到硬盘上，数据不能丢，但是由于硬盘读写性能差，因而持久化层往往吞吐量不能达到互联网应用要求的吞吐量，因而前面要有一层缓存层，使用 Redis 或者 memcached 将请求拦截一道，不能让所有的请求都进入数据库“中军大营”。 缓存和持久化层之上一般是基础服务层，这里面提供一些原子化的接口。例如，对于用户、商品、订单、库存的增删查改，将缓存和数据库对再上层的业务逻辑屏蔽一道。有了这一层，上层业务逻辑看到的都是接口，而不会调用数据库和缓存。因而对于缓存层的扩容，数据库的分库分表，所有的改变，都截止到这一层，这样有利于将来对于缓存和数据库的运维。再往上就是组合层。因为基础服务层只是提供简单的接口，实现简单的业务逻辑，而复杂的业务逻辑，比如下单，要扣优惠券，扣减库存等，就要在组合服务层实现。这样，Controller 层、组合服务层、基础服务层就会相互调用，这个调用是在数据中心内部的，量也会比较大，还是使用 RPC 的机制实现的。由于服务比较多，需要一个单独的注册中心来做服务发现。服务提供方会将自己提供哪些服务注册到注册中心中去，同时服务消费方订阅这个服务，从而可以对这个服务进行调用。调用的时候有一个问题，这里的 RPC 调用，应该用二进制还是文本类？其实文本的最大问题是，占用字节数目比较多。比如数字 123，其实本来二进制 8 位就够了，但是如果变成文本，就成了字符串 123。如果是 UTF-8 编码的话，就是三个字节；如果是 UTF-16，就是六个字节。同样的信息，要多费好多的空间，传输起来也更加占带宽，时延也高。因而对于数据中心内部的相互调用，很多公司选型的时候，还是希望采用更加省空间和带宽的二进制的方案。这里一个著名的例子就是 Dubbo 服务化框架二进制的 RPC 方式。 Dubbo 会在客户端的本地启动一个 Proxy，其实就是客户端的 Stub，对于远程的调用都通过这个 Stub 进行封装。接下来，Dubbo 会从注册中心获取服务端的列表，根据路由规则和负载均衡规则，在多个服务端中选择一个最合适的服务端进行调用。调用服务端的时候，首先要进行编码和序列化，形成 Dubbo 头和序列化的方法和参数。将编码好的数据，交给网络客户端进行发送，网络服务端收到消息后，进行解码。然后将任务分发给某个线程进行处理，在线程中会调用服务端的代码逻辑，然后返回结果。这个过程和经典的 RPC 模式何其相似啊！ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:45:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何解决协议约定问题？ 接下来我们还是来看 RPC 的三大问题，其中注册发现问题已经通过注册中心解决了。下面我们就来看协议约定问题。Dubbo 中默认的 RPC 协议是 Hessian2。为了保证传输的效率，Hessian2 将远程调用序列化为二进制进行传输，并且可以进行一定的压缩。这个时候你可能会疑惑，同为二进制的序列化协议，Hessian2 和前面的二进制的 RPC 有什么区别呢？这不绕了一圈又回来了吗？Hessian2 是解决了一些问题的。例如，原来要定义一个协议文件，然后通过这个文件生成客户端和服务端的 Stub，才能进行相互调用，这样使得修改就会不方便。Hessian2 不需要定义这个协议文件，而是自描述的。什么是自描述呢？所谓自描述就是，关于调用哪个函数，参数是什么，另一方不需要拿到某个协议文件、拿到二进制，靠它本身根据 Hessian2 的规则，就能解析出来。 原来有协议文件的场景，有点儿像两个人事先约定好，0 表示方法 add，然后后面会传两个数。服务端把两个数加起来，这样一方发送 012，另一方知道是将 1 和 2 加起来，但是不知道协议文件的，当它收到 012 的时候，完全不知道代表什么意思。而自描述的场景，就像两个人说的每句话都带前因后果。例如，传递的是“函数：add，第一个参数 1，第二个参数 2”。这样无论谁拿到这个表述，都知道是什么意思。但是只不过都是以二进制的形式编码的。这其实相当于综合了 XML 和二进制共同优势的一个协议。Hessian2 是如何做到这一点的呢？这就需要去看 Hessian2 的序列化的语法描述文件。 看起来很复杂，编译原理里面是有这样的语法规则的。我们从 Top 看起，下一层是 value，直到形成一棵树。这里面的有个思想，为了防止歧义，每一个类型的起始数字都设置成为独一无二的。这样，解析的时候，看到这个数字，就知道后面跟的是什么了。这里还是以加法为例子，“add(2,3)”被序列化之后是什么样的呢？ H x02 x00 # Hessian 2.0 C # RPC call x03 add # method \"add\" x92 # two arguments x92 # 2 - argument 1 x93 # 3 - argument 2 H 开头，表示使用的协议是 Hession，H 的二进制是 0x48。C 开头，表示这是一个 RPC 调用。0x03，表示方法名是三个字符。0x92，表示有两个参数。其实这里存的应该是 2，之所以加上 0x90，就是为了防止歧义，表示这里一定是一个 int。第一个参数是 2，编码为 0x92，第二个参数是 3，编码为 0x93。 这个就叫作自描述。另外，Hessian2 是面向对象的，可以传输一个对象。 class Car { String color; String model; } out.writeObject(new Car(\"red\", \"corvette\")); out.writeObject(new Car(\"green\", \"civic\")); --- C # object definition (#0) x0b example.Car # type is example.Car x92 # two fields x05 color # color field name x05 model # model field name O # object def (long form) x90 # object definition #0 x03 red # color field value x08 corvette # model field value x60 # object def #0 (short form) x05 green # color field value x05 civic # model field value 首先，定义这个类。对于类型的定义也传过去，因而也是自描述的。类名为 example.Car，字符长 11 位，因而前面长度为 0x0b。有两个成员变量，一个是 color，一个是 model，字符长 5 位，因而前面长度 0x05,。然后，传输的对象引用这个类。由于类定义在位置 0，因而对象会指向这个位置 0，编码为 0x90。后面 red 和 corvette 是两个成员变量的值，字符长分别为 3 和 8。接着又传输一个属于相同类的对象。这时候就不保存对于类的引用了，只保存一个 0x60，表示同上就可以了。可以看出，Hessian2 真的是能压缩尽量压缩，多一个 Byte 都不传。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:45:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"如何解决 RPC 传输问题？ 接下来，我们再来看 Dubbo 的 RPC 传输问题。前面我们也说了，基于 Socket 实现一个高性能的服务端，是很复杂的一件事情，在 Dubbo 里面，使用了 Netty 的网络传输框架。Netty 是一个非阻塞的基于事件的网络传输框架，在服务端启动的时候，会监听一个端口，并注册以下的事件。 连接事件：当收到客户端的连接事件时，会调用 void connected(Channel channel) 方法。当可写事件触发时，会调用 void sent(Channel channel, Object message)，服务端向客户端返回响应数据。当可读事件触发时，会调用 void received(Channel channel, Object message) ，服务端在收到客户端的请求数据。当发生异常时，会调用 void caught(Channel channel, Throwable exception)。 当事件触发之后，服务端在这些函数中的逻辑，可以选择直接在这个函数里面进行操作，还是将请求分发到线程池去处理。一般异步的数据读写都需要另外的线程池参与，在线程池中会调用真正的服务端业务代码逻辑，返回结果。Hessian2 是 Dubbo 默认的 RPC 序列化方式，当然还有其他选择。例如，Dubbox 从 Spark 那里借鉴 Kryo，实现高性能的序列化。到这里，我们说了数据中心里面的相互调用。为了高性能，大家都愿意用二进制，但是为什么后期 Spring Cloud 又兴起了呢？这是因为，并发量越来越大，已经到了微服务的阶段。同原来的 SOA 不同，微服务粒度更细，模块之间的关系更加复杂。在上面的架构中，如果使用二进制的方式进行序列化，虽然不用协议文件来生成 Stub，但是对于接口的定义，以及传的对象 DTO，还是需要共享 JAR。因为只有客户端和服务端都有这个 JAR，才能成功地序列化和反序列化。但当关系复杂的时候，JAR 的依赖也变得异常复杂，难以维护，而且如果在 DTO 里加一个字段，双方的 JAR 没有匹配好，也会导致序列化不成功，而且还有可能循环依赖。这个时候，一般有两种选择。 第一种，建立严格的项目管理流程。不允许循环调用，不允许跨层调用，只准上层调用下层，不允许下层调用上层。接口要保持兼容性，不兼容的接口新添加而非改原来的，当接口通过监控，发现不用的时候，再下掉。升级的时候，先升级服务提供端，再升级服务消费端。 第二种，改用 RESTful 的方式。使用 Spring Cloud，消费端和提供端不用共享 JAR，各声明各的，只要能变成 JSON 就行，而且 JSON 也是比较灵活的。使用 RESTful 的方式，性能会降低，所以需要通过横向扩展来抵消单机的性能损耗。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:45:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这节就到这里了，我们来总结一下。RESTful API 对于接入层和 Controller 层之外的调用，已基本形成事实标准，但是随着内部服务之间的调用越来越多，性能也越来越重要，于是 Dubbo 的 RPC 框架有了用武之地。Dubbo 通过注册中心解决服务发现问题，通过 Hessian2 序列化解决协议约定的问题，通过 Netty 解决网络传输的问题。在更加复杂的微服务场景下，Spring Cloud 的 RESTful 方式在内部调用也会被考虑，主要是 JAR 包的依赖和管理问题。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:45:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第36讲 | 跨语言类RPC协议：交流之前，双方先来个专业术语表 到目前为止，咱们讲了四种 RPC，分别是 ONC RPC、基于 XML 的 SOAP、基于 JSON 的 RESTful 和 Hessian2。通过学习，我们知道，二进制的传输性能好，文本类的传输性能差一些；二进制的难以跨语言，文本类的可以跨语言；要写协议文件的严谨一些，不写协议文件的灵活一些。虽然都有服务发现机制，有的可以进行服务治理，有的则没有。我们也看到了 RPC 从最初的客户端服务器模式，最终演进到微服务。对于 RPC 框架的要求越来越多了，具体有哪些要求呢？ 首先，传输性能很重要。因为服务之间的调用如此频繁了，还是二进制的越快越好。其次，跨语言很重要。因为服务多了，什么语言写成的都有，而且不同的场景适宜用不同的语言，不能一个语言走到底。最好既严谨又灵活，添加个字段不用重新编译和发布程序。最好既有服务发现，也有服务治理，就像 Dubbo 和 Spring Cloud 一样。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:46:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"Protocol Buffers 这是要多快好省地建设社会主义啊。理想还是要有的嘛，这里我就来介绍一个向“理想”迈进的 GRPC。GRPC 首先满足二进制和跨语言这两条，二进制说明压缩效率高，跨语言说明更灵活。但是又是二进制，又是跨语言，这就相当于两个人沟通，你不但说方言，还说缩略语，人家怎么听懂呢？所以，最好双方弄一个协议约定文件，里面规定好双方沟通的专业术语，这样沟通就顺畅多了。对于 GRPC 来讲，二进制序列化协议是 Protocol Buffers。首先，需要定义一个协议文件.proto。我们还看买极客时间专栏的这个例子。 syntax = “proto3”; package com.geektime.grpc option java_package = “com.geektime.grpc”; message Order { required string date = 1; required string classname = 2; required string author = 3; required int price = 4; } message OrderResponse { required string message = 1; } service PurchaseOrder { rpc Purchase (Order) returns (OrderResponse) {} } 在这个协议文件中，我们首先指定使用 proto3 的语法，然后我们使用 Protocol Buffers 的语法，定义两个消息的类型，一个是发出去的参数，一个是返回的结果。里面的每一个字段，例如 date、classname、author、price 都有唯一的一个数字标识，这样在压缩的时候，就不用传输字段名称了，只传输这个数字标识就行了，能节省很多空间。最后定义一个 Service，里面会有一个 RPC 调用的声明。无论使用什么语言，都有相应的工具生成客户端和服务端的 Stub 程序，这样客户端就可以像调用本地一样，调用远程的服务了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:46:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"协议约定问题 Protocol Buffers 是一款压缩效率极高的序列化协议，有很多设计精巧的序列化方法。对于 int 类型 32 位的，一般都需要 4 个 Byte 进行存储。在 Protocol Buffers 中，使用的是变长整数的形式。对于每一个 Byte 的 8 位，最高位都有特殊的含义。如果该位为 1，表示这个数字没完，后续的 Byte 也属于这个数字；如果该位为 0，则这个数字到此结束。其他的 7 个 Bit 才是用来表示数字的内容。因此，小于 128 的数字都可以用一个 Byte 表示；大于 128 的数字，比如 130，会用两个字节来表示。对于每一个字段，使用的是 TLV（Tag，Length，Value）的存储办法。其中 Tag = (field_num « 3) | wire_type。field_num 就是在 proto 文件中，给每个字段指定唯一的数字标识，而 wire_type 用于标识后面的数据类型。 例如，对于 string author = 3，在这里 field_num 为 3，string 的 wire_type 为 2，于是 (field_num « 3) | wire_type = (11000) | 10 = 11010 = 26；接下来是 Length，最后是 Value 为“liuchao”，如果使用 UTF-8 编码，长度为 7 个字符，因而 Length 为 7。可见，在序列化效率方面，Protocol Buffers 简直做到了极致。在灵活性方面，这种基于协议文件的二进制压缩协议往往存在更新不方便的问题。例如，客户端和服务器因为需求的改变需要添加或者删除字段。这一点上，Protocol Buffers 考虑了兼容性。在上面的协议文件中，每一个字段都有修饰符。比如： required：这个值不能为空，一定要有这么一个字段出现；optional：可选字段，可以设置，也可以不设置，如果不设置，则使用默认值；repeated：可以重复 0 到多次。 如果我们想修改协议文件，对于赋给某个标签的数字，例如 string author=3，这个就不要改变了，改变了就不认了；也不要添加或者删除 required 字段，因为解析的时候，发现没有这个字段就会报错。对于 optional 和 repeated 字段，可以删除，也可以添加。这就给了客户端和服务端升级的可能性。例如，我们在协议里面新增一个 string recommended 字段，表示这个课程是谁推荐的，就将这个字段设置为 optional。我们可以先升级服务端，当客户端发过来消息的时候，是没有这个值的，将它设置为一个默认值。我们也可以先升级客户端，当客户端发过来消息的时候，是有这个值的，那它将被服务端忽略。至此，我们解决了协议约定的问题。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:46:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"网络传输问题 接下来，我们来看网络传输的问题。如果是 Java 技术栈，GRPC 的客户端和服务器之间通过 Netty Channel 作为数据通道，每个请求都被封装成 HTTP 2.0 的 Stream。Netty 是一个高效的基于异步 IO 的网络传输框架，这个上一节我们已经介绍过了。HTTP 2.0 在第 14 讲，我们也介绍过。HTTP 2.0 协议将一个 TCP 的连接，切分成多个流，每个流都有自己的 ID，而且流是有优先级的。流可以是客户端发往服务端，也可以是服务端发往客户端。它其实只是一个虚拟的通道。HTTP 2.0 还将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。通过这两种机制，HTTP 2.0 的客户端可以将多个请求分到不同的流中，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据每个帧首部的流标识符重新组装，并且可以根据优先级，决定优先处理哪个流的数据。 由于基于 HTTP 2.0，GRPC 和其他的 RPC 不同，可以定义四种服务方法。第一种，也是最常用的方式是单向 RPC，即客户端发送一个请求给服务端，从服务端获取一个应答，就像一次普通的函数调用。 rpc SayHello(HelloRequest) returns (HelloResponse){} 第二种方式是服务端流式 RPC，即服务端返回的不是一个结果，而是一批。客户端发送一个请求给服务端，可获取一个数据流用来读取一系列消息。客户端从返回的数据流里一直读取，直到没有更多消息为止。 rpc LotsOfReplies(HelloRequest) returns (stream HelloResponse){} 第三种方式为客户端流式 RPC，也即客户端的请求不是一个，而是一批。客户端用提供的一个数据流写入并发送一系列消息给服务端。一旦客户端完成消息写入，就等待服务端读取这些消息并返回应答。 rpc LotsOfGreetings(stream HelloRequest) returns (HelloResponse) {} 第四种方式为双向流式 RPC，即两边都可以分别通过一个读写数据流来发送一系列消息。这两个数据流操作是相互独立的，所以客户端和服务端能按其希望的任意顺序读写，服务端可以在写应答前等待所有的客户端消息，或者它可以先读一个消息再写一个消息，或者读写相结合的其他方式。每个数据流里消息的顺序会被保持。 rpc BidiHello(stream HelloRequest) returns (stream HelloResponse){} 如果基于 HTTP 2.0，客户端和服务器之间的交互方式要丰富得多，不仅可以单方向远程调用，还可以实现当服务端状态改变的时候，主动通知客户端。至此，传输问题得到了解决。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:46:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"服务发现与治理问题 最后是服务发现与服务治理的问题。GRPC 本身没有提供服务发现的机制，需要借助其他的组件，发现要访问的服务端，在多个服务端之间进行容错和负载均衡。其实负载均衡本身比较简单，LVS、HAProxy、Nginx 都可以做，关键问题是如何发现服务端，并根据服务端的变化，动态修改负载均衡器的配置。在这里我们介绍一种对于 GRPC 支持比较好的负载均衡器 Envoy。其实 Envoy 不仅仅是负载均衡器，它还是一个高性能的 C++ 写的 Proxy 转发器，可以配置非常灵活的转发规则。这些规则可以是静态的，放在配置文件中的，在启动的时候加载。要想重新加载，一般需要重新启动，但是 Envoy 支持热加载和热重启，这在一定程度上缓解了这个问题。 当然，最好的方式是将规则设置为动态的，放在统一的地方维护。这个统一的地方在 Envoy 眼中被称为服务发现（Discovery Service），过一段时间去这里拿一下配置，就修改了转发策略。无论是静态的，还是动态的，在配置里面往往会配置四个东西。 第一个是 listener。Envoy 既然是 Proxy，专门做转发，就得监听一个端口，接入请求，然后才能够根据策略转发，这个监听的端口就称为 listener。第二个是 endpoint，是目标的 IP 地址和端口。这个是 Proxy 最终将请求转发到的地方。第三个是 cluster。一个 cluster 是具有完全相同行为的多个 endpoint，也即如果有三个服务端在运行，就会有三个 IP 和端口，但是部署的是完全相同的三个服务，它们组成一个 cluster，从 cluster 到 endpoint 的过程称为负载均衡，可以轮询。第四个是 route。有时候多个 cluster 具有类似的功能，但是是不同的版本号，可以通过 route 规则，选择将请求路由到某一个版本号，也即某一个 cluster。 如果是静态的，则将后端的服务端的 IP 地址拿到，然后放在配置文件里面就可以了。如果是动态的，就需要配置一个服务发现中心，这个服务发现中心要实现 Envoy 的 API，Envoy 可以主动去服务发现中心拉取转发策略。 看来，Envoy 进程和服务发现中心之间要经常相互通信，互相推送数据，所以 Envoy 在控制面和服务发现中心沟通的时候，就可以使用 GRPC，也就天然具备在用户面支撑 GRPC 的能力。Envoy 如果复杂的配置，都能干什么事呢？一种常见的规则是配置路由策略。例如后端的服务有两个版本，可以通过配置 Envoy 的 route，来设置两个版本之间，也即两个 cluster 之间的 route 规则，一个占 99% 的流量，一个占 1% 的流量。另一种常见的规则就是负载均衡策略。对于一个 cluster 下的多个 endpoint，可以配置负载均衡机制和健康检查机制，当服务端新增了一个，或者挂了一个，都能够及时配置 Envoy，进行负载均衡。 所有这些节点的变化都会上传到注册中心，所有这些策略都可以通过注册中心进行下发，所以，更严格的意义上讲，注册中心可以称为注册治理中心。Envoy 这么牛，是不是能够将服务之间的相互调用全部由它代理？如果这样，服务也不用像 Dubbo，或者 Spring Cloud 一样，自己感知到注册中心，自己注册，自己治理，对应用干预比较大。如果我们的应用能够意识不到服务治理的存在，就可以直接进行 GRPC 的调用。这就是未来服务治理的趋势 Serivce Mesh，也即应用之间的相互调用全部由 Envoy 进行代理，服务之间的治理也被 Envoy 进行代理，完全将服务治理抽象出来，到平台层解决。 至此 RPC 框架中有治理功能的 Dubbo、Spring Cloud、Service Mesh 就聚齐了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:46:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"小结 好了，这一节就到这里了，我们来总结一下。GRPC 是一种二进制，性能好，跨语言，还灵活，同时可以进行服务治理的多快好省的 RPC 框架，唯一不足就是还是要写协议文件。GRPC 序列化使用 Protocol Buffers，网络传输使用 HTTP 2.0，服务治理可以使用基于 Envoy 的 Service Mesh。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:46:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"网络协议串讲 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:47:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第37讲 | 知识串讲：用双十一的故事串起碎片的网络协议（上） 基本的网络知识我们都讲完了，还记得最初举的那个“双十一”下单的例子吗？这一节开始，我们详细地讲解这个过程，用这个过程串起我们讲过的网络协议。我把这个过程分为十个阶段，从云平台中搭建一个电商开始，到 BGP 路由广播，再到 DNS 域名解析，从客户看商品图片，到最终下单的整个过程，每一步我都会详细讲解。这节我们先来看前三个阶段。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:48:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"1. 部署一个高可用高并发的电商平台 首先，咱们要有个电商平台。假设我们已经有了一个特别大的电商平台，这个平台应该部署在哪里呢？假设我们用公有云，一般公有云会有多个位置，比如在华东、华北、华南都有。毕竟咱们的电商是要服务全国的，当然到处都要部署了。我们把主站点放在华东。 为了每个点都能“雨露均沾”，也为了高可用性，往往需要有多个机房，形成多个可用区（Available Zone）。由于咱们的应用是分布在两个可用区的，所以假如任何一个可用区挂了，都不会受影响。我们来回想数据中心那一节21讲，每个可用区里有一片一片的机柜，每个机柜上有一排一排的服务器，每个机柜都有一个接入交换机，有一个汇聚交换机将多个机柜连在一起。这些服务器里面部署的都是计算节点，每台上面都有 Open vSwitch 创建的虚拟交换机，将来在这台机器上创建的虚拟机，都会连到 Open vSwitch 上。 接下来，你在云计算的界面上创建一个 VPC（Virtual Private Cloud，虚拟私有网络），指定一个 IP 段，这样以后你部署的所有应用都会在这个虚拟网络里，使用你分配的这个 IP 段。为了不同的 VPC 相互隔离，每个 VPC 都会被分配一个 VXLAN 的 ID。尽管不同用户的虚拟机有可能在同一个物理机上，但是不同的 VPC 二层压根儿是不通的。由于有两个可用区，在这个 VPC 里面，要为每一个可用区分配一个 Subnet，也就是在大的网段里分配两个小的网段。当两个可用区里面网段不同的时候，就可以配置路由策略，访问另外一个可用区，走某一条路由了。接下来，应该创建数据库持久化层。大部分云平台都会提供 PaaS 服务，也就是说，不需要你自己搭建数据库，而是采用直接提供数据库的服务，并且单机房的主备切换都是默认做好的，数据库也是部署在虚拟机里面的，只不过从界面上，你看不到数据库所在的虚拟机而已。云平台会给每个 Subnet 的数据库实例分配一个域名。创建数据库实例的时候，需要你指定可用区和 Subnet，这样创建出来的数据库实例可以通过这个 Subnet 的私网 IP 进行访问。为了分库分表实现高并发的读写，在创建的多个数据库实例之上，会创建一个分布式数据库的实例，也需要指定可用区和 Subnet，还会为分布式数据库分配一个私网 IP 和域名。对于数据库这种高可用性比较高的，需要进行跨机房高可用，因而两个可用区都要部署一套，但是只有一个是主，另外一个是备，云平台往往会提供数据库同步工具，将应用写入主的数据同步给备数据库集群。 接下来是创建缓存集群。云平台也会提供 PaaS 服务，也需要每个可用区和 Subnet 创建一套，缓存的数据在内存中，由于读写性能要求高，一般不要求跨可用区读写。再往上层就是部署咱们自己写的程序了。基础服务层、组合服务层、Controller 层，以及 Nginx 层、API 网关等等，这些都是部署在虚拟机里面的。它们之间通过 RPC 相互调用，需要到注册中心进行注册。它们之间的网络通信是虚拟机和虚拟机之间的。如果是同一台物理机，则那台物理机上的 OVS 就能转发过去；如果是不同的物理机，这台物理机的 OVS 和另一台物理机的 OVS 中间有一个 VXLAN 的隧道，将请求转发过去。再往外就是负载均衡了，负载均衡也是云平台提供的 PaaS 服务，也是属于某个 VPC 的，部署在虚拟机里面的，但是负载均衡有个外网的 IP，这个外网的 IP 地址就是在网关节点的外网网口上的。在网关节点上，会有 NAT 规则，将外网 IP 地址转换为 VPC 里面的私网 IP 地址，通过这些私网 IP 地址访问到虚拟机上的负载均衡节点，然后通过负载均衡节点转发到 API 网关的节点。网关节点的外网网口是带公网 IP 地址的，里面有一个虚拟网关转发模块，还会有一个 OVS，将私网 IP 地址放到 VXLAN 隧道里面，转发到虚拟机上，从而实现外网和虚拟机网络之间的互通。 不同的可用区之间，通过核心交换机连在一起，核心交换机之外是边界路由器。在华北、华东、华南同样也部署了一整套，每个地区都创建了 VPC，这就需要有一种机制将 VPC 连接到一起。云平台一般会提供硬件的 VPC 互连的方式，当然也可以使用软件互连的方式，也就是使用 VPN 网关，通过 IPsec VPN 将不同地区的不同 VPC 通过 VPN 连接起来。对于不同地区和不同运营商的用户，我们希望他能够就近访问到网站，而且当一个点出了故障之后，我们希望能够在不同的地区之间切换，这就需要有智能 DNS，这个也是云平台提供的。对于一些静态资源，可以保持在对象存储里面，通过 CDN 下发到边缘节点，这样客户端就能尽快加载出来。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:48:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"2. 大声告诉全世界，可以到我这里买东西 当电商应用搭建完毕之后，接下来需要将如何访问到这个电商网站广播给全网。刚才那张图画的是一个可用区的情况，对于多个可用区的情况，我们可以隐去计算节点的情况，将外网访问区域放大。 外网 IP 是放在虚拟网关的外网网口上的，这个 IP 如何让全世界知道呢？当然是通过 BGP 路由协议了。每个可用区都有自己的汇聚交换机，如果机器数目比较多，可以直接用核心交换机，每个 Region 也有自己的核心交换区域。在核心交换外面是安全设备，然后就是边界路由器。边界路由器会和多个运营商连接，从而每个运营商都能够访问到这个网站。边界路由器可以通过 BGP 协议，将自己数据中心里面的外网 IP 向外广播，也就是告诉全世界，如果要访问这些外网 IP，都来我这里。每个运营商也有很多的路由器、很多的点，于是就可以将如何到达这些 IP 地址的路由信息，广播到全国乃至全世界。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:48:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"3. 打开手机来上网，域名解析得地址 这个时候，不但你的这个网站的 IP 地址全世界都知道了，你打的广告可能大家也都看到了，于是有客户下载 App 来买东西了。 客户的手机开机以后，在附近寻找基站 eNodeB，发送请求，申请上网。基站将请求发给 MME，MME 对手机进行认证和鉴权，还会请求 HSS 看有没有钱，看看是在哪里上网。当 MME 通过了手机的认证之后，开始建立隧道，建设的数据通路分两段路，其实是两个隧道。一段是从 eNodeB 到 SGW，第二段是从 SGW 到 PGW，在 PGW 之外，就是互联网。PGW 会为手机分配一个 IP 地址，手机上网都是带着这个 IP 地址的。当在手机上面打开一个 App 的时候，首先要做的事情就是解析这个网站的域名。在手机运营商所在的互联网区域里，有一个本地的 DNS，手机会向这个 DNS 请求解析 DNS。当这个 DNS 本地有缓存，则直接返回；如果没有缓存，本地 DNS 才需要递归地从根 DNS 服务器，查到.com 的顶级域名服务器，最终查到权威 DNS 服务器。如果你使用云平台的时候，配置了智能 DNS 和全局负载均衡，在权威 DNS 服务中，一般是通过配置 CNAME 的方式，我们可以起一个别名，例如 vip.yourcomany.com ，然后告诉本地 DNS 服务器，让它请求 GSLB 解析这个域名，GSLB 就可以在解析这个域名的过程中，通过自己的策略实现负载均衡。 GSLB 通过查看请求它的本地 DNS 服务器所在的运营商和地址，就知道用户所在的运营商和地址，然后将距离用户位置比较近的 Region 里面，三个负载均衡 SLB 的公网 IP 地址，返回给本地 DNS 服务器。本地 DNS 解析器将结果缓存后，返回给客户端。对于手机 App 来说，可以绕过刚才的传统 DNS 解析机制，直接只要 HTTPDNS 服务，通过直接调用 HTTPDNS 服务器，得到这三个 SLB 的公网 IP 地址。看，经过了如此复杂的过程，咱们的万里长征还没迈出第一步，刚刚得到 IP 地址，包还没发呢？话说手机 App 拿到了公网 IP 地址，接下来应该做什么呢？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:48:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第38讲 | 知识串讲：用双十一的故事串起碎片的网络协议（中） 上一节我们讲到，手机 App 经过了一个复杂的过程，终于拿到了电商网站的 SLB 的 IP 地址，是不是该下单了？别忙，俗话说的好，买东西要货比三家。大部分客户在购物之前要看很多商品图片，比来比去，最后好不容易才下决心，点了下单按钮。下单按钮一按，就要开始建立连接。建立连接这个过程也挺复杂的，最终还要经过层层封装，才构建出一个完整的网络包。今天我们就来看这个过程。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:49:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"4. 购物之前看图片，静态资源 CDN 客户想要在购物网站买一件东西的时候，一般是先去详情页看看图片，是不是想买的那一款。 我们部署电商应用的时候，一般会把静态资源保存在两个地方，一个是接入层 nginx 后面的 varnish 缓存里面，一般是静态页面；对于比较大的、不经常更新的静态图片，会保存在对象存储里面。这两个地方的静态资源都会配置 CDN，将资源下发到边缘节点。配置了 CDN 之后，权威 DNS 服务器上，会为静态资源设置一个 CNAME 别名，指向另外一个域名 cdn.com ，返回给本地 DNS 服务器。当本地 DNS 服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的时候就不是原来的权威 DNS 服务器了，而是 cdn.com 的权威 DNS 服务器。这是 CDN 自己的权威 DNS 服务器。在这个服务器上，还是会设置一个 CNAME，指向另外一个域名，也即 CDN 网络的全局负载均衡器。本地 DNS 服务器去请求 CDN 的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，将 IP 返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器，将内容拉到本地。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:49:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"5. 看上宝贝点下单，双方开始建连接 当你浏览了很多图片，发现实在喜欢某个商品，于是决定下单购买。电商网站会对下单的情况提供 RESTful 的下单接口，而对于下单这种需要保密的操作，需要通过 HTTPS 协议进行请求。在所有这些操作之前，首先要做的事情是建立连接。 HTTPS 协议是基于 TCP 协议的，因而要先建立 TCP 的连接。在这个例子中，TCP 的连接是在手机上的 App 和负载均衡器 SLB 之间的。尽管中间要经过很多的路由器和交换机，但是 TCP 的连接是端到端的。TCP 这一层和更上层的 HTTPS 无法看到中间的包的过程。尽管建立连接的时候，所有的包都逃不过在这些路由器和交换机之间的转发，转发的细节我们放到那个下单请求的发送过程中详细解读，这里只看端到端的行为。对于 TCP 连接来讲，需要通过三次握手建立连接，为了维护这个连接，双方都需要在 TCP 层维护一个连接的状态机。一开始，客户端和服务端都处于 CLOSED 状态。服务端先是主动监听某个端口，处于 LISTEN 状态。然后客户端主动发起连接 SYN，之后处于 SYN-SENT 状态。服务端收到发起的连接，返回 SYN，并且 ACK 客户端的 SYN，之后处于 SYN-RCVD 状态。客户端收到服务端发送的 SYN 和 ACK 之后，发送 ACK 的 ACK，之后处于 ESTABLISHED 状态。这是因为，它一发一收成功了。服务端收到 ACK 的 ACK 之后，也会处于 ESTABLISHED 状态，因为它的一发一收也成功了。 当 TCP 层的连接建立完毕之后，接下来轮到 HTTPS 层建立连接了，在 HTTPS 的交换过程中，TCP 层始终处于 ESTABLISHED。对于 HTTPS，客户端会发送 Client Hello 消息到服务器，用明文传输 TLS 版本信息、加密套件候选列表、压缩算法候选列表等信息。另外，还会有一个随机数，在协商对称密钥的时候使用。然后，服务器会返回 Server Hello 消息，告诉客户端，服务器选择使用的协议版本、加密套件、压缩算法等。这也有一个随机数，用于后续的密钥协商。然后，服务器会给你一个服务器端的证书，然后说：“Server Hello Done，我这里就这些信息了。” 客户端当然不相信这个证书，于是从自己信任的 CA 仓库中，拿 CA 的证书里面的公钥去解密电商网站的证书。如果能够成功，则说明电商网站是可信的。这个过程中，你可能会不断往上追溯 CA、CA 的 CA、CA 的 CA 的 CA，反正直到一个授信的 CA，就可以了。证书验证完毕之后，觉得这个服务端是可信的，于是客户端计算产生随机数字 Pre-master，发送 Client Key Exchange，用证书中的公钥加密，再发送给服务器，服务器可以通过私钥解密出来。接下来，无论是客户端还是服务器，都有了三个随机数，分别是：自己的、对端的，以及刚生成的 Pre-Master 随机数。通过这三个随机数，可以在客户端和服务器产生相同的对称密钥。有了对称密钥，客户端就可以说：“Change Cipher Spec，咱们以后都采用协商的通信密钥和加密算法进行加密通信了。”然后客户端发送一个 Encrypted Handshake Message，将已经商定好的参数等，采用协商密钥进行加密，发送给服务器用于数据与握手验证。 同样，服务器也可以发送 Change Cipher Spec，说：“没问题，咱们以后都采用协商的通信密钥和加密算法进行加密通信了”，并且也发送 Encrypted Handshake Message 的消息试试。当双方握手结束之后，就可以通过对称密钥进行加密传输了。真正的下单请求封装成网络包的发送过程，我们先放一放，我们来接着讲这个网络包的故事。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:49:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"6. 发送下单请求网络包，西行需要出网关 当客户端和服务端之间建立了连接后，接下来就要发送下单请求的网络包了。在用户层发送的是 HTTP 的网络包，因为服务端提供的是 RESTful API，因而 HTTP 层发送的就是一个请求。 POST /purchaseOrder HTTP/1.1 Host: www.geektime.com Content-Type: application/json; charset=utf-8 Content-Length: nnn { \"order\": { \"date\": \"2018-07-01\", \"className\": \"趣谈网络协议\", \"Author\": \"刘超\", \"price\": \"68\" } } HTTP 的报文大概分为三大部分。第一部分是请求行，第二部分是请求的首部，第三部分才是请求的正文实体。在请求行中，URL 就是 www.geektime.com/purchaseOrder ，版本为 HTTP 1.1。请求的类型叫作 POST，它需要主动告诉服务端一些信息，而非获取。需要告诉服务端什么呢？一般会放在正文里面。正文可以有各种各样的格式，常见的格式是 JSON。请求行下面就是我们的首部字段。首部是 key value，通过冒号分隔。Content-Type 是指正文的格式。例如，我们进行 POST 的请求，如果正文是 JSON，那么我们就应该将这个值设置为 JSON。接下来是正文，这里是一个 JSON 字符串，里面通过文本的形式描述了，要买一个课程，作者是谁，多少钱。 这样，HTTP 请求的报文格式就拼凑好了。接下来浏览器或者移动 App 会把它交给下一层传输层。怎么交给传输层呢？也是用 Socket 进行程序设计。如果用的是浏览器，这些程序不需要你自己写，有人已经帮你写好了；如果在移动 APP 里面，一般会用一个 HTTP 的客户端工具来发送，并且帮你封装好。HTTP 协议是基于 TCP 协议的，所以它使用面向连接的方式发送请求，通过 Stream 二进制流的方式传给对方。当然，到了 TCP 层，它会把二进制流变成一个个报文段发送给服务器。在 TCP 头里面，会有源端口号和目标端口号，目标端口号一般是服务端监听的端口号，源端口号在手机端，往往是随机分配一个端口号。这个端口号在客户端和服务端用于区分请求和返回，发给那个应用。在 IP 头里面，都需要加上自己的地址（即源地址）和它想要去的地方（即目标地址）。当一个手机上线的时候，PGW 会给这个手机分配一个 IP 地址，这就是源地址，而目标地址则是云平台的负载均衡器的外网 IP 地址。在 IP 层，客户端需要查看目标地址和自己是否是在同一个局域网，计算是否是同一个网段，往往需要通过 CIDR 子网掩码来计算。 对于这个下单场景，目标 IP 和源 IP 不会在同一个网段，因而需要发送到默认的网关。一般通过 DHCP 分配 IP 地址的时候，同时配置默认网关的 IP 地址。但是客户端不会直接使用默认网关的 IP 地址，而是发送 ARP 协议，来获取网关的 MAC 地址，然后将网关 MAC 作为目标 MAC，自己的 MAC 作为源 MAC，放入 MAC 头，发送出去。一个完整的网络包的格式是这样的。 真不容易啊，本来以为上篇就发送下单包了，结果到中篇这个包还没发送出去，只是封装了一个如此长的网络包。别着急，你可以自己先预想一下，接下来该做什么了？ ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:49:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第39讲 | 知识串讲：用双十一的故事串起碎片的网络协议（下） 发送的时候可以说是重重关隘，从手机到移动网络、互联网，还要经过多个运营商才能到达数据中心，到了数据中心就进入第二个复杂的过程，从网关到 VXLAN 隧道，到负载均衡，到 Controller 层、组合服务层、基础服务层，最终才下单入库。今天，我们就来看这最后一段过程。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:50:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"7. 一座座城池一道道关，流控拥塞与重传 网络包已经组合完毕，接下来我们来看，如何经过一道道城关，到达目标公网 IP。对于手机来讲，默认的网关在 PGW 上。在移动网络里面，从手机到 SGW，到 PGW 是有一条隧道的。在这条隧道里面，会将上面的这个包作为隧道的乘客协议放在里面，外面 SGW 和 PGW 在核心网机房的 IP 地址。网络包直到 PGW（PGW 是隧道的另一端）才将里面的包解出来，转发到外部网络。所以，从手机发送出来的时候，网络包的结构为： 源 MAC：手机也即 UE 的 MAC；目标 MAC：网关 PGW 上面的隧道端点的 MAC；源 IP：UE 的 IP 地址；目标 IP：SLB 的公网 IP 地址。 进入隧道之后，要封装外层的网络地址，因而网络包的格式为：外层源 MAC：E-NodeB 的 MAC；外层目标 MAC：SGW 的 MAC；外层源 IP：E-NodeB 的 IP；外层目标 IP：SGW 的 IP；内层源 MAC：手机也即 UE 的 MAC；内层目标 MAC：网关 PGW 上面的隧道端点的 MAC；内层源 IP：UE 的 IP 地址；内层目标 IP：SLB 的公网 IP 地址。 当隧道在 SGW 的时候，切换了一个隧道，会从 SGW 到 PGW 的隧道，因而网络包的格式为：外层源 MAC：SGW 的 MAC；外层目标 MAC：PGW 的 MAC；外层源 IP：SGW 的 IP；外层目标 IP：PGW 的 IP；内层源 MAC：手机也即 UE 的 MAC；内层目标 MAC：网关 PGW 上面的隧道端点的 MAC；内层源 IP：UE 的 IP 地址；内层目标 IP：SLB 的公网 IP 地址。 在 PGW 的隧道端点将包解出来，转发出去的时候，一般在 PGW 出外部网络的路由器上，会部署 NAT 服务，将手机的 IP 地址转换为公网 IP 地址，当请求返回的时候，再 NAT 回来。因而在 PGW 之后，相当于做了一次欧洲十国游型（8讲）的转发，网络包的格式为： 源 MAC：PGW 出口的 MAC；目标 MAC：NAT 网关的 MAC；源 IP：UE 的 IP 地址；目标 IP：SLB 的公网 IP 地址。 在 NAT 网关，相当于做了一次玄奘西游型的转发，网络包的格式变成： 源 MAC：NAT 网关的 MAC；目标 MAC：A2 路由器的 MAC；源 IP：UE 的公网 IP 地址；目标 IP：SLB 的公网 IP 地址。 出了 NAT 网关，就从核心网到达了互联网。在网络世界，每一个运营商的网络成为自治系统 AS。每个自治系统都有边界路由器，通过它和外面的世界建立联系。对于云平台来讲，它可以被称为 Multihomed AS，有多个连接连到其他的 AS，但是大多拒绝帮其他的 AS 传输包。例如一些大公司的网络。对于运营商来说，它可以被称为 Transit AS，有多个连接连到其他的 AS，并且可以帮助其他的 AS 传输包，比如主干网。如何从出口的运营商到达云平台的边界路由器？在路由器之间需要通过 BGP 协议实现，BGP 又分为两类，eBGP 和 iBGP。自治系统之间、边界路由器之间使用 eBGP 广播路由。内部网络也需要访问其他的自治系统。边界路由器如何将 BGP 学习到的路由导入到内部网络呢？通过运行 iBGP，使内部的路由器能够找到到达外网目的地最好的边界路由器。网站的 SLB 的公网 IP 地址早已经通过云平台的边界路由器，让全网都知道了。于是这个下单的网络包选择的下一跳是 A2，也即将 A2 的 MAC 地址放在目标 MAC 地址中。到达 A2 之后，从路由表中找到下一跳是路由器 C1，于是将目标 MAC 换成 C1 的 MAC 地址。到达 C1 之后，找到下一跳是 C2，将目标 MAC 地址设置为 C2 的 MAC。到达 C2 后，找到下一跳是云平台的边界路由器，于是将目标 MAC 设置为边界路由器的 MAC 地址。 你会发现，这一路，都是只换 MAC，不换目标 IP 地址。这就是所谓下一跳的概念。在云平台的边界路由器，会将下单的包转发进来，经过核心交换，汇聚交换，到达外网网关节点上的 SLB 的公网 IP 地址。我们可以看到，手机到 SLB 的公网 IP，是一个端到端的连接，连接的过程发送了很多包。所有这些包，无论是 TCP 三次握手，还是 HTTPS 的密钥交换，都是要走如此复杂的过程到达 SLB 的，当然每个包走的路径不一定一致。网络包走在这个复杂的道路上，很可能一不小心就丢了，怎么办？这就需要借助 TCP 的机制重新发送。既然 TCP 要对包进行重传，就需要维护 Sequence Number，看哪些包到了，哪些没到，哪些需要重传，传输的速度应该控制到多少，这就是 TCP 的滑动窗口协议。 整个 TCP 的发送，一开始会协商一个 Sequence Number，从这个 Sequence Number 开始，每个包都有编号。滑动窗口将接收方的网络包分成四个部分：已经接收，已经 ACK，已经交给应用层的包；已经接收，已经 ACK，未发送给应用层；已经接收，尚未发送 ACK；未接收，尚有空闲的缓存区域。 对于 TCP 层来讲，每一个包都有 ACK。ACK 需要从 SLB 回复到手机端，将上面的那个过程反向来一遍，当然路径不一定一致，可见 ACK 也不是那么轻松的事情。如果发送方超过一定的时间没有收到 ACK，就会重新发送。只有 TCP 层 ACK 过的包，才会发给应用层，并且只会发送一份，对于下单的场景，应用层是 HTTP 层。你可能会问了，TCP 老是重复发送，会不会导致一个单下了两遍？是否要求服务端实现幂等？从 TCP 的机制来看，是不会的。只有收不到 ACK 的包才会重复发，发到接收端，在窗口里面只保存一份，所以在同一个 TCP 连接中，不用担心重传导致二次下单。但是 TCP 连接会因为某种原因断了，例如手机信号不好，这个时候手机把所有的动作重新做一遍，建立一个新的 TCP 连接，在 HTTP 层调用两次 RESTful API。这个时候可能会导致两遍下单的情况，因而 RESTful API 需要实现幂等。当 ACK 过的包发给应用层之后，TCP 层的缓存就空了出来，这会导致上面图中的大三角，也即接收方能够容纳的总缓存，整体顺时针滑动。小的三角形，也即接收方告知发送方的窗口总大小，也即还没有完全确认收到的缓存大小，如果把这些填满了，就不能再发了，因为没确认收到，所以一个都不能扔。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:50:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"8. 从数据中心进网关，公网 NAT 成私网 包从手机端经历千难万险，终于到了 SLB 的公网 IP 所在的公网网口。由于匹配上了 MAC 地址和 IP 地址，因而将网络包收了进来。 在虚拟网关节点的外网网口上，会有一个 NAT 规则，将公网 IP 地址转换为 VPC 里面的私网 IP 地址，这个私网 IP 地址就是 SLB 的 HAProxy 所在的虚拟机的私网 IP 地址。当然为了承载比较大的吞吐量，虚拟网关节点会有多个，物理网络会将流量分发到不同的虚拟网关节点。同样 HAProxy 也会是一个大的集群，虚拟网关会选择某个负载均衡节点，将某个请求分发给它，负载均衡之后是 Controller 层，也是部署在虚拟机里面的。当网络包里面的目标 IP 变成私有 IP 地址之后，虚拟路由会查找路由规则，将网络包从下方的私网网口发出来。这个时候包的格式为： 源 MAC：网关 MAC；目标 MAC：HAProxy 虚拟机的 MAC；源 IP：UE 的公网 IP；目标 IP：HAProxy 虚拟机的私网 IP。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:50:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"9. 进入隧道打标签，RPC 远程调用下单 在虚拟路由节点上，也会有 OVS，将网络包封装在 VXLAN 隧道里面，VXLAN ID 就是给你的租户创建 VPC 的时候分配的。包的格式为： 外层源 MAC：网关物理机 MAC；外层目标 MAC：物理机 A 的 MAC；外层源 IP：网关物理机 IP；外层目标 IP：物理机 A 的 IP；内层源 MAC：网关 MAC；内层目标 MAC：HAProxy 虚拟机的 MAC；内层源 IP：UE 的公网 IP；内层目标 IP：HAProxy 虚拟机的私网 IP。 在物理机 A 上，OVS 会将包从 VXLAN 隧道里面解出来，发给 HAProxy 所在的虚拟机。HAProxy 所在的虚拟机发现 MAC 地址匹配，目标 IP 地址匹配，就根据 TCP 端口，将包发给 HAProxy 进程，因为 HAProxy 是在监听这个 TCP 端口的。因而 HAProxy 就是这个 TCP 连接的服务端，客户端是手机。对于 TCP 的连接状态、滑动窗口等，都是在 HAProxy 上维护的。在这里 HAProxy 是一个四层负载均衡，也即它只解析到 TCP 层，里面的 HTTP 协议它不关心，就将请求转发给后端的多个 Controller 层的一个。HAProxy 发出去的网络包就认为 HAProxy 是客户端了，看不到手机端了。网络包格式如下： 源 MAC：HAProxy 所在虚拟机的 MAC；目标 MAC：Controller 层所在虚拟机的 MAC；源 IP：HAProxy 所在虚拟机的私网 IP；目标 IP：Controller 层所在虚拟机的私网 IP。 当然这个包发出去之后，还是会被物理机上的 OVS 放入 VXLAN 隧道里面，网络包格式为： 外层源 MAC：物理机 A 的 MAC；外层目标 MAC：物理机 B 的 MAC；外层源 IP：物理机 A 的 IP；外层目标 IP：物理机 B 的 IP；内层源 MAC：HAProxy 所在虚拟机的 MAC；内层目标 MAC：Controller 层所在虚拟机的 MAC；内层源 IP：HAProxy 所在虚拟机的私网 IP；内层目标 IP：Controller 层所在虚拟机的私网 IP。 在物理机 B 上，OVS 会将包从 VXLAN 隧道里面解出来，发给 Controller 层所在的虚拟机。Controller 层所在的虚拟机发现 MAC 地址匹配，目标 IP 地址匹配，就根据 TCP 端口，将包发给 Controller 层的进程，因为它在监听这个 TCP 端口。在 HAProxy 和 Controller 层之间，维护一个 TCP 的连接。Controller 层收到包之后，它是关心 HTTP 里面是什么的，于是解开 HTTP 的包，发现是一个 POST 请求，内容是下单购买一个课程。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:50:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"10. 下单扣减库存优惠券，数据入库返回成功 下单是一个复杂的过程，因而往往在组合服务层会有一个专门管理下单的服务，Controller 层会通过 RPC 调用这个组合服务层。假设我们使用的是 Dubbo，则 Controller 层需要读取注册中心，将下单服务的进程列表拿出来，选出一个来调用。Dubbo 中默认的 RPC 协议是 Hessian2。Hessian2 将下单的远程调用序列化为二进制进行传输。 Netty 是一个非阻塞的基于事件的网络传输框架。Controller 层和下单服务之间，使用了 Netty 的网络传输框架。有了 Netty，就不用自己编写复杂的异步 Socket 程序了。Netty 使用的方式，就是咱们讲Socket 编程(13讲)的时候，一个项目组支撑多个项目（IO 多路复用，从派人盯着到有事通知）这种方式。Netty 还是工作在 Socket 这一层的，发送的网络包还是基于 TCP 的。在 TCP 的下层，还是需要封装上 IP 头和 MAC 头。如果跨物理机通信，还是需要封装的外层的 VXLAN 隧道里面。当然底层的这些封装，Netty 都不感知，它只要做好它的异步通信即可。 在 Netty 的服务端，也即下单服务中，收到请求后，先用 Hessian2 的格式进行解压缩。然后将请求分发到线程中进行处理，在线程中，会调用下单的业务逻辑。下单的业务逻辑比较复杂，往往要调用基础服务层里面的库存服务、优惠券服务等，将多个服务调用完毕，才算下单成功。下单服务调用库存服务和优惠券服务，也是通过 Dubbo 的框架，通过注册中心拿到库存服务和优惠券服务的列表，然后选一个调用。调用的时候，统一使用 Hessian2 进行序列化，使用 Netty 进行传输，底层如果跨物理机，仍然需要通过 VXLAN 的封装和解封装。 咱们以库存为例子的时候，讲述过幂等的接口实现的问题。因为如果扣减库存，仅仅是谁调用谁减一。这样存在的问题是，如果扣减库存因为一次调用失败，而多次调用，这里指的不是 TCP 多次重试，而是应用层调用的多次重试，就会存在库存扣减多次的情况。这里常用的方法是，使用乐观锁（Compare and Set，简称 CAS）。CAS 要考虑三个方面，当前的库存数、预期原来的库存数和版本，以及新的库存数。在操作之前，查询出原来的库存数和版本，真正扣减库存的时候，判断如果当前库存的值与预期原值和版本相匹配，则将库存值更新为新值，否则不做任何操作。这是一种基于状态而非基于动作的设计，符合 RESTful 的架构设计原则。这样的设计有利于高并发场景。当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 最终，当下单更新到分布式数据库中之后，整个下单过程才算真正告一段落。好了，经过了十个过程，下单终于成功了，你是否对这个过程了如指掌了呢？如果发现对哪些细节比较模糊，可以回去看一下相应的章节，相信会有更加深入的理解。到此，我带着你用下单过程把网络协议的知识都复习了一遍。授人以鱼不如授人以渔。下一节，我将会带你来搭建一个网络实验环境，配合实验来说明理论。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:50:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"第40讲 | 搭建一个网络实验环境：授人以鱼不如授人以渔 因为这门课是基础课程，而且配合音频的形式发布，所以我多以理论为主来进行讲解。在专栏更新的过程中，不断有同学让我推荐一些网络方面的书籍，还有同学说能不能配合一些实验来说明理论。的确，网络是一门实验性很强的学科，就像我在开篇词里面说的一样：一看觉得懂，一问就打鼓，一用就糊涂。 在写专栏的过程中，我自己也深深体会到了。这个时候，我常常会拿一个现实的环境，上手操作一下，抓个包看看，这样心里就会有定论。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:51:0","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"《TCP/IP 详解》实验环境搭建 对于网络方面的书籍，我当然首推 Rechard Stevens 的《TCP/IP illustrated》（《TCP/IP 详解》）。这本书把理论讲得深入浅出，还配有大量的上手实践和抓包，看到这些抓包，原来不理解的很多理论，一下子就能懂了。这本书里有个拓扑图，书上的很多实验都是基于这个图的，但是这个拓扑图还是挺复杂的。我这里先不说，一会儿详细讲。Rechard Stevens，因为工作中有这么一个环境，很方便做实验，最终才写出了这样一本书，而我们一般人学习网络，没有这个环境应该怎么办呢？时代不同了，咱们现在有更加强大的工具了。例如，这里这么多的机器，我们可以用 Docker 来实现，多个网络可以用 Open vSwitch 来实现。你甚至不需要一台物理机，只要一台 1 核 2G 的虚拟机，就能将这个环境搭建起来。搭建这个环境的时候，需要一些脚本。我把脚本都放在了Github里面，你可以自己取用。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:51:1","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"1. 创建一个 Ubuntu 虚拟机 在你的笔记本电脑上，用 VirtualBox 创建就行。1 核 2G，随便一台电脑都能搭建起来。首先，我们先下载一个 Ubuntu 的镜像。我是从Ubuntu 官方网站下载的。 然后，在 VirtualBox 里面安装 Ubuntu。安装过程网上一大堆教程，你可以自己去看，我这里就不详细说了。这里我需要说明的是网络的配置。对于这个虚拟机，我们创建两个网卡，一个是 Host-only，只有你的笔记本电脑上能够登录进去。这个网卡上的 IP 地址也只有在你的笔记本电脑上管用。这个网卡的配置比较稳定，用于在 SSH 上做操作。这样你的笔记本电脑就可以搬来搬去，在公司里安装一半，回家接着安装另一半都没问题。 这里有一个虚拟的网桥，这个网络可以在管理 \u003e 主机网络管理里面进行配置。 在这里可以虚拟网桥的的 IP 地址，同时启用一个 DHCP 服务器，为新创建的虚拟机配置 IP 地址。另一个网卡配置为 NAT 网络，用于访问互联网。配置了 NAT 网络之后，只要你的笔记本电脑能上网，虚拟机就能上网。由于咱们在 Ubuntu 里面要安装一些东西，因而需要联网。你可能会问了，这个配置复杂吗？一点儿都不复杂。咱们讲虚拟机网络（24讲）的时候，讲过这个。 安装完了 Ubuntu 之后，需要对 Ubuntu 里面的网卡进行配置。对于 Ubuntu 来讲，网卡的配置在 /etc/network/interfaces 这个文件里面。在我的环境里，NAT 的网卡名称为 enp0s3，Host-only 的网卡的名称为 enp0s8，都可以配置为自动配置。 auto lo iface lo inet loopback auto enp0s3 iface enp0s3 inet dhcp auto enp0s8 iface enp0s8 inet dhcp 这样，重启之后，IP 就配置好了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:51:2","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"2. 安装 Docker 和 Open vSwitch 接下来，在 Ubuntu 里面，以 root 用户，安装 Docker 和 Open vSwitch。你可以按照 Docker 的官方安装文档来做。我这里也贴一下我的安装过程。 apt-get remove docker docker-engine docker.io apt-get -y update apt-get -y install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg \u003e gpg apt-key add gpg apt-key fingerprint 0EBFCD88 add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" apt-get -y update apt-cache madison docker-ce apt-get -y install docker-ce=18.06.0~ce~3-0~ubuntu 之后，还需要安装 Open vSwitch 和 Bridge。 apt-get -y install openvswitch-common openvswitch-dbg openvswitch-switch python-openvswitch openvswitch-ipsec openvswitch-pki openvswitch-vtep apt-get -y install bridge-utils apt-get -y install arping ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:51:3","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"3. 准备一个 Docker 的镜像 每个节点都是一个 Docker，对应要有一个 Docker 镜像。这个镜像我已经打好了，你可以直接使用。 docker pull hub.c.163.com/liuchao110119163/ubuntu:tcpip 当然你也可以自己打这个镜像。Dockerfile 就像这样： FROM hub.c.163.com/public/ubuntu:14.04 RUN apt-get -y update \u0026\u0026 apt-get install -y iproute2 iputils-arping net-tools tcpdump curl telnet iputils-tracepath traceroute RUN mv /usr/sbin/tcpdump /usr/bin/tcpdump ENTRYPOINT /usr/sbin/sshd -D ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:51:4","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"4. 启动整个环境 启动这个环境还是比较复杂的，我写成了一个脚本。在 Git 仓库里面，有一个文件 setupenv.sh ，可以执行这个脚本，里面有两个参数，一个参数是 NAT 网卡的名字，一个是镜像的名称。 git clone https://github.com/popsuper1982/tcpipillustrated.git cd tcpipillustrated docker pull hub.c.163.com/liuchao110119163/ubuntu:tcpip chmod +x setupenv.sh ./setupenv.sh enp0s3 hub.c.163.com/liuchao110119163/ubuntu:tcpip 这样，整个环境就搭建起来了，所有的容器之间都可以 ping 通，而且都可以上网。不过，我写的这个脚本对一些人来说可能会有点儿复杂，我这里也解释一下。首先每一个节点，都启动一个容器。使用–privileged=true 方式，网络先不配置–net none。有两个二层网络，使用 ovs-vsctl 的 add-br 命令，创建两个网桥。pipework 是一个很好的命令行工具，可以将容器连接到两个二层网络上。但是我们上面那个图里有两个比较特殊的网络，一个是从 slip 到 bsdi 的 P2P 网络，需要创建一个 peer 的两个网卡，然后两个 Docker 的网络 namespace 里面各塞进去一个。有关操作 Docker 的网络 namespace 的方式，咱们在容器网络(29讲)那一节讲过 ip netns 命令。 这里需要注意的是，P2P 网络和下面的二层网络不是同一个网络。P2P 网络的 CIDR 是 140.252.13.64/27，而下面的二层网络的 CIDR 是 140.252.13.32/27。如果按照 /24，看起来是一个网络，但是 /27 就不是了。至于CIDR 的计算方法（3讲），你可以回去复习一下。 另外需要配置从 sun 到 netb 的点对点网络，方法还是通过 peer 网卡和 ip netns 的方式。这里有个特殊的地方，对于 netb 来讲，不是一个普通的路由器，因为 netb 两边是同一个二层网络，所以需要配置 arp proxy。为了所有的节点之间互通，要配置一下路由策略，这里需要通过 ip route 命令。 对于 slip 来讲，bsdi 左面 13.66 这个网口是网关。对于 bsdi 和 svr4 来讲，如果去外网，sun 下面的网口 13.33 是网关。对于 sun 来讲，上面的网口 1.29 属于上面的二层网络了，它如果去外网，gateway 下面的网口 1.4 就是外网网关。对于 aix，solaris，gemini 来讲，如果去外网，网关也是 gateway 下面的网口 1.4。如果去下面的二层网口，网关是 sun 上面的网口 1.29。 配置完了这些，图中的所有的节点都能相互访问了，最后还要解决如何访问外网的问题。我们还是需要创建一个 peer 网卡对。一个放在 gateway 里面，一个放在 gateway 外面。外面的网卡去外网的网关。在虚拟机上面，还需要配置一个 iptables 的地址伪装规则 MASQUERADE，其实就是一个 SNAT。因为容器里面要访问外网，因为外网是不认的，所以源地址不能用容器的地址，需要 SNAT 成为虚拟机的地址出去，回来的时候再 NAT 回来。配置这个环境还是挺复杂的，要用到咱们学到的很多知识。如果没有学习前面那些知识，直接就做这个实验，你肯定会很晕。但是只学理论也不行，要把理论都学过一遍，再做一遍实验，这才是一个不断迭代、更新知识库的过程。有了这个环境，《TCP/IP 详解》里面的所有实验都能做了，而且我打的这个 Docker 镜像里面，tcpdump 等网络工具都安装了，你可以“为所欲为”了。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:51:5","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["Advanced learning"],"content":"Open vSwitch 的实验 做了 TCP/IP 详解的实验之后，网络程序设计这部分，你就有了坚实的基础。但是涉及到数据中心内部的一些网络技术，什么 VLAN、VXLAN、STP 等偏运维方向的，学习还是会比较困难。好在我们有 Open vSwitch，也可以做大量的实验。Open vSwitch 门槛比较高，里面的概念也非常多，可谓千头万绪。不过，通过我这么多年研究的经验，可以告诉你，这里面有一个很好的线索，那就是 Open vSwitch 会将自己对于网络的配置保存在一个本地库里面。这个库的表结构之间的关系就像这样： 这个库其实是一个 JSON，如果把这个 JSON 打印出来，能够看到更加详细的特性。按照这些特性一一实验，可以逐渐把 Open vSwitch 各个特性都掌握。 这里面最重要的概念就是网桥。一个网桥会有流表控制网络包的处理过程，会有控制器下发流表，一个网桥上会有多个端口，可以对端口进行流控，一个端口可以设置 VLAN，一个端口可以包含多个网卡，可以做绑定，网卡可以设置成为 GRE 和 VXLAN。我写过一个 Open vSwitch 的实验教程，也放在了 Github 里面。这里面有这么几个比较重要的实验，你可以看一看。 实验一：查看 Open vSwitch 的架构。我们在讲 Open vSwitch 的时候，提过 Open vSwitch 的架构，在这个实验中，我们可以查看 Open vSwitch 的各个模块以及启动的参数。实验五：配置使用 OpenFlow Controller，体验一把作为小区物业在监控室里面管控整个小区道路的样子。实验八：测试 Port 的 VLAN 功能。看一下 VLAN 隔离究竟是什么样的。实验十：QoS 功能。体验一把如果使用 HTB 进行网卡限流。实验十一：GRE 和 VXLAN 隧道功能，看虚拟网络如何进行租户隔离。实验十五：对 Flow Table 的操作，体验流表对网络包随心所欲的处理。 从图中我们可以看到，在这个体系中：PVC 描述的，是 Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。PV 描述的，则是一个具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。而 StorageClass 的作用，则是充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。 当然，StorageClass 的另一个重要作用，是指定 PV 的 Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning 的话，Kubernetes 就可以自动为你创建 PV 了。基于上述讲述，为了统一概念和方便叙述，在本专栏中，我以后凡是提到“Volume”，指的就是一个远程存储服务挂载在宿主机上的持久化目录；而“PV”，指的是这个 Volume 在 Kubernetes 里的 API 对象。需要注意的是，这套容器持久化存储体系，完全是 Kubernetes 项目自己负责管理的，并不依赖于 docker volume 命令和 Docker 的存储插件。当然，这套体系本身就比 docker volume 命令的诞生时间还要早得多。 ","date":"2022-07-17 10:05:58","objectID":"/cn_proto/:51:6","tags":["computer network"],"title":"CN_proto","uri":"/cn_proto/"},{"categories":["k8s"],"content":" 2018 深入剖析k8s 张磊 2018-08-27 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:0:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"开篇词 | 打通“容器技术”的任督二脉 你好，我是张磊，Kubernetes 社区的一位资深成员和项目维护者。2012 年，我还在浙大读书的时候，就有幸组建了一个云计算与 PaaS 基础设施相关的科研团队，就这样，我从早期的 Cloud Foundry 社区开始，正式与容器结缘。这几年里，我大多数时间都在 Kubernetes 项目里从事上游技术工作，也得以作为一名从业者和社区成员的身份，参与和亲历了容器技术从“初出茅庐”到“尘埃落定”的全过程。而即使从 2013 年 Docker 项目发布开始算起，这次变革也不过短短 5 年时间，可在现如今的技术圈儿里，不懂容器，没听过 Kubernetes，你还真不好意思跟人打招呼。容器技术这样一个新生事物，完全重塑了整个云计算市场的形态。它不仅催生出了一批年轻有为的容器技术人，更培育出了一个具有相当规模的开源基础设施技术市场。在这个市场里，不仅有 Google、Microsoft 等技术巨擘们厮杀至今，更有无数的国内外创业公司前仆后继。而在国内，甚至连以前对开源基础设施领域涉足不多的 BAT、蚂蚁、滴滴这样的巨头们，也都从 AI、云计算、微服务、基础设施等维度多管齐下，争相把容器和 Kubernetes 项目树立为战略重心之一。 就在这场因“容器”而起的技术变革中，Kubernetes 项目已然成为容器技术的事实标准，重新定义了基础设施领域对应用编排与管理的种种可能。2014 年后，我开始以远程的方式，全职在 Kubernetes 和 Kata Containers 社区从事上游开发工作，先后发起了容器镜像亲密性调度、基于等价类的调度优化等多个核心特性，参与了容器运行时接口、安全容器沙盒等多个基础特性的设计和研发。还有幸作为主要的研发人员和维护者之一，亲历了 Serverless Container 概念的诞生与崛起。在 2015 年，我发起和组织撰写了《Docker 容器与容器云》一书，希望帮助更多的人利用容器解决实际场景中的问题。时至今日，这本书的第 2 版也已经出版快 2 年了，受到了广大容器技术读者们的好评。 2018 年，我又赴西雅图，在微软研究院（MSR）云计算与存储研究组，专门从事基于 Kubernetes 的深度学习基础设施相关的研究工作。我与容器打交道的这些年，一直在与关注容器生态的工程师们交流，并经常探讨容器在落地过程中遇到的问题。从这些交流中，我发现总有很多相似的问题被反复提及，比如： 为什么容器里只能跑“一个进程”？为什么我原先一直在用的某个 JVM 参数，在容器里就不好使了？为什么 Kubernetes 就不能固定 IP 地址？容器网络连不通又该如何去 Debug？Kubernetes 中 StatefulSet 和 Operator 到底什么区别？PV 和 PVC 这些概念又该怎么用？ 这些问题乍一看与我们平常的认知非常矛盾，但它们的答案和原理却并不复杂。不过很遗憾，对于刚刚开始学习容器的技术人员来说，它们却很难用一两句话就能解释清楚。究其原因在于，从过去以物理机和虚拟机为主体的开发运维环境，向以容器为核心的基础设施的转变过程，并不是一次温和的改革，而是涵盖了对网络、存储、调度、操作系统、分布式原理等各个方面的容器化理解和改造。 这就导致了很多初学者，对于容器技术栈表现出来的这些难题，要么知识储备不足，要么杂乱无章、无法形成体系。这，也是很多初次参与 PaaS 项目的从业者们共同面临的一个困境。其实，容器技术体系看似纷乱繁杂，却存在着很多可以“牵一发而动全身”的主线。比如，Linux 的进程模型对于容器本身的重要意义；或者，“控制器”模式对整个 Kubernetes 项目提纲挈领的作用。但是，这些关于 Linux 内核、分布式系统、网络、存储等方方面面的积累，并不会在 Docker 或者 Kubernetes 的文档中交代清楚。可偏偏就是它们，才是真正掌握容器技术体系的精髓所在，是每一位技术从业者需要悉心修炼的“内功”。 而这，也正是我开设这个专栏的初衷。我希望借由这个专栏，给你讲清楚容器背后的这些技术本质与设计思想，并结合着对核心特性的剖析与实践，加深你对容器技术的理解。为此，我把专栏划分成了 4 大模块： “白话”容器技术基础： 我希望用饶有趣味的解说，给你梳理容器技术生态的发展脉络，用最通俗易懂的语言描述容器底层技术的实现方式，让你知其然，也知其所以然。Kubernetes 集群的搭建与实践： Kubernetes 集群号称“非常复杂”，但是如果明白了其中的架构和原理，选择了正确的工具和方法，它的搭建却也可以“一键安装”，它的应用部署也可以浅显易懂。容器编排与 Kubernetes 核心特性剖析： 这是这个专栏最重要的内容。“编排”永远都是容器云项目的灵魂所在，也是 Kubernetes 社区持久生命力的源泉。在这一模块，我会从分布式系统设计的视角出发，抽象和归纳出这些特性中体现出来的普遍方法，然后带着这些指导思想去逐一阐述 Kubernetes 项目关于编排、调度和作业管理的各项核心特性。“不识庐山真面目，只缘身在此山中”，希望这样一个与众不同的角度，能够给你以全新的启发。Kubernetes 开源社区与生态：“开源生态”永远都是容器技术和 Kubernetes 项目成功的关键。在这个模块，我会和你一起探讨，容器社区在开源软件工程指导下的演进之路；带你思考，如何同团队一起平衡内外部需求，让自己逐渐成为社区中不可或缺的一员。 我希望通过这些对容器与 Kubernetes 项目的逐层剖析，能够让你面对容器化浪潮时不再踌躇无措，有一种拨云见日的酣畅淋漓。最后，我想再和你分享一个故事。 2015 年我在 InfoQ 举办的第一届容器技术大会上，结识了当时 CoreOS 的布道师 Kelsey Hightower，他热情地和大家一起安装和体验微信，谈笑风生间，还时不时地安利一番自家产品。但两年后也就是 2017 年，Kelsey 已经是全世界容器圈儿的意见领袖，是 Google 公司 Kubernetes 项目的首席布道师，而他的座右铭也变为了“只布道，不推销”。此时，就算你漂洋过海想要亲自拜会 Kelsey ，恐怕也得先预约下时间了。诚然，Kelsey 的“一夜成名”，与他的勤奋和天赋密不可分，但他对这次“容器”变革走向的准确把握却也是功不可没。这也正应了一句名言：一个人的命运啊，当然要靠自我奋斗，但是也要考虑到历史的行程。 眼下，你我可能已经错过了互联网技术大爆炸的时代，也没有在数字货币早期的狂热里分到一杯羹。可就在此时此刻，在沉寂了多年的云计算与基础设施领域，一次以“容器”为名的历史变革，正呼之欲出。这一次，我们又有什么理由作壁上观呢？如果你也想登上“容器”这趟高速前进的列车，我相信这个专栏，可以帮助你打通学习容器技术的“任督二脉”。**在专栏开始，我首先为你准备了 4 篇预习文章，详细地梳理了容器技术自兴起到现在的发展历程，同时也回答了“Kubernetes 为什么会赢”这个重要的问题，算是我额外为你准备的一份开学礼物吧。**机会总是留给有准备的人，现在就让我们一起开启这次充满挑战的容器之旅！ ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:1:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"01 | 预习篇 · 小鲸鱼大事记（一）：初出茅庐 如果我问你，现今最热门的服务器端技术是什么？想必你不假思索就能回答上来：当然是容器！可是，如果现在不是 2018 年而是 2013 年，你的回答还能这么斩钉截铁么？现在就让我们把时间拨回到五年前去看看吧。2013 年的后端技术领域，已经太久没有出现过令人兴奋的东西了。曾经被人们寄予厚望的云计算技术，也已经从当初虚无缥缈的概念蜕变成了实实在在的虚拟机和账单。而相比于如日中天的 AWS 和盛极一时的 OpenStack，以 Cloud Foundry 为代表的开源 PaaS 项目，却成为了当时云计算技术中的一股清流。这时，Cloud Foundry 项目已经基本度过了最艰难的概念普及和用户教育阶段，吸引了包括百度、京东、华为、IBM 等一大批国内外技术厂商，开启了以开源 PaaS 为核心构建平台层服务能力的变革。如果你有机会问问当时的云计算从业者们，他们十有八九都会告诉你：PaaS 的时代就要来了！ 这个说法其实一点儿没错，如果不是后来一个叫 Docker 的开源项目突然冒出来的话。事实上，当时还名叫 dotCloud 的 Docker 公司，也是这股 PaaS 热潮中的一份子。只不过相比于 Heroku、Pivotal、Red Hat 等 PaaS 弄潮儿们，dotCloud 公司实在是太微不足道了，而它的主打产品由于跟主流的 Cloud Foundry 社区脱节，长期以来也无人问津。眼看就要被如火如荼的 PaaS 风潮抛弃，dotCloud 公司却做出了这样一个决定：开源自己的容器项目 Docker。显然，这个决定在当时根本没人在乎。“容器”这个概念从来就不是什么新鲜的东西，也不是 Docker 公司发明的。即使在当时最热门的 PaaS 项目 Cloud Foundry 中，容器也只是其最底层、最没人关注的那一部分。说到这里，我正好以当时的事实标准 Cloud Foundry 为例，来解说一下 PaaS 技术。 PaaS 项目被大家接纳的一个主要原因，就是它提供了一种名叫“应用托管”的能力。 在当时，虚拟机和云计算已经是比较普遍的技术和服务了，那时主流用户的普遍用法，就是租一批 AWS 或者 OpenStack 的虚拟机，然后像以前管理物理服务器那样，用脚本或者手工的方式在这些机器上部署应用。当然，这个部署过程难免会碰到云端虚拟机和本地环境不一致的问题，所以当时的云计算服务，比的就是谁能更好地模拟本地服务器环境，能带来更好的“上云”体验。而 PaaS 开源项目的出现，就是当时解决这个问题的一个最佳方案。举个例子，创建好虚拟机之后，运维人员只需要在这些机器上部署一个 Cloud Foundry 项目，然后开发者只要执行一条命令就能把本地的应用部署到云上，这条命令就是： $ cf push \"我的应用\" 是不是很神奇？事实上，像 Cloud Foundry 这样的 PaaS 项目，最核心的组件就是一套应用的打包和分发机制。 Cloud Foundry 为每种主流编程语言都定义了一种打包格式，而“cf push”的作用，基本上等同于用户把应用的可执行文件和启动脚本打进一个压缩包内，上传到云上 Cloud Foundry 的存储中。接着，Cloud Foundry 会通过调度器选择一个可以运行这个应用的虚拟机，然后通知这个机器上的 Agent 把应用压缩包下载下来启动。这时候关键来了，由于需要在一个虚拟机上启动很多个来自不同用户的应用，Cloud Foundry 会调用操作系统的 Cgroups 和 Namespace 机制为每一个应用单独创建一个称作“沙盒”的隔离环境，然后在“沙盒”中启动这些应用进程。这样，就实现了把多个用户的应用互不干涉地在虚拟机里批量地、自动地运行起来的目的。 这，正是 PaaS 项目最核心的能力。 而这些 Cloud Foundry 用来运行应用的隔离环境，或者说“沙盒”，就是所谓的“容器”。而 Docker 项目，实际上跟 Cloud Foundry 的容器并没有太大不同，所以在它发布后不久，Cloud Foundry 的首席产品经理 James Bayer 就在社区里做了一次详细对比，告诉用户 Docker 实际上只是一个同样使用 Cgroups 和 Namespace 实现的“沙盒”而已，没有什么特别的黑科技，也不需要特别关注。然而，短短几个月，Docker 项目就迅速崛起了。它的崛起速度如此之快，以至于 Cloud Foundry 以及所有的 PaaS 社区还没来得及成为它的竞争对手，就直接被宣告出局了。那时候，一位多年的 PaaS 从业者曾经如此感慨道：这简直就是一场“降维打击”啊。 难道这一次，连闯荡多年的“老江湖”James Bayer 也看走眼了么？并没有。事实上，Docker 项目确实与 Cloud Foundry 的容器在大部分功能和实现原理上都是一样的，可偏偏就是这剩下的一小部分不一样的功能，成了 Docker 项目接下来“呼风唤雨”的不二法宝。 这个功能，就是 Docker 镜像。恐怕连 Docker 项目的作者 Solomon Hykes 自己当时都没想到，这个小小的创新，在短短几年内就如此迅速地改变了整个云计算领域的发展历程。我前面已经介绍过，PaaS 之所以能够帮助用户大规模部署应用到集群里，是因为它提供了一套应用打包的功能。可偏偏就是这个打包功能，却成了 PaaS 日后不断遭到用户诟病的一个“软肋”。出现这个问题的根本原因是，一旦用上了 PaaS，用户就必须为每种语言、每种框架，甚至每个版本的应用维护一个打好的包。这个打包过程，没有任何章法可循，更麻烦的是，明明在本地运行得好好的应用，却需要做很多修改和配置工作才能在 PaaS 里运行起来。而这些修改和配置，并没有什么经验可以借鉴，基本上得靠不断试错，直到你摸清楚了本地应用和远端 PaaS 匹配的“脾气”才能够搞定。最后结局就是，“cf push”确实是能一键部署了，但是为了实现这个一键部署，用户为每个应用打包的工作可谓一波三折，费尽心机。 而 Docker 镜像解决的，恰恰就是打包这个根本性的问题。 所谓 Docker 镜像，其实就是一个压缩包。但是这个压缩包里的内容，比 PaaS 的应用可执行文件 + 启停脚本的组合就要丰富多了。实际上，大多数 Docker 镜像是直接由一个完整操作系统的所有文件和目录构成的，所以这个压缩包里的内容跟你本地开发和测试环境用的操作系统是完全一样的。这就有意思了：假设你的应用在本地运行时，能看见的环境是 CentOS 7.2 操作系统的所有文件和目录，那么只要用 CentOS 7.2 的 ISO 做一个压缩包，再把你的应用可执行文件也压缩进去，那么无论在哪里解压这个压缩包，都可以得到与你本地测试时一样的环境。当然，你的应用也在里面！这就是 Docker 镜像最厉害的地方：只要有这个压缩包在手，你就可以使用某种技术创建一个“沙盒”，在“沙盒”中解压这个压缩包，然后就可以运行你的程序了。更重要的是，这个压缩包包含了完整的操作系统文件和目录，也就是包含了这个应用运行所需要的所有依赖，所以你可以先用这个压缩包在本地进行开发和测试，完成之后，再把这个压缩包上传到云端运行。在这个过程中，你完全不需要进行任何配置或者修改，因为这个压缩包赋予了你一种极其宝贵的能力：本地环境和云端环境的高度一致！ 这，正是 Docker 镜像的精髓。那么，有了 Docker 镜像这个利器，PaaS 里最核心的打包系统一下子就没了用武之地，最让用户抓狂的打包过程也随之消失了。相比之下，在当今的互联网里，Docker 镜像需要的操作系统文件和目录，可谓唾手可得。所以，你只需要提供一个下载好的操作系统文件与目录，然后使用它制作一个压缩包即可，这个命令就是： $ docker build \"我的镜像\" 一旦镜像制作完成，用户就可以让 Docker 创建一个“沙盒”来解压这个镜像，然后在“沙盒”中运行自己的应用，这个命令就是： $ docker run \"我的镜像\" 当然，docker run 创建的“沙盒”，也是使用 Cgroups 和 Namespace 机制创建出来的隔离环境。我会在后面的文章中，详细介绍这个机制的实现原理。所以，Docker 项目给 PaaS 世界带来的“降维打击”，其实是提供了一种非常便利的打包机制。这种机制直接打包了应用运行所需要的整个操作系统，从而保证了本地环境和云端环境的高度一致，避免了用户通过“试错”来匹配两种不同运行环境之间差异的痛苦过程。 而对于开发者们来说，在终于体验到了生产力解放所带来的痛快之后，他们自然选择了用脚投票，直接宣告了 PaaS 时代的结束。不过，Docker 项目固然解决了应用打包的难题，但正如前面所介绍的那样，它并不能代替 PaaS 完成大规模部署应用的职责。遗憾的是，考虑到 Docker 公司是一个与自己有潜在竞争关系的商业实体，再加上对 Docker 项目普及程度的错误判断，Cloud Foundry 项目并没有第一时间使用 Docker 作为自己的核心依赖，去替换自己那套饱受诟病的打包流程。反倒是一些机敏的创业公司，纷纷在第一时间推出了 Docker 容器集群管理的开源项目（比如 Deis 和 Flynn），它们一般称自己为 CaaS，即 Container-as-a-Service，用来跟“过时”的 PaaS 们划清界限。而在 2014 年底的 DockerCon 上，Docker 公司雄心勃勃地对外发布了自家研发的“Docker 原生”容器集","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:2:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 2013~2014 年，以 Cloud Foundry 为代表的 PaaS 项目，逐渐完成了教育用户和开拓市场的艰巨任务，也正是在这个将概念逐渐落地的过程中，应用“打包”困难这个问题，成了整个后端技术圈子的一块心病。Docker 项目的出现，则为这个根本性的问题提供了一个近乎完美的解决方案。这正是 Docker 项目刚刚开源不久，就能够带领一家原本默默无闻的 PaaS 创业公司脱颖而出，然后迅速占领了所有云计算领域头条的技术原因。而在成为了基础设施领域近十年难得一见的技术明星之后，dotCloud 公司则在 2013 年底大胆改名为 Docker 公司。不过，这个在当时就颇具争议的改名举动，也成为了日后容器技术圈风云变幻的一个关键伏笔。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:2:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"02 | 预习篇 · 小鲸鱼大事记（二）：崭露头角 在上一篇文章中，我说到，伴随着 PaaS 概念的逐步普及，以 Cloud Foundry 为代表的经典 PaaS 项目，开始进入基础设施领域的视野，平台化和 PaaS 化成了这个生态中的一个最为重要的进化趋势。就在对开源 PaaS 项目落地的不断尝试中，这个领域的从业者们发现了 PaaS 中最为棘手也最亟待解决的一个问题：究竟如何给应用打包？遗憾的是，无论是 Cloud Foundry、OpenShift，还是 Clodify，面对这个问题都没能给出一个完美的答案，反而在竞争中走向了碎片化的歧途。而就在这时，一个并不引人瞩目的 PaaS 创业公司 dotCloud，却选择了开源自家的一个容器项目 Docker。更出人意料的是，就是这样一个普通到不能再普通的技术，却开启了一个名为“Docker”的全新时代。你可能会有疑问，Docker 项目的崛起，是不是偶然呢？ 事实上，这个以“鲸鱼”为注册商标的技术创业公司，最重要的战略之一就是：坚持把“开发者”群体放在至高无上的位置。相比于其他正在企业级市场里厮杀得头破血流的经典 PaaS 项目们，Docker 项目的推广策略从一开始就呈现出一副“憨态可掬”的亲人姿态，把每一位后端技术人员（而不是他们的老板）作为主要的传播对象。简洁的 UI，有趣的 demo，“1 分钟部署一个 WordPress 网站”“3 分钟部署一个 Nginx 集群”，这种同开发者之间与生俱来的亲近关系，使 Docker 项目迅速成为了全世界 Meetup 上最受欢迎的一颗新星。在过去的很长一段时间里，相较于前端和互联网技术社区，服务器端技术社区一直是一个相对沉闷而小众的圈子。在这里，从事 Linux 内核开发的极客们自带“不合群”的“光环”，后端开发者们啃着多年不变的 TCP/IP 发着牢骚，运维更是天生注定的幕后英雄。而 Docker 项目，却给后端开发者提供了走向聚光灯的机会。就比如 Cgroups 和 Namespace 这种已经存在多年却很少被人们关心的特性，在 2014 年和 2015 年竟然频繁入选各大技术会议的分享议题，就因为听众们想要知道 Docker 这个东西到底是怎么一回事儿。 而 Docker 项目之所以能取得如此高的关注，一方面正如前面我所说的那样，它解决了应用打包和发布这一困扰运维人员多年的技术难题；而另一方面，就是因为它第一次把一个纯后端的技术概念，通过非常友好的设计和封装，交到了最广大的开发者群体手里。在这种独特的氛围烘托下，你不需要精通 TCP/IP，也无需深谙 Linux 内核原理，哪怕只是一个前端或者网站的 PHP 工程师，都会对如何把自己的代码打包成一个随处可以运行的 Docker 镜像充满好奇和兴趣。这种受众群体的变革，正是 Docker 这样一个后端开源项目取得巨大成功的关键。这也是经典 PaaS 项目想做却没有做好的一件事情：PaaS 的最终用户和受益者，一定是为这个 PaaS 编写应用的开发者们，而在 Docker 项目开源之前，PaaS 与开发者之间的关系却从未如此紧密过。 解决了应用打包这个根本性的问题，同开发者与生俱来的的亲密关系，再加上 PaaS 概念已经深入人心的完美契机，成为 Docker 这个技术上看似平淡无奇的项目一举走红的重要原因。一时之间，“容器化”取代“PaaS 化”成为了基础设施领域最炙手可热的关键词，一个以“容器”为中心的、全新的云计算市场，正呼之欲出。而作为这个生态的一手缔造者，此时的 dotCloud 公司突然宣布将公司名称改为“Docker”。这个举动，在当时颇受质疑。在大家印象中，Docker 只是一个开源项目的名字。可是现在，这个单词却成了 Docker 公司的注册商标，任何人在商业活动中使用这个单词，以及鲸鱼的 Logo，都会立刻受到法律警告。那么，Docker 公司这个举动到底卖的什么药？这个问题，我不妨后面再做解读，因为相较于这件“小事儿”，Docker 公司在 2014 年发布 Swarm 项目才是真正的“大事儿”。那么，Docker 公司为什么一定要发布 Swarm 项目呢？ 通过我对 Docker 项目崛起背后原因的分析，你应该能发现这样一个有意思的事实：虽然通过“容器”这个概念完成了对经典 PaaS 项目的“降维打击”，但是 Docker 项目和 Docker 公司，兜兜转转了一年多，却还是回到了 PaaS 项目原本深耕了多年的那个战场：如何让开发者把应用部署在我的项目上。没错，Docker 项目从发布之初就全面发力，从技术、社区、商业、市场全方位争取到的开发者群体，实际上是为此后吸引整个生态到自家“PaaS”上的一个铺垫。只不过这时，“PaaS”的定义已经全然不是 Cloud Foundry 描述的那个样子，而是变成了一套以 Docker 容器为技术核心，以 Docker 镜像为打包标准的、全新的“容器化”思路。这，正是 Docker 项目从一开始悉心运作“容器化”理念和经营整个 Docker 生态的主要目的。而 Swarm 项目，正是接下来承接 Docker 公司所有这些努力的关键所在。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:3:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 今天，我着重介绍了 Docker 项目在短时间内迅速崛起的三个重要原因：Docker 镜像通过技术手段解决了 PaaS 的根本性问题；Docker 容器同开发者之间有着与生俱来的密切关系；PaaS 概念已经深入人心的完美契机。 崭露头角的 Docker 公司，也终于能够以一个更加强硬的姿态来面对这个曾经无比强势，但现在却完全不知所措的云计算市场。而 2014 年底的 DockerCon 欧洲峰会，则正式拉开了 Docker 公司扩张的序幕。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:3:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"03 | 预习篇 · 小鲸鱼大事记（三）：群雄并起 在上一篇文章中，我剖析了 Docker 项目迅速走红背后的技术与非技术原因，也介绍了 Docker 公司开启平台化战略的野心。可是，Docker 公司为什么在 Docker 项目已经取得巨大成功之后，却执意要重新走回那条已经让无数先驱们尘沙折戟的 PaaS 之路呢？实际上，Docker 项目一日千里的发展势头，一直伴随着公司管理层和股东们的阵阵担忧。他们心里明白，虽然 Docker 项目备受追捧，但用户们最终要部署的，还是他们的网站、服务、数据库，甚至是云计算业务。这就意味着，只有那些能够为用户提供平台层能力的工具，才会真正成为开发者们关心和愿意付费的产品。而 Docker 项目这样一个只能用来创建和启停容器的小工具，最终只能充当这些平台项目的“幕后英雄”。 而谈到 Docker 项目的定位问题，就不得不说说 Docker 公司的老朋友和老对手 CoreOS 了。CoreOS 是一个基础设施领域创业公司。 它的核心产品是一个定制化的操作系统，用户可以按照分布式集群的方式，管理所有安装了这个操作系统的节点。从而，用户在集群里部署和管理应用就像使用单机一样方便了。Docker 项目发布后，CoreOS 公司很快就认识到可以把“容器”的概念无缝集成到自己的这套方案中，从而为用户提供更高层次的 PaaS 能力。所以，CoreOS 很早就成了 Docker 项目的贡献者，并在短时间内成为了 Docker 项目中第二重要的力量。 然而，这段短暂的蜜月期到 2014 年底就草草结束了。CoreOS 公司以强烈的措辞宣布与 Docker 公司停止合作，并直接推出了自己研制的 Rocket（后来叫 rkt）容器。这次决裂的根本原因，正是源于 Docker 公司对 Docker 项目定位的不满足。Docker 公司解决这种不满足的方法就是，让 Docker 项目提供更多的平台层能力，即向 PaaS 项目进化。而这，显然与 CoreOS 公司的核心产品和战略发生了严重冲突。也就是说，Docker 公司在 2014 年就已经定好了平台化的发展方向，并且绝对不会跟 CoreOS 在平台层面开展任何合作。这样看来，Docker 公司在 2014 年 12 月的 DockerCon 上发布 Swarm 的举动，也就一点都不突然了。相较于 CoreOS 是依托于一系列开源项目（比如 Container Linux 操作系统、Fleet 作业调度工具、systemd 进程管理和 rkt 容器），一层层搭建起来的平台产品，Swarm 项目则是以一个完整的整体来对外提供集群管理功能。而 Swarm 的最大亮点，则是它完全使用 Docker 项目原本的容器管理 API 来完成集群管理，比如： 单机 Docker 项目： $ docker run \"我的容器 多机 Docker 项目： $ docker run -H \"我的Swarm集群API地址\" \"我的容器\" 所以在部署了 Swarm 的多机环境下，用户只需要使用原先的 Docker 指令创建一个容器，这个请求就会被 Swarm 拦截下来处理，然后通过具体的调度算法找到一个合适的 Docker Daemon 运行起来。这个操作方式简洁明了，对于已经了解过 Docker 命令行的开发者们也很容易掌握。所以，这样一个“原生”的 Docker 容器集群管理项目一经发布，就受到了已有 Docker 用户群的热捧。而相比之下，CoreOS 的解决方案就显得非常另类，更不用说用户还要去接受完全让人摸不着头脑、新造的容器项目 rkt 了。当然，Swarm 项目只是 Docker 公司重新定义“PaaS”的关键一环而已。在 2014 年到 2015 年这段时间里，Docker 项目的迅速走红催生出了一个非常繁荣的“Docker 生态”。在这个生态里，围绕着 Docker 在各个层次进行集成和创新的项目层出不穷。 而此时已经大红大紫到“不差钱”的 Docker 公司，开始及时地借助这波浪潮通过并购来完善自己的平台层能力。其中一个最成功的案例，莫过于对 Fig 项目的收购。要知道，Fig 项目基本上只是靠两个人全职开发和维护的，可它却是当时 GitHub 上热度堪比 Docker 项目的明星。Fig 项目之所以受欢迎，在于它在开发者面前第一次提出了“容器编排”（Container Orchestration）的概念。 其实，“编排”（Orchestration）在云计算行业里不算是新词汇，它主要是指用户如何通过某些工具或者配置来完成一组虚拟机以及关联资源的定义、配置、创建、删除等工作，然后由云计算平台按照这些指定的逻辑来完成的过程。而容器时代，“编排”显然就是对 Docker 容器的一系列定义、配置和创建动作的管理。而 Fig 的工作实际上非常简单：假如现在用户需要部署的是应用容器 A、数据库容器 B、负载均衡容器 C，那么 Fig 就允许用户把 A、B、C 三个容器定义在一个配置文件中，并且可以指定它们之间的关联关系，比如容器 A 需要访问数据库容器 B。接下来，你只需要执行一条非常简单的指令： $ fig up Fig 就会把这些容器的定义和配置交给 Docker API 按照访问逻辑依次创建，你的一系列容器就都启动了；而容器 A 与 B 之间的关联关系，也会交给 Docker 的 Link 功能通过写入 hosts 文件的方式进行配置。更重要的是，你还可以在 Fig 的配置文件里定义各种容器的副本个数等编排参数，再加上 Swarm 的集群管理能力，一个活脱脱的 PaaS 呼之欲出。Fig 项目被收购后改名为 Compose，它成了 Docker 公司到目前为止第二大受欢迎的项目，一直到今天也依然被很多人使用。 当时的这个容器生态里，还有很多令人眼前一亮的开源项目或公司。比如，专门负责处理容器网络的 SocketPlane 项目（后来被 Docker 公司收购），专门负责处理容器存储的 Flocker 项目（后来被 EMC 公司收购），专门给 Docker 集群做图形化管理界面和对外提供云服务的 Tutum 项目（后来被 Docker 公司收购）等等。一时之间，整个后端和云计算领域的聪明才俊都汇集在了这个“小鲸鱼”的周围，为 Docker 生态的蓬勃发展献上了自己的智慧。 而除了这个异常繁荣的、围绕着 Docker 项目和公司的生态之外，还有一个势力在当时也是风头无两，这就是老牌集群管理项目 Mesos 和它背后的创业公司 Mesosphere。Mesos 作为 Berkeley 主导的大数据套件之一，是大数据火热时最受欢迎的资源管理项目，也是跟 Yarn 项目杀得难舍难分的实力派选手。不过，大数据所关注的计算密集型离线业务，其实并不像常规的 Web 服务那样适合用容器进行托管和扩容，也没有对应用打包的强烈需求，所以 Hadoop、Spark 等项目到现在也没在容器技术上投下更大的赌注；但是对于 Mesos 来说，天生的两层调度机制让它非常容易从大数据领域抽身，转而去支持受众更加广泛的 PaaS 业务。 在这种思路的指导下，Mesosphere 公司发布了一个名为 Marathon 的项目，而这个项目很快就成为了 Docker Swarm 的一个有力竞争对手。 虽然不能提供像 Swarm 那样的原生 Docker API，Mesos 社区却拥有一个独特的竞争力：超大规模集群的管理经验。早在几年前，Mesos 就已经通过了万台节点的验证，2014 年之后又被广泛使用在 eBay 等大型互联网公司的生产环境中。而这次通过 Marathon 实现了诸如应用托管和负载均衡的 PaaS 功能之后，Mesos+Marathon 的组合实际上进化成了一个高度成熟的 PaaS 项目，同时还能很好地支持大数据业务。所以，在这波容器化浪潮中，Mesosphere 公司不失时机地提出了一个名叫“DC/OS”（数据中心操作系统）的口号和产品，旨在使用户能够像管理一台机器那样管理一个万级别的物理机集群，并且使用 Docker 容器在这个集群里自由地部署应用。而这，对很多大型企业来说具有着非同寻常的吸引力。 这时，如果你再去审视当时的容器技术生态，就不难发现 CoreOS 公司竟然显得有些尴尬了。它的 rkt 容器完全打不开局面，Fleet 集群管理项目更是少有人问津，CoreOS 完全被 Docker 公司压制了。而处境同样不容乐观的似乎还有 RedHat，作为 Docker 项目早期的重要贡献者，RedHat 也是因为对 Docker 公司平台化战略不满而愤愤退出。但此时，它竟只剩下 OpenShift 这个跟 Cloud Foundry 同时代的经典 PaaS 一张牌可以打，跟 Docker Swarm 和转型后的 Mesos 完全不在同一个“竞技水平”之上。 那么，事实果真如此吗？2014 年注定是一个神奇的年份。就在这一年的 6 月，基础设施领域的翘楚 Google 公司突然发力，正式宣告了一个名叫 Kubernetes 项目的诞生。而这个项目，不仅挽救了当时的 CoreOS 和 RedHat，还如同当年 Docker 项目的横空出世一样，再一次改变了整个容器市场的格局。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:4:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 我分享了 Docker 公司平台化战略的来龙去脉，阐述了 Docker Swarm 项目发布的意义和它背后的设计思想，介绍了 Fig（后来的 Compose）项目如何成为了继 Docker 之后最受瞩目的新星。同时，我也和你一起回顾了 2014~2015 年间如火如荼的容器化浪潮里群雄并起的繁荣姿态。在这次生态大爆发中，Docker 公司和 Mesosphere 公司，依托自身优势率先占据了有利位置。但是，更强大的挑战者们，即将在不久后纷至沓来。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:4:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"04 | 预习篇 · 小鲸鱼大事记（四）：尘埃落定 在上一次的分享中我提到，伴随着 Docker 公司一手打造出来的容器技术生态在云计算市场中站稳了脚跟，围绕着 Docker 项目进行的各个层次的集成与创新产品，也如雨后春笋般出现在这个新兴市场当中。而 Docker 公司，不失时机地发布了 Docker Compose、Swarm 和 Machine“三件套”，在重新定义 PaaS 的方向上走出了最关键的一步。这段时间，也正是 Docker 生态创业公司们的春天，大量围绕着 Docker 项目的网络、存储、监控、CI/CD，甚至 UI 项目纷纷出台，也涌现出了很多 Rancher、Tutum 这样在开源与商业上均取得了巨大成功的创业公司。在 2014~2015 年间，整个容器社区可谓热闹非凡。 这令人兴奋的繁荣背后，却浮现出了更多的担忧。这其中最主要的负面情绪，是对 Docker 公司商业化战略的种种顾虑。事实上，很多从业者也都看得明白，Docker 项目此时已经成为 Docker 公司一个商业产品。而开源，只是 Docker 公司吸引开发者群体的一个重要手段。不过这么多年来，开源社区的商业化其实都是类似的思路，无非是高不高调、心不心急的问题罢了。而真正令大多数人不满意的是，Docker 公司在 Docker 开源项目的发展上，始终保持着绝对的权威和发言权，并在多个场合用实际行动挑战到了其他玩家（比如，CoreOS、RedHat，甚至谷歌和微软）的切身利益。那么，这个时候，大家的不满也就不再是在 GitHub 上发发牢骚这么简单了。 相信很多容器领域的老玩家们都听说过，Docker 项目刚刚兴起时，Google 也开源了一个在内部使用多年、经历过生产环境验证的 Linux 容器：lmctfy（Let Me Container That For You）。然而，面对 Docker 项目的强势崛起，这个对用户没那么友好的 Google 容器项目根本没有招架之力。所以，知难而退的 Google 公司，向 Docker 公司表示了合作的愿望：关停这个项目，和 Docker 公司共同推进一个中立的容器运行时（container runtime）库作为 Docker 项目的核心依赖。不过，Docker 公司并没有认同这个明显会削弱自己地位的提议，还在不久后，自己发布了一个容器运行时库 Libcontainer。这次匆忙的、由一家主导的、并带有战略性考量的重构，成了 Libcontainer 被社区长期诟病代码可读性差、可维护性不强的一个重要原因。至此，Docker 公司在容器运行时层面上的强硬态度，以及 Docker 项目在高速迭代中表现出来的不稳定和频繁变更的问题，开始让社区叫苦不迭。 这种情绪在 2015 年达到了一个小高潮，容器领域的其他几位玩家开始商议“切割”Docker 项目的话语权。而“切割”的手段也非常经典，那就是成立一个中立的基金会。于是，2015 年 6 月 22 日，由 Docker 公司牵头，CoreOS、Google、RedHat 等公司共同宣布，Docker 公司将 Libcontainer 捐出，并改名为 RunC 项目，交由一个完全中立的基金会管理，然后以 RunC 为依据，大家共同制定一套容器和镜像的标准和规范。 这套标准和规范，就是 OCI（ Open Container Initiative ）。OCI 的提出，意在将容器运行时和镜像的实现从 Docker 项目中完全剥离出来。这样做，一方面可以改善 Docker 公司在容器技术上一家独大的现状，另一方面也为其他玩家不依赖于 Docker 项目构建各自的平台层能力提供了可能。不过，不难看出，OCI 的成立更多的是这些容器玩家出于自身利益进行干涉的一个妥协结果。所以，尽管 Docker 是 OCI 的发起者和创始成员，它却很少在 OCI 的技术推进和标准制定等事务上扮演关键角色，也没有动力去积极地推进这些所谓的标准。这，也正是迄今为止 OCI 组织效率持续低下的根本原因。 眼看着 OCI 并没能改变 Docker 公司在容器领域一家独大的现状，Google 和 RedHat 等公司于是把与第二把武器摆上了台面。Docker 之所以不担心 OCI 的威胁，原因就在于它的 Docker 项目是容器生态的事实标准，而它所维护的 Docker 社区也足够庞大。可是，一旦这场斗争被转移到容器之上的平台层，或者说 PaaS 层，Docker 公司的竞争优势便立刻捉襟见肘了。在这个领域里，像 Google 和 RedHat 这样的成熟公司，都拥有着深厚的技术积累；而像 CoreOS 这样的创业公司，也拥有像 Etcd 这样被广泛使用的开源基础设施项目。可是 Docker 公司呢？它却只有一个 Swarm。 所以这次，Google、RedHat 等开源基础设施领域玩家们，共同牵头发起了一个名为 CNCF（Cloud Native Computing Foundation）的基金会。这个基金会的目的其实很容易理解：它希望，以 Kubernetes 项目为基础，建立一个由开源基础设施领域厂商主导的、按照独立基金会方式运营的平台级社区，来对抗以 Docker 公司为核心的容器商业生态。而为了打造出这样一条围绕 Kubernetes 项目的“护城河”，CNCF 社区就需要至少确保两件事情： Kubernetes 项目必须能够在容器编排领域取得足够大的竞争优势；CNCF 社区必须以 Kubernetes 项目为核心，覆盖足够多的场景。 我们先来看看 CNCF 社区如何解决 Kubernetes 项目在编排领域的竞争力的问题。在容器编排领域，Kubernetes 项目需要面对来自 Docker 公司和 Mesos 社区两个方向的压力。不难看出，Swarm 和 Mesos 实际上分别从两个不同的方向讲出了自己最擅长的故事：Swarm 擅长的是跟 Docker 生态的无缝集成，而 Mesos 擅长的则是大规模集群的调度与管理。这两个方向，也是大多数人做容器集群管理项目时最容易想到的两个出发点。也正因为如此，Kubernetes 项目如果继续在这两个方向上做文章恐怕就不太明智了。所以这一次，Kubernetes 选择的应对方式是：Borg。 如果你看过 Kubernetes 项目早期的 GitHub Issue 和 Feature 的话，就会发现它们大多来自于 Borg 和 Omega 系统的内部特性，这些特性落到 Kubernetes 项目上，就是 Pod、Sidecar 等功能和设计模式。这就解释了，为什么 Kubernetes 发布后，很多人“抱怨”其设计思想过于“超前”的原因：Kubernetes 项目的基础特性，并不是几个工程师突然“拍脑袋”想出来的东西，而是 Google 公司在容器化基础设施领域多年来实践经验的沉淀与升华。这，正是 Kubernetes 项目能够从一开始就避免同 Swarm 和 Mesos 社区同质化的重要手段。于是，CNCF 接下来的任务就是，如何把这些先进的思想通过技术手段在开源社区落地，并培育出一个认同这些理念的生态？这时，RedHat 就发挥了重要作用。 当时，Kubernetes 团队规模很小，能够投入的工程能力也十分紧张，而这恰恰是 RedHat 的长处。更难得的是，RedHat 是世界上为数不多的、能真正理解开源社区运作和项目研发真谛的合作伙伴。所以，RedHat 与 Google 联盟的成立，不仅保证了 RedHat 在 Kubernetes 项目上的影响力，也正式开启了容器编排领域“三国鼎立”的局面。这时，我们再重新审视容器生态的格局，就不难发现 Kubernetes 项目、Docker 公司和 Mesos 社区这三大玩家的关系已经发生了微妙的变化。其中，Mesos 社区与容器技术的关系，更像是“借势”，而不是这个领域真正的参与者和领导者。这个事实，加上它所属的 Apache 社区固有的封闭性，导致了 Mesos 社区虽然技术最为成熟，却在容器编排领域鲜有创新。 这也是为何，Google 公司很快就把注意力转向了动作更加激进的 Docker 公司。有意思的是，Docker 公司对 Mesos 社区也是类似的看法。所以从一开始，Docker 公司就把应对 Kubernetes 项目的竞争摆在了首要位置：一方面，不断强调“Docker Native”的“重要性”，另一方面，与 Kubernetes 项目在多个场合进行了直接的碰撞。不过，这次竞争的发展态势，很快就超过了 Docker 公司的预期。Kubernetes 项目并没有跟 Swarm 项目展开同质化的竞争，所以“Docker Native”的说辞并没有太大的杀伤力。相反地，Kubernetes 项目让人耳目一新的设计理念和号召力，很快就构建出了一个与众不同的容器编排与管理的生态。就这样，Kubernetes 项目在 GitHub 上的各项指标开始一骑绝尘，将 Swarm 项目远远地甩在了身后。 有了这个基础，CNCF 社区就可以放心地解决第二个问题了。在已经囊括了容器监控事实标准的 Prometheus 项目之后，CNCF 社区迅速在成员项目中添加了 Fluentd、OpenTracing、CNI 等一系列容器生态的知名工具和项目。而在看到了 CNCF 社区对用户表现出来的巨大吸引力之后，大量的公司和创业团队也开始专门针对 CNCF 社区而非 Docker 公司制定推广策略。面对这样的竞争态势，Docker 公","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:5:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 容器技术圈子在短短几年里发生了很多变数，但很多事情其实也都在情理之中。就像 Docker 这样一家创业公司，在通过开源社区的运作取得了巨大的成功之后，就不得不面对来自整个云计算产业的竞争和围剿。而这个产业的垄断特性，对于 Docker 这样的技术型创业公司其实天生就不友好。在这种局势下，接受微软的天价收购，在大多数人看来都是一个非常明智和实际的选择。可是 Solomon Hykes 却多少带有一些理想主义的影子，既然不甘于“寄人篱下”，那他就必须带领 Docker 公司去对抗来自整个云计算产业的压力。只不过，Docker 公司最后选择的对抗方式，是将开源项目与商业产品紧密绑定，打造了一个极端封闭的技术生态。而这，其实违背了 Docker 项目与开发者保持亲密关系的初衷。相比之下，Kubernetes 社区，正是以一种更加温和的方式，承接了 Docker 项目的未尽事业，即：以开发者为核心，构建一个相对民主和开放的容器生态。这也是为何，Kubernetes 项目的成功其实是必然的。现在，我们很难想象如果 Docker 公司最初选择了跟 Kubernetes 社区合作，如今的容器生态又将会是怎样的一番景象。不过我们可以肯定的是，Docker 公司在过去五年里的风云变幻，以及 Solomon Hykes 本人的传奇经历，都已经在云计算的长河中留下了浓墨重彩的一笔。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:5:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"容器技术概念入门篇 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:6:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"05 | 白话容器基础（一）：从进程说开去 紧接着，我详细介绍了容器技术圈在过去五年里的“风云变幻”，而通过这部分内容，我希望你能理解这样一个道理：容器本身没有价值，有价值的是“容器编排”。也正因为如此，容器技术生态才爆发了一场关于“容器编排”的“战争”。而这次战争，最终以 Kubernetes 项目和 CNCF 社区的胜利而告终。所以，这个专栏后面的内容，我会以 Docker 和 Kubernetes 项目为核心，为你详细介绍容器技术的各项实践与其中的原理。不过在此之前，你还需要搞清楚一个更为基础的问题：容器，到底是怎么一回事儿？在第一篇预习文章《小鲸鱼大事记（一）：初出茅庐》中，我已经提到过，容器其实是一种沙盒技术。顾名思义，沙盒就是能够像一个集装箱一样，把你的应用“装”起来的技术。这样，应用与应用之间，就因为有了边界而不至于相互干扰；而被装进集装箱的应用，也可以被方便地搬来搬去，这不就是 PaaS 最理想的状态嘛。不过，这两个能力说起来简单，但要用技术手段去实现它们，可能大多数人就无从下手了。 所以，我就先来跟你说说这个“边界”的实现手段。假如，现在你要写一个计算加法的小程序，这个程序需要的输入来自于一个文件，计算完成后的结果则输出到另一个文件中。由于计算机只认识 0 和 1，所以无论用哪种语言编写这段代码，最后都需要通过某种方式翻译成二进制文件，才能在计算机操作系统中运行起来。而为了能够让这些代码正常运行，我们往往还要给它提供数据，比如我们这个加法程序所需要的输入文件。这些数据加上代码本身的二进制文件，放在磁盘上，就是我们平常所说的一个“程序”，也叫代码的可执行镜像（executable image）。 然后，我们就可以在计算机上运行这个“程序”了。首先，操作系统从“程序”中发现输入数据保存在一个文件中，所以这些数据就会被加载到内存中待命。同时，操作系统又读取到了计算加法的指令，这时，它就需要指示 CPU 完成加法操作。而 CPU 与内存协作进行加法计算，又会使用寄存器存放数值、内存堆栈保存执行的命令和变量。同时，计算机里还有被打开的文件，以及各种各样的 I/O 设备在不断地调用中修改自己的状态。就这样，一旦“程序”被执行起来，它就从磁盘上的二进制文件，变成了计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。像这样一个程序运行起来后的计算机执行环境的总和，就是我们今天的主角：进程。 所以，对于进程来说，它的静态表现就是程序，平常都安安静静地待在磁盘上；而一旦运行起来，它就变成了计算机里的数据和状态的总和，这就是它的动态表现。而容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而 Namespace 技术则是用来修改进程视图的主要方法。你可能会觉得 Cgroups 和 Namespace 这两个概念很抽象，别担心，接下来我们一起动手实践一下，你就很容易理解这两项技术了。假设你已经有了一个 Linux 操作系统上的 Docker 项目在运行，比如我的环境是 Ubuntu 16.04 和 Docker CE 18.05。接下来，让我们首先创建一个容器来试试。 $ docker run -it busybox /bin/sh / # 这个命令是 Docker 项目最重要的一个操作，即大名鼎鼎的 docker run。而 -it 参数告诉了 Docker 项目在启动容器后，需要给我们分配一个文本输入 / 输出环境，也就是 TTY，跟容器的标准输入相关联，这样我们就可以和这个 Docker 容器进行交互了。而 /bin/sh 就是我们要在 Docker 容器里运行的程序。所以，上面这条指令翻译成人类的语言就是：请帮我启动一个容器，在容器里执行 /bin/sh，并且给我分配一个命令行终端跟这个容器交互。这样，我的 Ubuntu 16.04 机器就变成了一个宿主机，而一个运行着 /bin/sh 的容器，就跑在了这个宿主机里面。上面的例子和原理，如果你已经玩过 Docker，一定不会感到陌生。此时，如果我们在容器里执行一下 ps 指令，就会发现一些更有趣的事情： / # ps PID USER TIME COMMAND 1 root 0:00 /bin/sh 10 root 0:00 ps 可以看到，我们在 Docker 里最开始执行的 /bin/sh，就是这个容器内部的第 1 号进程（PID=1），而这个容器里一共只有两个进程在运行。这就意味着，前面执行的 /bin/sh，以及我们刚刚执行的 ps，已经被 Docker 隔离在了一个跟宿主机完全不同的世界当中。这究竟是怎么做到的呢？ 本来，每当我们在宿主机上运行了一个 /bin/sh 程序，操作系统都会给它分配一个进程编号，比如 PID=100。这个编号是进程的唯一标识，就像员工的工牌一样。所以 PID=100，可以粗略地理解为这个 /bin/sh 是我们公司里的第 100 号员工，而第 1 号员工就自然是比尔 · 盖茨这样统领全局的人物。而现在，我们要通过 Docker 把这个 /bin/sh 程序运行在一个容器当中。这时候，Docker 就会在这个第 100 号员工入职时给他施一个“障眼法”，让他永远看不到前面的其他 99 个员工，更看不到比尔 · 盖茨。这样，他就会错误地以为自己就是公司里的第 1 号员工。这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如 PID=1。可实际上，他们在宿主机的操作系统里，还是原来的第 100 号进程。 这种技术，就是 Linux 里面的 Namespace 机制。而 Namespace 的使用方式也非常有意思：它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建进程的系统调用是 clone()，比如： int pid = clone(main_function, stack_size, SIGCHLD, NULL); 这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。而当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如： int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。当然，我们还可以多次执行上面的 clone() 调用，这样就会创建多个 PID Namespace，而每个 Namespace 里的应用进程，都会认为自己是当前容器里的第 1 号进程，它们既看不到宿主机里真正的进程空间，也看不到其他 PID Namespace 里的具体情况。 而除了我们刚刚用到的 PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。比如，Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。 这，就是 Linux 容器最基本的实现原理了。所以，Docker 容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。 所以说，容器，其实是一种特殊的进程而已。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:7:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 谈到为“进程划分一个独立空间”的思想，相信你一定会联想到虚拟机。而且，你应该还看过一张虚拟机和容器的对比图。 这幅图的左边，画出了虚拟机的工作原理。其中，名为 Hypervisor 的软件是虚拟机最主要的部分。它通过硬件虚拟化功能，模拟出了运行一个操作系统需要的各种硬件，比如 CPU、内存、I/O 设备等等。然后，它在这些虚拟的硬件上安装了一个新的操作系统，即 Guest OS。这样，用户的应用进程就可以运行在这个虚拟的机器中，它能看到的自然也只有 Guest OS 的文件和目录，以及这个机器里的虚拟设备。这就是为什么虚拟机也能起到将不同的应用进程相互隔离的作用。而这幅图的右边，则用一个名为 Docker Engine 的软件替换了 Hypervisor。这也是为什么，很多人会把 Docker 项目称为“轻量级”虚拟化技术的原因，实际上就是把虚拟机的概念套在了容器上。 可是这样的说法，却并不严谨。在理解了 Namespace 的工作方式之后，你就会明白，跟真实存在的虚拟机不同，在使用 Docker 的时候，并没有一个真正的“Docker 容器”运行在宿主机里面。Docker 项目帮助用户启动的，还是原来的应用进程，只不过在创建这些进程时，Docker 为它们加上了各种各样的 Namespace 参数。这时，这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程，只能看到各自 Mount Namespace 里挂载的目录和文件，只能访问到各自 Network Namespace 里的网络设备，就仿佛运行在一个个“容器”里面，与世隔绝。不过，相信你此刻已经会心一笑：这些不过都是“障眼法”罢了。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:7:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"06 | 白话容器基础（二）：隔离与限制 在上一篇文章中，我详细介绍了 Linux 容器中用来实现“隔离”的技术手段：Namespace。而通过这些讲解，你应该能够明白，Namespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。但对于宿主机来说，这些被“隔离”了的进程跟其他进程并没有太大区别。说到这一点，相信你也能够知道我在上一篇文章最后给你留下的第一个思考题的答案了：在之前虚拟机与容器技术的对比图里，不应该把 Docker Engine 或者任何容器管理工具放在跟 Hypervisor 相同的位置，因为它们并不像 Hypervisor 那样对应用进程的隔离环境负责，也不会创建任何实体的“容器”，真正对隔离环境负责的是宿主机操作系统本身： 所以，在这个对比图里，我们应该把 Docker 画在跟应用同级别并且靠边的位置。这意味着，用户运行在容器里的应用进程，跟宿主机上的其他进程一样，都由宿主机操作系统统一管理，只不过这些被隔离的进程拥有额外设置过的 Namespace 参数。而 Docker 项目在这里扮演的角色，更多的是旁路式的辅助和管理工作。我在后续分享 CRI 和容器运行时的时候还会专门介绍，其实像 Docker 这样的角色甚至可以去掉。这样的架构也解释了为什么 Docker 项目比虚拟机更受欢迎的原因。 这是因为，使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。根据实验，一个运行着 CentOS 的 KVM 虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用 100~200 MB 内存。此外，用户应用运行在虚拟机里面，它对宿主机操作系统的调用就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘 I/O 的损耗非常大。而相比之下，容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。所以说，“敏捷”和“高性能”是容器相较于虚拟机最大的优势，也是它能够在 PaaS 这种更细粒度的资源管理平台上大行其道的重要原因。 不过，有利就有弊，基于 Linux Namespace 的隔离机制相比于虚拟化技术也有很多不足之处，其中最主要的问题就是：隔离得不彻底。 首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。尽管你可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。而相比之下，拥有硬件虚拟化技术和独立 Guest OS 的虚拟机就要方便得多了。最极端的例子是，Microsoft 的云计算平台 Azure，实际上就是运行在 Windows 服务器集群上的，但这并不妨碍你在它上面创建各种 Linux 虚拟机出来。 其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。这就意味着，如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。相比于在虚拟机里面可以随便折腾的自由度，在容器里部署应用的时候，“什么能做，什么不能做”，就是用户必须考虑的一个问题。 此外，由于上述问题，尤其是共享宿主机内核的事实，容器给应用暴露出来的攻击面是相当大的，应用“越狱”的难度自然也比虚拟机低得多。更为棘手的是，尽管在实践中我们确实可以使用 Seccomp 等技术，对容器内部发起的所有系统调用进行过滤和甄别来进行安全加固，但这种方法因为多了一层对系统调用的过滤，必然会拖累容器的性能。何况，默认情况下，谁也不知道到底该开启哪些系统调用，禁止哪些系统调用。所以，在生产环境中，没有人敢把运行在物理机上的 Linux 容器直接暴露到公网上。当然，我后续会讲到的基于虚拟化或者独立内核技术的容器实现，则可以比较好地在隔离与性能之间做出平衡。 在介绍完容器的“隔离”技术之后，我们再来研究一下容器的“限制”问题。也许你会好奇，我们不是已经通过 Linux Namespace 创建了一个“容器”吗，为什么还需要对容器做“限制”呢？我还是以 PID Namespace 为例，来给你解释这个问题。虽然容器内的第 1 号进程在“障眼法”的干扰下只能看到容器里的情况，但是宿主机上，它作为第 100 号进程与其他所有进程之间依然是平等的竞争关系。这就意味着，虽然第 100 号进程表面上被隔离了起来，但是它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器）占用的。当然，这个 100 号进程自己也可能把所有资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。 而 Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。有意思的是，Google 的工程师在 2006 年发起这项特性的时候，曾将它命名为“进程容器”（process container）。实际上，在 Google 内部，“容器”这个术语长期以来都被用于形容被 Cgroups 限制过的进程组。后来 Google 的工程师们说，他们的 KVM 虚拟机也运行在 Borg 所管理的“容器”里，其实也是运行在 Cgroups“容器”当中。这和我们今天说的 Docker 容器差别很大。 Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。在今天的分享中，我只和你重点探讨它与容器关系最紧密的“限制”能力，并通过一组实践来带你认识一下 Cgroups。在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。在 Ubuntu 16.04 机器里，我可以用 mount 指令把它们展示出来，这条命令是： $ mount -t cgroup cpuset on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cpu on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu) cpuacct on /sys/fs/cgroup/cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct) blkio on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) memory on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) ... 它的输出结果，是一系列文件系统目录。如果你在自己的机器上没有看到这些目录，那你就需要自己去挂载 Cgroups，具体做法可以自行 Google。可以看到，在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是： $ ls /sys/fs/cgroup/cpu cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 如果熟悉 Linux CPU 管理的话，你就会在它的输出里注意到 cfs_period 和 cfs_quota 这样的关键词。这两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。而这样的配置文件又如何使用呢？你需要在对应的子系统下面创建一个目录，比如，我们现在进入 /sys/fs/cgroup/cpu 目录下： root@ubuntu:/sys/fs/cgroup/cpu$ mkdir container root@ubuntu:/sys/fs/cgroup/cpu$ ls container/ cgroup.clone_childr","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:8:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在这篇文章中，我首先介绍了容器使用 Linux Namespace 作为隔离手段的优势和劣势，对比了 Linux 容器跟虚拟机技术的不同，进一步明确了“容器只是一种特殊的进程”这个结论。除了创建 Namespace 之外，在后续关于容器网络的分享中，我还会介绍一些其他 Namespace 的操作，比如看不见摸不着的 Linux Namespace 在计算机中到底如何表示、一个进程如何“加入”到其他进程的 Namespace 当中，等等。紧接着，我详细介绍了容器在做好了隔离工作之后，又如何通过 Linux Cgroups 实现资源的限制，并通过一系列简单的实验，模拟了 Docker 项目创建容器限制的过程。通过以上讲述，你现在应该能够理解，一个正在运行的 Docker 容器，其实就是一个启用了多个 Linux Namespace 的应用进程，而这个进程能够使用的资源量，则受 Cgroups 配置的限制。 这也是容器技术中一个非常重要的概念，即：容器是一个“单进程”模型。由于一个容器的本质就是一个进程，用户的应用进程实际上就是容器里 PID=1 的进程，也是其他后续创建的所有进程的父进程。这就意味着，在一个容器中，你没办法同时运行两个不同的应用，除非你能事先找到一个公共的 PID=1 的程序来充当两个不同应用的父进程，这也是为什么很多人都会用 systemd 或者 supervisord 这样的软件来代替应用本身作为容器的启动进程。但是，在后面分享容器设计模式时，我还会推荐其他更好的解决办法。这是因为容器本身的设计，就是希望容器和应用能够同生命周期，这个概念对后续的容器编排非常重要。否则，一旦出现类似于“容器是正常运行的，但是里面的应用早已经挂了”的情况，编排系统处理起来就非常麻烦了。另外，跟 Namespace 的情况类似，Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。众所周知，Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。但是，你如果在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。造成这个问题的原因就是，/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：/proc 文件系统不了解 Cgroups 限制的存在。在生产环境中，这个问题必须进行修正，否则应用程序在容器里读取到的 CPU 核数、可用内存等信息都是宿主机上的数据，这会给应用的运行带来非常大的困惑和风险。这也是在企业中，容器化应用碰到的一个常见问题，也是容器相较于虚拟机另一个不尽如人意的地方。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:8:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"07 | 白话容器基础（三）：深入理解容器镜像 在前两次的分享中，我讲解了 Linux 容器最基础的两种技术：Namespace 和 Cgroups。希望此时，你已经彻底理解了“容器的本质是一种特殊的进程”这个最重要的概念。而正如我前面所说的，Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不见的墙。这么一折腾，进程就真的被“装”在了一个与世隔绝的房间里，而这些房间就是 PaaS 项目赖以生存的应用“沙盒”。可是，还有一个问题不知道你有没有仔细思考过：这个房间四周虽然有了墙，但是如果容器进程低头一看地面，又是怎样一副景象呢？换句话说，容器里的进程看到的文件系统又是什么样子的呢？ 可能你立刻就能想到，这一定是一个关于 Mount Namespace 的问题：容器里的应用进程，理应看到一份完全独立的文件系统。这样，它就可以在自己的容器目录（比如 /tmp）下进行操作，而完全不会受宿主机以及其他容器的影响。那么，真实情况是这样吗？“左耳朵耗子”叔在多年前写的一篇关于 Docker 基础知识的博客里，曾经介绍过一段小程序。这段小程序的作用是，在创建子进程时开启指定的 Namespace。下面，我们不妨使用它来验证一下刚刚提到的问题。 c #define _GNU_SOURCE #include \u003csys/mount.h\u003e #include \u003csys/types.h\u003e #include \u003csys/wait.h\u003e #include \u003cstdio.h\u003e #include \u003csched.h\u003e #include \u003csignal.h\u003e #include \u003cunistd.h\u003e #define STACK_SIZE (1024 * 1024) static char container_stack[STACK_SIZE]; char* const container_args[] = { \"/bin/bash\", NULL }; int container_main(void* arg) { printf(\"Container - inside the container!\\n\"); execv(container_args[0], container_args); printf(\"Something's wrong!\\n\"); return 1; } int main() { printf(\"Parent - start a container!\\n\"); int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL); waitpid(container_pid, NULL, 0); printf(\"Parent - container stopped!\\n\"); return 0; } 这段代码的功能非常简单：在 main 函数里，我们通过 clone() 系统调用创建了一个新的子进程 container_main，并且声明要为它启用 Mount Namespace（即：CLONE_NEWNS 标志）。而这个子进程执行的，是一个“/bin/bash”程序，也就是一个 shell。所以这个 shell 就运行在了 Mount Namespace 的隔离环境中。我们来一起编译一下这个程序： $ gcc -o ns ns.c $ ./ns Parent - start a container! Container - inside the container! 这样，我们就进入了这个“容器”当中。可是，如果在“容器”里执行一下 ls 指令的话，我们就会发现一个有趣的现象： /tmp 目录下的内容跟宿主机的内容是一样的。 $ ls /tmp # 你会看到好多宿主机的文件 也就是说：即使开启了 Mount Namespace，容器进程看到的文件系统也跟宿主机完全一样。这是怎么回事呢？ 仔细思考一下，你会发现这其实并不难理解：Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知。但是，这也就意味着，只有在“挂载”这个操作发生之后，进程的视图才会被改变。而在此之前，新创建的容器会直接继承宿主机的各个挂载点。这时，你可能已经想到了一个解决办法：创建新进程时，除了声明要启用 Mount Namespace 之外，我们还可以告诉容器进程，有哪些目录需要重新挂载，就比如这个 /tmp 目录。于是，我们在容器进程执行前可以添加一步重新挂载 /tmp 目录的操作： c int container_main(void* arg) { printf(\"Container - inside the container!\\n\"); // 如果你的机器的根目录的挂载类型是shared，那必须先重新挂载根目录 // mount(\"\", \"/\", NULL, MS_PRIVATE, \"\"); mount(\"none\", \"/tmp\", \"tmpfs\", 0, \"\"); execv(container_args[0], container_args); printf(\"Something's wrong!\\n\"); return 1; } 可以看到，在修改后的代码里，我在容器进程启动之前，加上了一句 mount(“none”, “/tmp”, “tmpfs”, 0, “”) 语句。就这样，我告诉了容器以 tmpfs（内存盘）格式，重新挂载了 /tmp 目录。这段修改后的代码，编译执行后的结果又如何呢？我们可以试验一下： $ gcc -o ns ns.c $ ./ns Parent - start a container! Container - inside the container! $ ls /tmp 可以看到，这次 /tmp 变成了一个空目录，这意味着重新挂载生效了。我们可以用 mount -l 检查一下： $ mount -l | grep tmpfs none on /tmp type tmpfs (rw,relatime) 可以看到，容器里的 /tmp 目录是以 tmpfs 方式单独挂载的。更重要的是，因为我们创建的新进程启用了 Mount Namespace，所以这次重新挂载的操作，只在容器进程的 Mount Namespace 中有效。如果在宿主机上用 mount -l 来检查一下这个挂载，你会发现它是不存在的： # 在宿主机上 $ mount -l | grep tmpfs 这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。 可是，作为一个普通用户，我们希望的是一个更友好的情况：每当创建一个新容器时，我希望容器进程看到的文件系统就是一个独立的隔离环境，而不是继承自宿主机的文件系统。怎么才能做到这一点呢？不难想到，我们可以在容器进程启动之前重新挂载它的整个根目录“/”。而由于 Mount Namespace 的存在，这个挂载对宿主机不可见，所以容器进程就可以在里面随便折腾了。在 Linux 操作系统里，有一个名为 chroot 的命令可以帮助你在 shell 中方便地完成这个工作。顾名思义，它的作用就是帮你“change root file system”，即改变进程的根目录到你指定的位置。它的用法也非常简单。假设，我们现在有一个 $HOME/test 目录，想要把它作为一个 /bin/bash 进程的根目录。首先，创建一个 test 目录和几个 lib 文件夹： $ mkdir -p $HOME/test $ mkdir -p $HOME/test/{bin,lib64,lib} $ cd $T 然后，把 bash 命令拷贝到 test 目录对应的 bin 路径下： $ cp -v /bin/{bash,ls} $HOME/test/bin 接下来，把 bash 命令需要的所有 so 文件，也拷贝到 test 目录对应的 lib 路径下。找到 so 文件可以用 ldd 命令： $ T=$HOME/test $ list=\"$(ldd /bin/ls | egrep -o '/lib.*\\.[0-9]')\" $ for i in $list; do cp -v \"$i\" \"${T}${i}\"; done 最后，执行 chroot 命令，告诉操作系统，我们将使用 $HOME/test 目录作为 /bin/bash 进程的根目录： $ chroot $HOME/test /bin/bash 这时，你如果执行 “ls /\"，就会看到，它返回的都是 $HOME/test 目录下面的内容，而不是宿主机的内容。更重要的是，对于被 chroot 的进程来说，它并不会感受到自己的根目录已经被“修改”成 $HOME/test 了。这种视图被修改的原理，是不是跟我之前介绍的 Linux Namespace 很类似呢？没错！ 实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。当然，为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:9:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天的分享中，我着重介绍了 Linux 容器文件系统的实现方式。而这种机制，正是我们经常提到的容器镜像，也叫作：rootfs。它只是一个操作系统的所有文件和目录，并不包含内核，最多也就几百兆。而相比之下，传统虚拟机的镜像大多是一个磁盘的“快照”，磁盘有多大，镜像就至少有多大。通过结合使用 Mount Namespace 和 rootfs，容器就能够为进程构建出一个完善的文件系统隔离环境。当然，这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力。而在 rootfs 的基础上，Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案，这就是容器镜像中“层”的概念。通过“分层镜像”的设计，以 Docker 镜像为核心，来自不同公司、不同团队的技术人员被紧密地联系在了一起。而且，由于容器镜像的操作是增量式的，这样每次镜像拉取、推送的内容，比原本多个完整的操作系统的大小要小得多；而共享层的存在，可以使得所有这些容器镜像需要的总空间，也比每个镜像的总和要小。这样就使得基于容器镜像的团队协作，要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多。更重要的是，一旦这个镜像被发布，那么你在全世界的任何一个地方下载这个镜像，得到的内容都完全一致，可以完全复现这个镜像制作者当初的完整环境。这，就是容器技术“强一致性”的重要体现。而这种价值正是支撑 Docker 公司在 2014~2016 年间迅猛发展的核心动力。容器镜像的发明，不仅打通了“开发 - 测试 - 部署”流程的每一个环节，更重要的是： 容器镜像将会成为未来软件的主流发布方式。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:9:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"08 | 白话容器基础（四）：重新认识Docker容器 在前面的三次分享中，我分别从 Linux Namespace 的隔离能力、Linux Cgroups 的限制能力，以及基于 rootfs 的文件系统三个角度，为你剖析了一个 Linux 容器的核心实现原理。备注：之所以要强调 Linux 容器，是因为比如 Docker on Mac，以及 Windows Docker（Hyper-V 实现），实际上是基于虚拟化技术实现的，跟我们这个专栏着重介绍的 Linux 容器完全不同。而在今天的分享中，我会通过一个实际案例，对“白话容器基础”系列的所有内容做一次深入的总结和扩展。希望通过这次的讲解，能够让你更透彻地理解 Docker 容器的本质。在开始实践之前，你需要准备一台 Linux 机器，并安装 Docker。这个流程我就不再赘述了。这一次，我要用 Docker 部署一个用 Python 编写的 Web 应用。这个应用的代码部分（app.py）非常简单： py from flask import Flask import socket import os app = Flask(__name__) @app.route('/') def hello(): html = \"\u003ch3\u003eHello {name}!\u003c/h3\u003e\" \\ \"\u003cb\u003eHostname:\u003c/b\u003e {hostname}\u003cbr/\u003e\" return html.format(name=os.getenv(\"NAME\", \"world\"), hostname=socket.gethostname()) if __name__ == \"__main__\": app.run(host='0.0.0.0', port=80) 在这段代码中，我使用 Flask 框架启动了一个 Web 服务器，而它唯一的功能是：如果当前环境中有“NAME”这个环境变量，就把它打印在“Hello”之后，否则就打印“Hello world”，最后再打印出当前环境的 hostname。这个应用的依赖，则被定义在了同目录下的 requirements.txt 文件里，内容如下所示： $ cat requirements.txt Flask 而将这样一个应用容器化的第一步，是制作容器镜像。不过，相较于我之前介绍的制作 rootfs 的过程，Docker 为你提供了一种更便捷的方式，叫作 Dockerfile，如下所示。 # 使用官方提供的Python开发镜像作为基础镜像 FROM python:2.7-slim # 将工作目录切换为/app WORKDIR /app # 将当前目录下的所有内容复制到/app下 ADD . /app # 使用pip命令安装这个应用所需要的依赖 RUN pip install --trusted-host pypi.python.org -r requirements.txt # 允许外界访问容器的80端口 EXPOSE 80 # 设置环境变量 ENV NAME World # 设置容器进程为：python app.py，即：这个Python应用的启动命令 CMD [\"python\", \"app.py\"] 通过这个文件的内容，你可以看到 Dockerfile 的设计思想，是使用一些标准的原语（即大写高亮的词语），描述我们所要构建的 Docker 镜像。并且这些原语，都是按顺序处理的。比如 FROM 原语，指定了“python:2.7-slim”这个官方维护的基础镜像，从而免去了安装 Python 等语言环境的操作。否则，这一段我们就得这么写了： FROM ubuntu:latest RUN apt-get update -yRUN apt-get install -y python-pip python-dev build-essential ... 其中，RUN 原语就是在容器里执行 shell 命令的意思。而 WORKDIR，意思是在这一句之后，Dockerfile 后面的操作都以这一句指定的 /app 目录作为当前目录。所以，到了最后的 CMD，意思是 Dockerfile 指定 python app.py 为这个容器的进程。这里，app.py 的实际路径是 /app/app.py。所以，CMD [“python”, “app.py”]等价于\"docker run \u003c image\u003e python app.py”。另外，在使用 Dockerfile 时，你可能还会看到一个叫作 ENTRYPOINT 的原语。实际上，它和 CMD 都是 Docker 容器进程启动所必需的参数，完整执行格式是：“ENTRYPOINT CMD”。 但是，默认情况下，Docker 会为你提供一个隐含的 ENTRYPOINT，即：/bin/sh -c。所以，在不指定 ENTRYPOINT 时，比如在我们这个例子里，实际上运行在容器里的完整进程是：/bin/sh -c “python app.py”，即 CMD 的内容就是 ENTRYPOINT 的参数。备注：基于以上原因，我们后面会统一称 Docker 容器的启动进程为 ENTRYPOINT，而不是 CMD。 需要注意的是，Dockerfile 里的原语并不都是指对容器内部的操作。就比如 ADD，它指的是把当前目录（即 Dockerfile 所在的目录）里的文件，复制到指定容器内的目录当中。读懂这个 Dockerfile 之后，我再把上述内容，保存到当前目录里一个名叫“Dockerfile”的文件中： $ ls Dockerfile app.py requirements.txt 接下来，我就可以让 Docker 制作这个镜像了，在当前目录执行： $ docker build -t helloworld . 其中，-t 的作用是给这个镜像加一个 Tag，即：起一个好听的名字。docker build 会自动加载当前目录下的 Dockerfile 文件，然后按照顺序，执行文件中的原语。而这个过程，实际上可以等同于 Docker 使用基础镜像启动了一个容器，然后在容器中依次执行 Dockerfile 中的原语。需要注意的是，Dockerfile 中的每个原语执行后，都会生成一个对应的镜像层。即使原语本身并没有明显地修改文件的操作（比如，ENV 原语），它对应的层也会存在。只不过在外界看来，这个层是空的。docker build 操作完成后，我可以通过 docker images 命令查看结果： $ docker image ls REPOSITORY TAG IMAGE ID helloworld latest 653287cdf998 通过这个镜像 ID，你就可以使用在《白话容器基础（三）：深入理解容器镜像》中讲过的方法，查看这些新增的层在 AuFS 路径下对应的文件和目录了。接下来，我使用这个镜像，通过 docker run 命令启动容器： $ docker run -p 4000:80 helloworld 在这一句命令中，镜像名 helloworld 后面，我什么都不用写，因为在 Dockerfile 中已经指定了 CMD。否则，我就得把进程的启动命令加在后面： $ docker run -p 4000:80 helloworld python app.py 容器启动之后，我可以使用 docker ps 命令看到： $ docker ps CONTAINER ID IMAGE COMMAND CREATED 4ddf4638572d helloworld \"python app.py\" 10 seconds ago 同时，我已经通过 -p 4000:80 告诉了 Docker，请把容器内的 80 端口映射在宿主机的 4000 端口上。这样做的目的是，只要访问宿主机的 4000 端口，我就可以看到容器里应用返回的结果： $ curl http://localhost:4000 \u003ch3\u003eHello World!\u003c/h3\u003e\u003cb\u003eHostname:\u003c/b\u003e 4ddf4638572d\u003cbr/\u003e 否则，我就得先用 docker inspect 命令查看容器的 IP 地址，然后访问“http://\u003c 容器 IP 地址 \u003e:80”才可以看到容器内应用的返回。至此，我已经使用容器完成了一个应用的开发与测试，如果现在想要把这个容器的镜像上传到 DockerHub 上分享给更多的人，我要怎么做呢？为了能够上传镜像，我首先需要注册一个 Docker Hub 账号，然后使用 docker login 命令登录。接下来，我要用 docker tag 命令给容器镜像起一个完整的名字： $ docker tag helloworld geektime/helloworld:v1 注意：你自己做实验时，请将\"geektime\"替换成你自己的 Docker Hub 账户名称，比如 zhangsan/helloworld:v1其中，geektime 是我在 Docker Hub 上的用户名，它的“学名”叫镜像仓库（Repository）；“/”后面的 helloworld 是这个镜像的名字，而“v1”则是我给这个镜像分配的版本号。然后，我执行 docker push： $ docker push geektime/helloworld:v1 这样，我就可以把这个镜像上传到 Docker Hub 上了。此外，我还可以使用 docker commit 指令，把一个正在运行的容器，直接提交为一个镜像。一般来说，需要这么","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:10:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天的这次分享中，我用了一个非常经典的 Python 应用作为案例，讲解了 Docke 容器使用的主要场景。熟悉了这些操作，你也就基本上摸清了 Docker 容器的核心功能。更重要的是，我着重介绍了如何使用 Linux Namespace、Cgroups，以及 rootfs 的知识，对容器进行了一次庖丁解牛似的解读。借助这种思考问题的方法，最后的 Docker 容器，我们实际上就可以用下面这个“全景图”描述出来： 这个容器进程“python app.py”，运行在由 Linux Namespace 和 Cgroups 构成的隔离环境里；而它运行所需要的各种文件，比如 python，app.py，以及整个操作系统文件，则由多个联合挂载在一起的 rootfs 层提供。这些 rootfs 层的最下层，是来自 Docker 镜像的只读层。在只读层之上，是 Docker 自己添加的 Init 层，用来存放被临时修改过的 /etc/hosts 等文件。而 rootfs 的最上层是一个可读写层，它以 Copy-on-Write 的方式存放任何对只读层的修改，容器声明的 Volume 的挂载点，也出现在这一层。通过这样的剖析，对于曾经“神秘莫测”的容器技术，你是不是感觉清晰了很多呢？ ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:10:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"09 | 从容器到容器云：谈谈Kubernetes的本质 在前面的四篇文章中，我以 Docker 项目为例，一步步剖析了 Linux 容器的具体实现方式。通过这些讲解你应该能够明白：一个“容器”，实际上是一个由 Linux Namespace、Linux Cgroups 和 rootfs 三种技术构建出来的进程的隔离环境。从这个结构中我们不难看出，一个正在运行的 Linux 容器，其实可以被“一分为二”地看待：一组联合挂载在 /var/lib/docker/aufs/mnt 上的 rootfs，这一部分我们称为“容器镜像”（Container Image），是容器的静态视图；一个由 Namespace+Cgroups 构成的隔离环境，这一部分我们称为“容器运行时”（Container Runtime），是容器的动态视图。 更进一步地说，作为一名开发者，我并不关心容器运行时的差异。因为，在整个“开发 - 测试 - 发布”的流程中，真正承载着容器信息进行传递的，是容器镜像，而不是容器运行时。这个重要假设，正是容器技术圈在 Docker 项目成功后不久，就迅速走向了“容器编排”这个“上层建筑”的主要原因：作为一家云服务商或者基础设施提供商，我只要能够将用户提交的 Docker 镜像以容器的方式运行起来，就能成为这个非常热闹的容器生态图上的一个承载点，从而将整个容器技术栈上的价值，沉淀在我的这个节点上。更重要的是，只要从我这个承载点向 Docker 镜像制作者和使用者方向回溯，整条路径上的各个服务节点，比如 CI/CD、监控、安全、网络、存储等等，都有我可以发挥和盈利的余地。这个逻辑，正是所有云计算提供商如此热衷于容器技术的重要原因：通过容器镜像，它们可以和潜在用户（即，开发者）直接关联起来。从一个开发者和单一的容器镜像，到无数开发者和庞大的容器集群，容器技术实现了从“容器”到“容器云”的飞跃，标志着它真正得到了市场和生态的认可。 这样，容器就从一个开发者手里的小工具，一跃成为了云计算领域的绝对主角；而能够定义容器组织和管理规范的“容器编排”技术，则当仁不让地坐上了容器技术领域的“头把交椅”。 这其中，最具代表性的容器编排工具，当属 Docker 公司的 Compose+Swarm 组合，以及 Google 与 RedHat 公司共同主导的 Kubernetes 项目。我在前面介绍容器技术发展历史的四篇预习文章中，已经对这两个开源项目做了详细的剖析和评述。所以，在今天的这次分享中，我会专注于本专栏的主角 Kubernetes 项目，谈一谈它的设计与架构。跟很多基础设施领域先有工程实践、后有方法论的发展路线不同，Kubernetes 项目的理论基础则要比工程实践走得靠前得多，这当然要归功于 Google 公司在 2015 年 4 月发布的 Borg 论文了。Borg 系统，一直以来都被誉为 Google 公司内部最强大的“秘密武器”。虽然略显夸张，但这个说法倒不算是吹牛。因为，相比于 Spanner、BigTable 等相对上层的项目，Borg 要承担的责任，是承载 Google 公司整个基础设施的核心依赖。在 Google 公司已经公开发表的基础设施体系论文中，Borg 项目当仁不让地位居整个基础设施技术栈的最底层。 图片来源：Malte Schwarzkopf. “Operating system support for warehouse-scale computing”. PhD thesis. University of Cambridge Computer Laboratory (to appear), 2015, Chapter 2. 上面这幅图，来自于 Google Omega 论文的第一作者的博士毕业论文。它描绘了当时 Google 已经公开发表的整个基础设施栈。在这个图里，你既可以找到 MapReduce、BigTable 等知名项目，也能看到 Borg 和它的继任者 Omega 位于整个技术栈的最底层。 正是由于这样的定位，Borg 可以说是 Google 最不可能开源的一个项目。而幸运的是，得益于 Docker 项目和容器技术的风靡，它却终于得以以另一种方式与开源社区见面，这个方式就是 Kubernetes 项目。所以，相比于“小打小闹”的 Docker 公司、“旧瓶装新酒”的 Mesos 社区，Kubernetes 项目从一开始就比较幸运地站上了一个他人难以企及的高度：在它的成长阶段，这个项目每一个核心特性的提出，几乎都脱胎于 Borg/Omega 系统的设计与经验。更重要的是，这些特性在开源社区落地的过程中，又在整个社区的合力之下得到了极大的改进，修复了很多当年遗留在 Borg 体系中的缺陷和问题。 所以，尽管在发布之初被批评是“曲高和寡”，但是在逐渐觉察到 Docker 技术栈的“稚嫩”和 Mesos 社区的“老迈”之后，这个社区很快就明白了：Kubernetes 项目在 Borg 体系的指导下，体现出了一种独有的“先进性”与“完备性”，而这些特质才是一个基础设施领域开源项目赖以生存的核心价值。为了更好地理解这两种特质，我们不妨从 Kubernetes 的顶层设计说起。 首先，Kubernetes 项目要解决的问题是什么？编排？调度？容器云？还是集群管理？实际上，这个问题到目前为止都没有固定的答案。因为在不同的发展阶段，Kubernetes 需要着重解决的问题是不同的。但是，对于大多数用户来说，他们希望 Kubernetes 项目带来的体验是确定的：现在我有了应用的容器镜像，请帮我在一个给定的集群上把这个应用运行起来。更进一步地说，我还希望 Kubernetes 能给我提供路由网关、水平扩展、监控、备份、灾难恢复等一系列运维能力。 等一下，这些功能听起来好像有些耳熟？这不就是经典 PaaS（比如，Cloud Foundry）项目的能力吗？而且，有了 Docker 之后，我根本不需要什么 Kubernetes、PaaS，只要使用 Docker 公司的 Compose+Swarm 项目，就完全可以很方便地 DIY 出这些功能了！所以说，如果 Kubernetes 项目只是停留在拉取用户镜像、运行容器，以及提供常见的运维功能的话，那么别说跟“原生”的 Docker Swarm 项目竞争了，哪怕跟经典的 PaaS 项目相比也难有什么优势可言。 而实际上，在定义核心功能的过程中，Kubernetes 项目正是依托着 Borg 项目的理论优势，才在短短几个月内迅速站稳了脚跟，进而确定了一个如下图所示的全局架构： 我们可以看到，Kubernetes 项目的架构，跟它的原型项目 Borg 非常类似，都由 Master 和 Node 两种节点组成，而这两种角色分别对应着控制节点和计算节点。其中，控制节点，即 Master 节点，由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube-apiserver、负责调度的 kube-scheduler，以及负责容器编排的 kube-controller-manager。整个集群的持久化数据，则由 kube-apiserver 处理后保存在 Etcd 中。而计算节点上最核心的部分，则是一个叫作 kubelet 的组件。 在 Kubernetes 项目中，kubelet 主要负责同容器运行时（比如 Docker 项目）打交道。而这个交互所依赖的，是一个称作 CRI（Container Runtime Interface）的远程调用接口，这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数。这也是为何，Kubernetes 项目并不关心你部署的是什么容器运行时、使用的什么技术实现，只要你的这个容器运行时能够运行标准的容器镜像，它就可以通过实现 CRI 接入到 Kubernetes 项目当中。而具体的容器运行时，比如 Docker 项目，则一般通过 OCI 这个容器运行时规范同底层的 Linux 操作系统进行交互，即：把 CRI 请求翻译成对 Linux 操作系统的调用（操作 Linux Namespace 和 Cgroups 等）。 此外，kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。而 kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储。这两个插件与 kubelet 进行交互的接口，分别是 CNI（Container Networking Interface）和 CSI（Container Storage Interface）。实际上，kubelet 这个奇怪的名字，来自于 Borg 项目里的同源组件 Borglet。不过，如果你浏览过 Borg 论文的话，就会发现，这个命名方式可能是 kubelet 组件与 Borglet 组件的唯一相似之处。因为 Borg 项目，并不支持我们这里所讲的容器技术，而只是简单地使用了 Linux Cgroups 对进程进行限制。 这就意味着，像 Docker 这样的“容器镜像”在 Borg 中是不存在的，Borglet 组件也自然不需要像 kubelet 这样考虑如何同 Docker 进行交互、如何对容器镜像进行管理的问题，也不需要支持 CRI、CNI、CSI 等诸多容器技术接口。","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:11:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 首先，我和你一起回顾了容器的核心知识，说明了容器其实可以分为两个部分：容器运行时和容器镜像。然后，我重点介绍了 Kubernetes 项目的架构，详细讲解了它如何使用“声明式 API”来描述容器化业务和容器间关系的设计思想。实际上，过去很多的集群管理项目（比如 Yarn、Mesos，以及 Swarm）所擅长的，都是把一个容器，按照某种规则，放置在某个最佳节点上运行起来。这种功能，我们称为“调度”。而 Kubernetes 项目所擅长的，是按照用户的意愿和整个系统的规则，完全自动化地处理好容器之间的各种关系。这种功能，就是我们经常听到的一个概念：编排。所以说，Kubernetes 项目的本质，是为用户提供一个具有普遍意义的容器编排工具。不过，更重要的是，Kubernetes 项目为用户提供的不仅限于一个工具。它真正的价值，乃在于提供了一套基于容器构建分布式系统的基础依赖。关于这一点，相信你会在今后的学习中，体会越来越深。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:11:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"k8s集群搭建和实践 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:12:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"10 | Kubernetes一键部署利器：kubeadm 通过前面几篇文章的内容，我其实阐述了这样一个思想：要真正发挥容器技术的实力，你就不能仅仅局限于对 Linux 容器本身的钻研和使用。这些知识更适合作为你的技术储备，以便在需要的时候可以帮你更快地定位问题，并解决问题。而更深入地学习容器技术的关键在于，如何使用这些技术来“容器化”你的应用。比如，我们的应用既可能是 Java Web 和 MySQL 这样的组合，也可能是 Cassandra 这样的分布式系统。而要使用容器把后者运行起来，你单单通过 Docker 把一个 Cassandra 镜像跑起来是没用的。要把 Cassandra 应用容器化的关键，在于如何处理好这些 Cassandra 容器之间的编排关系。比如，哪些 Cassandra 容器是主，哪些是从？主从容器如何区分？它们之间又如何进行自动发现和通信？Cassandra 容器的持久化数据又如何保持，等等。这也是为什么我们要反复强调 Kubernetes 项目的主要原因：这个项目体现出来的容器化“表达能力”，具有独有的先进性和完备性。这就使得它不仅能运行 Java Web 与 MySQL 这样的常规组合，还能够处理 Cassandra 容器集群等复杂编排问题。所以，对这种编排能力的剖析、解读和最佳实践，将是本专栏最重要的一部分内容。 不过，万事开头难。作为一个典型的分布式项目，Kubernetes 的部署一直以来都是挡在初学者前面的一只“拦路虎”。尤其是在 Kubernetes 项目发布初期，它的部署完全要依靠一堆由社区维护的脚本。其实，Kubernetes 作为一个 Golang 项目，已经免去了很多类似于 Python 项目要安装语言级别依赖的麻烦。但是，除了将各个组件编译成二进制文件外，用户还要负责为这些二进制文件编写对应的配置文件、配置自启动脚本，以及为 kube-apiserver 配置授权文件等等诸多运维工作。目前，各大云厂商最常用的部署的方法，是使用 SaltStack、Ansible 等运维工具自动化地执行这些步骤。但即使这样，这个部署过程依然非常繁琐。因为，SaltStack 这类专业运维工具本身的学习成本，就可能比 Kubernetes 项目还要高。 难道 Kubernetes 项目就没有简单的部署方法了吗？ 这个问题，在 Kubernetes 社区里一直没有得到足够重视。直到 2017 年，在志愿者的推动下，社区才终于发起了一个独立的部署工具，名叫：kubeadm。这个项目的目的，就是要让用户能够通过这样两条指令完成一个 Kubernetes 集群的部署： # 创建一个Master节点 $ kubeadm init # 将一个Node节点加入到当前集群中 $ kubeadm join \u003cMaster节点的IP和端口\u003e 是不是非常方便呢？不过，你可能也会有所顾虑：Kubernetes 的功能那么多，这样一键部署出来的集群，能用于生产环境吗？为了回答这个问题，在今天这篇文章，我就先和你介绍一下 kubeadm 的工作原理吧。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:13:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"kubeadm 的工作原理 在上一篇文章《从容器到容器云：谈谈 Kubernetes 的本质》中，我已经详细介绍了 Kubernetes 的架构和它的组件。在部署时，它的每一个组件都是一个需要被执行的、单独的二进制文件。所以不难想象，SaltStack 这样的运维工具或者由社区维护的脚本的功能，就是要把这些二进制文件传输到指定的机器当中，然后编写控制脚本来启停这些组件。不过，在理解了容器技术之后，你可能已经萌生出了这样一个想法，为什么不用容器部署 Kubernetes 呢？ 这样，我只要给每个 Kubernetes 组件做一个容器镜像，然后在每台宿主机上用 docker run 指令启动这些组件容器，部署不就完成了吗？事实上，在 Kubernetes 早期的部署脚本里，确实有一个脚本就是用 Docker 部署 Kubernetes 项目的，这个脚本相比于 SaltStack 等的部署方式，也的确简单了不少。但是，这样做会带来一个很麻烦的问题，即：如何容器化 kubelet。 我在上一篇文章中，已经提到 kubelet 是 Kubernetes 项目用来操作 Docker 等容器运行时的核心组件。可是，除了跟容器运行时打交道外，kubelet 在配置容器网络、管理容器数据卷时，都需要直接操作宿主机。而如果现在 kubelet 本身就运行在一个容器里，那么直接操作宿主机就会变得很麻烦。对于网络配置来说还好，kubelet 容器可以通过不开启 Network Namespace（即 Docker 的 host network 模式）的方式，直接共享宿主机的网络栈。可是，要让 kubelet 隔着容器的 Mount Namespace 和文件系统，操作宿主机的文件系统，就有点儿困难了。 比如，如果用户想要使用 NFS 做容器的持久化数据卷，那么 kubelet 就需要在容器进行绑定挂载前，在宿主机的指定目录上，先挂载 NFS 的远程目录。可是，这时候问题来了。由于现在 kubelet 是运行在容器里的，这就意味着它要做的这个“mount -F nfs”命令，被隔离在了一个单独的 Mount Namespace 中。即，kubelet 做的挂载操作，不能被“传播”到宿主机上。对于这个问题，有人说，可以使用 setns() 系统调用，在宿主机的 Mount Namespace 中执行这些挂载操作；也有人说，应该让 Docker 支持一个–mnt=host 的参数。但是，到目前为止，在容器里运行 kubelet，依然没有很好的解决办法，我也不推荐你用容器去部署 Kubernetes 项目。正因为如此，kubeadm 选择了一种妥协方案： 把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。 所以，你使用 kubeadm 的第一步，是在机器上手动安装 kubeadm、kubelet 和 kubectl 这三个二进制文件。当然，kubeadm 的作者已经为各个发行版的 Linux 准备好了安装包，所以你只需要执行： $ apt-get install kubeadm 就可以了。接下来，你就可以使用“kubeadm init”部署 Master 节点了。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:13:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"kubeadm init 的工作流程 当你执行 kubeadm init 指令后，kubeadm 首先要做的，是一系列的检查工作，以确定这台机器可以用来部署 Kubernetes。这一步检查，我们称为“Preflight Checks”，它可以为你省掉很多后续的麻烦。 其实，Preflight Checks 包括了很多方面，比如： Linux 内核的版本必须是否是 3.10 以上？Linux Cgroups 模块是否可用？机器的 hostname 是否标准？在 Kubernetes 项目里，机器的名字以及一切存储在 Etcd 中的 API 对象，都必须使用标准的 DNS 命名（RFC 1123）。用户安装的 kubeadm 和 kubelet 的版本是否匹配？机器上是不是已经安装了 Kubernetes 的二进制文件？Kubernetes 的工作端口 10250/10251/10252 端口是不是已经被占用？ip、mount 等 Linux 指令是否存在？Docker 是否已经安装？…… 在通过了 Preflight Checks 之后，kubeadm 要为你做的，是生成 Kubernetes 对外提供服务所需的各种证书和对应的目录。 Kubernetes 对外提供服务时，除非专门开启“不安全模式”，否则都要通过 HTTPS 才能访问 kube-apiserver。这就需要为 Kubernetes 集群配置好证书文件。kubeadm 为 Kubernetes 项目生成的证书文件都放在 Master 节点的 /etc/kubernetes/pki 目录下。在这个目录下，最主要的证书文件是 ca.crt 和对应的私钥 ca.key。此外，用户使用 kubectl 获取容器日志等 streaming 操作时，需要通过 kube-apiserver 向 kubelet 发起请求，这个连接也必须是安全的。kubeadm 为这一步生成的是 apiserver-kubelet-client.crt 文件，对应的私钥是 apiserver-kubelet-client.key。除此之外，Kubernetes 集群中还有 Aggregate APIServer 等特性，也需要用到专门的证书，这里我就不再一一列举了。需要指出的是，你可以选择不让 kubeadm 为你生成这些证书，而是拷贝现有的证书到如下证书的目录里： /etc/kubernetes/pki/ca.{crt,key} 这时，kubeadm 就会跳过证书生成的步骤，把它完全交给用户处理。证书生成后，kubeadm 接下来会为其他组件生成访问 kube-apiserver 所需的配置文件。这些文件的路径是：/etc/kubernetes/xxx.conf： ls /etc/kubernetes/ admin.conf controller-manager.conf kubelet.conf scheduler.conf 这些文件里面记录的是，当前这个 Master 节点的服务器地址、监听端口、证书目录等信息。这样，对应的客户端（比如 scheduler，kubelet 等），可以直接加载相应的文件，使用里面的信息与 kube-apiserver 建立安全连接。接下来，kubeadm 会为 Master 组件生成 Pod 配置文件。我已经在上一篇文章中和你介绍过 Kubernetes 有三个 Master 组件 kube-apiserver、kube-controller-manager、kube-scheduler，而它们都会被使用 Pod 的方式部署起来。你可能会有些疑问：这时，Kubernetes 集群尚不存在，难道 kubeadm 会直接执行 docker run 来启动这些容器吗？ 当然不是。在 Kubernetes 中，有一种特殊的容器启动方法叫做“Static Pod”。它允许你把要部署的 Pod 的 YAML 文件放在一个指定的目录里。这样，当这台机器上的 kubelet 启动时，它会自动检查这个目录，加载所有的 Pod YAML 文件，然后在这台机器上启动它们。从这一点也可以看出，kubelet 在 Kubernetes 项目中的地位非常高，在设计上它就是一个完全独立的组件，而其他 Master 组件，则更像是辅助性的系统容器。在 kubeadm 中，Master 组件的 YAML 文件会被生成在 /etc/kubernetes/manifests 路径下。比如，kube-apiserver.yaml： yaml apiVersion: v1 kind: Pod metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --authorization-mode=Node,RBAC - --runtime-config=api/all=true - --advertise-address=10.168.0.2 ... - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key image: k8s.gcr.io/kube-apiserver-amd64:v1.11.1 imagePullPolicy: IfNotPresent livenessProbe: ... name: kube-apiserver resources: requests: cpu: 250m volumeMounts: - mountPath: /usr/share/ca-certificates name: usr-share-ca-certificates readOnly: true ... hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/ca-certificates type: DirectoryOrCreate name: etc-ca-certificates ... 关于一个 Pod 的 YAML 文件怎么写、里面的字段如何解读，我会在后续专门的文章中为你详细分析。在这里，你只需要关注这样几个信息：这个 Pod 里只定义了一个容器，它使用的镜像是：k8s.gcr.io/kube-apiserver-amd64:v1.11.1 。这个镜像是 Kubernetes 官方维护的一个组件镜像。这个容器的启动命令（commands）是 kube-apiserver –authorization-mode=Node,RBAC …，这样一句非常长的命令。其实，它就是容器里 kube-apiserver 这个二进制文件再加上指定的配置参数而已。如果你要修改一个已有集群的 kube-apiserver 的配置，需要修改这个 YAML 文件。这些组件的参数也可以在部署时指定，我很快就会讲到。 在这一步完成后，kubeadm 还会再生成一个 Etcd 的 Pod YAML 文件，用来通过同样的 Static Pod 的方式启动 Etcd。所以，最后 Master 组件的 Pod YAML 文件如下所示： $ ls /etc/kubernetes/manifests/ etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml 而一旦这些 YAML 文件出现在被 kubelet 监视的 /etc/kubernetes/manifests 目录下，kubelet 就会自动创建这些 YAML 文件中定义的 Pod，即 Master 组件的容器。Master 容器启动后，kubeadm 会通过检查 localhost:6443/healthz 这个 Master 组件的健康检查 URL，等待 Master 组件完全运行起来。然后，kubeadm 就会为集群生成一个 bootstrap token。在后面，只要持有这个 token，任何一个安装了 kubelet 和 kubadm 的节点，都可以通过 kubeadm join 加入到这个集群当中。这个 token 的值和使用方法，会在 kubeadm init 结束后被打印出来。 在 token 生成之后，kubeadm 会将 ca.crt 等 Master 节点的重要信息，通过 ConfigMap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用。这个 ConfigMap 的名字是 cluster-info。kubeadm init 的最后一步，就是安装默认插件。Kubernetes 默认 kube-proxy 和 DNS 这两个插件是必须安装的。它们分别用来提供整个集群的服务发现和 DNS 功能。其实，这两个插件也只是两个容器镜像而已，所以 kubeadm 只要用 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:13:2","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"kubeadm join 的工作流程 这个流程其实非常简单，kubeadm init 生成 bootstrap token 之后，你就可以在任意一台安装了 kubelet 和 kubeadm 的机器上执行 kubeadm join 了。可是，为什么执行 kubeadm join 需要这样一个 token 呢？因为，任何一台机器想要成为 Kubernetes 集群中的一个节点，就必须在集群的 kube-apiserver 上注册。可是，要想跟 apiserver 打交道，这台机器就必须要获取到相应的证书文件（CA 文件）。可是，为了能够一键安装，我们就不能让用户去 Master 节点上手动拷贝这些文件。所以，kubeadm 至少需要发起一次“不安全模式”的访问到 kube-apiserver，从而拿到保存在 ConfigMap 中的 cluster-info（它保存了 APIServer 的授权信息）。而 bootstrap token，扮演的就是这个过程中的安全验证的角色。只要有了 cluster-info 里的 kube-apiserver 的地址、端口、证书，kubelet 就可以以“安全模式”连接到 apiserver 上，这样一个新的节点就部署完成了。接下来，你只要在其他节点上重复这个指令就可以了。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:13:3","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"配置 kubeadm 的部署参数 我在前面讲了 kubeadm 部署 Kubernetes 集群最关键的两个步骤，kubeadm init 和 kubeadm join。相信你一定会有这样的疑问：kubeadm 确实简单易用，可是我又该如何定制我的集群组件参数呢？比如，我要指定 kube-apiserver 的启动参数，该怎么办？在这里，我强烈推荐你在使用 kubeadm init 部署 Master 节点时，使用下面这条指令： $ kubeadm init --config kubeadm.yaml 这时，你就可以给 kubeadm 提供一个 YAML 文件（比如，kubeadm.yaml），它的内容如下所示（我仅列举了主要部分）： yaml apiVersion: kubeadm.k8s.io/v1alpha2 kind: MasterConfiguration kubernetesVersion: v1.11.0 api: advertiseAddress: 192.168.0.102 bindPort: 6443 ... etcd: local: dataDir: /var/lib/etcd image: \"\" imageRepository: k8s.gcr.io kubeProxy: config: bindAddress: 0.0.0.0 ... kubeletConfiguration: baseConfig: address: 0.0.0.0 ... networking: dnsDomain: cluster.local podSubnet: \"\" serviceSubnet: 10.96.0.0/12 nodeRegistration: criSocket: /var/run/dockershim.sock ... 通过制定这样一个部署参数配置文件，你就可以很方便地在这个文件里填写各种自定义的部署参数了。比如，我现在要指定 kube-apiserver 的参数，那么我只要在这个文件里加上这样一段信息： ... apiServerExtraArgs: advertise-address: 192.168.0.103 anonymous-auth: false enable-admission-plugins: AlwaysPullImages,DefaultStorageClass audit-log-path: /home/johndoe/audit.log 然后，kubeadm 就会使用上面这些信息替换 /etc/kubernetes/manifests/kube-apiserver.yaml 里的 command 字段里的参数了。而这个 YAML 文件提供的可配置项远不止这些。比如，你还可以修改 kubelet 和 kube-proxy 的配置，修改 Kubernetes 使用的基础镜像的 URL（默认的k8s.gcr.io/xxx镜像 URL 在国内访问是有困难的），指定自己的证书文件，指定特殊的容器运行时等等。这些配置项，就留给你在后续实践中探索了。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:13:4","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天的这次分享中，我重点介绍了 kubeadm 这个部署工具的工作原理和使用方法。紧接着，我会在下一篇文章中，使用它一步步地部署一个完整的 Kubernetes 集群。从今天的分享中，你可以看到，kubeadm 的设计非常简洁。并且，它在实现每一步部署功能时，都在最大程度地重用 Kubernetes 已有的功能，这也就使得我们在使用 kubeadm 部署 Kubernetes 项目时，非常有“原生”的感觉，一点都不会感到突兀。而 kubeadm 的源代码，直接就在 kubernetes/cmd/kubeadm 目录下，是 Kubernetes 项目的一部分。其中，app/phases 文件夹下的代码，对应的就是我在这篇文章中详细介绍的每一个具体步骤。看到这里，你可能会猜想，kubeadm 的作者一定是 Google 公司的某个“大神”吧。 实际上，kubeadm 几乎完全是一位高中生的作品。他叫 Lucas Käldström，芬兰人，今年只有 18 岁。kubeadm，是他 17 岁时用业余时间完成的一个社区项目。所以说，开源社区的魅力也在于此：一个成功的开源项目，总能够吸引到全世界最厉害的贡献者参与其中。尽管参与者的总体水平参差不齐，而且频繁的开源活动又显得杂乱无章难以管控，但一个有足够热度的社区最终的收敛方向，却一定是代码越来越完善、Bug 越来越少、功能越来越强大。最后，我再来回答一下我在今天这次分享开始提到的问题：kubeadm 能够用于生产环境吗？ 到目前为止（2018 年 9 月），这个问题的答案是：不能。因为 kubeadm 目前最欠缺的是，一键部署一个高可用的 Kubernetes 集群，即：Etcd、Master 组件都应该是多节点集群，而不是现在这样的单点。这，当然也正是 kubeadm 接下来发展的主要方向。另一方面，Lucas 也正在积极地把 kubeadm phases 开放给用户，即：用户可以更加自由地定制 kubeadm 的每一个部署步骤。这些举措，都可以让这个项目更加完善，我对它的发展走向也充满了信心。当然，如果你有部署规模化生产环境的需求，我推荐使用kops或者 SaltStack 这样更复杂的部署工具。但，在本专栏接下来的讲解中，我都会以 kubeadm 为依据进行讲述。 一方面，作为 Kubernetes 项目的原生部署工具，kubeadm 对 Kubernetes 项目特性的使用和集成，确实要比其他项目“技高一筹”，非常值得我们学习和借鉴；另一方面，kubeadm 的部署方法，不会涉及到太多的运维工作，也不需要我们额外学习复杂的部署工具。而它部署的 Kubernetes 集群，跟一个完全使用二进制文件搭建起来的集群几乎没有任何区别。 因此，使用 kubeadm 去部署一个 Kubernetes 集群，对于你理解 Kubernetes 组件的工作方式和架构，最好不过了。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:13:5","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"11 | 从0到1：搭建一个完整的Kubernetes集群 不过，首先需要指出的是，本篇搭建指南是完全的手工操作，细节比较多，并且有些外部链接可能还会遇到特殊的“网络问题”。所以，对于只关心学习 Kubernetes 本身知识点、不太关注如何手工部署 Kubernetes 集群的同学，可以略过本节，直接使用 MiniKube 或者 Kind，来在本地启动简单的 Kubernetes 集群进行后面的学习即可。如果是使用 MiniKube 的话，阿里云还维护了一个国内版的 MiniKube，这对于在国内的同学来说会比较友好。在上一篇文章中，我介绍了 kubeadm 这个 Kubernetes 半官方管理工具的工作原理。既然 kubeadm 的初衷是让 Kubernetes 集群的部署不再让人头疼，那么这篇文章，我们就来使用它部署一个完整的 Kubernetes 集群吧。 备注：这里所说的“完整”，指的是这个集群具备 Kubernetes 项目在 GitHub 上已经发布的所有功能，并能够模拟生产环境的所有使用需求。但并不代表这个集群是生产级别可用的：类似于高可用、授权、多租户、灾难备份等生产级别集群的功能暂时不在本篇文章的讨论范围。目前，kubeadm 的高可用部署已经有了第一个发布。但是，这个特性还没有 GA（生产可用），所以包括了大量的手动工作，跟我们所预期的一键部署还有一定距离。GA 的日期预计是 2018 年底到 2019 年初。届时，如果有机会我会再和你分享这部分内容。 这次部署，我不会依赖于任何公有云或私有云的能力，而是完全在 Bare-metal 环境中完成。这样的部署经验会更有普适性。而在后续的讲解中，如非特殊强调，我也都会以本次搭建的这个集群为基础。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"准备工作 首先，准备机器。最直接的办法，自然是到公有云上申请几个虚拟机。当然，如果条件允许的话，拿几台本地的物理服务器来组集群是最好不过了。这些机器只要满足如下几个条件即可： 满足安装 Docker 项目所需的要求，比如 64 位的 Linux 操作系统、3.10 及以上的内核版本；x86 或者 ARM 架构均可；机器之间网络互通，这是将来容器之间网络互通的前提；有外网访问权限，因为需要拉取镜像；能够访问到gcr.io、quay.io这两个 docker registry，因为有小部分镜像需要在这里拉取；单机可用资源建议 2 核 CPU、8 GB 内存或以上，再小的话问题也不大，但是能调度的 Pod 数量就比较有限了；30 GB 或以上的可用磁盘空间，这主要是留给 Docker 镜像和日志文件用的。 在本次部署中，我准备的机器配置如下：2 核 CPU、 7.5 GB 内存；30 GB 磁盘；Ubuntu 16.04；内网互通；外网访问权限不受限制。 备注：在开始部署前，我推荐你先花几分钟时间，回忆一下 Kubernetes 的架构。 然后，我再和你介绍一下今天实践的目标：在所有节点上安装 Docker 和 kubeadm；部署 Kubernetes Master；部署容器网络插件；部署 Kubernetes Worker；部署 Dashboard 可视化插件；部署容器存储插件。好了，现在，就来开始这次集群部署之旅吧！ ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"安装 kubeadm 和 Docker 我在上一篇文章《 Kubernetes 一键部署利器：kubeadm》中，已经介绍过 kubeadm 的基础用法，它的一键安装非常方便，我们只需要添加 kubeadm 的源，然后直接使用 apt-get 安装即可，具体流程如下所示：备注：为了方便讲解，我后续都会直接在 root 用户下进行操作。 $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - $ cat \u003c\u003cEOF \u003e /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $ apt-get update $ apt-get install -y docker.io kubeadm 提示：如果 apt.kubernetes.io 因为网络问题访问不到，可以换成中科大的 Ubuntu 镜像源 deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main。 在上述安装 kubeadm 的过程中，kubeadm 和 kubelet、kubectl、kubernetes-cni 这几个二进制文件都会被自动安装好。另外，这里我直接使用 Ubuntu 的 docker.io 的安装源，原因是 Docker 公司每次发布的最新的 Docker CE（社区版）产品往往还没有经过 Kubernetes 项目的验证，可能会有兼容性方面的问题。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:2","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"部署 Kubernetes 的 Master 节点 在上一篇文章中，我已经介绍过 kubeadm 可以一键部署 Master 节点。不过，在本篇文章中既然要部署一个“完整”的 Kubernetes 集群，那我们不妨稍微提高一下难度：通过配置文件来开启一些实验性功能。所以，这里我编写了一个给 kubeadm 用的 YAML 文件（名叫：kubeadm.yaml）： apiVersion: kubeadm.k8s.io/v1alpha1 kind: MasterConfiguration controllerManagerExtraArgs: horizontal-pod-autoscaler-use-rest-clients: \"true\" horizontal-pod-autoscaler-sync-period: \"10s\" node-monitor-grace-period: \"10s\" apiServerExtraArgs: runtime-config: \"api/all=true\" kubernetesVersion: \"stable-1.11\" 这个配置中，我给 kube-controller-manager 设置了： horizontal-pod-autoscaler-use-rest-clients: \"true\" 这意味着，将来部署的 kube-controller-manager 能够使用自定义资源（Custom Metrics）进行自动水平扩展。这是我后面文章中会重点介绍的一个内容。其中，“stable-1.11”就是 kubeadm 帮我们部署的 Kubernetes 版本号，即：Kubernetes release 1.11 最新的稳定版，在我的环境下，它是 v1.11.1。你也可以直接指定这个版本，比如：kubernetesVersion: “v1.11.1”。然后，我们只需要执行一句指令： $ kubeadm init --config kubeadm.yaml 就可以完成 Kubernetes Master 的部署了，这个过程只需要几分钟。部署完成后，kubeadm 会生成一行指令： kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711 这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。此外，kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 而需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。现在，我们就可以使用 kubectl get 命令来查看当前唯一一个节点的状态了： $ kubectl get nodes NAME STATUS ROLES AGE VERSION master NotReady master 1d v1.11.1 可以看到，这个 get 指令输出的结果里，Master 节点的状态是 NotReady，这是为什么呢？在调试 Kubernetes 集群时，最重要的手段就是用 kubectl describe 来查看这个节点（Node）对象的详细信息、状态和事件（Event），我们来试一下： $ kubectl describe node master ... Conditions: ... Ready False ... KubeletNotReady runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 通过 kubectl describe 指令的输出，我们可以看到 NodeNotReady 的原因在于，我们尚未部署任何网络插件。另外，我们还可以通过 kubectl 检查这个节点上各个系统 Pod 的状态，其中，kube-system 是 Kubernetes 项目预留的系统 Pod 的工作空间（Namepsace，注意它并不是 Linux Namespace，它只是 Kubernetes 划分不同工作空间的单位）： $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-78fcdf6894-j9s52 0/1 Pending 0 1h coredns-78fcdf6894-jm4wf 0/1 Pending 0 1h etcd-master 1/1 Running 0 2s kube-apiserver-master 1/1 Running 0 1s kube-controller-manager-master 0/1 Pending 0 1s kube-proxy-xbd47 1/1 NodeLost 0 1h kube-scheduler-master 1/1 Running 0 1s 可以看到，CoreDNS、kube-controller-manager 等依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:3","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"部署网络插件 在 Kubernetes 项目“一切皆容器”的设计理念指导下，部署网络插件非常简单，只需要执行一句 kubectl apply 指令，以 Weave 为例： $ kubectl apply -f https://git.io/weave-kube-1.6 部署完成后，我们可以通过 kubectl get 重新检查 Pod 的状态： $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-78fcdf6894-j9s52 1/1 Running 0 1d coredns-78fcdf6894-jm4wf 1/1 Running 0 1d etcd-master 1/1 Running 0 9s kube-apiserver-master 1/1 Running 0 9s kube-controller-manager-master 1/1 Running 0 9s kube-proxy-xbd47 1/1 Running 0 1d kube-scheduler-master 1/1 Running 0 9s weave-net-cmk27 2/2 Running 0 19s 可以看到，所有的系统 Pod 都成功启动了，而刚刚部署的 Weave 网络插件则在 kube-system 下面新建了一个名叫 weave-net-cmk27 的 Pod，一般来说，这些 Pod 就是容器网络插件在每个节点上的控制组件。Kubernetes 支持容器网络插件，使用的是一个名叫 CNI 的通用接口，它也是当前容器网络的事实标准，市面上的所有容器网络开源项目都可以通过 CNI 接入 Kubernetes，比如 Flannel、Calico、Canal、Romana 等等，它们的部署方式也都是类似的“一键部署”。关于这些开源项目的实现细节和差异，我会在后续的网络部分详细介绍。至此，Kubernetes 的 Master 节点就部署完成了。如果你只需要一个单节点的 Kubernetes，现在你就可以使用了。不过，在默认情况下，Kubernetes 的 Master 节点是不能运行用户 Pod 的，所以还需要额外做一个小操作。在本篇的最后部分，我会介绍到它。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:4","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"部署 Kubernetes 的 Worker 节点 Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。 所以，相比之下，部署 Worker 节点反而是最简单的，只需要两步即可完成。第一步，在所有 Worker 节点上执行“安装 kubeadm 和 Docker”一节的所有步骤。第二步，执行部署 Master 节点时生成的 kubeadm join 指令： $ kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:5","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"通过 Taint/Toleration 调整 Master 执行 Pod 的策略 我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”。除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。其中，为节点打上“污点”（Taint）的命令是： $ kubectl taint nodes node1 foo=bar:NoSchedule 这时，该 node1 节点上就会增加一个键值对格式的 Taint，即：foo=bar:NoSchedule。其中值里面的 NoSchedule，意味着这个 Taint 只会在调度新 Pod 时产生作用，而不会影响已经在 node1 上运行的 Pod，哪怕它们没有 Toleration。那么 Pod 又如何声明 Toleration 呢？我们只要在 Pod 的.yaml 文件中的 spec 部分，加入 tolerations 字段即可： apiVersion: v1 kind: Pod ... spec: tolerations: - key: \"foo\" operator: \"Equal\" value: \"bar\" effect: \"NoSchedule\" 这个 Toleration 的含义是，这个 Pod 能“容忍”所有键值对为 foo=bar 的 Taint（ operator: “Equal”，“等于”操作）。现在回到我们已经搭建的集群上来。这时，如果你通过 kubectl describe 检查一下 Master 节点的 Taint 字段，就会有所发现了： $ kubectl describe node master Name: master Roles: master Taints: node-role.kubernetes.io/master:NoSchedule 可以看到，Master 节点默认被加上了node-role.kubernetes.io/master:NoSchedule这样一个“污点”，其中“键”是node-role.kubernetes.io/master，而没有提供“值”。此时，你就需要像下面这样用“Exists”操作符（operator: “Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上： apiVersion: v1 kind: Pod ... spec: tolerations: - key: \"foo\" operator: \"Exists\" effect: \"NoSchedule\" 当然，如果你就是想要一个单节点的 Kubernetes，删除这个 Taint 才是正确的选择： $ kubectl taint nodes --all node-role.kubernetes.io/master- 如上所示，我们在“node-role.kubernetes.io/master”这个键后面加上了一个短横线“-”，这个格式就意味着移除所有以“node-role.kubernetes.io/master”为键的 Taint。到了这一步，一个基本完整的 Kubernetes 集群就部署完毕了。是不是很简单呢？有了 kubeadm 这样的原生管理工具，Kubernetes 的部署已经被大大简化。更重要的是，像证书、授权、各个组件的配置等部署中最麻烦的操作，kubeadm 都已经帮你完成了。接下来，我们再在这个 Kubernetes 集群上安装一些其他的辅助插件，比如 Dashboard 和存储插件。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:6","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"部署 Dashboard 可视化插件 在 Kubernetes 社区中，有一个很受欢迎的 Dashboard 项目，它可以给用户提供一个可视化的 Web 界面来查看当前集群的各种信息。毫不意外，它的部署也相当简单： $ kubectl apply -f $ $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml 部署完成之后，我们就可以查看 Dashboard 对应的 Pod 的状态了： $ kubectl get pods -n kube-system kubernetes-dashboard-6948bdb78-f67xk 1/1 Running 0 1m 需要注意的是，由于 Dashboard 是一个 Web Server，很多人经常会在自己的公有云上无意地暴露 Dashboard 的端口，从而造成安全隐患。所以，1.7 版本之后的 Dashboard 项目部署完成后，默认只能通过 Proxy 的方式在本地访问。具体的操作，你可以查看 Dashboard 项目的官方文档。而如果你想从集群外访问这个 Dashboard 的话，就需要用到 Ingress，我会在后面的文章中专门介绍这部分内容。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:7","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"部署容器存储插件 接下来，让我们完成这个 Kubernetes 集群的最后一块拼图：容器持久化存储。我在前面介绍容器原理时已经提到过，很多时候我们需要用数据卷（Volume）把外面宿主机上的目录或者文件挂载进容器的 Mount Namespace 中，从而达到容器和宿主机共享这些目录或者文件的目的。容器里的应用，也就可以在这些数据卷中新建和写入文件。可是，如果你在某一台机器上启动的一个容器，显然无法看到其他机器上的容器在它们的数据卷里写入的文件。这是容器最典型的特征之一：无状态。 而容器的持久化存储，就是用来保存容器存储状态的重要手段：存储插件会在容器里挂载一个基于网络或者其他机制的远程数据卷，使得在容器里创建的文件，实际上是保存在远程存储服务器上，或者以分布式的方式保存在多个节点上，而与当前宿主机没有任何绑定关系。这样，无论你在其他哪个宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷里保存的内容。这就是“持久化”的含义。由于 Kubernetes 本身的松耦合设计，绝大多数存储项目，比如 Ceph、GlusterFS、NFS 等，都可以为 Kubernetes 提供持久化存储能力。在这次的部署实战中，我会选择部署一个很重要的 Kubernetes 存储插件项目：Rook。Rook 项目是一个基于 Ceph 的 Kubernetes 存储插件（它后期也在加入对更多存储实现的支持）。不过，不同于对 Ceph 的简单封装，Rook 在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能，使得这个项目变成了一个完整的、生产级别可用的容器存储插件。得益于容器化技术，用几条指令，Rook 就可以把复杂的 Ceph 存储后端部署起来： $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml 在部署完成后，你就可以看到 Rook 项目会将自己的 Pod 放置在由它自己管理的两个 Namespace 当中： $ kubectl get pods -n rook-ceph-system NAME READY STATUS RESTARTS AGE rook-ceph-agent-7cv62 1/1 Running 0 15s rook-ceph-operator-78d498c68c-7fj72 1/1 Running 0 44s rook-discover-2ctcv 1/1 Running 0 15s $ kubectl get pods -n rook-ceph NAME READY STATUS RESTARTS AGE rook-ceph-mon0-kxnzh 1/1 Running 0 13s rook-ceph-mon1-7dn2t 1/1 Running 0 2s 这样，一个基于 Rook 持久化存储集群就以容器的方式运行起来了，而接下来在 Kubernetes 项目上创建的所有 Pod 就能够通过 Persistent Volume（PV）和 Persistent Volume Claim（PVC）的方式，在容器里挂载由 Ceph 提供的数据卷了。而 Rook 项目，则会负责这些数据卷的生命周期管理、灾难备份等运维工作。关于这些容器持久化存储的知识，我会在后续章节中专门讲解。这时候，你可能会有个疑问：为什么我要选择 Rook 项目呢？其实，是因为这个项目很有前途。如果你去研究一下 Rook 项目的实现，就会发现它巧妙地依赖了 Kubernetes 提供的编排能力，合理的使用了很多诸如 Operator、CRD 等重要的扩展特性（这些特性我都会在后面的文章中逐一讲解到）。这使得 Rook 项目，成为了目前社区中基于 Kubernetes API 构建的最完善也最成熟的容器存储插件。我相信，这样的发展路线，很快就会得到整个社区的推崇。 备注：其实，在很多时候，大家说的所谓“云原生”，就是“Kubernetes 原生”的意思。而像 Rook、Istio 这样的项目，正是贯彻这个思路的典范。在我们后面讲解了声明式 API 之后，相信你对这些项目的设计思想会有更深刻的体会。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:8","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在本篇文章中，我们完全从 0 开始，在 Bare-metal 环境下使用 kubeadm 工具部署了一个完整的 Kubernetes 集群：这个集群有一个 Master 节点和多个 Worker 节点；使用 Weave 作为容器网络插件；使用 Rook 作为容器持久化存储插件；使用 Dashboard 插件提供了可视化的 Web 界面。这个集群，也将会是我进行后续讲解所依赖的集群环境，并且在后面的讲解中，我还会给它安装更多的插件，添加更多的新能力。另外，这个集群的部署过程并不像传说中那么繁琐，这主要得益于： kubeadm 项目大大简化了部署 Kubernetes 的准备工作，尤其是配置文件、证书、二进制文件的准备和制作，以及集群版本管理等操作，都被 kubeadm 接管了。Kubernetes 本身“一切皆容器”的设计思想，加上良好的可扩展机制，使得插件的部署非常简便。 上述思想，也是开发和使用 Kubernetes 的重要指导思想，即：基于 Kubernetes 开展工作时，你一定要优先考虑这两个问题：我的工作是不是可以容器化？我的工作是不是可以借助 Kubernetes API 和可扩展机制来完成？而一旦这项工作能够基于 Kubernetes 实现容器化，就很有可能像上面的部署过程一样，大幅简化原本复杂的运维工作。对于时间宝贵的技术人员来说，这个变化的重要性是不言而喻的。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:14:9","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"12 | 牛刀小试：我的第一个容器化应用 在上一篇文章《从 0 到 1：搭建一个完整的 Kubernetes 集群》中，我和你一起部署了一套完整的 Kubernetes 集群。这个集群虽然离生产环境的要求还有一定差距（比如，没有一键高可用部署），但也可以当作是一个准生产级别的 Kubernetes 集群了。而在这篇文章中，我们就来扮演一个应用开发者的角色，使用这个 Kubernetes 集群发布第一个容器化应用。在开始实践之前，我先给你讲解一下 Kubernetes 里面与开发者关系最密切的几个概念。作为一个应用开发者，你首先要做的，是制作容器的镜像。这一部分内容，我已经在容器基础部分《白话容器基础（三）：深入理解容器镜像》重点讲解过了。而有了容器镜像之后，你需要按照 Kubernetes 项目的规范和要求，将你的镜像组织为它能够“认识”的方式，然后提交上去。那么，什么才是 Kubernetes 项目能“认识”的方式呢？ 这就是使用 Kubernetes 的必备技能：编写配置文件。备注：这些配置文件可以是 YAML 或者 JSON 格式的。为方便阅读与理解，在后面的讲解中，我会统一使用 YAML 文件来指代它们。Kubernetes 跟 Docker 等很多项目最大的不同，就在于它不推荐你使用命令行的方式直接运行容器（虽然 Kubernetes 项目也支持这种方式，比如：kubectl run），而是希望你用 YAML 文件的方式，即：把容器的定义、参数、配置，统统记录在一个 YAML 文件中，然后用这样一句指令把它运行起来： $ kubectl create -f 我的配置文件 这么做最直接的好处是，你会有一个文件能记录下 Kubernetes 到底“run”了什么。比如下面这个例子： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 像这样的一个 YAML 文件，对应到 Kubernetes 中，就是一个 API Object（API 对象）。当你为这个对象的各个字段填好值并提交给 Kubernetes 之后，Kubernetes 就会负责创建出这些对象所定义的容器或者其他类型的 API 资源。可以看到，这个 YAML 文件中的 Kind 字段，指定了这个 API 对象的类型（Type），是一个 Deployment。所谓 Deployment，是一个定义多副本应用（即多个副本 Pod）的对象，我在前面的文章中（也是第 9 篇文章《从容器到容器云：谈谈 Kubernetes 的本质》）曾经简单提到过它的用法。此外，Deployment 还负责在 Pod 定义发生变化时，对每个副本进行滚动更新（Rolling Update）。 在上面这个 YAML 文件中，我给它定义的 Pod 副本个数 (spec.replicas) 是：2。而这些 Pod 具体的又长什么样子呢？ 为此，我定义了一个 Pod 模版（spec.template），这个模版描述了我想要创建的 Pod 的细节。在上面的例子里，这个 Pod 里只有一个容器，这个容器的镜像（spec.containers.image）是 nginx:1.7.9，这个容器监听端口（containerPort）是 80。关于 Pod 的设计和用法我已经在第 9 篇文章《从容器到容器云：谈谈 Kubernetes 的本质》中简单的介绍过。而在这里，你需要记住这样一句话： Pod 就是 Kubernetes 世界里的“应用”；而一个应用，可以由多个容器组成。 需要注意的是，像这样使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 Kubernetes 中，叫作“控制器”模式（controller pattern）。在我们的例子中，Deployment 扮演的正是 Pod 的控制器的角色。关于 Pod 和控制器模式的更多细节，我会在后续编排部分做进一步讲解。你可能还注意到，这样的每一个 API 对象都有一个叫作 Metadata 的字段，这个字段就是 API 对象的“标识”，即元数据，它也是我们从 Kubernetes 里找到这个对象的主要依据。这其中最主要使用到的字段是 Labels。顾名思义，Labels 就是一组 key-value 格式的标签。而像 Deployment 这样的控制器对象，就可以通过这个 Labels 字段从 Kubernetes 中过滤出它所关心的被控制对象。 比如，在上面这个 YAML 文件中，Deployment 会把所有正在运行的、携带“app: nginx”标签的 Pod 识别为被管理的对象，并确保这些 Pod 的总数严格等于两个。而这个过滤规则的定义，是在 Deployment 的“spec.selector.matchLabels”字段。我们一般称之为：Label Selector。另外，在 Metadata 中，还有一个与 Labels 格式、层级完全相同的字段叫 Annotations，它专门用来携带 key-value 格式的内部信息。所谓内部信息，指的是对这些信息感兴趣的，是 Kubernetes 组件本身，而不是用户。所以大多数 Annotations，都是在 Kubernetes 运行过程中，被自动加在这个 API 对象上。一个 Kubernetes 的 API 对象的定义，大多可以分为 Metadata 和 Spec 两个部分。前者存放的是这个对象的元数据，对所有 API 对象来说，这一部分的字段和格式基本上是一样的；而后者存放的，则是属于这个对象独有的定义，用来描述它所要表达的功能。 在了解了上述 Kubernetes 配置文件的基本知识之后，我们现在就可以把这个 YAML 文件“运行”起来。正如前所述，你可以使用 kubectl create 指令完成这个操作： $ kubectl create -f nginx-deployment.yaml 然后，通过 kubectl get 命令检查这个 YAML 运行起来的状态是不是与我们预期的一致： $ kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE nginx-deployment-67594d6bf6-9gdvr 1/1 Running 0 10m nginx-deployment-67594d6bf6-v6j7w 1/1 Running 0 10m kubectl get 指令的作用，就是从 Kubernetes 里面获取（GET）指定的 API 对象。可以看到，在这里我还加上了一个 -l 参数，即获取所有匹配 app: nginx 标签的 Pod。需要注意的是，在命令行中，所有 key-value 格式的参数，都使用“=”而非“:”表示。从这条指令返回的结果中，我们可以看到现在有两个 Pod 处于 Running 状态，也就意味着我们这个 Deployment 所管理的 Pod 都处于预期的状态。此外， 你还可以使用 kubectl describe 命令，查看一个 API 对象的细节，比如： $ kubectl describe pod nginx-deployment-67594d6bf6-9gdvr Name: nginx-deployment-67594d6bf6-9gdvr Namespace: default Priority: 0 PriorityClassName: \u003cnone\u003e Node: node-1/10.168.0.3 Start Time: Thu, 16 Aug 2018 08:48:42 +0000 Labels: app=nginx pod-template-hash=2315082692 Annotations: \u003cnone\u003e Status: Running IP: 10.32.0.23 Controlled By: ReplicaSet/nginx-deployment-67594d6bf6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned default/nginx-deployment-67594d6bf6-9gdvr to node-1 Normal Pulling 25s kubelet, node-1 pulling image \"nginx:1.7.9\" Normal Pulled 17s kubelet, node-1 Successfully pulled image \"nginx:1.7.9\" Normal Created 17s kubelet, node-1 Created container Normal Started 17s kubelet, node-1 Started container 在 kube","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:15:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天的分享中，我通过一个小案例，和你近距离体验了 Kubernetes 的使用方法。可以看到，Kubernetes 推荐的使用方式，是用一个 YAML 文件来描述你所要部署的 API 对象。然后，统一使用 kubectl apply 命令完成对这个对象的创建和更新操作。而 Kubernetes 里“最小”的 API 对象是 Pod。Pod 可以等价为一个应用，所以，Pod 可以由多个紧密协作的容器组成。在 Kubernetes 中，我们经常会看到它通过一种 API 对象来管理另一种 API 对象，比如 Deployment 和 Pod 之间的关系；而由于 Pod 是“最小”的对象，所以它往往都是被其他对象控制的。这种组合方式，正是 Kubernetes 进行容器编排的重要模式。而像这样的 Kubernetes API 对象，往往由 Metadata 和 Spec 两部分组成，其中 Metadata 里的 Labels 字段是 Kubernetes 过滤对象的主要手段。在这些字段里面，容器想要使用的数据卷，也就是 Volume，正是 Pod 的 Spec 字段的一部分。而 Pod 里的每个容器，则需要显式的声明自己要挂载哪个 Volume。上面这些基于 YAML 文件的容器管理方式，跟 Docker、Mesos 的使用习惯都是不一样的，而从 docker run 这样的命令行操作，向 kubectl apply YAML 文件这样的声明式 API 的转变，是每一个容器技术学习者，必须要跨过的第一道门槛。所以，如果你想要快速熟悉 Kubernetes，请按照下面的流程进行练习： 首先，在本地通过 Docker 测试代码，制作镜像；然后，选择合适的 Kubernetes API 对象，编写对应 YAML 文件（比如，Pod，Deployment）；最后，在 Kubernetes 上部署这个 YAML 文件。 更重要的是，在部署到 Kubernetes 之后，接下来的所有操作，要么通过 kubectl 来执行，要么通过修改 YAML 文件来实现，就尽量不要再碰 Docker 的命令行了。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:15:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"容器编排与k8s作业管理 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:16:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"13 | 为什么我们需要Pod？ 在前面的文章中，我详细介绍了在 Kubernetes 里部署一个应用的过程。在这些讲解中，我提到了这样一个知识点：Pod，是 Kubernetes 项目中最小的 API 对象。如果换一个更专业的说法，我们可以这样描述：Pod，是 Kubernetes 项目的原子调度单位。不过，我相信你在学习和使用 Kubernetes 项目的过程中，已经不止一次地想要问这样一个问题：为什么我们会需要 Pod？ 是啊，我们在前面已经花了很多精力去解读 Linux 容器的原理、分析了 Docker 容器的本质，终于，“Namespace 做隔离，Cgroups 做限制，rootfs 做文件系统”这样的“三句箴言”可以朗朗上口了，为什么 Kubernetes 项目又突然搞出一个 Pod 来呢？要回答这个问题，我们还是要一起回忆一下我曾经反复强调的一个问题：容器的本质到底是什么？你现在应该可以不假思索地回答出来：容器的本质是进程。没错。容器，就是未来云计算系统中的进程；容器镜像就是这个系统里的“.exe”安装包。那么 Kubernetes 呢？你应该也能立刻回答上来：Kubernetes 就是操作系统！非常正确。现在，就让我们登录到一台 Linux 机器里，执行一条如下所示的命令： $ pstree -g 这条命令的作用，是展示当前系统中正在运行的进程的树状结构。它的返回结果如下所示： systemd(1)-+-accounts-daemon(1984)-+-{gdbus}(1984) | `-{gmain}(1984) |-acpid(2044) ... |-lxcfs(1936)-+-{lxcfs}(1936) | `-{lxcfs}(1936) |-mdadm(2135) |-ntpd(2358) |-polkitd(2128)-+-{gdbus}(2128) | `-{gmain}(2128) |-rsyslogd(1632)-+-{in:imklog}(1632) | |-{in:imuxsock) S 1(1632) | `-{rs:main Q:Reg}(1632) |-snapd(1942)-+-{snapd}(1942) | |-{snapd}(1942) | |-{snapd}(1942) | |-{snapd}(1942) | |-{snapd}(1942) 不难发现，在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，而是以进程组的方式，“有原则地”组织在一起。比如，这里有一个叫作 rsyslogd 的程序，它负责的是 Linux 操作系统里的日志处理。可以看到，rsyslogd 的主程序 main，和它要用到的内核日志模块 imklog 等，同属于 1632 进程组。这些进程相互协作，共同完成 rsyslogd 程序的职责。 注意：我在本篇中提到的“进程”，比如，rsyslogd 对应的 imklog，imuxsock 和 main，严格意义上来说，其实是 Linux 操作系统语境下的“线程”。这些线程，或者说，轻量级进程之间，可以共享文件、信号、数据内存、甚至部分代码，从而紧密协作共同完成一个程序的职责。所以同理，我提到的“进程组”，对应的也是 Linux 操作系统语境下的“线程组”。这种命名关系与实际情况的不一致，是 Linux 发展历史中的一个遗留问题。对这个话题感兴趣的同学，可以阅读这篇技术文章来了解一下。 而 Kubernetes 项目所做的，其实就是将“进程组”的概念映射到了容器技术中，并使其成为了这个云计算“操作系统”里的“一等公民”。Kubernetes 项目之所以要这么做的原因，我在前面介绍 Kubernetes 和 Borg 的关系时曾经提到过：在 Borg 项目的开发和实践过程中，Google 公司的工程师们发现，他们部署的应用，往往都存在着类似于“进程和进程组”的关系。更具体地说，就是这些应用之间有着密切的协作关系，使得它们必须部署在同一台机器上。而如果事先没有“组”的概念，像这样的运维关系就会非常难以处理。 我还是以前面的 rsyslogd 为例子。已知 rsyslogd 由三个进程组成：一个 imklog 模块，一个 imuxsock 模块，一个 rsyslogd 自己的 main 函数主进程。这三个进程一定要运行在同一台机器上，否则，它们之间基于 Socket 的通信和文件交换，都会出现问题。现在，我要把 rsyslogd 这个应用给容器化，由于受限于容器的“单进程模型”，这三个模块必须被分别制作成三个不同的容器。而在这三个容器运行的时候，它们设置的内存配额都是 1 GB。 再次强调一下：容器的“单进程模型”，并不是指容器里只能运行“一个”进程，而是指容器没有管理多个进程的能力。这是因为容器里 PID=1 的进程就是应用本身，其他的进程都是这个 PID=1 进程的子进程。可是，用户编写的应用，并不能够像正常操作系统里的 init 进程或者 systemd 那样拥有进程管理的功能。比如，你的应用是一个 Java Web 程序（PID=1），然后你执行 docker exec 在后台启动了一个 Nginx 进程（PID=3）。可是，当这个 Nginx 进程异常退出的时候，你该怎么知道呢？这个进程退出后的垃圾收集工作，又应该由谁去做呢？ 假设我们的 Kubernetes 集群上有两个节点：node-1 上有 3 GB 可用内存，node-2 有 2.5 GB 可用内存。这时，假设我要用 Docker Swarm 来运行这个 rsyslogd 程序。为了能够让这三个容器都运行在同一台机器上，我就必须在另外两个容器上设置一个 affinity=main（与 main 容器有亲密性）的约束，即：它们俩必须和 main 容器运行在同一台机器上。然后，我顺序执行：“docker run main”“docker run imklog”和“docker run imuxsock”，创建这三个容器。这样，这三个容器都会进入 Swarm 的待调度队列。然后，main 容器和 imklog 容器都先后出队并被调度到了 node-2 上（这个情况是完全有可能的）。 可是，当 imuxsock 容器出队开始被调度时，Swarm 就有点懵了：node-2 上的可用资源只有 0.5 GB 了，并不足以运行 imuxsock 容器；可是，根据 affinity=main 的约束，imuxsock 容器又只能运行在 node-2 上。这就是一个典型的成组调度（gang scheduling）没有被妥善处理的例子。 在工业界和学术界，关于这个问题的讨论可谓旷日持久，也产生了很多可供选择的解决方案。比如，Mesos 中就有一个资源囤积（resource hoarding）的机制，会在所有设置了 Affinity 约束的任务都达到时，才开始对它们统一进行调度。而在 Google Omega 论文中，则提出了使用乐观调度处理冲突的方法，即：先不管这些冲突，而是通过精心设计的回滚机制在出现了冲突之后解决问题。可是这些方法都谈不上完美。资源囤积带来了不可避免的调度效率损失和死锁的可能性；而乐观调度的复杂程度，则不是常规技术团队所能驾驭的。但是，到了 Kubernetes 项目里，这样的问题就迎刃而解了：Pod 是 Kubernetes 里的原子调度单位。这就意味着，Kubernetes 项目的调度器，是统一按照 Pod 而非容器的资源需求进行计算的。所以，像 imklog、imuxsock 和 main 函数主进程这样的三个容器，正是一个典型的由三个容器组成的 Pod。Kubernetes 项目在调度时，自然就会去选择可用内存等于 3 GB 的 node-1 节点进行绑定，而根本不会考虑 node-2。 像这样容器间的紧密协作，我们可以称为“超亲密关系”。这些具有“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace（比如，一个容器要加入另一个容器的 Network Namespace）等等。这也就意味着，并不是所有有“关系”的容器都属于同一个 Pod。比如，PHP 应用容器和 MySQL 虽然会发生访问关系，但并没有必要、也不应该部署在同一台机器上，它们更适合做成两个 Pod。 不过，相信此时你可能会有第二个疑问：对于初学者来说，一般都是先学会了用 Docker 这种单容器的工具，才会开始接触 Pod。而如果 Pod 的设计只是出于调度上的考虑，那么 Kubernetes 项目似乎完全没有必要非得把 Pod 作为“一等公民”吧？这不是故意增加用户的学习门槛吗？没错，如果只是处理“超亲密关系”这样的调度问题，有 Borg 和 Omega 论文珠玉在前，Kubernetes 项目肯定可以在调度器层面给它解决掉。不过，Pod 在 Kubernetes 项目里还有更重要的意义，那就是：容器设计模式。 为了理解这一层含义，我就必须先给你介绍一下Pod 的实现原理。 首先，关于 Pod 最重要的一个事实是：它只是一个逻辑概念。也就是说，Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。那么，Pod 又是怎么被“创建”出来的呢？答案是：Pod，其实是一组共享了某些资源的容","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:17:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在本篇文章中我重点分享了 Kubernetes 项目中 Pod 的实现原理。Pod 是 Kubernetes 项目与其他单容器项目相比最大的不同，也是一位容器技术初学者需要面对的第一个与常规认知不一致的知识点。 事实上，直到现在，仍有很多人把容器跟虚拟机相提并论，他们把容器当做性能更好的虚拟机，喜欢讨论如何把应用从虚拟机无缝地迁移到容器中。但实际上，无论是从具体的实现原理，还是从使用方法、特性、功能等方面，容器与虚拟机几乎没有任何相似的地方；也不存在一种普遍的方法，能够把虚拟机里的应用无缝迁移到容器中。因为，容器的性能优势，必然伴随着相应缺陷，即：它不能像虚拟机那样，完全模拟本地物理机环境中的部署方法。所以，这个“上云”工作的完成，最终还是要靠深入理解容器的本质，即：进程。 实际上，一个运行在虚拟机里的应用，哪怕再简单，也是被管理在 systemd 或者 supervisord 之下的一组进程，而不是一个进程。这跟本地物理机上应用的运行方式其实是一样的。这也是为什么，从物理机到虚拟机之间的应用迁移，往往并不困难。可是对于容器来说，一个容器永远只能管理一个进程。更确切地说，一个容器，就是一个进程。这是容器技术的“天性”，不可能被修改。所以，将一个原本运行在虚拟机里的应用，“无缝迁移”到容器中的想法，实际上跟容器的本质是相悖的。这也是当初 Swarm 项目无法成长起来的重要原因之一：一旦到了真正的生产环境上，Swarm 这种单容器的工作方式，就难以描述真实世界里复杂的应用架构了。所以，你现在可以这么理解 Pod 的本质： Pod，实际上是在扮演传统基础设施里“虚拟机”的角色；而容器，则是这个虚拟机里运行的用户程序。 所以下一次，当你需要把一个运行在虚拟机里的应用迁移到 Docker 容器中时，一定要仔细分析到底有哪些进程（组件）运行在这个虚拟机里。然后，你就可以把整个虚拟机想象成为一个 Pod，把这些进程分别做成容器镜像，把有顺序关系的容器，定义为 Init Container。这才是更加合理的、松耦合的容器编排诀窍，也是从传统应用架构，到“微服务架构”最自然的过渡方式。 注意：Pod 这个概念，提供的是一种编排思想，而不是具体的技术方案。所以，如果愿意的话，你完全可以使用虚拟机来作为 Pod 的实现，然后把用户容器都运行在这个虚拟机里。比如，Mirantis 公司的virtlet 项目就在干这个事情。甚至，你可以去实现一个带有 Init 进程的容器项目，来模拟传统应用的运行方式。这些工作，在 Kubernetes 中都是非常轻松的，也是我们后面讲解 CRI 时会提到的内容。 相反的，如果强行把整个应用塞到一个容器里，甚至不惜使用 Docker In Docker 这种在生产环境中后患无穷的解决方案，恐怕最后往往会得不偿失。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:17:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"14 | 深入解析Pod对象（一）：基本概念 在上一篇文章中，我详细介绍了 Pod 这个 Kubernetes 项目中最重要的概念。而在今天这篇文章中，我会和你分享 Pod 对象的更多细节。现在，你已经非常清楚：Pod，而不是容器，才是 Kubernetes 项目中的最小编排单位。将这个设计落实到 API 对象上，容器（Container）就成了 Pod 属性里的一个普通的字段。那么，一个很自然的问题就是：到底哪些属性属于 Pod 对象，而又有哪些属性属于 Container 呢？要彻底理解这个问题，你就一定要牢记我在上一篇文章中提到的一个结论：Pod 扮演的是传统部署环境里“虚拟机”的角色。这样的设计，是为了使用户从传统环境（虚拟机环境）向 Kubernetes（容器环境）的迁移，更加平滑。而如果你能把 Pod 看成传统环境里的“机器”、把容器看作是运行在这个“机器”里的“用户程序”，那么很多关于 Pod 对象的设计就非常容易理解了。 比如，凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。这些属性的共同特征是，它们描述的是“机器”这个整体，而不是里面运行的“程序”。比如，配置这个“机器”的网卡（即：Pod 的网络定义），配置这个“机器”的磁盘（即：Pod 的存储定义），配置这个“机器”的防火墙（即：Pod 的安全定义）。更不用说，这台“机器”运行在哪个服务器之上（即：Pod 的调度）。 接下来，我就先为你介绍 Pod 中几个重要字段的含义和用法。 NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段，用法如下所示： apiVersion: v1 kind: Pod ... spec: nodeSelector: disktype: ssd 这样的一个配置，意味着这个 Pod 永远只能运行在携带了“disktype: ssd”标签（Label）的节点上；否则，它将调度失败。 NodeName：一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。 HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，用法如下： apiVersion: v1 kind: Pod ... spec: hostAliases: - ip: \"10.1.2.3\" hostnames: - \"foo.remote\" - \"bar.remote\" ... 在这个 Pod 的 YAML 文件中，我设置了一组 IP 和 hostname 的数据。这样，这个 Pod 启动后，/etc/hosts 文件的内容将如下所示： cat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ... 10.244.135.10 hostaliases-pod 10.1.2.3 foo.remote 10.1.2.3 bar.remote 其中，最下面两行记录，就是我通过 HostAliases 字段为 Pod 设置的。需要指出的是，在 Kubernetes 项目中，如果要设置 hosts 文件里的内容，一定要通过这种方法。否则，如果直接修改了 hosts 文件的话，在 Pod 被删除重建之后，kubelet 会自动覆盖掉被修改的内容。 除了上述跟“机器”相关的配置外，你可能也会发现，凡是跟容器的 Linux Namespace 相关的属性，也一定是 Pod 级别的。这个原因也很容易理解：Pod 的设计，就是要让它里面的容器尽可能多地共享 Linux Namespace，仅保留必要的隔离和限制能力。这样，Pod 模拟出的效果，就跟虚拟机里程序间的关系非常类似了。举个例子，在下面这个 Pod 的 YAML 文件中，我定义了 shareProcessNamespace=true： apiVersion: v1 kind: Pod metadata: name: nginx spec: shareProcessNamespace: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true 这就意味着这个 Pod 里的容器要共享 PID Namespace。而在这个 YAML 文件中，我还定义了两个容器：一个是 nginx 容器，一个是开启了 tty 和 stdin 的 shell 容器。我在前面介绍容器基础时，曾经讲解过什么是 tty 和 stdin。而在 Pod 的 YAML 文件里声明开启它们俩，其实等同于设置了 docker run 里的 -it（-i 即 stdin，-t 即 tty）参数。如果你还是不太理解它们俩的作用的话，可以直接认为 tty 就是 Linux 给用户提供的一个常驻小程序，用于接收用户的标准输入，返回操作系统的标准输出。当然，为了能够在 tty 中输入信息，你还需要同时开启 stdin（标准输入流）。于是，这个 Pod 被创建后，你就可以使用 shell 容器的 tty 跟这个容器进行交互了。我们一起实践一下： $ kubectl create -f nginx.yaml 接下来，我们使用 kubectl attach 命令，连接到 shell 容器的 tty 上： $ kubectl attach -it nginx -c shell 这样，我们就可以在 shell 容器里执行 ps 指令，查看所有正在运行的进程： $ kubectl attach -it nginx -c shell / # ps ax PID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off; 14 101 0:00 nginx: worker process 15 root 0:00 sh 21 root 0:00 ps ax 可以看到，在这个容器里，我们不仅可以看到它本身的 ps ax 指令，还可以看到 nginx 容器的进程，以及 Infra 容器的 /pause 进程。这就意味着，整个 Pod 里的每个容器的进程，对于所有容器来说都是可见的：它们共享了同一个 PID Namespace。类似地，凡是 Pod 中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的定义，比如： apiVersion: v1 kind: Pod metadata: name: nginx spec: hostNetwork: true hostIPC: true hostPID: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true 在这个 Pod 中，我定义了共享宿主机的 Network、IPC 和 PID Namespace。这就意味着，这个 Pod 里的所有容器，会直接使用宿主机的网络、直接与宿主机进行 IPC 通信、看到宿主机里正在运行的所有进程。当然，除了这些属性，Pod 里最重要的字段当属“Containers”了。而在上一篇文章中，我还介绍过“Init Containers”。其实，这两个字段都属于 Pod 对容器的定义，内容也完全相同，只是 Init Containers 的生命周期，会先于所有的 Containers，并且严格按照定义的顺序执行。Kubernetes 项目中对 Container 的定义，和 Docker 相比并没有什么太大区别。我在前面的容器技术概念入门系列文章中，和你分享的 Image（镜像）、Command（启动命令）、workingDir（容器的工作目录）、Ports（容器要开发的端口），以及 volumeMounts（容器要挂载的 Volume）都是构成 Kubernetes 项目中 Container 的主要字段。不过在这里，还有这么几个属性值得你额外关注。 首先，是 ImagePullPolicy 字段。它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。ImagePullPolicy 的值默认是 Always，即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时，ImagePullPolicy 也会被认为 Always。而如果它的值被定义为 Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。 其次，是 Lifecycle 字段。它定义的是 Container Lifecycle Hooks。顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子： apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - nam","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:18:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我详细讲解了 Pod API 对象，介绍了 Pod 的核心使用方法，并分析了 Pod 和 Container 在字段上的异同。希望这些讲解能够帮你更好地理解和记忆 Pod YAML 中的核心字段，以及这些字段的准确含义。实际上，Pod API 对象是整个 Kubernetes 体系中最核心的一个概念，也是后面我讲解各种控制器时都要用到的。在学习完这篇文章后，我希望你能仔细阅读 $GOPATH/src/k8s.io/kubernetes/vendor/k8s.io/api/core/v1/types.go 里，type Pod struct ，尤其是 PodSpec 部分的内容。争取做到下次看到一个 Pod 的 YAML 文件时，不再需要查阅文档，就能做到把常用字段及其作用信手拈来。而在下一篇文章中，我会通过大量的实践，帮助你巩固和进阶关于 Pod API 对象核心字段的使用方法，敬请期待吧。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:18:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"15 | 深入解析Pod对象（二）：使用进阶 在上一篇文章中，我深入解析了 Pod 的 API 对象，讲解了 Pod 和 Container 的关系。作为 Kubernetes 项目里最核心的编排对象，Pod 携带的信息非常丰富。其中，资源定义（比如 CPU、内存等），以及调度相关的字段，我会在后面专门讲解调度器时再进行深入的分析。在本篇，我们就先从一种特殊的 Volume 开始，来帮助你更加深入地理解 Pod 对象各个重要字段的含义。这种特殊的 Volume，叫作 Projected Volume，你可以把它翻译为“投射数据卷”。 备注：Projected Volume 是 Kubernetes v1.11 之后的新特性 这是什么意思呢？在 Kubernetes 中，有几种特殊的 Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机之间的数据交换。这些特殊 Volume 的作用，是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被 Kubernetes“投射”（Project）进入容器当中的。这正是 Projected Volume 的含义。到目前为止，Kubernetes 支持的 Projected Volume 一共有四种： Secret；ConfigMap；Downward API；ServiceAccountToken。 在今天这篇文章中，我首先和你分享的是 Secret。它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。Secret 最典型的使用场景，莫过于存放数据库的 Credential 信息，比如下面这个例子： apiVersion: v1 kind: Pod metadata: name: test-projected-volume spec: containers: - name: test-secret-volume image: busybox args: - sleep - \"86400\" volumeMounts: - name: mysql-cred mountPath: \"/projected-volume\" readOnly: true volumes: - name: mysql-cred projected: sources: - secret: name: user - secret: name: pass 在这个 Pod 中，我定义了一个简单的容器。它声明挂载的 Volume，并不是常见的 emptyDir 或者 hostPath 类型，而是 projected 类型。而这个 Volume 的数据来源（sources），则是名为 user 和 pass 的 Secret 对象，分别对应的是数据库的用户名和密码。这里用到的数据库的用户名、密码，正是以 Secret 对象的方式交给 Kubernetes 保存的。完成这个操作的指令，如下所示： $ cat ./username.txt admin $ cat ./password.txt c1oudc0w! $ kubectl create secret generic user --from-file=./username.txt $ kubectl create secret generic pass --from-file=./password.txt 其中，username.txt 和 password.txt 文件里，存放的就是用户名和密码；而 user 和 pass，则是我为 Secret 对象指定的名字。而我想要查看这些 Secret 对象的话，只要执行一条 kubectl get 命令就可以了： $ kubectl get secrets NAME TYPE DATA AGE user Opaque 1 51s pass Opaque 1 51s 当然，除了使用 kubectl create secret 指令外，我也可以直接通过编写 YAML 文件的方式来创建这个 Secret 对象，比如： apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: user: YWRtaW4= pass: MWYyZDFlMmU2N2Rm 可以看到，通过编写 YAML 文件创建出来的 Secret 对象只有一个。但它的 data 字段，却以 Key-Value 的格式保存了两份 Secret 数据。其中，“user”就是第一份数据的 Key，“pass”是第二份数据的 Key。需要注意的是，Secret 对象要求这些数据必须是经过 Base64 转码的，以免出现明文密码的安全隐患。这个转码操作也很简单，比如： $ echo -n 'admin' | base64 YWRtaW4= $ echo -n '1f2d1e2e67df' | base64 MWYyZDFlMmU2N2Rm 这里需要注意的是，像这样创建的 Secret 对象，它里面的内容仅仅是经过了转码，而并没有被加密。在真正的生产环境中，你需要在 Kubernetes 中开启 Secret 的加密插件，增强数据的安全性。关于开启 Secret 加密插件的内容，我会在后续专门讲解 Secret 的时候，再做进一步说明。接下来，我们尝试一下创建这个 Pod： $ kubectl create -f test-projected-volume.yaml 当 Pod 变成 Running 状态之后，我们再验证一下这些 Secret 对象是不是已经在容器里了： $ kubectl exec -it test-projected-volume -- /bin/sh $ ls /projected-volume/ user pass $ cat /projected-volume/user root $ cat /projected-volume/pass 1f2d1e2e67df 从返回结果中，我们可以看到，保存在 Etcd 里的用户名和密码信息，已经以文件的形式出现在了容器的 Volume 目录里。而这个文件的名字，就是 kubectl create secret 指定的 Key，或者说是 Secret 对象的 data 字段指定的 Key。更重要的是，像这样通过挂载方式进入到容器里的 Secret，一旦其对应的 Etcd 里的数据被更新，这些 Volume 里的文件内容，同样也会被更新。其实，这是 kubelet 组件在定时维护这些 Volume。 需要注意的是，这个更新可能会有一定的延时。所以在编写应用程序时，在发起数据库连接的代码处写好重试和超时的逻辑，绝对是个好习惯。与 Secret 类似的是 ConfigMap，它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件。比如，一个 Java 应用所需的配置文件（.properties 文件），就可以通过下面这样的方式保存在 ConfigMap 里： # .properties文件的内容 $ cat example/ui.properties color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice # 从.properties文件创建ConfigMap $ kubectl create configmap ui-config --from-file=example/ui.properties # 查看这个ConfigMap里保存的信息(data) $ kubectl get configmaps ui-config -o yaml apiVersion: v1 data: ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice kind: ConfigMap metadata: name: ui-config ... 备注：kubectl get -o yaml 这样的参数，会将指定的 Pod API 对象以 YAML 的方式展示出来。 接下来是 Downward API，它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。举个例子： apiVersion: v1 kind: Pod metadata: name: test-downwardapi-volume labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\"sh\", \"-c\"] args: - while true; do if [[ -e /etc","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:19:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我和你详细介绍了 Pod 对象更高阶的使用方法，希望通过对这些实例的讲解，你可以更深入地理解 Pod API 对象的各个字段。而在学习这些字段的同时，你还应该认真体会一下 Kubernetes“一切皆对象”的设计思想：比如应用是 Pod 对象，应用的配置是 ConfigMap 对象，应用要访问的密码则是 Secret 对象。所以，也就自然而然地有了 PodPreset 这样专门用来对 Pod 进行批量化、自动化修改的工具对象。在后面的内容中，我会为你讲解更多的这种对象，还会和你介绍 Kubernetes 项目如何围绕着这些对象进行容器编排。在本专栏中，Pod 对象相关的知识点非常重要，它是接下来 Kubernetes 能够描述和编排各种复杂应用的基石所在，希望你能够继续多实践、多体会。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:19:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"x16 | 编排其实很简单：谈谈“控制器”模型 在上一篇文章中，我和你详细介绍了 Pod 的用法，讲解了 Pod 这个 API 对象的各个字段。而接下来，我们就一起来看看“编排”这个 Kubernetes 项目最核心的功能吧。实际上，你可能已经有所感悟：Pod 这个看似复杂的 API 对象，实际上就是对容器的进一步抽象和封装而已。说得更形象些，“容器”镜像虽然好用，但是容器这样一个“沙盒”的概念，对于描述应用来说，还是太过简单了。这就好比，集装箱固然好用，但是如果它四面都光秃秃的，吊车还怎么把这个集装箱吊起来并摆放好呢？所以，Pod 对象，其实就是容器的升级版。它对容器进行了组合，添加了更多的属性和字段。这就好比给集装箱四面安装了吊环，使得 Kubernetes 这架“吊车”，可以更轻松地操作它。而 Kubernetes 操作这些“集装箱”的逻辑，都由控制器（Controller）完成。在前面的第 12 篇文章《牛刀小试：我的第一个容器化应用》中，我们曾经使用过 Deployment 这个最基本的控制器对象。现在，我们一起来回顾一下这个名叫 nginx-deployment 的例子： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 这个 Deployment 定义的编排动作非常简单，即：确保携带了 app=nginx 标签的 Pod 的个数，永远等于 spec.replicas 指定的个数，即 2 个。这就意味着，如果在这个集群中，携带 app=nginx 标签的 Pod 的个数大于 2 的时候，就会有旧的 Pod 被删除；反之，就会有新的 Pod 被创建。这时，你也许就会好奇：究竟是 Kubernetes 项目中的哪个组件，在执行这些操作呢？我在前面介绍 Kubernetes 架构的时候，曾经提到过一个叫作 kube-controller-manager 的组件。实际上，这个组件，就是一系列控制器的集合。我们可以查看一下 Kubernetes 项目的 pkg/controller 目录： $ cd kubernetes/pkg/controller/ $ ls -d */ deployment/ job/ podautoscaler/ cloud/ disruption/ namespace/ replicaset/ serviceaccount/ volume/ cronjob/ garbagecollector/ nodelifecycle/ replication/ statefulset/ daemon/ ... 这个目录下面的每一个控制器，都以独有的方式负责某种编排功能。而我们的 Deployment，正是这些控制器中的一种。实际上，这些控制器之所以被统一放在 pkg/controller 目录下，就是因为它们都遵循 Kubernetes 项目中的一个通用编排模式，即：控制循环（control loop）。比如，现在有一种待编排的对象 X，它有一个对应的控制器。那么，我就可以用一段 Go 语言风格的伪代码，为你描述这个控制循环： for { 实际状态 := 获取集群中对象X的实际状态（Actual State） 期望状态 := 获取集群中对象X的期望状态（Desired State） if 实际状态 == 期望状态{ 什么都不做 } else { 执行编排动作，将实际状态调整为期望状态 } } 在具体实现中，实际状态往往来自于 Kubernetes 集群本身。比如，kubelet 通过心跳汇报的容器状态和节点状态，或者监控系统中保存的应用监控数据，或者控制器主动收集的它自己感兴趣的信息，这些都是常见的实际状态的来源。而期望状态，一般来自于用户提交的 YAML 文件。 比如，Deployment 对象中 Replicas 字段的值。很明显，这些信息往往都保存在 Etcd 中。接下来，以 Deployment 为例，我和你简单描述一下它对控制器模型的实现： Deployment 控制器从 Etcd 中获取到所有携带了“app: nginx”标签的 Pod，然后统计它们的数量，这就是实际状态；Deployment 对象的 Replicas 字段的值就是期望状态；Deployment 控制器将两个状态做比较，然后根据比较结果，确定是创建 Pod，还是删除已有的 Pod（具体如何操作 Pod 对象，我会在下一篇文章详细介绍）。 可以看到，一个 Kubernetes 对象的主要编排逻辑，实际上是在第三步的“对比”阶段完成的。这个操作，通常被叫作调谐（Reconcile）。这个调谐的过程，则被称作“Reconcile Loop”（调谐循环）或者“Sync Loop”（同步循环）。所以，如果你以后在文档或者社区中碰到这些词，都不要担心，它们其实指的都是同一个东西：控制循环。而调谐的最终结果，往往都是对被控制对象的某种写操作。比如，增加 Pod，删除已有的 Pod，或者更新 Pod 的某个字段。这也是 Kubernetes 项目“面向 API 对象编程”的一个直观体现。 其实，像 Deployment 这种控制器的设计原理，就是我们前面提到过的，“用一种对象管理另一种对象”的“艺术”。其中，这个控制器对象本身，负责定义被管理对象的期望状态。比如，Deployment 里的 replicas=2 这个字段。而被控制对象的定义，则来自于一个“模板”。比如，Deployment 里的 template 字段。可以看到，Deployment 这个 template 字段里的内容，跟一个标准的 Pod 对象的 API 定义，丝毫不差。而所有被这个 Deployment 管理的 Pod 实例，其实都是根据这个 template 字段的内容创建出来的。像 Deployment 定义的 template 字段，在 Kubernetes 项目中有一个专有的名字，叫作 PodTemplate（Pod 模板）。这个概念非常重要，因为后面我要讲解到的大多数控制器，都会使用 PodTemplate 来统一定义它所要管理的 Pod。更有意思的是，我们还会看到其他类型的对象模板，比如 Volume 的模板。至此，我们就可以对 Deployment 以及其他类似的控制器，做一个简单总结了： 如上图所示，类似 Deployment 这样的一个控制器，实际上都是由上半部分的控制器定义（包括期望状态），加上下半部分的被控制对象的模板组成的。这就是为什么，在所有 API 对象的 Metadata 里，都有一个字段叫作 ownerReference，用于保存当前这个 API 对象的拥有者（Owner）的信息。那么，对于我们这个 nginx-deployment 来说，它创建出来的 Pod 的 ownerReference 就是 nginx-deployment 吗？或者说，nginx-deployment 所直接控制的，就是 Pod 对象么？这个问题的答案，我就留到下一篇文章时再做详细解释吧。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:20:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我以 Deployment 为例，和你详细分享了 Kubernetes 项目如何通过一个称作“控制器模式”（controller pattern）的设计方法，来统一地实现对各种不同的对象或者资源进行的编排操作。在后面的讲解中，我还会讲到很多不同类型的容器编排功能，比如 StatefulSet、DaemonSet 等等，它们无一例外地都有这样一个甚至多个控制器的存在，并遵循控制循环（control loop）的流程，完成各自的编排逻辑。 实际上，跟 Deployment 相似，这些控制循环最后的执行结果，要么就是创建、更新一些 Pod（或者其他的 API 对象、资源），要么就是删除一些已经存在的 Pod（或者其他的 API 对象、资源）。但也正是在这个统一的编排框架下，不同的控制器可以在具体执行过程中，设计不同的业务逻辑，从而达到不同的编排效果。这个实现思路，正是 Kubernetes 项目进行容器编排的核心原理。在此后讲解 Kubernetes 编排功能的文章中，我都会遵循这个逻辑展开，并且带你逐步领悟控制器模式在不同的容器化作业中的实现方式。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:20:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"17 | 经典PaaS的记忆：作业副本与水平扩展 在上一篇文章中，我为你详细介绍了 Kubernetes 项目中第一个重要的设计思想：控制器模式。而在今天这篇文章中，我就来为你详细讲解一下，Kubernetes 里第一个控制器模式的完整实现：Deployment。Deployment 看似简单，但实际上，它实现了 Kubernetes 项目中一个非常重要的功能：Pod 的“水平扩展 / 收缩”（horizontal scaling out/in）。这个功能，是从 PaaS 时代开始，一个平台级项目就必须具备的编排能力。举个例子，如果你更新了 Deployment 的 Pod 模板（比如，修改了容器的镜像），那么 Deployment 就需要遵循一种叫作“滚动更新”（rolling update）的方式，来升级现有的容器。而这个能力的实现，依赖的是 Kubernetes 项目中的一个非常重要的概念（API 对象）：ReplicaSet。ReplicaSet 的结构非常简单，我们可以通过这个 YAML 文件查看一下： apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-set labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 从这个 YAML 文件中，我们可以看到，一个 ReplicaSet 对象，其实就是由副本数目的定义和一个 Pod 模板组成的。不难发现，它的定义其实是 Deployment 的一个子集。更重要的是，Deployment 控制器实际操纵的，正是这样的 ReplicaSet 对象，而不是 Pod 对象。还记不记得我在上一篇文章《编排其实很简单：谈谈“控制器”模型》中曾经提出过这样一个问题：对于一个 Deployment 所管理的 Pod，它的 ownerReference 是谁？所以，这个问题的答案就是：ReplicaSet。明白了这个原理，我再来和你一起分析一个如下所示的 Deployment： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 可以看到，这就是一个我们常用的 nginx-deployment，它定义的 Pod 副本个数是 3（spec.replicas=3）。那么，在具体的实现上，这个 Deployment，与 ReplicaSet，以及 Pod 的关系是怎样的呢？我们可以用一张图把它描述出来： 通过这张图，我们就很清楚地看到，一个定义了 replicas=3 的 Deployment，与它的 ReplicaSet，以及 Pod 的关系，实际上是一种“层层控制”的关系。其中，ReplicaSet 负责通过“控制器模式”，保证系统中 Pod 的个数永远等于指定的个数（比如，3 个）。这也正是 Deployment 只允许容器的 restartPolicy=Always 的主要原因：只有在容器能保证自己始终是 Running 状态的前提下，ReplicaSet 调整 Pod 的个数才有意义。而在此基础上，Deployment 同样通过“控制器模式”，来操作 ReplicaSet 的个数和属性，进而实现“水平扩展 / 收缩”和“滚动更新”这两个编排动作。其中，“水平扩展 / 收缩”非常容易实现，Deployment Controller 只需要修改它所控制的 ReplicaSet 的 Pod 副本个数就可以了。比如，把这个值从 3 改成 4，那么 Deployment 所对应的 ReplicaSet，就会根据修改后的值自动创建一个新的 Pod。这就是“水平扩展”了；“水平收缩”则反之。而用户想要执行这个操作的指令也非常简单，就是 kubectl scale，比如： $ kubectl scale deployment nginx-deployment --replicas=4 deployment.apps/nginx-deployment scaled 那么，“滚动更新”又是什么意思，是如何实现的呢？接下来，我还以这个 Deployment 为例，来为你讲解“滚动更新”的过程。首先，我们来创建这个 nginx-deployment： $ kubectl create -f nginx-deployment.yaml --record 注意，在这里，我额外加了一个–record 参数。它的作用，是记录下你每次操作所执行的命令，以方便后面查看。然后，我们来检查一下 nginx-deployment 创建后的状态信息： $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s 在返回结果中，我们可以看到四个状态字段，它们的含义如下所示。DESIRED：用户期望的 Pod 副本个数（spec.replicas 的值）；CURRENT：当前处于 Running 状态的 Pod 的个数；UP-TO-DATE：当前处于最新版本的 Pod 的个数，所谓最新版本指的是 Pod 的 Spec 部分与 Deployment 里 Pod 模板里定义的完全一致；AVAILABLE：当前已经可用的 Pod 的个数，即：既是 Running 状态，又是最新版本，并且已经处于 Ready（健康检查正确）状态的 Pod 的个数。 可以看到，只有这个 AVAILABLE 字段，描述的才是用户所期望的最终状态。而 Kubernetes 项目还为我们提供了一条指令，让我们可以实时查看 Deployment 对象的状态变化。这个指令就是 kubectl rollout status： $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment.apps/nginx-deployment successfully rolled out 在这个返回结果中，“2 out of 3 new replicas have been updated”意味着已经有 2 个 Pod 进入了 UP-TO-DATE 状态。继续等待一会儿，我们就能看到这个 Deployment 的 3 个 Pod，就进入到了 AVAILABLE 状态： NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 20s 此时，你可以尝试查看一下这个 Deployment 所控制的 ReplicaSet： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3167673210 3 3 3 20s 如上所示，在用户提交了一个 Deployment 对象后，Deployment Controller 就会立即创建一个 Pod 副本个数为 3 的 ReplicaSet。这个 ReplicaSet 的名字，则是由 Deployment 的名字和一个随机字符串共同组成。这个随机字符串叫作 pod-template-hash，在我们这个例子里就是：3167673210。ReplicaSet 会把这个随机字符串加在它所控制的所有 Pod 的标签里，从而保证这些 Pod 不会与集群里的其他 Pod 混淆。而 ReplicaSet 的 DESIRED、CURRENT 和 READY 字段的含义，和 Deployment 中是一致的。所以，相比之下，Deployment 只是在 ReplicaSet 的基础上，添加了 UP-TO-DATE 这个跟版本有关的状态字段。 这个时候，如果我们修改了 Deployment 的 Pod 模板，“滚动更新”就会被自动触发。修改 Deployment 有很多方法。比如，我可以直接使用 kubectl edit 指令编辑 Etcd 里的 API 对象。 $ kubectl edit deployment/nginx-deployment ... spec: containers: - name: nginx image: nginx:1.9.1 # 1.7.9 -\u003e 1.9.1 ports: - containerPort: 80 ... deployment.extensions/nginx-deployment edited 这个 kubectl edit 指令，会帮你直接打开 n","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:21:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我为你详细讲解了 Deployment 这个 Kubernetes 项目中最基本的编排控制器的实现原理和使用方法。通过这些讲解，你应该了解到：Deployment 实际上是一个两层控制器。首先，它通过 ReplicaSet 的个数来描述应用的版本；然后，它再通过 ReplicaSet 的属性（比如 replicas 的值），来保证 Pod 的副本数量。备注：Deployment 控制 ReplicaSet（版本），ReplicaSet 控制 Pod（副本数）。这个两层控制关系一定要牢记。不过，相信你也能够感受到，Kubernetes 项目对 Deployment 的设计，实际上是代替我们完成了对“应用”的抽象，使得我们可以使用这个 Deployment 对象来描述应用，使用 kubectl rollout 命令控制应用的版本。可是，在实际使用场景中，应用发布的流程往往千差万别，也可能有很多的定制化需求。比如，我的应用可能有会话黏连（session sticky），这就意味着“滚动更新”的时候，哪个 Pod 能下线，是不能随便选择的。这种场景，光靠 Deployment 自己就很难应对了。对于这种需求，我在专栏后续文章中重点介绍的“自定义控制器”，就可以帮我们实现一个功能更加强大的 Deployment Controller。当然，Kubernetes 项目本身，也提供了另外一种抽象方式，帮我们应对其他一些用 Deployment 无法处理的应用编排场景。这个设计，就是对有状态应用的管理，也是我在下一篇文章中要重点讲解的内容。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:21:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"18 | 深入理解StatefulSet（一）：拓扑状态 在上一篇文章中，我在结尾处讨论到了 Deployment 实际上并不足以覆盖所有的应用编排问题。造成这个问题的根本原因，在于 Deployment 对应用做了一个简单化假设。它认为，一个应用的所有 Pod，是完全一样的。所以，它们互相之间没有顺序，也无所谓运行在哪台宿主机上。需要的时候，Deployment 就可以通过 Pod 模板创建新的 Pod；不需要的时候，Deployment 就可以“杀掉”任意一个 Pod。但是，在实际的场景中，并不是所有的应用都可以满足这样的要求。尤其是分布式应用，它的多个实例之间，往往有依赖关系，比如：主从关系、主备关系。还有就是数据存储类应用，它的多个实例，往往都会在本地磁盘上保存一份数据。而这些实例一旦被杀掉，即便重建出来，实例与数据之间的对应关系也已经丢失，从而导致应用失败。所以，这种实例之间有不对等关系，以及实例对外部数据有依赖关系的应用，就被称为“有状态应用”（Stateful Application）。容器技术诞生后，大家很快发现，它用来封装“无状态应用”（Stateless Application），尤其是 Web 服务，非常好用。但是，一旦你想要用容器运行“有状态应用”，其困难程度就会直线上升。而且，这个问题解决起来，单纯依靠容器技术本身已经无能为力，这也就导致了很长一段时间内，“有状态应用”几乎成了容器技术圈子的“忌讳”，大家一听到这个词，就纷纷摇头。 不过，Kubernetes 项目还是成为了“第一个吃螃蟹的人”。得益于“控制器模式”的设计思想，Kubernetes 项目很早就在 Deployment 的基础上，扩展出了对“有状态应用”的初步支持。这个编排功能，就是：StatefulSet。StatefulSet 的设计其实非常容易理解。它把真实世界里的应用状态，抽象为了两种情况：拓扑状态。这种情况意味着，应用的多个实例之间不是完全对等的关系。这些应用实例，必须按照某些顺序启动，比如应用的主节点 A 要先于从节点 B 启动。而如果你把 A 和 B 两个 Pod 删除掉，它们再次被创建出来时也必须严格按照这个顺序才行。并且，新创建出来的 Pod，必须和原来 Pod 的网络标识一样，这样原先的访问者才能使用同样的方法，访问到这个新 Pod。存储状态。这种情况意味着，应用的多个实例分别绑定了不同的存储数据。对于这些应用实例来说，Pod A 第一次读取到的数据，和隔了十分钟之后再次读取到的数据，应该是同一份，哪怕在此期间 Pod A 被重新创建过。这种情况最典型的例子，就是一个数据库应用的多个存储实例。 所以，StatefulSet 的核心功能，就是通过某种方式记录这些状态，然后在 Pod 被重新创建时，能够为新 Pod 恢复这些状态。在开始讲述 StatefulSet 的工作原理之前，我就必须先为你讲解一个 Kubernetes 项目中非常实用的概念：Headless Service。我在和你一起讨论 Kubernetes 架构的时候就曾介绍过，Service 是 Kubernetes 项目中用来将一组 Pod 暴露给外界访问的一种机制。比如，一个 Deployment 有 3 个 Pod，那么我就可以定义一个 Service。然后，用户只要能访问到这个 Service，它就能访问到某个具体的 Pod。那么，这个 Service 又是如何被访问的呢？ 第一种方式，是以 Service 的 VIP（Virtual IP，即：虚拟 IP）方式。比如：当我访问 10.0.23.1 这个 Service 的 IP 地址时，10.0.23.1 其实就是一个 VIP，它会把请求转发到该 Service 所代理的某一个 Pod 上。这里的具体原理，我会在后续的 Service 章节中进行详细介绍。第二种方式，就是以 Service 的 DNS 方式。比如：这时候，只要我访问“my-svc.my-namespace.svc.cluster.local”这条 DNS 记录，就可以访问到名叫 my-svc 的 Service 所代理的某一个 Pod。 而在第二种 Service DNS 的方式下，具体还可以分为两种处理方法：第一种处理方法，是 Normal Service。这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，正是 my-svc 这个 Service 的 VIP，后面的流程就跟 VIP 方式一致了。而第二种处理方法，正是 Headless Service。这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，直接就是 my-svc 代理的某一个 Pod 的 IP 地址。可以看到，这里的区别在于，Headless Service 不需要分配一个 VIP，而是可以直接以 DNS 记录的方式解析出被代理 Pod 的 IP 地址。那么，这样的设计又有什么作用呢？想要回答这个问题，我们需要从 Headless Service 的定义方式看起。下面是一个标准的 Headless Service 对应的 YAML 文件： apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx 可以看到，所谓的 Headless Service，其实仍是一个标准 Service 的 YAML 文件。只不过，它的 clusterIP 字段的值是：None，即：这个 Service，没有一个 VIP 作为“头”。这也就是 Headless 的含义。所以，这个 Service 被创建后并不会被分配一个 VIP，而是会以 DNS 记录的方式暴露出它所代理的 Pod。而它所代理的 Pod，依然是采用我在前面第 12 篇文章《牛刀小试：我的第一个容器化应用》中提到的 Label Selector 机制选择出来的，即：所有携带了 app=nginx 标签的 Pod，都会被这个 Service 代理起来。然后关键来了。当你按照这样的方式创建了一个 Headless Service 之后，它所代理的所有 Pod 的 IP 地址，都会被绑定一个这样格式的 DNS 记录，如下所示： \u003cpod-name\u003e.\u003csvc-name\u003e.\u003cnamespace\u003e.svc.cluster.local 这个 DNS 记录，正是 Kubernetes 项目为 Pod 分配的唯一的“可解析身份”（Resolvable Identity）。有了这个“可解析身份”，只要你知道了一个 Pod 的名字，以及它对应的 Service 的名字，你就可以非常确定地通过这条 DNS 记录访问到 Pod 的 IP 地址。那么，StatefulSet 又是如何使用这个 DNS 记录来维持 Pod 的拓扑状态的呢？为了回答这个问题，现在我们就来编写一个 StatefulSet 的 YAML 文件，如下所示： apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web 这个 YAML 文件，和我们在前面文章中用到的 nginx-deployment 的唯一区别，就是多了一个 serviceName=nginx 字段。这个字段的作用，就是告诉 StatefulSet 控制器，在执行控制循环（Control Loop）的时候，请使用 nginx 这个 Headless Service 来保证 Pod 的“可解析身份”。所以，当你通过 kubectl create 创建了上面这个 Service 和 StatefulSet 之后，就会看到如下两个对象： $ kubectl create -f svc.yaml $ kubectl get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP None \u003cnone\u003e 80/TCP 10s $ kubectl create -f statefulset.yaml $ kubectl get statefulset web NAME DESIRED CURRENT AGE web 2 1 19s 这时候，如果你手比较快的话，还可以通过 kubectl 的 -w 参数，即：Watch 功能，实时查看 StatefulSet 创建两个有状态实例的过程：备注：如果手不够快的话，Pod 很快就创建完了。不过，你依然可以通过这个 StatefulSet 的 Events 看到这些信息。 $ kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:22:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我首先和你分享了 StatefulSet 的基本概念，解释了什么是应用的“状态”。紧接着 ，我为你分析了 StatefulSet 如何保证应用实例之间“拓扑状态”的稳定性。如果用一句话来总结的话，你可以这么理解这个过程：StatefulSet 这个控制器的主要作用之一，就是使用 Pod 模板创建 Pod 的时候，对它们进行编号，并且按照编号顺序逐一完成创建工作。而当 StatefulSet 的“控制循环”发现 Pod 的“实际状态”与“期望状态”不一致，需要新建或者删除 Pod 进行“调谐”的时候，它会严格按照这些 Pod 编号的顺序，逐一完成这些操作。 所以，StatefulSet 其实可以认为是对 Deployment 的改良。与此同时，通过 Headless Service 的方式，StatefulSet 为每个 Pod 创建了一个固定并且稳定的 DNS 记录，来作为它的访问入口。实际上，在部署“有状态应用”的时候，应用的每个实例拥有唯一并且稳定的“网络标识”，是一个非常重要的假设。在下一篇文章中，我将会继续为你剖析 StatefulSet 如何处理存储状态。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:22:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"19 | 深入理解StatefulSet（二）：存储状态 在上一篇文章中，我和你分享了 StatefulSet 如何保证应用实例的拓扑状态，在 Pod 删除和再创建的过程中保持稳定。而在今天这篇文章中，我将继续为你解读 StatefulSet 对存储状态的管理机制。这个机制，主要使用的是一个叫作 Persistent Volume Claim 的功能。在前面介绍 Pod 的时候，我曾提到过，要在一个 Pod 里声明 Volume，只要在 Pod 里加上 spec.volumes 字段即可。然后，你就可以在这个字段里定义一个具体类型的 Volume 了，比如：hostPath。可是，你有没有想过这样一个场景：如果你并不知道有哪些 Volume 类型可以用，要怎么办呢？ 更具体地说，作为一个应用开发者，我可能对持久化存储项目（比如 Ceph、GlusterFS 等）一窍不通，也不知道公司的 Kubernetes 集群里到底是怎么搭建出来的，我也自然不会编写它们对应的 Volume 定义文件。所谓“术业有专攻”，这些关于 Volume 的管理和远程持久化存储的知识，不仅超越了开发者的知识储备，还会有暴露公司基础设施秘密的风险。比如，下面这个例子，就是一个声明了 Ceph RBD 类型 Volume 的 Pod： apiVersion: v1 kind: Pod metadata: name: rbd spec: containers: - image: kubernetes/pause name: rbd-rw volumeMounts: - name: rbdpd mountPath: /mnt/rbd volumes: - name: rbdpd rbd: monitors: - '10.16.154.78:6789' - '10.16.154.82:6789' - '10.16.154.83:6789' pool: kube image: foo fsType: ext4 readOnly: true user: admin keyring: /etc/ceph/keyring imageformat: \"2\" imagefeatures: \"layering\" 其一，如果不懂得 Ceph RBD 的使用方法，那么这个 Pod 里 Volumes 字段，你十有八九也完全看不懂。其二，这个 Ceph RBD 对应的存储服务器的地址、用户名、授权文件的位置，也都被轻易地暴露给了全公司的所有开发人员，这是一个典型的信息被“过度暴露”的例子。这也是为什么，在后来的演化中，Kubernetes 项目引入了一组叫作 Persistent Volume Claim（PVC）和 Persistent Volume（PV）的 API 对象，大大降低了用户声明和使用持久化 Volume 的门槛。举个例子，有了 PVC 之后，一个开发人员想要使用一个 Volume，只需要简单的两步即可。第一步：定义一个 PVC，声明想要的 Volume 的属性： kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pv-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 可以看到，在这个 PVC 对象里，不需要任何关于 Volume 细节的字段，只有描述性的属性和定义。比如，storage: 1Gi，表示我想要的 Volume 大小至少是 1 GiB；accessModes: ReadWriteOnce，表示这个 Volume 的挂载方式是可读写，并且只能被挂载在一个节点上而非被多个节点共享。备注：关于哪种类型的 Volume 支持哪种类型的 AccessMode，你可以查看 Kubernetes 项目官方文档中的详细列表。 第二步：在应用的 Pod 中，声明使用这个 PVC： apiVersion: v1 kind: Pod metadata: name: pv-pod spec: containers: - name: pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: pv-storage volumes: - name: pv-storage persistentVolumeClaim: claimName: pv-claim 可以看到，在这个 Pod 的 Volumes 定义中，我们只需要声明它的类型是 persistentVolumeClaim，然后指定 PVC 的名字，而完全不必关心 Volume 本身的定义。这时候，只要我们创建这个 PVC 对象，Kubernetes 就会自动为它绑定一个符合条件的 Volume。可是，这些符合条件的 Volume 又是从哪里来的呢？答案是，它们来自于由运维人员维护的 PV（Persistent Volume）对象。接下来，我们一起看一个常见的 PV 对象的 YAML 文件： kind: PersistentVolume apiVersion: v1 metadata: name: pv-volume labels: type: local spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce rbd: monitors: # 使用 kubectl get pods -n rook-ceph 查看 rook-ceph-mon- 开头的 POD IP 即可得下面的列表 - '10.16.154.78:6789' - '10.16.154.82:6789' - '10.16.154.83:6789' pool: kube image: foo fsType: ext4 readOnly: true user: admin keyring: /etc/ceph/keyring 可以看到，这个 PV 对象的 spec.rbd 字段，正是我们前面介绍过的 Ceph RBD Volume 的详细定义。而且，它还声明了这个 PV 的容量是 10 GiB。这样，Kubernetes 就会为我们刚刚创建的 PVC 对象绑定这个 PV。所以，Kubernetes 中 PVC 和 PV 的设计，实际上类似于“接口”和“实现”的思想。开发者只要知道并会使用“接口”，即：PVC；而运维人员则负责给“接口”绑定具体的实现，即：PV。这种解耦，就避免了因为向开发者暴露过多的存储系统细节而带来的隐患。此外，这种职责的分离，往往也意味着出现事故时可以更容易定位问题和明确责任，从而避免“扯皮”现象的出现。 而 PVC、PV 的设计，也使得 StatefulSet 对存储状态的管理成为了可能。我们还是以上一篇文章中用到的 StatefulSet 为例（你也可以借此再回顾一下《深入理解 StatefulSet（一）：拓扑状态》中的相关内容）： apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 这次，我们为这个 StatefulSet 额外添加了一个 volumeClaimTemplates 字段。从名字就可以看出来，它跟 Deployment 里 Pod 模板（PodTemplate）的作用类似。也就是说，凡是被这个 StatefulSet 管理的 Pod，都会声明一个对应的 PVC；而这个 PVC 的定义，就来自于 volumeClaimTemplates 这个模板字段。更重要的是，这个 PVC 的名字，会被分配一个与这个 Pod 完全一致的编号。这个自动创建的 PVC，与 PV 绑定成功后，就会进入 Bound 状态，这就意味着这个 Pod 可以挂载并使用这个 PV 了。如果你还是不太理解 PVC 的话，可以先记住这样一个结论：**PVC 其实就是一种特殊的 Volume。**只不过一个 PVC 具体是什么类型的 Volume，要在跟某个 PV 绑定之后才知道。关于 PV、PVC 更详细的知识，我会在容器存储部分做进一步解读。 当然，PVC 与 PV 的绑定得以实现的前提是，运维人员已经在系统里创建好了符合条件的 PV（比如，我们在前面用到的 pv-volume）；或者，你的 Kubernetes 集群运行在公有云上，这样 Kubernetes 就会","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:23:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我为你详细介绍了 StatefulSet 处理存储状态的方法。然后，以此为基础，我为你梳理了 StatefulSet 控制器的工作原理。从这些讲述中，我们不难看出 StatefulSet 的设计思想：StatefulSet 其实就是一种特殊的 Deployment，而其独特之处在于，它的每个 Pod 都被编号了。而且，这个编号会体现在 Pod 的名字和 hostname 等标识信息上，这不仅代表了 Pod 的创建顺序，也是 Pod 的重要网络标识（即：在整个集群里唯一的、可被访问的身份）。有了这个编号后，StatefulSet 就使用 Kubernetes 里的两个标准功能：Headless Service 和 PV/PVC，实现了对 Pod 的拓扑状态和存储状态的维护。实际上，在下一篇文章的“有状态应用”实践环节，以及后续的讲解中，你就会逐渐意识到，StatefulSet 可以说是 Kubernetes 中作业编排的“集大成者”。因为，几乎每一种 Kubernetes 的编排功能，都可以在编写 StatefulSet 的 YAML 文件时被用到。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:23:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"20 | 深入理解StatefulSet（三）：有状态应用实践 在前面的两篇文章中，我详细讲解了 StatefulSet 的工作原理，以及处理拓扑状态和存储状态的方法。而在今天这篇文章中，我将通过一个实际的例子，再次为你深入解读一下部署一个 StatefulSet 的完整流程。今天我选择的实例是部署一个 MySQL 集群，这也是 Kubernetes 官方文档里的一个经典案例。但是，很多工程师都曾向我吐槽说这个例子“完全看不懂”。其实，这样的吐槽也可以理解：相比于 Etcd、Cassandra 等“原生”就考虑了分布式需求的项目，MySQL 以及很多其他的数据库项目，在分布式集群的搭建上并不友好，甚至有点“原始”。所以，这次我就直接选择了这个具有挑战性的例子，和你分享如何使用 StatefulSet 将它的集群搭建过程“容器化”。 备注：在开始实践之前，请确保我们之前一起部署的那个 Kubernetes 集群还是可用的，并且网络插件和存储插件都能正常运行。具体的做法，请参考第 11 篇文章《从 0 到 1：搭建一个完整的 Kubernetes 集群》的内容。 首先，用自然语言来描述一下我们想要部署的“有状态应用”。是一个“主从复制”（Maser-Slave Replication）的 MySQL 集群；有 1 个主节点（Master）；有多个从节点（Slave）；从节点需要能水平扩展；所有的写操作，只能在主节点上执行；读操作可以在所有节点上执行。 这就是一个非常典型的主从模式的 MySQL 集群了。我们可以把上面描述的“有状态应用”的需求，通过一张图来表示。 在常规环境里，部署这样一个主从模式的 MySQL 集群的主要难点在于：如何让从节点能够拥有主节点的数据，即：如何配置主（Master）从（Slave）节点的复制与同步。所以，在安装好 MySQL 的 Master 节点之后，你需要做的第一步工作，就是通过 XtraBackup 将 Master 节点的数据备份到指定目录。备注：XtraBackup 是业界主要使用的开源 MySQL 备份和恢复工具。这一步会自动在目标目录里生成一个备份信息文件，名叫：xtrabackup_binlog_info。这个文件一般会包含如下两个信息： $ cat xtrabackup_binlog_info TheMaster-bin.000001 481 这两个信息会在接下来配置 Slave 节点的时候用到。第二步：配置 Slave 节点。Slave 节点在第一次启动前，需要先把 Master 节点的备份数据，连同备份信息文件，一起拷贝到自己的数据目录（/var/lib/mysql）下。然后，我们执行这样一句 SQL： TheSlave|mysql\u003e CHANGE MASTER TO MASTER_HOST='$masterip', MASTER_USER='xxx', MASTER_PASSWORD='xxx', MASTER_LOG_FILE='TheMaster-bin.000001', MASTER_LOG_POS=481; 其中，MASTER_LOG_FILE 和 MASTER_LOG_POS，就是该备份对应的二进制日志（Binary Log）文件的名称和开始的位置（偏移量），也正是 xtrabackup_binlog_info 文件里的那两部分内容（即：TheMaster-bin.000001 和 481）。第三步，启动 Slave 节点。在这一步，我们需要执行这样一句 SQL： TheSlave|mysql\u003e START SLAVE; 这样，Slave 节点就启动了。它会使用备份信息文件中的二进制日志文件和偏移量，与主节点进行数据同步。第四步，在这个集群中添加更多的 Slave 节点。 需要注意的是，新添加的 Slave 节点的备份数据，来自于已经存在的 Slave 节点。所以，在这一步，我们需要将 Slave 节点的数据备份在指定目录。而这个备份操作会自动生成另一种备份信息文件，名叫：xtrabackup_slave_info。同样地，这个文件也包含了 MASTER_LOG_FILE 和 MASTER_LOG_POS 两个字段。然后，我们就可以执行跟前面一样的“CHANGE MASTER TO”和“START SLAVE” 指令，来初始化并启动这个新的 Slave 节点了。通过上面的叙述，我们不难看到，将部署 MySQL 集群的流程迁移到 Kubernetes 项目上，需要能够“容器化”地解决下面的“三座大山”： Master 节点和 Slave 节点需要有不同的配置文件（即：不同的 my.cnf）；Master 节点和 Slave 节点需要能够传输备份信息文件；在 Slave 节点第一次启动之前，需要执行一些初始化 SQL 操作； 而由于 MySQL 本身同时拥有拓扑状态（主从节点的区别）和存储状态（MySQL 保存在本地的数据），我们自然要通过 StatefulSet 来解决这“三座大山”的问题。其中，“第一座大山：Master 节点和 Slave 节点需要有不同的配置文件”，很容易处理：我们只需要给主从节点分别准备两份不同的 MySQL 配置文件，然后根据 Pod 的序号（Index）挂载进去即可。正如我在前面文章中介绍过的，这样的配置文件信息，应该保存在 ConfigMap 里供 Pod 使用。它的定义如下所示： apiVersion: v1 kind: ConfigMap metadata: name: mysql labels: app: mysql data: master.cnf: | # 主节点MySQL的配置文件 [mysqld] log-bin slave.cnf: | # 从节点MySQL的配置文件 [mysqld] super-read-only 在这里，我们定义了 master.cnf 和 slave.cnf 两个 MySQL 的配置文件。master.cnf 开启了 log-bin，即：使用二进制日志文件的方式进行主从复制，这是一个标准的设置。slave.cnf 的开启了 super-read-only，代表的是从节点会拒绝除了主节点的数据同步操作之外的所有写操作，即：它对用户是只读的。 而上述 ConfigMap 定义里的 data 部分，是 Key-Value 格式的。比如，master.cnf 就是这份配置数据的 Key，而“|”后面的内容，就是这份配置数据的 Value。这份数据将来挂载进 Master 节点对应的 Pod 后，就会在 Volume 目录里生成一个叫作 master.cnf 的文件。备注：如果你对 ConfigMap 的用法感到陌生的话，可以稍微复习一下第 15 篇文章《深入解析 Pod 对象（二）：使用进阶》中，我讲解 Secret 对象部分的内容。因为，ConfigMap 跟 Secret，无论是使用方法还是实现原理，几乎都是一样的。 接下来，我们需要创建两个 Service 来供 StatefulSet 以及用户使用。这两个 Service 的定义如下所示： apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql 可以看到，这两个 Service 都代理了所有携带 app=mysql 标签的 Pod，也就是所有的 MySQL Pod。端口映射都是用 Service 的 3306 端口对应 Pod 的 3306 端口。不同的是，第一个名叫“mysql”的 Service 是一个 Headless Service（即：clusterIP= None）。所以它的作用，是通过为 Pod 分配 DNS 记录来固定它的拓扑状态，比如“mysql-0.mysql”和“mysql-1.mysql”这样的 DNS 名字。其中，编号为 0 的节点就是我们的主节点。而第二个名叫“mysql-read”的 Service，则是一个常规的 Service。 并且我们规定，所有用户的读请求，都必须访问第二个 Service 被自动分配的 DNS 记录，即：“mysql-read”（当然，也可以访问这个 Service 的 VIP）。这样，读请求就可以被转发到任意一个 MySQL 的主节点或者从节点上。备注：Kubernetes 中的所有 Service、Pod 对象，都会被自动分配同名的 DNS 记录。具体细节，我会在后面 Service 部分做重点讲解。而所有用户的写请求，则必须直接以 DNS 记录的方式访问到 MySQL 的主节点，也就是：“mysql-0.mysql“这条 DNS 记录。 接下来，我们再一起解决“第二座大山：Master 节点和 Slave 节点需要能够传输备份文件”的问题。 翻越这座大山的思路，我比较推荐的做法是：先搭建框架，再完善细节。其中，Pod 部分如何定义，是完善细节时的重点。所以首先，我们先为 StatefulSet 对象规划一个大致的","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:24:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我以 MySQL 集群为例，和你详细分享了一个实际的 StatefulSet 的编写过程。这个 YAML 文件的链接在这里，希望你能多花一些时间认真消化。在这个过程中，有以下几个关键点（坑）特别值得你注意和体会。 “人格分裂”：在解决需求的过程中，一定要记得思考，该 Pod 在扮演不同角色时的不同操作。“阅后即焚”：很多“有状态应用”的节点，只是在第一次启动的时候才需要做额外处理。所以，在编写 YAML 文件时，你一定要考虑“容器重启”的情况，不要让这一次的操作干扰到下一次的容器启动。“容器之间平等无序”：除非是 InitContainer，否则一个 Pod 里的多个容器之间，是完全平等的。所以，你精心设计的 sidecar，绝不能对容器的顺序做出假设，否则就需要进行前置检查。 最后，相信你也已经能够理解，StatefulSet 其实是一种特殊的 Deployment，只不过这个“Deployment”的每个 Pod 实例的名字里，都携带了一个唯一并且固定的编号。这个编号的顺序，固定了 Pod 的拓扑关系；这个编号对应的 DNS 记录，固定了 Pod 的访问方式；这个编号对应的 PV，绑定了 Pod 与持久化存储的关系。所以，当 Pod 被删除重建时，这些“状态”都会保持不变。而一旦你的应用没办法通过上述方式进行状态的管理，那就代表了 StatefulSet 已经不能解决它的部署问题了。这时候，我后面讲到的 Operator，可能才是一个更好的选择。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:24:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"21 | 容器化守护进程的意义：DaemonSet 在上一篇文章中，我和你详细分享了使用 StatefulSet 编排“有状态应用”的过程。从中不难看出，StatefulSet 其实就是对现有典型运维业务的容器化抽象。也就是说，你一定有方法在不使用 Kubernetes、甚至不使用容器的情况下，自己 DIY 一个类似的方案出来。但是，一旦涉及到升级、版本管理等更工程化的能力，Kubernetes 的好处，才会更加凸现。比如，如何对 StatefulSet 进行“滚动更新”（rolling update）？很简单。你只要修改 StatefulSet 的 Pod 模板，就会自动触发“滚动更新”: $ kubectl patch statefulset mysql --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\":\"mysql:5.7.23\"}]' statefulset.apps/mysql patched 在这里，我使用了 kubectl patch 命令。它的意思是，以“补丁”的方式（JSON 格式的）修改一个 API 对象的指定字段，也就是我在后面指定的“spec/template/spec/containers/0/image”。这样，StatefulSet Controller 就会按照与 Pod 编号相反的顺序，从最后一个 Pod 开始，逐一更新这个 StatefulSet 管理的每个 Pod。而如果更新发生了错误，这次“滚动更新”就会停止。此外，StatefulSet 的“滚动更新”还允许我们进行更精细的控制，比如金丝雀发布（Canary Deploy）或者灰度发布，这意味着应用的多个实例中被指定的一部分不会被更新到最新的版本。这个字段，正是 StatefulSet 的 spec.updateStrategy.rollingUpdate 的 partition 字段。比如，现在我将前面这个 StatefulSet 的 partition 字段设置为 2： $ kubectl patch statefulset mysql -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":2}}}}' statefulset.apps/mysql patched 其中，kubectl patch 命令后面的参数（JSON 格式的），就是 partition 字段在 API 对象里的路径。所以，上述操作等同于直接使用 kubectl edit 命令，打开这个对象，把 partition 字段修改为 2。这样，我就指定了当 Pod 模板发生变化的时候，比如 MySQL 镜像更新到 5.7.23，那么只有序号大于或者等于 2 的 Pod 会被更新到这个版本。并且，如果你删除或者重启了序号小于 2 的 Pod，等它再次启动后，也会保持原先的 5.7.2 版本，绝不会被升级到 5.7.23 版本。StatefulSet 可以说是 Kubernetes 项目中最为复杂的编排对象，希望你课后能认真消化，动手实践一下这个例子。 而在今天这篇文章中，我会为你重点讲解一个相对轻松的知识点：DaemonSet。顾名思义，DaemonSet 的主要作用，是让你在 Kubernetes 集群里，运行一个 Daemon Pod。 所以，这个 Pod 有如下三个特征：这个 Pod 运行在 Kubernetes 集群里的每一个节点（Node）上；每个节点上只有一个这样的 Pod 实例；当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉。 这个机制听起来很简单，但 Daemon Pod 的意义确实是非常重要的。我随便给你列举几个例子：各种网络插件的 Agent 组件，都必须运行在每一个节点上，用来处理这个节点上的容器网络；各种存储插件的 Agent 组件，也必须运行在每一个节点上，用来在这个节点上挂载远程存储目录，操作容器的 Volume 目录；各种监控组件和日志组件，也必须运行在每一个节点上，负责这个节点上的监控信息和日志搜集。 更重要的是，跟其他编排对象不一样，DaemonSet 开始运行的时机，很多时候比整个 Kubernetes 集群出现的时机都要早。这个乍一听起来可能有点儿奇怪。但其实你来想一下：如果这个 DaemonSet 正是一个网络插件的 Agent 组件呢？这个时候，整个 Kubernetes 集群里还没有可用的容器网络，所有 Worker 节点的状态都是 NotReady（NetworkReady=false）。这种情况下，普通的 Pod 肯定不能运行在这个集群上。所以，这也就意味着 DaemonSet 的设计，必须要有某种“过人之处”才行。为了弄清楚 DaemonSet 的工作原理，我们还是按照老规矩，先从它的 API 对象的定义说起。 apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 这个 DaemonSet，管理的是一个 fluentd-elasticsearch 镜像的 Pod。这个镜像的功能非常实用：通过 fluentd 将 Docker 容器里的日志转发到 ElasticSearch 中。可以看到，DaemonSet 跟 Deployment 其实非常相似，只不过是没有 replicas 字段；它也使用 selector 选择管理所有携带了 name=fluentd-elasticsearch 标签的 Pod。而这些 Pod 的模板，也是用 template 字段定义的。在这个字段中，我们定义了一个使用 fluentd-elasticsearch:1.20 镜像的容器，而且这个容器挂载了两个 hostPath 类型的 Volume，分别对应宿主机的 /var/log 目录和 /var/lib/docker/containers 目录。显然，fluentd 启动之后，它会从这两个目录里搜集日志信息，并转发给 ElasticSearch 保存。这样，我们通过 ElasticSearch 就可以很方便地检索这些日志了。需要注意的是，Docker 容器里应用的日志，默认会保存在宿主机的 /var/lib/docker/containers/{ {. 容器 ID}}/{ {. 容器 ID}}-json.log 文件里，所以这个目录正是 fluentd 的搜集目标。 那么，DaemonSet 又是如何保证每个 Node 上有且只有一个被管理的 Pod 呢？显然，这是一个典型的“控制器模型”能够处理的问题。DaemonSet Controller，首先从 Etcd 里获取所有的 Node 列表，然后遍历所有的 Node。这时，它就可以很容易地去检查，当前这个 Node 上是不是有一个携带了 name=fluentd-elasticsearch 标签的 Pod 在运行。而检查的结果，可能有这么三种情况： 没有这种 Pod，那么就意味着要在这个 Node 上创建这样一个 Pod；有这种 Pod，但是数量大于 1，那就说明要把多余的 Pod 从这个 Node 上删除掉；正好只有一个这种 Pod，那说明这个节点是正常的。 其中，删除节点（Node）上多余的 Pod 非常简单，直接调用 Kubernetes API 就可以了。但是，如何在指定的 Node 上创建新 Pod 呢？如果你已经熟悉了 Pod API 对象的话，那一定可以立刻说出答案：用 nodeSelector，选择 Node ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:25:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我首先简单介绍了 StatefulSet 的“滚动更新”，然后重点讲解了本专栏的第三个重要编排对象：DaemonSet。相比于 Deployment，DaemonSet 只管理 Pod 对象，然后通过 nodeAffinity 和 Toleration 这两个调度器的小功能，保证了每个节点上有且只有一个 Pod。这个控制器的实现原理简单易懂，希望你能够快速掌握。与此同时，DaemonSet 使用 ControllerRevision，来保存和管理自己对应的“版本”。这种“面向 API 对象”的设计思路，大大简化了控制器本身的逻辑，也正是 Kubernetes 项目“声明式 API”的优势所在。而且，相信聪明的你此时已经想到了，StatefulSet 也是直接控制 Pod 对象的，那么它是不是也在使用 ControllerRevision 进行版本管理呢？没错。在 Kubernetes 项目里，ControllerRevision 其实是一个通用的版本管理对象。这样，Kubernetes 项目就巧妙地避免了每种控制器都要维护一套冗余的代码和逻辑的问题。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:25:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"22 | 撬动离线业务：Job与CronJob 在前面的几篇文章中，我和你详细分享了 Deployment、StatefulSet，以及 DaemonSet 这三个编排概念。你有没有发现它们的共同之处呢？实际上，它们主要编排的对象，都是“在线业务”，即：Long Running Task（长作业）。比如，我在前面举例时常用的 Nginx、Tomcat，以及 MySQL 等等。这些应用一旦运行起来，除非出错或者停止，它的容器进程会一直保持在 Running 状态。但是，有一类作业显然不满足这样的条件，这就是“离线业务”，或者叫作 Batch Job（计算业务）。这种业务在计算完成后就直接退出了，而此时如果你依然用 Deployment 来管理这种业务的话，就会发现 Pod 会在计算结束后退出，然后被 Deployment Controller 不断地重启；而像“滚动更新”这样的编排功能，更无从谈起了。所以，早在 Borg 项目中，Google 就已经对作业进行了分类处理，提出了 LRS（Long Running Service）和 Batch Jobs 两种作业形态，对它们进行“分别管理”和“混合调度”。不过，在 2015 年 Borg 论文刚刚发布的时候，Kubernetes 项目并不支持对 Batch Job 的管理。直到 v1.4 版本之后，社区才逐步设计出了一个用来描述离线业务的 API 对象，它的名字就是：Job。 Job API 对象的定义非常简单，我来举个例子，如下所示： apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=10000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 此时，相信你对 Kubernetes 的 API 对象已经不再陌生了。在这个 Job 的 YAML 文件里，你肯定一眼就会看到一位“老熟人”：Pod 模板，即 spec.template 字段。在这个 Pod 模板中，我定义了一个 Ubuntu 镜像的容器（准确地说，是一个安装了 bc 命令的 Ubuntu 镜像），它运行的程序是： echo \"scale=10000; 4*a(1)\" | bc -l 其中，bc 命令是 Linux 里的“计算器”；-l 表示，我现在要使用标准数学库；而 a(1)，则是调用数学库中的 arctangent 函数，计算 atan(1)。这是什么意思呢？中学知识告诉我们：tan(π/4) = 1。所以，4*atan(1)正好就是π，也就是 3.1415926…。备注：如果你不熟悉这个知识也不必担心，我也是在查阅资料后才知道的。所以，这其实就是一个计算π值的容器。而通过 scale=10000，我指定了输出的小数点后的位数是 10000。在我的计算机上，这个计算大概用时 1 分 54 秒。但是，跟其他控制器不同的是，Job 对象并不要求你定义一个 spec.selector 来描述要控制哪些 Pod。具体原因，我马上会讲解到。现在，我们就可以创建这个 Job 了： $ kubectl create -f job.yaml 在成功创建后，我们来查看一下这个 Job 对象，如下所示： $ kubectl describe jobs/pi Name: pi Namespace: default Selector: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Annotations: \u003cnone\u003e Parallelism: 1 Completions: 1 .. Pods Statuses: 0 Running / 1 Succeeded / 0 Failed Pod Template: Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Containers: ... Volumes: \u003cnone\u003e Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {job-controller } Normal SuccessfulCreate Created pod: pi-rq5rl 可以看到，这个 Job 对象在创建后，它的 Pod 模板，被自动加上了一个 controller-uid=\u003c 一个随机字符串 \u003e 这样的 Label。而这个 Job 对象本身，则被自动加上了这个 Label 对应的 Selector，从而 保证了 Job 与它所管理的 Pod 之间的匹配关系。而 Job Controller 之所以要使用这种携带了 UID 的 Label，就是为了避免不同 Job 对象所管理的 Pod 发生重合。需要注意的是，这种自动生成的 Label 对用户来说并不友好，所以不太适合推广到 Deployment 等长作业编排对象上。接下来，我们可以看到这个 Job 创建的 Pod 进入了 Running 状态，这意味着它正在计算 Pi 的值。 $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-rq5rl 1/1 Running 0 10s 而几分钟后计算结束，这个 Pod 就会进入 Completed 状态： $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-rq5rl 0/1 Completed 0 4m 这也是我们需要在 Pod 模板中定义 restartPolicy=Never 的原因：离线计算的 Pod 永远都不应该被重启，否则它们会再重新计算一遍。事实上，restartPolicy 在 Job 对象里只允许被设置为 Never 和 OnFailure；而在 Deployment 对象里，restartPolicy 则只允许被设置为 Always。此时，我们通过 kubectl logs 查看一下这个 Pod 的日志，就可以看到计算得到的 Pi 值已经被打印了出来： $ kubectl logs pi-rq5rl 3.141592653589793238462643383279... 这时候，你一定会想到这样一个问题，如果这个离线作业失败了要怎么办？比如，我们在这个例子中定义了 restartPolicy=Never，那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod，如下所示： $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-55h89 0/1 ContainerCreating 0 2s pi-tqbcz 0/1 Error 0 5s 可以看到，这时候会不断地有新 Pod 被创建出来。当然，这个尝试肯定不能无限进行下去。所以，我们就在 Job 对象的 spec.backoffLimit 字段里定义了重试次数为 4（即，backoffLimit=4），而这个字段的默认值是 6。需要注意的是，Job Controller 重新创建 Pod 的间隔是呈指数增加的，即下一次重新创建 Pod 的动作会分别发生在 10 s、20 s、40 s …后。而如果你定义的 restartPolicy=OnFailure，那么离线作业失败后，Job Controller 就不会去尝试创建新的 Pod。但是，它会不断地尝试重启 Pod 里的容器。这也正好对应了 restartPolicy 的含义（你也可以借此机会再回顾一下第 15 篇文章《深入解析 Pod 对象（二）：使用进阶》中的相关内容）。 如前所述，当一个 Job 的 Pod 运行结束后，它会进入 Completed 状态。但是，如果这个 Pod 因为某种原因一直不肯结束呢？在 Job 的 API 对象里，有一个 spec.activeDeadlineSeconds 字段可以设置最长运行时间，比如： spec: backoffLimit: 5 activeDeadlineSeconds: 100 一旦运行超过了 100 s，这个 Job 的所有 Pod 都会被终止。并且，你可以在 Pod 的状态里看到终止的原因是 reason: DeadlineExceeded。以上，就是一个 Job API 对象最主要的概念和用法了。不过，离线业务之所以被称为 Batch Job，当然是因为它们可以以“Batch”，也就是并行的方式去运行。接下来，我就来为你讲解一下Job Controller 对并行作业的控制方法。在 Job 对象中，负责并行控制的参数有两个： spec.parallelism，它定义的是一个 Job 在任意时间最多可以启动多少个 Pod 同时运行；","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:26:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我主要和你分享了 Job 这个离线业务的编排方法，讲解了 completions 和 parallelism 字段的含义，以及 Job Controller 的执行原理。紧接着，我通过实例和你分享了 Job 对象三种常见的使用方法。但是，根据我在社区和生产环境中的经验，大多数情况下用户还是更倾向于自己控制 Job 对象。所以，相比于这些固定的“模式”，掌握 Job 的 API 对象，和它各个字段的准确含义会更加重要。最后，我还介绍了一种 Job 的控制器，叫作：CronJob。这也印证了我在前面的分享中所说的：用一个对象控制另一个对象，是 Kubernetes 编排的精髓所在。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:26:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"23 | 声明式API与Kubernetes编程范式 在前面的几篇文章中，我和你分享了很多 Kubernetes 的 API 对象。这些 API 对象，有的是用来描述应用，有的则是为应用提供各种各样的服务。但是，无一例外地，为了使用这些 API 对象提供的能力，你都需要编写一个对应的 YAML 文件交给 Kubernetes。这个 YAML 文件，正是 Kubernetes 声明式 API 所必须具备的一个要素。不过，是不是只要用 YAML 文件代替了命令行操作，就是声明式 API 了呢？举个例子。我们知道，Docker Swarm 的编排操作都是基于命令行的，比如： $ docker service create --name nginx --replicas 2 nginx $ docker service update --image nginx:1.7.9 nginx 像这样的两条命令，就是用 Docker Swarm 启动了两个 Nginx 容器实例。其中，第一条 create 命令创建了这两个容器，而第二条 update 命令则把它们“滚动更新”成了一个新的镜像。对于这种使用方式，我们称为命令式命令行操作。 那么，像上面这样的创建和更新两个 Nginx 容器的操作，在 Kubernetes 里又该怎么做呢？这个流程，相信你已经非常熟悉了：我们需要在本地编写一个 Deployment 的 YAML 文件： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 然后，我们还需要使用 kubectl create 命令在 Kubernetes 里创建这个 Deployment 对象： $ kubectl create -f nginx.yaml 这样，两个 Nginx 的 Pod 就会运行起来了。而如果要更新这两个 Pod 使用的 Nginx 镜像，该怎么办呢？我们前面曾经使用过 kubectl set image 和 kubectl edit 命令，来直接修改 Kubernetes 里的 API 对象。不过，相信很多人都有这样的想法，我能不能通过修改本地 YAML 文件来完成这个操作呢？这样我的改动就会体现在这个本地 YAML 文件里了。当然可以。比如，我们可以修改这个 YAML 文件里的 Pod 模板部分，把 Nginx 容器的镜像改成 1.7.9，如下所示： ... spec: containers: - name: nginx image: nginx:1.7.9 而接下来，我们就可以执行一句 kubectl replace 操作，来完成这个 Deployment 的更新： $ kubectl replace -f nginx.yaml 可是，上面这种基于 YAML 文件的操作方式，是“声明式 API”吗？并不是。对于上面这种先 kubectl create，再 replace 的操作，我们称为命令式配置文件操作。也就是说，它的处理方式，其实跟前面 Docker Swarm 的两句命令，没什么本质上的区别。只不过，它是把 Docker 命令行里的参数，写在了配置文件里而已。 那么，到底什么才是“声明式 API”呢？答案是，kubectl apply 命令。在前面的文章中，我曾经提到过这个 kubectl apply 命令，并推荐你使用它来代替 kubectl create 命令（你也可以借此机会再回顾一下第 12 篇文章《牛刀小试：我的第一个容器化应用》中的相关内容）。现在，我就使用 kubectl apply 命令来创建这个 Deployment： $ kubectl apply -f nginx.yaml 这样，Nginx 的 Deployment 就被创建了出来，这看起来跟 kubectl create 的效果一样。然后，我再修改一下 nginx.yaml 里定义的镜像： ... spec: containers: - name: nginx image: nginx:1.7.9 这时候，关键来了。在修改完这个 YAML 文件之后，我不再使用 kubectl replace 命令进行更新，而是继续执行一条 kubectl apply 命令，即： $ kubectl apply -f nginx.yaml 这时，Kubernetes 就会立即触发这个 Deployment 的“滚动更新”。可是，它跟 kubectl replace 命令有什么本质区别吗？实际上，你可以简单地理解为，kubectl replace 的执行过程，是使用新的 YAML 文件中的 API 对象，替换原有的 API 对象；而 kubectl apply，则是执行了一个对原有 API 对象的 PATCH 操作。类似地，kubectl set image 和 kubectl edit 也是对已有 API 对象的修改。 更进一步地，这意味着 kube-apiserver 在响应命令式请求（比如，kubectl replace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能。而对于声明式请求（比如，kubectl apply），一次能处理多个写操作，并且具备 Merge 能力。这种区别，可能乍一听起来没那么重要。而且，正是由于要照顾到这样的 API 设计，做同样一件事情，Kubernetes 需要的步骤往往要比其他项目多不少。但是，如果你仔细思考一下 Kubernetes 项目的工作流程，就不难体会到这种声明式 API 的独到之处。接下来，我就以 Istio 项目为例，来为你讲解一下声明式 API 在实际使用时的重要意义。 在 2017 年 5 月，Google、IBM 和 Lyft 公司，共同宣布了 Istio 开源项目的诞生。很快，这个项目就在技术圈儿里，掀起了一阵名叫“微服务”的热潮，把 Service Mesh 这个新的编排概念推到了风口浪尖。而 Istio 项目，实际上就是一个基于 Kubernetes 项目的微服务治理框架。它的架构非常清晰，如下所示： 在上面这个架构图中，我们不难看到 Istio 项目架构的核心所在。Istio 最根本的组件，是运行在每一个应用 Pod 里的 Envoy 容器。这个 Envoy 项目是 Lyft 公司推出的一个高性能 C++ 网络代理，也是 Lyft 公司对 Istio 项目的唯一贡献。而 Istio 项目，则把这个代理服务以 sidecar 容器的方式，运行在了每一个被治理的应用 Pod 中。我们知道，Pod 里的所有容器都共享同一个 Network Namespace。所以，Envoy 容器就能够通过配置 Pod 里的 iptables 规则，把整个 Pod 的进出流量接管下来。这时候，Istio 的控制层（Control Plane）里的 Pilot 组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy 代理进行配置，从而实现微服务治理。 我们一起来看一个例子。假设这个 Istio 架构图左边的 Pod 是已经在运行的应用，而右边的 Pod 则是我们刚刚上线的应用的新版本。这时候，Pilot 通过调节这两 Pod 里的 Envoy 容器的配置，从而将 90% 的流量分配给旧版本的应用，将 10% 的流量分配给新版本应用，并且，还可以在后续的过程中随时调整。这样，一个典型的“灰度发布”的场景就完成了。比如，Istio 可以调节这个流量从 90%-10%，改到 80%-20%，再到 50%-50%，最后到 0%-100%，就完成了这个灰度发布的过程。更重要的是，在整个微服务治理的过程中，无论是对 Envoy 容器的部署，还是像上面这样对 Envoy 代理的配置，用户和应用都是完全“无感”的。这时候，你可能会有所疑惑：Istio 项目明明需要在每个 Pod 里安装一个 Envoy 容器，又怎么能做到“无感”的呢？ 实际上，Istio 项目使用的，是 Kubernetes 中的一个非常重要的功能，叫作 Dynamic Admission Control。在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后，总有一些“初始化”性质的工作需要在它们被 Kubernetes 项目正式处理之前进行。比如，自动为所有 Pod 加上某些标签（Labels）。而这个“初始化”操作的实现，借助的是一个叫作 Admission 的功能。它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码，可以选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。但这就意味着，如果你现在想要添加一些自己的规则到 Admission Controller，就会比较困难。因为，这要求重新编译并重启 APIServer。显然，这种使用方法对 Istio 来说，影响太大了。所以，Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admissio","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:27:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我为你重点讲解了 Kubernetes 声明式 API 的含义。并且，通过对 Istio 项目的剖析，我为你说明了它使用 Kubernetes 的 Initializer 特性，完成 Envoy 容器“自动注入”的原理。事实上，从“使用 Kubernetes 部署代码”，到“使用 Kubernetes 编写代码”的蜕变过程，正是你从一个 Kubernetes 用户，到 Kubernetes 玩家的晋级之路。而，如何理解“Kubernetes 编程范式”，如何为 Kubernetes 添加自定义 API 对象，编写自定义控制器，正是这个晋级过程中的关键点，也是我要在后面几篇文章中分享的核心内容。此外，基于今天这篇文章所讲述的 Istio 的工作原理，尽管 Istio 项目一直宣称它可以运行在非 Kubernetes 环境中，但我并不建议你花太多时间去做这个尝试。毕竟，无论是从技术实现还是在社区运作上，Istio 与 Kubernetes 项目之间都是紧密的、唇齿相依的关系。如果脱离了 Kubernetes 项目这个基础，那么这条原本就不算平坦的“微服务”之路，恐怕会更加困难重重。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:27:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"24 | 深入解析声明式API（一）：API对象的奥秘 在上一篇文章中，我为你详细讲解了 Kubernetes 声明式 API 的设计、特点，以及使用方式。而在今天这篇文章中，我就来为你讲解一下 Kubernetes 声明式 API 的工作原理，以及如何利用这套 API 机制，在 Kubernetes 里添加自定义的 API 对象。你可能一直就很好奇：当我把一个 YAML 文件提交给 Kubernetes 之后，它究竟是如何创建出一个 API 对象的呢？这得从声明式 API 的设计谈起了。在 Kubernetes 项目中，一个 API 对象在 Etcd 里的完整资源路径，是由：Group（API 组）、Version（API 版本）和 Resource（API 资源类型）三个部分组成的。通过这样的结构，整个 Kubernetes 里的所有 API 对象，实际上就可以用如下的树形结构表示出来： 在这幅图中，你可以很清楚地看到 Kubernetes 里 API 对象的组织方式，其实是层层递进的。比如，现在我要声明要创建一个 CronJob 对象，那么我的 YAML 文件的开始部分会这么写： apiVersion: batch/v2alpha1 kind: CronJob ... 在这个 YAML 文件中，“CronJob”就是这个 API 对象的资源类型（Resource），“batch”就是它的组（Group），v2alpha1 就是它的版本（Version）。当我们提交了这个 YAML 文件之后，Kubernetes 就会把这个 YAML 文件里描述的内容，转换成 Kubernetes 里的一个 CronJob 对象。那么，Kubernetes 是如何对 Resource、Group 和 Version 进行解析，从而在 Kubernetes 项目里找到 CronJob 对象的定义呢？ 首先，Kubernetes 会匹配 API 对象的组。需要明确的是，对于 Kubernetes 里的核心 API 对象，比如：Pod、Node 等，是不需要 Group 的（即：它们的 Group 是“”）。所以，对于这些 API 对象来说，Kubernetes 会直接在 /api 这个层级进行下一步的匹配过程。而对于 CronJob 等非核心 API 对象来说，Kubernetes 就必须在 /apis 这个层级里查找它对应的 Group，进而根据“batch”这个 Group 的名字，找到 /apis/batch。不难发现，这些 API Group 的分类是以对象功能为依据的，比如 Job 和 CronJob 就都属于“batch” （离线业务）这个 Group。 然后，Kubernetes 会进一步匹配到 API 对象的版本号。对于 CronJob 这个 API 对象来说，Kubernetes 在 batch 这个 Group 下，匹配到的版本号就是 v2alpha1。在 Kubernetes 中，同一种 API 对象可以有多个版本，这正是 Kubernetes 进行 API 版本化管理的重要手段。这样，比如在 CronJob 的开发过程中，对于会影响到用户的变更就可以通过升级新版本来处理，从而保证了向后兼容。 最后，Kubernetes 会匹配 API 对象的资源类型。在前面匹配到正确的版本之后，Kubernetes 就知道，我要创建的原来是一个 /apis/batch/v2alpha1 下的 CronJob 对象。这时候，APIServer 就可以继续创建这个 CronJob 对象了。为了方便理解，我为你总结了一个如下所示流程图来阐述这个创建过程： 首先，当我们发起了创建 CronJob 的 POST 请求之后，我们编写的 YAML 的信息就被提交给了 APIServer。而 APIServer 的第一个功能，就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等。然后，请求会进入 MUX 和 Routes 流程。如果你编写过 Web Server 的话就会知道，MUX 和 Routes 是 APIServer 完成 URL 和 Handler 绑定的场所。而 APIServer 的 Handler 要做的事情，就是按照我刚刚介绍的匹配过程，找到对应的 CronJob 类型定义。接着，APIServer 最重要的职责就来了：根据这个 CronJob 类型定义，使用用户提交的 YAML 文件里的字段，创建一个 CronJob 对象。而在这个过程中，APIServer 会进行一个 Convert 工作，即：把用户提交的 YAML 文件，转换成一个叫作 Super Version 的对象，它正是该 API 资源类型所有版本的字段全集。这样用户提交的不同版本的 YAML 文件，就都可以用这个 Super Version 对象来进行处理了。接下来，APIServer 会先后进行 Admission() 和 Validation() 操作。比如，我在上一篇文章中提到的 Admission Controller 和 Initializer，就都属于 Admission 的内容。而 Validation，则负责验证这个对象里的各个字段是否合法。这个被验证过的 API 对象，都保存在了 APIServer 里一个叫作 Registry 的数据结构中。也就是说，只要一个 API 对象的定义能在 Registry 里查到，它就是一个有效的 Kubernetes API 对象。最后，APIServer 会把验证过的 API 对象转换成用户最初提交的版本，进行序列化操作，并调用 Etcd 的 API 把它保存起来。 由此可见，声明式 API 对于 Kubernetes 来说非常重要。所以，APIServer 这样一个在其他项目里“平淡无奇”的组件，却成了 Kubernetes 项目的重中之重。它不仅是 Google Borg 设计思想的集中体现，也是 Kubernetes 项目里唯一一个被 Google 公司和 RedHat 公司双重控制、其他势力根本无法参与其中的组件。此外，由于同时要兼顾性能、API 完备性、版本化、向后兼容等很多工程化指标，所以 Kubernetes 团队在 APIServer 项目里大量使用了 Go 语言的代码生成功能，来自动化诸如 Convert、DeepCopy 等与 API 资源相关的操作。这部分自动生成的代码，曾一度占到 Kubernetes 项目总代码的 20%~30%。这也是为何，在过去很长一段时间里，在这样一个极其“复杂”的 APIServer 中，添加一个 Kubernetes 风格的 API 资源类型，是一个非常困难的工作。不过，在 Kubernetes v1.7 之后，这个工作就变得轻松得多了。这，当然得益于一个全新的 API 插件机制：CRD。CRD 的全称是 Custom Resource Definition。顾名思义，它指的就是，允许用户在 Kubernetes 中添加一个跟 Pod、Node 类似的、新的 API 资源类型，即：自定义 API 资源。举个例子，我现在要为 Kubernetes 添加一个名叫 Network 的 API 资源类型。它的作用是，一旦用户创建一个 Network 对象，那么 Kubernetes 就应该使用这个对象定义的网络参数，调用真实的网络插件，比如 Neutron 项目，为用户创建一个真正的“网络”。这样，将来用户创建的 Pod，就可以声明使用这个“网络”了。这个 Network 对象的 YAML 文件，名叫 example-network.yaml，它的内容如下所示： apiVersion: samplecrd.k8s.io/v1 kind: Network metadata: name: example-network spec: cidr: \"192.168.0.0/16\" gateway: \"192.168.0.1\" 可以看到，我想要描述“网络”的 API 资源类型是 Network；API 组是samplecrd.k8s.io；API 版本是 v1。那么，Kubernetes 又该如何知道这个 API（samplecrd.k8s.io/v1/network）的存在呢？其实，上面的这个 YAML 文件，就是一个具体的“自定义 API 资源”实例，也叫 CR（Custom Resource）。而为了能够让 Kubernetes 认识这个 CR，你就需要让 Kubernetes 明白这个 CR 的宏观定义是什么，也就是 CRD（Custom Resource Definition）。这就好比，你想让计算机认识各种兔子的照片，就得先让计算机明白，兔子的普遍定义是什么。比如，兔子“是哺乳动物”“有长耳朵，三瓣嘴”。所以，接下来，我就先编写一个 CRD 的 YAML 文件，它的名字叫作 network.yaml，内容如下所示： apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: networks.samplecrd.k8s.io spec: group: samplecrd.k8s.io version: v1 names: kind: Network plural: networks scope: Namespaced 可以看到，在这个 CRD 中，我指定了“g","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:28:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我为你详细解析了 Kubernetes 声明式 API 的工作原理，讲解了如何遵循声明式 API 的设计，为 Kubernetes 添加一个名叫 Network 的 API 资源类型。从而达到了通过标准的 kubectl create 和 get 操作，来管理自定义 API 对象的目的。不过，创建出这样一个自定义 API 对象，我们只是完成了 Kubernetes 声明式 API 的一半工作。接下来的另一半工作是：为这个 API 对象编写一个自定义控制器（Custom Controller）。这样， Kubernetes 才能根据 Network API 对象的“增、删、改”操作，在真实环境中做出相应的响应。比如，“创建、删除、修改”真正的 Neutron 网络。而这，正是 Network 这个 API 对象所关注的“业务逻辑”。这个业务逻辑的实现过程，以及它所使用的 Kubernetes API 编程库的工作原理，就是我要在下一篇文章中讲解的主要内容。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:28:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"25 | 深入解析声明式API（二）：编写自定义控制器 在上一篇文章中，我和你详细分享了 Kubernetes 中声明式 API 的实现原理，并且通过一个添加 Network 对象的实例，为你讲述了在 Kubernetes 里添加 API 资源的过程。在今天的这篇文章中，我就继续和你一起完成剩下一半的工作，即：为 Network 这个自定义 API 对象编写一个自定义控制器（Custom Controller）。正如我在上一篇文章结尾处提到的，“声明式 API”并不像“命令式 API”那样有着明显的执行逻辑。这就使得基于声明式 API 的业务功能实现，往往需要通过控制器模式来“监视”API 对象的变化（比如，创建或者删除 Network），然后以此来决定实际要执行的具体工作。接下来，我就和你一起通过编写代码来实现这个过程。这个项目和上一篇文章里的代码是同一个项目，你可以从这个 GitHub 库里找到它们。我在代码里还加上了丰富的注释，你可以随时参考。总得来说，编写自定义控制器代码的过程包括：编写 main 函数、编写自定义控制器的定义，以及编写控制器里的业务逻辑三个部分。 首先，我们来编写这个自定义控制器的 main 函数。main 函数的主要工作就是，定义并初始化一个自定义控制器（Custom Controller），然后启动它。这部分代码的主要内容如下所示： func main() { ... cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) ... kubeClient, err := kubernetes.NewForConfig(cfg) ... networkClient, err := clientset.NewForConfig(cfg) ... networkInformerFactory := informers.NewSharedInformerFactory(networkClient, ...) controller := NewController(kubeClient, networkClient, networkInformerFactory.Samplecrd().V1().Networks()) go networkInformerFactory.Start(stopCh) if err = controller.Run(2, stopCh); err != nil { glog.Fatalf(\"Error running controller: %s\", err.Error()) } } 可以看到，这个 main 函数主要通过三步完成了初始化并启动一个自定义控制器的工作。第一步：main 函数根据我提供的 Master 配置（APIServer 的地址端口和 kubeconfig 的路径），创建一个 Kubernetes 的 client（kubeClient）和 Network 对象的 client（networkClient）。但是，如果我没有提供 Master 配置呢？这时，main 函数会直接使用一种名叫 InClusterConfig 的方式来创建这个 client。这个方式，会假设你的自定义控制器是以 Pod 的方式运行在 Kubernetes 集群里的。而我在第 15 篇文章《深入解析 Pod 对象（二）：使用进阶》中曾经提到过，Kubernetes 里所有的 Pod 都会以 Volume 的方式自动挂载 Kubernetes 的默认 ServiceAccount。所以，这个控制器就会直接使用默认 ServiceAccount 数据卷里的授权信息，来访问 APIServer。第二步：main 函数为 Network 对象创建一个叫作 InformerFactory（即：networkInformerFactory）的工厂，并使用它生成一个 Network 对象的 Informer，传递给控制器。第三步：main 函数启动上述的 Informer，然后执行 controller.Run，启动自定义控制器。至此，main 函数就结束了。 看到这，你可能会感到非常困惑：编写自定义控制器的过程难道就这么简单吗？这个 Informer 又是个什么东西呢？别着急。接下来，我就为你详细解释一下这个自定义控制器的工作原理。在 Kubernetes 项目中，一个自定义控制器的工作原理，可以用下面这样一幅流程图来表示（在后面的叙述中，我会用“示意图”来指代它）： 我们先从这幅示意图的最左边看起。这个控制器要做的第一件事，是从 Kubernetes 的 APIServer 里获取它所关心的对象，也就是我定义的 Network 对象。这个操作，依靠的是一个叫作 Informer（可以翻译为：通知器）的代码库完成的。Informer 与 API 对象是一一对应的，所以我传递给自定义控制器的，正是一个 Network 对象的 Informer（Network Informer）。不知你是否已经注意到，我在创建这个 Informer 工厂的时候，需要给它传递一个 networkClient。事实上，Network Informer 正是使用这个 networkClient，跟 APIServer 建立了连接。不过，真正负责维护这个连接的，则是 Informer 所使用的 Reflector 包。更具体地说，Reflector 使用的是一种叫作 ListAndWatch 的方法，来“获取”并“监听”这些 Network 对象实例的变化。在 ListAndWatch 机制下，一旦 APIServer 端有新的 Network 实例被创建、删除或者更新，Reflector 都会收到“事件通知”。这时，该事件及它对应的 API 对象这个组合，就被称为增量（Delta），它会被放进一个 Delta FIFO Queue（即：增量先进先出队列）中。而另一方面，Informe 会不断地从这个 Delta FIFO Queue 里读取（Pop）增量。每拿到一个增量，Informer 就会判断这个增量里的事件类型，然后创建或者更新本地对象的缓存。这个缓存，在 Kubernetes 里一般被叫作 Store。比如，如果事件类型是 Added（添加对象），那么 Informer 就会通过一个叫作 Indexer 的库把这个增量里的 API 对象保存在本地缓存中，并为它创建索引。相反，如果增量的事件类型是 Deleted（删除对象），那么 Informer 就会从本地缓存中删除这个对象。 这个同步本地缓存的工作，是 Informer 的第一个职责，也是它最重要的职责。而 Informer 的第二个职责，则是根据这些事件的类型，触发事先注册好的 ResourceEventHandler。这些 Handler，需要在创建控制器的时候注册给它对应的 Informer。接下来，我们就来编写这个控制器的定义，它的主要内容如下所示： func NewController( kubeclientset kubernetes.Interface, networkclientset clientset.Interface, networkInformer informers.NetworkInformer) *Controller { ... controller := \u0026Controller{ kubeclientset: kubeclientset, networkclientset: networkclientset, networksLister: networkInformer.Lister(), networksSynced: networkInformer.Informer().HasSynced, workqueue: workqueue.NewNamedRateLimitingQueue(..., \"Networks\"), ... } networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.enqueueNetwork, UpdateFunc: func(old, new interface{}) { oldNetwork := old.(*samplecrdv1.Network) newNetwork := new.(*samplecrdv1.Network) if oldNetwork.ResourceVersion == newNetwork.ResourceVersion { return } controller.enqueueNetwork(new) }, DeleteFunc: controller.enqueueNetworkForDelete, return controller } 我前面在 main 函数里创建了两个 client（kubeclientset 和 networkclientset），然后在这段代码里，使用这两个 client 和前面创建的 Informer，初始化了自定义控制器。值得注意的是，在这个自定义控制器里，我还设置了一个工作队列（work queue），它正是处于示意图中间位置的 WorkQueue。这个工作队列的作用是，负责同步 Informer 和控制循环之间的数据。 实际上，Kuber","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:29:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我为你剖析了 Kubernetes API 编程范式的具体原理，并编写了一个自定义控制器。这其中，有如下几个概念和机制，是你一定要理解清楚的：所谓的 Informer，就是一个自带缓存和索引机制，可以触发 Handler 的客户端库。这个本地缓存在 Kubernetes 中一般被称为 Store，索引一般被称为 Index。Informer 使用了 Reflector 包，它是一个可以通过 ListAndWatch 机制获取并监视 API 对象变化的客户端封装。Reflector 和 Informer 之间，用到了一个“增量先进先出队列”进行协同。而 Informer 与你要编写的控制循环之间，则使用了一个工作队列来进行协同。在实际应用中，除了控制循环之外的所有代码，实际上都是 Kubernetes 为你自动生成的，即：pkg/client/{informers, listers, clientset}里的内容。而这些自动生成的代码，就为我们提供了一个可靠而高效地获取 API 对象“期望状态”的编程库。所以，接下来，作为开发者，你就只需要关注如何拿到“实际状态”，然后如何拿它去跟“期望状态”做对比，从而决定接下来要做的业务逻辑即可。以上内容，就是 Kubernetes API 编程范式的核心思想。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:29:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"26 | 基于角色的权限控制：RBAC 在前面的文章中，我已经为你讲解了很多种 Kubernetes 内置的编排对象，以及对应的控制器模式的实现原理。此外，我还剖析了自定义 API 资源类型和控制器的编写方式。这时候，你可能已经冒出了这样一个想法：控制器模式看起来好像也不难嘛，我能不能自己写一个编排对象呢？答案当然是可以的。而且，这才是 Kubernetes 项目最具吸引力的地方。毕竟，在互联网级别的大规模集群里，Kubernetes 内置的编排对象，很难做到完全满足所有需求。所以，很多实际的容器化工作，都会要求你设计一个自己的编排对象，实现自己的控制器模式。而在 Kubernetes 项目里，我们可以基于插件机制来完成这些工作，而完全不需要修改任何一行代码。不过，你要通过一个外部插件，在 Kubernetes 里新增和操作 API 对象，那么就必须先了解一个非常重要的知识：RBAC。我们知道，Kubernetes 中所有的 API 对象，都保存在 Etcd 里。可是，对这些 API 对象的操作，却一定都是通过访问 kube-apiserver 实现的。其中一个非常重要的原因，就是你需要 APIServer 来帮助你做授权工作。 而在 Kubernetes 项目中，负责完成授权（Authorization）工作的机制，就是 RBAC：基于角色的访问控制（Role-Based Access Control）。如果你直接查看 Kubernetes 项目中关于 RBAC 的文档的话，可能会感觉非常复杂。但实际上，等到你用到这些 RBAC 的细节时，再去查阅也不迟。而在这里，我只希望你能明确三个最基本的概念。 Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限。Subject：被作用者，既可以是“人”，也可以是“机器”，也可以是你在 Kubernetes 里定义的“用户”。RoleBinding：定义了“被作用者”和“角色”的绑定关系。 而这三个概念，其实就是整个 RBAC 体系的核心所在。我先来讲解一下 Role。实际上，Role 本身就是一个 Kubernetes 的 API 对象，定义如下所示： kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: mynamespace name: example-role rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 首先，这个 Role 对象指定了它能产生作用的 Namepace 是：mynamespace。Namespace 是 Kubernetes 项目里的一个逻辑管理单位。不同 Namespace 的 API 对象，在通过 kubectl 命令进行操作的时候，是互相隔离开的。比如，kubectl get pods -n mynamespace。当然，这仅限于逻辑上的“隔离”，Namespace 并不会提供任何实际的隔离或者多租户能力。而在前面文章中用到的大多数例子里，我都没有指定 Namespace，那就是使用的是默认 Namespace：default。 然后，这个 Role 对象的 rules 字段，就是它所定义的权限规则。在上面的例子里，这条规则的含义就是：允许“被作用者”，对 mynamespace 下面的 Pod 对象，进行 GET、WATCH 和 LIST 操作。那么，这个具体的“被作用者”又是如何指定的呢？这就需要通过 RoleBinding 来实现了。当然，RoleBinding 本身也是一个 Kubernetes 的 API 对象。它的定义如下所示： kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io 可以看到，这个 RoleBinding 对象里定义了一个 subjects 字段，即“被作用者”。它的类型是 User，即 Kubernetes 里的用户。这个用户的名字是 example-user。可是，在 Kubernetes 中，其实并没有一个叫作“User”的 API 对象。而且，我们在前面和部署使用 Kubernetes 的流程里，既不需要 User，也没有创建过 User。 这个 User 到底是从哪里来的呢？实际上，Kubernetes 里的“User”，也就是“用户”，只是一个授权系统里的逻辑概念。它需要通过外部认证服务，比如 Keystone，来提供。或者，你也可以直接给 APIServer 指定一个用户名、密码文件。那么 Kubernetes 的授权系统，就能够从这个文件里找到对应的“用户”了。当然，在大多数私有的使用环境中，我们只要使用 Kubernetes 提供的内置“用户”，就足够了。这部分知识，我后面马上会讲到。接下来，我们会看到一个 roleRef 字段。正是通过这个字段，RoleBinding 对象就可以直接通过名字，来引用我们前面定义的 Role 对象（example-role），从而定义了“被作用者（Subject）”和“角色（Role）”之间的绑定关系。需要再次提醒的是，Role 和 RoleBinding 对象都是 Namespaced 对象（Namespaced Object），它们对权限的限制规则仅在它们自己的 Namespace 内有效，roleRef 也只能引用当前 Namespace 里的 Role 对象。那么，对于非 Namespaced（Non-namespaced）对象（比如：Node），或者，某一个 Role 想要作用于所有的 Namespace 的时候，我们又该如何去做授权呢？ 这时候，我们就必须要使用 ClusterRole 和 ClusterRoleBinding 这两个组合了。这两个 API 对象的用法跟 Role 和 RoleBinding 完全一样。只不过，它们的定义里，没有了 Namespace 字段，如下所示： kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrole rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrolebinding subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: example-clusterrole apiGroup: rbac.authorization.k8s.io 上面的例子里的 ClusterRole 和 ClusterRoleBinding 的组合，意味着名叫 example-user 的用户，拥有对所有 Namespace 里的 Pod 进行 GET、WATCH 和 LIST 操作的权限。更进一步地，在 Role 或者 ClusterRole 里面，如果要赋予用户 example-user 所有权限，那你就可以给它指定一个 verbs 字段的全集，如下所示： verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] 这些就是当前 Kubernetes（v1.11）里能够对 API 对象进行的所有操作了。类似地，Role 对象的 rules 字段也可以进一步细化。比如，你可以只针对某一个具体的对象进行权限设置，如下所示： rules: - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"my-config\"] verbs: [\"get\"] 这个例子就表示，这条规则的“被作用者”，只对名叫“my-config”的 ConfigMap 对象，有进行 GET 操作的权限。而正如我前面介绍过的，在大多数时候，我们其实都不太使用“用户”这个功能，而是直接使用 Kubernetes 里的“内置用户”。这个由 Kubernetes 负责管理的“内置用户”，正是我们前面曾经提到过的：ServiceAccount。接下来，我通过一个具体的实例来为你讲解一下为 ServiceAccount 分配权限的过程。 首先，我们要定义一个 ServiceAccount。它的 API 对象非常简单，如下所示： apiVersion: v1 kind:","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:30:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我主要为你讲解了基于角色的访问控制（RBAC）。其实，你现在已经能够理解，所谓角色（Role），其实就是一组权限规则列表。而我们分配这些权限的方式，就是通过创建 RoleBinding 对象，将被作用者（subject）和权限列表进行绑定。另外，与之对应的 ClusterRole 和 ClusterRoleBinding，则是 Kubernetes 集群级别的 Role 和 RoleBinding，它们的作用范围不受 Namespace 限制。而尽管权限的被作用者可以有很多种（比如，User、Group 等），但在我们平常的使用中，最普遍的用法还是 ServiceAccount。所以，Role + RoleBinding + ServiceAccount 的权限分配方式是你要重点掌握的内容。我们在后面编写和安装各种插件的时候，会经常用到这个组合。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:30:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"27 | 聪明的微创新：Operator工作原理解读 在前面的几篇文章中，我已经和你分享了 Kubernetes 项目中的大部分编排对象（比如 Deployment、StatefulSet、DaemonSet，以及 Job），也介绍了“有状态应用”的管理方法，还阐述了为 Kubernetes 添加自定义 API 对象和编写自定义控制器的原理和流程。可能你已经感觉到，在 Kubernetes 中，管理“有状态应用”是一个比较复杂的过程，尤其是编写 Pod 模板的时候，总有一种“在 YAML 文件里编程序”的感觉，让人很不舒服。而在 Kubernetes 生态中，还有一个相对更加灵活和编程友好的管理“有状态应用”的解决方案，它就是：Operator。接下来，我就以 Etcd Operator 为例，来为你讲解一下 Operator 的工作原理和编写方法。Etcd Operator 的使用方法非常简单，只需要两步即可完成： 第一步，将这个 Operator 的代码 Clone 到本地： $ git clone https://github.com/coreos/etcd-operator 第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里。不过，在部署 Etcd Operator 的 Pod 之前，你需要先执行这样一个脚本： $ example/rbac/create_role.sh 不用我多说你也能够明白：这个脚本的作用，就是为 Etcd Operator 创建 RBAC 规则。这是因为，Etcd Operator 需要访问 Kubernetes 的 APIServer 来创建对象。更具体地说，上述脚本为 Etcd Operator 定义了如下所示的权限：对 Pod、Service、PVC、Deployment、Secret 等 API 对象，有所有权限；对 CRD 对象，有所有权限；对属于 etcd.database.coreos.com 这个 API Group 的 CR（Custom Resource）对象，有所有权限。 而 Etcd Operator 本身，其实就是一个 Deployment，它的 YAML 文件如下所示： apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.2 command: - etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name ... 所以，我们就可以使用上述的 YAML 文件来创建 Etcd Operator，如下所示： $ kubectl create -f example/deployment.yaml 而一旦 Etcd Operator 的 Pod 进入了 Running 状态，你就会发现，有一个 CRD 被自动创建了出来，如下所示： $ kubectl get pods NAME READY STATUS RESTARTS AGE etcd-operator-649dbdb5cb-bzfzp 1/1 Running 0 20s $ kubectl get crd NAME CREATED AT etcdclusters.etcd.database.coreos.com 2018-09-18T11:42:55Z 这个 CRD 名叫etcdclusters.etcd.database.coreos.com 。你可以通过 kubectl describe 命令看到它的细节，如下所示： $ kubectl describe crd etcdclusters.etcd.database.coreos.com ... Group: etcd.database.coreos.com Names: Kind: EtcdCluster List Kind: EtcdClusterList Plural: etcdclusters Short Names: etcd Singular: etcdcluster Scope: Namespaced Version: v1beta2 ... 可以看到，这个 CRD 相当于告诉了 Kubernetes：接下来，如果有 API 组（Group）是etcd.database.coreos.com、API 资源类型（Kind）是“EtcdCluster”的 YAML 文件被提交上来，你可一定要认识啊。所以说，通过上述两步操作，你实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型。而 Etcd Operator 本身，就是这个自定义资源类型对应的自定义控制器。而当 Etcd Operator 部署好之后，接下来在这个 Kubernetes 里创建一个 Etcd 集群的工作就非常简单了。你只需要编写一个 EtcdCluster 的 YAML 文件，然后把它提交给 Kubernetes 即可，如下所示： $ kubectl apply -f example/example-etcd-cluster.yaml 这个 example-etcd-cluster.yaml 文件里描述的，是一个 3 个节点的 Etcd 集群。我们可以看到它被提交给 Kubernetes 之后，就会有三个 Etcd 的 Pod 运行起来，如下所示： $ kubectl get pods NAME READY STATUS RESTARTS AGE example-etcd-cluster-dp8nqtjznc 1/1 Running 0 1m example-etcd-cluster-mbzlg6sd56 1/1 Running 0 2m example-etcd-cluster-v6v6s6stxd 1/1 Running 0 2m 那么，究竟发生了什么，让创建一个 Etcd 集群的工作如此简单呢？我们当然还是得从这个 example-etcd-cluster.yaml 文件开始说起。不难想到，这个文件里定义的，正是 EtcdCluster 这个 CRD 的一个具体实例，也就是一个 Custom Resource（CR）。而它的内容非常简单，如下所示： apiVersion: \"etcd.database.coreos.com/v1beta2\" kind: \"EtcdCluster\" metadata: name: \"example-etcd-cluster\" spec: size: 3 version: \"3.2.13\" 可以看到，EtcdCluster 的 spec 字段非常简单。其中，size=3 指定了它所描述的 Etcd 集群的节点个数。而 version=“3.2.13”，则指定了 Etcd 的版本，仅此而已。而真正把这样一个 Etcd 集群创建出来的逻辑，就是 Etcd Operator 要实现的主要工作了。看到这里，相信你应该已经对 Operator 有了一个初步的认知： Operator 的工作原理，实际上是利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的“有状态应用”；然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。 所以，编写一个 Etcd Operator，与我们前面编写一个自定义控制器的过程，没什么不同。不过，考虑到你可能还不太清楚Etcd 集群的组建方式，我在这里先简单介绍一下这部分知识。Etcd Operator 部署 Etcd 集群，采用的是静态集群（Static）的方式。静态集群的好处是，它不必依赖于一个额外的服务发现机制来组建集群，非常适合本地容器化部署。而它的难点，则在于你必须在部署的时候，就规划好这个集群的拓扑结构，并且能够知道这些节点固定的 IP 地址。比如下面这个例子： $ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\ --listen-peer-urls http://10.0.1.10:2380 \\ ... --initial-cluster-token etcd-cluster-1 \\ --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\ --initial-cluster-state new $ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\ --listen-peer-urls http://10.0.1.11:2380 \\ ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:31:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天这篇文章中，我以 Etcd Operator 为例，详细介绍了一个 Operator 的工作原理和编写过程。可以看到，Etcd 集群本身就拥有良好的分布式设计和一定的高可用能力。在这种情况下，StatefulSet“为 Pod 编号”和“将 Pod 同 PV 绑定”这两个主要的特性，就不太有用武之地了。而相比之下，Etcd Operator 把一个 Etcd 集群，抽象成了一个具有一定“自治能力”的整体。而当这个“自治能力”本身不足以解决问题的时候，我们可以通过两个专门负责备份和恢复的 Operator 进行修正。这种实现方式，不仅更加贴近 Etcd 的设计思想，也更加编程友好。不过，如果我现在要部署的应用，既需要用 StatefulSet 的方式维持拓扑状态和存储状态，又有大量的编程工作要做，那我到底该如何选择呢？ 其实，Operator 和 StatefulSet 并不是竞争关系。你完全可以编写一个 Operator，然后在 Operator 的控制循环里创建和控制 StatefulSet 而不是 Pod。比如，业界知名的Prometheus 项目的 Operator，正是这么实现的。此外，CoreOS 公司在被 RedHat 公司收购之后，已经把 Operator 的编写过程封装成了一个叫作Operator SDK的工具（整个项目叫作 Operator Framework），它可以帮助你生成 Operator 的框架代码。感兴趣的话，你可以试用一下。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:31:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"k8s容器持久化存储 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:32:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"28 | PV、PVC、StorageClass，这些到底在说啥？ 在前面的文章中，我重点为你分析了 Kubernetes 的各种编排能力。在这些讲解中，你应该已经发现，容器化一个应用比较麻烦的地方，莫过于对其“状态”的管理。而最常见的“状态”，又莫过于存储状态了。所以，从今天这篇文章开始，我会通过 4 篇文章为你剖析 Kubernetes 项目处理容器持久化存储的核心原理，从而帮助你更好地理解和使用这部分内容。首先，我们来回忆一下我在第 19 篇文章《深入理解 StatefulSet（二）：存储状态》中，和你分享 StatefulSet 如何管理存储状态的时候，介绍过的Persistent Volume（PV）和 Persistent Volume Claim（PVC）这套持久化存储体系。其中，PV 描述的，是持久化存储数据卷。这个 API 对象主要定义的是一个持久化存储在宿主机上的目录，比如一个 NFS 的挂载目录。通常情况下，PV 对象是由运维人员事先创建在 Kubernetes 集群里待用的。比如，运维人员可以定义这样一个 NFS 类型的 PV，如下所示： apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: server: 10.244.1.4 path: \"/\" 而 PVC 描述的，则是 Pod 所希望使用的持久化存储的属性。比如，Volume 存储的大小、可读写权限等等。PVC 对象通常由开发人员创建；或者以 PVC 模板的方式成为 StatefulSet 的一部分，然后由 StatefulSet 控制器负责创建带编号的 PVC。比如，开发人员可以声明一个 1 GiB 大小的 PVC，如下所示： apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: manual resources: requests: storage: 1Gi 而用户创建的 PVC 要真正被容器使用起来，就必须先和某个符合条件的 PV 进行绑定。这里要检查的条件，包括两部分：第一个条件，当然是 PV 和 PVC 的 spec 字段。比如，PV 的存储（storage）大小，就必须满足 PVC 的要求。而第二个条件，则是 PV 和 PVC 的 storageClassName 字段必须一样。这个机制我会在本篇文章的最后一部分专门介绍。 在成功地将 PVC 和 PV 进行绑定之后，Pod 就能够像使用 hostPath 等常规类型的 Volume 一样，在自己的 YAML 文件里声明使用这个 PVC 了，如下所示： apiVersion: v1 kind: Pod metadata: labels: role: web-frontend spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: nfs mountPath: \"/usr/share/nginx/html\" volumes: - name: nfs persistentVolumeClaim: claimName: nfs 可以看到，Pod 需要做的，就是在 volumes 字段里声明自己要使用的 PVC 名字。接下来，等这个 Pod 创建之后，kubelet 就会把这个 PVC 所对应的 PV，也就是一个 NFS 类型的 Volume，挂载在这个 Pod 容器内的目录上。不难看出，PVC 和 PV 的设计，其实跟“面向对象”的思想完全一致。 PVC 可以理解为持久化存储的“接口”，它提供了对某种持久化存储的描述，但不提供具体的实现；而这个持久化存储的实现部分则由 PV 负责完成。这样做的好处是，作为应用开发者，我们只需要跟 PVC 这个“接口”打交道，而不必关心具体的实现是 NFS 还是 Ceph。毕竟这些存储相关的知识太专业了，应该交给专业的人去做。 而在上面的讲述中，其实还有一个比较棘手的情况。比如，你在创建 Pod 的时候，系统里并没有合适的 PV 跟它定义的 PVC 绑定，也就是说此时容器想要使用的 Volume 不存在。这时候，Pod 的启动就会报错。但是，过了一会儿，运维人员也发现了这个情况，所以他赶紧创建了一个对应的 PV。这时候，我们当然希望 Kubernetes 能够再次完成 PVC 和 PV 的绑定操作，从而启动 Pod。所以在 Kubernetes 中，实际上存在着一个专门处理持久化存储的控制器，叫作 Volume Controller。这个 Volume Controller 维护着多个控制循环，其中有一个循环，扮演的就是撮合 PV 和 PVC 的“红娘”的角色。它的名字叫作 PersistentVolumeController。 PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态，从而结束“单身”之旅。而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。所以，接下来 Kubernetes 只要获取到这个 PVC 对象，就一定能够找到它所绑定的 PV。 那么，这个 PV 对象，又是如何变成容器里的一个持久化存储的呢？我在前面讲解容器基础的时候，已经为你详细剖析了容器 Volume 的挂载机制。用一句话总结，所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起。（你可以借此机会，再回顾一下专栏的第 8 篇文章《白话容器基础（四）：重新认识 Docker 容器》中的相关内容）而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性”。即：这个目录里面的内容，既不会因为容器的删除而被清理掉，也不会跟当前的宿主机绑定。这样，当容器被重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个 Volume，访问到这些内容。 显然，我们前面使用的 hostPath 和 emptyDir 类型的 Volume 并不具备这个特征：它们既有可能被 kubelet 清理掉，也不能被“迁移”到其他节点上。所以，大多数情况下，持久化 Volume 的实现，往往依赖于一个远程存储服务，比如：远程文件存储（比如，NFS、GlusterFS）、远程块存储（比如，公有云提供的远程磁盘）等等。而 Kubernetes 需要做的工作，就是使用这些存储服务，来为容器准备一个持久化的宿主机目录，以供将来进行绑定挂载时使用。而所谓“持久化”，指的是容器在这个目录里写入的文件，都会保存在远程存储中，从而使得这个目录具备了“持久性”。这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理”。 接下来，我通过一个具体的例子为你说明。当一个 Pod 调度到一个节点上之后，kubelet 就要负责为这个 Pod 创建它的 Volume 目录。默认情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径： /var/lib/kubelet/pods/\u003cPod的ID\u003e/volumes/kubernetes.io~\u003cVolume类型\u003e/\u003cVolume名字\u003e 接下来，kubelet 要做的操作就取决于你的 Volume 类型了。如果你的 Volume 类型是远程块存储，比如 Google Cloud 的 Persistent Disk（GCE 提供的远程磁盘服务），那么 kubelet 就需要先调用 Goolge Cloud 的 API，将它所提供的 Persistent Disk 挂载到 Pod 所在的宿主机上。备注：你如果不太了解块存储的话，可以直接把它理解为：一块磁盘。 这相当于执行： $ gcloud compute instances attach-disk \u003c虚拟机名字\u003e --disk \u003c远程磁盘名字\u003e 这一步为虚拟机挂载远程磁盘的操作，对应的正是“两阶段处理”的第一阶段。在 Kubernetes 中，我们把这个阶段称为 Attach。Attach 阶段完成后，为了能够使用这个远程磁盘，kubelet 还要进行第二个操作，即：格式化这个磁盘设备，然后将它挂载到宿主机指定的挂载点上。不难理解，这个挂载点，正是我在前面反复提到的 Volume 的宿主机目录。所以，这一步相当于执行： 这个将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。Mount 阶段完成后，这个 Volume 的宿主机目录就是一个“持久化”的目录了，容器在它里面写入的内容，会保存在 Google Cloud 的远程磁盘中。而如果你的 Volume 类型是远程文件存储（比如 NFS）的话","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:33:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在今天的分享中，我为你详细解释了 PVC 和 PV 的设计与实现原理，并为你阐述了 StorageClass 到底是干什么用的。这些概念之间的关系，可以用如下所示的一幅示意图描述： ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:33:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"29 | PV、PVC体系是不是多此一举？从本地持久化卷谈起 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:34:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"30 | 编写自己的存储插件：FlexVolume与CSI ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:35:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"31 | 容器存储实践：CSI插件编写指南 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:36:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"k8s容器网络 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:37:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"32 | 浅谈容器网络 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:38:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"33 | 深入解析容器跨主机网络 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:39:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"34 | Kubernetes网络模型与CNI网络插件 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:40:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"35 | 解读Kubernetes三层网络方案 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:41:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"36 | 为什么说Kubernetes只有soft multi-tenancy？ ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:42:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"37 | 找到容器不容易：Service、DNS与服务发现 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:43:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"38 | 从外界连通Service与Service调试“三板斧” ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:44:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"39 | 谈谈Service与Ingress ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:45:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"k8s作业调度和资源管理 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:46:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"40 | Kubernetes的资源模型与资源管理 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:47:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"41 | 十字路口上的Kubernetes默认调度器 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:48:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"42 | Kubernetes默认调度器调度策略解析 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:49:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"43 | Kubernetes默认调度器的优先级与抢占机制 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:50:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"44 | Kubernetes GPU管理与Device Plugin机制 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:51:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"k8s容器运行时 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:52:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"45 | 幕后英雄：SIG-Node与CRI ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:53:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"46 | 解读 CRI 与 容器运行时 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:54:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"47 | 绝不仅仅是安全：Kata Containers 与 gVisor ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:55:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"k8s容器监控与日志 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:56:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"48 | Prometheus、Metrics Server与Kubernetes监控体系 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:57:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"49 | Custom Metrics: 让Auto Scaling不再“食之无味” ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:58:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"50 | 让日志无处可逃：容器日志收集与管理 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:59:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"再谈k8s开源与社区 在前面的文章中，我已经为你详细讲解了容器与 Kubernetes 项目的所有核心技术点。在今天这最后一篇文章里，我就跟你谈一谈 Kubernetes 开源社区以及 CNCF 相关的一些话题。我们知道 Kubernetes 这个项目是托管在 CNCF 基金会下面的。但是，我在专栏最前面讲解容器与 Kubernetes 的发展历史的时候就已经提到过，CNCF 跟 Kubernetes 的关系，并不是传统意义上的基金会与托管项目的关系，CNCF 实际上扮演的，是 Kubernetes 项目的 Marketing 的角色。这就好比，本来 Kubernetes 项目应该是由 Google 公司一家维护、运营和推广的。但是为了表示中立，并且吸引更多的贡献者加入，Kubernetes 项目从一开始就选择了由基金会托管的模式。而这里的关键在于，这个基金会本身，就是 Kubernetes 背后的“大佬们”一手创建出来的，然后以中立的方式，对 Kubernetes 项目进行运营和 Marketing。通过这种方式，Kubernetes 项目既避免了因为 Google 公司在开源社区里的“不良作风”和非中立角色被竞争对手口诛笔伐，又可以站在开源基金会的制高点上团结社区里所有跟容器相关的力量。而随后 CNCF 基金会的迅速发展和壮大，也印证了这个思路其实是非常正确和有先见之明的。不过，在 Kubernetes 和 Prometheus 这两个 CNCF 的一号和二号项目相继毕业之后，现在 CNCF 社区的更多职能，就是扮演一个传统的开源基金会的角色，吸纳会员，帮助项目孵化和运转。只不过，由于 Kubernetes 项目的巨大成功，CNCF 在云计算领域已经取得了极高的声誉和认可度，也填补了以往 Linux 基金会在这一领域的空白。所以说，你可以认为现在的 CNCF，就是云计算领域里的 Apache ，而它的作用跟当年大数据领域里 Apache 基金会的作用是一样的。 不过，需要指出的是，对于开源项目和开源社区的运作来说，第三方基金会从来就不是一个必要条件。事实上，这个世界上绝大多数成功的开源项目和社区，都来自于一个聪明的想法或者一帮杰出的黑客。在这些项目的发展过程中，一个独立的、第三方基金会的作用，更多是在该项目发展到一定程度后主动进行商业运作的一部分。开源项目与基金会间的这一层关系，希望你不要本末倒置了。另外，需要指出的是，CNCF 基金会仅仅负责成员项目的 Marketing， 而绝不会、也没有能力直接影响具体项目的发展历程。无论是任何一家成员公司或者是 CNCF 的 TOC（Technical Oversight Committee，技术监督委员会），都没有对 Kubernetes 项目“指手画脚”的权利，除非这位 TOC 本人就是 Kubernetes 项目里的关键人物。所以说，真正能够影响 Kubernetes 项目发展的，当然还是 Kubernetes 社区本身。可能你会好奇，Kubernetes 社区本身的运作方式，又是怎样的呢？ 通常情况下，一个基金会下面托管的项目，都需要遵循基金会本身的管理机制，比如统一的 CI 系统、Code Review 流程、管理方式等等。但是，在我们这个社区的实际情况，是先有的 Kubernetes，然后才有的 CNCF，并且 CNCF 基金会还是 Kubernetes “一手带大”的。所以，在项目治理这个事情上，Kubernetes 项目早就自成体系，并且发展得非常完善了。而基金会里的其他项目一般各自为阵，CNCF 不会对项目本身的治理方法提出过多的要求。而说到 Kubernetes 项目的治理方式，其实还是比较贴近 Google 风格的，即：重视代码，重视社区的民主性。 首先，Kubernetes 项目是一个没有“Maintainer”的项目。这一点非常有意思，Kubernetes 项目里曾经短时间内存在过 Maintainer 这个角色，但是很快就被废弃了。取而代之的，则是 approver+reviewer 机制。这里具体的原理，是在 Kubernetes 的每一个目录下，你都可以添加一个 OWNERS 文件，然后在文件里写入这样的字段： approvers: - caesarxuchao reviewers: - lavalamp labels: - sig/api-machinery - area/apiserver 比如，上面这个例子里，approver 的 GitHub ID 就是 caesarxuchao （Xu Chao），reviewer 就是 lavalamp。这就意味着，任何人提交的 Pull Request（PR，代码修改请求），只要修改了这个目录下的文件，那么就必须要经过 lavalamp 的 Code Review，然后再经过 caesarxuchao 的 Approve 才可以被合并。当然，在这个文件里，caesarxuchao 的权力是最大的，它可以既做 Code Review，也做最后的 Approve。但， lavalamp 是不能进行 Approve 的。当然，无论是 Code Review 通过，还是 Approve，这些维护者只需要在 PR 下面 Comment /lgtm 和 /approve，Kubernetes 项目的机器人（k8s-ci-robot）就会自动给该 PR 加上 lgtm 和 approve 标签，然后进入 Kubernetes 项目 CI 系统的合并队列，最后被合并。此外，如果你要对这个项目加标签，或者把它 Assign 给其他人，也都可以通过 Comment 的方式来进行。可以看到，在上述整个过程中，代码维护者不需要对 Kubernetes 项目拥有写权限，就可以完成代码审核、合并等所有流程。这当然得益于 Kubernetes 社区完善的机器人机制，这也是 GitHub 最吸引人的特性之一。 顺便说一句，很多人问我，GitHub 比 GitLab 或者其他代码托管平台强在哪里？实际上， GitHub 庞大的 API 和插件生态，才是这个产品最具吸引力的地方。当然，当你想要将你的想法以代码的形式提交给 Kubernetes 项目时，除非你的改动是 bugfix 或者很简单的改动，否则，你直接提交一个 PR 上去，是大概率不会被 Approve 的。这里的流程，一定要按照我下面的讲解来进行： 在 Kubernetes 主库里创建 Issue，详细地描述你希望解决的问题、方案，以及开发计划。而如果社区里已经有相关的 Issue 存在，那你就必须要在这里把它们引用过来。而如果社区里已经存在相同的 Issue 了，你就需要确认一下，是不是应该直接转到原有 issue 上进行讨论。给 Issue 加上与它相关的 SIG 的标签。比如，你可以直接 Comment /sig node，那么这个 Issue 就会被加上 sig-node 的标签，这样 SIG-Node 的成员就会特别留意这个 Issue。收集社区对这个 Issue 的信息，回复 Comment，与 SIG 成员达成一致。必要的时候，你还需要参加 SIG 的周会，更好地阐述你的想法和计划。在与 SIG 的大多数成员达成一致后，你就可以开始进行详细的设计了。如果设计比较复杂的话，你还需要在 Kubernetes 的设计提议目录（在 Kubernetes Community 库里）下提交一个 PR，把你的设计文档加进去。这时候，所有关心这个设计的社区成员，都会来对你的设计进行讨论。不过最后，在整个 Kubernetes 社区只有很少一部分成员才有权限来 Review 和 Approve 你的设计文档。他们当然也被定义在了这个目录下面的 OWNERS 文件里，如下所示： reviewers: - brendandburns - dchen1107 - jbeda - lavalamp - smarterclayton - thockin - wojtek-t - bgrant0607 approvers: - brendandburns - dchen1107 - jbeda - lavalamp - smarterclayton - thockin - wojtek-t - bgrant0607 labels: - kind/design 这几位成员，就可以称为社区里的“大佬”了。不过我在这里要提醒你的是，“大佬”并不一定代表水平高，所以你还是要擦亮眼睛。此外，Kubernetes 项目的几位创始成员，被称作 Elders（元老），分别是 jbeda、bgrant0607、brendandburns、dchen1107 和 thockin。你可以查看一下这个列表与上述“大佬”名单有什么不同。 上述 Design Proposal 被合并后，你就可以开始按照设计文档的内容编写代码了。这个流程，才是正常大家所熟知的编写代码、提交 PR、通过 CI 测试、进行 Code Review，然后等待合并的流程。 如果你的 feature 是需要要在 Kubernetes 的正式 Release 里发布上线的，那么你还需要在Kubernetes Enhancements这个库里面提交一个 KEP（即 Kubernetes Enhancement Proposal）。这个 KEP 的主要内容，是详细地描述你的编码计划、测试计划、发布计划，以及向后兼容计划等软件工程相关的信息，供全社区进行监督和指导。 以上内容，","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:60:0","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"总结 在本篇文章里，我为你详细讲述了 CNCF 和 Kubernetes 社区的关系，以及 Kubernetes 社区的运作方式，希望能够帮助你更好地理解这个社区的特点和它的先进之处。除此之外，你可能还听说过 Kubernetes 社区里有一个叫作 Kubernetes Steering Committee 的组织。这个组织，其实也是属于Kubernetes Community 库的一部分。这个组织成员的主要职能，是对 Kubernetes 项目治理的流程进行约束和规范，但通常并不会直接干涉 Kubernetes 具体的设计和代码实现。其实，到目前为止，Kubernetes 社区最大的一个优点，就是把“搞政治”的人和“搞技术”的人分得比较清楚。相信你也不难理解，这两种角色在一个活跃的开源社区里其实都是需要的，但是，如果这两部分人发生了大量的重合，那对于一个开源社区来说，恐怕就是个灾难了。 ","date":"2022-07-14 10:38:32","objectID":"/k8s_advanced/:60:1","tags":["k8s"],"title":"K8s_advanced","uri":"/k8s_advanced/"},{"categories":["k8s"],"content":"k8s入门实战 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:0:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"开篇词｜迎难而上，做云原生时代的弄潮儿 你好，我是罗剑锋，不过更愿意你称呼我“Chrono”。先来简单介绍一下我自己吧。作为一个有着近二十年工作经验的“技术老兵”，我一直奋斗在开发第一线，从 Windows 到 Linux、从硬件到软件，从单机到集群、云，开发了各种形式的应用，也经历了许多大小不一的公司，现在是在 API 管理和微服务平台公司 Kong，基于 Nginx/OpenResty 研发 Kong Gateway、Kong ingress Controller 等产品。其实我应该算是极客时间的老朋友了，在 2019 年开了《透视 HTTP 协议》的课程，在 2020 年开了《C++ 实战笔记》的课程，然后因为工作上的事情比较多，“消失”了近两年的时间。不过这段日子里我倒没有“两耳不闻窗外事”，而是一直在关注业界的新技术新动向，所以今天，我再次回到了极客时间的这个大讲堂，想和你聊聊如今风头正劲的 Kubernetes。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:1:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"现在的 Kubernetes 你一定听说过 Kubernetes 吧，也许更熟悉一点的，是许多人总挂在嘴边的缩写——“K8s”。自从 2013 年 Docker 诞生以来，容器一跃成为了 IT 界最热门的话题。而 Kubernetes 则趁着容器的“东风”，借助 Google 和 CNCF 的强力“背书”，击败了 Docker Swarm 和 Apache Mesos，成为了“容器编排”领域的王者至尊。 换一个更通俗易懂的说法，那就是：现在 Kubernetes 已经没有了实际意义上的竞争对手，它的地位就如同 Linux 一样，成为了事实上的云原生操作系统，是构建现代应用的基石。毕竟，现代应用是什么？是微服务，是服务网格，这些统统要围绕着容器来开发、部署和运行，而使用容器就必然要用到容器编排技术，在现在只有唯一的选项，那就是 Kubernetes。不管你是研发、测试、还是运维，不管你是前台、后台、还是中台，不管你用的是 C++、Java 还是 Python，不管你是搞数据库、区块链、还是人工智能，不管你是从事网站、电商、还是音视频，在这个“云原生”时代，Kubernetes 都是一个绕不过去的产品，是我们工作中迟早要面对的“坎儿”。 你也许会有疑惑：我现在的工作和“云”毫不沾边，而且 Kubernetes 都“火”了这么久，现在才开始学，会不会有点晚了？值不值呢？这里我就要引用一句老话了：“艺多不压身”，还有另一句：“机遇总是偏爱有准备的人”。“云原生”已经是现在 IT 界的普遍共识，是未来的大势所趋。也许这个“浪潮”暂时还没有打到你这里来，但一旦它真正来临，只有你提前做好了知识储备，才能够迎难而进，站上浪头成为“弄潮儿”，否则就可能会被“拍在沙滩上”。 我和你说一下我自己的亲身经历吧。早在 Docker 和 Kubernetes 发布之初，我就对它们有过关注。不过因为我的主要工作语言是 C/C++，而 Docker 和 Kubernetes 用的都是 Go，当时 Go 的性能还比较差（比如垃圾回收机制导致的著名 Stop the World），所以我只是简单了解了，没有去特别研究。过了几年，一个偶然的机会，我们要在客户的环境里部署自研应用，但依赖库差异太大，很难搞定。这个时候我又想起了 Docker，经过一个多星期的折腾，艰难地啃下了一大堆资料之后，总算是把系统正常上线了。虽然任务完成了，但也让我意识到自己从前对 Docker 的轻视是非常错误的，于是就痛下决心，开始从头、系统地学习整理容器知识，之后也就很自然地搭上了 Kubernetes 这条“大船”。再后来，我想换新工作，面试的时候 Boss 出了道“偏门”题，讲 Kubernetes 的容器和环境安全。虽然我不熟悉这个方向，但凭借着之前的积累，只用了一个晚上就赶出了 20 多页的 PPT，第二天面对几位评委侃侃而谈，最终顺利拿下了 Offer。 你看，如果我当时一味固执己见，只呆在自己的“舒适区”里，不主动去学习容器技术和 Kubernetes，当机遇不期而至的时候，很可能就会因为手足无措而错失了升职加薪的良机。所以也希望你不要犯我当初的错误，我们应当看清楚时代的走向，尽可能超前于时代，越早掌握 Kubernetes，将来自己成功的几率就越大。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:1:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"学习 Kubernetes 有哪些难点 那么，我们应该怎么来学习 Kubernetes 呢？其实今天学习 Kubernetes 的难度，比起前几年来说，已经是极大地下降了，网上资料非常多，有博客、专题、视频等各种形式，而且 Kubernetes 为了推广自身，在官网上还放出了非常详细的教程和参考手册，只要你肯花时间，完全可以“自学成才”。不过，“理想很丰满，现实很骨感”。理论上讲，学习 Kubernetes 只要看资料就足够了，但实际情况却是学习起来仍然困难重重，我们会遇到很多意想不到的问题。 这是因为 Kubernetes 是一个分布式、集群化、云时代的系统，有许多新概念和新思维方式，和我们以往经验、认知的差异很大。我觉得，Kubernetes 技术栈的特点可以用四个字来概括，那就是“新、广、杂、深”。 “新”是指 Kubernetes 用到的基本上都是比较前沿、陌生的技术，而且版本升级很快，经常变来变去。“广”是指 Kubernetes 涉及的应用领域很多、覆盖面非常广，不太好找到合适的切入点或者突破口。“杂”是指 Kubernetes 的各种实现比较杂乱，谁都可以上来“掺和”一下，让人看的眼晕。“深”是指 Kubernetes 面对的每个具体问题和方向，都需要有很深的技术背景和底蕴，想要吃透很不容易。 这四个特点就导致 Kubernetes 的“门槛”相当高，学习曲线非常陡峭，学习成本非常昂贵，有可能花费了大量的时间和精力却南辕北辙、收效甚微，这点我确实是深有体会。比如在初学的过程中我就遇到过这些疑问，不知道你有没有同感： Docker、Containerd、K8s、K3s、MicroK8s、Minikube……这么多项目，该如何选择？容器的概念太抽象了，怎么才能够快速准确地理解？镜像的命名稀奇古怪，里面的“bionic”“buster”等都是什么意思？不知道怎么搭建出 Kubernetes 环境，空有理论知识，无法联系实际。YAML 文件又长又乱，到哪里能找到说明，能否遵循什么简单规律写出来？Pod、Deployment、StatefulSet……这么多的对象，有没有什么内在的脉络和联系？ 遗憾的是，这些问题很难在现有的 Kubernetes 资料里找到答案。我个人感觉，它们往往“站得太高”，没有为“零基础”的初学者考虑，总会预设一些前提，比如熟悉 Linux 系统、知道编程语言、了解网络技术等等，有时候还会因为版本过时而失效，或者是忽略一些关键的细节。这就让我们初学者经常“卡”在一些看似无关紧要却又非常现实的难点上，这样的点越积越多，最后就让人逐渐丧失了学习 Kubernetes 的信心和勇气。所以，我就想以自己的学习经历为基础，融合个人感悟、经验教训和心得技巧，整理出一个初学者面对 Kubernetes 这门新技术的入门路线和系统思路，让你在学习时有捷径可走，不再有迷茫和困惑，能快速高效地迈入 Kubernetes 的宏伟殿堂。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:1:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"在这个专栏里你会怎么学习 Kubernetes 讲到这里，你一定很想知道，这个专栏有什么特点，和别的课程有哪些不一样，结合刚才讲的 Kubernetes 技术栈四个特点“技术新、领域广、实现杂、方向深”，我来仔细说一说我的想法和考量。 第一，没有太多前提，不会 Go 你也可以学。在这门课里，我不会要求你学习 Go 语言，也不会去讲 Kubernetes 的源码。虽然是研发出身，但我并没有特别深入地了解 Go 语言，但是，我认为这反而是一个优势。因为面对 Kubernetes 的时候我和你是“平等”的，不会“下意识”地去从源码层次来讲解它的运行原理，更能够设身处地为你着想。 讲源码虽然会很透彻，但它的前置条件实在是太高了，不是所有的人都具备这个基础的。为了学习 Kubernetes 要先了解 Go 语言，有点“本末倒置”，如同钱钟书老先生所说：“如果你吃了个鸡蛋，觉得味道不错，何必去认识那个下蛋的母鸡呢?”（我觉得这方面也可以对比一下 Linux 操作系统，它是用 C 语言写的，但几乎没有人要求我们在学习 Linux 之前需要事先掌握 C 语言。）不过如果你真想做 Kubernetes 开发，等学会了 Kubernetes 的基本概念和用法，再回头去学 Go 语言也完全来得及。 第二，这个专栏我会定位在“入门”，也就是说，不会去讲那些高深的大道理和复杂的工作流程，语言也尽量朴素平实，少用专业术语和缩略词。毕竟 Kubernetes 系统涉及的领域太过庞大，对于初次接触的人来说直接“抠”内部细节不太合适，那样很容易会“跑偏”“钻牛角尖”。我觉得学习 Kubernetes 最好的方式是尽快建立一个全局观和大局观，等到你对这个陌生领域的全貌有了粗略但完整的认识之后，再挑选一个自己感兴趣的方向去研究，才是性价比最高的做法。 而且前面也说过，Kubernetes 版本更新很快，有的功能点或许一段时间之后就成了废弃的特性（比如 ComponentStatus 在 1.19 被废弃、PodSecurityPolicy 在 1.21 被废弃），如果讲得太细，万一今后它过时无用，就实在是太尴尬了。 第三，课程会以实战为导向，强调眼手脑结合，鼓励你多动手、多实际操作，我认为这是这个课程最大的特点。Kubernetes 一般每年都会发布一个大版本，大版本又会有很多的小版本，每个版本都会持续改进功能特性，但一味求新，不符合当前的实际情况，毕竟生产环境里稳定是最重要的。所以，我就选择了今年（2022 年）初发布的 Kubernetes 1.23.3，它是最后一个支持 Docker 的大版本，承上启下，具有很多的新特性，同时也保留了足够的历史兼容性，非常适合用来学习 Kubernetes。 在课程里，我会先从 Docker 开始，陆续用 minikube、kubeadm 搭建单机和多机的 Kubernetes 集群环境，在讲解概念的同时，还会给出大量的 docker、kubectl 操作命令，让你能够看完课程后立即上手演练，用实际操作来强化学习效果。 第四，具体到每一节课上，我不会“贪大求全”，而是会“短小精悍”，做减法而不是加法，力争每节课只聚焦在一个知识点。 这是因为 Kubernetes 涉及的领域太广了，它的知识结构是网状的，之间的联系很密切，在学习时稍不注意就会跳跃到其他的地方，很容易“发散”“跑题”，导致思维不集中。所以我在讲解的时候会尽量克制，把每节课收束在一个相对独立的范围之内，不会有太多的外延话题，也不会机械地罗列 API、命令参数、属性字段（这些你都可以查阅 Kubernetes 文档），在讲解复杂的知识点时还会配上图片，让你能够精准地理解吸收知识。比如 Pod 等众多 API 对象之间的关系一直是学习 Kubernetes 的难点，单用文字很难解释清楚，所以我画了很多图，帮助你形象地理解它们的联系。就像这张： 因为每一讲都聚焦在一个知识点上，专栏的整个结构，我也梳理出了一条独特路线：把 Kubernetes 的知识点由网状结构简化成了线性结构，你可以在这条路线上循序渐进，由浅入深、由易到难地学习完整的 Kubernetes 知识体系。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:1:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"专栏的线性结构是什么样的 从这些设想出发，我把专栏主要划分成了五个模块。 课前准备 在正式学习前首先有一节课前准备，这也是我写专栏的惯例了，会跟你说一下我们学习的实验环境，用虚拟机软件搭建出一个 Linux 系统，为零基础的同学扫除一些非常简单但是其他地方可能没有讲到的后顾之忧。 入门篇 我会用最流行的 Docker 来讲解 Kubernetes 的基础技术：容器，让你了解它的基本原理，熟悉常用的 Docker 命令，能够轻松地拉取、构建镜像，运行容器，能够使用容器在本机搭建开发测试环境。初级篇在具备了容器的知识之后，我们就可以来学习 Kubernetes 了，用的是单机环境 minikube。你会了解为什么容器会发展到容器编排，Kubernetes 解决了什么问题，它的基本架构是什么样子的，再学习 YAML 语言、核心对象 Pod，还有离线业务对象 Job/CronJob、配置信息对象 ConfigMap/Secret。 中级篇 我们会在“初级篇”的基础上更进一步，使用 kubeadm 搭建出一个多节点的集群，模拟真实的生产环境，并介绍 Kubernetes 里的 4 个重要概念：Deployment、DaemonSet、Service、Ingress。学习了这些对象，你就能够明白 Kubernetes 的优点、特点是什么，知道为什么它能够一统天下，成为云原生时代的操作系统。 高级篇 经过前面几个模块的学习，你就已经对 Kubernetes 有了比较全面的认识了，所以我会再讲解一些深层次知识点和高级应用技巧，包括持久化存储、有状态的对象、应用的滚动更新和自动伸缩、容器和节点的管理等等。 当然，这种纯线性学习也难免会有缺点，我也会用其他的形式来补充完善，让你的学习过程更丰富多样，比如每一讲后面的知识小贴士、互动答疑。在专栏的中后期，我还会为你准备一些“加餐”，讲讲 Kubernetes 相关的一些“花边逸闻”，比如 docker-compose、CNCF、API Gateway 等等，扩展一些虽然是外围但也很有实际意义的知识。前面说过要多动手实践，为了强化实战效果，每个模块的知识点学完后，我都会安排一节实战演练课和一节视频课： 实战课，我们会应用模块中学习的知识，来实战搭建 WordPress 网站，你可以跟着课程一路走下来，看着它如何从单机应用演变到集群里的高可用系统的。视频课，我会把这个模块里大部分重要的知识点都实机操作演示给你看，相信通过“文字 + 图片 + 音频 + 视频”的多种形式，你的学习一定会非常充实而满足。你的 Kubernetes 之旅马上就要开始了，我再送你一张课程的知识地图，希望你能够在今后的三个月里以它为伴，用努力与坚持去浇灌学习之花，收获丰硕的知识和喜悦之果！ ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:1:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"课前准备｜动手实践才是最好的学习方式 如果你看过我的另外两个极客时间专栏（《透视 HTTP 协议》和《C++ 实战笔记》）就会知道，我一直都很强调实验环境的重要程度，毕竟计算机这门学科的实践性要大于理论性，而且有一个能够上手操作的实际环境，对学习理论也非常有帮助。落到我们的这个 Kubernetes 学习课上，实验环境更是必不可少的，因为和网络协议、编程语言不同，Kubernetes 是一个更贴近于生产环境的庞大系统，如果“光说不练”，即使你掌握了再多的知识，但不能和实际相结合，也只能是“纸上谈兵”。俗话说：“工欲善其事，必先利其器”，所以在正式学习之前，我们必须要有一个基本的实验环境，要能够在环境中熟悉 Kubernetes 的操作命令、验证测试 Kubernetes 的各种特性，有这样的“帮手”作为辅助，我们的学习才能够事半功倍。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:2:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"选择什么样的实验环境 但想要得到一个完整的 Kubernetes 环境不那么容易，因为它太复杂了，对软硬件的要求都比较高，安装部署过程中还有许多的小细节，这些都会成为学习过程中的“拦路虎”。那么，应该怎么搭建出符合我们要求的实验环境呢？你也许会说：现在的云厂商到处都是，去网上申请一个就好了。这也许是一个比较便捷的获取途径，不过我有一些不同的意见。首先，这些网上的“云主机”很少是免费的，都需要花钱，而且想要好配置还要花更多的钱，对于我们的学习来说性价比不高。其次，“云主机”都是在“云”上，免不了会受网络和厂商的限制，存在不稳定因素。再次，这些“云主机”都是厂商已经为我们配好了的，很多软硬件都是固定的，不能随意定制，特别是很难真正“从零搭建”。 考虑上面的这三点，我建议还是在本地搭建实验环境最好，不会受制于人，完全自主可控。不过，Kubernetes 通常都运行在集群环境下，由多台服务器组成，难道我们还要自己置办几台电脑来组网吗？这倒大可不必。因为现在的虚拟机软件已经非常成熟可靠了，能够在一台电脑里虚拟出多台主机，这些虚拟主机用起来和真实的物理主机几乎没有什么差异，只要你的电脑配置不是太差，组成一个三四台虚拟服务器的小集群是毫无问题的，而且虚拟机的创建删除都非常简单，成本极低。使用虚拟机软件还有一点额外的好处，由于很多云服务商内部也在大量使用虚拟服务器，Kubernetes 里的容器技术也与虚拟机有很多相似之处，通过使用虚拟机，我们还能顺便对比这些技术的异同点，加深对 Kubernetes 的理解。 所以综合来看，我建议你挑选一台配置不算太差的笔记本或者台式机，在里面使用虚拟机来搭建我们这门课程的实验环境。作为宿主机电脑的 CPU 和硬盘的要求不高，4 核、300G 就可以了，关键是内存要足够大，因为虚拟机和 Kubernetes 都很能“吃”内存，最少要有 8G，这样起码能够支持开两个虚拟机组成最小的集群。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:2:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"选择什么样的虚拟机软件 确定了我们的实验环境大方向——虚拟机之后，我们就要选择虚拟机软件了。目前市面上的主流虚拟机软件屈指可数，所以选择起来并不算困难，我个人推荐的有两个：VirtualBox 和 VMWare Fusion。 我们先讲适用面广的 VirtualBox。VirtualBox 是 Oracle 推出的一款虚拟机软件，历史很悠久，一直坚持免费政策，使用条款上也没有什么限制，是一个难得的精品软件。VirtualBox 支持 Windows 和 macOS，但有一个小缺点，它只能运行在 Intel（x86_64）芯片上，不支持 Apple 新出的 M1（arm64/aarch64）芯片，这导致它无法在新款 Mac 上使用，不得不说是一大遗憾。所以，如果你手里是 Apple M1 Mac，就只能选择其他的虚拟机软件了。在 macOS 上，虚拟机最出名的应该是 Parallel Desktop 和 VMWare Fusion 这两个了，都需要付费。这里我比较推荐 VMWare Fusion。不过对于 VMWare Fusion 来说，它对 M1 的支持进展比较迟缓，所以在正式的付费版出来之前，公布了一个“技术预览版”，是完全免费的，而且功能、性能也比较好，虽然有使用时间的限制（大约 300 天），但对于我们的学习来说是足够了。这里我给出 VirtualBox（https://www.virtualbox.org/wiki/Downloads）和 VMWare Fusion（https://communities.vmware.com/t5/Fusion-for-Apple-Silicon-Tech/ct-p/3022）的网址，你可以课后去看一下，尽快下载。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:2:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"选择哪种 Linux 发行版 有了虚拟机软件之后，我们就要在上面安装操作系统，在这方面毫无疑问只能是 Linux，因为 Kubernetes 只能运行在 Linux 之上。不过麻烦的是，Linux 世界又分裂成很多不同的发行版，流行的有 CentOS/Fedora、 Ubuntu/Debian、SUSE 等等，没有一个占据绝对统治地位的系统。 那选哪个比较好呢？我们的主要目的是学习，所以易用性应该是首要关注点，另外系统还应该能够同时支持 x86_64 和 arm64。筛选下来我建议选择 Ubuntu 22.04 Jammy Jellyfish 桌面版（https://ubuntu.com/download/desktop），它有足够新的特性，非常适合运行 Kubernetes，而内置的浏览器、终端等工具也很方便我们的调试和测试。但对 Apple M1 用户来说，有一个不太好的消息，Ubuntu 22.04 在内核由 5.13 升级到 5.15 的时候引入了一个小 Bug，导致 VMWare Fusion 无法正常安装启动，这个问题直到 4 月份的正式版发布还没有解决。好在我当初为了测试，下载了一个较早的“daily build”版本，它可以在 VMWare Fusion 里正常安装，我把它上传到了云盘（https://www.aliyundrive.com/s/zzKcAQwQjR9），你可以下载后使用。需要注意一点，由于网站的限制，文件的后缀名被改成了 .mov ，你必须去掉这个后缀，还原成原始的 .iso 才能使用。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:2:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何配置虚拟机 准备好虚拟机软件和 Ubuntu 光盘镜像之后，我们就可以来安装虚拟机了。不过在安装之前，我们必须要把虚拟机适当地配置一下。因为 Kubernetes 不是一般的应用软件，而是一个复杂的系统软件，对硬件资源的要求有一点高，好在并不太高，2 核 CPU、2G 内存是最低要求，如果条件允许，我建议把内存增大到 4G，硬盘 40G 以上，这样运行起来会更流畅一些。另外，一些对于服务器来说不必要的设备也可以禁用或者删除，比如声卡、摄像头、软驱等等，可以节约一点系统资源。 由于 Linux 服务器大多数要以终端登录的方式使用，多台服务器还要联网，所以在网络方面我们还需要特别设置。前面说虚拟机软件首选 VirtualBox，Apple M1 Mac 备选 VMWare Fusion 技术预览版，这里我也分别说下两个软件的不同设置。对于 VirtualBox，首先，你需要在“工具 - 网络”里创建一个“Host-only”的网络，IP 地址段随意，比如这里就使用了它自动分配的“192.168.56.1/24”： 然后，在虚拟机的配置里，你需要启用两个网卡。“网卡 1”就设置成刚才创建的“Host-only”网络，它是我们在本地终端登录和联网时用的；而“网卡 2”是“网络地址转换（NAT）”，用来上外网： 对于 VMWare Fusion，你需要在“偏好设置 - 网络”里，添加一个自定义的网络，比如这里的“vmnet3”，网段是“192.168.10.0”，允许使用 NAT 连接外网，然后在虚拟机的网络设置里选用这个网络： ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:2:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何安装虚拟机 把 CPU、内存、硬盘、网络都配置好之后，再加载上 Ubuntu 22.04 的光盘镜像，我们就可以开始安装 Linux 了。在安装的过程中，为了节约时间，建议选择“最小安装”，同时物理断网，避免下载升级包。注意，断网对于 Apple M1 来说特别重要，否则 Ubuntu 会自动更新到 5.15 内核，导致安装后无法正常启动。安装完 Linux 系统之后，我们还要再做一些环境的初始化操作。首先我们需要用 Ctrl + Alt + T 打开命令行窗口，然后用 apt 从 Ubuntu 的官方软件仓库安装 git、vim、curl 等常用工具： sudo apt update sudo apt install -y git vim curl jq Ubuntu 桌面版默认是不支持远程登录的，所以为了让后续的实验更加便利，我们还需要安装“openssh-server”，再使用命令 ip addr ，查看虚拟机的 IP 地址，然后就可以在宿主机上使用 ssh 命令登录虚拟机： sudo apt install -y openssh-server ip addr 从这个截图里可以看到，这台 VirtualBox 虚拟机有 3 个网卡，其中名字是“enp0s3”的网卡就是我们之前配置的“192.168.56.1/24”网段，IP 地址是自动分配的“192.168.56.11”。如果你对自动分配的 IP 地址不是很满意，也可以在 Ubuntu 右上角的系统设置里修改网卡，把它从动态地址（DHCP）改成静态地址（Manual），具体的参数可以参考下面的截图，重启后新的 IP 地址就生效了。 这些工作完成之后，我建议你再给虚拟机拍个快照，做好备份工作，这样万一后面有什么意外发生环境被弄乱了，也可以轻松回滚到拍快照时的正确状态。 现在，让我们启动一个命令行终端（我用的是 Mac 里的“iTerm2”），使用 ssh ，输入用户名、密码和 IP 地址，就能够登录创建好的虚拟机了： ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:2:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"有哪些常用的 Linux 操作 到这里，我们的实验环境就算是搭建完毕了，虽然目前只有最基本的 Linux 系统，但在后面的“入门篇”“初级篇”“中级篇”里，我们会以它为基础逐步完善，实现完整的 Kubernetes 环境。特别提醒一下，因为 Kubernetes 基于 Linux，虽然也有图形化的 Dashboard，但更多的时候都是在命令行里工作，所以你需要对基本的 Linux 操作有所了解。学习 Linux 操作系统是另外一个很大的话题了，虽然它很重要，但并不是我们这门课的目标，我这里简单列一些比较常用的知识，你可以检测一下自己的掌握程度，如果有不了解的，希望你课后再查找相关资料补上这些点： 命令行界面称为“Shell”，支持交互操作，也支持脚本操作，也就是“Shell 编程”。root 用户有最高权限，但有安全隐患，所以通常我们都只使用普通用户身份，必要的时候使用 sudo 来临时使用 root 权限。查看系统当前进程列表的命令是 ps ，它是 Linux 里最常用的命令之一。查看文件可以使用 cat ，如果内容太多，可以用管道符 | ，后面跟 more 、less 。vim 是 Linux 里最流行的编辑器，但它的使用方式与一般的编辑器不同，学习成本略高。curl 能够以命令行的方式发送 HTTP 请求，多用来测试 HTTP 服务器（例如 Nginx）。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:2:6","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，我们的课前准备就要结束了，我再简单小结一下今天的要点内容：一个完善的实验环境能够很好地辅助我们的学习，建议在本地使用虚拟机从零开始搭建 Kubernetes 环境。虚拟机软件可以选择 VirtualBox（intel 芯片）和 VMWare Fusion（Apple M1 芯片），因为 Kubernetes 只能运行在 Linux 上，建议选择最新的 Ubuntu 22.04。虚拟机要事先配置好内存、网络等参数，安装系统时选最小安装，然后再安装一些常用的工具。虚拟机都支持快照，环境设置好后要及时备份，出现问题可以随时回滚恢复，避免重复安装系统浪费时间。 另外，我写专栏的惯例是在 GitHub 上开一个配套的学习项目，这门课程的仓库就叫“k8s_study”（https://github.com/chronolaw/k8s_study），里面有文档链接、安装脚本、测试命令、YAML 描述文件等等，你可以克隆下来在后续的课程中参照着学习。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:2:7","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"入门 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:3:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"01｜初识容器 俗话说：“万事开头难”，对于 Kubernetes 这个庞大而陌生的领域来说更是如此，如何迈出学习的第一步非常关键，所以，今天我们先从最简单、最基本的知识入手，聊聊最流行的容器技术 Docker，先搭建实验环境，再动手操作一下，进而破除它的神秘感。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:4:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Docker 的诞生 现在我们都已经对 Container、Kubernetes 这些技术名词耳熟能详了，但你知道这一切的开端——Docker，第一次在世界上的亮相是什么样子的吗？九年前，也就是 2013 年 3 月 15 日，在北美的圣克拉拉市召开了一场 Python 开发者社区的主题会议 PyCon，研究和探讨各种 Python 开发技术和应用，与我们常说的“云”“PaaS”“SaaS”根本毫不相关。在当天的会议日程快结束时，有一个“闪电演讲”（lighting talk）的小环节。其中有一位开发者，用了 5 分钟的时间，做了题为 “The future of Linux Containers” 的演讲，不过临近末尾因为超时而被主持人赶下了台，场面略显尴尬（你可以在这里回看这段具有历史意义的视频）。 相信你一定猜到了，这个只有短短 5 分钟的技术演示，就是我们目前所看到的、席卷整个业界的云原生大潮的开端。正是在这段演讲里，Solomon Hykes（dotCloud 公司，也就是 Docker 公司的创始人）首次向全世界展示了 Docker 技术。5 分钟的时间非常短，但演讲里却包含了几个现在已经普及，但当时却非常新奇的概念，比如容器、镜像、隔离运行进程等，信息量非常大。PyCon2013 大会之后，许多人都意识到了容器的价值和重要性，发现它能够解决困扰了云厂商多年的打包、部署、管理、运维等问题，Docker 也就迅速流行起来，成为了 GitHub 上的明星项目。然后在几个月的时间里，Docker 更是吸引了 Amazon、Google、Red Hat 等大公司的关注，这些公司利用自身的技术背景，纷纷在容器概念上大做文章，最终成就了我们今天所看到的至尊王者 Kubernetes 的出现。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:4:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Docker 的形态 好了，下面我们就要来一个“情境再现”，在我们的 Linux 虚拟机上搭建一个容器运行环境，模拟一下当年 Solomon Hykes 初次展示 Docker 的场景。当然，如今的 Docker 经过了九年的发展，已经远不是当初的“吴下阿蒙”了，不过最核心的那些概念和操作还是保持了一贯性，没有太大的变化。首先，我们需要对 Docker 的形态有所了解。目前使用 Docker 基本上有两个选择：Docker Desktop 和 Docker Engine。 Docker Desktop 是专门针对个人使用而设计的，支持 Mac 和 Windows 快速安装，具有直观的图形界面，还集成了许多周边工具，方便易用。不过，我个人不是太推荐使用 Docker Desktop，原因有两个。第一个，它是商业产品，难免会带有 Docker 公司的“私人气息”，有一些自己的、非通用的东西，不利于我们后续的 Kubernetes 学习。第二个，它只是对个人学习免费，受条款限制不能商用，我们在日常工作中难免会“踩到雷区”。Docker Engine 则和 Docker Desktop 正好相反，完全免费，但只能在 Linux 上运行，只能使用命令行操作，缺乏辅助工具，需要我们自己动手 DIY 运行环境。不过要是较起真来，它才是 Docker 当初的真正形态，“血脉”最纯正，也是现在各个公司在生产环境中实际使用的 Docker 产品，毕竟机房里 99% 的服务器跑的都是 Linux。所以，在接下来的学习过程里，我推荐使用 Docker Engine，之后在本专栏内，如果没有什么特别的声明，Docker 这个词通常指的就是 Docker Engine。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:4:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Docker 的安装 在课前准备里，我们已经在 Linux 虚拟机里安装了一些常用软件，用的是 Ubuntu 的包管理工具 apt，所以，我们仍然可以使用同样的方式来安装 Docker。先让我们尝试输入命令 docker ，会得到“命令未找到”的提示，还有如何安装的建议： …… ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:4:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Docker 的使用 现在，我们已经有了可用的 Docker 运行环境，就可以来重现 9 年前 Solomon Hykes 的那场简短的技术演示了。首先，我们使用命令 docker ps，它会列出当前系统里运行的容器，就像我们在 Linux 系统里使用 ps 命令列出运行的进程一样。注意，所有的 Docker 操作都是这种形式：以 docker 开始，然后是一个具体的子命令，之前的 docker version 和 docker info 也遵循了这样的规则。你还可以用 help 或者 –help 来获取帮助信息，查看命令清单和更详细的说明。 因为我们刚刚安装好 Docker 环境，这个时候还没有运行任何容器，所以列表显然是空的。 接下来，让我们尝试另一个非常重要的命令 docker pull ，从外部的镜像仓库（Registry）拉取一个 busybox 镜像（image），你可以把它类比成是 Ubuntu 里的“apt install”下载软件包： docker pull 会有一些看起来比较奇怪的输出信息，现在我们暂时不用管，后续的课程会有详细解释。我们再执行命令 docker images ，它会列出当前 Docker 所存储的所有镜像： 可以看到，命令会显示有一个叫 busybox 的镜像，镜像的 ID 号是一串 16 进制数字，大小是 1.41MB。现在，我们就要从这个镜像启动容器了，命令是 docker run ，执行 echo 输出字符串，这也正是 Solomon Hykes 在大会上所展示的最精彩的那部分： docker run busybox echo hello world 然后我们再用 docker ps 命令，加上一个参数 -a ，就可以看到这个已经运行完毕的容器 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:4:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Docker 的架构 这里我再稍微讲一下 Docker Engine 的架构，让你有个初步的印象，也为之后的学习做一个铺垫。下面的这张图来自 Docker 官网（https://docs.docker.com/get-started/overview/），精准地描述了 Docker Engine 的内部角色和工作流程，对我们的学习研究非常有指导意义。 刚才我们敲的命令行 docker 实际上是一个客户端 client ，它会与 Docker Engine 里的后台服务 Docker daemon 通信，而镜像则存储在远端的仓库 Registry 里，客户端并不能直接访问镜像仓库。Docker client 可以通过 build、pull、run等命令向 Docker daemon 发送请求，而 Docker daemon 则是容器和镜像的“大管家”，负责从远端拉取镜像、在本地存储镜像，还有从镜像生成容器、管理容器等所有功能。所以，在 Docker Engine 里，真正干活的其实是默默运行在后台的 Docker daemon，而我们实际操作的命令行工具“docker”只是个“传声筒”的角色。Docker 官方还提供一个“hello-world”示例，可以为你展示 Docker client 到 Docker daemon 再到 Registry 的详细工作流程，你只需要执行这样一个命令： docker run hello-world 它会先检查本地镜像，如果没有就从远程仓库拉取，再运行容器，最后输出运行信息： ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:4:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们初步了解了容器技术，再简单小结一下主要的内容：容器技术起源于 Docker，它目前有两个产品：Docker Desktop 和 Docker Engine，我们的课程里推荐使用免费的 Docker Engine，它可以在 Ubuntu 系统里直接用 apt 命令安装。Docker Engine 需要使用命令行操作，主命令是 docker，后面再接各种子命令。查看 Docker 的基本信息的命令是 docker version 和 docker info ，其他常用的命令有 docker ps、docker pull、docker images、docker run。Docker Engine 是典型的客户端 / 服务器（C/S）架构，命令行工具 Docker 直接面对用户，后面的 Docker daemon 和 Registry 协作完成各种功能。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:4:6","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"02｜被隔离的进程：一起来看看容器的本质 广义上来说，容器技术是动态的容器、静态的镜像和远端的仓库这三者的组合。不过，“容器”这个术语作为容器技术里的核心概念，不仅是大多数初次接触这个领域的人，即使是一些已经有使用经验的人，想要准确地把握它们的内涵、本质都是比较困难的。那么今天，我们就一起来看看究竟什么是容器（即狭义的、动态的容器）。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:5:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"容器到底是什么 从字面上来看，容器就是 Container，一般把它形象地比喻成现实世界里的集装箱，它也正好和 Docker 的现实含义相对应，因为码头工人（那只可爱的小鲸鱼）就是不停地在搬运集装箱。 集装箱的作用是标准化封装各种货物，一旦打包完成之后，就可以从一个地方迁移到任意的其他地方。相比散装形式而言，集装箱隔离了箱内箱外两个世界，保持了货物的原始形态，避免了内外部相互干扰，极大地简化了商品的存储、运输、管理等工作。再回到我们的计算机世界，容器也发挥着同样的作用，不过它封装的货物是运行中的应用程序，也就是进程，同样它也会把进程与外界隔离开，让进程与外部系统互不影响。我们还是来实际操作一下吧，来看看在容器里运行的进程是个什么样子。首先，我们使用 docker pull 命令，拉取一个新的镜像——操作系统 Alpine： docker pull alpine 然后我们使用 docker run 命令运行它的 Shell 程序： docker run -it alpine sh 注意我们在这里多加了一个 -it 参数，这样我们就会暂时离开当前的 Ubuntu 操作系统，进入容器内部。现在，让我们执行 cat /etc/os-release ，还有 ps 这两个命令，最后再使用 exit 退出，看看容器里与容器外有什么不同： 就像这张截图里所显示的，在容器里查看系统信息，会发现已经不再是外面的 Ubuntu 系统了，而是变成了 Alpine Linux 3.15，使用 ps 命令也只会看到一个完全“干净”的运行环境，除了 Shell（即 sh）没有其他的进程存在。也就是说，在容器内部是一个全新的 Alpine 操作系统，在这里运行的应用程序完全看不到外面的 Ubuntu 系统，两个系统被互相“隔离”了，就像是一个“世外桃源”。我们还可以再拉取一个 Ubuntu 18.04 的镜像，用同样的方式进入容器内部，然后执行 apt update、apt install 等命令来看看： docker pull ubuntu:18.04 docker run -it ubuntu:18.04 sh # 下面的命令都是在容器内执行 cat /etc/os-release apt update apt install -y wget redis redis-server \u0026 这里我就不截图了，具体的结果留给你课下去实际操作体会。可以看到的是，容器里是另一个完整的 Ubuntu 18.04 系统，我们可以在这个“世外桃源”做任意的事情，比如安装应用、运行 Redis 服务等。但无论我们在容器里做什么，都不会影响外面的 Ubuntu 系统（当然不是绝对的）。到这里，我们就可以得到一个初步的结论：容器，就是一个特殊的隔离环境，它能够让进程只看到这个环境里的有限信息，不能对外界环境施加影响。那么，很自然地，我们会产生另外一个问题：为什么需要创建这样的一个隔离环境，直接让进程在系统里运行不好吗？ ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:5:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"为什么要隔离 相信因为这两年疫情，你对“隔离”这个词不会感觉到太陌生。为了防止疫情蔓延，我们需要建立方舱、定点医院，把患病人群控制在特定的区域内，更进一步还会实施封闭小区、关停商场等行动。虽然这些措施带来了一些不便，但都是为了整个社会更大范围的正常运转。同样的，在计算机世界里的隔离也是出于同样的考虑，也就是系统安全。对于 Linux 操作系统来说，一个不受任何限制的应用程序是十分危险的。这个进程能够看到系统里所有的文件、所有的进程、所有的网络流量，访问内存里的任何数据，那么恶意程序很容易就会把系统搞瘫痪，正常程序也可能会因为无意的 Bug 导致信息泄漏或者其他安全事故。虽然 Linux 提供了用户权限控制，能够限制进程只访问某些资源，但这个机制还是比较薄弱的，和真正的“隔离”需求相差得很远。而现在，使用容器技术，我们就可以让应用程序运行在一个有严密防护的“沙盒”（Sandbox）环境之内，就好像是把进程请进了“隔离酒店”，它可以在这个环境里自由活动，但绝不允许“越界”，从而保证了容器外系统的安全。 另外，在计算机里有各种各样的资源，CPU、内存、硬盘、网卡，虽然目前的高性能服务器都是几十核 CPU、上百 GB 的内存、数 TB 的硬盘、万兆网卡，但这些资源终究是有限的，而且考虑到成本，也不允许某个应用程序无限制地占用。容器技术的另一个本领就是为应用程序加上资源隔离，在系统里切分出一部分资源，让它只能使用指定的配额，比如只能使用一个 CPU，只能使用 1GB 内存等等，就好像在隔离酒店里保证一日三餐，但想要吃山珍海味那是不行的。这样就可以避免容器内进程的过度系统消耗，充分利用计算机硬件，让有限的资源能够提供稳定可靠的服务。所以，虽然进程被“关”在了容器里，损失了一些自由，但却保证了整个系统的安全。而且只要进程遵守隔离规定，不做什么出格的事情，也完全是可以正常运行的。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:5:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"与虚拟机的区别是什么 你也许会说，这么看来，容器不过就是常见的“沙盒”技术中的一种，和虚拟机差不了多少，那么它与虚拟机的区别在哪里呢？又有什么样的优势呢？在我看来，其实容器和虚拟机面对的都是相同的问题，使用的也都是虚拟化技术，只是所在的层次不同，我们可以参考 Docker 官网上的两张图，把这两者对比起来会更利于学习理解。 （Docker 官网的图示其实并不太准确，容器并不直接运行在 Docker 上，Docker 只是辅助建立隔离环境，让容器基于 Linux 操作系统运行）首先，容器和虚拟机的目的都是隔离资源，保证系统安全，然后是尽量提高资源的利用率。之前在使用 VirtualBox/VMware 创建虚拟机的时候，你也应该看到了，它们能够在宿主机系统里完整虚拟化出一套计算机硬件，在里面还能够安装任意的操作系统，这内外两个系统也同样是完全隔离，互不干扰。而在数据中心的服务器上，虚拟机软件（即图中的 Hypervisor）同样可以把一台物理服务器虚拟成多台逻辑服务器，这些逻辑服务器彼此独立，可以按需分隔物理服务器的资源，为不同的用户所使用。从实现的角度来看，虚拟机虚拟化出来的是硬件，需要在上面再安装一个操作系统后才能够运行应用程序，而硬件虚拟化和操作系统都比较“重”，会消耗大量的 CPU、内存、硬盘等系统资源，但这些消耗其实并没有带来什么价值，属于“重复劳动”和“无用功”，不过好处就是隔离程度非常高，每个虚拟机之间可以做到完全无干扰。 我们再来看容器（即图中的 Docker），它直接利用了下层的计算机硬件和操作系统，因为比虚拟机少了一层，所以自然就会节约 CPU 和内存，显得非常轻量级，能够更高效地利用硬件资源。不过，因为多个容器共用操作系统内核，应用程序的隔离程度就没有虚拟机那么高了。运行效率，可以说是容器相比于虚拟机最大的优势，在这个对比图中就可以看到，同样的系统资源，虚拟机只能跑 3 个应用，其他的资源都用来支持虚拟机运行了，而容器则能够把这部分资源释放出来，同时运行 6 个应用。 当然，这个对比图只是一个形象的展示，不是严谨的数值比较，不过我们还可以用手里现有的 VirtualBox/VMware 虚拟机与 Docker 容器做个简单对比。一个普通的 Ubuntu 虚拟机安装完成之后，体积都是 GB 级别的，再安装一些应用很容易就会上到 10GB，启动的时间通常需要几分钟，我们的电脑上同时运行十来个虚拟机可能就是极限了。而一个 Ubuntu 镜像大小则只有几十 MB，启动起来更是非常快，基本上不超过一秒钟，同时跑上百个容器也毫无问题。不过，虚拟机和容器这两种技术也不是互相排斥的，它们完全可以结合起来使用，就像我们的课程里一样，用虚拟机实现与宿主机的强隔离，然后在虚拟机里使用 Docker 容器来快速运行应用程序。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:5:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"隔离是怎么实现的 我们知道虚拟机使用的是 Hypervisor（KVM、Xen 等），那么，容器是怎么实现和下层计算机硬件和操作系统交互的呢？为什么它会具有高效轻便的隔离特性呢？其实奥秘就在于 Linux 操作系统内核之中，为资源隔离提供了三种技术：namespace、cgroup、chroot，虽然这三种技术的初衷并不是为了实现容器，但它们三个结合在一起就会发生奇妙的“化学反应”。namespace 是 2002 年从 Linux 2.4.19 开始出现的，和编程语言里的 namespace 有点类似，它可以创建出独立的文件系统、主机名、进程号、网络等资源空间，相当于给进程盖了一间小板房，这样就实现了系统全局资源和进程局部资源的隔离。cgroup 是 2008 年从 Linux 2.6.24 开始出现的，它的全称是 Linux Control Group，用来实现对进程的 CPU、内存等资源的优先级和配额限制，相当于给进程的小板房加了一个天花板。 chroot 的历史则要比前面的 namespace、cgroup 要古老得多，早在 1979 年的 UNIX V7 就已经出现了，它可以更改进程的根目录，也就是限制访问文件系统，相当于给进程的小板房铺上了地砖。你看，综合运用这三种技术，一个四四方方、具有完善的隔离特性的容器就此出现了，进程就可以搬进这个小房间，过它的“快乐生活”了。我觉得用鲁迅先生的一句诗来描述这个情景最为恰当：躲进小楼成一统，管他冬夏与春秋。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:5:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们一起学习了容器技术中最关键的概念：动态的容器，再简单小结一下课程的要点：容器就是操作系统里一个特殊的“沙盒”环境，里面运行的进程只能看到受限的信息，与外部系统实现了隔离。容器隔离的目的是为了系统安全，限制了进程能够访问的各种资源。相比虚拟机技术，容器更加轻巧、更加高效，消耗的系统资源非常少，在云计算时代极具优势。容器的基本实现技术是 Linux 系统里的 namespace、cgroup、chroot。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:5:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"03｜容器化的应用：会了这些你就是Docker高手 在上一次课里，我们了解了容器技术中最核心的概念：容器，知道它就是一个系统中被隔离的特殊环境，进程可以在其中不受干扰地运行。我们也可以把这段描述再简化一点：容器就是被隔离的进程。相比笨重的虚拟机，容器有许多优点，那我们应该如何创建并运行容器呢？是要用 Linux 内核里的 namespace、cgroup、chroot 三件套吗？当然不会，那样的方式实在是太原始了，所以今天，我们就以 Docker 为例，来看看什么是容器化的应用，怎么来操纵容器化的应用。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:6:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"什么是容器化的应用 之前我们运行容器的时候，显然不是从零开始的，而是要先拉取一个“镜像”（image），再从这个“镜像”来启动容器，像第一节课这样： docker pull busybox docker run busybox echo hello world 那么，这个“镜像”到底是什么东西呢？它又和“容器”有什么关系呢？其实我们在其他场合中也曾经见到过“镜像”这个词，比如最常见的光盘镜像，重装电脑时使用的硬盘镜像，还有虚拟机系统镜像。这些“镜像”都有一些相同点：只读，不允许修改，以标准格式存储了一系列的文件，然后在需要的时候再从中提取出数据运行起来。容器技术里的镜像也是同样的道理。因为容器是由操作系统动态创建的，那么必然就可以用一种办法把它的初始环境给固化下来，保存成一个静态的文件，相当于是把容器给“拍扁”了，这样就可以非常方便地存放、传输、版本化管理了。 如果还拿之前的“小板房”来做比喻的话，那么镜像就可以说是一个“样板间”，把运行进程所需要的文件系统、依赖库、环境变量、启动参数等所有信息打包整合到了一起。之后镜像文件无论放在哪里，操作系统都能根据这个“样板间”快速重建容器，应用程序看到的就会是一致的运行环境了。从功能上来看，镜像和常见的 tar、rpm、deb 等安装包一样，都打包了应用程序，但最大的不同点在于它里面不仅有基本的可执行文件，还有应用运行时的整个系统环境。这就让镜像具有了非常好的跨平台便携性和兼容性，能够让开发者在一个系统上开发（例如 Ubuntu），然后打包成镜像，再去另一个系统上运行（例如 CentOS），完全不需要考虑环境依赖的问题，是一种更高级的应用打包方式。 理解了这一点，我们再回过头来看看第一节课里运行的 Docker 命令。docker pull busybox ，就是获取了一个打包了 busybox 应用的镜像，里面固化了 busybox 程序和它所需的完整运行环境。docker run busybox echo hello world ，就是提取镜像里的各种信息，运用 namespace、cgroup、chroot 技术创建出隔离环境，然后再运行 busybox 的 echo 命令，输出 hello world 的字符串。这两个步骤，由于是基于标准的 Linux 系统调用和只读的镜像文件，所以，无论是在哪种操作系统上，或者是使用哪种容器实现技术，都会得到完全一致的结果。 推而广之，任何应用都能够用这种形式打包再分发后运行，这也是无数开发者梦寐以求的“一次编写，到处运行（Build once, Run anywhere）”的至高境界。所以，所谓的“容器化的应用”，或者“应用的容器化”，就是指应用程序不再直接和操作系统打交道，而是封装成镜像，再交给容器环境去运行。现在你就应该知道了，镜像就是静态的应用容器，容器就是动态的应用镜像，两者互相依存，互相转化，密不可分。之前的那张 Docker 官方架构图你还有印象吧，我们在第一节课曾经简单地介绍过。可以看到，在 Docker 里的核心处理对象就是镜像（image）和容器（container）： 好，理解了什么是容器化的应用，接下来我们再来学习怎么操纵容器化的应用。因为镜像是容器运行的根本，先有镜像才有容器，所以先来看看关于镜像的一些常用命令。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:6:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"常用的镜像操作有哪些 在前面的课程里你应该已经了解了两个基本命令，docker pull 从远端仓库拉取镜像，docker images 列出当前本地已有的镜像。docker pull 的用法还是比较简单的，和普通的下载非常像，不过我们需要知道镜像的命名规则，这样才能准确地获取到我们想要的容器镜像。 镜像的完整名字由两个部分组成，名字和标签，中间用 : 连接起来。 名字表明了应用的身份，比如 busybox、Alpine、Nginx、Redis 等等。标签（tag）则可以理解成是为了区分不同版本的应用而做的额外标记，任何字符串都可以，比如 3.15 是纯数字的版本号、jammy 是项目代号、1.21-alpine 是版本号加操作系统名等等。其中有一个比较特殊的标签叫“latest”，它是默认的标签，如果只提供名字没有附带标签，那么就会使用这个默认的“latest”标签。那么现在，你就可以把名字和标签组合起来，使用 docker pull 来拉取一些镜像了： docker pull alpine:3.15 docker pull ubuntu:jammy docker pull nginx:1.21-alpine docker pull nginx:alpine docker pull redis 有了这些镜像之后，我们再用 docker images 命令来看看它们的具体信息 在这个列表里，你可以看到，REPOSITORY 列就是镜像的名字，TAG 就是这个镜像的标签，那么第三列“IMAGE ID”又是什么意思呢？它可以说是镜像唯一的标识，就好像是身份证号一样。比如这里我们可以用“ubuntu:jammy”来表示 Ubuntu 22.04 镜像，同样也可以用它的 ID“d4c2c……”来表示。另外，你可能还会注意到，截图里的两个镜像“nginx:1.21-alpine”和“nginx:alpine”的 IMAGE ID 是一样的，都是“a63aa……”。这其实也很好理解，这就像是人的身份证号码是唯一的，但可以有大名、小名、昵称、绰号，同一个镜像也可以打上不同的标签，这样应用在不同的场合就更容易理解。IMAGE ID 还有一个好处，因为它是十六进制形式且唯一，Docker 特意为它提供了“短路”操作，在本地使用镜像的时候，我们不用像名字那样要完全写出来这一长串数字，通常只需要写出前三位就能够快速定位，在镜像数量比较少的时候用两位甚至一位数字也许就可以了。 来看另一个镜像操作命令 docker rmi ，它用来删除不再使用的镜像，可以节约磁盘空间，注意命令 rmi ，实际上是“remove image”的简写。下面我们就来试验一下，使用名字和 IMAGE ID 来删除镜像： docker rmi redis docker rmi d4c 这里的第一个 rmi 删除了 Redis 镜像，因为没有显式写出标签，默认使用的就是“latest”。第二个 rmi 没有给出名字，而是直接使用了 IMAGE ID 的前三位，也就是“d4c”，Docker 就会直接找到这个 ID 前缀的镜像然后删除。Docker 里与镜像相关的命令还有很多，不过以上的 docker pull、docker images、docker rmi 就是最常用的三个了，其他的命令我们后续课程会陆续介绍。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:6:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"常用的容器操作 有哪些现在我们已经在本地存放了镜像，就可以使用 docker run 命令把这些静态的应用运行起来，变成动态的容器了。基本的格式是“docker run 设置参数”，再跟上“镜像名或 ID”，后面可能还会有附加的“运行命令”。比如这个命令： docker run -h srv alpine hostname 这里的 -h srv 就是容器的运行参数，alpine 是镜像名，它后面的 hostname 表示要在容器里运行的“hostname”这个程序，输出主机名。docker run 是最复杂的一个容器操作命令，有非常多的额外参数用来调整容器的运行状态，你可以加上 –help 来看它的帮助信息，今天我只说几个最常用的参数。-it 表示开启一个交互式操作的 Shell，这样可以直接进入容器内部，就好像是登录虚拟机一样。（它实际上是“-i”和“-t”两个参数的组合形式）-d 表示让容器在后台运行，这在我们启动 Nginx、Redis 等服务器程序的时候非常有用。–name 可以为容器起一个名字，方便我们查看，不过它不是必须的，如果不用这个参数，Docker 会分配一个随机的名字。 下面我们就来练习一下这三个参数，分别运行 Nginx、Redis 和 Ubuntu： docker run -d nginx:alpine # 后台运行Nginx docker run -d --name red_srv redis # 后台运行Redis docker run -it --name ubuntu 2e6 sh # 使用IMAGE ID，登录Ubuntu18.04 因为第三个命令使用的是 -it 而不是 -d ，所以它会进入容器里的 Ubuntu 系统，我们需要另外开一个终端窗口，使用 docker ps 命令来查看容器的运行状态： 可以看到，每一个容器也会有一个“CONTAINER ID”，它的作用和镜像的“IMAGE ID”是一样的，唯一标识了容器。对于正在运行中的容器，我们可以使用 docker exec 命令在里面执行另一个程序，效果和 docker run 很类似，但因为容器已经存在，所以不会创建新的容器。它最常见的用法是使用 -it 参数打开一个 Shell，从而进入容器内部，例如： docker exec -it red_srv sh 这样我们就“登录”进了 Redis 容器，可以很方便地查看服务的运行状态或者日志。运行中的容器还可以使用 docker stop 命令来强制停止，这里我们仍然可以使用容器名字，不过或许用“CONTAINER ID”的前三位数字会更加方便。 docker stop ed4 d60 45c 容器被停止后使用 docker ps 命令就看不到了，不过容器并没有被彻底销毁，我们可以使用 docker ps -a 命令查看系统里所有的容器，当然也包括已经停止运行的容器： 这些停止运行的容器可以用 docker start 再次启动运行，如果你确定不再需要它们，可以使用 docker rm 命令来彻底删除。 注意，这个命令与 docker rmi 非常像，区别在于它没有后面的字母“i”，所以只会删除容器，不删除镜像。下面我们就来运行 docker rm 命令，使用“CONTAINER ID”的前两位数字来删除这些容器： docker rm ed d6 45 执行删除命令之后，再用 docker ps -a 查看列表就会发现这些容器已经彻底消失了。你可能会感觉这样的容器管理方式很麻烦，启动后要 ps 看 ID 再删除，如果稍微不注意，系统就会遗留非常多的“死”容器，占用系统资源，有没有什么办法能够让 Docker 自动删除不需要的容器呢？办法当然有，就是在执行 docker run 命令的时候加上一个 –rm 参数，这就会告诉 Docker 不保存容器，只要运行完毕就自动清除，省去了我们手工管理容器的麻烦。我们还是用刚才的 Nginx、Redis 和 Ubuntu 这三个容器来试验一下，加上 –rm 参数（省略了 name 参数）： docker run -d --rm nginx:alpine docker run -d --rm redis docker run -it --rm 2e6 sh 然后我们用 docker stop 停止容器，再用 docker ps -a ，就会发现不需要我们再手动执行 docker rm ，Docker 已经自动删除了这三个容器。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:6:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们一起学习了容器化的应用，然后使用 Docker 实际操作了镜像和容器，运行了被容器化的 Alpine、Nginx、Redis 等应用。镜像是容器的静态形式，它打包了应用程序的所有运行依赖项，方便保存和传输。使用容器技术运行镜像，就形成了动态的容器，由于镜像只读不可修改，所以应用程序的运行环境总是一致的。而容器化的应用就是指以镜像的形式打包应用程序，然后在容器环境里从镜像启动容器。由于 Docker 的命令比较多，而且每个命令还有许多参数，一节课里很难把它们都详细说清楚，希望你课下参考 Docker 自带的帮助或者官网文档（https://docs.docker.com/reference/），再多加实操练习，相信你一定能够成为 Docker 高手。 我这里就对今天的镜像操作和容器操作做个小结：常用的镜像操作有 docker pull、docker images、docker rmi，分别是拉取镜像、查看镜像和删除镜像。用来启动容器的 docker run 是最常用的命令，它有很多参数用来调整容器的运行状态，对于后台服务来说应该使用 -d。docker exec 命令可以在容器内部执行任意程序，对于调试排错特别有用。其他常用的容器操作还有 docker ps、docker stop、docker rm，用来查看容器、停止容器和删除容器。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:6:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"04｜创建容器镜像：如何编写正确、高效的Dockerfile 上一次的课程里我们一起学习了容器化的应用，也就是被打包成镜像的应用程序，然后再用各种 Docker 命令来运行、管理它们。那么这又会带来一个疑问：这些镜像是怎么创建出来的？我们能不能够制作属于自己的镜像呢？所以今天，我就来讲解镜像的内部机制，还有高效、正确地编写 Dockerfile 制作容器镜像的方法。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:7:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"镜像的内部机制是什么 现在你应该知道，镜像就是一个打包文件，里面包含了应用程序还有它运行所依赖的环境，例如文件系统、环境变量、配置参数等等。环境变量、配置参数这些东西还是比较简单的，随便用一个 manifest 清单就可以管理，真正麻烦的是文件系统。为了保证容器运行环境的一致性，镜像必须把应用程序所在操作系统的根目录，也就是 rootfs，都包含进来。虽然这些文件里不包含系统内核（因为容器共享了宿主机的内核），但如果每个镜像都重复做这样的打包操作，仍然会导致大量的冗余。可以想象，如果有一千个镜像，都基于 Ubuntu 系统打包，那么这些镜像里就会重复一千次 Ubuntu 根目录，对磁盘存储、网络传输都是很大的浪费。 很自然的，我们就会想到，应该把重复的部分抽取出来，只存放一份 Ubuntu 根目录文件，然后让这一千个镜像以某种方式共享这部分数据。这个思路，也正是容器镜像的一个重大创新点：分层，术语叫“Layer”。容器镜像内部并不是一个平坦的结构，而是由许多的镜像层组成的，每层都是只读不可修改的一组文件，相同的层可以在镜像之间共享，然后多个层像搭积木一样堆叠起来，再使用一种叫“Union FS 联合文件系统”的技术把它们合并在一起，就形成了容器最终看到的文件系统（图片来源）。 我来拿大家都熟悉的千层糕做一个形象的比喻吧。千层糕也是由很多层叠加在一起的，从最上面可以看到每层里面镶嵌的葡萄干、核桃、杏仁、青丝等，每一层糕就相当于一个 Layer，干果就好比是 Layer 里的各个文件。但如果某两层的同一个位置都有干果，也就是有文件同名，那么我们就只能看到上层的文件，而下层的就被屏蔽了。你可以用命令 docker inspect 来查看镜像的分层信息，比如 nginx:alpine 镜像： docker inspect nginx:alpine 它的分层信息在“RootFS”部分： 通过这张截图就可以看到，nginx:alpine 镜像里一共有 6 个 Layer。相信你现在也就明白，之前在使用 docker pull、docker rmi 等命令操作镜像的时候，那些“奇怪”的输出信息是什么了，其实就是镜像里的各个 Layer。Docker 会检查是否有重复的层，如果本地已经存在就不会重复下载，如果层被其他镜像共享就不会删除，这样就可以节约磁盘和网络成本。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:7:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Dockerfile 是什么 知道了容器镜像的内部结构和基本原理，我们就可以来学习如何自己动手制作容器镜像了，也就是自己打包应用。在之前我们讲容器的时候，曾经说过容器就是“小板房”，镜像就是“样板间”。那么，要造出这个“样板间”，就必然要有一个“施工图纸”，由它来规定如何建造地基、铺设水电、开窗搭门等动作。这个“施工图纸”就是“Dockerfile”。比起容器、镜像来说，Dockerfile 非常普通，它就是一个纯文本，里面记录了一系列的构建指令，比如选择基础镜像、拷贝文件、运行脚本等等，每个指令都会生成一个 Layer，而 Docker 顺序执行这个文件里的所有步骤，最后就会创建出一个新的镜像出来。我们来看一个最简单的 Dockerfile 实例： # Dockerfile.busybox FROM busybox # 选择基础镜像 CMD echo \"hello world\" # 启动容器时默认运行的命令 这个文件里只有两条指令。第一条指令是 FROM，所有的 Dockerfile 都要从它开始，表示选择构建使用的基础镜像，相当于“打地基”，这里我们使用的是 busybox。第二条指令是 CMD，它指定 docker run 启动容器时默认运行的命令，这里我们使用了 echo 命令，输出“hello world”字符串。现在有了 Dockerfile 这张“施工图纸”，我们就可以请出“施工队”了，用 docker build 命令来创建出镜像： docker build -f Dockerfile.busybox . Sending build context to Docker daemon 7.68kB Step 1/2 : FROM busybox ---\u003e d38589532d97 Step 2/2 : CMD echo \"hello world\" ---\u003e Running in c5a762edd1c8 Removing intermediate container c5a762edd1c8 ---\u003e b61882f42db7 Successfully built b61882f42db7 你需要特别注意命令的格式，用 -f 参数指定 Dockerfile 文件名，后面必须跟一个文件路径，叫做“构建上下文”（build’s context），这里只是一个简单的点号，表示当前路径的意思。接下来，你就会看到 Docker 会逐行地读取并执行 Dockerfile 里的指令，依次创建镜像层，再生成完整的镜像。新的镜像暂时还没有名字（用 docker images 会看到是 \u003c none\u003e），但我们可以直接使用“IMAGE ID”来查看或者运行： docker inspect b61 docker run b61 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:7:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"怎样编写正确、高效的 Dockerfile 大概了解了 Dockerfile 之后，我再来讲讲编写 Dockerfile 的一些常用指令和最佳实践，帮你在今后的工作中把它写好、用好。首先因为构建镜像的第一条指令必须是 FROM，所以基础镜像的选择非常关键。如果关注的是镜像的安全和大小，那么一般会选择 Alpine；如果关注的是应用的运行稳定性，那么可能会选择 Ubuntu、Debian、CentOS。 FROM alpine:3.15 # 选择Alpine镜像 FROM ubuntu:bionic # 选择Ubuntu镜像 我们在本机上开发测试时会产生一些源码、配置等文件，需要打包进镜像里，这时可以使用 COPY 命令，它的用法和 Linux 的 cp 差不多，不过拷贝的源文件必须是“构建上下文”路径里的，不能随意指定文件。也就是说，如果要从本机向镜像拷贝文件，就必须把这些文件放到一个专门的目录，然后在 docker build 里指定“构建上下文”到这个目录才行。这里有两个 COPY 命令示例，你可以看一下： COPY ./a.txt /tmp/a.txt # 把构建上下文里的a.txt拷贝到镜像的/tmp目录 COPY /etc/hosts /tmp # 错误！不能使用构建上下文之外的文件 接下来要说的就是 Dockerfile 里最重要的一个指令 RUN ，它可以执行任意的 Shell 命令，比如更新系统、安装应用、下载文件、创建目录、编译程序等等，实现任意的镜像构建步骤，非常灵活。RUN 通常会是 Dockerfile 里最复杂的指令，会包含很多的 Shell 命令，但 Dockerfile 里一条指令只能是一行，所以有的 RUN 指令会在每行的末尾使用续行符 \\，命令之间也会用 \u0026\u0026 来连接，这样保证在逻辑上是一行，就像下面这样： RUN apt-get update \\ \u0026\u0026 apt-get install -y \\ build-essential \\ curl \\ make \\ unzip \\ \u0026\u0026 cd /tmp \\ \u0026\u0026 curl -fSL xxx.tar.gz -o xxx.tar.gz\\ \u0026\u0026 tar xzf xxx.tar.gz \\ \u0026\u0026 cd xxx \\ \u0026\u0026 ./config \\ \u0026\u0026 make \\ \u0026\u0026 make clean 有的时候在 Dockerfile 里写这种超长的 RUN 指令很不美观，而且一旦写错了，每次调试都要重新构建也很麻烦，所以你可以采用一种变通的技巧：把这些 Shell 命令集中到一个脚本文件里，用 COPY 命令拷贝进去再用 RUN 来执行： COPY setup.sh /tmp/ # 拷贝脚本到/tmp目录 RUN cd /tmp \u0026\u0026 chmod +x setup.sh \\ # 添加执行权限 \u0026\u0026 ./setup.sh \u0026\u0026 rm setup.sh # 运行脚本然后再删除 RUN 指令实际上就是 Shell 编程，如果你对它有所了解，就应该知道它有变量的概念，可以实现参数化运行，这在 Dockerfile 里也可以做到，需要使用两个指令 ARG 和 ENV。它们区别在于 ARG 创建的变量只在镜像构建过程中可见，容器运行时不可见，而 ENV 创建的变量不仅能够在构建镜像的过程中使用，在容器运行时也能够以环境变量的形式被应用程序使用。下面是一个简单的例子，使用 ARG 定义了基础镜像的名字（可以用在“FROM”指令里），使用 ENV 定义了两个环境变量： ARG IMAGE_BASE=\"node\" ARG IMAGE_TAG=\"alpine\" ENV PATH=$PATH:/tmp ENV DEBUG=OFF 还有一个重要的指令是 EXPOSE，它用来声明容器对外服务的端口号，对现在基于 Node.js、Tomcat、Nginx、Go 等开发的微服务系统来说非常有用： EXPOSE 443 # 默认是tcp协议 EXPOSE 53/udp # 可以指定udp协议 讲了这些 Dockerfile 指令之后，我还要特别强调一下，因为每个指令都会生成一个镜像层，所以 Dockerfile 里最好不要滥用指令，尽量精简合并，否则太多的层会导致镜像臃肿不堪。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:7:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"docker build 是怎么工作的 Dockerfile 必须要经过 docker build 才能生效，所以我们再来看看 docker build 的详细用法。刚才在构建镜像的时候，你是否对“构建上下文”这个词感到有些困惑呢？它到底是什么含义呢？我觉得用 Docker 的官方架构图来理解会比较清楚（注意图中与“docker build”关联的虚线）。因为命令行“docker”是一个简单的客户端，真正的镜像构建工作是由服务器端的“Docker daemon”来完成的，所以“docker”客户端就只能把“构建上下文”目录打包上传（显示信息 Sending build context to Docker daemon ），这样服务器才能够获取本地的这些文件。 明白了这一点，你就会知道，“构建上下文”其实与 Dockerfile 并没有直接的关系，它其实指定了要打包进镜像的一些依赖文件。而 COPY 命令也只能使用基于“构建上下文”的相对路径，因为“Docker daemon”看不到本地环境，只能看到打包上传的那些文件。但这个机制也会导致一些麻烦，如果目录里有的文件（例如 readme/.git/.svn 等）不需要拷贝进镜像，docker 也会一股脑地打包上传，效率很低。为了避免这种问题，你可以在“构建上下文”目录里再建立一个 .dockerignore 文件，语法与 .gitignore 类似，排除那些不需要的文件。下面是一个简单的示例，表示不打包上传后缀是“swp”“sh”的文件： # docker ignore *.swp *.sh 另外关于 Dockerfile，一般应该在命令行里使用 -f 来显式指定。但如果省略这个参数，docker build 就会在当前目录下找名字是 Dockerfile 的文件。所以，如果只有一个构建目标的话，文件直接叫“Dockerfile”是最省事的。现在我们使用 docker build 应该就没什么难点了，不过构建出来的镜像只有“IMAGE ID”没有名字，不是很方便。为此你可以加上一个 -t 参数，也就是指定镜像的标签（tag），这样 Docker 就会在构建完成后自动给镜像添加名字。当然，名字必须要符合上节课里的命名规范，用 : 分隔名字和标签，如果不提供标签默认就是“latest”。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:7:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 了，今天我们一起学习了容器镜像的内部结构，重点理解容器镜像是由多个只读的 Layer 构成的，同一个 Layer 可以被不同的镜像共享，减少了存储和传输的成本。如何编写 Dockerfile 内容稍微多一点，我再简单做个小结： 创建镜像需要编写 Dockerfile，写清楚创建镜像的步骤，每个指令都会生成一个 Layer。Dockerfile 里，第一个指令必须是 FROM，用来选择基础镜像，常用的有 Alpine、Ubuntu 等。其他常用的指令有：COPY、RUN、EXPOSE，分别是拷贝文件，运行 Shell 命令，声明服务端口号。docker build 需要用 -f 来指定 Dockerfile，如果不指定就使用当前目录下名字是“Dockerfile”的文件。docker build 需要指定“构建上下文”，其中的文件会打包上传到 Docker daemon，所以尽量不要在“构建上下文”中存放多余的文件。创建镜像的时候应当尽量使用 -t 参数，为镜像起一个有意义的名字，方便管理。 今天讲了不少，但关于创建镜像还有很多高级技巧等待你去探索，比如使用缓存、多阶段构建等等，你可以再参考 Docker 官方文档（https://docs.docker.com/engine/reference/builder/），或者一些知名应用的镜像（如 Nginx、Redis、Node.js 等）进一步学习。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:7:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"05｜镜像仓库：该怎样用好Docker Hub这个宝藏 上一次课里我们学习了“Dockerfile”和“docker build”的用法，知道了如何创建自己的镜像。那么镜像文件应该如何管理呢，具体来说，应该如何存储、检索、分发、共享镜像呢？不解决这些问题，我们的容器化应用还是无法顺利地实施。今天，我就来谈一下这个话题，聊聊什么是镜像仓库，还有该怎么用好镜像仓库。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:8:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"什么是镜像仓库（Registry） 之前我们已经用过 docker pull 命令拉取镜像，也说过有一个“镜像仓库”（Registry）的概念，那到底什么是镜像仓库呢？还是来看 Docker 的官方架构图（它真的非常重要）： 图里右边的区域就是镜像仓库，术语叫 Registry，直译就是“注册中心”，意思是所有镜像的 Repository 都在这里登记保管，就像是一个巨大的档案馆。然后我们再来看左边的“docker pull”，虚线显示了它的工作流程，先到“Docker daemon”，再到 Registry，只有当 Registry 里存有镜像才能真正把它下载到本地。当然了，拉取镜像只是镜像仓库最基本的一个功能，它还会提供更多的功能，比如上传、查询、删除等等，是一个全面的镜像管理服务站点。你也可以把镜像仓库类比成手机上的应用商店，里面分门别类存放了许多容器化的应用，需要什么去找一下就行了。有了它，我们使用镜像才能够免除后顾之忧。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:8:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"什么是 Docker Hub 不过，你有没有注意到，在使用 docker pull 获取镜像的时候，我们并没有明确地指定镜像仓库。在这种情况下，Docker 就会使用一个默认的镜像仓库，也就是大名鼎鼎的“Docker Hub”（https://hub.docker.com/）。Docker Hub 是 Docker 公司搭建的官方 Registry 服务，创立于 2014 年 6 月，和 Docker 1.0 同时发布。它号称是世界上最大的镜像仓库，和 GitHub 一样，几乎成为了容器世界的基础设施。Docker Hub 里面不仅有 Docker 自己打包的镜像，而且还对公众免费开放，任何人都可以上传自己的作品。经过这 8 年的发展，Docker Hub 已经不再是一个单纯的镜像仓库了，更应该说是一个丰富而繁荣的容器社区。你可以看看下面的这张截图，里面列出的都是下载量超过 10 亿次（1 Billion）的最受欢迎的应用程序，比如 Nginx、MongoDB、Node.js、Redis、OpenJDK 等等。显然，把这些容器化的应用引入到我们自己的系统里，就像是站在了巨人的肩膀上，一开始就会有一个高水平的起点。 但和 GitHub、App Store 一样，面向所有人公开的 Docker Hub 也有一个不可避免的缺点，就是“良莠不齐”。在 Docker Hub 搜索框里输入关键字，比如 Nginx、MySQL，它立即就会给出几百几千个搜索结果，有点“乱花迷人眼”的感觉，这么多镜像，应该如何挑选出最适合自己的呢？下面我就来说说自己在这方面的一些经验。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:8:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何在 Docker Hub 上挑选镜像 首先，你应该知道，在 Docker Hub 上有官方镜像、认证镜像和非官方镜像的区别。官方镜像是指 Docker 公司官方提供的高质量镜像（https://github.com/docker-library/official-images），都经过了严格的漏洞扫描和安全检测，支持 x86_64、arm64 等多种硬件架构，还具有清晰易读的文档，一般来说是我们构建镜像的首选，也是我们编写 Dockerfile 的最佳范例。官方镜像目前有大约 100 多个，基本上囊括了现在的各种流行技术，下面就是官方的 Nginx 镜像网页截图： 你会看到，官方镜像会有一个特殊的“Official image”的标记，这就表示这个镜像经过了 Docker 公司的认证，有专门的团队负责审核、发布和更新，质量上绝对可以放心。第二类是认证镜像，标记是“Verified publisher”，也就是认证发行商，比如 Bitnami、Rancher、Ubuntu 等。它们都是颇具规模的大公司，具有不逊于 Docker 公司的实力，所以就在 Docker Hub 上开了个认证账号，发布自己打包的镜像，有点类似我们微博上的“大 V”。 这些镜像有公司背书，当然也很值得信赖，不过它们难免会带上一些各自公司的“烙印”，比如 Bitnami 的镜像就统一以“minideb”为基础，灵活性上比 Docker 官方镜像略差，有的时候也许会不符合我们的需求。除了官方镜像和认证镜像，剩下的就都属于非官方镜像了，不过这里面也可以分出两类。第一类是“半官方”镜像。因为成为“Verified publisher”是要给 Docker 公司交钱的，而很多公司不想花这笔“冤枉钱”，所以只在 Docker Hub 上开了公司账号，但并不加入认证。这里我以 OpenResty 为例，看一下它的 Docker Hub 页面，可以看到显示的是 OpenResty 官方发布，但并没有经过 Docker 正式认证，所以难免就会存在一些风险，有被“冒名顶替”的可能，需要我们在使用的时候留心鉴别一下。不过一般来说，这种“半官方”镜像也是比较可靠的。 第二类就是纯粹的“民间”镜像了，通常是个人上传到 Docker Hub 的，因为条件所限，测试不完全甚至没有测试，质量上难以得到保证，下载的时候需要小心谨慎。除了查看镜像是否为官方认证，我们还应该再结合其他的条件来判断镜像质量是否足够好。做法和 GitHub 差不多，就是看它的下载量、星数、还有更新历史，简单来说就是“好评”数量。一般来说下载量是最重要的参考依据，好的镜像下载量通常都在百万级别（超过 1M），而有的镜像虽然也是官方认证，但缺乏维护，更新不及时，用的人很少，星数、下载数都寥寥无几，那么还是应该选择下载量最多的镜像，通俗来说就是“随大流”。下面的这张截图就是 OpenResty 在 Docker Hub 上的搜索结果。可以看到，有两个认证发行商的镜像（Bitnami、IBM），但下载量都很少，还有一个“民间”镜像下载量虽然超过了 1M，但更新时间是 3 年前，所以毫无疑问，我们应该选择排在第三位，但下载量超过 10M、有 360 多个星的“半官方”镜像。 看了这么多 Docker Hub 上的镜像，你一定注意到了，应用都是一样的名字，比如都是 Nginx、Redis、OpenResty，该怎么区分不同作者打包出的镜像呢？如果你熟悉 GitHub，就会发现 Docker Hub 也使用了同样的规则，就是“用户名 / 应用名”的形式，比如 bitnami/nginx、ubuntu/nginx、rancher/nginx 等等。所以，我们在使用 docker pull 下载这些非官方镜像的时候，就必须把用户名也带上，否则默认就会使用官方镜像： docker pull bitnami/nginx docker pull ubuntu/nginx ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:8:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Docker Hub 上镜像命名的规则是什么 确定了要使用的镜像还不够，因为镜像还会有许多不同的版本，也就是“标签”（tag）。直接使用默认的“latest”虽然简单方便，但在生产环境里是一种非常不负责任的做法，会导致版本不可控。所以我们还需要理解 Docker Hub 上标签命名的含义，才能够挑选出最适合我们自己的镜像版本。下面我就拿官方的 Redis 镜像作为例子，解释一下这些标签都是什么意思。 通常来说，镜像标签的格式是应用的版本号加上操作系统。版本号你应该比较了解吧，基本上都是主版本号 + 次版本号 + 补丁号的形式，有的还会在正式发布前出 rc 版（候选版本，release candidate）。而操作系统的情况略微复杂一些，因为各个 Linux 发行版的命名方式“花样”太多了。Alpine、CentOS 的命名比较简单明了，就是数字的版本号，像这里的 alpine3.15 ，而 Ubuntu、Debian 则采用了代号的形式。比如 Ubuntu 18.04 是 bionic，Ubuntu 20.04 是 focal，Debian 9 是 stretch，Debian 10 是 buster，Debian 11 是 bullseye。另外，有的标签还会加上 slim、fat，来进一步表示这个镜像的内容是经过精简的，还是包含了较多的辅助工具。通常 slim 镜像会比较小，运行效率高，而 fat 镜像会比较大，适合用来开发调试。 下面我就列出几个标签的例子来说明一下。nginx:1.21.6-alpine，表示版本号是 1.21.6，基础镜像是最新的 Alpine。redis:7.0-rc-bullseye，表示版本号是 7.0 候选版，基础镜像是 Debian 11。node:17-buster-slim，表示版本号是 17，基础镜像是精简的 Debian 10。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:8:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"该怎么上传自己的镜像 现在，我想你应该对如何在 Docker Hub 上选择镜像有了比较全面的了解，那么接下来的问题就是，我们自己用 Dockerfile 创建的镜像该如何上传到 Docker Hub 上呢？这件事其实一点也不难，只需要 4 个步骤就能完成。第一步，你需要在 Docker Hub 上注册一个用户，这个就不必再多说了。第二步，你需要在本机上使用 docker login 命令，用刚才注册的用户名和密码认证身份登录，像这里就用了我的用户名“chronolaw”： 第三步很关键，需要使用 docker tag 命令，给镜像改成带用户名的完整名字，表示镜像是属于这个用户的。或者简单一点，直接用 docker build -t 在创建镜像的时候就起好名字。这里我就用上次课里的镜像“ngx-app”作为例子，给它改名成 chronolaw/ngx-app:1.0： docker tag ngx-app chronolaw/ngx-app:1.0 第四步，用 docker push 把这个镜像推上去，我们的镜像发布工作就大功告成了： docker push chronolaw/ngx-app:1.0 你还可以登录 Docker Hub 网站验证一下镜像发布的效果，可以看到它会自动为我们生成一个页面模板，里面还可以进一步丰富完善，比如添加描述信息、使用说明等等： 现在你就可以把这个镜像的名字（用户名 / 应用名: 标签）告诉你的同事，让他去用 docker pull 下载部署了。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:8:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"离线环境该怎么办 使用 Docker Hub 来管理镜像的确是非常方便，不过有一种场景下它却是无法发挥作用，那就是企业内网的离线环境，连不上外网，自然也就不能使用 docker push、docker pull 来推送拉取镜像了。那这种情况有没有解决办法呢？方法当然有，而且有很多。最佳的方法就是在内网环境里仿造 Docker Hub，创建一个自己的私有 Registry 服务，由它来管理我们的镜像，就像我们自己搭建 GitLab 做版本管理一样。自建 Registry 已经有很多成熟的解决方案，比如 Docker Registry，还有 CNCF Harbor，不过使用它们还需要一些目前没有讲到的知识，步骤也有点繁琐，所以我会在后续的课程里再介绍。下面我讲讲存储、分发镜像的一种“笨”办法，虽然比较“原始”，但简单易行，可以作为临时的应急手段。Docker 提供了 save 和 load 这两个镜像归档命令，可以把镜像导出成压缩包，或者从压缩包导入 Docker，而压缩包是非常容易保管和传输的，可以联机拷贝，FTP 共享，甚至存在 U 盘上随身携带。 需要注意的是，这两个命令默认使用标准流作为输入输出（为了方便 Linux 管道操作），所以一般会用 -o、-i 参数来使用文件的形式，例如： docker save ngx-app:latest -o ngx.tar docker load -i ngx.tar ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:8:6","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们一起学习了镜像仓库，了解了 Docker Hub 的使用方法，整理一下要点方便你加深理解：镜像仓库（Registry）是一个提供综合镜像服务的网站，最基本的功能是上传和下载。Docker Hub 是目前最大的镜像仓库，拥有许多高质量的镜像。上面的镜像非常多，选择的标准有官方认证、下载量、星数等，需要综合评估。镜像也有很多版本，应该根据版本号和操作系统仔细确认合适的标签。在 Docker Hub 注册之后就可以上传自己的镜像，用 docker tag 打上标签再用 docker push 推送。离线环境可以自己搭建私有镜像仓库，或者使用 docker save 把镜像存成压缩包，再用 docker load 从压缩包恢复成镜像。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:8:7","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"06｜打破次元壁：容器该如何与外界互联互通 在前面的几节课里，我们已经学习了容器、镜像、镜像仓库的概念和用法，也知道了应该如何创建镜像，再以容器的形式启动应用。不过，用容器来运行“busybox”“hello world”这样比较简单的应用还好，如果是 Nginx、Redis、MySQL 这样的后台服务应用，因为它们运行在容器的“沙盒”里，完全与外界隔离，无法对外提供服务，也就失去了价值。这个时候，容器的隔离环境反而成为了一种负面特性。所以，容器的这个“小板房”不应该是一个完全密闭的铁屋子，而是应该给它开几扇门窗，让应用在“足不出户”的情况下，也能够与外界交换数据、互通有无，这样“有限的隔离”才是我们真正所需要的运行环境。那么今天，我就以 Docker 为例，来讲讲有哪些手段能够在容器与外部系统之间沟通交流。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:9:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何拷贝容器内的数据 我们首先来看看 Docker 提供的 cp 命令，它可以在宿主机和容器之间拷贝文件，是最基本的一种数据交换功能。试验这个命令需要先用 docker run 启动一个容器，就用 Redis 吧： docker run -d --rm redis 注意这里使用了 -d、–rm 两个参数，表示运行在后台，容器结束后自动删除，然后使用 docker ps 命令可以看到 Redis 容器正在运行，容器 ID 是“062”。docker cp 的用法很简单，很类似 Linux 的“cp”“scp”，指定源路径（src path）和目标路径（dest path）就可以了。如果源路径是宿主机那么就是把文件拷贝进容器，如果源路径是容器那么就是把文件拷贝出容器，注意需要用容器名或者容器 ID 来指明是哪个容器的路径。假设当前目录下有一个“a.txt”的文件，现在我们要把它拷贝进 Redis 容器的“/tmp”目录，如果使用容器 ID，命令就会是这样： docker cp a.txt 062:/tmp 接下来我们可以使用 docker exec 命令，进入容器看看文件是否已经正确拷贝了： docker exec -it 062 sh 可以看到，在“/tmp”目录下，确实已经有了一个“a.txt”。现在让我们再来试验一下从容器拷贝出文件，只需要把 docker cp 后面的两个路径调换一下位置： docker cp 062:/tmp/a.txt ./b.txt 这样，在宿主机的当前目录里，就会多出一个新的“b.txt”，也就是从容器里拿到的文件。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:9:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何共享主机上的文件 docker cp 的用法模仿了操作系统的拷贝命令，偶尔一两次的文件共享还可以应付，如果容器运行时经常有文件来往互通，这样反复地拷来拷去就显得很麻烦，也很容易出错。你也许会联想到虚拟机有一种“共享目录”的功能。它可以在宿主机上开一个目录，然后把这个目录“挂载”进虚拟机，这样就实现了两者共享同一个目录，一边对目录里文件的操作另一边立刻就能看到，没有了数据拷贝，效率自然也会高很多。沿用这个思路，容器也提供了这样的共享宿主机目录的功能，效果也和虚拟机几乎一样，用起来很方便，只需要在 docker run 命令启动容器的时候使用 -v 参数就行，具体的格式是“宿主机路径: 容器内路径”。 我还是以 Redis 为例，启动容器，使用 -v 参数把本机的“/tmp”目录挂载到容器里的“/tmp”目录，也就是说让容器共享宿主机的“/tmp”目录： docker run -d --rm -v /tmp:/tmp redis 然后我们再用 docker exec 进入容器，查看一下容器内的“/tmp”目录，应该就可以看到文件与宿主机是完全一致的。 docker exec -it b5a sh # b5a是容器ID 你也可以在容器里的“/tmp”目录下随便做一些操作，比如删除文件、建立新目录等等，再回头观察一下宿主机，会发现修改会即时同步，这就表明容器和宿主机确实已经共享了这个目录。-v 参数挂载宿主机目录的这个功能，对于我们日常开发测试工作来说非常有用，我们可以在不变动本机环境的前提下，使用镜像安装任意的应用，然后直接以容器来运行我们本地的源码、脚本，非常方便。这里我举一个简单的例子。比如我本机上只有 Python 2.7，但我想用 Python 3 开发，如果同时安装 Python 2 和 Python 3 很容易就会把系统搞乱，所以我就可以这么做： 先使用 docker pull 拉取一个 Python 3 的镜像，因为它打包了完整的运行环境，运行时有隔离，所以不会对现有系统的 Python 2.7 产生任何影响。在本地的某个目录编写 Python 代码，然后用 -v 参数让容器共享这个目录。现在就可以在容器里以 Python 3 来安装各种包，再运行脚本做开发了。 dockerpullpython:alpinedockerrun-it--rm-v`pwd`:/tmppython:alpinesh 显然，这种方式比把文件打包到镜像或者 docker cp 会更加灵活，非常适合有频繁修改的开发测试工作。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:9:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何实现网络互通 现在我们使用 docker cp 和 docker run -v 可以解决容器与外界的文件互通问题，但对于 Nginx、Redis 这些服务器来说，网络互通才是更要紧的问题。网络互通的关键在于“打通”容器内外的网络，而处理网络通信无疑是计算机系统里最棘手的工作之一，有许许多多的名词、协议、工具，在这里我也没有办法一下子就把它都完全说清楚，所以只能从“宏观”层面讲个大概，帮助你快速理解。 Docker 提供了三种网络模式，分别是 null、host 和 bridge。null 是最简单的模式，也就是没有网络，但允许其他的网络插件来自定义网络连接，这里就不多做介绍了。host 的意思是直接使用宿主机网络，相当于去掉了容器的网络隔离（其他隔离依然保留），所有的容器会共享宿主机的 IP 地址和网卡。这种模式没有中间层，自然通信效率高，但缺少了隔离，运行太多的容器也容易导致端口冲突。host 模式需要在 docker run 时使用 –net=host 参数，下面我就用这个参数启动 Nginx： docker run -d --rm --net=host nginx:alpine 为了验证效果，我们可以在本机和容器里分别执行 ip addr 命令，查看网卡信息： ip addr # 本机查看网卡 docker exec xxx ip addr # 容器查看网卡 可以看到这两个 ip addr 命令的输出信息是完全一样的，比如都是一个网卡 ens160，IP 地址是“192.168.10.208”，这就证明 Nginx 容器确实与本机共享了网络栈。第三种 bridge，也就是桥接模式，它有点类似现实世界里的交换机、路由器，只不过是由软件虚拟出来的，容器和宿主机再通过虚拟网卡接入这个网桥（图中的 docker0），那么它们之间也就可以正常的收发网络数据包了。不过和 host 模式相比，bridge 模式多了虚拟网桥和网卡，通信效率会低一些。 和 host 模式一样，我们也可以用 –net=bridge 来启用桥接模式，但其实并没有这个必要，因为 Docker 默认的网络模式就是 bridge，所以一般不需要显式指定。下面我们启动两个容器 Nginx 和 Redis，就像刚才说的，没有特殊指定就会使用 bridge 模式： docker run -d --rm nginx:alpine # 默认使用桥接模式 docker run -d --rm redis # 默认使用桥接模式 然后我们还是在本机和容器里执行 ip addr 命令（Redis 容器里没有 ip 命令，所以只能在 Nginx 容器里执行） 对比一下刚才 host 模式的输出，就可以发现容器里的网卡设置与宿主机完全不同，eth0 是一个虚拟网卡，IP 地址是 B 类私有地址“172.17.0.2”。我们还可以用 docker inspect 直接查看容器的 ip 地址： docker inspect xxx |grep IPAddress 这显示出两个容器的 IP 地址分别是“172.17.0.2”和“172.17.0.3”，而宿主机的 IP 地址则是“172.17.0.1”，所以它们都在“172.17.0.0/16”这个 Docker 的默认网段，彼此之间就能够使用 IP 地址来实现网络通信了。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:9:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何分配服务端口号 使用 host 模式或者 bridge 模式，我们的容器就有了 IP 地址，建立了与外部世界的网络连接，接下来要解决的就是网络服务的端口号问题。你一定知道，服务器应用都必须要有端口号才能对外提供服务，比如 HTTP 协议用 80、HTTPS 用 443、Redis 是 6379、MySQL 是 3306。第 4 讲我们在学习编写 Dockerfile 的时候也看到过，可以用 EXPOSE 指令声明容器对外的端口号。一台主机上的端口号数量是有限的，而且多个服务之间还不能够冲突，但我们打包镜像应用的时候通常都使用的是默认端口，容器实际运行起来就很容易因为端口号被占用而无法启动。 解决这个问题的方法就是加入一个“中间层”，由容器环境例如 Docker 来统一管理分配端口号，在本机端口和容器端口之间做一个“映射”操作，容器内部还是用自己的端口号，但外界看到的却是另外一个端口号，这样就很好地避免了冲突。端口号映射需要使用 bridge 模式，并且在 docker run 启动容器时使用 -p 参数，形式和共享目录的 -v 参数很类似，用 : 分隔本机端口和容器端口。比如，如果要启动两个 Nginx 容器，分别跑在 80 和 8080 端口上： docker run -d -p 80:80 --rm nginx:alpine docker run -d -p 8080:80 --rm nginx:alpine 这样就把本机的 80 和 8080 端口分别“映射”到了两个容器里的 80 端口，不会发生冲突，我们可以用 curl 再验证一下： 使用 docker ps 命令能够在“PORTS”栏里更直观地看到端口的映射情况： ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:9:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们一起学习了容器与外部系统之间沟通交流的几种方法。你会发现，这些方法几乎消除了容器化的应用和本地应用因为隔离特性而产生的差异，而因为镜像独特的打包机制，容器技术显然能够比 apt/yum 更方便地安装各种应用，绝不会“污染”已有的系统。今天的课里我列举了 Python、Nginx 等例子，你还可以举一反三，借鉴它们把本地配置文件加载到容器里适当的位置，再映射端口号，把 Redis、MySQL、Node.js 都运行起来，让容器成为我们工作中的得力助手。照例简单小结一下这次的要点： docker cp 命令可以在容器和主机之间互相拷贝文件，适合简单的数据交换。docker run -v 命令可以让容器和主机共享本地目录，免去了拷贝操作，提升工作效率。host 网络模式让容器与主机共享网络栈，效率高但容易导致端口冲突。bridge 网络模式实现了一个虚拟网桥，容器和主机都在一个私有网段内互联互通。docker run -p 命令可以把主机的端口号映射到容器的内部端口号，解决了潜在的端口冲突问题。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:9:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"07｜实战演练：玩转Docker 要提醒你的是，Docker 相关的内容很多很广，在入门篇中，我只从中挑选出了一些最基本最有用的介绍给你。而且在我看来，我们不需要完全了解 Docker 的所有功能，我也不建议你对 Docker 的内部架构细节和具体的命令行参数做过多的了解，太浪费精力，只要会用够用，需要的时候能够查找官方手册就行。毕竟我们这门课程的目标是 Kubernetes，而 Docker 只不过是众多容器运行时（Container Runtime）中最出名的一款而已。当然，如果你当前的工作是与 Docker 深度绑定，那就另当别论了。好下面我先把容器技术做一个简要的总结，然后演示两个实战项目：使用 Docker 部署 Registry 和 WordPress。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:10:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"容器技术要点回顾 容器技术是后端应用领域的一项重大创新，它彻底变革了应用的开发、交付与部署方式，是“云原生”的根本（01 讲）。容器基于 Linux 底层的 namespace、cgroup、chroot 等功能，虽然它们很早就出现了，但直到 Docker“横空出世”，把它们整合在一起，容器才真正走近了大众的视野，逐渐为广大开发者所熟知（02 讲）。容器技术中有三个核心概念：容器（Container）、镜像（Image），以及镜像仓库（Registry）（03 讲）。 从本质上来说，容器属于虚拟化技术的一种，和虚拟机（Virtual Machine）很类似，都能够分拆系统资源，隔离应用进程，但容器更加轻量级，运行效率更高，比虚拟机更适合云计算的需求。镜像是容器的静态形式，它把应用程序连同依赖的操作系统、配置文件、环境变量等等都打包到了一起，因而能够在任何系统上运行，免除了很多部署运维和平台迁移的麻烦。镜像内部由多个层（Layer）组成，每一层都是一组文件，多个层会使用 Union FS 技术合并成一个文件系统供容器使用。这种细粒度结构的好处是相同的层可以共享、复用，节约磁盘存储和网络传输的成本，也让构建镜像的工作变得更加容易（04 讲）。为了方便管理镜像，就出现了镜像仓库，它集中存放各种容器化的应用，用户可以任意上传下载，是分发镜像的最佳方式（05 讲）。目前最知名的公开镜像仓库是 Docker Hub，其他的还有 quay.io、gcr.io，我们可以在这些网站上找到许多高质量镜像，集成到我们自己的应用系统中。容器技术有很多具体的实现，Docker 是最初也是最流行的容器技术，它的主要形态是运行在 Linux 上的“Docker Engine”。我们日常使用的 docker 命令其实只是一个前端工具，它必须与后台服务“Docker daemon”通信才能实现各种功能。操作容器的常用命令有 docker ps、docker run、docker exec、docker stop 等；操作镜像的常用命令有 docker images、docker rmi、docker build、docker tag 等；操作镜像仓库的常用命令有 docker pull、docker push 等。好简单地回顾了容器技术，下面我们就来综合运用在“入门篇”所学到的各个知识点，开始实战演练，玩转 Docker。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:10:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"搭建私有镜像仓库 在第 5 节课讲 Docker Hub 的时候曾经说过，在离线环境里，我们可以自己搭建私有仓库。但因为镜像仓库是网络服务的形式，当时还没有学到容器网络相关的知识，所以只有到了现在，我们具备了比较完整的 Docker 知识体系，才能够搭建私有仓库。私有镜像仓库有很多现成的解决方案，今天我只选择最简单的 Docker Registry，而功能更完善的 CNCF Harbor 留到后续学习 Kubernetes 时再介绍。你可以在 Docker Hub 网站上搜索“registry”，找到它的官方页面（https://registry.hub.docker.com/_/registry/）： Docker Registry 的网页上有很详细的说明，包括下载命令、用法等，我们可以完全照着它来操作。首先，你需要使用 docker pull 命令拉取镜像： docker pull registry 然后，我们需要做一个端口映射，对外暴露端口，这样 Docker Registry 才能提供服务。它的容器内端口是 5000，简单起见，我们在外面也使用同样的 5000 端口，所以运行命令就是 docker run -d -p 5000:5000 registry ： docker run -d -p 5000:5000 registry 启动 Docker Registry 之后，你可以使用 docker ps 查看它的运行状态，可以看到它确实把本机的 5000 端口映射到了容器内的 5000 端口。 接下来，我们就要使用 docker tag 命令给镜像打标签再上传了。因为上传的目标不是默认的 Docker Hub，而是本地的私有仓库，所以镜像的名字前面还必须再加上仓库的地址（域名或者 IP 地址都行），形式上和 HTTP 的 URL 非常像。比如在这里，我就把“nginx:alpine”改成了“127.0.0.1:5000/nginx:alpine”： docker tag nginx:alpine 127.0.0.1:5000/nginx:alpine 现在，这个镜像有了一个附加仓库地址的完整名字，就可以用 docker push 推上去了： docker push 127.0.0.1:5000/nginx:alpine 为了验证是否已经成功推送，我们可以把刚才打标签的镜像删掉，再重新下载： docker rmi 127.0.0.1:5000/nginx:alpine docker pull 127.0.0.1:5000/nginx:alpine 这里 docker pull 确实完成了镜像下载任务，不过因为原来的层原本就已经存在，所以不会有实际的下载动作，只会创建一个新的镜像标签。Docker Registry 虽然没有图形界面，但提供了 RESTful API，也可以发送 HTTP 请求来查看仓库里的镜像，具体的端点信息可以参考官方文档（https://docs.docker.com/registry/spec/api/），下面的这两条 curl 命令就分别获取了镜像列表和 Nginx 镜像的标签列表： curl 127.1:5000/v2/_catalog curl 127.1:5000/v2/nginx/tags/list 可以看到，因为应用被封装到了镜像里，所以我们只用简单的一两条命令就完成了私有仓库的搭建工作，完全不需要复杂的软件安装、环境设置、调试测试等繁琐的操作，这在容器技术出现之前简直是不可想象的。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:10:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"搭建 WordPress 网站 Docker Registry 应用比较简单，只用单个容器就运行了一个完整的服务，下面我们再来搭建一个有点复杂的 WordPress 网站。网站需要用到三个容器：WordPress、MariaDB、Nginx，它们都是非常流行的开源项目，在 Docker Hub 网站上有官方镜像，网页上的说明也很详细，所以具体的搜索过程我就略过了，直接使用 docker pull 拉取它们的镜像： docker pull wordpress:5 docker pull mariadb:10 docker pull nginx:alpine 我画了一个简单的网络架构图，你可以直观感受一下它们之间的关系： 这个系统可以说是比较典型的网站了。MariaDB 作为后面的关系型数据库，端口号是 3306；WordPress 是中间的应用服务器，使用 MariaDB 来存储数据，它的端口是 80；Nginx 是前面的反向代理，它对外暴露 80 端口，然后把请求转发给 WordPress。我们先来运行 MariaDB。根据说明文档，需要配置“MARIADB_DATABASE”等几个环境变量，用 –env 参数来指定启动时的数据库、用户名和密码，这里我指定数据库是“db”，用户名是“wp”，密码是“123”，管理员密码（root password）也是“123”。下面就是启动 MariaDB 的 docker run 命令： docker run -d --rm \\ --env MARIADB_DATABASE=db \\ --env MARIADB_USER=wp \\ --env MARIADB_PASSWORD=123 \\ --env MARIADB_ROOT_PASSWORD=123 \\ mariadb:10 启动之后，我们还可以使用 docker exec 命令，执行数据库的客户端工具“mysql”，验证数据库是否正常运行： docker exec -it 9ac mysql -u wp -p 输入刚才设定的用户名“wp”和密码“123”之后，我们就连接上了 MariaDB，可以使用 show databases; 和 show tables; 等命令来查看数据库里的内容。当然，现在肯定是空的。 因为 Docker 的 bridge 网络模式的默认网段是“172.17.0.0/16”，宿主机固定是“172.17.0.1”，而且 IP 地址是顺序分配的，所以如果之前没有其他容器在运行的话，MariaDB 容器的 IP 地址应该就是“172.17.0.2”，这可以通过 docker inspect 命令来验证： docker inspect 9ac |grep IPAddress 现在数据库服务已经正常，该运行应用服务器 WordPress 了，它也要用 –env 参数来指定一些环境变量才能连接到 MariaDB，注意“WORDPRESS_DB_HOST”必须是 MariaDB 的 IP 地址，否则会无法连接数据库： docker run -d --rm \\ --env WORDPRESS_DB_HOST=172.17.0.2 \\ --env WORDPRESS_DB_USER=wp \\ --env WORDPRESS_DB_PASSWORD=123 \\ --env WORDPRESS_DB_NAME=db \\ wordpress:5 WordPress 容器在启动的时候并没有使用 -p 参数映射端口号，所以外界是不能直接访问的，我们需要在前面配一个 Nginx 反向代理，把请求转发给 WordPress 的 80 端口。配置 Nginx 反向代理必须要知道 WordPress 的 IP 地址，同样可以用 docker inspect 命令查看，如果没有什么意外的话它应该是“172.17.0.3”，所以我们就能够写出如下的配置文件（Nginx 的用法可参考其他资料，这里就不展开讲了）： server { listen 80; default_type text/html; location / { proxy_http_version 1.1; proxy_set_header Host $host; proxy_pass http://172.17.0.3; } } 有了这个配置文件，最关键的一步就来了，我们需要用 -p 参数把本机的端口映射到 Nginx 容器内部的 80 端口，再用 -v 参数把配置文件挂载到 Nginx 的“conf.d”目录下。这样，Nginx 就会使用刚才编写好的配置文件，在 80 端口上监听 HTTP 请求，再转发到 WordPress 应用： dockerrun-d--rm\\-p80:80\\-v`pwd`/wp.conf:/etc/nginx/conf.d/default.conf\\nginx:alpine 三个容器都启动之后，我们再用 docker ps 来看看它们的状态： 可以看到，WordPress 和 MariaDB 虽然使用了 80 和 3306 端口，但被容器隔离，外界不可见，只有 Nginx 有端口映射，能够从外界的 80 端口收发数据，网络状态和我们的架构图是一致的。现在整个系统就已经在容器环境里运行好了，我们来打开浏览器，输入本机的“127.0.0.1”或者是虚拟机的 IP 地址（我这里是“http://192.168.10.208”），就可以看到 WordPress 的界面： 在创建基本的用户、初始化网站之后，我们可以再登录 MariaDB，看看是否已经有了一些数据： 可以看到，WordPress 已经在数据库里新建了很多的表，这就证明我们的容器化的 WordPress 网站搭建成功。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:10:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们简单地回顾了一下容器技术，这里有一份思维导图，是对前面所有容器知识要点的总结，你可以对照着用来复习。 我们还使用 Docker 实际搭建了两个服务：Registry 镜像仓库和 WordPress 网站。通过这两个项目的实战演练，你应该能够感受到容器化对后端开发带来的巨大改变，它简化了应用的打包、分发和部署，简单的几条命令就可以完成之前需要编写大量脚本才能完成的任务，对于开发、运维来绝对是一个“福音”。不过，在感受容器便利的同时，你有没有注意到它还是存在一些遗憾呢？比如说： 我们还是要手动运行一些命令来启动应用，然后再人工确认运行状态。运行多个容器组成的应用比较麻烦，需要人工干预（如检查 IP 地址）才能维护网络通信。现有的网络模式功能只适合单机，多台服务器上运行应用、负载均衡该怎么做？如果要增加应用数量该怎么办？这时容器技术完全帮不上忙。 其实，如果我们仔细整理这些运行容器的 docker run 命令，写成脚本，再加上一些 Shell、Python 编程来实现自动化，也许就能够得到一个勉强可用的解决方案。这个方案已经超越了容器技术本身，是在更高的层次上规划容器的运行次序、网络连接、数据持久化等应用要素，也就是现在我们常说的“容器编排”（Container Orchestration）的雏形，也正是后面要学习的 Kubernetes 的主要出发点。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:10:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"初级 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:11:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"09｜走近云原生：如何在本机搭建小巧完备的Kubernetes环境 在前面的“入门篇”里，我们学习了以 Docker 为代表的容器技术，做好了充分的准备，那么今天我们就来看看什么是容器编排、什么是 Kubernetes，还有应该怎么在自己的电脑上搭建出一个小巧完善的 Kubernetes 环境，一起走近云原生。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:12:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"什么是容器编排 容器技术的核心概念是容器、镜像、仓库，使用这三大基本要素我们就可以轻松地完成应用的打包、分发工作，实现“一次开发，到处运行”的梦想。不过，当我们熟练地掌握了容器技术，信心满满地要在服务器集群里大规模实施的时候，却会发现容器技术的创新只是解决了运维部署工作中一个很小的问题。现实生产环境的复杂程度实在是太高了，除了最基本的安装，还会有各式各样的需求，比如服务发现、负载均衡、状态监控、健康检查、扩容缩容、应用迁移、高可用等等。 虽然容器技术开启了云原生时代，但它也只走出了一小步，再继续前进就无能为力了，因为这已经不再是隔离一两个进程的普通问题，而是要隔离数不清的进程，还有它们之间互相通信、互相协作的超级问题，困难程度可以说是指数级别的上升。这些容器之上的管理、调度工作，就是这些年最流行的词汇：“容器编排”（Container Orchestration）。 容器编排这个词听起来好像挺高大上，但如果你理解了之后就会发现其实也并不神秘。像我们在上次课里使用 Docker 部署 WordPress 网站的时候，把 Nginx、WordPress、MariaDB 这三个容器理清次序、配好 IP 地址去运行，就是最初级的一种“容器编排”，只不过这是纯手工操作，比较原始、粗糙。面对单机上的几个容器，“人肉”编排调度还可以应付，但如果规模上到几百台服务器、成千上万的容器，处理它们之间的复杂联系就必须要依靠计算机了，而目前计算机用来调度管理的“事实标准”，就是我们专栏的主角：Kubernetes。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:12:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"什么是 Kubernetes 现在大家谈到容器都会说是 Docker，但其实早在 Docker 之前，Google 在公司内部就使用了类似的技术（cgroup 就是 Google 开发再提交给 Linux 内核的），只不过不叫容器。作为世界上最大的搜索引擎，Google 拥有数量庞大的服务器集群，为了提高资源利用率和部署运维效率，它专门开发了一个集群应用管理系统，代号 Borg，在底层支持整个公司的运转。 2014 年，Google 内部系统要“升级换代”，从原来的 Borg 切换到 Omega，于是按照惯例，Google 会发表公开论文。因为之前在发表 MapReduce、BigTable、GFS 时吃过亏（被 Yahoo 开发的 Hadoop 占领了市场），所以 Google 决定借着 Docker 的“东风”，在发论文的同时，把 C++ 开发的 Borg 系统用 Go 语言重写并开源，于是 Kubernetes 就这样诞生了。 由于 Kubernetes 背后有 Borg 系统十多年生产环境经验的支持，技术底蕴深厚，理论水平也非常高，一经推出就引起了轰动。然后在 2015 年，Google 又联合 Linux 基金会成立了 CNCF（Cloud Native Computing Foundation，云原生基金会），并把 Kubernetes 捐献出来作为种子项目。有了 Google 和 Linux 这两大家族的保驾护航，再加上宽容开放的社区，作为 CNCF 的“头把交椅”，Kubernetes 旗下很快就汇集了众多行业精英，仅用了两年的时间就打败了同期的竞争对手 Apache Mesos 和 Docker Swarm，成为了这个领域的唯一霸主。 那么，Kubernetes 到底能够为我们做什么呢？简单来说，Kubernetes 就是一个生产级别的容器编排平台和集群管理系统，不仅能够创建、调度容器，还能够监控、管理服务器，它凝聚了 Google 等大公司和开源社区的集体智慧，从而让中小型公司也可以具备轻松运维海量计算节点——也就是“云计算”的能力。 什么是 minikube Kubernetes 一般都运行在大规模的计算集群上，管理很严格，这就对我们个人来说造成了一定的障碍，没有实际操作环境怎么能够学好用好呢？好在 Kubernetes 充分考虑到了这方面的需求，提供了一些快速搭建 Kubernetes 环境的工具，在官网（https://kubernetes.io/zh/docs/tasks/tools/）上推荐的有两个：kind 和 minikube，它们都可以在本机上运行完整的 Kubernetes 环境。 我说一下对这两个工具的个人看法，供你参考。kind 基于 Docker，意思是“Kubernetes in Docker”。它功能少，用法简单，也因此运行速度快，容易上手。不过它缺少很多 Kubernetes 的标准功能，例如仪表盘、网络插件，也很难定制化，所以我认为它比较适合有经验的 Kubernetes 用户做快速开发测试，不太适合学习研究。不选 kind 还有一个原因，它的名字与 Kubernetes YAML 配置里的字段 kind 重名，会对初学者造成误解，干扰学习。再来看 minikube，从名字就能够看出来，它是一个“迷你”版本的 Kubernetes，自从 2016 年发布以来一直在积极地开发维护，紧跟 Kubernetes 的版本更新，同时也兼容较旧的版本（最多只到之前的 6 个小版本）。minikube 最大特点就是“小而美”，可执行文件仅有不到 100MB，运行镜像也不过 1GB，但就在这么小的空间里却集成了 Kubernetes 的绝大多数功能特性，不仅有核心的容器编排功能，还有丰富的插件，例如 Dashboard、GPU、Ingress、Istio、Kong、Registry 等等，综合来看非常完善。所以，我建议你在这个专栏里选择 minikube 来学习 Kubernetes。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:12:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何搭建 minikube 环境 minikube 支持 Mac、Windows、Linux 这三种主流平台，你可以在它的官网（https://minikube.sigs.k8s.io）找到详细的安装说明，当然在我们这里就只用虚拟机里的 Linux 了。minikube 的最新版本是 1.25.2，支持的 Kubernetes 版本是 1.23.3，所以我们就选定它作为我们初级篇的学习工具。 minikube 不包含在系统自带的 apt/yum 软件仓库里，我们只能自己去网上找安装包。不过因为它是用 Go 语言开发的，整体就是一个二进制文件，没有多余的依赖，所以安装过程也非常简单，只需要用 curl 或者 wget 下载就行。minikube 的官网提供了各种系统的安装命令，通常就是下载、拷贝这两步，不过你需要注意一下本机电脑的硬件架构，Intel 芯片要选择带“amd64”后缀，Apple M1 芯片要选择“arm64”后缀，选错了就会因为 CPU 指令集不同而无法运行： 我也把官网上 Linux 系统安装的命令抄在了这里，你可以直接拷贝后安装： # Intel x86_64 curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 # Apple arm64 curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-arm64 sudo install minikube /usr/local/bin/ 安装完成之后，你可以执行命令 minikube version，看看它的版本号，验证是否安装成功： minikube version 不过 minikube 只能够搭建 Kubernetes 环境，要操作 Kubernetes，还需要另一个专门的客户端工具“kubectl”。kubectl 的作用有点类似之前我们学习容器技术时候的工具“docker”，它也是一个命令行工具，作用也比较类似，同样是与 Kubernetes 后台服务通信，把我们的命令转发给 Kubernetes，实现容器和集群的管理功能。kubectl 是一个与 Kubernetes、minikube 彼此独立的项目，所以不包含在 minikube 里，但 minikube 提供了安装它的简化方式，你只需执行下面的这条命令： minikube kubectl 它就会把与当前 Kubernetes 版本匹配的 kubectl 下载下来，存放在内部目录（例如 .minikube/cache/linux/arm64/v1.23.3），然后我们就可以使用它来对 Kubernetes“发号施令”了。所以，在 minikube 环境里，我们会用到两个客户端：minikube 管理 Kubernetes 集群环境，kubectl 操作实际的 Kubernetes 功能，和 Docker 比起来有点复杂。我画了一个简单的 minikube 环境示意图，方便你理解它们的关系。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:12:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"实际验证 minikube 环境 前面的工作都做完之后，我们就可以在本机上运行 minikube，创建 Kubernetes 实验环境了。使用命令 minikube start 会从 Docker Hub 上拉取镜像，以当前最新版本的 Kubernetes 启动集群。不过为了保证实验环境的一致性，我们可以在后面再加上一个参数 –kubernetes-version，明确指定要使用 Kubernetes 版本。这里我使用“1.23.3”，启动命令就是： minikube start --kubernetes-version=v1.23.3 （它的启动过程使用了比较活泼的表情符号，可能是想表现得平易近人吧，如果不喜欢也可以调整设置关闭它。）现在 Kubernetes 集群就已经在我们本地运行了，你可以使用 minikube status、minikube node list这两个命令来查看集群的状态： minikube status minikube node list 从截图里可以看到，Kubernetes 集群里现在只有一个节点，名字就叫“minikube”，类型是“Control Plane”，里面有 host、kubelet、apiserver 三个服务，IP 地址是 192.168.49.2。你还可以用命令 minikube ssh 登录到这个节点上，虽然它是虚拟的，但用起来和实机也没什么区别 有了集群，接下来我们就可以使用 kubectl 来操作一下，初步体会 Kubernetes 这个容器编排系统，最简单的命令当然就是查看版本： kubectl version 不过这条命令还不能直接用，因为使用 minikube 自带的 kubectl 有一点形式上的限制，要在前面加上 minikube 的前缀，后面再有个 –，像这样： minikube kubectl -- version 为了避免这个不大不小的麻烦，我建议你使用 Linux 的“alias”功能，为它创建一个别名，写到当前用户目录下的 .bashrc 里，也就是这样： alias kubectl=\"minikube kubectl --\" 另外，kubectl 还提供了命令自动补全的功能，你还应该再加上“kubectl completion”： source \u003c(kubectl completion bash) 现在，我们就可以愉快地使用 kubectl 了 下面我们在 Kubernetes 里运行一个 Nginx 应用，命令与 Docker 一样，也是 run，不过形式上有点区别，需要用 –image 指定镜像，然后 Kubernetes 会自动拉取并运行： kubectl run ngx --image=nginx:alpine 这里涉及 Kubernetes 里的一个非常重要的概念：Pod，你可以暂时把它理解成是“穿了马甲”的容器，查看 Pod 列表需要使用命令 kubectl get pod，它的效果类似 docker ps 命令执行之后可以看到，在 Kubernetes 集群里就有了一个名字叫 ngx 的 Pod 正在运行，表示我们的这个单节点 minikube 环境已经搭建成功。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:12:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们先了解了容器编排概念和 Kubernetes 的历史，然后在 Linux 虚拟机上安装了 minikube 和 kubectl，运行了一个简单但完整的 Kubernetes 集群，实现了与云原生的“第一次亲密接触”。那什么是云原生呢？这在 CNCF 上有明确的定义，不过我觉得太学术化了，我也不想机械重复，就讲讲我自己的通俗理解吧。所谓的“云”，现在就指的是 Kubernetes，那么“云原生”的意思就是应用的开发、部署、运维等一系列工作都要向 Kubernetes 看齐，使用容器、微服务、声明式 API 等技术，保证应用的整个生命周期都能够在 Kubernetes 环境里顺利实施，不需要附加额外的条件。换句话说，“云原生”就是 Kubernetes 里的“原住民”，而不是从其他环境迁过来的“移民”。最后照例小结一下今天的内容： 容器技术只解决了应用的打包、安装问题，面对复杂的生产环境就束手无策了，解决之道就是容器编排，它能够组织管理各个应用容器之间的关系，让它们顺利地协同运行。Kubernetes 源自 Google 内部的 Borg 系统，也是当前容器编排领域的事实标准。minikube 可以在本机搭建 Kubernetes 环境，功能很完善，适合学习研究。操作 Kubernetes 需要使用命令行工具 kubectl，只有通过它才能与 Kubernetes 集群交互。kubectl 的用法与 docker 类似，也可以拉取镜像运行，但操作的不是简单的容器，而是 Pod。 另外还要说一下 Kubernetes 的官网（https://kubernetes.io/zh/），里面有非常详细的文档，包括概念解释、入门教程、参考手册等等，最难得的是它有全中文版本，我们阅读起来完全不会有语言障碍，希望你有时间多上去看看，及时获取官方第一手知识。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:12:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"10｜自动化的运维管理：探究Kubernetes工作机制的奥秘 在上一次课里，我们看到容器技术只实现了应用的打包分发，到运维真正落地实施的时候仍然会遇到很多困难，所以就需要用容器编排技术来解决这些问题，而 Kubernetes 是这个领域的唯一霸主，已经成为了“事实标准”。那么，Kubernetes 凭什么能担当这样的领军重任呢？难道仅仅因为它是由 Google 主导开发的吗？今天我就带你一起来看看 Kubernetes 的内部架构和工作机制，了解它能够傲视群雄的秘密所在。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:13:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"云计算时代的操作系统 前面我曾经说过，Kubernetes 是一个生产级别的容器编排平台和集群管理系统，能够创建、调度容器，监控、管理服务器。容器是什么？容器是软件，是应用，是进程。服务器是什么？服务器是硬件，是 CPU、内存、硬盘、网卡。那么，既可以管理软件，也可以管理硬件，这样的东西应该是什么？你也许会脱口而出：这就是一个操作系统（Operating System）！没错，从某种角度来看，Kubernetes 可以说是一个集群级别的操作系统，主要功能就是资源管理和作业调度。但 Kubernetes 不是运行在单机上管理单台计算资源和进程，而是运行在多台服务器上管理几百几千台的计算资源，以及在这些资源上运行的上万上百万的进程，规模要大得多。 所以，你可以把 Kubernetes 与 Linux 对比起来学习，而这个新的操作系统里自然会有一系列新名词、新术语，你也需要使用新的思维方式来考虑问题，必要的时候还得和过去的习惯“说再见”。Kubernetes 这个操作系统与 Linux 还有一点区别你值得注意。Linux 的用户通常是两类人：Dev 和 Ops，而在 Kubernetes 里则只有一类人：DevOps。在以前的应用实施流程中，开发人员和运维人员分工明确，开发完成后需要编写详细的说明文档，然后交给运维去部署管理，两者之间不能随便“越线”。而在 Kubernetes 这里，开发和运维的界限变得不那么清晰了。由于云原生的兴起，开发人员从一开始就必须考虑后续的部署运维工作，而运维人员也需要在早期介入开发，才能做好应用的运维监控工作。这就会导致很多 Kubernetes 的新用户会面临身份的转变，一开始可能会有点困难。不过不用担心，这也非常正常，任何的学习过程都有个适应期，只要过了最初的概念理解阶段就好了。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:13:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Kubernetes 的基本架构 操作系统的一个重要功能就是抽象，从繁琐的底层事务中抽象出一些简洁的概念，然后基于这些概念去管理系统资源。Kubernetes 也是这样，它的管理目标是大规模的集群和应用，必须要能够把系统抽象到足够高的层次，分解出一些松耦合的对象，才能简化系统模型，减轻用户的心智负担。所以，Kubernetes 扮演的角色就如同一个“大师级别”的系统管理员，具有丰富的集群运维经验，独创了自己的一套工作方式，不需要太多的外部干预，就能够自主实现原先许多复杂的管理工作。下面我们就来看看这位资深管理员的“内功心法”。Kubernetes 官网上有一张架构图，但我觉得不是太清晰、重点不突出，所以另外找了一份（图片来源）。虽然这张图有点“老”，但对于我们初学 Kubernetes 还是比较合适的。 Kubernetes 采用了现今流行的“控制面 / 数据面”（Control Plane / Data Plane）架构，集群里的计算机被称为“节点”（Node），可以是实机也可以是虚机，少量的节点用作控制面来执行集群的管理维护工作，其他的大部分节点都被划归数据面，用来跑业务应用。控制面的节点在 Kubernetes 里叫做 Master Node，一般简称为 Master，它是整个集群里最重要的部分，可以说是 Kubernetes 的大脑和心脏。数据面的节点叫做 Worker Node，一般就简称为 Worker 或者 Node，相当于 Kubernetes 的手和脚，在 Master 的指挥下干活。Node 的数量非常多，构成了一个资源池，Kubernetes 就在这个池里分配资源，调度应用。因为资源被“池化”了，所以管理也就变得比较简单，可以在集群中任意添加或者删除节点。在这张架构图里，我们还可以看到有一个 kubectl，它就是 Kubernetes 的客户端工具，用来操作 Kubernetes，但它位于集群之外，理论上不属于集群。你可以使用命令 kubectl get node 来查看 Kubernetes 的节点状态： kubectl get node 可以看到当前的 minikube 集群里只有一个 Master，那 Node 怎么不见了？这是因为 Master 和 Node 的划分不是绝对的。当集群的规模较小，工作负载较少的时候，Master 也可以承担 Node 的工作，就像我们搭建的 minikube 环境，它就只有一个节点，这个节点既是 Master 又是 Node。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:13:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"节点内部的结构 Kubernetes 的节点内部也具有复杂的结构，是由很多的模块构成的，这些模块又可以分成组件（Component）和插件（Addon）两类。组件实现了 Kubernetes 的核心功能特性，没有这些组件 Kubernetes 就无法启动，而插件则是 Kubernetes 的一些附加功能，属于“锦上添花”，不安装也不会影响 Kubernetes 的正常运行。接下来我先来讲讲 Master 和 Node 里的组件，然后再捎带提一下插件，理解了它们的工作流程，你就会明白为什么 Kubernetes 有如此强大的自动化运维能力。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:13:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Master 里的组件有哪些 Master 里有 4 个组件，分别是 apiserver、etcd、scheduler、controller-manager。 apiserver 是 Master 节点——同时也是整个 Kubernetes 系统的唯一入口，它对外公开了一系列的 RESTful API，并且加上了验证、授权等功能，所有其他组件都只能和它直接通信，可以说是 Kubernetes 里的联络员。etcd 是一个高可用的分布式 Key-Value 数据库，用来持久化存储系统里的各种资源对象和状态，相当于 Kubernetes 里的配置管理员。注意它只与 apiserver 有直接联系，也就是说任何其他组件想要读写 etcd 里的数据都必须经过 apiserver。scheduler 负责容器的编排工作，检查节点的资源状态，把 Pod 调度到最适合的节点上运行，相当于部署人员。因为节点状态和 Pod 信息都存储在 etcd 里，所以 scheduler 必须通过 apiserver 才能获得。controller-manager 负责维护容器和节点等资源的状态，实现故障检测、服务迁移、应用伸缩等功能，相当于监控运维人员。同样地，它也必须通过 apiserver 获得存储在 etcd 里的信息，才能够实现对资源的各种操作。这 4 个组件也都被容器化了，运行在集群的 Pod 里，我们可以用 kubectl 来查看它们的状态，使用命令： kubectl get pod -n kube-system 注意命令行里要用 -n kube-system 参数，表示检查“kube-system”名字空间里的 Pod，至于名字空间是什么，我们后面会讲到。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:13:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Node 里的组件有哪些 Master 里的 apiserver、scheduler 等组件需要获取节点的各种信息才能够作出管理决策，那这些信息该怎么来呢？这就需要 Node 里的 3 个组件了，分别是 kubelet、kube-proxy、container-runtime。kubelet 是 Node 的代理，负责管理 Node 相关的绝大部分操作，Node 上只有它能够与 apiserver 通信，实现状态报告、命令下发、启停容器等功能，相当于是 Node 上的一个“小管家”。kube-proxy 的作用有点特别，它是 Node 的网络代理，只负责管理容器的网络通信，简单来说就是为 Pod 转发 TCP/UDP 数据包，相当于是专职的“小邮差”。第三个组件 container-runtime 我们就比较熟悉了，它是容器和镜像的实际使用者，在 kubelet 的指挥下创建容器，管理 Pod 的生命周期，是真正干活的“苦力”。 我们一定要注意，因为 Kubernetes 的定位是容器编排平台，所以它没有限定 container-runtime 必须是 Docker，完全可以替换成任何符合标准的其他容器运行时，例如 containerd、CRI-O 等等，只不过在这里我们使用的是 Docker。这 3 个组件中只有 kube-proxy 被容器化了，而 kubelet 因为必须要管理整个节点，容器化会限制它的能力，所以它必须在 container-runtime 之外运行。使用 minikube ssh 命令登录到节点后，可以用 docker ps 看到 kube-proxy： minikube ssh docker ps |grep kube-proxy 而 kubelet 用 docker ps 是找不到的，需要用操作系统的 ps 命令： ps -ef|grep kubelet 现在，我们再把 Node 里的组件和 Master 里的组件放在一起来看，就能够明白 Kubernetes 的大致工作流程了：每个 Node 上的 kubelet 会定期向 apiserver 上报节点状态，apiserver 再存到 etcd 里。每个 Node 上的 kube-proxy 实现了 TCP/UDP 反向代理，让容器对外提供稳定的服务。scheduler 通过 apiserver 得到当前的节点状态，调度 Pod，然后 apiserver 下发命令给某个 Node 的 kubelet，kubelet 调用 container-runtime 启动容器。controller-manager 也通过 apiserver 得到实时的节点状态，监控可能的异常情况，再使用相应的手段去调节恢复。 其实，这和我们在 Kubernetes 出现之前的操作流程也差不了多少，但 Kubernetes 的高明之处就在于把这些都抽象化规范化了。于是，这些组件就好像是无数个不知疲倦的运维工程师，把原先繁琐低效的人力工作搬进了高效的计算机里，就能够随时发现集群里的变化和异常，再互相协作，维护集群的健康状态。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:13:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"插件（Addons）有哪些 只要服务器节点上运行了 apiserver、scheduler、kubelet、kube-proxy、container-runtime 等组件，就可以说是一个功能齐全的 Kubernetes 集群了。不过就像 Linux 一样，操作系统提供的基础功能虽然“可用”，但想达到“好用”的程度，还是要再安装一些附加功能，这在 Kubernetes 里就是插件（Addon）。由于 Kubernetes 本身的设计非常灵活，所以就有大量的插件用来扩展、增强它对应用和集群的管理能力。minikube 也支持很多的插件，使用命令 minikube addons list 就可以查看插件列表： minikube addons list 插件中我个人认为比较重要的有两个：DNS 和 Dashboard。DNS 你应该比较熟悉吧，它在 Kubernetes 集群里实现了域名解析服务，能够让我们以域名而不是 IP 地址的方式来互相通信，是服务发现和负载均衡的基础。由于它对微服务、服务网格等架构至关重要，所以基本上是 Kubernetes 的必备插件。Dashboard 就是仪表盘，为 Kubernetes 提供了一个图形化的操作界面，非常直观友好，虽然大多数 Kubernetes 工作都是使用命令行 kubectl，但有的时候在 Dashboard 上查看信息也是挺方便的。你只要在 minikube 环境里执行一条简单的命令，就可以自动用浏览器打开 Dashboard 页面，而且还支持中文： minikube dashboard ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:13:6","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们一起来研究了 Kubernetes 的内部架构和工作机制，可以看到它的功能非常完善，实现了大部分常见的运维管理工作，而且是全自动化的，能够节约大量的人力成本。由于 Kubernetes 的抽象程度比较高，有很多陌生的新术语，不太好理解，所以我画了一张思维导图，你可以对照着再加深理解。 最后小结一下今天的要点：Kubernetes 能够在集群级别管理应用和服务器，可以认为是一种集群操作系统。它使用“控制面 / 数据面”的基本架构，Master 节点实现管理控制功能，Worker 节点运行具体业务。Kubernetes 由很多模块组成，可分为核心的组件和选配的插件两类。Master 里有 4 个组件，分别是 apiserver、etcd、scheduler、controller-manager。Node 里有 3 个组件，分别是 kubelet、kube-proxy、container-runtime。通常必备的插件有 DNS 和 Dashboard。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:13:7","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"加餐｜Kubernetes“弃用Docker”是怎么回事？ 在“入门篇”学习容器技术的过程中，我看到有不少同学留言问 Kubernetes“弃用 Docker”的事情，担心现在学 Docker 是否还有价值，是否现在就应该切换到 containerd 或者是其他 runtime。这些疑虑的确是有些道理。两年前，Kubernetes 放出消息要“弃用 Docker”的时候，确确实实在 Kubernetes 社区里掀起了一场“轩然大波”，影响甚至波及到社区之外，也导致 Kubernetes 不得不写了好几篇博客来反复解释这么做的原因。两年过去了，虽然最新的 Kubernetes 1.24 已经达成了“弃用”的目标，但很多人对这件事似乎还是没有非常清晰的认识。所以今天，我们就来聊聊这个话题，我也讲讲我的一些看法。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:14:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"什么是 CRI 要了解 Kubernetes 为什么要“弃用 Docker”，还得追根溯源，回头去看 Kubernetes 的发展历史。2014 年，Docker 正如日中天，在容器领域没有任何对手，而这时 Kubernetes 才刚刚诞生，虽然背后有 Google 和 Borg 的支持，但还是比较弱小的。所以，Kubernetes 很自然就选择了在 Docker 上运行，毕竟“背靠大树好乘凉”，同时也能趁机“养精蓄锐”逐步发展壮大自己。时间一转眼到了 2016 年，CNCF 已经成立一年了，而 Kubernetes 也已经发布了 1.0 版，可以正式用于生产环境，这些都标志着 Kubernetes 已经成长起来了，不再需要“看脸色吃饭”。于是它就宣布加入了 CNCF，成为了第一个 CNCF 托管项目，想要借助基金会的力量联合其他厂商，一起来“扳倒”Docker。那它是怎么做的呢？ 在 2016 年底的 1.5 版里，Kubernetes 引入了一个新的接口标准：CRI ，Container Runtime Interface。CRI 采用了 ProtoBuffer 和 gPRC，规定 kubelet 该如何调用容器运行时去管理容器和镜像，但这是一套全新的接口，和之前的 Docker 调用完全不兼容。Kubernetes 意思很明显，就是不想再绑定在 Docker 上了，允许在底层接入其他容器技术（比如 rkt、kata 等），随时可以把 Docker“踢开”。但是这个时候 Docker 已经非常成熟，而且市场的惯性也非常强大，各大云厂商不可能一下子就把 Docker 全部替换掉。所以 Kubernetes 也只能同时提供一个“折中”方案，在 kubelet 和 Docker 中间加入一个“适配器”，把 Docker 的接口转换成符合 CRI 标准的接口（图片来源）： 因为这个“适配器”夹在 kubelet 和 Docker 之间，所以就被形象地称为是“shim”，也就是“垫片”的意思。有了 CRI 和 shim，虽然 Kubernetes 还使用 Docker 作为底层运行时，但也具备了和 Docker 解耦的条件，从此就拉开了“弃用 Docker”这场大戏的帷幕。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:14:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"什么是 containerd 面对 Kubernetes“咄咄逼人”的架势，Docker 是看在眼里痛在心里，虽然有苦心经营了多年的社区和用户群，但公司的体量太小，实在是没有足够的实力与大公司相抗衡。不过 Docker 也没有“坐以待毙”，而是采取了“断臂求生”的策略，推动自身的重构，把原本单体架构的 Docker Engine 拆分成了多个模块，其中的 Docker daemon 部分就捐献给了 CNCF，形成了 containerd。containerd 作为 CNCF 的托管项目，自然是要符合 CRI 标准的。但 Docker 出于自己诸多原因的考虑，它只是在 Docker Engine 里调用了 containerd，外部的接口仍然保持不变，也就是说还不与 CRI 兼容。 由于 Docker 的“固执己见”，这时 Kubernetes 里就出现了两种调用链：第一种是用 CRI 接口调用 dockershim，然后 dockershim 调用 Docker，Docker 再走 containerd 去操作容器。第二种是用 CRI 接口直接调用 containerd 去操作容器。 显然，由于都是用 containerd 来管理容器，所以这两种调用链的最终效果是完全一样的，但是第二种方式省去了 dockershim 和 Docker Engine 两个环节，更加简洁明了，损耗更少，性能也会提升一些。在 2018 年 Kubernetes 1.10 发布的时候，containerd 也更新到了 1.1 版，正式与 Kubernetes 集成，同时还发表了一篇博客文章（https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/），展示了一些性能测试数据： 从这些数据可以看到，containerd1.1 相比当时的 Docker 18.03，Pod 的启动延迟降低了大约 20%，CPU 使用率降低了 68%，内存使用率降低了 12%，这是一个相当大的性能改善，对于云厂商非常有诱惑力。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:14:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"正式“弃用 Docker” 有了 CRI 和 containerd 这两件强大的武器，胜利的天平已经明显向 Kubernetes 倾斜了。又是两年之后，到了 2020 年，Kubernetes 1.20 终于正式向 Docker“宣战”：kubelet 将弃用 Docker 支持，并会在未来的版本中彻底删除。但由于 Docker 几乎成为了容器技术的代名词，而且 Kubernetes 也已经使用 Docker 很多年，这个声明在不断传播的过程中很快就“变味”了，“kubelet 将弃用 Docker 支持”被简化成了更吸引眼球的“Kubernetes 将弃用 Docker”。 这自然就在 IT 界引起了恐慌，“不明真相的广大群众”纷纷表示震惊：用了这么久的 Docker 突然就不能用了，Kubernetes 为什么要如此对待 Docker？之前在 Docker 上的投入会不会就全归零了？现有的大量镜像该怎么办？其实，如果你理解了前面讲的 CRI 和 containerd 这两个项目，就会知道 Kubernetes 的这个举动也没有什么值得大惊小怪的，一切都是“水到渠成”的：它实际上只是“弃用了 dockershim”这个小组件，也就是说把 dockershim 移出了 kubelet，并不是“弃用了 Docker”这个软件产品。所以，“弃用 Docker”对 Kubernetes 和 Docker 来说都不会有什么太大的影响，因为他们两个都早已经把下层都改成了开源的 containerd，原来的 Docker 镜像和容器仍然会正常运行，唯一的变化就是 Kubernetes 绕过了 Docker，直接调用 Docker 内部的 containerd 而已。这个关系你可以参考下面的这张图来理解： 当然，影响也不是完全没有。如果 Kubernetes 直接使用 containerd 来操纵容器，那么它就是一个与 Docker 独立的工作环境，彼此都不能访问对方管理的容器和镜像。换句话说，使用命令 docker ps 就看不到在 Kubernetes 里运行的容器了。这对有的人来说可能需要稍微习惯一下，改用新的工具 crictl，不过用来查看容器、镜像的子命令还是一样的，比如 ps、images 等等，适应起来难度不大（但如果我们一直用 kubectl 来管理 Kubernetes 的话，这就是没有任何影响了）。“宣战”之后，Kubernetes 原本打算用一年的时间完成“弃用 Docker”的工作，但它也确实低估了 Docker 的根基，到了 1.23 版还是没能移除 dockershim，不得已又往后推迟了半年，终于在今年 5 月份发布的 1.24 版把 dockershim 的代码从 kubelet 里删掉了。自此，Kubernetes 彻底和 Docker“分道扬镳”，今后就是“大路朝天，各走一边”。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:14:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"Docker 的未来 那么，Docker 的未来会是怎么样的呢？难道云原生时代就没有它的立足之地了吗？这个问题的答案很显然是否定的。作为容器技术的初创者，Docker 的历史地位无人能够质疑，虽然现在 Kubernetes 不再默认绑定 Docker，但 Docker 还是能够以其他的形式与 Kubernetes 共存的。首先，因为容器镜像格式已经被标准化了（OCI 规范，Open Container Initiative），Docker 镜像仍然可以在 Kubernetes 里正常使用，原来的开发测试、CI/CD 流程都不需要改动，我们仍然可以拉取 Docker Hub 上的镜像，或者编写 Dockerfile 来打包应用。其次，Docker 是一个完整的软件产品线，不止是 containerd，它还包括了镜像构建、分发、测试等许多服务，甚至在 Docker Desktop 里还内置了 Kubernetes。单就容器开发的便利性来讲，Docker 还是暂时难以被替代的，广大云原生开发者可以在这个熟悉的环境里继续工作，利用 Docker 来开发运行在 Kubernetes 里的应用。 再次，虽然 Kubernetes 已经不再包含 dockershim，但 Docker 公司却把这部分代码接管了过来，另建了一个叫 cri-dockerd（https://github.com/mirantis/cri-dockerd）的项目，作用也是一样的，把 Docker Engine 适配成 CRI 接口，这样 kubelet 就又可以通过它来操作 Docker 了，就仿佛是一切从未发生过。综合来看，Docker 虽然在容器编排战争里落败，被 Kubernetes 排挤到了角落，但它仍然具有强韧的生命力，多年来积累的众多忠实用户和数量庞大的应用镜像是它的最大资本和后盾，足以支持它在另一条不与 Kubernetes 正面交锋的道路上走下去。而对于我们这些初学者来说，Docker 方便易用，具有完善的工具链和友好的交互界面，市面上很难找到能够与它媲美的软件了，应该说是入门学习容器技术和云原生的“不二之选”。至于 Kubernetes 底层用的什么，我们又何必太过于执着和关心呢？ ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:14:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"11｜YAML：Kubernetes世界里的通用语 在上次课里，我们一起研究了 Kubernetes 的内部架构和组成，知道它分为控制面和数据面。控制面管理集群，数据面跑业务应用，节点内部又有 apiserver、etcd、scheduler、kubelet、kube-proxy 等组件，它们互相协作来维护整个集群的稳定运行。这套独特的 Master/Node 架构是 Kubernetes 得以安身立命的根本，但仅依靠这套“内功心法”是不是就能够随意仗剑走天涯了呢？显然不行。就像许多武侠、玄幻作品里的人物一样，Kubernetes 也需要一份“招式秘籍”才能把自己的“内功”完全发挥出来，只有内外兼修才能够达到笑傲江湖的境界。而这份“招式秘籍”，就是 Kubernetes 世界里的标准工作语言 YAML，所以今天，我就来讲讲为什么要有 YAML、它是个什么样子、该怎么使用。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:15:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"声明式与命令式是怎么回事 Kubernetes 使用的 YAML 语言有一个非常关键的特性，叫“声明式”（Declarative），对应的有另外一个词：“命令式”（Imperative）。所以在详细了解 YAML 之前，我们得先来看看“声明式”与“命令式”这两种工作方式，它们在计算机世界里的关系有点像小说里的“剑宗”与“气宗”。我们在入门篇里学习的 Docker 命令和 Dockerfile 就属于“命令式”，大多数编程语言也属于命令式，它的特点是交互性强，注重顺序和过程，你必须“告诉”计算机每步该做什么，所有的步骤都列清楚，这样程序才能够一步步走下去，最后完成任务，显得计算机有点“笨”。“声明式”，在 Kubernetes 出现之前比较少见，它与“命令式”完全相反，不关心具体的过程，更注重结果。我们不需要“教”计算机该怎么做，只要告诉它一个目标状态，它自己就会想办法去完成任务，相比起来自动化、智能化程度更高。这两个概念比较抽象，不太好理解，也是 Kubernetes 初学者经常遇到的障碍之一。Kubernetes 官网上特意以空调为例，解说“声明式”的原理，但我感觉还是没有说得太清楚，所以这里我就再以“打车”来形象地解释一下“命令式”和“声明式”的区别。 假设你要打车去高铁站，但司机不熟悉路况，你就只好不厌其烦地告诉他该走哪条路、在哪个路口转向、在哪里进出主路、停哪个站口。虽然最后到达了目的地，但这一路上也费了很多口舌，发出了无数的“命令”。很显然，这段路程就属于“命令式”。现在我们来换一种方式，同样是去高铁站，但司机经验丰富，他知道哪里有拥堵、哪条路的红绿灯多、哪段路有临时管控、哪里可以抄小道，此时你再多嘴无疑会干扰他的正常驾驶，所以，你只要给他一个“声明”：我要去高铁站，接下来就可以舒舒服服地躺在后座上休息，顺利到达目的地了。在这个“打车”的例子里，Kubernetes 就是这样的一位熟练的司机，Master/Node 架构让它对整个集群的状态了如指掌，内部的众多组件和插件也能够自动监控管理应用。这个时候我们再用“命令式”跟它打交道就不太合适了，因为它知道的信息比我们更多更全面，不需要我们这个外行去指导它这个内行，所以我们最好是做一个“甩手掌柜”，用“声明式”把任务的目标告诉它，比如使用哪个镜像、什么时候运行，让它自己去处理执行过程中的细节。那么，该用什么方式去给 Kubernetes 发出一个“声明”呢？容器技术里的 Shell 脚本和 Dockerfile 可以很好地描述“命令式”，但对于“声明式”就不太合适了，这个时候，我们需要使用专门的 YAML 语言。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:15:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"什么是 YAML YAML 语言创建于 2001 年，比 XML 晚了三年。XML 你应该知道吧，它是一种类似 HTML 的标签式语言，有很多繁文缛节。而 YAML 虽然在名字上模仿了 XML，但实质上与 XML 完全不同，更适合人类阅读，计算机解析起来也很容易。YAML 的官网（https://yaml.org/）有对语言规范的完整介绍，所以我就不在这里列举语言的细节了，只讲一些与 Kubernetes 相关的要点，帮助你快速掌握。你需要知道，YAML 是 JSON 的超集，支持整数、浮点数、布尔、字符串、数组和对象等数据类型。也就是说，任何合法的 JSON 文档也都是 YAML 文档，如果你了解 JSON，那么学习 YAML 会容易很多。但和 JSON 比起来，YAML 的语法更简单，形式也更清晰紧凑，比如： 使用空白与缩进表示层次（有点类似 Python），可以不使用花括号和方括号。可以使用 # 书写注释，比起 JSON 是很大的改进。对象（字典）的格式与 JSON 基本相同，但 Key 不需要使用双引号。数组（列表）是使用 - 开头的清单形式（有点类似 MarkDown）。表示对象的 : 和表示数组的 - 后面都必须要有空格。可以使用 — 在一个文件里分隔多个 YAML 对象。 下面我们来看几个 YAML 的简单示例。首先是数组，它使用 - 列出了三种操作系统： # YAML数组(列表) OS: - linux - macOS - Windows 这段 YAML 对应的 JSON 如下： { \"OS\": [\"linux\", \"macOS\", \"Windows\"] } 对比可以看到 YAML 形式上很简单，没有闭合花括号、方括号的麻烦，每个元素后面也不需要逗号。再来看一个 YAML 对象，声明了 1 个 Master 节点，3 个 Worker 节点： # YAML对象(字典) Kubernetes: master: 1 worker: 3 它等价的 JSON 如下： { \"Kubernetes\": { \"master\": 1, \"worker\": 3 } } 注意到了吗 YAML 里的 Key 都不需要使用双引号，看起来更舒服。把 YAML 的数组、对象组合起来，我们就可以描述出任意的 Kubernetes 资源对象，第三个例子略微复杂点，你可以自己尝试着解释一下： # 复杂的例子，组合数组和对象 Kubernetes: master: - apiserver: running - etcd: running node: - kubelet: running - kube-proxy: down - container-runtime: [docker, containerd, cri-o] 关于 YAML 语言的其他知识点我就不再一一细说了，都整理在了这张图里，你可以参考YAML 官网，在今后的课程中慢慢体会。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:15:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"什么是 API 对象 学到这里还不够，因为 YAML 语言只相当于“语法”，要与 Kubernetes 对话，我们还必须有足够的“词汇”来表示“语义”。那么应该声明 Kubernetes 里的哪些东西，才能够让 Kubernetes 明白我们的意思呢？作为一个集群操作系统，Kubernetes 归纳总结了 Google 多年的经验，在理论层面抽象出了很多个概念，用来描述系统的管理运维工作，这些概念就叫做“API 对象”。说到这个名字，你也许会联想到上次课里讲到的 Kubernetes 组件 apiserver。没错，它正是来源于此。因为 apiserver 是 Kubernetes 系统的唯一入口，外部用户和内部组件都必须和它通信，而它采用了 HTTP 协议的 URL 资源理念，API 风格也用 RESTful 的 GET/POST/DELETE 等等，所以，这些概念很自然地就被称为是“API 对象”了。那都有哪些 API 对象呢？你可以使用 kubectl api-resources 来查看当前 Kubernetes 版本支持的所有对象： kubectl api-resources 在输出的“NAME”一栏，就是对象的名字，比如 ConfigMap、Pod、Service 等等，第二栏“SHORTNAMES”则是这种资源的简写，在我们使用 kubectl 命令的时候很有用，可以少敲几次键盘，比如 Pod 可以简写成 po，Service 可以简写成 svc。在使用 kubectl 命令的时候，你还可以加上一个参数 –v=9，它会显示出详细的命令执行过程，清楚地看到发出的 HTTP 请求，比如： kubectl get pod --v=9 从截图里可以看到，kubectl 客户端等价于调用了 curl，向 8443 端口发送了 HTTP GET 请求，URL 是 /api/v1/namespaces/default/pods。目前的 Kubernetes 1.23 版本有 50 多种 API 对象，全面地描述了集群的节点、应用、配置、服务、账号等等信息，apiserver 会把它们都存储在数据库 etcd 里，然后 kubelet、scheduler、controller-manager 等组件通过 apiserver 来操作它们，就在 API 对象这个抽象层次实现了对整个集群的管理。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:15:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何描述 API 对象 现在我们就来看看如何以 YAML 语言，使用“声明式”在 Kubernetes 里描述并创建 API 对象。之前我们运行 Nginx 的命令你还记得吗？使用的是 kubectl run，和 Docker 一样是“命令式”的： kubectl run ngx --image=nginx:alpine 我们来把它改写成“声明式”的 YAML，说清楚我们想要的 Nginx 应用是个什么样子，也就是“目标状态”，让 Kubernetes 自己去决定如何拉取镜像运行： apiVersion: v1 kind: Pod metadata: name: ngx-pod labels: env: demo owner: chrono spec: containers: - image: nginx:alpine name: ngx ports: - containerPort: 80 有了刚才 YAML 语言知识“打底”，相信你基本上能够把它看明白，知道它是一个 Pod，要使用 nginx:alpine 镜像创建一个容器，开放端口 80，而其他的部分，就是 Kubernetes 对 API 对象强制的格式要求了。因为 API 对象采用标准的 HTTP 协议，为了方便理解，我们可以借鉴一下 HTTP 的报文格式，把 API 对象的描述分成“header”和“body”两部分。“header”包含的是 API 对象的基本信息，有三个字段：apiVersion、kind、metadata。 apiVersion 表示操作这种资源的 API 版本号，由于 Kubernetes 的迭代速度很快，不同的版本创建的对象会有差异，为了区分这些版本就需要使用 apiVersion 这个字段，比如 v1、v1alpha1、v1beta1 等等。kind 表示资源对象的类型，这个应该很好理解，比如 Pod、Node、Job、Service 等等。metadata 这个字段顾名思义，表示的是资源的一些“元信息”，也就是用来标记对象，方便 Kubernetes 管理的一些信息。 apiVersion: v1 kind: Pod metadata: name: ngx-pod labels: env: demo owner: chrono 比如在这个 YAML 示例里就有两个“元信息”，一个是 name，给 Pod 起了个名字叫 ngx-pod，另一个是 labels，给 Pod“贴”上了一些便于查找的标签，分别是 env 和 owner。apiVersion、kind、metadata 都被 kubectl 用于生成 HTTP 请求发给 apiserver，你可以用 –v=9 参数在请求的 URL 里看到它们，比如： https://192.168.49.2:8443/api/v1/namespaces/default/pods/ngx-pod 和 HTTP 协议一样，“header”里的 apiVersion、kind、metadata 这三个字段是任何对象都必须有的，而“body”部分则会与对象特定相关，每种对象会有不同的规格定义，在 YAML 里就表现为 spec 字段（即 specification），表示我们对对象的“期望状态”（desired status）。还是来看这个 Pod，它的 spec 里就是一个 containers 数组，里面的每个元素又是一个对象，指定了名字、镜像、端口等信息： spec: containers: - image: nginx:alpine name: ngx ports: - containerPort: 80 现在把这些字段综合起来，我们就能够看出，这份 YAML 文档完整地描述了一个类型是 Pod 的 API 对象，要求使用 v1 版本的 API 接口去管理，其他更具体的名称、标签、状态等细节都记录在了 metadata 和 spec 字段等里。使用 kubectl apply、kubectl delete，再加上参数 -f，你就可以使用这个 YAML 文件，创建或者删除对象了： kubectl apply -f ngx-pod.yml kubectl delete -f ngx-pod.yml Kubernetes 收到这份“声明式”的数据，再根据 HTTP 请求里的 POST/DELETE 等方法，就会自动操作这个资源对象，至于对象在哪个节点上、怎么创建、怎么删除完全不用我们操心。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:15:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何编写 YAML 讲到这里，相信你对如何使用 YAML 与 Kubernetes 沟通应该大概了解了，不过疑问也会随之而来：这么多 API 对象，我们怎么知道该用什么 apiVersion、什么 kind？metadata、spec 里又该写哪些字段呢？还有，YAML 看起来简单，写起来却比较麻烦，缩进对齐很容易搞错，有没有什么简单的方法呢？这些问题最权威的答案无疑是 Kubernetes 的官方参考文档（https://kubernetes.io/docs/reference/kubernetes-api/），API 对象的所有字段都可以在里面找到。不过官方文档内容太多太细，查阅起来有些费劲，所以下面我就介绍几个简单实用的小技巧。 第一个技巧其实前面已经说过了，就是 kubectl api-resources 命令，它会显示出资源对象相应的 API 版本和类型，比如 Pod 的版本是“v1”，Ingress 的版本是“networking.k8s.io/v1”，照着它写绝对不会错。第二个技巧，是命令 kubectl explain，它相当于是 Kubernetes 自带的 API 文档，会给出对象字段的详细说明，这样我们就不必去网上查找了。比如想要看 Pod 里的字段该怎么写，就可以这样： kubectl explain pod kubectl explain pod.metadata kubectl explain pod.spec kubectl explain pod.spec.containers 使用前两个技巧编写 YAML 就基本上没有难度了。不过我们还可以让 kubectl 为我们“代劳”，生成一份“文档样板”，免去我们打字和对齐格式的工作。这第三个技巧就是 kubectl 的两个特殊参数 –dry-run=client 和 -o yaml，前者是空运行，后者是生成 YAML 格式，结合起来使用就会让 kubectl 不会有实际的创建动作，而只生成 YAML 文件。例如，想要生成一个 Pod 的 YAML 样板示例，可以在 kubectl run 后面加上这两个参数： kubectl run ngx --image=nginx:alpine --dry-run=client -o yaml 就会生成一个绝对正确的 YAML 文件： apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: ngx name: ngx spec: containers: - image: nginx:alpine name: ngx resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} 接下来你要做的，就是查阅对象的说明文档，添加或者删除字段来定制这个 YAML 了。这个小技巧还可以再进化一下，把这段参数定义成 Shell 变量（名字任意，比如$do/$go，这里用的是$out），用起来会更省事，比如： export out=\"--dry-run=client -o yaml\" kubectl run ngx --image=nginx:alpine $out 今后除了一些特殊情况，我们都不会再使用 kubectl run 这样的命令去直接创建 Pod，而是会编写 YAML，用“声明式”来描述对象，再用 kubectl apply 去发布 YAML 来创建对象。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:15:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天就到这里，我们一起学习了“声明式”和“命令式”的区别、YAML 语言的语法、如何用 YAML 来描述 API 对象，还有一些编写 YAML 文件的技巧。Kubernetes 采用 YAML 作为工作语言是它有别与其他系统的一大特色，声明式的语言能够更准确更清晰地描述系统状态，避免引入繁琐的操作步骤扰乱系统，与 Kubernetes 高度自动化的内部结构相得益彰，而且纯文本形式的 YAML 也很容易版本化，适合 CI/CD。再小结一下今天的内容要点：YAML 是 JSON 的超集，支持数组和对象，能够描述复杂的状态，可读性也很好。Kubernetes 把集群里的一切资源都定义为 API 对象，通过 RESTful 接口来管理。描述 API 对象需要使用 YAML 语言，必须的字段是 apiVersion、kind、metadata。命令 kubectl api-resources 可以查看对象的 apiVersion 和 kind，命令 kubectl explain 可以查看对象字段的说明文档。命令 kubectl apply、kubectl delete 发送 HTTP 请求，管理 API 对象。使用参数 –dry-run=client -o yaml 可以生成对象的 YAML 模板，简化编写工作。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:15:6","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"12｜Pod：如何理解这个Kubernetes里最核心的概念？ 前两天我们学习了 Kubernetes 世界里的工作语言 YAML，还编写了一个简短的 YAML 文件，描述了一个 API 对象：Pod，它在 spec 字段里包含了容器的定义。那么为什么 Kubernetes 不直接使用已经非常成熟稳定的容器？为什么要再单独抽象出一个 Pod 对象？为什么几乎所有人都说 Pod 是 Kubernetes 里最核心最基本的概念呢？今天我就来逐一解答这些问题，希望你学完今天的这次课，心里面能够有明确的答案。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:16:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"为什么要有 Pod Pod 这个词原意是“豌豆荚”，后来又延伸出“舱室”“太空舱”等含义，你可以看一下这张图片，形象地来说 Pod 就是包含了很多组件、成员的一种结构。 容器技术我想你现在已经比较熟悉了，它让进程在一个“沙盒”环境里运行，具有良好的隔离性，对应用是一个非常好的封装。不过，当容器技术进入到现实的生产环境中时，这种隔离性就带来了一些麻烦。因为很少有应用是完全独立运行的，经常需要几个进程互相协作才能完成任务，比如在“入门篇”里我们搭建 WordPress 网站的时候，就需要 Nginx、WordPress、MariaDB 三个容器一起工作。WordPress 例子里的这三个应用之间的关系还是比较松散的，它们可以分别调度，运行在不同的机器上也能够以 IP 地址通信。但还有一些特殊情况，多个应用结合得非常紧密以至于无法把它们拆开。比如，有的应用运行前需要其他应用帮它初始化一些配置，还有就是日志代理，它必须读取另一个应用存储在本地磁盘的文件再转发出去。这些应用如果被强制分离成两个容器，切断联系，就无法正常工作了。那么把这些应用都放在一个容器里运行可不可以呢？当然可以，但这并不是一种好的做法。因为容器的理念是对应用的独立封装，它里面就应该是一个进程、一个应用，如果里面有多个应用，不仅违背了容器的初衷，也会让容器更难以管理。为了解决这样多应用联合运行的问题，同时还要不破坏容器的隔离，就需要在容器外面再建立一个“收纳舱”，让多个容器既保持相对独立，又能够小范围共享网络、存储等资源，而且永远是“绑在一起”的状态。所以，Pod 的概念也就呼之欲出了，容器正是“豆荚”里那些小小的“豌豆”，你可以在 Pod 的 YAML 里看到，“spec.containers”字段其实是一个数组，里面允许定义多个容器。如果再拿之前讲过的“小板房”来比喻的话，Pod 就是由客厅、卧室、厨房等预制房间拼装成的一个齐全的生活环境，不仅同样具备易于拆装易于搬迁的优点，而且要比单独的“一居室”功能强大得多，能够让进程“住”得更舒服。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:16:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"为什么 Pod 是 Kubernetes 的核心对象 因为 Pod 是对容器的“打包”，里面的容器是一个整体，总是能够一起调度、一起运行，绝不会出现分离的情况，而且 Pod 属于 Kubernetes，可以在不触碰下层容器的情况下任意定制修改。所以有了 Pod 这个抽象概念，Kubernetes 在集群级别上管理应用就会“得心应手”了。Kubernetes 让 Pod 去编排处理容器，然后把 Pod 作为应用调度部署的最小单位，Pod 也因此成为了 Kubernetes 世界里的“原子”（当然这个“原子”内部是有结构的，不是铁板一块），基于 Pod 就可以构建出更多更复杂的业务形态了。下面的这张图你也许在其他资料里见过，它从 Pod 开始，扩展出了 Kubernetes 里的一些重要 API 对象，比如配置信息 ConfigMap、离线作业 Job、多实例部署 Deployment 等等，它们都分别对应到现实中的各种实际运维需求。 不过这张图虽然很经典，参考价值很高，但毕竟有些年头了，随着 Kubernetes 的发展，它已经不能够全面地描述 Kubernetes 的资源对象了。受这张图的启发，我自己重新画了一份以 Pod 为中心的 Kubernetes 资源对象关系图，添加了一些新增的 Kubernetes 概念，今后我们就依据这张图来探索 Kubernetes 的各项功能。 从这两张图中你也应该能够看出来，所有的 Kubernetes 资源都直接或者间接地依附在 Pod 之上，所有的 Kubernetes 功能都必须通过 Pod 来实现，所以 Pod 理所当然地成为了 Kubernetes 的核心对象。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:16:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何使用 YAML 描述 Pod 既然 Pod 这么重要，那么我们就很有必要来详细了解一下 Pod，理解了 Pod 概念，我们的 Kubernetes 学习之旅就成功了一半。还记得吧，我们始终可以用命令 kubectl explain 来查看任意字段的详细说明，所以接下来我就只简要说说写 YAML 时 Pod 里的一些常用字段。因为 Pod 也是 API 对象，所以它也必然具有 apiVersion、kind、metadata、spec 这四个基本组成部分。“apiVersion”和“kind”这两个字段很简单，对于 Pod 来说分别是固定的值 v1 和 Pod，而一般来说，“metadata”里应该有 name 和 labels 这两个字段。我们在使用 Docker 创建容器的时候，可以不给容器起名字，但在 Kubernetes 里，Pod 必须要有一个名字，这也是 Kubernetes 里所有资源对象的一个约定。在课程里，我通常会为 Pod 名字统一加上 pod 后缀，这样可以和其他类型的资源区分开。 name 只是一个基本的标识，信息有限，所以 labels 字段就派上了用处。它可以添加任意数量的 Key-Value，给 Pod“贴”上归类的标签，结合 name 就更方便识别和管理了。比如说，我们可以根据运行环境，使用标签 env=dev/test/prod，或者根据所在的数据中心，使用标签 region: north/south，还可以根据应用在系统中的层次，使用 tier=front/middle/back ……如此种种，只需要发挥你的想象力。下面这段 YAML 代码就描述了一个简单的 Pod，名字是“busy-pod”，再附加上一些标签： apiVersion: v1 kind: Pod metadata: name: busy-pod labels: owner: chrono env: demo region: north tier: back “metadata”一般写上 name 和 labels 就足够了，而“spec”字段由于需要管理、维护 Pod 这个 Kubernetes 的基本调度单元，里面有非常多的关键信息，今天我介绍最重要的“containers”，其他的 hostname、restartPolicy 等字段你可以课后自己查阅文档学习。“containers”是一个数组，里面的每一个元素又是一个 container 对象，也就是容器。和 Pod 一样，container 对象也必须要有一个 name 表示名字，然后当然还要有一个 image 字段来说明它使用的镜像，这两个字段是必须要有的，否则 Kubernetes 会报告数据验证错误。container 对象的其他字段基本上都可以和“入门篇”学过的 Docker、容器技术对应，理解起来难度不大，我就随便列举几个： ports：列出容器对外暴露的端口，和 Docker 的 -p 参数有点像。imagePullPolicy：指定镜像的拉取策略，可以是 Always/Never/IfNotPresent，一般默认是 IfNotPresent，也就是说只有本地不存在才会远程拉取镜像，可以减少网络消耗。env：定义 Pod 的环境变量，和 Dockerfile 里的 ENV 指令有点类似，但它是运行时指定的，更加灵活可配置。command：定义容器启动时要执行的命令，相当于 Dockerfile 里的 ENTRYPOINT 指令。args：它是 command 运行时的参数，相当于 Dockerfile 里的 CMD 指令，这两个命令和 Docker 的含义不同，要特别注意。 现在我们就来编写“busy-pod”的 spec 部分，添加 env、command、args 等字段： spec: containers: - image: busybox:latest name: busy imagePullPolicy: IfNotPresent env: - name: os value: \"ubuntu\" - name: debug value: \"on\" command: - /bin/echo args: - \"$(os), $(debug)\" 这里我为 Pod 指定使用镜像 busybox:latest，拉取策略是 IfNotPresent ，然后定义了 os 和 debug 两个环境变量，启动命令是 /bin/echo，参数里输出刚才定义的环境变量。把这份 YAML 文件和 Docker 命令对比一下，你就可以看出，YAML 在 spec.containers 字段里用“声明式”把容器的运行状态描述得非常清晰准确，要比 docker run 那长长的命令行要整洁的多，对人、对机器都非常友好。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:16:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何使用 kubectl 操作 Pod 有了描述 Pod 的 YAML 文件，现在我就介绍一下用来操作 Pod 的 kubectl 命令。kubectl apply、kubectl delete 这两个命令在上次课里已经说过了，它们可以使用 -f 参数指定 YAML 文件创建或者删除 Pod，例如： kubectl apply -f busy-pod.yml kubectl delete -f busy-pod.yaml 不过，因为我们在 YAML 里定义了“name”字段，所以也可以在删除的时候直接指定名字来删除： kubectl delete pod busy-pod 和 Docker 不一样，Kubernetes 的 Pod 不会在前台运行，只能在后台（相当于默认使用了参数 -d），所以输出信息不能直接看到。我们可以用命令 kubectl logs，它会把 Pod 的标准输出流信息展示给我们看，在这里就会显示出预设的两个环境变量的值： kubectl logs busy-pod 使用命令 kubectl get pod 可以查看 Pod 列表和运行状态： kubectl get pod 你会发现这个 Pod 运行有点不正常，状态是“CrashLoopBackOff”，那么我们可以使用命令 kubectl describe 来检查它的详细状态，它在调试排错时很有用： kubectl describe pod busy-pod 通常需要关注的是末尾的“Events”部分，它显示的是 Pod 运行过程中的一些关键节点事件。对于这个 busy-pod，因为它只执行了一条 echo 命令就退出了，而 Kubernetes 默认会重启 Pod，所以就会进入一个反复停止 - 启动的循环错误状态。因为 Kubernetes 里运行的应用大部分都是不会主动退出的服务，所以我们可以把这个 busy-pod 删掉，用上次课里创建的 ngx-pod.yml，启动一个 Nginx 服务，这才是大多数 Pod 的工作方式。 kubectl apply -f ngx-pod.yml 启动之后，我们再用 kubectl get pod 来查看状态，就会发现它已经是“Running”状态了： 另外，kubectl 也提供与 docker 类似的 cp 和 exec 命令，kubectl cp 可以把本地文件拷贝进 Pod，kubectl exec 是进入 Pod 内部执行 Shell 命令，用法也差不多。比如我有一个“a.txt”文件，那么就可以使用 kubectl cp 拷贝进 Pod 的“/tmp”目录里： echo 'aaa' \u003e a.txt kubectl cp a.txt ngx-pod:/tmp 不过 kubectl exec 的命令格式与 Docker 有一点小差异，需要在 Pod 后面加上 –，把 kubectl 的命令与 Shell 命令分隔开，你在用的时候需要小心一些： kubectl exec -it ngx-pod -- sh ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:16:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们一起学习了 Kubernetes 里最核心最基本的概念 Pod，知道了应该如何使用 YAML 来定制 Pod，还有如何使用 kubectl 命令来创建、删除、查看、调试 Pod。Pod 屏蔽了容器的一些底层细节，同时又具有足够的控制管理能力，比起容器的“细粒度”、虚拟机的“粗粒度”，Pod 可以说是“中粒度”，灵活又轻便，非常适合在云计算领域作为应用调度的基本单元，因而成为了 Kubernetes 世界里构建一切业务的“原子”。今天的知识要点我简单列在了下面： 现实中经常会有多个进程密切协作才能完成任务的应用，而仅使用容器很难描述这种关系，所以就出现了 Pod，它“打包”一个或多个容器，保证里面的进程能够被整体调度。Pod 是 Kubernetes 管理应用的最小单位，其他的所有概念都是从 Pod 衍生出来的。Pod 也应该使用 YAML“声明式”描述，关键字段是“spec.containers”，列出名字、镜像、端口等要素，定义内部的容器运行状态。操作 Pod 的命令很多与 Docker 类似，如 kubectl run、kubectl cp、kubectl exec 等，但有的命令有些小差异，使用的时候需要注意。 虽然 Pod 是 Kubernetes 的核心概念，非常重要，但事实上在 Kubernetes 里通常并不会直接创建 Pod，因为它只是对容器做了简单的包装，比较脆弱，离复杂的业务需求还有些距离，需要 Job、CronJob、Deployment 等其他对象增添更多的功能才能投入生产使用。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:16:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"13｜Job/CronJob：为什么不直接用Pod来处理业务？ 在上次的课里我们学习了 Kubernetes 的核心对象 Pod，用来编排一个或多个容器，让这些容器共享网络、存储等资源，总是共同调度，从而紧密协同工作。因为 Pod 比容器更能够表示实际的应用，所以 Kubernetes 不会在容器层面来编排业务，而是把 Pod 作为在集群里调度运维的最小单位。前面我们也看到了一张 Kubernetes 的资源对象关系图，以 Pod 为中心，延伸出了很多表示各种业务的其他资源对象。那么你会不会有这样的疑问：Pod 的功能已经足够完善了，为什么还要定义这些额外的对象呢？为什么不直接在 Pod 里添加功能，来处理业务需求呢？这个问题体现了 Google 对大规模计算集群管理的深度思考，今天我就说说 Kubernetes 基于 Pod 的设计理念，先从最简单的两种对象——Job 和 CronJob 讲起。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:17:0","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"为什么不直接使用 Pod 现在你应该知道，Kubernetes 使用的是 RESTful API，把集群中的各种业务都抽象为 HTTP 资源对象，那么在这个层次之上，我们就可以使用面向对象的方式来考虑问题。如果你有一些编程方面的经验，就会知道面向对象编程（OOP），它把一切都视为高内聚的对象，强调对象之间互相通信来完成任务。虽然面向对象的设计思想多用于软件开发，但它放到 Kubernetes 里却意外地合适。因为 Kubernetes 使用 YAML 来描述资源，把业务简化成了一个个的对象，内部有属性，外部有联系，也需要互相协作，只不过我们不需要编程，完全由 Kubernetes 自动处理（其实 Kubernetes 的 Go 语言内部实现就大量应用了面向对象）。面向对象的设计有许多基本原则，其中有两条我认为比较恰当地描述了 Kubernetes 对象设计思路，一个是“单一职责”，另一个是“组合优于继承”。 “单一职责”的意思是对象应该只专注于做好一件事情，不要贪大求全，保持足够小的粒度才更方便复用和管理。“组合优于继承”的意思是应该尽量让对象在运行时产生联系，保持松耦合，而不要用硬编码的方式固定对象的关系。应用这两条原则，我们再来看 Kubernetes 的资源对象就会很清晰了。因为 Pod 已经是一个相对完善的对象，专门负责管理容器，那么我们就不应该再“画蛇添足”地盲目为它扩充功能，而是要保持它的独立性，容器之外的功能就需要定义其他的对象，把 Pod 作为它的一个成员“组合”进去。这样每种 Kubernetes 对象就可以只关注自己的业务领域，只做自己最擅长的事情，其他的工作交给其他对象来处理，既不“缺位”也不“越位”，既有分工又有协作，从而以最小成本实现最大收益。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:17:1","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"为什么要有 Job/CronJob 现在我们来看看 Kubernetes 里的两种新对象：Job 和 CronJob，它们就组合了 Pod，实现了对离线业务的处理。上次课讲 Pod 的时候我们运行了两个 Pod：Nginx 和 busybox，它们分别代表了 Kubernetes 里的两大类业务。一类是像 Nginx 这样长时间运行的“在线业务”，另一类是像 busybox 这样短时间运行的“离线业务”。“在线业务”类型的应用有很多，比如 Nginx、Node.js、MySQL、Redis 等等，一旦运行起来基本上不会停，也就是永远在线。而“离线业务”类型的应用也并不少见，它们一般不直接服务于外部用户，只对内部用户有意义，比如日志分析、数据建模、视频转码等等，虽然计算量很大，但只会运行一段时间。“离线业务”的特点是必定会退出，不会无期限地运行下去，所以它的调度策略也就与“在线业务”存在很大的不同，需要考虑运行超时、状态检查、失败重试、获取计算结果等管理事项。而这些业务特性与容器管理没有必然的联系，如果由 Pod 来实现就会承担不必要的义务，违反了“单一职责”，所以我们应该把这部分功能分离到另外一个对象上实现，让这个对象去控制 Pod 的运行，完成附加的工作。 “离线业务”也可以分为两种。一种是“临时任务”，跑完就完事了，下次有需求了说一声再重新安排；另一种是“定时任务”，可以按时按点周期运行，不需要过多干预。对应到 Kubernetes 里，“临时任务”就是 API 对象 Job，“定时任务”就是 API 对象 CronJob，使用这两个对象你就能够在 Kubernetes 里调度管理任意的离线业务了。由于 Job 和 CronJob 都属于离线业务，所以它们也比较相似。我们先学习通常只会运行一次的 Job 对象以及如何操作。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:17:2","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何使用 YAML 描述 Job Job 的 YAML“文件头”部分还是那几个必备字段，我就不再重复解释了，简单说一下：apiVersion 不是 v1，而是 batch/v1。kind 是 Job，这个和对象的名字是一致的。metadata 里仍然要有 name 标记名字，也可以用 labels 添加任意的标签。 如果记不住这些也不要紧，你还可以使用命令 kubectl explain job 来看它的字段说明。不过想要生成 YAML 样板文件的话不能使用 kubectl run，因为 kubectl run 只能创建 Pod，要创建 Pod 以外的其他 API 对象，需要使用命令 kubectl create，再加上对象的类型名。比如用 busybox 创建一个“echo-job”，命令就是这样的： export out=\"--dry-run=client -o yaml\" # 定义Shell变量 kubectl create job echo-job --image=busybox $out 会生成一个基本的 YAML 文件，保存之后做点修改，就有了一个 Job 对象： apiVersion: batch/v1 kind: Job metadata: name: echo-job spec: template: spec: restartPolicy: OnFailure containers: - image: busybox name: echo-job imagePullPolicy: IfNotPresent command: [\"/bin/echo\"] args: [\"hello\", \"world\"] 你会注意到 Job 的描述与 Pod 很像，但又有些不一样，主要的区别就在“spec”字段里，多了一个 template 字段，然后又是一个“spec”，显得有点怪。如果你理解了刚才说的面向对象设计思想，就会明白这种做法的道理。它其实就是在 Job 对象里应用了组合模式，template 字段定义了一个“应用模板”，里面嵌入了一个 Pod，这样 Job 就可以从这个模板来创建出 Pod。而这个 Pod 因为受 Job 的管理控制，不直接和 apiserver 打交道，也就没必要重复 apiVersion 等“头字段”，只需要定义好关键的 spec，描述清楚容器相关的信息就可以了，可以说是一个“无头”的 Pod 对象。为了辅助你理解，我把 Job 对象重新组织了一下，用不同的颜色来区分字段，这样你就能够很容易看出来，其实这个“echo-job”里并没有太多额外的功能，只是把 Pod 做了个简单的包装： 总的来说，这里的 Pod 工作非常简单，在 containers 里写好名字和镜像，command 执行 /bin/echo，输出“hello world”。不过，因为 Job 业务的特殊性，所以我们还要在 spec 里多加一个字段 restartPolicy，确定 Pod 运行失败时的策略，OnFailure 是失败原地重启容器，而 Never 则是不重启容器，让 Job 去重新调度生成一个新的 Pod。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:17:3","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何在 Kubernetes 里操作 Job 现在让我们来创建 Job 对象，运行这个简单的离线作业，用的命令还是 kubectl apply： kubectl apply -f job.yml 创建之后 Kubernetes 就会从 YAML 的模板定义中提取 Pod，在 Job 的控制下运行 Pod，你可以用 kubectl get job、kubectl get pod 来分别查看 Job 和 Pod 的状态： kubectl get job kubectl get pod 可以看到，因为 Pod 被 Job 管理，它就不会反复重启报错了，而是会显示为 Completed 表示任务完成，而 Job 里也会列出运行成功的作业数量，这里只有一个作业，所以就是 1/1。你还可以看到，Pod 被自动关联了一个名字，用的是 Job 的名字（echo-job）再加上一个随机字符串（pb5gh），这当然也是 Job 管理的“功劳”，免去了我们手工定义的麻烦，这样我们就可以使用命令 kubectl logs 来获取 Pod 的运行结果： 到这里，你可能会觉得，经过了 Job、Pod 对容器的两次封装，虽然从概念上很清晰，但好像并没有带来什么实际的好处，和直接跑容器也差不了多少。其实 Kubernetes 的这套 YAML 描述对象的框架提供了非常多的灵活性，可以在 Job 级别、Pod 级别添加任意的字段来定制业务，这种优势是简单的容器技术无法相比的。这里我列出几个控制离线作业的重要字段，其他更详细的信息可以参考 Job 文档： activeDeadlineSeconds，设置 Pod 运行的超时时间。backoffLimit，设置 Pod 的失败重试次数。completions，Job 完成需要运行多少个 Pod，默认是 1 个。parallelism，它与 completions 相关，表示允许并发运行的 Pod 数量，避免过多占用资源。 要注意这 4 个字段并不在 template 字段下，而是在 spec 字段下，所以它们是属于 Job 级别的，用来控制模板里的 Pod 对象。下面我再创建一个 Job 对象，名字叫“sleep-job”，它随机睡眠一段时间再退出，模拟运行时间较长的作业（比如 MapReduce）。Job 的参数设置成 15 秒超时，最多重试 2 次，总共需要运行完 4 个 Pod，但同一时刻最多并发 2 个 Pod： apiVersion: batch/v1 kind: Job metadata: name: sleep-job spec: activeDeadlineSeconds: 15 backoffLimit: 2 completions: 4 parallelism: 2 template: spec: restartPolicy: OnFailure containers: - image: busybox name: echo-job imagePullPolicy: IfNotPresent command: - sh - -c - sleep $(($RANDOM % 10 + 1)) \u0026\u0026 echo done 使用 kubectl apply 创建 Job 之后，我们可以用 kubectl get pod -w 来实时观察 Pod 的状态，看到 Pod 不断被排队、创建、运行的过程： kubectl apply -f sleep-job.yml kubectl get pod -w 等到 4 个 Pod 都运行完毕，我们再用 kubectl get 来看看 Job 和 Pod 的状态： 就会看到 Job 的完成数量如同我们预期的是 4，而 4 个 Pod 也都是完成状态。显然，“声明式”的 Job 对象让离线业务的描述变得非常直观，简单的几个字段就可以很好地控制作业的并行度和完成数量，不需要我们去人工监控干预，Kubernetes 把这些都自动化实现了。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:17:4","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"如何使用 YAML 描述 CronJob 学习了“临时任务”的 Job 对象之后，再学习“定时任务”的 CronJob 对象也就比较容易了，我就直接使用命令 kubectl create 来创建 CronJob 的样板。要注意两点。第一，因为 CronJob 的名字有点长，所以 Kubernetes 提供了简写 cj，这个简写也可以使用命令 kubectl api-resources 看到；第二，CronJob 需要定时运行，所以我们在命令行里还需要指定参数 –schedule。 export out=\"--dry-run=client -o yaml\" # 定义Shell变量 kubectl create cj echo-cj --image=busybox --schedule=\"\" $out 然后我们编辑这个 YAML 样板，生成 CronJob 对象： apiVersion: batch/v1 kind: CronJob metadata: name: echo-cj spec: schedule: '*/1 * * * *' jobTemplate: spec: template: spec: restartPolicy: OnFailure containers: - image: busybox name: echo-cj imagePullPolicy: IfNotPresent command: [\"/bin/echo\"] args: [\"hello\", \"world\"] 我们还是重点关注它的 spec 字段，你会发现它居然连续有三个 spec 嵌套层次：第一个 spec 是 CronJob 自己的对象规格声明第二个 spec 从属于“jobTemplate”，它定义了一个 Job 对象。第三个 spec 从属于“template”，它定义了 Job 里运行的 Pod。 所以，CronJob 其实是又组合了 Job 而生成的新对象，我还是画了一张图，方便你理解它的“套娃”结构： 除了定义 Job 对象的“jobTemplate”字段之外，CronJob 还有一个新字段就是“schedule”，用来定义任务周期运行的规则。它使用的是标准的 Cron 语法，指定分钟、小时、天、月、周，和 Linux 上的 crontab 是一样的。像在这里我就指定每分钟运行一次，格式具体的含义你可以课后参考 Kubernetes 官网文档。除了名字不同，CronJob 和 Job 的用法几乎是一样的，使用 kubectl apply 创建 CronJob，使用 kubectl get cj、kubectl get pod 来查看状态： kubectl apply -f cronjob.yml kubectl get cj kubectl get pod ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:17:5","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["k8s"],"content":"小结 好了，今天我们以面向对象思想分析了一下 Kubernetes 里的资源对象设计，它强调“职责单一”和“对象组合”，简单来说就是“对象套对象”。通过这种嵌套方式，Kubernetes 里的这些 API 对象就形成了一个“控制链”：CronJob 使用定时规则控制 Job，Job 使用并发数量控制 Pod，Pod 再定义参数控制容器，容器再隔离控制进程，进程最终实现业务功能，层层递进的形式有点像设计模式里的 Decorator（装饰模式），链条里的每个环节都各司其职，在 Kubernetes 的统一指挥下完成任务。小结一下今天的内容： Pod 是 Kubernetes 的最小调度单元，但为了保持它的独立性，不应该向它添加多余的功能。Kubernetes 为离线业务提供了 Job 和 CronJob 两种 API 对象，分别处理“临时任务”和“定时任务”。Job 的关键字段是 spec.template，里面定义了用来运行业务的 Pod 模板，其他的重要字段有 completions、parallelism 等CronJob 的关键字段是 spec.jobTemplate 和 spec.schedule，分别定义了 Job 模板和定时运行的规则。 ","date":"2022-07-14 10:38:08","objectID":"/k8s_base/:17:6","tags":["k8s"],"title":"K8s_base","uri":"/k8s_base/"},{"categories":["interview"],"content":"【Golang开发面经】百度（三轮技术面） ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:0:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"写在前面 百度一顿面试下来感觉挺不错的，面试官水平很高，不愧是互联网的黄埔军校，技术都很硬。可能是我项目讲的不好吧，最终挂了。 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:1:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"笔试 略 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:2:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"一面 一直深挖项目，挖了快半小时。然后再写两道题，最后再问一些简单的问题。 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:3:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"算法：判断是否为镜面二叉树 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:4:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"算法：二叉树的俯视图 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:5:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"一个协程被网络io卡住了，对应的线程会不会卡住？ 不会。因为都用epoll那是非阻塞调用，网络io和系统调用不一样的处理方式。网络io 是利用非阻塞，系统调用会创建新的线程来接管其他 goroutine。 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:6:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"go 里面 make 和 new 有什么区别？ make 一般用来创建引用类型 slice、map 以及 channel 等等，并且是非零值的。而new 用于类型的内存分配，并且内存置为零。make 返回的是引用类型本身；而 new 返回的是指向类型的指针。 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:7:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"map 是怎么实现的？ map 的底层是一个结构体 // Go map 的底层结构体表示 type hmap struct { count int // map中键值对的个数，使用len()可以获取 flags uint8 B uint8 // 哈希桶的数量的log2，比如有8个桶，那么B=3 noverflow uint16 // 溢出桶的数量 hash0 uint32 // 哈希种子 buckets unsafe.Pointer // 指向哈希桶数组的指针，数量为 2^B oldbuckets unsafe.Pointer // 扩容时指向旧桶的指针，当扩容时不为nil nevacuate uintptr extra *mapextra // 可选字段 } const ( bucketCntBits = 3 bucketCnt = 1 \u003c\u003c bucketCntBits // 桶数量 1 \u003c\u003c 3 = 8 ) // Go map 的一个哈希桶，一个桶最多存放8个键值对 type bmap struct { // tophash存放了哈希值的最高字节 tophash [bucketCnt]uint8 // 在这里有几个其它的字段没有显示出来，因为k-v的数量类型是不确定的，编译的时候才会确定 // keys: 是一个数组，大小为bucketCnt=8，存放Key // elems: 是一个数组，大小为bucketCnt=8，存放Value // 你可能会想到为什么不用空接口，空接口可以保存任意类型。但是空接口底层也是个结构体，中间隔了一层。因此在这里没有使用空接口。 // 注意：之所以将所有key存放在一个数组，将value存放在一个数组，而不是键值对的形式，是为了消除例如map[int64]所需的填充整数8（内存对齐） // overflow: 是一个指针，指向溢出桶，当该桶不够用时，就会使用溢出桶 } 当向 map 中存储一个 kv 时，通过 k 的 hash 值与 buckets 长度取余，定位到 key 在哪一个bucket中，hash 值的高8位存储在 bucket 的 tophash[i] 中，用来快速判断 key是否存在。当一个 bucket 满时，通过 overflow 指针链接到下一个 bucket。 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:8:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"二面 又深挖项目，这次挖了快40分钟了。。 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:9:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"go里面 slice 和 array 有区别吗？ slice， 是切片，是引用类型，长度可变。 array，是数组，是值类型，长度不可变。 slice的底层其实是基于array 实现的。 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:10:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"归并排序是稳定的吗？时间复杂度是多少？ 是的，归并排序中相等元素的顺序不会改变。 总时间 = 分解时间+解决问题时间+合并时间。 分解时间就是把一个待排序序列分解成两序列，时间为一常数，时间复杂度o(1)。 解决问题时间是两个递归式，把一个规模为n 的问题分成两个规模分别为 n/2 的子问题，时间为2T(n/2)。 合并时间复杂度为o(n)。 总时间T(n)=2T(n/2)+o(n)，这个递归式可以用递归树来解，其解是o(nlogn)。 所以是O(nlogn) ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:11:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"写一个归并排序吧 func mergeSort(arr *[]int, l int, r int) { if l \u003e= r { //不可再分隔,只有一个元素 return } mid := (l+r)/2 mergeSort(arr,l,mid) mergeSort(arr,mid+1,r) if (*arr)[mid] \u003e (*arr)[mid+1] { merge(arr, l, mid, r) } } //将arr[l...mid]和arr[mid+1...r]两部分进行归并 func merge(arr *[]int, l int, mid int, r int) { //先把arr中[l..r]区间的值copy一份到arr2 //注意:这里可优化，copyarr 可改为长度为r-l+1的数组,下面的赋值等操作按偏移量l来修改即可， copyarr := make([]int, r+1) for index := l; index \u003c= r; index++ { copyarr[index] = (*arr)[index] } //定义要合并的两个子数组各自目前数组内还没被合并的首位数字下标为i,j //初始化i，j i := l j := mid+1 //遍历并逐个确定数组[l,r]区间内数字的顺序 for k := l; k \u003c= r; k++ { //防止i/j\"越界\"，应该先判断i和j的下变是否符合条件（因为两个子数组应该符合i\u003c=mid j\u003c=r） if i \u003e mid { (*arr)[k] = copyarr[j] j++ }else if j \u003e r { (*arr)[k] = copyarr[i] i++ }else if copyarr[i] \u003c copyarr[j] { (*arr)[k] = copyarr[i] i++ }else{ (*arr)[k] = copyarr[j] j++ } } } func TestMerge(t *testing.T) { arr := []int{8,6,2,3,1,5,7,4} mergeSort(\u0026arr, 0, len(arr)-1) fmt.Println(arr) } ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:12:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"空chan和关闭的chan进行读写会怎么样？ 空chan 读会读取到该chan类型的零值。 写会直接写到chan中。 关闭的chan 读已经关闭的 chan 能一直读到东西，但是读到的内容根据通道内关闭前是否有元素而不同。如果有元素，就继续读剩下的元素，如果没有就是这个chan类型的零值，比如整型是 int，字符串是\"\" 。 写已经关闭的 chan 会 panic。因为源码上面就是这样写的，可以看src/runtime/chan.go ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:13:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"三面 感觉就走个过场。。面完简历就共享了。。 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:14:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"讲讲redis分布式锁的设计与实现 这篇博客很详细了，可以看这篇博客 redis分布式锁实现 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:15:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"讲讲redis的哨兵模式 一主两从三哨兵集群，当 master 节点宕机时，通过哨兵(sentinel)重新推选出新的master节点，保证集群的可用性。 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:16:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"限流算法有哪些？令牌桶算法怎么实现？ 限流算法常见有计数器算法，滑动窗口，令牌桶算法。 令牌桶算法是比较常见的限流算法之一，如下： 所有的请求在处理之前都需要拿到一个可用的令牌才会被处理； 根据限流大小，设置按照一定的速率往桶里添加令牌； 桶设置最大的放置令牌限制，当桶满时、新添加的令牌就被丢弃或者拒绝； 请求达到后首先要获取令牌桶中的令牌，拿着令牌才可以进行其他的业务逻辑，处理完业务逻辑之后，将令牌直接删除； 令牌桶有最低限额，当桶中的令牌达到最低限额的时候，请求处理完之后将不会删除令牌，以此保证足够的限流； ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:17:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"算法：交换二叉树左右节点 ","date":"2022-06-28 15:30:06","objectID":"/c.-%E7%99%BE%E5%BA%A6/:18:0","tags":["interview"],"title":"interview-bd","uri":"/c.-%E7%99%BE%E5%BA%A6/"},{"categories":["interview"],"content":"【Golang开发面经】B站（两轮技术面） ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:0:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"写在前面 面试下来我感觉我都讲出来了，算法题也写出来了，但是二面完一查结果就直接淘汰了。这个B是不是不招人啊。。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:1:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"笔试 略 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:2:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"一面 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:3:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"Go的GMP模型 G：表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P：表示逻辑processor，P 的数量决定了系统内最大可并行的 G 的数量（前提：系统的物理cpu核数 \u003e= P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M：M 代表着真正的执行计算资源，物理 Processor。 G 如果想运行起来必须依赖 P，因为 P 是它的逻辑处理单元，但是 P 要想真正的运行，他也需要与 M 绑定，这样才能真正的运行起来，P 和 M 的这种关系就相当于 Linux 系统中的用户层面的线程和内核的线程是一样的。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:4:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"GO的GC Go是采用三色标记法来进行垃圾回收的，是传统 Mark-Sweep 的一个改进，它是一个并发的 GC 算法。on-the-fly 原理如下 整个进程空间里申请每个对象占据的内存可以视为一个图， 初始状态下每个内存对象都是白色标记。 先stop the world，将扫描任务作为多个并发的goroutine立即入队给调度器，进而被CPU处理，第一轮先扫描所有可达的内存对象，标记为灰色放入队列 第二轮可以恢复start the world，将第一步队列中的对象引用的对象置为灰色加入队列，一个对象引用的所有对象都置灰并加入队列后，这个对象才能置为黑色并从队列之中取出。循环往复，最后队列为空时，整个图剩下的白色内存空间即不可到达的对象，即没有被引用的对象； 第三轮再次stop the world，将第二轮过程中新增对象申请的内存进行标记（灰色），这里使用了writebarrier（写屏障）去记录这些内存的身份； 这个算法可以实现 on-the-fly，也就是在程序执行的同时进行收集，并不需要暂停整个程序。 简化步骤如下： 1、首先创建三个集合：白、灰、黑。 2、将所有对象放入白色集合中。 3、然后从根节点开始遍历所有对象（注意这里并不递归遍历），把遍历到的对象从白色集合放入灰色集合。 因为root set 指向了A、F，所以从根结点开始遍历的是A、F，所以是把A、F放到灰色集合中。 4、之后遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合 我们可以发现这个A指向了B，C，D所以也就是把BCD放到灰色中，把A放到黑色中，而F没有指任何的对象，所以直接放到黑色中。 5、重复 4 直到灰色中无任何对象 因为D指向了A所以D也放到了黑色中，而B和C能放到黑色集合中的道理和F一样，已经没有了可指向的对象了。 6、通过write-barrier检测对象有无变化，重复以上操作 由于这个EGH并没有和RootSet有直接或是间接的关系，所以就会被清除。 7、收集所有白色对象（垃圾） 所以我们可以看出这里的情况，只要是和root set根集合直接相关的对象或是间接相关的对象都不会被清楚。只有不相关的才会被回收。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:5:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"Go的map底层是怎么实现的？ map 的底层是一个结构体 // Go map 的底层结构体表示 type hmap struct { count int // map中键值对的个数，使用len()可以获取 flags uint8 B uint8 // 哈希桶的数量的log2，比如有8个桶，那么B=3 noverflow uint16 // 溢出桶的数量 hash0 uint32 // 哈希种子 buckets unsafe.Pointer // 指向哈希桶数组的指针，数量为 2^B oldbuckets unsafe.Pointer // 扩容时指向旧桶的指针，当扩容时不为nil nevacuate uintptr extra *mapextra // 可选字段 } const ( bucketCntBits = 3 bucketCnt = 1 \u003c\u003c bucketCntBits // 桶数量 1 \u003c\u003c 3 = 8 ) // Go map 的一个哈希桶，一个桶最多存放8个键值对 type bmap struct { // tophash存放了哈希值的最高字节 tophash [bucketCnt]uint8 // 在这里有几个其它的字段没有显示出来，因为k-v的数量类型是不确定的，编译的时候才会确定 // keys: 是一个数组，大小为bucketCnt=8，存放Key // elems: 是一个数组，大小为bucketCnt=8，存放Value // 你可能会想到为什么不用空接口，空接口可以保存任意类型。但是空接口底层也是个结构体，中间隔了一层。因此在这里没有使用空接口。 // 注意：之所以将所有key存放在一个数组，将value存放在一个数组，而不是键值对的形式，是为了消除例如map[int64]所需的填充整数8（内存对齐） // overflow: 是一个指针，指向溢出桶，当该桶不够用时，就会使用溢出桶 } 当向 map 中存储一个 kv 时，通过 k 的 hash 值与 buckets 长度取余，定位到 key 在哪一个bucket中，hash 值的高8位存储在 bucket 的 tophash[i] 中，用来快速判断 key是否存在。当一个 bucket 满时，通过 overflow 指针链接到下一个 bucket。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:6:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"遍历map是有序的吗？为什么？ 不是有序的，使用 range 多次遍历 map 时输出的 key 和 value 的顺序可能不同，map在遍历时，并不是从固定的0号bucket开始遍历的，每次遍历，都会从一个随机值序号的bucket，再从其中随机的 cell 开始遍历。map 遍历时，是按序遍历 bucket，同时按需遍历 bucket 和其 overflow bucket 中 的 cell。 但是 map 在扩容后，会发生 key 的搬迁，这造成原来落在一个 bucket 中的 key，搬迁后，有可能会落到其他 bucket 中了，从这个角度看，遍历 map 的结果就不可能是按照原来的顺序了。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:7:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"map作为函数是什么传递? map 传的是地址值 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:8:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"在函数里面修改map会影响原来的吗？ 会的，因为传递的map的地址，会对原来的map进行修改。 func TestMap(t *testing.T) { a := make(map[int]int) a[0] = 1 fmt.Println(a) changeMap(a) fmt.Println(a) } func changeMap(b map[int]int) { b[0] = 2 } 结果如下 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:9:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"那数组呢？ 数组不会，数组不是引用类型，传的值传递，并不是应用传递。 func TestArray(t *testing.T) { a := [3]int{1, 2, 3} fmt.Println(a) changeArray(a) fmt.Println(a) } func changeArray(a [3]int) { a[0] = 1 } ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:10:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"切片呢？ 会，和 map 一样，都是引用类型。传递的是地址。 func TestSlice(t *testing.T) { a := make([]int, 3, 3) a[0] = 1 a[1] = 2 a[2] = 3 fmt.Println(a) changeSlice(a) fmt.Println(a) } func changeSlice(a []int) { a[0] = 4 } ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:11:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"linux有用过是吧？如何查看一个服务是否在运行？ 如果我们知道服务名，我们可以使用 ps 命令： ps -ef | grep 服务名 或 ps aux |grep 服务名 如果知道端口号，我们可以使用 lsof 命令：lsof -i:端口号 或者也可以用这个：systemctl status 服务名 或 service 服务名 status 如果是基于 tcp 的连接，还可以使用 netstat 查看端口和进程等相关内容。netstat -tnlp\r ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:12:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"我只知道这个文件名，能找到这个文件在哪里吗？ 可以使用 find 命令 find / -name \"main.go\" ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:13:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"用过mysql是吧？mysql 索引说一下？ mysql 的索引包括** 基于InnoDB的聚集索引、基于MyISAM的非聚集索引、primary key 主键索引、secondary key 次要索引** 基于InnoDB的聚集索引：主键索引(聚集索引)的叶子结点会存储数据行，也就是说数据和索引在一起，辅助索引只会存储主键值 基于MyISAM的非聚集索引：B+树叶子结点只会存储数据行（数据文件）的指针，简单来说就是数据和索引不在一起，非聚集索引包含 主键索引 和 辅助索引 到会存储指针的值。 primary key 主键索引：InnoDB要求表必须有主键(MyISAM可以没有)，如果没有，MySQL系统会自动选择一个唯一标识数据记录的列作为主键。 secondary key 次要索引：结构和主键搜索引没有任何区别，同样用 B+Tree，data域存储相应记录主键的值而不是地址。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:14:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"死锁是怎么产生的 产生死锁就有四个必要条件：互斥条件、请求和保持条件、不剥夺条件、环路等待条件。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:15:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"了解分布式锁吗？讲讲红锁？ 只磕磕绊绊讲了一些分布式锁，红锁就不太记得了（ 可以看看这篇博客 基于Go语言的分布式红锁 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:16:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"算法：反转链表。这写不出来的话，就说不过去了。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:17:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"二面 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:18:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"TCP和UDP区别？ TCP 是可靠传输，面向连接，基于流，占用资源多，效率低。 UDP是尽最大努力交付，基于无连接，基于报文，UDP 占用系统资源较少，效率高。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:19:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"TCP是可靠传输，为什么还有丢包的情况？ 丢包是网络问题，TCP的可靠是可靠在如果发生丢包，那么会立即重传报文段。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:20:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"UDP能实现可靠传输吗？怎么实现？ 可以的，我们只需要仿照TCP的可靠传输机制就可以了，比如说设置ACK确认机制，一旦没有收到，或是收到三次上一个报文的ACK，我们就立即重传丢失的报文。再比如说设置滑动窗口来保证数据传输的安全性等等… ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:21:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"TCP和IP的区别是什么？ TCP 是传输控制协议（Transmission Control Protocal），是基于IP的传输层协议，是传输层的，IP 是因特网协议（Internet Protocol）在网络层的。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:22:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"四次挥手的细节？ 数据传输完毕之后，通信的双方都可释放连接。现在A和B都处于ESTABLISHED状态。 A的应用进程先向TCP发出连接释放报文段，并停止再发送数据，主动关闭TCP连接。A把链接释放报文段首部的终止控制位FIN置为1，其序号为seq=u，它等于前面以传送过的数据的最后一个字节的序号加1.这时候A进入了FIN-WAIT-1(终止等待1)状态，等待B的确认。 注意：TCP规定，FIN报文段即使不携带数据，他也消耗掉一个序号！！ B 收到链接释放报文段后即发出确认，确认号是ack = u + 1，而这个报文段自己的序号是v，等于B前面已传送过的数据的最后一个字节的序号加1.然后B就进入CLOSE-WAIT(关闭等待)状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的链接就释放了，这时的TCP链接处于半关闭状态，即A已经没有数据要发送了，但B若发送数据，A仍要接收，也就是说，从B到A这个方向的连接并未关闭。这个状态可能要维持一段时间。 A收到来自B的确认后，就进入了FIN-WAIT-2(终止等待2)状态满等待B发出的连接释放报文段。若B已经没有要向A发送的数据，其应用进程就通知TCP释放连接，这时B发出的连接释放报文段必须使FIN = 1，现假定B的序号为w(在半关闭状态B可能又发送了一些数据)。B还必须重复上次已发送过的确认号ack = u + 1.这时B就进入LAST-ACK(最后确认)状态，等待A的确认。 A在收到了B的链接释放报文段后，必须对此发出确认。在确认报文段中把ACK置1，确认号ack=w+1，而自己的序号是seq=u+1(根据TCP标准，前面发送过的FIN报文段要消耗一个序号)。然后进入到TIME-WAIT(时间等待)状态。注意： 现在TCP连接还没有还没有释放掉。必须经过时间等待计时器设置的时间2MSL后，A才能进入CLOSED状态。 时间MSL叫做最长报文段寿命，RFC793建议设在两分钟。但是在现在工程来看两分钟太长了，所以TCP允许不同的实现可以根据具体情况使用更小的MSL值。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:23:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"为什么 time_wait 是2MLS? 为了保证A发送的最后一个ACK报文段能够到达B。 这个ACK报文段有可能丢失，因而使处于在 LAST-ASK 状态的B收不到对己发送的 FIN-ACK 报文段的确认。B会超时重传这个FIN+ACK报文段，而A就能在 2MSL 时间内收到这个重传的FIN+ACK报文段。而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。接着A重传一次确认，重新启动2MSL计时器。最后的A和B都正常进入CLOSED状态。如果A在TIME-WAIT状态不等待一段时间，而实发送完ACK报文段后立即释放连接，那么就无法收到B重传的FIN+ACK报文段，因而也不会再发送一次确认报文段。 防止了“已失效的连接请求报文段”。 A在发送完最后一个ACK报文段后，在经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，这样就可以使下一个连接中不会出现这种旧的连接请求报文段。B只要收到了A发出的确认，就进入CLOSED状态。同样，B在撤销相应的传输控制块TCB后，就结束了这次的TCP连接。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:24:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"大量处于 close wait 的是什么场景？ 如何解决？ 通常出现大量的CLOSE_WAIT，说明Server端没有发起close()操作，这基本上是用户server 端程序的问题了； 通常情况下，Server都是等待Client访问，如果Client退出请求关闭连接，server端自觉close()对应的连接。 一般是程序 Bug，或者关闭 socket 不及时。服务端接口耗时较长，客户端主动断开了连接，此时，服务端就会出现 close_wait。 这个我们就只能检查自己代码了，用netstat或是其他工具，检测代码为啥耗时长。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:25:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"cookie和session有什么区别？ cookie和session的共同之处在于：cookie和session都是用来跟踪浏览器用户身份的会话方式。cookie数据保存在客户端，session数据保存在服务器端。 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:26:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"你是用go的是吧？chan用过吧？那说说对一个关闭的chan做读写会发生什么操作？为什么？ 读已经关闭的 chan 能一直读到东西，但是读到的内容根据通道内关闭前是否有元素而不同。如果有元素，就继续读剩下的元素，如果没有就是这个chan类型的零值，比如整型是 int，字符串是 \"\"。 写已经关闭的 chan 会 panic。因为源码上面就是这样写的，可以看src/runtime/chan.go ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:27:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"map 的底层说一下？ 看上面 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:28:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"如果你这个项目，我突然有一个时间段，多了很多流量，要怎么处理? 我们要延长一些 token，cookie的设置时间，或是设置这些过期时间不一样，防止缓存雪崩的情况。 设置布隆过滤器，防止缓冲击穿情况。 使用 nginx 进行 http/https 的流量分发。使用轮询，随机，哈希，一致性哈希等等进行负载均衡等等… 提高服务自身性能，比如sql的索引，语法层面的调参等等… 引入CDN进行加速。 提高服务器配置。 … ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:29:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"算法：连续子序列的最大和 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:30:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"参考资料 [1] https://blog.csdn.net/Peerless__/article/details/125458742 ","date":"2022-06-28 15:30:06","objectID":"/b.-b%E7%AB%99/:31:0","tags":["interview"],"title":"interview-bz","uri":"/b.-b%E7%AB%99/"},{"categories":["interview"],"content":"【Golang开发面经】滴滴（三轮技术面） ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:0:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"写在前面 滴滴面试赶紧感觉还行吧，挺注重基础的，很多时间都花在了挖项目上面，所以大家一定要很熟悉自己的项目！面试官水平也很高。不经感叹这个曾经的大厂现在变成这个样子，唉。。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:1:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"笔试 略 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:2:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"一面 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:3:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"进程间通信方式 管道、消息队列、信号量、共享内存 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:4:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"栈上分配内存快还是堆上，为什么? 显然从栈上分配内存更快，因为从栈上分配内存仅仅就是栈指针的移动而已 操作系统会在底层对栈提供支持，会分配专门的寄存器存放栈的地址。 栈的入栈出栈操作也十分简单，并且有专门的指令执行，所以栈的效率比较高也比较快。 堆的生长空间向上，地址越来越大，栈的生长空间向下，地址越来越小。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:5:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"channel 底层 type hchan struct { qcount uint // channel 里的元素计数 dataqsiz uint // 可以缓冲的数量，如 ch := make(chan int, 10)。 此处的 10 即 dataqsiz elemsize uint16 // 要发送或接收的数据类型大小 buf unsafe.Pointer // 当 channel 设置了缓冲数量时，该 buf 指向一个存储缓冲数据的区域，该区域是一个循环队列的数据结构 closed uint32 // 关闭状态 sendx uint // 当 channel 设置了缓冲数量时，数据区域即循环队列此时已发送数据的索引位置 recvx uint // 当 channel 设置了缓冲数量时，数据区域即循环队列此时已接收数据的索引位置 recvq waitq // 想读取数据但又被阻塞住的 goroutine 队列 sendq waitq // 想发送数据但又被阻塞住的 goroutine 队列 lock mutex ... } ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:6:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"七层模型 从上而下分别是 应用层，表示层，会话层，传输层，网络层，数据链路层，物理层等等… ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:7:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"tcp、udp TCP 是可靠传输，面向连接，基于流，占用资源多，效率低。 UDP是尽最大努力交付，基于无连接，基于报文，UDP 占用系统资源较少，效率高。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:8:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"redis持久化的方式以及使用场景 Redis提供 RDB 和 AOF 两种持久化机制 ， 有了持久化机制我们基本上就可以避免进程异常退出时所造成的数据丢失的问题了，Redis能在下一次重启的时候利用之间产生的持久化文件实现数据恢复。 RDB持久化就是指的讲当前进程的数据生成快照存入到磁盘中，触发RDB机制又分为手动触发与自动触发。 AOF 持久化是以独立的日志记录每次写命令，重启 Redis 的时候再重新执行AOF文件中命令以达到恢复数据，所以AOF主要就是解决持久化的实时性。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:9:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"如何实现线程池？ 线程池有两部分组成：同步队列和线程池。 同步队列：以链表的形式创建一个同步队列，同步队列的主要工作就是通过Put()函数向队列里面添加事务，通过Get()函数从队列里面取出事务。 线程池：主要由线程组（注：线程组里面存放的时 thread 类型 的共享只能指针，指向工作的线程）构成，通过Start()函数向线程组中添加numThreads个线程，并使得每一个线程调用RunThread()函数来获取同步队列的事务并执行事务；通过Stop() 函数停止线程池的工作。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:10:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"算法：二叉树俯视图 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:11:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"二面 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:12:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"怎么判断给定ip是否在给定ip区间内？ IP地址可以转换成整数，可以将IP返回化整为整数范围进行排查。 package main import ( \"fmt\" \"strconv\" \"strings\" ) func main() { ipVerifyList := \"192.168.1.0-192.172.3.255\" ip := \"192.170.223.1\" ipSlice := strings.Split(ipVerifyList, `-`) if len(ipSlice) \u003c 0 { return } if ip2Int(ip) \u003e= ip2Int(ipSlice[0]) \u0026\u0026 ip2Int(ip) \u003c= ip2Int(ipSlice[1]) { fmt.Println(\"ip in iplist\") return } fmt.Println(\"ip not in iplist\") } func ip2Int(ip string) int64 { if len(ip) == 0 { return 0 } bits := strings.Split(ip, \".\") if len(bits) \u003c 4 { return 0 } b0 := string2Int(bits[0]) b1 := string2Int(bits[1]) b2 := string2Int(bits[2]) b3 := string2Int(bits[3]) var sum int64 sum += int64(b0) \u003c\u003c 24 sum += int64(b1) \u003c\u003c 16 sum += int64(b2) \u003c\u003c 8 sum += int64(b3) return sum } func string2Int(in string) (out int) { out, _ = strconv.Atoi(in) return } ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:13:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"如何优化慢SQL？ 查看是否使用到了索引。 查看 SQL语句 是否符合最左匹配原则。 对查询进行优化，尽可能避免全表扫描 字段冗余，减少跨库查询或多表连接操作 将一些常用的数据结构放在缓冲中（部门名字，组织架构之类的），就不需要查数据库了。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:14:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"什么情况不建议使用索引 在where条件中（包括group by以及order by）里用不到的字段不需要创建索引，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的。 数据量小的表最好不要使用索引，少于1000个 字段中如果有大量重复数据（性别），也不用创建索引 避免对经常更新的表创建过多的索引。 不建议使用无序的值作为索引。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:15:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"线程池的几种拒绝策略及其应用场景 当时都不知道这是个啥… 具体看这篇博客吧 线程池拒绝策略应用场景 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:16:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"http与tcp区别 http 就是基于传输层的 tcp 实现的一个应用层协议。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:17:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"长连接与短连接 长连接意味着进行一次数据传输后，不关闭连接，长期保持连通状态。 短连接意味着每一次的数据传输都需要建立一个新的连接，用完再马上关闭它。下次再用的时候重新建立一个新的连接，如此反复。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:18:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"讲讲tcp的挥手？ 数据传输完毕之后，通信的双方都可释放连接。现在A和B都处于ESTABLISHED状态。 A的应用进程先向TCP发出连接释放报文段，并停止再发送数据，主动关闭TCP连接。A把链接释放报文段首部的终止控制位FIN置为1，其序号为seq=u，它等于前面以传送过的数据的最后一个字节的序号加1.这时候A进入了FIN-WAIT-1(终止等待1)状态，等待B的确认。 注意：TCP规定，FIN报文段即使不携带数据，他也消耗掉一个序号！！ B 收到链接释放报文段后即发出确认，确认号是ack = u + 1，而这个报文段自己的序号是v，等于B前面已传送过的数据的最后一个字节的序号加1.然后B就进入CLOSE-WAIT(关闭等待)状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的链接就释放了，这时的TCP链接处于半关闭状态，即A已经没有数据要发送了，但B若发送数据，A仍要接收，也就是说，从B到A这个方向的连接并未关闭。这个状态可能要维持一段时间。 A收到来自B的确认后，就进入了FIN-WAIT-2(终止等待2)状态满等待B发出的连接释放报文段。若B已经没有要向A发送的数据，其应用进程就通知TCP释放连接，这时B发出的连接释放报文段必须使FIN = 1，现假定B的序号为w(在半关闭状态B可能又发送了一些数据)。B还必须重复上次已发送过的确认号ack = u + 1.这时B就进入LAST-ACK(最后确认)状态，等待A的确认。 A在收到了B的链接释放报文段后，必须对此发出确认。在确认报文段中把ACK置1，确认号ack=w+1，而自己的序号是seq=u+1(根据TCP标准，前面发送过的FIN报文段要消耗一个序号)。然后进入到TIME-WAIT(时间等待)状态。注意： 现在TCP连接还没有还没有释放掉。必须经过时间等待计时器设置的时间2MSL后，A才能进入CLOSED状态。 时间MSL叫做最长报文段寿命，RFC793建议设在两分钟。但是在现在工程来看两分钟太长了，所以TCP允许不同的实现可以根据具体情况使用更小的MSL值。 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:19:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"time wait，过多怎么办 修改TIME_WAIT连接状态的上限值 启动快速回收机制 开启复用机制 修改短连接为长连接方式 由客户端来主动断开连接 ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:20:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"算法：忘了… ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:21:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"三面 聊人生…. ","date":"2022-06-28 15:30:06","objectID":"/d.-%E6%BB%B4%E6%BB%B4/:22:0","tags":["interview"],"title":"interview-dd","uri":"/d.-%E6%BB%B4%E6%BB%B4/"},{"categories":["interview"],"content":"【Golang开发面经】字节跳动（三轮技术面） ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:0:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"写在前面 整体面试下来，感觉其实字节对于语言本身并没有很多的涉及，更加注重基础，比如数据结构与算法，计算机网络，组成原理，操作系统，数据库等等，语言本身并没有涉及太多。 这里就省去了一些我简历上的问题，也就是深挖项目。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:1:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"笔试 略 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:2:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"一面 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:3:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"epoll、select、poll 区别 select 机制刚开始的时候，需要把 fd_set 从用户空间拷贝到内核空间，并且检测的 fd 数是有限制的，由 FD_SETSIZE 设置，一般是1024。数组实现。 poll 的实现和 select 非常相似，只是描述 fd集合 的方式不同，poll使用 pollfd结构 而不是 select的 fd_set 结构，其他的都差不多。链表实现。 epol l引入了 epoll_ctl系统调用，将高频调用的 epoll_wait 和低频的 epoll_ctl 隔离开。epoll_ctl 通过(EPOLL_CTL_ADD、EPOLL_CTL_MOD、EPOLL_CTL_DEL)三个操作来分散对需要监控的fds集合的修改，做到了有变化才变更，将select或poll高频、大块内存拷贝(集中处理)变成epoll_ctl的低频、小块内存的拷贝(分散处理)，避免了大量的内存拷贝。 epoll使用 红黑树 来组织监控的fds集合 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:4:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"epoll 的水平触发和边缘触发的区别 Edge Triggered (ET) 边沿触发： socket 的接收缓冲区状态变化时触发读事件，即空的接收缓冲区刚接收到数据时触发读事件。 socket 的发送缓冲区状态变化时触发写事件，即满的缓冲区刚空出空间时触发读事件。 仅在缓冲区状态变化时触发事件。 Level Triggered (LT) 水平触发： socket 接收缓冲区不为空，有数据可读，则读事件一直触发。 socket 发送缓冲区不满可以继续写入数据，则写事件一直触发。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:5:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"TCP 的流量控制 因为我们总希望数据传输的更快一些。但如果发送方把数据发得过快，接收方就可能来不及接收，这就会造成数据的丢失。流量控制（flow control）就是让发送方的发送速率不要太快，要让接收方来得及接收。 利用滑动窗口机制可以很方便地在 TCP连接上实现发送方流量控制。通过接收方的确认报文中的窗口字段，发送方能够准确地控制发送字节数。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:6:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"为什么有了流量控制还要有拥塞控制? 流量控制是避免发送方的数据填满接收方的缓存，但并不知道网络中发生了什么。 计算机网络是处在一个共享的环境中。因此也有可能会发生网络的拥堵。在网络出现拥堵时，如果继续发送大量的数据包，可能会导致数据包时延、丢失，这时 TCP 就会重传数据，但是⼀重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包。 控制的目的就是避免发送方的数据填满整个网络。为了在发送方调节所要发送数据的数据量，定义了⼀个叫做「拥塞窗口」的概念。拥塞窗口 cwnd 是发送方维护的⼀个的状态变量，它会根据网络的拥塞程度动态变化的。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:7:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"TCP 不是可靠传输吗？为什么会丢包呢？ TCP的可靠传输是会在丢包的时候进行重传，来形成可靠的传输，丢包这是网络的问题，而不是TCP机制的问题。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:8:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"那你介绍一下拥塞控制的算法？ 拥塞控制一共有四个算法： 慢启动：TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是⼀点⼀点的提高发送数据包的数量。慢启动的算法规则：当发送方每收到⼀个 ACK，拥塞窗⼝ cwnd 的大小就会加 1。 拥塞避免：当拥塞窗口 cwnd「超过」慢启动门限 ssthresh 就会进⼊拥塞避免算法。那么进⼊拥塞避免算法后，它的规则是：每当收到⼀个 ACK 时，cwnd 增加 1/cwnd。 快重传：当接收方发现丢了⼀个中间包的时候，发送三次前⼀个包的ACK，于是发送端就会快速地重传，不必等待超时再重传。 快恢复：快重传和快恢复⼀般同时s使用，快速恢复算法是认为，你还能收到 3 个重复的 ACK 说明网络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。进⼊快速恢复之前， cwnd 和 ssthresh 已被更新了：cwnd = cwnd/2 ，也就是设置为原来的⼀半; ssthresh = cwnd 。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:9:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"进程、线程的区别 进程 线程 系统中正在运行的一个应用程序 系统分配处理器时间资源的基本单元 程序一旦运行就是进程 进程之内独立执行的一个单元执行流 资源分配的最小单位 程序执行的最小单位 进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。 线程有自己的堆栈和局部变量，但线程没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些 要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:10:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"Go里面GMP模型是怎么样的？ G：表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P：表示逻辑processor，P 的数量决定了系统内最大可并行的 G 的数量（前提：系统的物理cpu核数 \u003e= P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M：M 代表着真正的执行计算资源，物理 Processor。 G 如果想运行起来必须依赖 P，因为 P 是它的逻辑处理单元，但是 P 要想真正的运行，他也需要与 M 绑定，这样才能真正的运行起来，P 和 M 的这种关系就相当于 Linux 系统中的用户层面的线程和内核的线程是一样的。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:11:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"算法：旋转矩阵，牛客上写过，easy，秒 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:12:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"二面 上来就一个算法，结束又一个算法，难受。。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:13:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"如何用栈实现队列 我们可以通过切片来模拟出队列。 只需要完成简单的 push，pop，再判断是否为空即可！ package test_queue import ( \"fmt\" \"testing\" ) //结构体，包含两个一维切片 type GoQueue struct { stack []int back []int } //初始化， func NewQueue() GoQueue { return GoQueue{ stack: make([]int, 0), back: make([]int, 0), } } func (q *GoQueue) Push(x int) { q.stack = append(q.stack, x) } func (q *GoQueue) Pop() int { if len(q.back) == 0 { for len(q.stack) != 0 { val := q.stack[len(q.stack)-1] q.stack = q.stack[:len(q.stack)-1] //切片，更新栈 q.back = append(q.back, val) } } val := q.back[len(q.back)-1] q.back = q.back[:len(q.back)-1] return val } func (q *GoQueue) Empty() bool { return len(q.back) == 0 \u0026\u0026 len(q.stack) == 0 } func TestQueue(t *testing.T) { q := NewQueue() q.Push(1) q.Push(2) q.Push(3) for !q.Empty() { fmt.Println(q.Pop()) } } ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:14:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"如何判断一个链表有没有环？ 用hash来判断是否存在相同的值，也就是环。 快慢指针：快指针走两步，慢指针走一步，最终会相遇。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:15:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"那为什么快慢指针一定能够相遇？ 因为当快指针出现在慢指针后面之后，每一次快指针往前走两步、慢指针往前走一步，相当于快指针和慢指针之间的相对距离减少1步。当快指针刚刚绕到慢指针后面时，快指针离慢指针有 n 步。那么，对于接下来的每一次快指针往前走两步、慢指针往前走一步，快指针和慢指针之间的距离由 n 步变成 n-1 步、由 n-1 步变成 n-2 步、……、由 3 步变成 2 步、由 2 步变成 1 步、由 1 步变成 0 步。最终相遇。 快慢指针第一次相遇的时候，慢指针少走了多少？ 我们设从起点到 X 点的距离是 x，因此当在第一次相遇的时候，在n点，慢指针的路程是 x+n ，而因为快指针的速度是慢指针的一倍，它们从头开始，运动时间就是完全一致的，因此当二者发生相遇的时候，有如下几点值得注意： 当二者发生相遇时，慢指针一定已经进入了环中； 快指针一定先于慢指针进入环中； 快指针移动过的路程累计是慢指针的1倍，也就是2 * ( x + n )，其中早在路程为x时，快指针已经进入了环，它在环中运动了 x+2n 的路程。 在 n 点时，慢指针的路程是 x+n，快指针的路程是2*(x+n)，说明此时快指针比慢指针多走了 x+n，这同时也说明，从 n 处开始，继续往后推移 x+n 个距离，可以再次回到Z点，因此 x+n 的距离至少是环巡距离的一倍。慢指针和快指针在环中第一次相遇时的路程差值，是环路程的倍数。 所以慢指针少走了环巡距离的一倍。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:16:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"你用的是 mysql 是吧，那 B树 和 B+树 的区别是？ B 树 B+树 B+ 树内节点不存储数据，所有 data 存储在叶节点导致查询时间复杂度固定为log (n)。而 B-树 查询时间复杂度不固定，与 key 在树中的位置有关，最好为 O(1)。 B+ 树叶节点两两相连可大大增加区间访问性，可使用在范围查询等，而 B- 树 每个节点 key 和 data 在一起，则无法区间查找。同时我们访问某数据后，可以将相近的数据预读进内存，这是利用了空间局部性。 B+ 树更适合外部存储。由于内节点无 data 域，每个节点能索引的范围更大更精确。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:17:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"介绍一下死锁产生的必要条件 互斥条件，请求和保持条件，不剥夺条件，环路等待条件。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:18:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"如何实现互斥锁？ 一个互斥锁需要有阻塞和唤醒功能，所以需要一下几个情况。 需要有一个标记锁状态的 state 变量。 需要记录哪个线程持有了锁。 需要有一个队列维护所有的线程。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:19:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"如何实现自旋锁？ 当时真不会，只磕磕绊绊说了一些自旋锁的东西。。 大家可以看这篇博客 自旋锁C++实现方式 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:20:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"算法：三数之和。秒 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:21:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"三面 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:22:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"kafka 和 其他消息队列，比如 rocketmq，rabbitmq ，有什么优势？ kafka 具备非常高可靠的分布式架构，功能简单，只支持一些主要的MQ功能，像一些消息查询，消息回溯等功能没有提供。时效是在ms级别以内的。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:23:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"kafka如何保证消息不丢失？ 那我们可以从两个方面也就是生产者，消费者以及 broker ，来说说 生产者：生产者(Producer) 调用 send 方法发送消息之后，消息可能因为网络问题并没有发送过去。Kafka提供了同步发送消息方法，会返回一个Future对象，调用get()方法进行阻塞等待，就可以知道消息是否发送成功。如果消息发送失败的话，可以通过 Producer 的 retries（重试次数）参数进行设置，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议还要设置重试间隔，因为间隔太小的话重试的效果就不明显。 消费者：消息在被追加到 Partition(分区) 的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。 Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。 可以通过enable.auto.commit设置为false，关闭自动提交 offset ，每次在真正消费完消息之后之后再自己手动提交 offset 。 broker：当消息发送到了分区的leader副本，leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。解决办法就是 将producer设置 acks = all 。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:24:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"https为什么是安全的？ 因为 https 是基于 ssl 和 tls 加密而成的，https 的 s 就代表 ssl 和 tls。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:25:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"ssl/tls 是怎么保证安全的？经过几次握手？ 经过四次握手，客户端向服务器端索要并验证公钥，双方协商生成\"对话密钥\"，双方采用\"对话密钥\"进行加密通信。 四次握手主要是交换以下信息： 数字证书：该证书包含了公钥等信息，一般是由服务器发给客户端，接收方通过验证这个证书是不是由信赖的CA签发，或者与本地的证书相对比，来判断证书是否可信；假如需要双向验证，则服务器和客户端都需要发送数字证书给对方验证； 三个随机数：这三个随机数构成了后续通信过程中用来对数据进行对称加密解密的“对话密钥”。 首先客户端先发第一个随机数N1，然后服务器回了第二个随机数N2（这个过程同时把之前提到的证书发给客户端），这两个随机数都是明文的；而第三个随机数N3，客户端用数字证书的公钥进行非对称加密，发给服务器； 而服务器用只有自己知道的私钥来解密，获取第三个随机数。 这样，服务端和客户端都有了三个随机数N1+N2+N3，然后两端就使用这三个随机数来生成“对话密钥”，在此之后的通信都是使用这个“对话密钥”来进行对称加密解密。 因为这个过程中，服务端的私钥只用来解密第三个随机数，从来没有在网络中传输过，这样的话，只要私钥没有被泄露，那么数据就是安全的。 加密通信协议：就是双方商量使用哪一种加密方式，假如两者支持的加密方式不匹配，则无法进行通信； ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:26:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"事务的四大特性？ 原子性、隔离性、一致性、持久性 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:27:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"用过哪些排序？ 快排，堆排，归并比较常用。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:28:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"快排一定最快吗？ 不一定，当待排序的序列已经有序，不管是升序还是降序。此时快速排序最慢，一般当数据量很大的时候，用快速排序比较好，为了避免原来的序列有序。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:29:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"场景题：如果我有100G文件，但是只有 500 M 的内存，这些文件存着一行行的数字，如何获取最小的10个？ 这是经典的 Top K 问题，分治+堆排，我们可以先进行一个将文件进行分治，将100G的文件的分成一个个的 500 M，以此放到内存中每个500M的文件取出进行最小堆的排序，取出前10个写到新文件中。再对着一个个的新文件进行合并成 500 M 再进行最小堆的排序，最终获取最小的10个数字。 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:30:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"算法：最长有序括号，常见题，秒 ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:31:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["interview"],"content":"参考链接 [1] https://blog.csdn.net/weixin_35973945/article/details/124245495 [2] https://www.cnblogs.com/yaochunhui/p/14128777.html ","date":"2022-06-28 15:30:06","objectID":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/:32:0","tags":["interview"],"title":"interview-zj","uri":"/a.-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"},{"categories":["School courses"],"content":"第1章计算机网络体系结构 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:1:0","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"1.1计算机网络概述 1.1.1概念 简而言之，计算机网络是一些互联的、自治的计算机系统的集合。 广义观点：能够实现远程信息处理的系统或进一步达到资源共享 资源共享观点：能够以相互资源共享的方式互联起来的自治计算机系统的集合，计算机需遵循统一规则–网络协议 用户透明性观点：能够为用户自动管理资源的网络操作系统，对用户透明。用户使用网络就像使用一台单一的超级计算机，无须了解网络的存在、资源的位置信息。用户透明性观点的定义描述了一个分布式系统，它是网络未来发展追求的目标。 1.1.2组成 不同角度： 组成部分：硬件、软件、协议 硬件主要由主机(也称端系统)、通信链路(如双绞线、光纤)、交换设备(如路由器、交换机等)和通信处理机(如网卡)等组成。 软件主要包括各种实现资源共享的软件和方便用户使用的各种工具软件(如网络操作系统、邮件收发程序、FTP 程序、聊天程序等)。软件部分多属于应用层。 协议是计算机网络的核心，协议规定了网络传输数据时所遵循的规范。1.2.1 节将详细讨论协议。 工作方式：可分为边缘部分和核心部分。 边缘部分由所有连接到因特网上、供用户直接使用的主机组成，用来进行通信(如传输数据、音频或视频)和资源共享; 核心部分由大量的网络和连接这些网络的路由器组成，它为边缘部分提供连通性和交换服务。 功能组成：由通信子网和资源子网组成。 通信子网由各种传输介质、通信设备和相应的网络协议组成，它使网络具有数据传输、交换、控制和存储的能力，实现联网计算机之间的数据通信。 资源子网是实现资源共享功能的设备及其软件的集合,向网络用户提供共享其他计算机上的硬件资源、软件资源和数据资源的服务。 1.1.3计算机网络的功能 数据通信：比如文件传输、电子邮件等 资源共享：软件、数据、硬件共享 分布式处理：某个计算机系统负荷过重，分配任务给空闲计算机资源 提高可靠性：计算机网络中的各台计算机可通过网络互为替代机 负载均衡：均衡分配工作任务 1.1.4计算机网络的分类 分布范围： WAN：几十千米到几千千米 MAN：5~50千米 LAN：几十米到几千米 PAN：10M 传输技术： 广播式网络：联网计算机共享公共通信信道（LAN基本都采用这个，WAN中的无线、卫星通信网络也采用这个） 点对点网络：WAN基本都属于这个 拓扑结构：（网络中节点和线路之间的几何关系，主要指通信子网的拓扑结构） 总线形网络 星形网络 环形网络 网状形网络 以上四种互联形成的更复杂网络 使用者： 公用网 专用网 交换技术：（交换信息所采用的数据格式和交换装置的方式） 电路交换网络：时延小但线路利用率低 报文交换网络：充分利用线路容量、格式转换、差错控制等，但增大了资源开销、缓冲时延、缓冲区难以管理（因为报文大小不确定，不能预知报文大小）等 分组交换网络/包交换网络：具备报文交换的优点，且缓冲区易于管理，包的平均时延更小等，目前为主流网络。 传输介质： 有线 无线 1.1.5计算机网络的标准化工作及相关组织（略） 计算机网络的标准化对计算机网络的发展和推广起到了极为重要的作用。 因特网的所有标准都以RFC ( Request For Comments)的形式在因特网上发布，但并每个RFC都是因特网标准，RFC要上升为因特网的正式标准需经过以下4个阶段。 1)因特网草案(Internet Draft)。这个阶段还不是RFC文档。 2)建议标准( Proposed Standard)。从这个阶段开始就成为RFC文档。 3)草案标准(Draft Standard)。 4)因特网标准(Internet Standard)。 此外，还有实验的RFC和提供信息的RFC。各种RFC之间的关系略。 1.1.6计算机网络的性能指标 带宽：b/s(比特每秒) 时延： 发送时延（传输） 传播时延 处理时延 排队时延 时延带宽积 传播时延和信道带宽之积 往返时延 发送端发送数据开始，到发送端接收到来自接收端的确认。 吞吐量 单位时间内通过某个网络的数据量 速率：b/s 信道利用率 某信道有百分之几的时间是有数据通过的 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:1:1","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"1.2计算机网络体系结构与参考模型 1.2.1计算机网络分层结构 在计算机网络体系结构的各个层次中，每个报文都分为两部分:一是数据部分，即SDU; 是控制信息部分，即PCI，它们共同组成PDU。 服务数据单元(SDU): 为完成用户所要求的功能而应传送的数据。第n层的服务数据单元 记为n-SDU。 协议控制信息(PCD): 控制协议操作的信息。第n层的协议控制信息记为n-PCI. 协议数据单元(PDU):对等层次之间传送的数据单位称为该层的PDU。第n层的协议数据 单元记为n-PDU。在实际的网络中，每层的协议数据单元都有一个通俗的名称，如物理层的PDU 称为比特，链路层的PDU称为帧，网络层的PDU称为分组，传输层的PDU称为报文。 在各层间传输数据时，把从第n+ 1层收到的PDU作为第n层的SDU,加上第n层的PCI, 就变成了第n层的PDU,交给第n-1层后作为SDU发送，接收方接收时做相反的处理，因此可 知三者的关系为n-SDU + n-PCI= n-PDU =(n- 1)-SDU,其变换过程如图1.5所示。1.2.2协议、接口、服务 协议：规则的集合 接口：同一结点内相邻两层间交换信息的连接点 服务：指下层为紧邻的上层提供的功能调用，是垂直的 OSI中的服务原语有四类 1)请求(Request)。 由服务用户发往服务提供者，请求完成某项工作。 2)指示(Indication)。由服务提供者发往服务用户，指示用户做某件事情。 3)响应(Response)。由服务用户发往服务提供者，作为对指示的响应。 4)证实(Confirmation)。由服务提供者发往服务用户，作为对请求的证实。 有应答服务包括上述4类，无服务应答只有请求和指示 计算机提供的服务可以三种方式分类 面向连接服务和无连接服务 可靠服务和不可靠服务 有应答服务和无应答服务 1.2.3ISO/OSI参考模型和TCP/IP模型 ISO国际标准化组织，OSI/RM开放系统互连参考模型简称OSI参考模型，7层，低三层为通信子网，高三层为资源子网，传输层承上启下 会话层： 会话层允许不同主机上的各个进程之间进行会话。会话层利用传输层提供的端到端的服务，向表示层提供它的增值服务。这种服务主要为表示层实体或用户进程建立连接并在连接上有序地传输数据，这就是会话，也称建立同步(SYN)。 会话层负责管理主机间的会话进程，包括建立、管理及终止进程间的会话。会话层可以使用校验点使通信会话在通信失效时从校验点继续恢复通信，实现数据同步。 表示层 表示层主要处理在两个通信系统中交换信息的表示方式。不同机器采用的编码和表示方法不同，使用的数据结构也不同。为了使不同表示方法的数据和信息之间能互相交换，表示层采用抽象的标准方法定义数据结构，并采用标准的编码形式。数据压缩、加密和解密也是表示层可提供的数据表示变换功能。 应用层 应用层是OSI模型的最高层，是用户与网络的界面。应用层为特定类型的网络应用提供访问OSI环境的手段。因为用户的实际应用多种多样，这就要求应用层采用不同的应用协议来解决不同类型的应用要求，因此应用层是最复杂的一层，使用的协议也最多。典型的协议有用于文件传送的FTP、用于电子邮件的SMTP、用于万维网的HTTP等。 TCP/IP: 网络接口层类似OSI的物理层和数据链路层，网际层是关键部分 传输层：传输控制协议TCP+用户数据报协议UDP 应用层(用户-用户)包含所有的高层协议，如虚拟终端协议(Telnet)、文件传输协议(FTP)、域名解析服务(DNS)、 电子邮件协议(SMTP)和超文本传输协议(HTTP)。 学习计算机网络时，我们往往采取折中的办法，即综合OSI和TCP/IP的优点，采用一种只有五层协议的体系结构，即我们所熟知的物理层、数据链路层、网络层、传输层和应用层。本书也采用这种体系结构进行讨论。 应用层 传输层 网络层 数据链路层 物理层 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:1:2","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"1.3本章小结及疑难点 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:1:3","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"第2章物理层 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:2:0","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"2.1通信基础 2.1.1概念 数据、信号、码元 码元：用固定时长的信号波形/数字脉冲表示一位k进制数字 信源、信道、信宿 信道按传输信号形式的不同可分为传送模拟信号的模拟信道和传送数字信号的数字信道两大类;信道按传输介质的不同可分为无线信道和有线信道。 信道上传送的信号有基带信号和宽带信号之分。基带信号将数字信号1和0直接用两种不同的电压表示，然后送到数字信道上传输(称为基带传输);宽带信号将基带信号进行调制后形成频分复用模拟信号，然后传送到模拟信道上去传输(称为宽带传输)。 从通信双方信息的交互方式看，可分为三种基本方式: 1)单工通信。只有一个方向的通信而没有反方向的交互，仅需要一 条信道。例如，无线电广播、电视广播就属于这种类型。 2)半双工通信。通信的双方都可以发送或接收信息，但任何一方都不能同时发送和接收信息，此时需要两条信道。 3)全双工通信。通信双方可以同时发送和接收信息，也需要两条信道。 信道的极限容量是指信道的最高码元传输速率或信道的极限信息传输速率。 速率、波特、带宽 码元传输速率（也称调制速率、波形速率、符号速率）：单位是波特Baud，单位时间传输的码元数。和进制数无关 信息传输速率：也称比特率b/s,表示单位时间内数字通信系统传输的二进制码元个数，也就是比特数 一个码元若携带n比特的信息量，M Baud的码元传输速率对应Mn b/s的信息传输速率。 2.1.2奈奎斯特定理、香农定理 奈奎斯特定理： 奈奎斯特(Nyquist) 定理又称奈氏准则，它指出在理想低通(没有噪声、带宽有限)的信道中，极限码元传输率为2W波特，其中W是理想低通信道的带宽，单位为Hz。若用V表示每个码元离散电平的数目(码元的离散电平数目是指有多少种不同的码元，比如有16种不同的码元，则需要4位二进制位，因此数据传输率是码元传输率的4倍)，则极限数据率为： 理想低通信道下的极限数据传输率= $2W log_2V$ ( 单位为b/s) 对于奈氏准则，可以得出以下结论: 1)在任何信道中，码元传输的速率是有上限的。若传输速率超过此上限，就会出现严重的码间串扰问题(指在接收端收到的信号波形失去了码元之间的清晰界限)，使得接收端不可能完全正确识别码元。 2)信道的频带越宽(即通过的信号高频分量越多)，就可用更高的速率进行码元的有效传输。 3)奈氏准则给出了码元传输速率的限制，但并未对信息传输速率给出限制，即未对一个码元可以对应多少个二进制位给出限制。由于码元的传输速率受奈氏准则的制约，所以要提高数据的传输速率，就必须设法使每个码元携带更多个比特的信息量，此时就需要采用多元制的调制方法。 香农定理： 香农(Shannon)定理给出了带宽受限且有高斯白噪声干扰的信道的极限数据传输率，当用此速率进行传输时，可以做到不产生误差。香农定理定义为 信道的极限数据传输率= $Wlog_2(1 + S/N)$(单位为b/s) 式中，W为信道的带宽，S为信道所传输信号的平均功率，N为信道内部的高斯噪声功率。S/N为信噪比，即信号的平均功率与噪声的平均功率之比，信噪比= $10log_10(S/N)$(单位为dB),例如如当S/N= 10时，信噪比为10dB， 而当S/N= 1000时，信噪比为30dB。 对于香农定理，可以得出以下结论: 1)信道的带宽或信道中的信噪比越大，信息的极限传输速率越高。 2)对一定的传输带宽和一定的信噪比，信息传输速率的上限是确定的。 3)只要信息的传输速率低于信道的极限传输速率，就能找到某种方法来实现无差错的传输。 4)香农定理得出的是极限信息传输速率，实际信道能达到的传输速率要比它低不少。 从香农定理可以看出，若信道带宽W或信噪比S/N没有上限(实际信道当然不可能这样)，则信道的极限信息传输速率也没有上限。 奈氏准则只考虑了带宽与极限码元传输速率的关系，而香农定理不仅考虑到了带宽，也考虑到了信噪比。这从另一个侧面表明，一个码元对应的二进制位数是有限的。 2.1.3编码与调制 数字数据编码为数字信号 归零编码（RZ） 每个时钟周期中间均跳变为低电平 非归零编码（NRZ） 不归零，但无法传递时钟信号，双方难以同步，需要时钟线 反向非归零编码（NRZI） 不归零，信号翻转代表0，保持不变代表1，集成了前两种的优点。 曼彻斯特编码 每个码元中间出现电平跳变，频带宽度翻倍。 以太网使用的编码方式就是曼彻斯特编码 差分曼彻斯特编码 常用于局域网传输，码元为1，则前半个码元的电平与上个码元的后半个码元的电平一致；若码元为0，则相反。每个码元中间会有跳转，可以实现自同步，抗干扰性好。 4B/5B编码 将欲发送数据流的每4位作为-组，然后按照4B/5B编码规则将其转换成相应的5位码。5位码共32种组合，但只采用其中的16种对应16种不同的4位码，其他的16种作为控制码(帧的开始和结束、线路的状态信息等)或保留。 数字数据调制为模拟信号 1)幅移键控(ASK)。通过改变载波信号的振幅来表示数字信号1和0，而载波的频率和相位都不改变。比较容易实现，但抗干扰能力差。 2)频移键控(FSK)。通过改变载波信号的频率来表示数字信号1和0，而载波的振幅和相位都不改变。容易实现，抗干扰能力强，目前应用较为广泛。 3)相移键控(PSK)。 通过改变载波信号的相位来表示数字信号1和0，而载波的振幅和频率都不改变。它又分为绝对调相和相对调相。 4)正交振幅调制(QAM)。在频率相同的前提下，将ASK与PSK结合起来，形成叠加信号。设波特率为B,采用m个相位，每个相位有n种振幅，则该QAM技术的数据传输率R为 $R = Blog_2(mn)$(单位为b/s) 模拟数据编码为数字信号 这种编码方式最典型的例子是常用于对音频信号进行编码的脉码调制(PCM)。它主要包括三个步骤，即采样、量化和编码。 先来介绍采样定理:在通信领域，带宽是指信号最高频率与最低频率之差，单位为Hz。因此，将模拟信号转换成数字信号时，假设原始信号中的最大频率为f,那么采样频率$f_{采祥}$必须大于等于最大频率f的两倍，才能保证采样后的数字信号完整保留原始模拟信号的信息(只需记住结论)。另外，采样定理又称奈奎斯特定理。 1)采样是指对模拟信号进行周期性扫描，把时间上连续的信号变成时间上离散的信号。根据采样定理，当采样的频率大于等于模拟数据的频带带宽(最高变化频率)的两倍时，所得的离散信号可以无失真地代表被采样的模拟数据。 2)量化是把采样取得的电平幅值按照–定的分级标度转化为对应的数字值并取整数，这样就把连续的电平幅值转换为了离散的数字量。采样和量化的实质就是分割和转换。 3)编码是把量化的结果转换为与之对应的二进制编码。 模拟数据调制为模拟信号 为了实现传输的有效性，可能需要较高的频率。这种调制方式还可以使用频分复用(FDM)技术，充分利用带宽资源。电话机和本地局交换机采用模拟信号传输模拟数据的编码方式;模拟的声音数据是加载到模拟的载波信号中传输的。 2.1.4电路交换、报文交换、分组交换 电路交换： 连接建立，传输数据、连接释放 报文交换： 报文交换在交换节点采用的是存储转发的传输方式。报文大小未知。 分组交换： 也是存储转发，解决了大报文传输问题。 缺点： 1)存在传输时延。尽管分组交换比报文交换的传输时延少，但相对于电路交换仍存在存储转发时延，而且其结点交换机必须具有更强的处理能力。 2)需要传输额外的信息量。每个小数据块都要加上源地址、目的地址和分组编号等信息，从而构成分组，因此使得传送的信息量增大了5%~10%，一定程度 上降低了通信效率，增加了处理的时间，使控制复杂，时延增加。 3)当分组交换采用数据报服务时，可能会出现失序、丢失或重复分组，分组到达目的结点时，要对分组按编号进行排序等工作，因此很麻烦。若采用虚电路服务，虽无失序问题，但有呼叫建立、数据传输和虛电路释放三个过程。 2.1.5数据报与虚电路 分组交换根据其通信子网向端点系统提供的服务，还可进一步分为面向连接的虛电路方式和无连接的数据报方式。这两种服务方式都由网络层提供。要注意****数据报方式和虚电路方式是分组交换的两种方式。 数据报 虚电路（ppt无）： 虚电路方式试图将数据报方式与电路交换方式结合起来，充分发挥两种方法的优点，以达到最佳的数据交换效果。在分组发送之前，要求在发送方和接收方建立一条逻辑上相连的虚电路，并且连接一旦建立，就固定了虚电路所对应的物理路径。与电路交换类似，整个通信过程分为三个阶段:虚电路建立、数据传输与虚电路释放。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:2:1","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"2.2传输介质 2.2.1双绞线、同轴电缆、光纤与无线传输介质 双绞线 STP屏蔽双绞线 UTP非屏蔽双绞线 同轴电缆 50欧 同轴电缆和75欧同轴电缆。其中，50欧同轴电缆主要用于传送基带数字信号，又称基带同轴电缆，它在局域网中应用广泛; 75欧同轴电缆主要用于传送宽带信号，又称宽带同轴电缆，主要用于有线电视系统。 光纤 无线传输介质 无线电波 微波、红外线、激光。三者都需要发送者和接收者之间存在视线通路。 2.2.2物理层接口的特性 1)机械特性。主要定义物理连接的边界点，即接插装置。规定物理连接时所采用的规格、引线的数目、引脚的数量和排列情况等。 2)电气特性。规定传输二进制位时，线路上信号的电压高低、阻抗匹配、传输速率和距离限制等。 3)功能特性。指明某条线上出现的某一一电平的电压表示何种意义，接口部件的信号线(数据线、控制线、定时线等)的用途。 4)规程特性。主要定义各条物理线路的工作规程和时序关系。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:2:2","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"2.3物理层设备 2.3.1中继器 2.2.2集线器 多端口中继器 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:2:3","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"2.4本章小结及疑难点 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:2:4","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"第3章数据链路层 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:0","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"3.1数据链路层的功能 3.1.1为网络层提供服务 1)无确认的无连接服务。源机器发送数据帧时不需先建立链路连接，目的机器收到数据帧时不需发回确认。对丢失的帧，数据链路层不负责重发而交给上层处理。适用于实时通信或误码率较低的通信信道，如以太网。 2)有确认的无连接服务。源机器发送数据帧时不需先建立链路连接，但目的机器收到数据帧时必须发回确认。源机器在所规定的时间内未收到确定信号时，就重传丢失的帧，以提高传输的可靠性。该服务适用于误码率较高的通信信道，如无线通信。 3)有确认的面向连接服务。帧传输过程分为三个阶段:建立数据链路、传输帧、释放数据链路。目的机器对收到的每一帧都要给出确认，源机器收到确认后才能发送下一帧，因 而该服务的可靠性最高。该服务适用于通信要求(可靠性、实时性)较高的场合。 注意:有连接就一定要有确认，即不存在无确认的面向连接的服务。 3.1.2链路管理 数据链路层连接的建立、维持和释放过程称为链路管理，它主要用于面向连接的服务。 3.1.3帧定界、帧同步、透明传输 帧同步指的是接收方应能从接收到的二进制比特流中区分出帧的起始与终止。 如果在数据中恰好出现与帧定界符相同的比特组合(会误认为“传输结束”而丢弃后面的数 据)，那么就要采取有效的措施解决这个问题，即透明传输。 3.1.4流量控制 流量控制实际上就是限制发送方的数据流量，使其发送速率不超过接收方的接收能力。 流量控制并不是数据链路层特有的功能，许多高层协议中也提供此功能，只不过控制的对象不同而已。对于数据链路层来说，控制的是相邻两结点之间数据链路上的流量,而对于传输层来说，控制的则是从源端到目的端之间的流量。 3.1.5差错控制 由于信道噪声等各种原因，帧在传输过程中可能会出现错误。用以使发送方确定接收方是否正确收到由其发送的数据的方法称为差错控制。通常，这些错误可分为位错和帧错。 位错指帧中某些位出现了差错。通常采用循环冗余校验(CRC)方式发现位错，通过自动重传请求(Automatic Repeat reQuest, ARQ) 方式来重传出错的帧。具体做法是:让发送方将要发送的数据帧附加一定的CRC冗余检错码一并发送，接收方则根据检错码对数据帧进行错误检测，若发现错误则丢弃，发送方超时重传该数据帧。这种差错控制方法称为ARQ法。ARQ法只需返回很少的控制信息就可有效地确认所发数据帧是否被正确接收。 帧错指帧的丢失、重复或失序等错误。在数据链路层引入定时器和编号机制，能保证每一帧最终都能有且仅有一次正确地交付给目的结点。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:1","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"3.2组帧 组帧主要解决：帧定界、帧同步、透明传输等问题 有以下四种方法： 3.2.1字符计数法 字符计数法是指在帧头部使用一一个计数字段来标明帧内字符数。目的结点的数据链路层收到字节计数值时，就知道后面跟随的字节数，从而可以确定帧结束的位置(计数字段提供的字节数包含自身所占用的一个字节)。 但如果计数字段出错，便失去了帧边界划分的依据。 3.2.2字符填充的首尾定界符法 字符填充法使用一些特定的字符来定界一帧的开始(DLE STX)与结束(DLE ETX)。为了 使信息位中出现的特殊字符不被误判为帧的首尾定界符，可以在特殊字符前面填充-一个转义字符(DLE)来加以区分(注意，转义字符是ASCII码中的控制字符，是一-个字符，而非“D”“L”“E”三个字符的组合)，以实现数据的透明传输。接收方收到转义字符后，就知道其后面紧跟的是数据信息，而不是控制信息。 3.2.3零比特填充的首尾标志法 如图3.5所示，零比特填充法允许数据帧包含任意个数的比特，也允许每个字符的编码包含任意个数的比特。它使用一个特定的比特模式，即01111110来标志一帧的开始和结束。为了不使信息位中出现的比特流01111110被误判为帧的首尾标志，发送方的数据链路层在信息位中遇到5个连续的“1”时，将自动在其后插入一个“0”;而接收方做该过程的逆操作，即每收到5个连续的“1” 时，自动删除后面紧跟的“0”，以恢复原信息。 很容易由硬件实现，性能优于字符填充法。 3.2.4违规编码法 在物理层进行比特编码时，通常采用违规编码法。例如，曼彻斯特编码方法将数据比特“1”编码成“高-低”电平对，将数据比特“0” 编码成“低-高”电平对，而“高高”电平对和“低-低”电平对在数据比特中是违规的(即没有采用)。可以借用这些违规编码序列来定界帧的起始和终止。局域网IEEE 802标准就采用了这种方法。 违规编码法不需要采用任何填充技术，便能实现数据传输的透明性，但它只适用于采用冗余编码的特殊编码环境。 由于字节计数法中计数字段的脆弱性和字符填充法实现上的复杂性与不兼容性，目前较常用的组帧方法是比特填充法和违规编码法。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:2","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"3.3差错控制 实际通信链路都不是理想的，比特在传输过程中可能会产生差错，1可能会变成0，0也 可能会变成1,这就是比特差错。比特差错是传输差错中的一种，本节仅讨论比特差错。 通常利用编码技术进行差错控制，主要有两类:自动重传请求ARQ和前向纠错FEC。在ARQ方式中，接收端检测出差错时，就设法通知发送端重发，直到接收到正确的码字为止。在FEC方式中，接收端不但能发现差错，而且能确定比特串的错误位置，从而加以纠正。因此，差错控制又可分为检错编码和纠错编码。 3.3.1检错编码 检错编码都采用冗余编码技术，其核心思想是在有效数据(信息位)被发送前，先按某种关系附加一定的冗余位，构成一个符合某一规则的码字后再发送。当要发送的有效数据变化时，相应的冗余位也随之变化，使得码字遵从不变的规则。接收端根据收到的码字是否仍符合原规则来判断是否出错。常见的检错编码有奇偶校验码和循环冗余码。 1.奇偶校验码 奇偶校验码是奇校验码和偶校验码的统称，是一种最基本的检错码。它由n-1位信息元和1位校验元组成，如果是奇校验码，那么在附加一个校验元后，码长为n的码字中“1”的个数为奇数;如果是偶校验码，那么在附加一个校验元以后，码长为n的码字中“1” 的个数为偶数。它又分为垂直奇偶校验、水平奇偶校验和水平垂直奇偶校验。 2.循环冗余码 给定一个 m bit的帧或报文，发送器生成一个 r bit的序列，称为帧检验序列(FCS)。 这样所形成的帧将由m +r比特组成。发送方和接收方事先商定一个多项式G(x) (最高位和最低位必须为1)，使这个带检验码的帧刚好能被预先确定的多项式G(x)整除。接收方用相同的多项式去除收到的帧，如果无余数，那么认为无差错。 假设-一个帧有m位，其对应的多项式为M(x)，则计算冗余码的步骤如下: 1)加0。假设G(x)的阶为r, 在帧的低位端加上r个0。 2)模2除。利用模2除法，用G(x)对应的数据串去除1)中计算出的数据串，得到的余数即为冗余码(共r位，前面的0不可省略)。 多项式以2为模运算。按照模2运算规则，加法不进位，减法不借位，它刚好是异或操作。乘除法类似于二进制的运算，只是在做加减法时按模2规则进行。 冗余码的计算举例:设G(x)=1101 (即r=3)，待传送数据M= 101001 (即m=6)，经模2除法运算后的结果是:商Q= 110101 (这个商没什么用)，余数R= 001。所以发送出去的数据为101001 001 (即2’M+FCS)，共有m+r位。循环冗余码的运算过程如图3.6所示。 3.3.2纠错编码 在数据通信的过程中，解决差错问题的一种方法是在每个要发送的数据块上附加足够的冗余信息，使接收方能够推导出发送方实际送出的应该是什么样的比特串。最常见的纠错编码是海明码，其实现原理是在有效信息位中加入几个校验位形成海明码，并把海明码的每个二进制位分配到几个奇偶校验组中。当某一位出错后，就会引起有关的几个校验位的值发生变化，这不但可以发现错位，而且能指出错位的位置，为自动纠错提供依据。 海明码的编码原理和过程：略 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:3","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"3.4流量控制与可靠传输机制 3.4.1流量控制、可靠传输和滑动窗口机制 流量控制的基本方法是由接收方控制发送方发送数据的速率，常见的方式有两种: 停止-等待协议和滑动窗口协议。 1.停止-等待流量控制基本原理 发送方每发送- -帧,都要等待接收方的应答信号，之后才能发送下- -帧;接收方每接收一帧，都要反馈一个应答信号，表示可接收下一帧，如果接收方不反馈应答信号，那么发送方必须一直等待。每次只允许发送–帧，然后就陷入等待接收方确认信息的过程中，因而传输效率很低。 2.滑动窗口流量控制基本原理 在任意时刻，发送方都维持-组连续的允许发送的帧的序号，称为发送窗口;同时接收方也维持一组连续的允许接收帧的序号，称为接收窗口。发送窗口用来对发送方进行流量控制，而发送窗口的大小Wr代表在还未收到对方确认信息的情况下发送方最多还可以发送多少个数据帧。同理，在接收端设置接收窗口是为了控制可以接收哪些数据帧和不可以接收哪些帧。在接收方，只有收到的数据帧的序号落入接收窗口内时，才允许将该数据帧收下。若接收到的数据帧落在接收窗口之外，则一律将其丢弃。 图3.7给出了发送窗口的工作原理，图3.8给出了接收窗口的工作原理。 发送端每收到一一个确认帧，发送窗口就向前滑动-一个帧的位置，当发送窗口内没有可以发送的帧(即窗口内的帧全部是已发送但未收到确认的帧)时，发送方就会停止发送，直到收到接收方发送的确认帧使窗口移动，窗口内有可以发送的帧后，才开始继续发送。 接收端收到数据帧后，将窗口向前移-一个位置， 并发回确认帧，若收到的数据帧落在接收窗口之外，则一律丢弃。 滑动窗口有以下重要特性: 1)只有接收窗口向前滑动(同时接收方发送了确认帧)时，发送窗口才有可能(只有发送 方收到确认帧后才- -定)向前滑动。 2)从滑动窗口的概念看，停止等待协议、后退N帧协议和选择重传协议只在发送窗口大小与接收窗口大小上有所差别: 停止-等待协议:发送窗口大小=1,接收窗口大小= 1。 后退N帧协议:发送窗口大小\u003e 1,接收窗口大小= 1。 选择重传协议:发送窗口大小\u003e1,接收窗口大小\u003e 1。 3)接收窗口的大小为1时，可保证帧的有序接收。 4)数据链路层的滑动窗口协议中，窗口的大小在传输过程中是固定的(注意与第5章传输层的滑动窗口协议的区别)。 3.可靠传输机制 数据链路层的可靠传输通常使用确认和超时重传两种机制来完成。确认是一–种无数据的控制帧，这种控制帧使得接收方可以让发送方知道哪些内容被正确接收。有些情况下为了提高传输效率，将确认捎带在一一个回复帧中，称为捎带确认。超时重传是指发送方在发送某个数据帧后就开启一个计时器，在- -定时间内如果没有得到发送的数据帧的确认帧，那么就重新发送该数据帧，直到发送成功为止。 自动重传请求(Auto Repeat reQuest, ARQ)通过接收方请求发送方重传出错的数据帧来恢复出错的帧，是通信中用于处理信道所带来差错的方法之一。传统自动重传请求分为三种，即停止等待(Stop-and-Wait) ARQ、后退N帧(Go-Back-N) ARQ和选择性重传(Selective Repeat)ARQ。后两种协议是滑动窗口技术与请求重发技术的结合，由于窗口尺寸开到足够大时，帧在线路上可以连续地流动，因此又称其为连续ARQ协议。注意，在数据链路层中流量控制机制和可靠传输机制是交织在一起的。 3.4.2单帧滑动窗口与停止-等待协议 3.4.3多帧滑动窗口与后退N帧协议(GBN) 3.4.4多帧滑动窗口与选择重传协议(SR) ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:4","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"3.5介质访问控制 介质访问控制所要完成的主要任务是，为使用介质的每个结点隔离来自同一信道上其他结点所传送的信号，以协调活动结点的传输。用来决定广播信道中信道分配的协议属于数据链路层的一个子层，称为介质访问控制(Medium Access Control, MAC) 子层。 常见的介质访问控制方法有信道划分介质访问控制、随机访问介质访问控制和轮询访问介质访问控制。其中前者是静态划分信道的方法，而后两者是动态分配信道的方法。 3.5.1信道划分介质访问控制 信道划分介质访问控制将使用介质的每个设备与来自同–通信信道上的其他设备的通信隔离开来，把时域和频域资源合理地分配给网络上的设备。 下面介绍多路复用技术的概念。当传输介质的带宽超过传输单个信号所需的带宽时，人们就通过在一条介质上同时携带多个传输信号的方法来提高传输系统的利用率，这就是所谓的多路复用，也是实现信道划分介质访问控制的途径。多路复用技术把多个信号组合在一条物理信道上进行传输，使多个计算机或终端设备共享信道资源，提高了信道的利用率。 采用多路复用技术可把多个输入通道的信息整合到一一个复用通道中，在接收端把收到的信息分离出来并传送到对应的输出通道。 信道划分的实质就是通过分时、分频、分码等方法把原来的一条广播信道，逻辑上分为几条用于两个结点之间通信的互不干扰的子信道，实际上就是把广播信道转变为点对点信道。 信道划分介质访问控制分为以下4种。 频分多路复用(FDM) 频分多路复用是一-种将多路基带信号调制到不同频率载波上，再叠加形成-一个复合信号的多路复用技术。在物理信道的可用带宽超过单个原始信号所需带宽的情况下，可将该物理信道的总带宽分割成若干与传输单个信号带宽相同(或略宽)的子信道，每个子信道传输一-种信号，这就是频分多路复用，如图3.14所示。每个子信道分配的带宽可不相同，但它们的总和必须不超过信道的总带宽。在实际应用中，为了防止子信道之间的干扰，相邻信道之间需要加入“保护频带”。 频分多路复用的优点在于充分利用了传输介质的带宽，系统效率较高:由于技术比较成熟，实现也较容易 时分多路复用(TDM) 时分多路复用是将一条物理信道按时间分成若干时间片，轮流地分配给多个信号使用。每个时间片由复用的一个信号占用，而不像FDM那样，同一时间同时发送多路信号。这样，利用每个信号在时间上的交叉，就可以在一条物理信道上传输多个信号，如图3.15所示。 就某个时刻来看,时分多路复用信道上传送的仅是某一对设备之间的信号;就某段时间而言，传送的是按时间分割的多路复用信号。但由于计算机数据的突发性，-一个用户对已经分配到的子信道的利用率一般不高。 统计时分多路复用(STDM， 又称异步时分多路复用)是TDM的一种改进，它采用STDM帧，STDM帧并不固定分配时隙，而按需动态地分配时隙，当终端有数据要传送时，才会分配到时间片，因此可以提高线路的利用率。例如，线路传输率为8000b/s, 4个用户的平均速率都为2000b/s，当采用TDM方式时，每个用户的最高速率为2000b/s，而在STDM方式下，每个用户的最高速率可达8000b/s。 波分多路复用(WDM) 波分多路复用即光的频分多路复用，它在一根光纤中传输多种不同波长(频率)的光信号，由于波长(频率)不同，各路光信号互不干扰，最后再用波长分解复用器将各路波长分解出来。由于光波处于频谱的高频段，有很高的带宽，因而可以实现多路的波分复用 码分多路复用(CDM) 码分多路复用是采用不同的编码来区分各路原始信号的一种复用方式。与FDM和TDM不同，它既共享信道的频率，又共享时间。 3.5.2随机访问介质访问控制 在随机访问协议中，不采用集中控制方式解决发送信息的次序问题，所有用户能根据自己的意愿随机地发送信息，占用信道全部速率。在总线形网络中，当有两个或多个用户同时发送信息时，就会产生帧的冲突(碰撞，即前面所说的相互干扰)，导致所有冲突用户的发送均以失败告终。为了解决随机接入发生的碰撞，每个用户需要按照一-定 的规则反复地重传它的帧，直到该帧无碰撞地通过。这些规则就是随机访问介质访问控制协议，常用的协议有ALOHA协议、CSMA协议、CSMA/CD协议和CSMA/CA协议等，它们的核心思想都是:胜利者通过争用获得信道，从而获得信息的发送权。因此，随机访问介质访问控制协议又称争用型协议。 读者会发现，如果介质访问控制采用信道划分机制，那么结点之间的通信要么共享空间要 么共享时间，要么两者都共享;而如果采用随机访问控制机制，那么各结点之间的通信就可既不共享时间，也不共享空间。所以随机介质访问控制实质上是一-种将广 播信道转化为点到点信道的行为. ALOHA协议 夏威夷大学早期研制的随机接入系统称为ALOHA,它是Additive Link On-line HAwaii system的缩写。ALOHA协议分为纯ALOHA协议和时隙ALOHA协议两种。 (1)纯ALOHA协议 纯ALOHA协议的基本思想是，当网络中的任何-一个站点需要发送数据时，可以不进行任何检测就发送数据。如果在- -段时间内未收到确认，那么该站点就认为传输过程中发生了冲突。发送站点需要等待一段时间后 再发送数据，直至发送成功。 图3.20表示-一个纯ALOHA协议的工作原理。每个站均自由地发送数据帧。为简化问题，不考虑由信道不良而产生的误码，并假定所有站发送的帧都是定长的，帧的长度不用比特而用发送这个帧所需的时间来表示，在图3.20中用To表示这段时间。 在图3.20的例子中，当站1发送帧1时，其他站都未发送数据，所以站1的发送必定是成功 的。但随后站2和站N-1发送的帧2和帧3在时间上重叠了一-些(即发生了碰撞)。碰撞的结果是，碰撞双方(有时也可能是多方)所发送的数据出现了差错，因而都须进行重传。但是发生碰撞的各站并不能马上进行重传，因为这样做必然会继续发生碰撞。纯ALOHA系统采用的重传策略是让各站等待一段随机的时间， 然后再进行重传。若再次发生碰撞，则需要再等待一段随机的时间，直到重传成功为止。图中其余一些帧的发送情况是帧4发送成功，而帧5和帧6发生碰撞。 假设网络负载(To时间内所有站点发送成功的和未成功而重传的帧数)为G，则纯ALOHA 网络的吞吐量(To 时间内成功发送的平均帧数)为 $S=Ge^{-2G}$。 当G=0.5时，$S= 0.5e^{-1}≈0.184$,这是吞吐量s可能达到的极大值。可见，纯ALOHA网络的吞吐量很低。为了克服这- -缺点，人们在原始的纯ALOHA协议的基础.上进行改进，产生了时隙ALOHA协议。 时隙ALOHA协议 时隙ALOHA协议把所有各站在时间.上同步起来，并将时间划分为一段段等长的时 隙(Slot),规定只能在每个时隙开始时才能发送一一个帧。 从而避免了用户发送数据的随意性，减少了数据产生冲突的可能性，提高了信道的利用率。 图3.21为两个站的时隙ALOHA协议的工作原理示意图。时隙的长度To使得每个帧正好在一个时隙内发送完毕。每个帧在到达后，-般都要在缓存中等待一段小于 To的时间，然后才能发送出去。在一个时隙内有两个或两个以上的帧到达时，在下一个时隙将产生碰撞。碰撞后重传的策略与纯ALOHA的情况是相似的。 时隙ALOHA网络的吞吐量S与网络负载G的关系是$S=Ge^{-G}$。当G=1时，$S=e^{-1}≈0.368$。这是吞吐量S可能达到的极大值。可见，时隙ALOHA网络比纯ALOHA网络的吞吐量大了1倍。 CSMA协议: 时隙ALOHA系统的效率虽然是纯ALOHA系统的两倍，但每个站点都是随心所欲地发送数 据的，即使其他站点正在发送也照发不误，因此发送碰撞的概率很大。 若每个站点在发送前都先侦听- -下共用信道，发现信道空闲后再发送，则就会大大降低冲突的可能，从而提高信道的利用率，载波侦听多路访问(Carrier Sense Multiple Access, CSMA)协议依据的正是这一思想。CSMA协议是在ALOHA协议基础上提出的一种改进协议， 它与ALOHA协议的主要区别是多了一个载波侦听装置。 根据侦听方式和侦听到信道忙后的处理方式不同，CSMA 协议分为三种。 (1) 1-坚持CSMA 1-坚持CSMA ( l-persistent CSMA)的基本思想是: -一个结点要发送数据时，首先侦听信道;如果信道空闲，那么立即发送数据;如果信道忙，那么等待，同时继续侦听直至信道空闲;如果发生冲突，那么随机等待一段时间后， 再重新开始侦听信道。 “1-坚持”的含义是:侦听到信道忙后，继续坚持侦听信道;侦听到信道空闲后，发送帧的概率为1,即立刻发送数据。 传播延迟对1-坚持CSMA协议的性能影响较大。结点A开始发送数据时，结点B也正好有数 据要发送，但这时结点A发出数据的信号还未到达结点B,结点B侦听到信道空闲，于是立即发送数据，结果必然导致冲突。即使不考虑延迟，1-坚持CSMA协议也可能产生冲突。例如，结点A正在发送数据时，结点B和C也准备发送数据，侦听到信道忙，于是坚持侦听，结果当结点A一发送完毕，结点B和C就会立即发送数据，同样导致冲突。 (2)非坚持CSMA 非坚持CSMA (Non-persistent CSMA)的基本思想是: - 一个结点要发送数据时，首先侦听信道;如果信道空闲，那么立即发送数据;如果信道忙，那么放弃侦听，等待一个随机的时间后再重复上述过程。 非坚持CSMA协议在侦听到信道忙后就放弃侦听，因此降低了多个结点等待信道空闲后同时发送数据导致冲突的概率，但也会增加数据在网络中的平均延迟。可见，信道利用率的提高是以增加数","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:5","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"3.6局域网 3.6.1局域网 (Local Area Network, LAN) 是指在-一个较小的地理范围(如一-所学校)内，将各种 计算机、外部设备和数据库系统等通过双绞线、同轴电缆等连接介质互相连接起来，组成资源和信息共享的计算机互联网络。主要特点如下: 1)为一个单位所拥有，且地理范围和站点数目均有限。 2)所有站点共享较高的总带宽(即较高的数据传输率)。 3)较低的时延和较低的误码率。 4)各站为平等关系而非主从关系。 5)能进行广播和组播。 局域网的特性主要由三个要素决定:拓扑结构、传输介质、介质访问控制方式，其中最重要的是介质访问控制方式，它决定着局域网的技术特性。 常见的局域网拓扑结构主要有以下4大类:①星形结构;②环形结构;③总线形结构;④星形和总线形结合的复合型结构。 局域网可以使用双绞线、铜缆和光纤等多种传输介质，其中双绞线为主流传输介质。 局域网的介质访问控制方法主要有CSMA/CD、令牌总线和令牌环，其中前两种方法主要用 于总线形局域网，令牌环主要用于环形局域网。 三种特殊的局域网拓扑实现如下: 以太网(目前使用范围最广的局域网)。逻辑拓扑是总线形结构，物理拓扑是星形或拓展星形结构。 令牌环 (Token Ring, IEEE 802.5)。逻辑拓扑是环形结构，物理拓扑是星形结构。 FDDI (光纤分布数字接口，IEEE 802.8)。逻辑拓扑是环形结构，物理拓扑是双环结构。 IEEE802标准定义的局域网参考模型只对应于OSI参考模型的数据链路层和物理层,并将数 据链路层拆分为两个子层:逻辑链路控制(LLC) 子层和媒体接入控制(MAC)子层。与接入传输媒体有关的内容都放在MAC子层，它向上层屏蔽对物理层访问的各种差异，提供对物理层的统一访问接口，主要功能包括:组帧和拆卸帧、比特传输差错检测、透明传输。LLC子层与传输媒体无关，它向网络层提供无确认无连接、面向连接、带确认无连接、高速传送4种不同的连接服务类型。 由于以太网在局域网市场中取得垄断地位，几乎成为局域网的代名词，而802委员会制定的LLC子层作用已经不大，因此现在许多网卡仅装有MAC协议而没有LLC协议。 3.6.2以太网与IEEE 802.3 以太网的传输介质与网卡 以太网常用的传输介质有4种:粗缆、细缆、双绞线和光纤。各种传输介质的适用情况见表3.2。 计算机与外界局域网的连接是通过主机箱内插入的一块网络接口板[又称网络适配器(Adapter)或网络接口卡(Network Interface Card，NIC)]实现的。网卡上装有处理器和存储器，是工作在数据链路层的网路组件。网卡是局域网中连接计算机和传输介质的接口，不仅能实现与局域网传输介质之间的物理连接和电信号匹配，还涉及帧的发送与接收、帧的封装与拆封、介质访问控制、数据的编码与解码及数据缓存功能等。 全世界的每块网卡在出厂时都有一个唯一的代码，称为介质访问控制(MAC)地址，这个地址用于控制主机在网络上的数据通信。数据链路层设备(网桥、交换机等）都使用各个网卡的MAC地址。另外，网卡控制着主机对介质的访问，因此网卡也工作在物理层，因为它只关注比特，而不关注任何地址信息和高层协议信息。 以太网的MAC帧 每块网络适配器（网卡）都有一个地址，称为MAC 地址，也称物理地址;MAC地址长6字节，一般用由连字符(或冒号）分隔的6个十六进制数表示，如02-60-8c-c4-b1-21。高24位为厂商代码，低24位为厂商自行分配的网卡序列号。 由于总线上使用的是广播通信，因此网卡从网络上每收到一个MAC帧，首先要用硬件检查MAC帧中的MAC地址。如果是发往本站的帧，那么就收下，否则丢弃。 以太网MAC帧格式有两种标准:DIX Ethernet V2标准（即以太网V2标准）和IEEE 802.3标准。这里只介绍最常用的以太网V2的MAC帧格式，如图3.24所示。 前导码:使接收端与发送端时钟同步。在帧前面插入的8字节可再分为两个字段:第一个字段共7字节，是前同步码，用来快速实现MAC帧的比特同步﹔第二个字段是帧开始定界符，表示后面的信息就是MAC帧。 注意:MAC帧并不需要帧结束符，因为以太网在传送帧时，各帧之间必须有一定的间隙。因此，接收端只要找到帧开始定界符，其后面连续到达的比特流就都属于同一个MAC帧，所以图3.24只有帧开始定界符。但不要误以为以太网MAC帧不需要尾部，在数据链路层上，帧既要加首部，也要加尾部。 地址:通常使用6字节(48bit）地址（MAC地址)。 类型:2字节，指出数据域中携带的数据应交给哪个协议实体处理。 数据:46~1500字节，包含高层的协议消息。由于CSMA/CD算法的限制，以太网帧必须满足最小长度要求64字节，数据较少时必须加以填充（0～46字节)。 注意:46和1500是怎么来的?首先，由CSMA/CD算法可知以太网帧的最短帧长为64B，而MAC帧的首部和尾部的长度为18字节，所以数据最短为64-18=46字节。其次，最大的1500字节是规定的，没有为什么。 填充:0~46字节，当帧长太短时填充帧，使之达到64字节的最小长度。 校验码（FCS):4字节，校验范围从目的地址段到数据段的末尾，算法采用32位循环冗余码(CRC)，不但需要检验MAC帧的数据部分，还要检验目的地址、源地址和类型字段，但不校验前导码。 802.3帧格式与DIX以太帧格式的不同之处在于用长度域替代了DIX帧中的类型域，指出数据域的长度。在实践中，前述长度/类型两种机制可以并存，由于IEEE 802.3数据段的最大字节数是1500，所以长度段的最大值是1500，因此从1501到65535的值可用于类型段标识符 高速以太网 速率达到或超过100Mb/s的以太网称为高速以太网。 (1）100BASE-T以太网 100BASE-T以太网是在双绞线上传送100Mb/s基带信号的星形拓扑结构以太网，它使用CSMA/CD 协议。这种以太网既支持全双工方式，又支持半双工方式，可在全双工方式下工作而无冲突发生。因此，在全双工方式下不使用CSMA/CD协议。 MAC帧格式仍然是802.3标准规定的。保持最短帧长不变，但将一个网段的最大电缆长度减小到100m。帧间时间间隔从原来的9.6us 改为现在的0.96js。 (2）吉比特以太网 吉比特以太网又称千兆以太网，允许在1Gb/s下用全双工和半双工两种方式工作。使用802.3协议规定的帧格式。在半双工方式下使用CSMA/CD协议（全双工方式不需要使用CSMA/CD协议)。与10BASE-T和100BASE-T技术向后兼容。 (3）10吉比特以太网 10吉比特以太网与10Mb/s、100Mbls 和 1Gbls 以太网的帧格式完全相同。10吉比特以太网还保留了802.3标准规定的以太网最小和最大帧长，便于升级。10吉比特以太网不再使用铜线而只使用光纤作为传输媒体。10吉比特以太网只工作在全双工方式，因此没有争用问题，也不使用CSMA/CD协议。 以太网从10Mb/s到10Gb/s的演进证明了以太网是可扩展的(从10Mb/s到10Gb/s)、灵活的(多种传输媒体、全/半双工、共享/交换)，易于安装，稳健性好。 3.6.3IEEE 802.11 IEEE802.11是无线局域网的一系列协议标准，包括802.11a和802.11b等。它们制定了MAC层协议，运行在多个物理层标准上。除基本的协调访问问题外，标准还进行错误控制（以克服通道固有的不可靠性)、适宜的寻址和关联规程（以处理站的可携带性和移动性)、互联过程（以扩展无线站的通信范围)，并且允许用户在移动的同时进行通信。 802.11的MAC层采用CSMA/CA协议进行介质访问控制。冲突避免要求每个发送结点在发送帧之前先侦听信道。如果信道空闲，那么结点可以发送帧;发送站在发送完一帧之后，必须再等待一个短的时间间隔，检查接收站是否发回帧的确认ACK。如果接收到确认，那么说明此次发送未出现冲突，发送成功;如果在规定的时间内没有接收到确认，那么表明出现冲突，发送失败，重发该帧，直到在规定的最大重发次数之内，发送成功。 注意:在无线局域网中，即使在发送过程中发生了碰撞，也要把整个帧发送完毕。而在有线局域网中，发生冲突则结点立即停止发送数据。再次提醒读者要熟悉局域网的各种协议。 无线局域网可分为两大类:固定基础设施无线局域网和无固定基础设施无线局域网自组织网络（Ad Hoc Network)。 有固定基础设施无线局域网 无固定基础设施无线局域网自组织网络 3.6.4令牌环网的基本原理 令牌环网的基本原理如图3.27所示。令牌环网的每一站通过电缆与环接口干线耦合器(TCU)相连。TCU的主要作用是，传递所有经过的帧，为接入站发送和接收数据提供接口。与此对应，TCU 的状态也有两个:收听状态和发送状态。数据总是在某个特定的方向上从一个TCU到下一个TCU逐比特地依次传送，每个TCU重新产生并重新传输每一比特。令牌环网的媒体接入控制机制采用的是分布式控制模式的循环方法。在令牌环网中有一个令牌（Token）沿着环形总线在入网结点计算机间依次传递，令牌实际上是一个特殊格式的MAC控制帧，它本身并不包含信息，仅控制信道的使用，确保在同一时刻只有一个结点能够独占信道。站点只有取得令牌后才能发送数据帧，因此令牌环网不会发生碰撞。由于令牌在网环上是按顺序依次传递的，因此对所有入网计算机而言，访问权是公平的。 令牌环网中令牌和数据的传递过程如下: 1）网络空闲时，环路中只有令牌帧在循环传递。 2）令牌传递到有数据要发送的站点处时，该站点就修改令牌中的一个标志位，","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:6","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"3.7广域网 3.7.1广域网基本概念 3.7.2PPP协议 PPP (Point-to-Point Protocol)是使用串行线路通信的面向字节的协议，该协议应用在直接连接两个结点的链路上。设计的目的主要是用来通过拨号或专线方式建立点对点连接发送数据，使其成为各种主机、网桥和路由器之间简单连接的一种共同的解决方案。 PPP协议是在SLIP协议的基础上发展而来的，它既可以在异步线路上传输，又可在同步线路上使用;不仅用于Modem链路，也用于租用的路由器到路由器的线路。 背景:SLIP主要完成数据报的传送，但没有寻址、数据检验、分组类型识别和数据压缩等功能，只能传送P分组。如果上层不是IP协议，那么无法传输，并且此协议对一些高层应用也不支持，但实现比较简单。为了改进SLIP的缺点，于是制定了点对点协议(PPP)。 PPP协议有三个组成部分: 1)链路控制协议(LCP)。一种扩展链路控制协议，用于建立、配置、测试和管理数据链路。2）网络控制协议(NCP)。PPP协议允许同时采用多种网络层协议，每个不同的网络层协议 要用一个相应的NCP来配置，为网络层协议建立和配置逻辑连接。 3)一个将I数据报封装到串行链路的方法。IP数据报在PPP帧中就是其信息部分，这个信 息部分的长度受最大传送单元(MTU)的限制。 PPP帧的格式如图3.29所示。PPP帧的前3个字段和最后2个字段与HDLC帧是一样的，标志字段(F)仍为7E（01111110)，前后各占1字节，若它出现在信息字段中，就必须做字节填充,使用的控制转义字节是7D(01111101)。但在PPP中，地址字段(A)占1字节，规定为0xFF，控制字段(C)占1字节，规定为0x03，两者的内容始终是固定不变的。PPP是面向字符的，因而所有PPP帧的长度都是整数个字节。 第4个字段是协议段，占2字节，在HDLC中没有该字段，它是说明信息段中运载的是什么种类的分组。以比特О开始的是诸如IP、IPX和AppleTalk这样的网络层协议;以比特1开始的被用来协商其他协议，包括LCP及每个支持的网络层协议的一个不同的NCP。 第5段信息段的长度是可变的，大于等于0且小于等于1500B。为了实现透明传输，当信息段中出现和标志字段一样的比特组合时，必须采用一些措施来改进。 注意:因为PPP是点对点的，并不是总线形，所以无须采用CSMA/CD 协议，自然就没有最短帧，所以信息段占0~1500字节，而不是46~1500字节。另外，当数据部分出现和标志位一样的比特组合时，就需要采用一些措施来实现透明传输。 第6个字段是帧检验序列(FCS)，占2字节，即循环冗余码检验中的冗余码。检验区包括地址字段、控制字段、协议字段和信息字段。 图3.30给出了PPP链路建立、使用、撤销所经历的状态图。当线路处于静止状态时，不存在物理层连接。当线路检测到载波信号时，建立物理连接，线路变为建立状态。此时，LCP开始选项商定，商定成功后就进入身份验证状态。双发身份验证通过后，进入网络状态。这时，采用NCP 配置网络层，配置成功后，进入打开状态，然后就可进行数据传输。当数据传输完成后，线路转为终止状态。载波停止后则回到静止状态。 注意: 1 )PPP提供差错检测但不提供纠错功能，只保证无差错接收（通过硬件进行CRC校验)。 它是不可靠的传输协议，因此也不使用序号和确认机制。 2）它仅支持点对点的链路通信，不支持多点线路。 3 ) PPP只支持全双工链路。 4)PPP的两端可以运行不同的网络层协议，但仍然可使用同一个PPP进行通信。 5 ) PPP是面向字节的，当信息字段出现和标志字段一致的比特组合时，PPP有两种不同的处 理方法:若 PPP用在异步线路（默认)，则采用字节填充法;若PPP用在SONET/SDH等同步线路，则协议规定采用硬件来完成比特填充（和HDLC的做法一样).3.7.3HDLC协议 高级数据链路控制（High-level Data Link Control，HDLC)协议是ISO制定的面向比特（记住PPP协议是面向字节的）的数据链路层协议。该协议不依赖于任何一种字符编码集;数据报文可透明传输，用于实现透明传输的“О比特插入法”易于硬件实现;全双工通信，有较高的数据链路传输效率;所有帧采用CRC 检验，对信息帧进行顺序编号，可防止漏收或重发，传输可靠性高;传输控制功能与处理功能分离，具有较大的灵活性。 HDLC适用于链路的两种基本配置:非平衡配置和平衡配置。 1）非平衡配置的特点是由一个主站控制整个链路的工作。 2）平衡配置的特点是链路两端的两个站都是复合站，每个复合站都可以平等地发起数据传 输，而不需要得到对方复合站的允许。 站 HDLC有3种站类型:主站、从站和复合站。主站负责控制链路的操作，主站发出的帧称为命令帧。从站受控于主站，按主站的命令进行操作;发出的帧称为响应帧。另外，有些站既具有主站的功能，又具有从站的功能，所以这类站称为复合站，它可以发出命令帧和响应帧。 数据操作方式 HDLC有3种数据操作方式: 1)）正常响应方式。这是一种非平衡结构操作方式，即主站向从站传输数据，从站响应传输, 但从站只有在收到主站的许可后，才可进行响应。 2）异步平衡方式。这是一种平衡结构操作方式。在这种方式中，每个复合站都可以进行对 另一站的数据传输。 3）异步响应方式。这是一种非平衡结构操作方式。在这种方式中，从站即使未受到主站的 允许，也可进行传输。 HDLC帧 图3.31所示为HDLC的帧格式，它由标志、地址、控制、信息和帧校验序列(FCS)等字段构成。标志字段F，为01111110。在接收端只要找到标志字段就可确定一个帧的位置。HDLC协议采用比特填充的首尾标志法实现透明传输。在发送端，当一串比特流数据中有5个连续的1时，就立即在其后填入一个0。接收帧时，先找到F字段以确定帧的边界，接着对比特流进行扫描。每当发现5个连续的1时，就将其后的一个0删除，以还原成原来的比特流。 地址字段A，共8位，在使用非平衡方式传送数据时，站地址字段总是写入从站的地址;在使用平衡方式传送数据时，站地址字段填入的是应答站的地址。 控制字段C，共8位，是最复杂的字段。HDILC的许多重要功能都靠控制字段来实现。根据其第1位或第1、2位的取值，可将HDLC帧划分为三类: 1）信息帧(I)，第1位为o，用来传输数据信息，或使用捎带技术对数据进行确认。 2）监督帧(S)，第1、2位分别为1、0，用于流量控制和差错控制，执行对信息帧的确认、 请求重发和请求暂停发送等功能; 3）无编号帧（U)，第1、2位均为1，用于提供对链路的建立、拆除等多种控制功能。由图3.29和图3.31可知，PPP帧和 HDLC帧的格式很相似。但两者有以下几点不同:1 ) PPP协议是面向字节的，HDLC协议是面向比特的。 2)PPP帧比HDLC帧多一个2字节的协议字段。当协议字段值为0x0021时，表示信息字段 是IP数据报。 3）PPP协议不使用序号和确认机制，只保证无差错接收（通过硬件进行CRC 检验)，而端 到端差错检测由高层协议负责。HDLC协议的信息帧使用了编号和确认机制，能够提供可靠传输。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:7","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"3.8数据链路层设备 3.8.1网桥的概念与基本原理 两个或多个以太网通过网桥连接后，就成为一个覆盖范围更大的以太网，而原来的每个以太网就称为一个网段。网桥工作在链路层的MAC子层，可以使以太网各网段成为隔离开的碰撞域。如果把网桥换成工作在物理层的转发器，那么就没有这种过滤通信量的功能。由于各网段相对独立，因此一个网段的故障不会影响到另一个网段的运行。 注意:网桥处理数据的对象是帧，所以它是工作在数据链路层的设备，中继器、放大器处理数据的对象是信号，所以它是工作在物理层的设备。 网络1和网络2通过网桥连接后，网桥接收网络1发送的数据帧，检查数据帧中的地址，如果是网络2的地址，那么就转发给网络2;如果是网络1的地址，那么就将其丢弃，因为源站和目的站处在同一个网段，目的站能够直接收到这个帧而不需要借助网桥转发。 如图3.32所示，设每个网段的数据率都是10Mbls，那么三个网段合起来的最大吞吐量就变成了30Mb/s。如果把两个网桥换成集线器或转发器,那么整个网络仍然是一个碰撞域(即冲突域),当A和B通信时，所有其他站点都不能通信，整个碰撞域的最大吞吐量仍然是10Mb/s。网桥的基本特点:①网桥必须具备寻址和路径选择能力，以确定帧的传输方向;②从源网络接收帧，以目的网络的介质访问控制协议向目的网络转发该帧;③网桥在不同或相同类型的LAN之间存储并转发帧，必要时还进行链路层上的协议转换。注意，一般情况下，存储转发类设备都能进行协议转换，即连接的两个网段可以使用不同的协议;④网桥对接收到的帧不做任何修改，或只对帧的封装格式做很少的修改;⑤网桥可以通过执行帧翻译互联不同类型的局域网，即把原协议的信息段的内容作为另一种协议的信息部分封装在帧中;⑥网桥应有足够大的缓冲空间，因为在短时间内帧的到达速率可能高于转发速率。 网桥的优点:①能过滤通信量;②扩大了物理范围;③可使用不同的物理层;④可互联不同类型的局域网;⑤提高了可靠性;⑥性能得到改善。 网桥的缺点:①增大了时延;②MAC子层没有流量控制功能（流量控制需要用到编号机制，编号机制的实现在LLC子层);③不同MAC子层的网段桥接在一起时，需要进行帧格式的转换;④网桥只适合于用户数不多和通信量不大的局域网，否则有时还会因传播过多的广播信息而产生网络拥塞，这就是所谓的广播风暴。 网桥必须具有路径选择的功能，接收到帧后，要决定正确的路径，将该帧转送到相应的目的局域网站点。根据路径选择算法的不同，可将网桥分为透明网桥和源路由网桥。 透明网桥（选择的不是最佳路由） 透明网桥以混杂方式工作，它接收与之连接的所有LAN传送的每一帧。到达帧的路由选择过程取决于源LAN和目的LAN:①如果源LAN和目的LAN相同,那么丢弃该帧;②如果源LAN和目的LAN不同，那么转发该帧;③如果目的LAN未知，那么扩散该帧。 当网桥刚连接到以太网时，其转发表是空的，网桥按照自学习算法处理收到的帧。该算法的基本思想是:若从站A发出的帧从某端口进入网桥，那么从这个端口出发沿相反方向一定可把一个帧传送到站A。所以网桥每收到一个帧，就记下其源地址和进入网桥的端口，作为转发表中的一个项目（源地址、进入的接口和时间)。在建立转发表时，把帧首部中的源地址写在“地址”一栏的下面。在转发帧时，则根据收到的帧首部中的目的地址来转发。这时就把在“地址”栏下面已经记下的源地址当作目的地址，而把记下的进入端口当作转发端口。网桥就是在这样的转发过程中逐渐将其转发表建立起来的。 为了避免转发的帧在网络中不断地“兜圈子”，透明网桥使用了一种生成树算法（无环)，以确保每个源到每个目的地只有唯一的路径。生成树使得整个扩展局域网在逻辑上形成树形结构，所以工作时逻辑上没有环路，但生成树一般不是最佳路由。 源路由网桥（选择的是最佳路由） 在源路由网桥中，路由选择由发送数据帧的源站负责，网桥只根据数据真正的路由信息对帧进行接收和转发。 源路由网桥对主机是不透明的，主机必须知道网桥的标识及连接到哪个网段上。路由选择由发送帧的源站负责，那么源站如何知道应当选择什么样的路由呢?为了找到最佳的路由，源站以广播方式向目的站发送一个发现帧（Discovery Frame)作为探测之用。源路由的生成过程是:在未知路径前，源站要先发送一个发现帧;途中的每个网桥都转发此帧，最终该发现帧可能从多个途径到达目的站;目的站也将一一发送应答帧;每个应答帧将通过原路径返回，途经的网桥把自己的标志记录在应答帧中;源站选择出一个最佳路由。以后，凡从这个源站向该目的站发送的帧的首部，都必须携带这一路由信息。 此外，发送帧还可以帮助源站确定整个网络可以通过的帧的最大长度。由于发现帧的数量指数式增加，可能会使网络严重拥塞。 两种网桥的比较 使用源路由网桥可以利用最佳路由。若在两个以太网之间使用并联的源路由网桥，则还可使通信量较平均地分配给每个网桥。采用透明网桥时，只能使用生成树，而使用生成树一般并不能保证所用的路由是最佳的，也不能在不同的链路中进行负载均衡。 注意:透明网桥和源路由网桥中提到的最佳路由并不是经过路由器最少的路由，而可以是发送帧往返时间最短的路由，这样才能真正地进行负载平衡，因为往返时间长说明中间某个路由器可能超载了，所以不走这条路，换个往返时间短的路走。 3.8.2局域网交换机及其工作原理 局域网交换机 桥接器的主要限制是在任一时刻通常只能执行一个帧的转发操作，于是出现了局域网交换机，又称以太网交换机。从本质上说，以太网交换机是一个多端口的网桥，它工作在数据链路层。交换机能经济地将网络分成小的冲突域，为每个工作站提供更高的带宽。 以太网交换机对工作站是透明的，因此管理开销低廉，简化了网络结点的增加、移动和网络变化的操作。利用以太网交换机还可以方便地实现虚拟局域网(Virtual LAN，VLAN)，VLAN不仅可以隔离冲突域，而且可以隔离广播域。 原理 以太网交换机的原理是，它检测从以太端口来的数据帧的源和目的地的MAC(介质访问层)地址，然后与系统内部的动态查找表进行比较，若数据帧的MAC地址不在查找表中，则将该地址加入查找表，并将数据帧发送给相应的目的端口。 特点 以太网交换机的特点如下: 1）以太网交换机的每个端口都直接与单台主机相连（普通网桥的端口往往连接到以太网的一个网段)，并且-一般都工作在全双工方式。 2〉以太网交换机能同时连通许多对端口，使每对相互通信的主机都能像独占通信媒体那样，无碰撞地传输数据。 3〉以太网交换机也是一种即插即用设备（和透明网桥一样)，其内部的帧的转发表也是通过自学习算法自动地逐渐建立起来的。 4)以太网交换机由于使用了专用的交换结构芯片，因此交换速率较高。 5）以太网交换机独占传输媒体的带宽。 对于普通10Mb/s的共享式以太网，若共有N个用户，则每个用户占有的平均带宽只有总带宽(10Mb/s）的1/N。在使用以太网交换机时，虽然从每个端口到主机的带宽还是10Mb/s，但由于一个用户在通信时是独占而不是和其他网络用户共享传输媒体的带宽，因此拥有N对端口的交换机的总容量为N×10Mb/s。这正是交换机的最大优点。 以太网交换机一般都具有多种速率的端口，例如可以具有10Mb/s、100Mb/s和1Gb/s的端口的各种组合，因此大大方便了各种不同情况的用户。 两种交换模式 目前，以太网交换机主要采用两种交换模式，即直通式和存储转发式。 1)）直通式交换机只检查帧的目的地址，这使得帧在接收后几乎能马上被传出去。这种方式 速度快，但缺乏智能性和安全性，也无法支持具有不同速率的端口的交换。 2）存储转发式交换机先将接收到的帧缓存到高速缓存器中，并检查数据是否正确，确认无 误后通过查找表转换成输出端口将该帧发送出去。如果发现帧有错，那么就将其丢弃。存储转发式的优点是可靠性高，并能支持不同速率端口间的转换，缺点是延迟较大。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:8","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"3.9本章小结及疑难点 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:3:9","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"第4章网络层 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:0","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"4.1网络层的功能 4.1.1异构网络互联 要在全球范围内把数以百万计的网络互联起来，并且能够互相通信，是一项非常复杂的任务，此时需要解决许多问题，比如不同的寻址方案、不同的网络接入机制、不同的差错处理方法、不同的路由选择机制等。用户的需求是多样的，没有一种单一的网络能适应所有用户的需求。网络层所要完成的任务之一就是使这些异构的网络实现互联。 所谓网络互联，是指将两个以上的计算机网络，通过一定的方法，用一种或多种通信处理设备（即中间设备）相互连接起来，以构成更大的网络系统。中间设备又称中间系统或中继系统。根据所在的层次，中继系统分为以下4种: 1）物理层中继系统:中继器，集线器（Hub)。 2）数据链路层中继系统:网桥或交换机。3）网络层中继系统:路由器。 4)网络层以上的中继系统:网关。 使用物理层或数据链路层的中继系统时，只是把一个网络扩大了，而从网络层的角度看，它仍然是同一个网络，一般并不称之为网络互联。因此网络互联通常是指用路由器进行网络互联和路由选择。路由器是一台专用计算机，用于在互联网中进行路由选择。 TCP/P体系在网络互联上采用的做法是在网络层（即P层）采用标准化协议，但相互连接的网络可以是异构的。图4.1(a)显示了许多计算机网络通过一些路由器进行的互联。由于参加互联的计算机网络都使用相同的网际协议（Internet Protocol，TP)，因此可以把互联后的计算机网络视为如图4.1(b)所示的一个虚拟IP网络。虚拟互联网络也就是逻辑互联网络，即互联起来的各种物理网络的异构性本来是客观存在的，但是通过使用P就可以使这些性能各异的网络在网络层上看起来好像是一个统一的网络。这种使用IP的虚拟互联网络可简称为IP网络。 使用虚拟互联网络的好处是:当互联网上的主机进行通信时，就好像在一个网络上通信一样，而看不见互联的具体的网络异构细节(如具体的编址方案、路由选择协议等)。 4.1.2路由和转发 路由器主要完成两个功能:一是路由选择（确定哪一条路径)，二是分组转发（当一个分组到达时所采取的动作)。前者是根据特定的路由选择协议构造出路由表，同时经常或定期地和相邻路由器交换路由信息而不断地更新和维护路由表。后者处理通过路由器的数据流，关键操作是转发表查询、转发及相关的队列管理和任务调度等。 1）路由选择。指按照复杂的分布式算法，根据从各相邻路由器所得到的关于整个网络拓扑 的变化情况，动态地改变所选择的路由。 2）分组转发。指路由器根据转发表将用户的P数据报从合适的端口转发出去。 路由表是根据路由选择算法得出的，而转发表是从路由表得出的。转发表的结构应当使查找过程最优化，路由表则需要对网络拓扑变化的计算最优化。在讨论路由选择的原理时，往往不去区分转发表和路由表，而是笼统地使用路由表一词。 4.1.3拥塞控制 在通信子网中，因出现过量的分组而引起网络性能下降的现象称为拥塞。例如，某个路由器所在链路的带宽为R B/s，如果IP分组只从它的某个端口进入，那么其速率为r;n B/s。当rn=R时，可能看起来是件“好事”，因为链路带宽被充分利用。但是，如图4.2所示，当分组到达路由器的速率接近R时，平均时延急剧增加，并且会有大量的分组被丢弃（路由器端口的缓冲区是有限的)，整个网络的吞吐量会骤降，源与目的地之间的平均时延也会变得近乎无穷大。 判断网络是否进入拥塞状态的方法是，观察网络的吞吐量与网络负载的关系:如果随着网络负载的增加，网络的吞吐量明显小于正常的吞吐量，那么网络就可能已进入“轻度拥塞”状态;如果网络的吞吐量随着网络负载的增大而下降，那么网络就可能已进入拥塞状态;如果网络的负载继续增大，而网络的吞吐量下降到零，那么网络就可能已进入死锁状态。 为避免拥塞现象的出现，要采用能防止拥塞的一系列方法对子网进行拥塞控制。拥塞控制主要解决的问题是如何获取网络中发生拥塞的信息，从而利用这些信息进行控制，以避免由于拥塞而出现分组的丢失，以及严重拥塞而产生网络死锁的现象。 拥塞控制的作用是确保子网能够承载所达到的流量，这是一个全局性的过程，涉及各方面的行为:主机、路由器及路由器内部的转发处理过程等。单一地增加资源并不能解决拥塞。 流量控制和拥塞控制的区别:流量控制往往是指在发送端和接收端之间的点对点通信量的控制。流量控制所要做的是抑制发送端发送数据的速率，以便使接收端来得及接收。而拥塞控制必须确保通信子网能够传送待传送的数据，是一个全局性的问题，涉及网络中所有的主机、路由器及导致网络传输能力下降的所有因素。 拥塞控制的方法有两种: 1)开环控制。在设计网络时事先将有关发生拥塞的因素考虑周到，力求网络在工作时不产 生拥塞。这是一种静态的预防方法。一旦整个系统启动并运行，中途就不再需要修改。开环控制手段包括确定何时可接收新流量、何时可丢弃分组及丢弃哪些分组，确定何种调度决策等。所有这些手段的共性是，在做决定时不考虑当前网络的状态。 2）闭环控制。事先不考虑有关发生拥塞的各种因素，采用监测网络系统去监视，及时检测 哪里发生了拥塞，然后将拥塞信息传到合适的地方，以便调整网络系统的运行，并解决出现的问题。闭环控制是基于反馈环路的概念，是一种动态的方法。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:1","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"4.2路由算法 4.2.1静态路由和动态路由 路由器转发分组是通过路由表转发的，而路由表是通过各种算法得到的。从能否随网络的通信量或拓扑自适应地进行调整变化来划分，路由算法可分为如下两大类。 静态路由算法（又称非自适应路由算法)。指由网络管理员手工配置的路由信息。当网络的拓扑结构或链路的状态发生变化时，网络管理员需要手工去修改路由表中相关的静态路由信息。大型和复杂的网络环境通常不宜采用静态路由。一方面，网络管理员难以全面了解整个网络的拓扑结构;另一方面，当网络的拓扑结构和链路状态发生变化时，路由器中的静态路由信息需要大范围地调整，这一工作的难度和复杂程度非常高。 动态路由算法（又称自适应路由算法)。指路由器上的路由表项是通过相互连接的路由器之间彼此交换信息，然后按照一定的算法优化出来的，而这些路由信息会在一定时间间隙里不断更新，以适应不断变化的网络，随时获得最优的寻路效果。 静态路由算法的优点是简便、可靠，在负荷稳定、拓扑变化不大的网络中运行效果很好，因此仍广泛用于高度安全的军事系统和较小的商业网络。动态路由算法能改善网络的性能并有助于流量控制;但算法复杂，会增加网络的负担，有时因对动态变化的反应太快而引起振荡，或反应太慢而影响网络路由的一致性，因此要仔细设计动态路由算法，以发挥其优势。常用的动态路由算法可分为两类:距离-向量路由算法和链路状态路由算法。 4.2.2距离向量路由算法 在距离-向量路由算法中,所有结点都定期地将它们的整个路由选择表传送给所有与之直接相邻的结点。这种路由选择表包含: ●每条路径的目的地(另一结点)。●路径的代价（也称距离)。 注意:这里的距离是一个抽象的概念，如RIP就将距离定义为“跳数”。跳数指从源端口到达目的端口所经过的路由个数，每经过一个路由器，跳数加1。 在这种算法中，所有结点都必须参与距离向量交换，以保证路由的有效性和一致性，也就是说，所有的结点都监听从其他结点传来的路由选择更新信息，并在下列情况下更新它们的路由选择表: 1）被通告一条新的路由，该路由在本结点的路由表中不存在，此时本地系统加入这条新的 路由。 2）发来的路由信息中有一条到达某个目的地的路由，该路由与当前使用的路由相比，有较 短的距离（较小的代价)。此种情况下，就用经过发送路由信息的结点的新路由替换路由表中到达那个目的地的现有路由。 距离-向量路由算法的实质是，迭代计算一条路由中的站段数或延迟时间，从而得到到达一个目标的最短(最小代价）通路。它要求每个结点在每次更新时都将它的全部路由表发送给所有相邻的结点。显然，更新报文的大小与通信子网的结点个数成正比，大的通信子网将导致很大的更新报文。由于更新报文发给直接邻接的结点，所以所有结点都将参加路由选择信息交换。基于这些原因，在通信子网上传送的路由选择信息的数量很容易变得非常大。 最常见的距离-向量路由算法是RIP算法，它采用“跳数”作为距离的度量。 4.2.3链路状态路由算法 链路状态路由算法要求每个参与该算法的结点都具有完全的网络拓扑信息，它们执行下述两项任务。第一，主动测试所有邻接结点的状态。两个共享一条链接的结点是相邻结点，它们连接到同一条链路，或者连接到同一广播型物理网络。第二，定期地将链路状态传播给所有其他结点(或称路由结点)。典型的链路状态算法是OSPF算法。 在一个链路状态路由选择中，一个结点检查所有直接链路的状态，并将所得的状态信息发送给网上的所有其他结点，而不是仅送给那些直接相连的结点。每个结点都用这种方式从网上所有其他的结点接收包含直接链路状态的路由选择信息。 每当链路状态报文到达时，路由结点便使用这些状态信息去更新自己的网络拓扑和状态“视野图”，一旦链路状态发生变化,结点就对更新的网络图利用Dijsktra最短路径算法重新计算路由,从单一的源出发计算到达所有目的结点的最短路径。 链路状态路由算法主要有三个特征: 1)向本自治系统（见4.2.4节)中所有路由器发送信息，这里使用的方法是泛洪法，即路由 器通过所有端口向所有相邻的路由器发送信息。而每个相邻路由器又将此信息发往其所有相邻路由器（但不再发送给刚刚发来信息的那个路由器)。 2）发送的信息是与路由器相邻的所有路由器的链路状态，但这只是路由器所知道的部分信 息。所谓“链路状态”，是指说明本路由器与哪些路由器相邻及该链路的“度量”。对于OSPF 算法，链路状态的“度量”主要用来表示费用、距离、时延、带宽等。 3）只有当链路状态发生变化时，路由器才向所有路由器发送此消息。 由于一个路由器的链路状态只涉及相邻路由器的连通状态，而与整个互联网的规模并无直接关系，因此链路状态路由算法可以用于大型的或路由信息变化聚敛的互联网环境。 链路状态路由算法的主要优点是，每个路由结点都使用同样的原始状态数据独立地计算路径，而不依赖中间结点的计算;链路状态报文不加改变地传播，因此采用该算法易于查找故障。当一个结点从所有其他结点接收到报文时，它可以在本地立即计算正确的通路，保证一步汇聚。最后，由于链路状态报文仅运载来自单个结点关于直接链路的信息，其大小与网络中的路由结点数目无关，因此链路状态算法比距离-向量算法有更好的规模可伸展性。 距离-向量路由算法与链路状态路由算法的比较:在距离-向量路由算法中，每个结点仅与它的直接邻居交谈，它为它的邻居提供从自己到网络中所有其他结点的最低费用估计。在链路状态路由算法中，每个结点通过广播的方式与所有其他结点交谈，但它仅告诉它们与它直接相连的链路的费用。相较之下，距离-向量路由算法有可能遇到路由环路等问题。 4.2.4层次路由 当网络规模扩大时，路由器的路由表成比例地增大。这不仅会消耗越来越多的路由器缓冲区空间，而且需要用更多CPU时间来扫描路由表，用更多的带宽来交换路由状态信息。因此路由选择必须按照层次的方式进行。 因特网将整个互联网划分为许多较小的自治系统（注意一个自治系统中包含很多局域网)，每个自治系统有权自主地决定本系统内应采用何种路由选择协议。如果两个自治系统需要通信，那么就需要一种在两个自治系统之间的协议来屏蔽这些差异。据此，因特网把路由选择协议划分为两大类: 1)一个自治系统内部所使用的路由选择协议称为内部网关协议(IGP)，也称域内路由选择， 具体的协议有RIP和 OSPF等。 2〉自治系统之间所使用的路由选择协议称为外部网关协议（EGP)，也称域间路由选择，用 在不同自治系统的路由器之间交换路由信息，并负责为分组在不同自治系统之间选择最优的路径。具体的协议有BGP。 使用层次路由时，OSPF 将一个自治系统再划分为若干区域(Area)，每个路由器都知道在本区域内如何把分组路由到目的地的细节，但不用知道其他区域的内部结构。 采用分层次划分区域的方法虽然会使交换信息的种类增多，但也会使OSPF 协议更加复杂。但这样做却能使每个区域内部交换路由信息的通信量大大减小，因而使OSPF 协议能够用于规模很大的自治系统中。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:2","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"4.3 IPv4 4.3.1IPv4分组 IPv4即现在普遍使用的IP(版本4)。IP定义数据传送的基本单元——IP分组及其确切的数据格式。I 也包括一套规则，指明分组如何处理、错误怎样控制。特别是P还包含非可靠投递的思想，以及与此关联的分组路由选择的思想。 IPv4分组的格式 一个IP分组由首部和数据两部分组成。首部前一部分的长度固定，共20B，是所有IP分组必须具有的。在首部固定部分的后面是一些可选字段，其长度可变，用来提供错误检测及安全等机制。IP数据报的格式如图4.3所示。IP首部的部分重要字段含义如下: 1）版本。指IP的版本，目前广泛使用的版本号为4。 2）首部长度。占4位。以32位为单位，最大值为60B (15x4B)。最常用的首部长度是20B, 此时不使用任何选项（即可选字段)。 3)总长度。占16位。指首部和数据之和的长度，单位为字节，因此数据报的最大长度为216-1=65535B。以太网帧的最大传送单元(MTU)为1500B，因此当一个P数据报封装成帧时，数据报的总长度（首部加数据）一定不能超过下面数据链路层的MTU值。 4)标识。占16位。它是一个计数器，每产生一个数据报就加1，并赋值给标识字段。但它 并不是“序号”(因为P是无连接服务)。当一个数据报的长度超过网络的MTU时，必须分片，此时每个数据报片都复制一次标识号，以便能正确重装成原来的数据报。 5）标志。占3位。标志字段的最低位为MF，MF= 1表示后面还有分片，MF=0表示最后一个分片。标志字段中间的一位是DF，只有当DF=0时才允许分片。 6）片偏移。占13位。它指出较长的分组在分片后，某片在原分组中的相对位置。片偏移以8个字节为偏移单位，即每个分片的长度一定是8B(64位）的整数倍。 7）首部校验和。占16位。P数据报的首部校验和只校验分组的首部，而不校验数据部分。8）生存时间（TTL)。占8位。数据报在网络中可通过的路由器数的最大值，标识分组在网 络中的寿命，以确保分组不会永远在网络中循环。路由器在转发分组前，先把TTL减1。若TTL被减为0，则该分组必须丢弃。 9）协议。占8位。指出此分组携带的数据使用何种协议，即分组的数据部分应交给哪个传 输层协议，如TCP、UDP等。其中值为6表示TCP，值为17表示UDP。 10）源地址字段。占4B，标识发送方的P地址。 11）目的地址字段。占4B，标识接收方的P地址。 注意:在IP数据报首部中有三个关于长度的标记，一个是首部长度、一个是总长度、一个是片偏移，基本单位分别为4B、1B、8B(这个一定要记住)。题目中经常会出现这几个长度之间的加减运算。另外，读者要熟悉IP数据报首部的各个字段的意义和功能，但不需要记忆P数据报的首部，正常情况下如果需要参考首部，题目都会直接给出。第5章学到的TCP、UDP的首部也是一样的。 IP数据报分片 一个链路层数据报能承载的最大数据量称为最大传送单元(MTU)。因为IP数据报被封装在链路层数据报中，因此链路层的MTU严格地限制着IP数据报的长度，而且在P数据报的源与目的地路径上的各段链路可能使用不同的链路层协议，有不同的MTU。例如，以太网的MTU为1500B，而许多广域网的MTU不超过576B。当P数据报的总长度大于链路MTU时，就需要将IP数据报中的数据分装在两个或多个较小的I数据报中，这些较小的数据报称为片。 片在目的地的网络层被重新组装。目的主机使用IP首部中的标识、标志和片偏移字段来完成对片的重组。创建一个P数据报时，源主机为该数据报加上一个标识号。当一个路由器需要将一个数据报分片时，形成的每个数据报（即片）都具有原始数据报的标识号。当目的主机收到来自同一发送主机的一批数据报时，它可以通过检查数据报的标识号来确定哪些数据报属于同一个原始数据报的片。IP首部中的标志位有3比特，但只有后2比特有意义，分别是 MF位(MoreFragment）和 DF位(Don’t Fragment)。只有当DF=0时，该P数据报才可以被分片。MF则用来告知目的主机该IP数据报是否为原始数据报的最后一个片。当MF = 1时，表示相应的原始数据报还有后续的片;当MF = 0时，表示该数据报是相应原始数据报的最后一个片。目的主机在对片进行重组时，使用片偏移字段来确定片应放在原始IP数据报的哪个位置。 IP分片涉及一定的计算。例如，一个长4000B的IP数据报（首部20B，数据部分3980B)到达一个路由器，需要转发到一条MTU为1500B的链路上。这意味着原始数据报中的3980B数据必须被分配到3个独立的片中（每片也是一个IP数据报)。假定原始数据报的标识号为777，那么分成的3片如图4.4所示。可以看出，由于偏移值的单位是8B，所以除最后一个片外，其他所有片中的有效数据载荷都是8的倍数。 网络层转发分组的流程 网络层的路由器执行的分组转发算法如下: 1）从数据报的首部提取目的主机的P地址D，得出目的网络地址N。 2）若网络N与此路由器直接相连，则把数据报直接交付给目的主机D，这称为路由器的直 接交付;否则是间接交付，执行步骤3)。 3）若路由表中有目的地址为D的特定主机路由（对特定的目的主机指明一个特定的路由， 通常是为了控制或测试网络，或出于安全考虑才采用的)，则把数据报传送给路由表中所指明的下一跳路由器;否则，执行步骤4)。 4）若路由表中有到达网络N的路由，则把数据报传送给路由表指明的下一跳路由器;否则, 执行步骤5)。 5）若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器;否则， 执行步骤6)。 6）报告转发分组出错。 注意:得到下一跳路由器的IP地址后并不是直接将该地址填入待发送的数据报，而是将该IP地址转换成MAC地址（通过ARP，见 4.3.4节)，将其放到MAC帧首部中，然后根据这个MAC地址找到下一跳路由器。在不同网络中传送时，MAC帧中的源地址和目的地址要发生变化,但是网桥在转发帧时，不改变帧的源地址，请注意区分。 4.3.2IPv4地址与NAT IPv4地址 连接到因特网上的每台主机(或路由器）都分配一个32比特的全球唯一标识符，即P地址。传统的IP地址是分类的地址，分为A、B、C、D、E五类。 无论哪类地址，都由网络号和主机号两部分组成。即P地址::= {\u003c网络号\u003e,\u003c主机号\u003e}。其中网络号标志主机（或路由器）所连接到的网络。一个网络号在整个因特网范围内必须是唯一的。主机号标志该主机（或路由器)。一台主机号在它前面的网络号所指明的网络范围内必须是唯的。由此可见，一个IP地址在整个因特网范围内是唯一的。 分类的P地址如图4.5所示。在各类P地址中，有些IP地址具有特殊用途，不用做主机的P地址:·主机号全为0表示本网络本身，如202.98.174.0。 主机号全为1表示本网络的广播地址，又称直接广播地址，如202.98.174.255。 127.0.0.0保留为环路自检(Loopback Test)地址，此地址表示任意主机本身，目的地址为环回地址的P数据报永远不会出现在任何网络上。 32位全为0，即0.0.0.0表示本网络上的本主机。 32位全为1，即255.255.255.255表示整个TCP/IP网络的广播地址，又称受限广播地址。实际使用时，由于路由器对广播域的隔离，255.255.255.255等效为本网络的广播地址。常用的三种类别P地址的使用范围见表4.1。 在表4.1中，A类地址可用的网络数为$2^7$-2，减2的原因是:第一，网络号字段全为0的IP地址是保留地址，意思是“本网络”，第二，网络号为127的P地址是环回测试地址。B类地址的可用网络数为$2^14$-1，减1的原因是128.0这个网络号是不可指派的。C类地址的可用网络数为$2^21$-1，减1的原因是网络号为192.0.0的网络是不可指派的。IP地址有以下重要特点: 1)每个P地址都由网络号和主机号两部分组成，因此IP地址是一种分等级的地址结构。分 等级的好处是:①IP地址管理机构在分配P地址时只分配网络号（第一级)，而主机号(第二级）则由得到该网络的单位自行分配，方便了IP地址的管理;②路由器仅根据目的主机所连接的网络号来转发分组（而不考虑目标主机号)，从而减小了路由表所占的存储空间。 2)IP地址是标志一台主机（或路由器）和一条链路的接口。当一台主机同时连接到两个网 络时，该主机就必须同时具有两个相应的P地址，每个地址的网络号必须与所在网络的网络号相同，且这两个P地址的网络号是不同的。因此P网络上的一个路由器必然至少应具有两个P地址（路由器每个端口必须至少分配一个IP地址)。 3）用转发器或桥接器（网桥等）连接的若干LAN仍然是同一个网络（同一个广播域)，因 此该LAN中所有主机的IP地址的网络号必须相同，但主机号必须不同。 4）在IP地址中，所有分配到网络号的网络（无论是LAN还是WAN）都是平等的。 5）在同一个局域网上的主机或路由器的P地址中的网络号必须是一样的。路由器总是具有 两个或两个以上的IP地址，路由器的每个端口都有一个不同网络号的IP地址。 网络地址转化 网络地址转换(NAT)是指通过将专用网络地址(如Intranet）转换为公用地址(如Internet),从而对外隐藏内部管理的IP地址。它使得整个专用网只需要一个全球IP地址就可以与因特网连通，由于专用网本地IP地址是可重用的，所以NAT大大节省了P地址的消耗。同时，它隐藏了内部网络结构，从而降低了内部网络受到攻击的风险。 此外，为了网络安全，划出了部分IP地址为私有P地址。私有IP地址只用于LAN，不用于WAN连接(","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:3","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"4.4 IPv6 4.4.1IPv6主要特点 解决IP地址耗尽问题的措施有以下三种:①采用无类别编址CIDR，使P地址的分配更加合理;②采用网络地址转换(NAT)方法以节省全球IP地址;③采用具有更大地址空间的新版本的IPv6。其中前两种方法只是延长了IPv4地址分配结束的时间，只有第三种方法从根本上解决了IP地址的耗尽问题。 IPv6的主要特点如下: 1)更大的地址空间。IPv6将地址从IPv4的32位增大到了128位。IPv6的字节数〈16B）是IPv4字节数（4B）的平方。 2）扩展的地址层次结构。3）灵活的首部格式。4）改进的选项。 5）允许协议继续扩充。 6）支持即插即用（即自动配置)。7）支持资源的预分配。 8）IPv6 只有在包的源结点才能分片，是端到端的，传输路径中的路由器不能分片，所以从一般意义上说，IPv6不允许分片（不允许类似IPv4在路由分片)。 9)IPv6首部长度必须是8B的整数倍，而IPv4首部是4B的整数倍。10）增大了安全性。身份验证和保密功能是IPv6的关键特征。 虽然IPv6与IPv4不兼容，但总体而言它与所有其他的因特网协议兼容，包括TCP、UDP、ICMP、IGMP、OSPF、BGP和 DNS，只是在少数地方做了必要的修改(大部分是为了处理长的地址)。IPv6相当好地满足了预定的目标，主要体现在: 1）首先也是最重要的，IPv6有比 IPv4长得多的地址。IPv6的地址用16个字节表示，地址 空间是IPv4的2128-32=2倍，从长远来看，这些地址是绝对够用的。 2）简化了IP分组头，它包含8个域(IPv4是 12个域)。这一改变使得路由器能够更快地处 理分组，从而可以改善吞吐率。 3）更好地支持选项。这一改变对新的分组首部很重要，因为一些从前必要的段现在变成了 可选段。此外，表示选项的方式的改变还能加快分组的处理速度。 4.4.2IPv6地址 IPv6数据报的目的地址可以是以下三种基本类型地址之一:1）单播。单播就是传统的点对点通信。 2〉多播。多播是一点对多点的通信，分组被交付到一组计算机的每台计算机。 3)任播。这是IPv6增加的一种类型。任播的目的站是一组计算机，但数据报在交付时只交 付其中的一台计算机，通常是距离最近的一台计算机。 IPv4地址通常使用点分十进制表示法。如果 IPv6也使用这种表示法，那么地址书写起来将会相当长。在IPv6标准中指定了一种比较紧凑的表示法，即把地址中的每4位用一个十六进制数表示，并用冒号分隔每16位，如4BF5:AA12:0216:FEBC:BA5F:039A:BE9A:2170。 通常可以把IPv6地址缩写成更紧凑的形式。当16位域的开头有一些0时，可以采用一种缩写表示法，但在域中必须至少有一个数字。例如，可以把地址4BF5:0000:0000:0000:BA 5F:039A:000A:2176缩写为4BF5:0:0O:0:BA5F:39A:A:2176。 当有相继的О值域时，还可以进一步缩写。这些域可以用双冒号缩写(::)。当然，双冒号表示法在一个地址中仅能出现一次，因为0值域的个数没有编码，需要从指定的总的域的个数来推算。这样一来，前述地址可被更紧凑地书写成4BF5:;BA5F:39A:A:2176。 IPv6扩展了IPv4地址的分级概念，它使用以下3个等级:第一级（顶级）指明全球都知道的公共拓扑;第二级（场点级）指明单个场点;第三级指明单个网络接口。IPv6地址采用多级体系主要是为了使路由器能够更快地查找路由。 IPv4向IPv6过渡只能采用逐步演进的办法,同时还必须使新安装的IPv6系统能够向后兼容。IPv6系统必须能够接收和转发IPv4分组，并且能够为IPv4分组选择路由。 IPv4向IPv6过渡可以采用双协议栈和隧道技术两种策略:双协议栈技术是指在一台设备上同时装有IPv4和IPv6协议栈，那么这台设备既能和IPv4网络通信，又能和IPv6网络通信。如果这台设备是一个路由器，那么在路由器的不同接口上分别配置了IPv4地址和IPv6地址，并很可能分别连接了IPv4网络和IPv6网络;如果这台设备是一台计算机，那么它将同时拥有IPv4地址和IPv6地址，并具备同时处理这两个协议地址的功能。隧道技术是将整个IPv6数据报封装到IPv4数据报的数据部分，使得IPv6数据报可以在IPv4网络的隧道中传输。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:4","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"4.5路由协议 4.5.1自治系统 自治系统（Autonomous System，AS):单一技术管理下的一组路由器，这些路由器使用一种AS内部的路由选择协议和共同的度量来确定分组在该AS内的路由，同时还使用一种AS之间的路由选择协议来确定分组在AS之间的路由。 一个自治系统内的所有网络都由一个行政单位（如一家公司、一所大学、一个政府部门等)管辖，一个自治系统的所有路由器在本自治系统内都必须是连通的。 4.5.2域内路由与域间路由 自治系统内部的路由选择称为域内路由选择，自治系统之间的路由选择称为域间路由选择。因特网有两大类路由选择协议。 内部网关协议 内部网关协议即在一个自治系统内部使用的路由选择协议，它与互联网中其他自治系统选用什么路由选择协议无关。目前这类路由选择协议使用得最多，如RIP和 OSPF。 外部网关协议 若源站和目的站处在不同的自治系统中，当数据报传到一个自治系统的边界时（两个自治系统可能使用不同的IGP)，就需要使用一种协议将路由选择信息传递到另一个自治系统中。这样的协议就是外部网关协议（EGP)。目前使用最多的外部网关协议是 BGP-4。 图4.7是两个自治系统互联的示意图。每个自治系统自己决定在本自治系统内部运行哪个内部路由选择协议（例如，可以是RIP，也可以是OSPF)，但每个自治系统都有一个或多个路由器(图中的路由器R1和R2)。除运行本系统的内部路由选择协议外，还要运行自治系统间的路由选择协议（如 BGP-4)。 4.5.3路由信息协议（RIP） 路由信息协议（Routing Information Protocol，RIP）是内部网关协议(IGP）中最先得到广泛应用的协议。RIP是一种分布式的基于距离向量的路由选择协议，其最大优点就是简单。 RIP规定 1）网络中的每个路由器都要维护从它自身到其他每个目的网络的距离记录〈因此这是一组 距离，称为距离向量)。 2〉距离也称跳数(Hop Count)，规定从一个路由器到直接连接网络的距离（跳数〉为1。而 每经过一个路由器，距离（跳数）加1。 3）RIP认为好的路由就是它通过的路由器的数目少，即优先选择跳数少的路径。 4）RIP允许一条路径最多只能包含15个路由器（即最多允许15跳)。因此距离等于16时， 它表示网络不可达。可见RIP只适用于小型互联网。距离向量路由可能会出现环路的情况，规定路径上的最高跳数的目的是为了防止数据报不断循环在环路上，减少网络拥塞的可能性。 5）RIP默认在任意两个使用RIP的路由器之间每30秒广播一次RIP路由更新信息，以便自 动建立并维护路由表（动态维护)。 6）在 RIP中不支持子网掩码的RIP广播，所以RIP中每个网络的子网掩码必须相同。但在 新的RIP2中，支持变长子网掩码和CIDR。 RIP的特点（注意与OSPF的特点比较）1）仅和相邻路由器交换信息。 2）路由器交换的信息是当前路由器所知道的全部信息，即自己的路由表。3）按固定的时间间隔交换路由信息，如每隔30秒。 RIP通过距离向量算法来完成路由表的更新。最初，每个路由器只知道与自己直接相连的网络。通过每30秒的RIP广播，相邻两个路由器相互将自己的路由表发给对方。于是经过第一次RIP广播，每个路由器就知道了与自己相邻的路由器的路由表〈即知道了距离自己跳数为Ⅰ的网络的路由)。同理,经过第二次RIP广播,每个路由器就知道了距离自己跳数为2的网络的路由……因此经过若干RIP广搢后，所有路由器都最终知道了整个P网络的路由表，成为RIP最终是收敛的。通过RIP收敛后,每个路由器到每个目标网络的路由都是距离最短的(即跳数最少，最短路由),哪怕还存在另一条高速（低时延〉但路由器较多的路由。 距离向量算法 每个路由表项目都有三个关键数据:\u003c目的网络N，距离d，下一跳路由器X\u003e。对于每个相邻路由器发送过来的RIP报文，执行如下步骤: 1）对地址为X的相邻路由器发来的RIP报文，先修改此报文中的所有项目:把“下一跳” 字段中的地址都改为X，并把所有“距离”字段的值加1。 2）对修改后的RIP报文中的每个项目，执行如下步骤; 当原来的路由表中没有目的网络N时，把该项目添加到路由表中。 当原来的路由表中有目的网络N，且下一跳路由器的地址是X时，用收到的项目替换 原路由表中的项目。 ③当原来的路由表中有目的网络N，且下一跳路由器的地址不是X时，如果收到的项目 中的距离d小于路由表中的距离，那么就用收到的项目替换原路由表中的项目﹔否则什么也不做。 3）如果180秒(RIP默认超时时间为180秒)还没有收到相邻路由器的更新路由表，那么把 此相邻路由器记为不可达路由器，即把距离设置为16（距离为16表示不可达)。 4）返回。 RIP最大的优点是实现简单、开销小、收敛过程较快。RIP的缺点如下:1) RIP限制了网络的规模，它能使用的最大距离为15(16表示不可达)。 2）路由器之间交换的是路由器中的完整路由表，因此网络规模越大，开销也越大。 3)网络出现故障时，会出现慢收敛现象（即需要较长时间才能将此信息传送到所有路由器), 俗称“坏消息传得慢”，使更新过程的收敛时间长。 RIP是应用层协议，它使用UDP传送数据（端口520)。RIP选择的路径不一定是时间最短的，但一定是具有最少路由器的路径。因为它是根据最少的跳数进行路径选择的。 4.5.4开放最短路径优先(OSPF)协议 基本特点 开放最短路径优先（OSPF）协议是使用分布式链路状态路由算法的典型代表，也是内部网关协议（IGP）的一种。OSPF与 RIP相比有以下4点主要区别: 1)OSPF向本自治系统中的所有路由器发送信息，这里使用的方法是洪泛法。而RIP仅向自 己相邻的几个路由器发送信息。 2）发送的信息是与本路由器相邻的所有路由器的链路状态，但这只是路由器所知道的部分 信息。“链路状态”说明本路由器和哪些路由器相邻及该链路的“度量”(或代价)。而在RIP中，发送的信息是本路由器所知道的全部信息，即整个路由表。 3）只有当链路状态发生变化时，路由器才用洪泛法向所有路由器发送此信息，并且更新过 程收敛得快，不会出现 RIP“坏消息传得慢”的问题。而在 RIP中，不管网络拓扑是否发生变化，路由器之间都会定期交换路由表的信息。 4)OSPF是网络层协议，它不使用UDP或TCP，而直接用P数据报传送（其P数据报首 部的协议字段为89)。而RIP是应用层协议，它在传输层使用UDP。 除以上区别外，OSPF 还有以下特点: 1)OSPF对不同的链路可根据分组的不同服务类型（TOS）而设置成不同的代价。因此， OSPF 对于不同类型的业务可计算出不同的路由，十分灵活。 2）如果到同一个目的网络有多条相同代价的路径，那么可以将通信量分配给这几条路径。 这称为多路径间的负载平衡。 3）所有在OSPF路由器之间交换的分组都具有鉴别功能，因而保证了仅在可信赖的路由器 之间交换链路状态信息。 4）支持可变长度的子网划分和无分类编址CIDR。 5）每个链路状态都带上一个32位的序号，序号越大，状态就越新。 基本工作原理 由于各路由器之间频繁地交换链路状态信息，因此所有路由器最终都能建立一个链路状态数据库。这个数据库实际上就是全网的拓扑结构图，它在全网范围内是一致的（称为链路状态数据库的同步)。然后，每个路由器根据这个全网拓扑结构图，使用 Dijkstra最短路径算法计算从自己到各目的网络的最优路径，以此构造自己的路由表。此后，当链路状态发生变化时，每个路由器重新计算到各目的网络的最优路径，构造新的路由表。注意:虽然使用Dijkstra算法能计算出完整的最优路径，但路由表中不会存储完整路径，而只存储“下一跳”(只有到了下一跳路由器，才能知道再下一跳应当怎样走). 为使OSPF 能够用于规模很大的网络，OSPF将一个自治系统再划分为若干更小的范围，称为区域。划分区域的好处是，将利用洪泛法交换链路状态信息的范围局限于每个区域而非整个自治系统，减少了整个网络上的通信量。在一个区域内部的路由器只知道本区域的完整网络拓扑，而不知道其他区域的网络拓扑情况。这些区域也有层次之分。处在上层的域称为主干区域，负责连通其他下层的区域，并且还连接其他自治域。 五种分组类型 OSPF共有以下五种分组类型: 1)问候分组，用来发现和维持邻站的可达性。 2）数据库描述分组，向邻站给出自己的链路状态数据库中的所有链路状态项目的摘要信息。3）链路状态请求分组，向对方请求发送某些链路状态项目的详细信息。 4）链路状态更新分组，用洪泛法对全网更新链路状态。 5）链路状态确认分组，对链路更新分组的确认。 通常每隔10秒，每两个相邻路由器要交换一次问候分组，以便知道哪些站可达。在路由器刚开始工作时，OSPF 让每个路由器使用数据库描述分组和相邻路由器交换本数据库中已有的链路状态摘要信息。然后，路由器使用链路状态请求分组，向对方请求发送自己所缺少的某些链路状态项目的详细信息。经过一系列的这种分组交换，就建立了全网同步的链路数据库。图4.8给出了OSPF 的基本操作，说明了两个路由器需要交换的各种类型的分组。在网络运行的过程中，只要一个路由器的链路状态发生变化，该路由器就要使用链路状态更新分组，用洪泛法向全网更新链路状态。其他路由器在更新后，发送链路状态确认分组对更新分组进行确认。 为了确保链路状态数据库与全网的状态保持一致，OSPF还规定每隔一段时间（如30分钟)就刷新一次数据库中的链路状态。由于一个路由器的链路状态只涉及与相邻路由器的连通状态，因而与整个互联网的规模并无直接关系。因此，当互联网规模很大时，OSPF 要比R","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:5","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"4.6 IP组播 4.6.1组播概念 为了能够支持像视频点播和视频会议这样的多媒体应用，网络必须实施某种有效的组播机制。使用多个单播传送来仿真组播总是可能的，但这会引起主机上大量的处理开销和网络上太多的交通量。人们所需要的组播机制是让源计算机一次发送的单个分组可以抵达用一个组地址标识的若干目标主机，并被它们正确接收。 组播一定仅应用于UDP，它对将报文同时送往多个接收者的应用来说非常重要。而TCP是一个面向连接的协议，它意味着分别运行于两台主机(由lP地址来确定)内的两个进程（由端口号来确定）之间存在一条连接，因此会一对一地发送。 使用组播的缘由是，有的应用程序要把一个分组发送给多个目的地主机。不是让源主机给每个目的地主机都发送一个单独的分组，而是让源主机把单个分组发送给一个组播地址，该组播地址标识一组地址。网络（如因特网）把这个分组的副本投递给该组中的每台主机。主机可以选择加入或离开一个组，因此一台主机可以同时属于多个组。 因特网中的IP 组播也使用组播组的概念，每个组都有一个特别分配的地址，要给该组发送的计算机将使用这个地址作为分组的目标地址。在IPv4中，这些地址在D类地址空间中分配，而IPv6也有一部分地址空间保留给组播组。 主机使用一个称为IGMP(因特网组管理协议）的协议加入组播组。它们使用该协议通知本地网络上的路由器关于要接收发送给某个组播组的分组的愿望。通过扩展路由器的路由选择和转发功能，可以在许多路由器互联的支持硬件组播的网络上面实现因特网组播。 需要注意的是，主机组播时仅发送一份数据，只有数据在传送路径出现分岔时才将分组复制后继续转发。因此，对发送者而言，数据只需发送一次就可发送到所有接收者，大大减轻了网络的负载和发送者的负担。组播需要路由器的支持才能实现，能够运行组播协议的路由器称为组播路由器。单播与组播的比较如图4.10所示。4.6.2IP组播地址 IP组播使用D类地址格式。D类地址的前四位是1110，因此D类地址范围是224.0.0.0～239.255.255.255。每个D类IP地址标志一个组播组。 组播数据报和一般的数据报的区别是，前者使用D类P地址作为目的地址，并且首部中的协议字段值是2，表明使用IGMP。需要注意的是: 1）组播数据报也是“尽最大努力交付”，不提供可靠交付。2）组播地址只能用于目的地址，而不能用于源地址。 3）对组播数据报不产生ICMP差错报文。因此，若在PING命令后面键入组播地址，将永远 不会收到响应。 4）并非所有的D类地址都可作为组播地址。 I组播可以分为两种:一种只在本局域网上进行硬件组播;另一种则在因特网的范围内进行组播。在因特网上进行组播的最后阶段，还是要把组播数据报在局域网上用硬件组播交付给组播组的所有成员〔见图4.10(b)]。下面讨论这种硬件组播。 IANA拥有的以太网组播地址的范围是从01-00-5E-00-00-00到01-00-5E-7F-FF-FF。不难看出，在每个地址中，只有23位可用作组播。这只能和D类P地址中的23位有一一对应关系。D类I地址可供分配的有28位,可见在这28位中,前5位不能用来构成以太网的硬件地址,如图4.11所示。例如，IP组播地址224.128.64.32（即 EO-80-40-20）和另一个P组播地址224.0.64.32（即EO-00-40-20)转换成以太网的硬件组播地址都是01-00-5E-00-40-20。由于组播IP地址与以太网硬件地址的映射关系不是唯一的，因此收到组播数据报的主机，还要在lP层利用软件进行过滤，把不是本主机要接收的数据报丢弃。 4.6.3IGMP与组播路由算法 要使路由器知道组播组成员的信息，需要利用因特网组管理协议(Internet Group ManagementProtocol，IGMP)。连接到局域网上的组播路由器还必须和因特网上的其他组播路由器协同工作，以便把组播数据报用最小代价传送给所有组成员，这就需要使用组播路由选择协议。 IGMP并不是在因特网范围内对所有组播组成员进行管理的协议。IGMP不知道IP组播组包含的成员数，也不知道这些成员分布在哪些网络上。IGMP让连接到本地局域网上的组播路由器知道本局域网上是否有主机参加或退出了某个组播组。 IGMP应视为TCP/IP的一部分，其工作可分为两个阶段。 第一阶段:当某台主机加入新的组播组时，该主机应向组播组的组播地址发送一个IGMP报文，声明自己要成为该组的成员。本地的组播路由器收到IGMP报文后，将组成员关系转发给因特网上的其他组播路由器。 第二阶段:因为组成员关系是动态的，本地组播路由器要周期性地探询本地局域网上的主机,以便知道这些主机是否仍继续是组的成员。只要对某个组有一台主机响应，那么组播路由器就认为这个组是活跃的。但一个组在经过几次的探询后仍然没有一台主机响应时，则不再将该组的成员关系转发给其他的组播路由器。 组播路由选择实际上就是要找出以源主机为根结点的组播转发树，其中每个分组在每条链路上只传送一次（即在组播转发树上的路由器不会收到重复的组播数据报)。不同的多播组对应于不同的多播转发树;同一个多播组，对不同的源点也会有不同的多播转发树。 在许多由路由器互联的支持硬件多点传送的网络上实现因特网组播时，主要有三种路由算法:第一种是基于链路状态的路由选择;第二种是基于距离-向量的路由选择;第三种可以建立在任何路由器协议之上，因此称为协议无关的组播(PIM)。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:6","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"4.7移动IP 4.7.1概念 支持移动性的因特网体系结构与协议共称为移动IP，它是为了满足移动结点(计算机、服务器、网段等）在移动中保持其连接性而设计的。更确切地说，移动技术是指移动结点以固定的网络P地址实现跨越不同网段的漫游功能，并保证基于网络的网络权限在漫游过程中不发生任何改变。移动I的目标是把分组自动地投递给移动结点。一个移动结点是把其连接点从一个网络或子网改变到另一个网络或子网的主机。使用移动IP，一个移动结点可以在不改变其P地址的情况下改变其驻留位置。 基于IPv4的移动P定义三种功能实体:移动结点、归属代理（也称本地代理）和外埠代理(也称外部代理)。归属代理和外埠代理又统称为移动代理。 1）移动结点。具有永久IP地址的移动结点。 2）本地代理。在一个网络环境中，一个移动结点的永久“居所”被称为归属网络，在归属 网络中代表移动结点执行移动管理功能的实体称为归属代理（本地代理)，它根据移动用户的转交地址，采用隧道技术转交移动结点的数据包。 3）外部代理。在外部网络中帮助移动结点完成移动管理功能的实体称为外部代理。 移动I和移动自组网络并不相同,移动IP技术使漫游的主机可以用多种方式连接到因特网，移动IP的核心网络功能仍然是基于固定互联网中一直使用的各种路由选择协议,移动自组网络是将移动性扩展到无线领域中的自治系统，它具有自己独特的路由选择协议，并且可以不和因特网相联。 移动IP与动态IP是两个完全不同的概念，动态I指的是局域网中的计算机可以通过网络中的DHCP服务器动态地获得一个IP地址，而不需要用户在计算机的网络设置中指定IP地址，动态IP和 DHCP经常会应用在我们的实际工作环境中。 4.7.2移动IP通信过程 在移动IP中，每个移动结点都有一个唯一的本地地址，当移动结点移动时，它的本地地址是不变的，在本地网络链路上每个本地结点还必须有一个本地代理来为它维护当前的位置信息，这就需要引入转交地址。当移动结点连接到外地网络链路上时，转交地址就用来标识移动结点现在所处的位置，以便进行路由选择。移动结点的本地地址与当前转交地址的联合称为移动绑定或简称绑定。当移动结点得到一个新的转交地址时，通过绑定向本地代理进行注册，以便让本地代理即时了解移动结点的当前位置。 移动IP技术的基本通信流程如下: 1）移动结点在本地网时，按传统的TCP/IP方式进行通信（在本地网中有固有的地址)。2〉移动结点漫游到一个外地网络时，仍然使用固定的地址进行通信。为了能够收到通信 对端发给它的IP分组，移动结点需要向本地代理注册当前的位置地址，这个位置地址就是转交地址（它可以是外部代理的地址或动态配置的一个地址)。 3）本地代理接收来自转交地址的注册后，会构建一条通向转交地址的隧道，将截获的发给 移动结点的P分组通过隧道送到转交地址处。 4）在转交地址处解除隧道封装，恢复原始的P分组，最后送到移动结点，这样移动结点在 外网就能够收到这些发送给它的I分组。 5）移动结点在外网通过外网的路由器或外部代理向通信对端发送IP数据包。 6）移动结点来到另一个外网时，只需向本地代理更新注册的转交地址，就可继续通信。7）移动结点回到本地网时，移动结点向本地代理注销转交地址，这时移动结点又将使用传 统的TCP/IP方式进行通信。 移动IP为移动主机设置了两个IP地址，即主地址和辅地址（转交地址)。移动主机在本地网时，使用的是主地址。当移动到另一个网络时，需要获得一个临时的辅地址，但此时主地址仍然不变。从外网移回本地网时，辅地址改变或撤销，而主地址仍然保持不变。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:7","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"4.8网络层设备 4.8.1路由器的组成和功能 路由器是一种具有多个输入/输出端口的专用计算机，其任务是连接不同的网络（连接异构网络）并完成路由转发。在多个逻辑网络（即多个广播域）互联时必须使用路由器。 当源主机要向目标主机发送数据报时，路由器先检查源主机与目标主机是否连接在同–个网络上。如果源主机和目标主机在同一个网络上，那么直接交付而无须通过路由器。如果源主机和目标主机不在同一个网络上，那么路由器按照转发表（路由表）指出的路由将数据报转发给下一个路由器，这称为间接交付。可见，在同-个网络中传递数据无须路由器的参与，而跨网络通信必须通过路由器进行转发。例如，路由器可以连接不同的LAN，连接不同的VLAN，连接不同的WAN，或者把LAN和WAN互联起来。路由器隔离了广播域。 从结构上看，路由器由路由选择和分组转发两部分构成，如图4.12所示。而从模型的角度看，路由器是网络层设备，它实现了网络模型的下三层，即物理层、数据链路层和网络层。 注意，如果一个存储转发设备实现了某个层次的功能，那么它就可以互联两个在该层次上使用不同协议的网段（网络)。如果网桥实现了物理层和数据链路层，那么网桥可以互联两个物理层和数据链路层不同的网段;但中继器实现了物理层后，却不能互联两个物理层不同的网段，这是因为中继器不是存储转发设备，它属于直通式设备。 路由选择部分也称控制部分，其核心构件是路由选择处理机。路由选择处理机的任务是根据所选定的路由选择协议构造出路由表，同时经常或定期地和其他相邻路由器交换路由信息而不断更新和维护路由表。分组转发部分由三部分组成:交换结构、一组输入端口和一组输出端口。输入端口在从物理层接收到的比特流中提取出链路层帧，进而从帧中提取出网络层数据报，输出端口则执行恰好相反的操作。交换结构是路由器的关键部件，它根据转发表对分组进行处理，将某个输入端口进入的分组从一个合适的输出端口转发出去。有三种常用的交换方法:通过存储器进行交换、通过总线进行交换和通过互联网络进行交换。交换结构本身就是一个网络。 路由器主要完成两个功能:一是分组转发，二是路由计算。前者处理通过路由器的数据流，关键操作是转发表查询、转发及相关的队列管理和任务调度等;后者通过和其他路由器进行基于路由协议的交互，完成路由表的计算。 路由器和网桥的重要区别是:网桥与高层协议无关，而路由器是面向协议的，它依据网络地址进行操作，并进行路径选择、分段、帧格式转换、对数据报的生存时间和流量进行控制等。现今的路由器一般都提供多种协议的支持，包括OSI、TCP/IP、IPX等。 4.8.2路由表和路由转发 路由表是根据路由选择算法得出的，主要用途是路由选择。从历年统考真题可以看出，标准的路由表有4个项目:目的网络P地址、子网掩码、下一跳I地址、接口。在如图4.13所示的网络拓扑中，R1的路由表见表4.4，该路由表包含到互联网的默认路由。转发表是从路由表得出的，其表项和路由表项有直接的对应关系。但转发表的格式和路由表的格式不同，其结构应使查找过程最优化（而路由表则需对网络拓扑变化的计算最优化)。转发表中含有一个分组将要发往的目的地址，以及分组的下一跳（即下一步接收者的目的地址，实际为MAC地址)。为了减少转发表的重复项目，可以使用一个默认路由代替所有具有相同“下一跳”的项目，并将默认路由设置得比其他项目的优先级低，如图4.14所示。路由表总是用软件来实现的;转发表可以用软件来实现，甚至也可以用特殊的硬件来实现。注意转发和路由选择的区别;“转发”是路由器根据转发表把收到的IP数据报从合适的端口转发出去，它仅涉及一个路由器。而“路由选择”则涉及很多路由器，路由表是许多路由器协同工作的结果。这些路由器按照复杂的路由算法，根据从各相邻路由器得到的关于网络拓扑的变化情况，动态地改变所选择的路由，并由此构造出整个路由表。 注意，在讨论路由选择的原理时，往往不去区分转发表和路由表的区别，但要注意路由表不等于转发表。分组的实际转发是靠直接查找转发表，而不是直接查找路由表。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:8","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"4.9本章小结及疑难点 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:4:9","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"第5章传输层 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:5:0","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"5.1传输层提供的服务 5.1.1传输层的功能 从通信和信息处理的角度看，传输层向它上面的应用层提供通信服务，它属于面向通信部分的最高层，同时也是用户功能中的最低层。 传输层位于网络层之上，它为运行在不同主机上的进程之间提供了逻辑通信，而网络层提供主机之间的逻辑通信。显然，即使网络层协议不可靠（网络层协议使分组丢失、混乱或重复),传输层同样能为应用程序提供可靠的服务。 从图5.1可以看出，网络的边缘部分的两台主机使用网络核心部分的功能进行端到端的通信时，只有主机的协议栈才有传输层和应用层，而路由器在转发分组时都只用到下三层的功能（即在通信子网中没有传输层，传输层只存在于通信子网以外的主机中)。 传输层的功能如下: 1)传输层提供应用进程之间的逻辑通信（即端到端的通信)。与网络层的区别是，网络层提 供的是主机之间的逻辑通信。 从网络层来说，通信的双方是两台主机，IP数据报的首部给出了这两台主机的P地址。但“两台主机之间的通信”实际上是两台主机中的应用进程之间的通信，应用进程之间的通信又称端到端的逻辑通信。这里“逻辑通信”的意思是:传输层之间的通信好像是沿水平方向传送数据，但事实上这两个传输层之间并没有一条水平方向的物理连接。 2〉复用和分用。复用是指发送方不同的应用进程都可使用同一个传输层协议传送数据;分 用是指接收方的传输层在剥去报文的首部后能够把这些数据正确交付到目的应用进程。 注意:传输层的复用分用功能与网络层的复用分用功能不同。网络层的复用是指发送方不同协议的数据都可以封装成P数据报发送出去,分用是指接收方的网络层在剥去首部后把数据交付给相应的协议。3）传输层还要对收到的报文进行差错检测（首部和数据部分)。而网络层只检查IP数据报 的首部，不检验数据部分是否出错。 4）提供两种不同的传输协议，即面向连接的TCP和无连接的UDP。而网络层无法同时实现 两种协议（即在网络层要么只提供面向连接的服务，如虚电路;要么只提供无连接服务，如数据报，而不可能在网络层同时存在这两种方式)。 传输层向高层用户屏蔽了低层网络核心的细节（如网络拓扑、路由协议等)，它使应用进程看见的是好像在两个传输层实体之间有一条端到端的逻辑通信信道，这条逻辑通信信道对上层的表现却因传输层协议不同而有很大的差别。当传输层采用面向连接的TCP时，尽管下面的网络是不可靠的，但这种逻辑通信信道就相当于一条全双工的可靠信道。但当传输层采用无连接的UDP时，这种逻辑通信信道仍然是一条不可靠信道。 5.1.2传输层的寻址与端口 端口的作用 端口能够让应用层的各种应用进程将其数据通过端口向下交付给传输层，以及让传输层知道应当将其报文段中的数据向上通过端口交付给应用层相应的进程。端口是传输层服务访问点(TSAP)，它在传输层的作用类似于P地址在网络层的作用或MAC地址在数据链路层的作用，只不过P地址和 MAC地址标识的是主机，而端口标识的是主机中的应用进程。 数据链路层的SAP是MAC地址，网络层的SAP是地址，传输层的SAP是端口。在协议栈层间的抽象的协议端口是软件端口，它与路由器或交换机上的硬件端口是完全不同的概念。硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层的各种协议进程与传输实体进行层间交互的一种地址。传输层使用的是软件端口。 端口号 应用进程通过端口号进行标识，端口号长度为16bit，能够表示65536(21)个不同的端口号。端口号只具有本地意义，即端口号只标识本计算机应用层中的各进程，在因特网中不同计算机的相同端口号是没有联系的。根据端口号范围可将端口分为两类: 1）服务端使用的端口号。这里又分为两类，最重要的一类是熟知端口号，数值为0~1023, IANA（互联网地址指派机构）把这些端口号指派给了TCP/IP最重要的一些应用程序，让所有的用户都知道。另一类称为登记端口号，数值为1024~49151。它是供没有熟知端口号的应用程序使用的，使用这类端口号必须在IANA登记，以防止重复。 一些常用的熟知端口号如下:2）客户端使用的端口号，数值为49152~65535。由于这类端口号仅在客户进程运行时才动 态地选择，因此又称短暂端口号（也称临时端口)。通信结束后，刚用过的客户端口号就不复存在，从而这个端口号就可供其他客户进程使用。 套接字 在网络中通过IP地址来标识和区别不同的主机，通过端口号来标识和区分一台主机中的不同应用进程。在网络中采用发送方和接收方的套接字（Socket）组合来识别端点。所谓套接字，实际上是一个通信端点，即 套接字=(主机IP地址，端口号） 它唯一地标识网络中的一台主机和其上的一个应用（进程)。 在网络通信中，主机A发给主机B的报文段包含目的端口号和源端口号，源端口号是“返回地址”的一部分，即当B需要发回一个报文段给A时,B到A的报文段中的目的端口号便是A到B的报文段中的源端口号(完全的返回地址是A的P地址和源端口号)。 5.1.3无连接服务与面向连接服务 面向连接服务就是在通信双方进行通信之前，必须先建立连接，在通信过程中，整个连接的情况一直被实时地监控和管理。通信结束后，应该释放这个连接。 无连接服务是指两个实体之间的通信不需要先建立好连接，需要通信时，直接将信息发送到“网络”中，让该信息的传递在网上尽力而为地往目的地传送。 TCP/IP协议族在层之上使用了两个传输协议:一个是面向连接的传输控制协议（TCP)，采用TCP时，传输层向上提供的是一条全双工的可靠逻辑信道;另一个是无连接的用户数据报协议（UDP)，采用UDP时，传输层向上提供的是一条不可靠的逻辑信道。 TCP提供面向连接的服务，在传送数据之前必须先建立连接，数据传送结束后要释放连接。TCP不提供广播或组播服务。由于TCP提供面向连接的可靠传输服务，因此不可避免地增加了许多开销，如确认、流量控制、计时器及连接管理等。这不仅使协议数据单元的头部增大很多，还要占用许多的处理机资源。因此TCP主要适用于可靠性更重要的场合，如文件传输协议(FTP)超文本传输协议（HTTP)、远程登录（TELNET）等。 UDP是一个无连接的非可靠传输层协议。它在P之上仅提供两个附加服务:多路复用和对数据的错误检查。IP知道怎样把分组投递给一台主机，但不知道怎样把它们投递给主机上的具体应用。UDP在传送数据之前不需要先建立连接，远程主机的传输层收到UDP报文后，不需要给出任何确认。由于UDP比较简单，因此执行速度比较快、实时性好。使用UDP的应用主要包括小文件传送协议(TFTP)、DNS、SNMP和实时传输协议（RTP)。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:5:1","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"5.2 UDP协议 5.2.1UDP数据报 UDP概述 RFC 768定义的UDP只是做了传输协议能够做的最少工作，它仅在P的数据报服务之上增加了两个最基本的服务:复用和分用以及差错检测。如果应用程序开发者选择UDP而非 TCP,那么应用程序几乎直接与P打交道。 为什么应用开发人员宁愿在UDP之上构建应用,也不选择TCP?既然TCP提供可靠的服务，而UDP不提供，那么TCP总是首选吗?答案是否定的，因为有很多应用更适合用UDP，主要是因为UDP具有如下优点: 1)UDP无须建立连接。因此UDP不会引入建立连接的时延。试想如果DNS运行在TCP而 非UDP 上，那么DNS的速度会慢很多。HTTP使用TCP而非 UDP，是因为对于基于文本数据的Web网页来说，可靠性是至关重要的。 2）无连接状态。TCP需要在端系统中维护连接状态。此连接状态包括接收和发送缓存、拥 塞控制参数和序号与确认号的参数。而UDP不维护连接状态，也不跟踪这些参数。因此,某些专用应用服务器使用UDP时，一般都能支持更多的活动客户机。 3）分组首部开销小。TCP有20B的首部开销，而UDP仅有8B的开销。4）应用层能更好地控制要发送的数据和发送时间。UDP没有拥塞控制，因此网络中的拥塞 不会影响主机的发送效率。某些实时应用要求以稳定的速度发送，能容忍一些数据的丢失，但不允许有较大的时延，而 UDP正好满足这些应用的需求。 UDP常用于一次性传输较少数据的网络应用、如 DNS、SNMP等，因为对于这些应用，若采用TCP，则将为连接创建、维护和拆除带来不小的开销。UDP也常用于多媒体应用（如IP电话、实时视频会议、流媒体等)，显然，可靠数据传输对这些应用来说并不是最重要的，但TCP的拥塞控制会导致数据出现较大的延迟，这是它们不可容忍的。 UDP提供尽最大努力的交付，即不保证可靠交付，但这并不意味着应用对数据的要求是不可靠的，因此所有维护传输可靠性的工作需要用户在应用层来完成。应用实体可以根据应用的需求来灵活设计自己的可靠性机制。 UDP是面向报文的。发送方UDP对应用层交下来的报文,在添加首部后就向下交付给P层，既不合并，也不拆分，而是保留这些报文的边界;接收方UDP对P层交上来UDP用户数据报,在去除首部后就原封不动地交付给上层应用进程，一次交付一个完整的报文。因此报文不可分割,是UDP数据报处理的最小单位。 UDP的首部格式 UDP数据报包含两部分:UDP首部和用户数据，整个UDP数据报作为IP数据报的数据部分封装在IP数据报中，如图5.2所示。UDP首部有8B，由4个字段组成，每个字段的长度都是2B，如图5.2所示。各字段意义如下: 1）源端口。源端口号。在需要对方回信时选用，不需要时可用全0。2）目的端口。目的端口号。这在终点交付报文时必须使用到。 3）长度。UDP数据报的长度（包括首部和数据)，其最小值是8（仅有首部)。 4）校验和。检测UDP数据报在传输中是否有错。有错就丢弃。该字段是可选的，当源主机 不想计算校验和时，则直接令该字段为全0。当传输层从IP层收到UDP数据报时,就根据首部中的目的端口,把 UDP数据报通过相应的端口上交给应用进程，如图5.3所示。 如果接收方UDP发现收到的报文中的目的端口号不正确（即不存在对应于端口号的应用进程)，那么就丢弃该报文，并由ICMP发送“端口不可达”差错报文给发送方。 5.2.2UDP校验 在计算校验和时,要在UDP数据报之前增加12B的伪首部,伪首部并不是UDP的真正首部。只是在计算校验和时，临时添加在UDP数据报的前面，得到一个临时的UDP数据报。校验和就是按照这个临时的UDP数据报计算的。伪首部既不向下传送也不向上递交，而仅为了计算校验和。这样的校验和，既检查了UDP数据报，又对P数据报的源IP地址和目的IP地址进行了检验。图5.4给出了UDP 数据报的伪首部各字段的内容。UDP校验和的计算方法和P数据报首部校验和的计算方法相似，都使用二进制反码运算求和再取反。但不同的是，P数据报的校验和只检验P数据报的首部，但UDP的校验和则检查首部和数据部分。 发送方首先把全零放入校验和字段并添加伪首部，然后把UDP数据报视为许多16位的字连接起来。若UDP数据报的数据部分不是偶数个字节，则要在数据部分末尾增加一个全零字节(但此字节不发送)。接下来按二进制反码计算出这些16位字的和，并将此和的二进制反码写入校验和字段。接收方把收到的UDP数据报加上伪首部（如果不为偶数个字节，那么还需要补上全零字节)后，按二进制反码计算出这些16位字的和。当无差错时其结果应全为1，否则表明有差错出现，接收方就应该丢弃这个UDP数据报。 图5.5给出了一个计算UDP校验和的例子。本例中，UDP数据报的长度是15B（不含伪首部)，因此需要添加一个全0字节。注意: 1)校验时，若UDP数据报部分的长度不是偶数个字节，则需填入一个全0字节，如图5.5 所示。但是此字节和伪首部一样，是不发送的。 2）如果UDP校验和校验出UDP数据报是错误的，那么可以丢弃，也可以交付给上层，但 是需要附上错误报告，即告诉上层这是错误的数据报。 3）通过伪首部，不仅可以检查源端口号、目的端口号和UDP用户数据报的数据部分，还可 以检查IP数据报的源IP地址和目的地址。 这种简单的差错检验方法的检错能力并不强，但它的好处是简单、处理速度快。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:5:2","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"5.3 TCP协议 5.3.1TCP协议特点 TCP是在不可靠的P层之上实现的可靠的数据传输协议，它主要解决传输的可靠、有序、无丢失和不重复问题。TCP是TCP/IP体系中非常复杂的一个协议，主要特点如下: 1)TCP是面向连接的传输层协议。 2)每条TCP连接只能有两个端点，每条TCP连接只能是点对点的（一对一)。3）TCP提供可靠的交付服务，保证传送的数据无差错、不丢失、不重复且有序。 4）TCP提供全双工通信，允许通信双方的应用进程在任何时候都能发送数据，为此TCP连 接的两端都设有发送缓存和接收缓存，用来临时存放双向通信的数据。 发送缓存用来暂时存放以下数据:①发送应用程序传送给发送方TCP准备发送的数据;②TCP已发送但尚未收到确认的数据。接收缓存用来暂时存放以下数据:①按序到达但尚未被接收应用程序读取的数据;②不按序到达的数据。 5)TCP是面向字节流的，虽然应用程序和TCP的交互是一次一个数据块（大小不等)，但 TCP把应用程序交下来的数据仅视为一连串的无结构的字节流。 5.3.2TCP报文段 TCP传送的数据单元称为报文段。一个TCP报文段分为TCP首部和TCP数据两部分，整个TCP报文段作为IP数据报的数据部分封装在P数据报中，如图5.6所示。其首部的前20B是固定的。TCP报文段的首部最短为20B，后面有4N字节是根据需要而增加的选项，通常长度为4B的整数倍。 TCP报文段既可以用来运载数据，又可以用来建立连接、释放连接和应答。 各字段意义如下: 1）源端口和目的端口字段。各占2B。端口是运输层与应用层的服务接口，运输层的复用和 分用功能都要通过端口实现。 2〉序号字段。占4B。TCP是面向字节流的（即TCP传送时是逐个字节传送的)，所以TCP 连接传送的数据流中的每个字节都编上一个序号。序号字段的值指的是本报文段所发送的数据的第一个字节的序号。 例如，一报文段的序号字段值是301，而携带的数据共有100B，表明本报文段的数据的最后一个字节的序号是400，因此下一个报文段的数据序号应从401开始。3）确认号字段。占4B，是期望收到对方的下一个报文段的数据的第一个字节的序号。若确认 号为N，则表明到序号N-1为止的所有数据都已正确收到。 例如,B正确收到了A发送过来的-个报文段，其序号字段是501，而数据长度是200B(序号501～700)，这表明B正确收到了A发送的到序号700为止的数据。因此B期望收到A的下一个数据序号是701，于是B在发送给A的确认报文段中把确认号置为701。 4）数据偏移（即首部长度)。占4位，这里不是P数据报分片的那个数据偏移，而是表示 首部长度，它指出TCP报文段的数据起始处距离TCP报文段的起始处有多远。 “数据偏移”的单位是32位(以4B为计算单位)。因此当此字段的值为15时，达到TCP首部的最大长度60B。 5）保留字段。占6位，保留为今后使用，但目前应置为0，该字段可以忽略不计。 6）紧急位URG。URG= 1时，表明紧急指针字段有效。它告诉系统报文段中有紧急数据， 应尽快传送（相当于高优先级的数据)。但URG需要和紧急指针配套使用，即数据从第一个字节到紧急指针所指字节就是紧急数据。 7）确认位ACK。只有当ACK=1时确认号字段才有效。当ACK=0时，确认号无效。 TCP规定，在连接建立后所有传送的报文段都必须把ACK置1。 8）推送位PSH (Push)。接收TCP收到PSH=1的报文段，就尽快地交付给接收应用进程, 而不再等到整个缓存都填满后再向上交付。 9)复位位RST (Reset)。RST=1时，表明TCP连接中出现严重差错（如主机崩溃或其他原 因)，必须释放连接，然后再重新建立运输连接。 10）同步位SYN。同步SYN=1表示这是一个连接请求或连接接收报文。 当SYN= 1，ACK=0时，表明这是一个连接请求报文，对方若同意建立连接，则在响应报文中使用SYN=1，ACK=1。即SYN=1表示这是一个连接请求或连接接收报文。11)终止位FIN (Finish)。用来释放一个连接。FIN=1表明此报文段的发送方的数据已发送 完毕，并要求释放传输连接。 12）窗口字段。占2B。它指出现在允许对方发送的数据量，接收方的数据缓存空间是有限 的，因此用窗口值作为接收方让发送方设置其发送窗口的依据，单位为字节。 例如，假设确认号是701，窗口字段是1000。这表明，从701号算起，发送此报文段的一方还有接收1000B数据（字节序号为701～1700）的接收缓存空间。 13）校验和。占2B。校验和字段检验的范围包括首部和数据两部分。在计算校验和时，和 UDP一样，要在TCP报文段的前面加上12B的伪首部(只需将UDP伪首部的第4个字段，即协议字段的17改成6，其他的和UDP一样)。 14）紧急指针字段。占16位，指出在本报文段中紧急数据共有多少字节（紧急数据放在本 报文段数据的最前面)。 15)选项字段。长度可变。TCP最初只规定了一种选项，即最大报文段长度(Maximum Segment Size，MSS)。MSS是TCP报文段中的数据字段的最大长度。 16）填充字段。这是为了使整个首部长度是4B的整数倍。 5.3.3TCP连接管理 TCP是面向连接的协议，因此每个TCP连接都有三个阶段:连接建立、数据传送和连接释放。TCP连接的管理就是使运输连接的建立和释放都能正常进行。 在TCP连接建立的过程中，要解决以下三个问题: 1)要使每一方都能够确知对方的存在。 2）要允许双方协商一些参数（如最大窗口值、是否使用窗口扩大选项、时间戳选项及服务 质量等)。 3）能够对运输实体资源（如缓存大小、连接表中的项目等）进行分配。 TCP把连接作为最基本的抽象，每条TCP连接有两个端点，TCP连接的端点不是主机，不是主机的P地址,不是应用进程,也不是传输层的协议端口。TCP连接的端口称为套接字( socket)或插口。端口拼接到IP地址即构成套接字。 每条TCP连接唯一地被通信两端的两个端点(即两个套接字）确定。 TCP连接的建立采用客户机/服务器方式。主动发起连接建立的应用进程称为客户机(Client),而被动等待连接建立的应用进程称为服务器(Server)。 TCP连接的建立 连接的建立经历以下3个步骤，通常称为三次握手，如图5.7所示。第一步:客户机的TCP首先向服务器的TCP发送一个连接请求报文段。这个特殊的报文段中不含应用层数据，其首部中的SYN标志位被置为1。另外，客户机会随机选择一个起始序号seq=x(连接请求报文不携带数据，但要消耗-一个序号)。 第二步:服务器的TCP收到连接请求报文段后，如同意建立连接，就向客户机发回确认，并为该TCP连接分配TCP缓存和变量。在确认报文段中，SYN和ACK位都被置为1，确认号字段的值为x+1,并且服务器随机产生起始序号 seq=y(确认报文不携带数据,但也要消耗一个序号)。确认报文段同样不包含应用层数据。 第三步:当客户机收到确认报文段后，还要向服务器给出确认，并且也要给该连接分配缓存和变量。这个报文段的ACK标志位被置1，序号字段为x+1，确认号字段ack=y+1。该报文段可以携带数据，若不携带数据则不消耗序号。 成功进行以上三步后，就建立了TCP连接，接下来就可以传送应用层数据。TCP提供的是全双工通信，因此通信双方的应用进程在任何时候都能发送数据。 另外，值得注意的是，服务器端的资源是在完成第二次握手时分配的，而客户端的资源是在完成第三次握手时分配的，这就使得服务器易于受到SYN洪泛攻击。 TCP连接的释放 天下没有不散的筵席，TCP同样如此。参与TCP连接的两个进程中的任何一个都能终止该连接。TCP连接释放的过程通常称为四次握手，如图5.8所示。第一步:客户机打算关闭连接时，向其TCP发送一个连接释放报文段，并停止发送数据，主动关闭TCP连接，该报文段的FIN标志位被置1，seq= u，它等于前面已传送过的数据的最后一个字节的序号加1(FIN报文段即使不携带数据，也要消耗一个序号)。TCP是全双工的，即可以想象为一条TCP 连接上有两条数据通路。发送FIN报文时，发送FIN 的一端不能再发送数据，即关闭了其中一条数据通路，但对方还可以发送数据。 第二步:服务器收到连接释放报文段后即发出确认，确认号是ack = u+l，而这个报文段自己的序号是v，等于它前面已传送过的数据的最后一个字节的序号加1。此时，从客户机到服务器这个方向的连接就释放了，TCP连接处于半关闭状态。但服务器若发送数据，客户机仍要接收，即从服务器到客户机这个方向的连接并未关闭。 第三步:若服务器已经没有要向客户机发送的数据，就通知TCP释放连接，此时其发出FIN=1的连接释放报文段。 第四步:客户机收到连接释放报文段后，必须发出确认。在确认报文段中，ACK字段被置为1，确认号ack = w +1，序号 seq=u+1。此时TCP连接还未释放，必须经过时间等待计时器设置的时间2MSL后，A才进入连接关闭状态。 对上述TCP连接建立和释放的总结如下:1）连接建立。分为3步: SYN= 1，seq=X。 SYN= 1，ACK=1，seq= y， ack =x +1。③ACK=1，seq=x+1，ack =y+1。 2）释放连接。分为4步: ①FIN= 1，seq= u。 ②ACK=1，seq= v, ack = u+1。 FIN=1，ACK=1，seq = w,ack = u+1。ACK= 1，seq= u+1，ack = w+1。 选择题喜欢考查（关于连接和释放的题目，ACK、SYN、FI","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:5:3","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"5.4本章小结及疑难点 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:5:4","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"第6章应用层 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:6:0","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"6.1网络应用模型 6.1.1CS模型 在客户/服务器(Client/Server，C/S）模型中，有一个总是打开的主机称为服务器，它服务于许多来自其他称为客户机的主机请求。其工作流程如下: 1)）服务器处于接收请求的状态。 2）客户机发出服务请求，并等待接收结果。 3）服务器收到请求后，分析请求，进行必要的处理，得到结果并发送给客户机。 客户程序必须知道服务器程序的地址，客户机上一般不需要特殊的硬件和复杂的操作系统。而服务器上运行的软件则是专门用来提供某种服务的程序，可同时处理多个远程或本地客户的要求。系统启动后即自动调用并一直不断地运行着，被动地等待并接收来自各地客户的请求。因此,服务器程序不需要知道客户程序的地址。 客户/服务器模型最主要的特征是:客户是服务请求方，服务器是服务提供方。如Web应用程序，其中总是打开的Web服务器服务于运行在客户机上的浏览器的请求。当Web服务器接收到来自客户机对某对象的请求时，它向该客户机发送所请求的对象以做出响应。常见的使用客户/服务器模型的应用包括 Web、文件传输协议(FTP)、远程登录和电子邮件等。 客户/服务器模型的主要特点还有: 1）网络中各计算机的地位不平等，服务器可以通过对用户权限的限制来达到管理客户机的 目的，使它们不能随意存储/删除数据，或进行其他受限的网络活动。整个网络的管理工作由少数服务器担当，因此网络的管理非常集中和方便。 2）客户机相互之间不直接通信。例如，在 Web应用中两个浏览器并不直接通信。3）可扩展性不佳。受服务器硬件和网络带宽的限制，服务器支持的客户机数有限。 6.1.2P2P模型 不难看出，在CIS模型中（见图6.1)，服务器性能的好坏决定了整个系统的性能，当大量用户请求服务时，服务器就必然成为系统的瓶颈。P2P模型（见图6.2）的思想是整个网络中的传输内容不再被保存在中心服务器上，每个结点都同时具有下载、上传的功能，其权利和义务都是大体对等的。在P2P模型中，各计算机没有固定的客户和服务器划分。相反，任意一对计算机——称为对等方(Peer)，直接相互通信。实际上，P2P模型从本质上来看仍然使用客户/服务器方式，每个结点既作为客户访问其他结点的资源,也作为服务器提供资源给其他结点访问。当前比较流行的P2P应用有PPlive、Bittorrent和电驴等。 与C/S模型相比，P2P模型的优点主要体现如下: 1）减轻了服务器的计算压力，消除了对某个服务器的完全依赖，可以将任务分配到各个结 点上，因此大大提高了系统效率和资源利用率（例如，播放流媒体时对服务器的压力过大，而通过P2P模型，可以利用大量的客户机来提供服务)。 2）多个客户机之间可以直接共享文档。 3）可扩展性好，传统服务器有响应和带宽的限制，因此只能接受一定数量的请求。4）网络健壮性强，单个结点的失效不会影响其他部分的结点。 P2P模型也有缺点。在获取服务的同时，还要给其他结点提供服务，因此会占用较多的内存，影响整机速度。例如，经常进行P2P下载还会对硬盘造成较大的损伤。据某互联网调研机构统计，当前P2Р程序已占互联网50%～90%的流量，使网络变得非常拥塞，因此各大ISP(互联网服务提供商，如电信、网通等）通常都对P2P应用持反对态度。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:6:1","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"6.2域名系统(DNS) 6.2.1层次域名空间 因特网采用层次树状结构的命名方法。采用这种命名方法，任何一个连接到因特网的主机或路由器，都有一个唯一的层次结构名称，即域名（Domain Name)。域(Domain〉是名字空间中一个可被管理的划分。域还可以划分为子域，而子域还可以继续划分为子域的子域，这样就形成了顶级域、二级域、三级域等。每个域名都由标号序列组成，而各标号之间用点(“.”)隔开。一个典型的例子如图6.3所示，它是王道论坛用于提供wwW服务的计算机(Web服务器）的域名，它由三个标号组成，其中标号com是顶级域名,标号cskaoyan是二级域名,标号www是三级域名。关于域名中的标号有以下几点需要注意:1）标号中的英文不区分大小写。 2）标号中除连字符(-)外不能使用其他的标点符号。 3）每个标号不超过63个字符，多标号组成的完整域名最长不超过255个字符。4）级别最低的域名写在最左边，级别最高的顶级域名写在最右边。 顶级域名（Top Level Domain，TLD)分为如下三大类: 1）国家顶级域名(nTLD)。国家和某些地区的域名，如“.cn”表示中国，“.us”表示美国， “.uk”表示英国。 2）通用顶级域名(gTLD)。常见的有“.com”(公司)、“.net”(网络服务机构)、“.org”(非 营利性组织）和“.gov”(国家或政府部门）等。 3）基础结构域名。这种顶级域名只有一个，即 arpa，用于反向域名解析，因此又称反向 域名。 国家顶级域名下注册的二级域名均由该国家自行确定。图6.4展示了域名空间的树状结构。在域名系统中，每个域分别由不同的组织进行管理。每个组织都可以将它的域再分成一定数目的子域，并将这些子域委托给其他组织去管理。例如，管理CN域的中国将EDU.CN子域授权给中国教育和科研计算机网(CERNET)来管理。 6.2.2域名服务器 因特网的域名系统被设计成一个联机分布式的数据库系统，并采用客户/服务器模型。域名到IP地址的解析是由运行在域名服务器上的程序完成的，一个服务器所负责管辖的(或有权限的)范围称为区（不以“域”为单位)，各单位根据具体情况来划分自己管辖范围的区，但在一个区中的所有结点必须是能够连通的，每个区设置相应的权限域名服务器，用来保存该区中的所有主机的域名到IP地址的映射。每个域名服务器不但能够进行一些域名到P地址的解析，而且还必须具有连向其他域名服务器的信息。当自己不能进行域名到P地址的转换时，能够知道到什么地方去找其他域名服务器。 DNS使用了大量的域名服务器，它们以层次方式组织。没有一台域名服务器具有因特网上所有主机的映射，相反，该映射分布在所有的 DNS 上。采用分布式设计的 DNS，是一个在因特网上实现分布式数据库的精彩范例。主要有4种类型的域名服务器。 根域名服务器 根域名服务器是最高层次的域名服务器，所有的根域名服务器都知道所有的顶级域名服务器的P地址。根域名服务器也是最重要的域名服务器，不管是哪个本地域名服务器，若要对因特网上任何一个域名进行解析，只要自己无法解析，就首先要求助于根域名服务器。因特网上有13个根域名服务器，尽管我们将这13个根域名服务器中的每个都视为单个服务器，但每个“服务器”实际上是冗余服务器的集群，以提供安全性和可靠性。需要注意的是，根域名服务器用来管辖顶级域（如.com)，通常它并不直接把待查询的域名直接转换成P地址，而是告诉本地域名服务器下一步应当找哪个顶级域名服务器进行查询。 顶级域名服务器 这些域名服务器负责管理在该顶级域名服务器注册的所有二级域名。收到DNS查询请求时，就给出相应的回答（可能是最后的结果，也可能是下一步应当查找的域名服务器的P地址)。 授权域名服务器（权限域名服务器） 每台主机都必须在授权域名服务器处登记。为了更加可靠地工作，一台主机最好至少有两个授权域名服务器。实际上，许多域名服务器都同时充当本地域名服务器和授权域名服务器。授权域名服务器总能将其管辖的主机名转换为该主机的IP地址。 本地域名服务器 本地域名服务器对域名系统非常重要。每个因特网服务提供者（ISP)，或一所大学，甚至一所大学中的各个系，都可以拥有一个本地域名服务器。当一台主机发出 DNS查询请求时，这个查询请求报文就发送给该主机的本地域名服务器。事实上，我们在Windows系统中配置“本地连接”时，就需要填写DNS地址，这个地址就是本地DNS(域名服务器）的地址。 DNS的层次结构如图6.5所示。 6.2.3域名解析过程 域名解析是指把域名映射成为ⅣP地址或把Ⅳ地址映射成域名的过程。前者称为正向解析，后者称为反向解析。当客户端需要域名解析时,通过本机的DNS客户端构造一个 DNS请求报文，以UDP 数据报方式发往本地域名服务器。域名解析有两种方式:递归查询和递归与迭代相结合的查询。递归查询的过程如图6.6(a)所示，由于该方法给根域名服务造成的负载过大，所以在实际中几乎不使用。 常用递归与迭代相结合的查询方式如图6.6(b)所示，该方式分为两个部分。 (1）主机向本地域名服务器的查询采用的是递归查询 也就是说，如果本地主机所询问的本地域名服务器不知道被查询域名的IP地址，那么本地域名服务器就以DNS客户的身份，向根域名服务器继续发出查询请求报文（即替该主机继续查询)，而不是让该主机自己进行下一步的查询。在这种情况下，本地域名服务器只需向根域名服务器查询一次，后面的几次查询都是递归地在其他几个域名服务器之间进行的〔见图6.6(a)中的步骤③~⑥]。在步骤⑦中，本地域名服务器从根域名服务器得到了所需的IP地址，最后在步骤⑧中，本地域名服务器把查询结果告诉主机 m.xyz.com。(2）本地域名服务器向根域名服务器的查询采用迭代查询 当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的IP地址，要么告诉本地域名服务器:“你下一步应当向哪个顶级域名服务器进行查询”。然后让本地域名服务器向这个顶级域名服务器进行后续的查询，如图6.6(b)所示。同样，顶级域名服务器收到查询报文后，要么给出所要查询的IP地址，要么告诉本地域名服务器下一步应向哪个权限域名服务器查询。最后，知道所要解析的域名的P地址后，把这个结果返回给发起查询的主机。 下面举例说明域名解析的过程。假定某客户机想获知域名为y.abc.com主机的IP地址，域名解析的过程（共使用8个UDP报文)如下: 客户机向其本地域名服务器发出DNS请求报文。 本地域名服务器收到请求后，查询本地缓存，若没有该记录，则以DNS客户的身份向根 域名服务器发出解析请求。 ③根域名服务器收到请求后，判断该域名属于.com域，将对应的顶级域名服务器dns.com 的P地址返回给本地域名服务器。 本地域名服务器向顶级域名服务器dns.com发出解析请求报文。 顶级域名服务器dns.com收到请求后，判断该域名属于abc.com域，因此将对应的授权域 名服务器dns.abc.com的IP地址返回给本地域名服务器。 ⑥本地域名服务器向授权域名服务器dns.abc.com发起解析请求报文。 ⑦授权域名服务器dns.abc.com收到请求后，将查询结果返回给本地域名服务器。本地域名服务器将查询结果保存到本地缓存，同时返回给客户机。 为了提高DNS的查询效率，并减少因特网上的DNS查询报文数量，在域名服务器中广泛地使用了高速缓存。当一个 DNS 服务器接收到DNS查询结果时，它能将该DNS信息缓存在高速缓存中。这样，当另一个相同的域名查询到达该DNS服务器时，该服务器就能够直接提供所要求的IP地址，而不需要再去向其他DNS服务器询问。因为主机名和P地址之间的映射不是永久的，所以DNS服务器将在一段时间后丢弃高速缓存中的信息。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:6:2","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"6.3文件传输协议(FTP) 6.3.1FTP工作原理 文件传输协议（File Transfer Protocol，FTP)是因特网上使用得最广泛的文件传输协议。FTP提供交互式的访问，允许客户指明文件的类型与格式，并允许文件具有存取权限。它屏蔽了各计算机系统的细节，因而适合于在异构网络中的任意计算机之间传送文件。 FTP提供以下功能: ①提供不同种类主机系统（硬、软件体系等都可以不同)之间的文件传输能力。以用户权限管理的方式提供用户对远程FTP服务器上的文件管理能力。 以匿名FTP的方式提供公用文件共享的能力。 FTP采用客户/服务器的工作方式，它使用TCP可靠的传输服务。一个FTP服务器进程可同时为多个客户进程提供服务。FTP的服务器进程由两大部分组成:一个主进程，负责接收新的请求;另外有若干从属进程，负责处理单个请求。其工作步骤如下: ①打开熟知端口21(控制端口)，使客户进程能够连接上。等待客户进程发连接请求。 启动从属进程来处理客户进程发来的请求。主进程与从属进程并发执行，从属进程对客 户进程的请求处理完毕后即终止。 ④回到等待状态，继续接收其他客户进程的请求。 FTP服务器必须在整个会话期间保留用户的状态信息。特别是服务器必须把指定的用户账户与控制连接联系起来，服务器必须追踪用户在远程目录树上的当前位置。 6.3.2控制连接和数据连接 FTP在工作时使用两个并行的TCP连接（见图6.7):一个是控制连接（端口号21)，一个是数据连接（端口号20)。使用两个不同的端口号可使协议更加简单和更容易实现。 控制连接 服务器监听21号端口，等待客户连接，建立在这个端口上的连接称为控制连接，控制连接用来传输控制信息（如连接请求、传送请求等)，并且控制信息都以┐位ASCII格式传送。FTP客户发出的传送请求，通过控制连接发送给服务器端的控制进程，但控制连接并不用来传送文件。在传输文件时还可以使用控制连接（如客户在传输中途发一个中止传输的命令)，因此控制连接在整个会话期间一直保持打开状态。 数据连接 服务器端的控制进程在接收到FTP客户发来的文件传输请求后，就创建“数据传送进程”和“数据连接”。数据连接用来连接客户端和服务器端的数据传送进程，数据传送进程实际完成文件的传送，在传送完毕后关闭“数据传送连接”并结束运行。 因为FTP使用了一个分离的控制连接，所以也称FTP的控制信息是带外（Out-of-band）传送的。使用FTP时，若要修改服务器上的文件，则需要先将此文件传送到本地主机，然后再将修改后的文件副本传送到原服务器。网络文件系统(NFS）允许进程打开一个远程文件，并在该文件的某个特定位置开始读写数据。这样，NFS可使用户复制一个大文件中的一个很小的片段，而不需要复制整个大文件。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:6:3","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"6.4电子邮件 6.4.1电子邮件系统的组成结构 自从有了因特网，电子邮件就在因特网上流行起来。电子邮件是一种异步通信方式，通信时不需要双方同时在场。电子邮件把邮件发送到收件人使用的邮件服务器，并放在其中的收件人邮箱中，收件人可以随时上网到自己使用的邮件服务器进行读取。 一个电子邮件系统应具有图6.8所示的三个最主要的组成构件，即用户代理(User Agent)、邮件服务器和电子邮件使用的协议，如SMTP、POP3（或IMAP)等。用户代理(UA):用户与电子邮件系统的接口。用户代理使用户能够通过一个很友好的接口发送和接收邮件，用户代理至少应当具有撰写、显示和邮件处理的功能。通常情况下，用户代理就是一个运行在PC上的程序，常见的有Outlook、Foxmail和Thunderbird等。 邮件服务器:组成电子邮件系统的核心。邮件服务器的功能是发送和接收邮件，同时还要向发信人报告邮件传送的情况（已交付、被拒绝、丢失等)。邮件服务器采用客户/服务器方式工作，但它能够同时充当客户和服务器。例如，当邮件服务器A向邮件服务器B发送邮件时，A就作为SMTP客户，而B是SMTP服务器;反之，当B向A发送邮件时，B就是SMTP客户，而A就是SMTP服务器。 邮件发送协议和读取协议:邮件发送协议用于用户代理向邮件服务器发送邮件或在邮件服务器之间发送邮件，通常使用的是SMTP;邮件读取协议用于用户代理从邮件服务器读取邮件，如POP3。注意，SMTP采用的是“推”(Push)的通信方式，即在用户代理向邮件服务器发送邮件及在邮件服务器之间发送邮件时，SMTP客户端主动将邮件“推”送到SMTP服务器端。而POP3采用的是“拉”(Pull)的通信方式，即用户读取邮件时，用户代理向邮件服务器发出请求，“拉”取用户邮箱中的邮件。 电子邮件的发送、接收过程可简化为如图6.9所示。下面简单介绍电子邮件的收发过程。 发信人调用用户代理来撰写和编辑要发送的邮件。用户代理用SMTP把邮件传送给发送 方邮件服务器。 发送方邮件服务器将邮件放入邮件缓存队列中，等待发送。 运行在发送方邮件服务器的SMTP客户进程，发现邮件缓存中有待发送的邮件，就向运 行在接收方邮件服务器的SMTP服务器进程发起建立TCP连接。 TCP连接建立后，SMTP客户进程开始向远程SMTP服务器进程发送邮件。当所有待发送 邮件发完后，SMTP就关闭所建立的TCP连接。 运行在接收方邮件服务器中的SMTP服务器进程收到邮件后，将邮件放入收信人的用户 邮箱，等待收信人在方便时进行读取。 收信人打算收信时，调用用户代理，使用POP3(或IMAP）协议将自己的邮件从接收方 邮件服务器的用户邮箱中取回(如果邮箱中有来信的话)。 6.4.2电子邮件格式与MIME 电子邮件格式 一个电子邮件分为信封和内容两大部分，邮件内容又分为首部和主体两部分。RFC 822规定了邮件的首部格式，而邮件的主体部分则让用户自由撰写。用户写好首部后，邮件系统自动地将信封所需的信息提取出来并写在信封上，用户不需要亲自填写信封上的信息。 邮件内容的首部包含一些首部行，每个首部行由一个关键字后跟冒号再后跟值组成。有些关键字是必需的，有些则是可选的。最重要的关键字是To:和 Subject:。 To是必需的关键字，后面填入一个或多个收件人的电子邮件地址。电子邮件地址的规定格式为:收件人邮箱名@邮箱所在主机的域名，如 abc@cskaoyan.com，其中收信人邮箱名即用户名，abc在cskaoyan.com这个邮件服务器上必须是唯一的。这也就保证了abc@cskaoyan.com这个邮件地址在整个因特网上是唯一的。 Subject是可选关键字，是邮件的主题，反映了邮件的主要内容。 当然，还有一个必填的关键字是 From，但它通常由邮件系统自动填入。首部与主体之间用一个空行进行分割。典型的邮件内容如下: 多用途网际邮件扩充(MIME) 由于SMTP只能传送一定长度的ASCII码，许多其他非英语国家的文字(如中文、俄文，甚至带重音符号的法文或德文）就无法传送，且无法传送可执行文件及其他二进制对象，因此提出了多用途网络邮件扩充(Multipurpose Internet MailExtensions，MIME)。 MIME并未改动SMTP或取代它。MIME的意图是继续使用目前的格式，但增加了邮件主体的结构，并定义了传送非ASCII码的编码规则。也就是说，MIME邮件可在现有的电子邮件程序和协议下传送。MIME与 SMTP的关系如图6.10所示。MIME主要包括以下三部分内容: 5个新的邮件首部字段，包括MIME版本、内容描述、内容标识、内容传送编码和内容 类型。 定义了许多邮件内容的格式，对多媒体电子邮件的表示方法进行了标准化。 定义了传送编码，可对任何内容格式进行转换，而不会被邮件系统改变。 6.4.3SMTP和POP3 SMTP 简单邮件传输协议(Simple Mail Transfer Protocol，SMTP）是一种提供可靠且有效的电子邮件传输的协议，它控制两个相互通信的SMTP进程交换信息。由于SMTP使用客户/服务器方式,因此负责发送邮件的SMTP进程就是SMTP客户,而负责接收邮件的SMTP进程就是SMTP服务器。SMTP用的是TCP连接，端口号为25。SMTP通信有以下三个阶段。 (1）连接建立 发件人的邮件发送到发送方邮件服务器的邮件缓存中后，SMTP客户就每隔一定时间对邮件缓存扫描一次。如发现有邮件，就使用SMTP的熟知端口号(25）与接收方邮件服务器的SMTP服务器建立TCP连接。连接建立后，接收方SMTP服务器发出220 Service ready(服务就绪)。然后SMTP客户向SMTP服务器发送HELO命令，附上发送方的主机名。 SMTP不使用中间邮件服务器。TCP连接总是在发送方和接收方这两个邮件服务器之间直接建立，而不管它们相隔多远。接收方的邮件服务器因故障暂时不能建立连接时，发送方的邮件服务器只能等待一段时间后再次尝试连接。 (2）邮件传送 连接建立后，就可开始传送邮件。邮件的传送从MAIL命令开始，MAIL命令后面有发件人的地址。如 MAIL FROM:hoopdog@hust.edu.cn。若SMTP服务器已准备好接收邮件，则回答250 OK。接着SMTP客户端发送一个或多个RCPT(收件人recipient 的缩写)命令，格式为RCPTTO:\u003c收件人地址\u003e。每发送一个RCPT命令，都应有相应的信息从SMTP服务器返回，如250 OK.或550 No such user here（无此用户)。 RCPT命令的作用是，先弄清接收方系统是否已做好接收邮件的准备，然后才发送邮件，以便不至于发送了很长的邮件后才知道地址错误，进而避免浪费通信资源。 获得OK的回答后，客户端就使用DATA命令，表示要开始传输邮件的内容。正常情况下，SMTP服务器回复信息是354 Start mail input; end with ,。表示回车换行。此时SMTP客户端就可开始传送邮件内容，并用.(两个回车，中间一个点）表示邮件内容的结束。 (3）连接释放 邮件发送完毕后，SMTP客户应发送QUIT命令。SMTP服务器返回的信息是221(服务关闭)，表示SMTP同意释放TCP连接。邮件传送的全部过程就此结束。 POP3 邮局协议（Post Office Protocol，POP）是一个非常简单但功能有限的邮件读取协议，现在使用的是它的第3个版本POP3。POP3采用的是“拉”(Pull)的通信方式，当用户读取邮件时，用户代理向邮件服务器发出请求，“拉”取用户邮箱中的邮件。 POP也使用客户/服务器的工作方式，在传输层使用TCP，端口号为110。接收方的用户代理上必须运行POP客户程序，而接收方的邮件服务器上则运行POP服务器程序。POP有两种工作方式:“下载并保留”和“下载并删除”。在“下载并保留”方式下，用户从邮件服务器上读取邮件后，邮件依然会保存在邮件服务器上，用户可再次从服务器上读取该邮件;而使用“下载并删除”方式时，邮件一旦被读取，就被从邮件服务器上删除，用户不能再次从服务器上读取。 另一个邮件接收协议是因特网报文存取协议（IMAP)，它比POP复杂得多，IMAP为用户提供了创建文件夹、在不同文件夹之间移动邮件及在远程文件夹中查询邮件的命令，为此IMAP服务器维护了会话用户的状态信息。IMAP 的另一特性是允许用户代理只获取报文的某些部分，例如可以只读取一个报文的首部,或一个多部分MIME报文的一部分。这非常适用于低带宽的情况，用户可能并不想取回邮箱中的所有邮件，尤其是包含很多音频或视频的大邮件。此外，随着万维网的流行，目前出现了很多基于万维网的电子邮件，如Hotmail、Gmail等。这种电子邮件的特点是，用户浏览器与Hotmail或Gmail 的邮件服务器之间的邮件发送或接收使用的是HTTP，而仅在不同邮件服务器之间传送邮件时才使用SMTP。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:6:4","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"6.5万维网(WWW) 6.5.1WWW的概念与组成结构 万维网(World Wide Web，wWW）是一个资料空间，在这个空间中:一样有用的事物称为一样“资源”,并由一个全域“统一资源定位符”(URL)标识。这些资源通过超文本传输协议(HTTP)传送给使用者，而后者通过单击链接来获取资源。 万维网使用链接的方法能让用户非常方便地从因特网上的一个站点访问另一个站点，从而主动地按需获取丰富的信息。超文本标记语言（HyperText Markup Language，HTML)使得万维网页面的设计者可以很方便地用一个超链接从本页面的某处链接到因特网上的任何一个万维网页面，并能够在自己的计算机屏幕上显示这些页面。 万维网的内核部分是由三个标准构成的: 1)统一资源定位符（URL)。负责标识万维网上的各种文档，并使每个文档在整个万维网的 范围内具有唯一的标识符URL。 2)超文本传输协议（HTTP)。一个应用层协议，它使用TCP连接进行可靠的传输，HTTP 是万维网客户程序和服务器程序之间交互所必须严格遵守的协议。 3）超文本标记语言(HTML)。一种文档结构的标记语言，它使用一些约定的标记对页面上 的各种信息（包括文字、声音、图像、视频等)、格式进行描述。 URL是对可以从因特网上得到的资源的位置和访问方法的一种简洁表示。URL相当于一个文件名在网络范围的扩展。 URL的一般形式是:\u003c协议\u003e:!//\u003c主机\u003e:\u003c端口\u003e/\u003c路径\u003e。 常见的\u003c协议\u003e有 http、ftp等;\u003c主机\u003e是存放资源的主机在因特网中的域名，也可以是P地址;\u003c端口\u003e和\u003c路径\u003e有时可以省略。在URL中不区分大小写。 万维网以客户/服务器方式工作。浏览器是在用户计算机上的万维网客户程序，而万维网文档所驻留的计算机则运行服务器程序，这台计算机称万维网服务器。客户程序向服务器程序发出请求，服务器程序向客户程序送回客户所要的文档。工作流程如下: Web 用户使用浏览器（指定URL)与Web服务器建立连接，并发送浏览请求。 Web 服务器把URL转换为文件路径，并返回信息给Web浏览器。 通信完成，关闭连接。 万维网是无数个网络站点和网页的集合，它们在一起构成了因特网最主要的部分(因特网也包括电子邮件、Usenet 和新闻组)。 6.5.2HTTP HTTP定义了浏览器（万维网客户进程）怎样向万维网服务器请求万维网文档，以及服务器怎样把文档传送给浏览器。从层次的角度看，HTTP是面向事务的(Transaction-oriented）应用层协议，它规定了在浏览器和服务器之间的请求和响应的格式与规则，是万维网上能够可靠地交换文件（包括文本、声音、图像等各种多媒体文件）的重要基础。 HTTP的操作过程 从协议执行过程来说，浏览器要访问wwW服务器时，首先要完成对wwW服务器的域名解析。一旦获得了服务器的地址，浏览器就通过TCP向服务器发送连接建立请求。万维网的大致工作过程如图6.11所示。每个万维网站点都有一个服务器进程，它不断地监听TCP的端口80（默认)，当监听到连接请求后便与浏览器建立连接。TCP连接建立后，浏览器就向服务器发送请求获取某个Web页面的HTTP请求。服务器收到HTTP请求后，将构建所请求 Web 页的必需信息，并通过HTTP响应返回给浏览器。浏览器再将信息进行解释，然后将Web页显示给用户。最后，TCP连接释放。在浏览器和服务器之间的请求与响应的交互，必须遵循规定的格式和规则，这些格式和规则就是HTTP。因此HTTP有两类报文: 请求报文(从 Web客户端向Web服务器发送服务请求〉和响应报文（从Web服务器对Web客户端请求的回答)。 用户单击鼠标后所发生的事件按顺序如下〔以访问清华大学的网站为例): 1）浏览器分析链接指向页面的URL (http://www.tsinghua.edu.cn /chn/index.htm)。 2）浏览器向DNS请求解析www.tsinghua.edu.cn的IP地址。 3）域名系统DNS解析出清华大学服务器的P地址。 4）浏览器与该服务器建立TCP连接（默认端口号为80)。5）浏览器发出HTTP请求:GET /chn/index.htm。 6）服务器通过HTTP响应把文件 index.htm发送给浏览器。7)TCP连接释放。 8）浏览器解释文件index.htm，并将Web页显示给用户。 HTTP的特点 HTTP是无状态的。也就是说，同一个客户第二次访问同一个服务器上的页面时，服务器的响应与第一次被访问时的相同。因为服务器并不记得曾经访问过的这个客户，也不记得为该客户曾经服务过多少次。 HTTP的无状态特性简化了服务器的设计，使服务器更容易支持大量并发的HTTP请求。在实际应用中，通常使用Cookie 加数据库的方式来跟踪用户的活动（如记录用户最近浏览的商品等)。Cookie是一个存储在用户主机中的文本文件，里面含有一串“识别码”，如“123456”，用于Web服务识别用户。Web服务器根据Cookie就能从数据库中查询到该用户的活动记录，进而执行一些个性化的工作，如根据用户之前浏览过的商品向其推荐新产品等。 HTTP 采用TCP作为运输层协议，保证了数据的可靠传输。HTTP不必考虑数据在传输过程中被丢弃后又怎样被重传。但是，HTTP本身是无连接的（请读者务必注意)。也就是说，虽然HTTP使用了TCP连接，但通信的双方在交换HTTP报文之前不需要先建立HTTP连接。 HTTP既可以使用非持久连接，也可以使用持久连接(HTTP/1.1支持)。 对于非持久连接，每个网页元素对象（如JPEG图形、Flash等）的传输都需要单独建立一个TCP连接，如图6.12所示(第三次握手的报文段中捎带了客户对万维网文档的请求)。也就是说，请求一个万维网文档所需的时间是该文档的传输时间(与文档大小成正比)加上两倍往返时间RTT(一个RTT用于TCP连接，另一个RTT用于请求和接收文档)。 所谓持久连接，是指万维网服务器在发送响应后仍然保持这条连接，使同一个客户和服务器可以继续在这条连接上传送后续的HTTP请求与响应报文，如图6.13所示。持久连接又分为非流水线和流水线两种方式。对于非流水线方式，客户在收到前一个响应后才能发出下一个请求。HTTP/1.1的默认方式是使用流水线的持久连接。这种情况下，客户每遇到一个对象引用就立即发出一个请求，因而客户可以逐个地连续发出对各个引用对象的请求。如果所有的请求和响应都是连续发送的，那么所有引用的对象共计经历1个RTT延迟，而不是像非流水线方式那样，每个引用都必须有1个RTT延迟。 HTTP的报文结构 HTTP是面向文本的(Text-Oriented)，因此报文中的每个字段都是一些ASCII码串，并且每个字段的长度都是不确定的。有两类HTTP报文: 请求报文:从客户向服务器发送的请求报文，如图6.14(a)所示。响应报文:从服务器到客户的回答，如图6.14(b)所示。HTTP请求报文和响应报文都由三个部分组成。从图6.14可以看出，这两种报文格式的区别是开始行不同。 开始行:用于区分是请求报文还是响应报文。在请求报文中的开始行称为请求行，而在响应报文中的开始行称为状态行。开始行的三个字段之间都以空格分隔，最后的“CR”和“LF”分别代表“回车”和“换行”。请求报文的“请求行”有三个内容:方法、请求资源的URL及HTTP的版本。其中，“方法”是对所请求对象进行的操作，这些方法实际上也就是一些命令。表6.1 给出了HTTP请求报文中常用的几个方法。首部行:用来说明浏览器、服务器或报文主体的一些信息。首部可有几行，但可不使用。在每个首部行都有首部字段名和它的值，每行在结束的地方都要有“回车”和“换行”。整个首部结束时，还有一空行将首部行和后面的实体主体分开。 实体主体:在请求报文中一般不用这个字段，而在响应报文中也可能没有这个字段。 图6.15所示为使用Wireshark捕获的HTTP请求报文的示例，下面将结合前面几章的内容对请求报文（图中下部分）进行分析。 根据帧（见3.1.1节）)的结构定义，在图6.15所示的以太网数据帧中，第1~6个字节为目的MAC地址（默认网关地址)，即00-Of-e2-3f-27-3f;第7～12个字节为本机 MAC 地址，即00-27-13-67-73-8d;第13~14个字节08～00为类型字段，表示上层使用的是P数据报协议。第15~34个字节(共20B)为P数据报的首部，其中第27~30个字节为源P地址，即 db-df-d2-70,转换成十进制为219.223.210.112;第31~34个字节为目的P地址，即71-69-4e-0a，转换成十进制为113.105.78.10。第35～54个字节(共20B）为TCP报文段的首部。 从第55个字节开始才是TCP数据部分(阴影部分)，即从应用层传递下来的数据（本例中即请求报文)，GET对应请求行的方法，lface/20.gif对应请求行的URL，HTTP/1.1对应请求行的版本，左边数字是对应字符的ASCII码，如’G’=0x47、E'=0x45、“T'=Ox54等。右下角开始的“…?”?.’ .gs…E..%..@.@..0…pgi”等是上面介绍过的第1~54个字节中对应的ASCH码字符，而这些字符本身不代表任何意义。 常见应用层协议小结如表6.2所示。 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:6:5","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["School courses"],"content":"6.6本章小结及疑难点 ","date":"2022-06-03 10:46:24","objectID":"/cn_wangdao/:6:6","tags":["computer network"],"title":"CN_wangdao","uri":"/cn_wangdao/"},{"categories":["tools"],"content":"└── .git ├── COMMIT_EDITMSG # 保存最新的commit message ├── config # 仓库的配置文件 ├── description # 仓库的描述信息，主要给gitweb使用 ├── HEAD # 指向当前分支 ├── hooks # 存放一些shell脚本，可以设置特定的git命令后触发相应的脚本 ├── index # 二进制暂存区（stage） ├── info # 仓库的其他信息 │ └── exclude # 本地的排除文件规则，功能和.gitignore类似 ├── logs # 保存所有更新操作的引用记录，主要用于git reflog等 ├── objects # 所有文件的存储对象 └── refs # 具体的引用，主要存储分支和标签的引用 链接：https://juejin.cn/post/6844903986839945229 git check,restore,reset git fetch 相当于是从远程获取最新到本地，不会自动merge git pull：相当于是从远程获取最新版本并merge到本地 清空暂存区：rm .git/index 更改远程origin仓库git remote set-url origin git://new.url.here 注意：在 Git 2.23 版本中，引入了一个名为 git switch 的新命令，最终会取代 git checkout，因为 checkout 作为单个命令有点超载（它承载了很多独立的功能）。 如果你想创建一个新的分支同时切换到新创建的分支的话，可以通过 git checkout -b 来实现。 接下来咱们看看如何将两个分支合并到一起。就是说我们新建一个分支，在其上开发某个新功能，开发完成后再合并回主线。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:0","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"git tag https://git-scm.com/book/zh/v2/Git-%E5%9F%BA%E7%A1%80-%E6%89%93%E6%A0%87%E7%AD%BE ","date":"2022-05-31 14:55:58","objectID":"/git/:0:1","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"git merge git merge xxx 将xxx分支合并到当前分支 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:2","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"git rebase 第二种合并分支的方法是 git rebase(把当前分支的工作直接移到目标分支)。Rebase 实际上就是取出一系列的提交记录，“复制”它们，然后在另外一个地方逐个的放下去。 Rebase 的优势就是可以创造更线性的提交历史，这听上去有些难以理解。如果只允许使用 Rebase 的话，代码库的提交历史将会变得异常清晰。 git rebase b1 b2(将b2合并到b1下) git rebase b1(将当前分支合并到b1下) 在接触 Git 更高级功能之前，我们有必要先学习在你项目的提交树上前后移动的几种方法。 一旦熟悉了如何在 Git 提交树上移动，你驾驭其它命令的能力也将水涨船高！ ","date":"2022-05-31 14:55:58","objectID":"/git/:0:3","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"分离HEAD 我们首先看一下 “HEAD”。 HEAD 是一个对当前检出记录的符号引用 —— 也就是指向你正在其基础上进行工作的提交记录。 HEAD 总是指向当前分支上最近一次提交记录。大多数修改提交树的 Git 命令都是从改变 HEAD 的指向开始的。 HEAD 通常情况下是指向分支名的（如 bugFix）。在你提交时，改变了 bugFix 的状态，这一变化通过 HEAD 变得可见。 分离的 HEAD 就是让其指向了某个具体的提交记录（哈希值）而不是分支名。 通过git checkout c1(某个提交记录)可以将HEAD从默认的分支名移到c1 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:4","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"相对引用 通过指定提交记录哈希值的方式在 Git 中移动不太方便。在实际应用时，并没有像本程序中这么漂亮的可视化提交树供你参考，所以你就不得不用 git log 来查查看提交记录的哈希值。 并且哈希值在真实的 Git 世界中也会更长（译者注：基于 SHA-1，共 40 位）。例如前一关的介绍中的提交记录的哈希值可能是 fed2da64c0efc5293610bdd892f82a58e8cbc5d8。舌头都快打结了吧… 比较令人欣慰的是，Git 对哈希的处理很智能。你只需要提供能够唯一标识提交记录的前几个字符即可。因此我可以仅输入fed2 而不是上面的一长串字符。 正如我前面所说，通过哈希值指定提交记录很不方便，所以 Git 引入了相对引用。这个就很厉害了! 使用相对引用的话，你就可以从一个易于记忆的地方（比如 bugFix 分支或 HEAD）开始计算。 相对引用非常给力，这里我介绍两个简单的用法： 使用 ^ 向上移动 1 个提交记录eg:git checkout HEAD^ 使用 ~ 向上移动多个提交记录，如 ~3 eg:git checkout HEAD~4 我使用相对引用最多的就是移动分支。可以直接使用 -f 选项让分支指向另一个提交。例如: git branch -f main HEAD~3 上面的命令会将 main 分支强制指向 HEAD 的第 3 级父提交。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:5","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"撤销变更 在 Git 里撤销变更的方法很多。和提交一样，撤销变更由底层部分（暂存区的独立文件或者片段）和上层部分（变更到底是通过哪种方式被撤销的）组成。我们这个应用主要关注的是后者。 主要有两种方法用来撤销变更 —— 一是 git reset，还有就是 git revert。接下来咱们逐个进行讲解。 git reset 通过把分支记录回退几个提交记录来实现撤销改动。你可以将这想象成“改写历史”。git reset 向上移动分支，原来指向的提交记录就跟从来没有提交过一样。 git reset HEAD~1(撤回本地的最近一次提交) 虽然在你的本地分支中使用 git reset 很方便，但是这种“改写历史”的方法对大家一起使用的远程分支是无效的哦！ 为了撤销更改并分享给别人，我们需要使用 git revert。 git revert HEAD（撤回远程的最近一次提交） ","date":"2022-05-31 14:55:58","objectID":"/git/:0:6","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"整理提交记录 到现在我们已经学习了 Git 的基础知识 —— 提交、分支以及在提交树上移动。 这些概念涵盖了 Git 90% 的功能，同样也足够满足开发者的日常需求 然而, 剩余的 10% 在处理复杂的工作流时(或者当你陷入困惑时）可能就显得尤为重要了。接下来要讨论的这个话题是“整理提交记录” —— 开发人员有时会说“我想要把这个提交放到这里, 那个提交放到刚才那个提交的后面”, 而接下来就讲的就是它的实现方式，非常清晰、灵活，还很生动。 看起来挺复杂, 其实是个很简单的概念。 Git Cherry-pick 本系列的第一个命令是 git cherry-pick, 命令形式为: git cherry-pick \u003c提交号\u003e... 如果你想将一些提交复制到当前所在的位置（HEAD）下面的话， Cherry-pick 是最直接的方式了。我个人非常喜欢 cherry-pick，因为它特别简单。 这里有一个仓库, 我们想将 side 分支上的工作复制到 main 分支，你立刻想到了之前学过的 rebase 了吧？但是咱们还是看看 cherry-pick 有什么本领吧。 git cherry-pick c2 c4（把c2 c4两次提交记录依次放在HEAD后面） 交互式的 rebase 当你知道你所需要的提交记录（并且还知道这些提交记录的哈希值）时, 用 cherry-pick 再好不过了 —— 没有比这更简单的方式了。 但是如果你不清楚你想要的提交记录的哈希值呢? 幸好 Git 帮你想到了这一点, 我们可以利用交互式的 rebase —— 如果你想从一系列的提交记录中找到想要的记录, 这就是最好的方法了 咱们具体来看一下…… 交互式 rebase 指的是使用带参数 –interactive 的 rebase 命令, 简写为 -i 如果你在命令后增加了这个选项, Git 会打开一个 UI 界面并列出将要被复制到目标分支的备选提交记录，它还会显示每个提交记录的哈希值和提交说明，提交说明有助于你理解这个提交进行了哪些更改。 在实际使用时，所谓的 UI 窗口一般会在文本编辑器 —— 如 Vim —— 中打开一个文件。 考虑到课程的初衷，我弄了一个对话框来模拟这些操作。 当 rebase UI界面打开时, 你能做3件事: 调整提交记录的顺序（通过鼠标拖放来完成） 删除你不想要的提交（通过切换 pick 的状态来完成，关闭就意味着你不想要这个提交记录） 合并提交。 遗憾的是由于某种逻辑的原因，我们的课程不支持此功能，因此我不会详细介绍这个操作。简而言之，它允许你把多个提交记录合并成一个。 git rebase -i HEAD~4 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:7","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"本地栈式提交 来看一个在开发中经常会遇到的情况：我正在解决某个特别棘手的 Bug，为了便于调试而在代码中添加了一些调试命令并向控制台打印了一些信息。 这些调试和打印语句都在它们各自的提交记录里。最后我终于找到了造成这个 Bug 的根本原因，解决掉以后觉得沾沾自喜！ 最后就差把 bugFix 分支里的工作合并回 main 分支了。你可以选择通过 fast-forward 快速合并到 main 分支上，但这样的话 main 分支就会包含我这些调试语句了。你肯定不想这样，应该还有更好的方式…… 实际我们只要让 Git 复制解决问题的那一个提交记录就可以了。跟之前我们在“整理提交记录”中学到的一样，我们可以使用 git rebase -i git cherry-pick 来达到目的。 提交的技巧 #1 接下来这种情况也是很常见的：你之前在 newImage 分支上进行了一次提交，然后又基于它创建了 caption 分支，然后又提交了一次。 此时你想对某个以前的提交记录进行一些小小的调整。比如设计师想修改一下 newImage 中图片的分辨率，尽管那个提交记录并不是最新的了。 Close Next 我们可以通过下面的方法来克服困难： 先用 git rebase -i 将提交重新排序，然后把我们想要修改的提交记录挪到最前 然后用 git commit –amend 来进行一些小修改 接着再用 git rebase -i 来将他们调回原来的顺序 最后我们把 main 移到修改的最前端（用你自己喜欢的方法），就大功告成啦！ 当然完成这个任务的方法不止上面提到的一种（我知道你在看 cherry-pick 啦），之后我们会多点关注这些技巧啦，但现在暂时只专注上面这种方法。 最后有必要说明一下目标状态中的那几个' —— 我们把这个提交移动了两次，每移动一次会产生一个 ‘；而 C2 上多出来的那个是我们在使用了 amend 参数提交时产生的，所以最终结果就是这样了。 也就是说，我在对比结果的时候只会对比提交树的结构，对于 ' 的数量上的不同，并不纳入对比范围内。只要你的 main 分支结构与目标结构相同，我就算你通过。 提交的技巧 #2 如果你还没有完成“提交的技巧 #1”（前一关）的话，请先通过以后再来！ 正如你在上一关所见到的，我们可以使用 rebase -i 对提交记录进行重新排序。只要把我们想要的提交记录挪到最前端，我们就可以很轻松的用 –amend 修改它，然后把它们重新排成我们想要的顺序。 但这样做就唯一的问题就是要进行两次排序，而这有可能造成由 rebase 而导致的冲突。下面还是看看 git cherry-pick 是怎么做的吧。 要在心里牢记 cherry-pick 可以将提交树上任何地方的提交记录取过来追加到 HEAD 上（只要不是 HEAD 上游的提交就没问题）。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:8","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"Git Tags 相信通过前面课程的学习你已经发现了：分支很容易被人为移动，并且当有新的提交时，它也会移动。分支很容易被改变，大部分分支还只是临时的，并且还一直在变。 你可能会问了：有没有什么可以永远指向某个提交记录的标识呢，比如软件发布新的大版本，或者是修正一些重要的 Bug 或是增加了某些新特性，有没有比分支更好的可以永远指向这些提交的方法呢？ 当然有了！Git 的 tag 就是干这个用的啊，它们可以（在某种程度上 —— 因为标签可以被删除后重新在另外一个位置创建同名的标签）永久地将某个特定的提交命名为里程碑，然后就可以像分支一样引用了。 更难得的是，它们并不会随着新的提交而移动。你也不能切换到某个标签上面进行修改提交，它就像是提交树上的一个锚点，标识了某个特定的位置。 git tag v1 C1 我们将这个标签命名为 v1，并且明确地让它指向提交记录 C1，如果你不指定提交记录，Git 会用 HEAD 所指向的位置。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:9","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"Git Describe 由于标签在代码库中起着“锚点”的作用，Git 还为此专门设计了一个命令用来描述离你最近的锚点（也就是标签），它就是 git describe！ Git Describe 能帮你在提交历史中移动了多次以后找到方向；当你用 git bisect（一个查找产生 Bug 的提交记录的指令）找到某个提交记录时，或者是当你坐在你那刚刚度假回来的同事的电脑前时， 可能会用到这个命令。 git describe 的​​语法是： git describe 可以是任何能被 Git 识别成提交记录的引用，如果你没有指定的话，Git 会以你目前所检出的位置（HEAD）。 它输出的结果是这样的： __g tag 表示的是离 ref 最近的标签， numCommits 是表示这个 ref 与 tag 相差有多少个提交记录， hash 表示的是你所给定的 ref 所表示的提交记录哈希值的前几位。 当 ref 提交记录上有某个标签时，则只输出标签名称 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:10","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"远程仓库 远程仓库并不复杂, 在如今的云计算盛行的世界很容易把远程仓库想象成一个富有魔力的东西, 但实际上它们只是你的仓库在另个一台计算机上的拷贝。你可以通过因特网与这台计算机通信 —— 也就是增加或是获取提交记录 话虽如此, 远程仓库却有一系列强大的特性 首先也是最重要的的点, 远程仓库是一个强大的备份。本地仓库也有恢复文件到指定版本的能力, 但所有的信息都是保存在本地的。有了远程仓库以后，即使丢失了本地所有数据, 你仍可以通过远程仓库拿回你丢失的数据。 还有就是, 远程让代码社交化了! 既然你的项目被托管到别的地方了, 你的朋友可以更容易地为你的项目做贡献(或者拉取最新的变更) 现在用网站来对远程仓库进行可视化操作变得越发流行了(像 GitHub), 但远程仓库永远是这些工具的顶梁柱, 因此理解其概念非常的重要! 我们创建远程仓库的命令 直到现在, 教程都聚焦于本地仓库的操作（branch、merge、rebase 等等）。但我们现在需要学习远程仓库的操作 —— 我们需要一个配置这种环境的命令, 它就是 git clone。 从技术上来讲，git clone 命令在真实的环境下的作用是在本地创建一个远程仓库的拷贝（比如从 github.com）。 但在我们的教程中使用这个命令会有一些不同 —— 它会在远程创建一个你本地仓库的副本。显然这和真实命令的意思刚好相反，但是它帮咱们把本地仓库和远程仓库关联到了一起，在教程中就凑合着用吧。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:11","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"远程分支 注意：在 Git 2.23 版本中，引入了一个名为 git switch 的新命令，最终会取代 git checkout，因为 checkout 作为单个命令有点超载（它承载了很多独立的功能）。 由于现在很多人还无法使用 switch，本次课程仍然使用 checkout 而不是 switch， 但是如果你想尝试一下新命令，我们的应用也是支持的！并且你可以从这里学到更多关于新命令的内容。 既然你已经看过 git clone 命令了，咱们深入地看一下发生了什么。 你可能注意到的第一个事就是在我们的本地仓库多了一个名为 o/main 的分支, 这种类型的分支就叫远程分支。由于远程分支的特性导致其拥有一些特殊属性。 远程分支反映了远程仓库(在你上次和它通信时)的状态。这会有助于你理解本地的工作与公共工作的差别 —— 这是你与别人分享工作成果前至关重要的一步. 远程分支有一个特别的属性，在你检出时自动进入分离 HEAD 状态。Git 这么做是出于不能直接在这些分支上进行操作的原因, 你必须在别的地方完成你的工作, （更新了远程分支之后）再用远程分享你的工作成果。 为什么有 o/？ 你可能想问这些远程分支的前面的 o/ 是什么意思呢？好吧, 远程分支有一个命名规范 —— 它们的格式是: /因此，如果你看到一个名为 o/main 的分支，那么这个分支就叫 main，远程仓库的名称就是 o。 大多数的开发人员会将它们主要的远程仓库命名为 origin，并不是 o。这是因为当你用 git clone 某个仓库时，Git 已经帮你把远程仓库的名称设置为 origin 了 不过 origin 对于我们的 UI 来说太长了，因此不得不使用简写 o :) 但是要记住, 当你使用真正的 Git 时, 你的远程仓库默认为 origin! 说了这么多，让我们看看实例。 git checkout o/main ;git commit 正如你所见，Git 变成了分离 HEAD 状态，当添加新的提交时 o/main 也不会更新。这是因为 o/main 只有在远程仓库中相应的分支更新了以后才会更新。 在 main 分支上做一次提交；然后检出 o/main，再做一提交。这有助于你理解远程分支的不同，他们的更新只是反映了远程的状态。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:12","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"Git Fetch Git 远程仓库相当的操作实际可以归纳为两点：向远程仓库传输数据以及从远程仓库获取数据。既然我们能与远程仓库同步，那么就可以分享任何能被 Git 管理的更新（因此可以分享代码、文件、想法、情书等等）。 本节课我们将学习如何从远程仓库获取数据 —— 命令如其名，它就是 git fetch。 你会看到当我们从远程仓库获取数据时, 远程分支也会更新以反映最新的远程仓库。在上一节课程中我们已经提及过这一点了。 git fetch 做了些什么 git fetch 完成了仅有的但是很重要的两步: 从远程仓库下载本地仓库中缺失的提交记录 更新远程分支指针(如 o/main) git fetch 实际上将本地仓库中的远程分支更新成了远程仓库相应分支最新的状态。 如果你还记得上一节课程中我们说过的，远程分支反映了远程仓库在你最后一次与它通信时的状态，git fetch 就是你与远程仓库通信的方式了！希望我说的够明白了，你已经了解 git fetch 与远程分支之间的关系了吧。 git fetch 通常通过互联网（使用 http:// 或 git:// 协议) 与远程仓库通信。 git fetch 不会做的事 git fetch 并不会改变你本地仓库的状态。它不会更新你的 main 分支，也不会修改你磁盘上的文件。 理解这一点很重要，因为许多开发人员误以为执行了 git fetch 以后，他们本地仓库就与远程仓库同步了。它可能已经将进行这一操作所需的所有数据都下载了下来，但是并没有修改你本地的文件。我们在后面的课程中将会讲解能完成该操作的命令 :D 所以, 你可以将 git fetch 的理解为单纯的下载操作。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:13","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"Git Pull 既然我们已经知道了如何用 git fetch 获取远程的数据, 现在我们学习如何将这些变化更新到我们的工作当中。 其实有很多方法的 —— 当远程分支中有新的提交时，你可以像合并本地分支那样来合并远程分支。也就是说就是你可以执行以下命令: git cherry-pick o/main git rebase o/main git merge o/main 等等 实际上，由于先抓取更新再合并到本地分支这个流程很常用，因此 Git 提供了一个专门的命令来完成这两个操作。它就是我们要讲的 git pull。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:14","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"模拟团队合作 这里有一件棘手的事 —— 为了接下来的课程, 我们需要先教你如何制造远程仓库的变更。 这意味着，我们需要“假装”你的同事、朋友、合作伙伴更新了远程仓库，有可能是某个特定的分支，或是几个提交记录。 为了做到这点，我们引入一个自造命令 git fakeTeamwork！它的名称已经说明了一切，先看演示.. ","date":"2022-05-31 14:55:58","objectID":"/git/:0:15","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"Git Push OK，我们已经学过了如何从远程仓库获取更新并合并到本地的分支当中。这非常棒……但是我如何与大家分享我的成果呢？ 嗯，上传自己分享内容与下载他人的分享刚好相反，那与 git pull 相反的命令是什么呢？git push！ git push 负责将你的变更上传到指定的远程仓库，并在远程仓库上合并你的新提交记录。一旦 git push 完成, 你的朋友们就可以从这个远程仓库下载你分享的成果了！ 你可以将 git push 想象成发布你成果的命令。它有许多应用技巧，稍后我们会了解到，但是咱们还是先从基础的开始吧…… 注意 —— git push 不带任何参数时的行为与 Git 的一个名为 push.default 的配置有关。它的默认值取决于你正使用的 Git 的版本，但是在教程中我们使用的是 upstream。 这没什么太大的影响，但是在你的项目中进行推送之前，最好检查一下这个配置。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:16","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"偏离的工作 现在我们已经知道了如何从其它地方 pull 提交记录，以及如何 push 我们自己的变更。看起来似乎没什么难度，但是为何还会让人们如此困惑呢？ 困难来自于远程库提交历史的偏离。在讨论这个问题的细节前，我们先来看一个例子…… 假设你周一克隆了一个仓库，然后开始研发某个新功能。到周五时，你新功能开发测试完毕，可以发布了。但是 —— 天啊！你的同事这周写了一堆代码，还改了许多你的功能中使用的 API，这些变动会导致你新开发的功能变得不可用。但是他们已经将那些提交推送到远程仓库了，因此你的工作就变成了基于项目旧版的代码，与远程仓库最新的代码不匹配了。 这种情况下, git push 就不知道该如何操作了。如果你执行 git push，Git 应该让远程仓库回到星期一那天的状态吗？还是直接在新代码的基础上添加你的代码，亦或由于你的提交已经过时而直接忽略你的提交？ 因为这情况（历史偏离）有许多的不确定性，Git 是不会允许你 push 变更的。实际上它会强制你先合并远程最新的代码，然后才能分享你的工作。 你需要做的就是使你的工作基于最新的远程分支。 有许多方法做到这一点呢，不过最直接的方法就是通过 rebase 调整你的工作。咱们继续，看看怎么 rebase！ 我们用 git fetch 更新了本地仓库中的远程分支，然后用 rebase 将我们的工作移动到最新的提交记录下，最后再用 git push 推送到远程仓库。 还有其它的方法可以在远程仓库变更了以后更新我的工作吗? 当然有，我们还可以使用 merge 尽管 git merge 不会移动你的工作（它会创建新的合并提交），但是它会告诉 Git 你已经合并了远程仓库的所有变更。这是因为远程分支现在是你本地分支的祖先，也就是说你的提交已经包含了远程分支的所有变化。 git pull 就是 fetch 和 merge 的简写，类似的 git pull –rebase 就是 fetch 和 rebase 的简写！ 由 fetch、rebase/merge 和 push 组成的工作流很普遍 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:17","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"远程服务器拒绝!(Remote Rejected) 如果你是在一个大的合作团队中工作, 很可能是main被锁定了, 需要一些Pull Request流程来合并修改。如果你直接提交(commit)到本地main, 然后试图推送(push)修改, 你将会收到这样类似的信息: ! [远程服务器拒绝] main -\u003e main (TF402455: 不允许推送(push)这个分支; 你必须使用pull request来更新这个分支.) 为什么会被拒绝? 远程服务器拒绝直接推送(push)提交到main, 因为策略配置要求 pull requests 来提交更新. 你应该按照流程,新建一个分支, 推送(push)这个分支并申请pull request,但是你忘记并直接提交给了main.现在你卡住并且无法推送你的更新. 解决办法 新建一个分支feature, 推送到远程服务器. 然后reset你的main分支和远程服务器保持一致, 否则下次你pull并且他人的提交和你冲突的时候就会有问题. ","date":"2022-05-31 14:55:58","objectID":"/git/:0:18","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"合并特性分支 既然你应该很熟悉 fetch、pull、push 了，现在我们要通过一个新的工作流来测试你的这些技能。 在大型项目中开发人员通常会在（从 main 上分出来的）特性分支上工作，工作完成后只做一次集成。这跟前面课程的描述很相像（把 side 分支推送到远程仓库），不过本节我们会深入一些. 但是有些开发人员只在 main 上做 push、pull —— 这样的话 main 总是最新的，始终与远程分支 (o/main) 保持一致。 对于接下来这个工作流，我们集成了两个步骤： 将特性分支集成到 main 上 推送并更新远程分支 快速的更新 main 分支并推送到远程。 git pull –rebase;git push ","date":"2022-05-31 14:55:58","objectID":"/git/:0:19","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"为什么不用 merge 呢? 为了 push 新变更到远程仓库，你要做的就是包含远程仓库中最新变更。意思就是只要你的本地分支包含了远程分支（如 o/main）中的最新变更就可以了，至于具体是用 rebase 还是 merge，并没有限制。 那么既然没有规定限制，为何前面几节都在着重于 rebase 呢？为什么在操作远程分支时不喜欢用 merge 呢？ 在开发社区里，有许多关于 merge 与 rebase 的讨论。以下是关于 rebase 的优缺点： 优点: Rebase 使你的提交树变得很干净, 所有的提交都在一条线上 缺点: Rebase 修改了提交树的历史 比如, 提交 C1 可以被 rebase 到 C3 之后。这看起来 C1 中的工作是在 C3 之后进行的，但实际上是在 C3 之前。 一些开发人员喜欢保留提交历史，因此更偏爱 merge。而其他人（比如我自己）可能更喜欢干净的提交树，于是偏爱 rebase。仁者见仁，智者见智。 :D ","date":"2022-05-31 14:55:58","objectID":"/git/:0:20","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"远程跟踪分支 在前几节课程中有件事儿挺神奇的，Git 好像知道 main 与 o/main 是相关的。当然这些分支的名字是相似的，可能会让你觉得是依此将远程分支 main 和本地的 main 分支进行了关联。这种关联在以下两种情况下可以清楚地得到展示： pull 操作时, 提交记录会被先下载到 o/main 上，之后再合并到本地的 main 分支。隐含的合并目标由这个关联确定的。 push 操作时, 我们把工作从 main 推到远程仓库中的 main 分支(同时会更新远程分支 o/main) 。这个推送的目的地也是由这种关联确定的！ 远程跟踪 直接了当地讲，main 和 o/main 的关联关系就是由分支的“remote tracking”属性决定的。main 被设定为跟踪 o/main —— 这意味着为 main 分支指定了推送的目的地以及拉取后合并的目标。 你可能想知道 main 分支上这个属性是怎么被设定的，你并没有用任何命令指定过这个属性呀！好吧, 当你克隆仓库的时候, Git 就自动帮你把这个属性设置好了。 当你克隆时, Git 会为远程仓库中的每个分支在本地仓库中创建一个远程分支（比如 o/main）。然后再创建一个跟踪远程仓库中活动分支的本地分支，默认情况下这个本地分支会被命名为 main。 克隆完成后，你会得到一个本地分支（如果没有这个本地分支的话，你的目录就是“空白”的），但是可以查看远程仓库中所有的分支（如果你好奇心很强的话）。这样做对于本地仓库和远程仓库来说，都是最佳选择。 这也解释了为什么会在克隆的时候会看到下面的输出： local branch “main” set to track remote branch “o/main” 你可以让任意分支跟踪 o/main, 然后该分支会像 main 分支一样得到隐含的 push 目的地以及 merge 的目标。 这意味着你可以在分支 totallyNotMain 上执行 git push，将工作推送到远程仓库的 main 分支上。 有两种方法设置这个属性，第一种就是通过远程分支检出一个新的分支，执行: git checkout -b totallyNotMain o/main 就可以创建一个名为 totallyNotMain 的分支，它跟踪远程分支 o/main。 第二种方法 另一种设置远程追踪分支的方法就是使用：git branch -u 命令，执行： git branch -u o/main foo 这样 foo 就会跟踪 o/main 了。如果当前就在 foo 分支上, 还可以省略 foo： git branch -u o/main ","date":"2022-05-31 14:55:58","objectID":"/git/:0:21","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"Git Push 的参数 很好! 既然你知道了远程跟踪分支，我们可以开始揭开 git push、fetch 和 pull 的神秘面纱了。我们会逐个介绍这几个命令，它们在理念上是非常相似的。 首先来看 git push。在远程跟踪课程中，你已经学到了 Git 是通过当前检出分支的属性来确定远程仓库以及要 push 的目的地的。这是未指定参数时的行为，我们可以为 push 指定参数，语法是： git push 参数是什么意思呢？我们稍后会深入其中的细节, 先看看例子, 这个命令是: git push origin main 把这个命令翻译过来就是： 切到本地仓库中的“main”分支，获取所有的提交，再到远程仓库“origin”中找到“main”分支，将远程仓库中没有的提交记录都添加上去，搞定之后告诉我。 我们通过“place”参数来告诉 Git 提交记录来自于 main, 要推送到远程仓库中的 main。它实际就是要同步的两个仓库的位置。 需要注意的是，因为我们通过指定参数告诉了 Git 所有它需要的信息, 所以它就忽略了我们所检出的分支的属性！ 不指定参数的话，检出的 HEAD 如果没有跟踪任何分支，那将会导致命令失败 参数详解 还记得之前课程说的吧，当为 git push 指定 place 参数为 main 时，我们同时指定了提交记录的来源和去向。 你可能想问 —— 如果来源和去向分支的名称不同呢？比如你想把本地的 foo 分支推送到远程仓库中的 bar 分支。 哎，很遗憾 Git 做不到…… 开个玩笑，别当真！当然是可以的啦 :) Git 拥有超强的灵活性（有点过于灵活了） 接下来咱们看看是怎么做的…… 要同时为源和目的地指定 的话，只需要用冒号 : 将二者连起来就可以了： git push origin : 这个参数实际的值是个 refspec，“refspec” 是一个自造的词，意思是 Git 能识别的位置（比如分支 foo 或者 HEAD~1） 一旦你指定了独立的来源和目的地，就可以组织出言简意赅的远程操作命令了，让我们看看演示！ git push origin foo^:main 这是个令人困惑的命令，但是它确实是可以运行的 ——Git 将 foo^ 解析为一个位置，上传所有未被包含到远程仓库里 main 分支中的提交记录 如果你要推送到的目的分支不存在会怎么样呢？没问题！Git 会在远程仓库中根据你提供的名称帮你创建这个分支！ ","date":"2022-05-31 14:55:58","objectID":"/git/:0:22","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"Git fetch 的参数 我们刚学习了 git push 的参数，很酷的 参数，还有用冒号分隔的 refspecs（:）。 这些参数可以用于 git fetch 吗？ 你猜中了！git fetch 的参数和 git push 极其相似。他们的概念是相同的，只是方向相反罢了（因为现在你是下载，而非上传） 让我们逐个讨论下这些概念…… 参数 如果你像如下命令这样为 git fetch 设置 的话： git fetch origin foo Git 会到远程仓库的 foo 分支上，然后获取所有本地不存在的提交，放到本地的 o/foo 上。 来看个例子（还是前面的例子，只是命令不同了） git fetch 它不会更新你的本地的非远程分支, 只是下载提交记录（这样, 你就可以对远程分支进行检查或者合并了）。 “如果我们指定 : 会发生什么呢？” 如果你觉得直接更新本地分支很爽，那你就用冒号分隔的 refspec 吧。不过，你不能在当前检出的分支上干这个事，但是其它分支是可以的。 这里有一点是需要注意的 —— source 现在指的是远程仓库中的位置，而 才是要放置提交的本地仓库的位置。它与 git push 刚好相反，这是可以讲的通的，因为我们在往相反的方向传送数据。 理论上虽然行的通，但开发人员很少这么做。我在这里介绍它主要是为了从概念上说明 fetch 和 push 的相似性，只是方向相反罢了。 git fetch origin foo~1:bar Git 将 foo~1 解析成一个 origin 仓库的位置，然后将那些提交记录下载到了本地的 bar 分支（一个本地分支）上。注意由于我们指定了目标分支，foo 和 o/foo 都没有被更新。 如果执行命令前目标分支不存在会怎样呢？我们看一下上个对话框中没有 bar 分支的情况。 跟 git push 一样，Git 会在 fetch 前自己创建立本地分支, 就像是 Git 在 push 时，如果远程仓库中不存在目标分支，会自己在建立一样。 如果 git fetch 没有参数，它会下载所有的提交记录到各个远程分支…… 使用 fetch 时, 你最好指定 source 和 destination。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:23","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"古怪的 Git 有两种关于 的用法是比较诡异的，即你可以在 git push 或 git fetch 时不指定任何 source，方法就是仅保留冒号和 destination 部分，source 部分留空。 git push origin :side git fetch origin :bugFix 我们分别来看一下这两条命令的作用…… git push origin :foo 我们通过给 push 传空值 source，成功删除了远程仓库中的 foo 分支, 这真有意思… git fetch origin :bar 如果 fetch 空 到本地，会在本地创建一个新分支。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:24","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"Git pull 参数 既然你已经掌握关于 git fetch 和 git push 参数的方方面面了，关于 git pull 几乎没有什么可以讲的了 :) 因为 git pull 到头来就是 fetch 后跟 merge 的缩写。你可以理解为用同样的参数执行 git fetch，然后再 merge 你所抓取到的提交记录。 还可以和其它更复杂的参数一起使用, 来看一些例子: 以下命令在 Git 中是等效的: git pull origin foo 相当于： git fetch origin foo; git merge o/foo 还有… git pull origin bar~1:bugFix 相当于： git fetch origin bar~1:bugFix; git merge bugFix 看到了? git pull 实际上就是 fetch + merge 的缩写, git pull 唯一关注的是提交最终合并到哪里（也就是为 git fetch 所提供的 destination 参数） pull 也可以用 source:destination 吗? 当然喽 git pull origin main:foo 它先在本地创建了一个叫 foo 的分支，从远程仓库中的 main 分支中下载提交记录，并合并到 foo，然后再 merge 到我们的当前检出的分支 bar 上。 ","date":"2022-05-31 14:55:58","objectID":"/git/:0:25","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"gitignore https://zhuanlan.zhihu.com/p/52885189 ","date":"2022-05-31 14:55:58","objectID":"/git/:1:0","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"生成ssh密钥 ls -al ~/.ssh 生成秘钥 ssh-keygen -t rsa -C ‘2838264218@qq.com’ -t 指定密钥类型，默认是 rsa ，可以省略。 -C 设置注释文字，比如邮箱。 -f 指定密钥文件存储文件名。 把公钥放在github上 使用下面的命令测试是否成功 ssh -T git@github.com ","date":"2022-05-31 14:55:58","objectID":"/git/:2:0","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"私人仓库 setting-\u003edeveloper settings-\u003epersonal access tockens git remote add https://username:token@github.com/username/rep/git ","date":"2022-05-31 14:55:58","objectID":"/git/:3:0","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["tools"],"content":"其他 强制push :加参数-f ","date":"2022-05-31 14:55:58","objectID":"/git/:4:0","tags":["git"],"title":"Git","uri":"/git/"},{"categories":["Go"],"content":" https://go.dev/doc/effective_go ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:0:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"引言 Go 是一门全新的语言。尽管它从既有的语言中借鉴了许多理念，但其与众不同的特性，使得用 Go 编程在本质上就不同于其它语言。将现有的 C++ 或 Java 程序直译为 Go 程序并不能令人满意——毕竟 Java 程序是用 Java 编写的，而不是 Go。 另一方面，若从 Go 的角度去分析问题，你就能编写出同样可行但大不相同的程序。 换句话说，要想将 Go 程序写得好，就必须理解其特性和风格。了解命名、格式化、程序结构等既定规则也同样重要，这样你编写的程序才能更容易被其他程序员所理解。 本文档就如何编写清晰、地道的 Go 代码提供了一些技巧。它是对 语言规范、 Go 语言之旅 以及 如何使用 Go 编程 的补充说明，因此我们建议您先阅读这些文档。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:1:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"格式化 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:2:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"注释 Go 语言支持 C 风格的块注释 /* */ 和 C++ 风格的行注释 //。 行注释更为常用，而块注释则主要用作包的注释，当然也可在禁用一大段代码时使用。 每个包都应包含一段包注释，即放置在包子句前的一个块注释。对于包含多个文件的包， 包注释只需出现在其中的任一文件中即可。包注释应在整体上对该包进行介绍，并提供包的相关信息。 它将出现在 godoc 页面中的最上面，并为紧随其后的内容建立详细的文档。 /* Package regexp implements a simple library for regular expressions. The syntax of the regular expressions accepted is: regexp: concatenation { '|' concatenation } concatenation: { closure } closure: term [ '*' | '+' | '?' ] term: '^' '$' '.' character '[' [ '^' ] character-ranges ']' '(' regexp ')' */ package regexp 注释无需进行额外的格式化，如用星号来突出等。生成的输出甚至可能无法以等宽字体显示， 因此不要依赖于空格对齐，godoc 会像 gofmt 那样处理好这一切。 注释是不会被解析的纯文本，因此像 HTML 或其它类似于 这样 的东西将按照 原样 输出，因此不应使用它们。godoc 所做的调整， 就是将已缩进的文本以等宽字体显示，来适应对应的程序片段。 在包中，任何顶级声明前面的注释都将作为该声明的文档注释。 在程序中，每个可导出（首字母大写）的名称都应该有文档注释。 若注释总是以名称开头，godoc 的输出就能通过 grep 变得更加有用。假如你记不住 “Compile” 这个名称，而又在找正则表达式的解析函数， 那就可以运行 $ godoc regexp | grep parse 若包中的所有文档注释都以 “此函数…” 开头，grep 就无法帮你记住此名称。 但由于每个包的文档注释都以其名称开头，你就能看到这样的内容，它能显示你正在寻找的词语。 Go的声明语法允许成组声明。单个文档注释应介绍一组相关的常量或变量。 由于是整体声明，这种注释往往较为笼统。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:3:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"命名 当一个包被导入后，包名就会成了内容的访问器。在之后，被导入的包就能通过 bytes.Buffer 来引用了。 若所有人都能以相同的名称来引用其内容，这将大有裨益，因此，包应当有个恰当的名称：其名称应该简洁明了而易于理解。按照惯例， 包应当以小写的单个单词来命名，且不应使用下划线或驼峰记法。err 的命名就是出于简短考虑的，因为任何使用该包的人都会键入该名称。 不必担心引用次序的冲突。包名就是导入时所需的唯一默认名称， 它并不需要在所有源码中保持唯一，即便在少数发生冲突的情况下， 也可为导入的包选择一个别名来局部使用。 无论如何，通过文件名来判定使用的包，都是不会产生混淆的。 包的导入者可通过包名来引用其内容，因此包中的可导出名称可以此来避免冲突。 （请勿使用 import . 记法，它可以简化必须在被测试包外运行的测试， 除此之外应尽量避免使用。）例如，bufio 包中的缓存读取器类型叫做 Reader 而非 BufReader，因为用户将它看做 bufio.Reader，这是个清楚而简洁的名称。 此外，由于被导入的项总是通过它们的包名来确定，因此 bufio.Reader 不会与 io.Reader 发生冲突。同样，用于创建 ring.Ring 的新实例的函数（这就是 Go 中的构造函数）一般会称之为 NewRing，但由于 Ring 是该包所导出的唯一类型，且该包也叫 ring，因此它可以只叫做 New，它跟在包的后面，就像 ring.New。使用包结构可以帮助你选择好的名称。 另一个简短的例子是 once.Do，once.Do(setup) 表述足够清晰， 使用 once.DoOrWaitUntilDone(setup) 完全就是画蛇添足。 长命名并不会使其更具可读性。一份有用的说明文档通常比额外的长名更有价值。 Go 并不对获取器（getter）和设置器（setter）提供自动支持。 你应当自己提供获取器和设置器，通常很值得这样做，但若要将 Get 放到获取器的名字中，既不符合习惯，也没有必要。若你有个名为 owner （小写，未导出）的字段，其获取器应当名为 Owner（大写，可导出）而非 GetOwner。大写字母即为可导出的这种规定为区分方法和字段提供了便利。 若要提供设置器方法，SetOwner 是个不错的选择。两个命名看起来都很合理： owner := obj.Owner() if owner != user { obj.SetOwner(user) } 接口名： 按照约定，只包含一个方法的接口应当以该方法的名称加上 - er 后缀来命名，如 Reader、Writer、 Formatter、CloseNotifier 等。 诸如此类的命名有很多，遵循它们及其代表的函数名会让事情变得简单。 Read、Write、Close、Flush、 String 等都具有典型的签名和意义。为避免冲突，请不要用这些名称为你的方法命名， 除非你明确知道它们的签名和意义相同。反之，若你的类型实现了的方法， 与一个众所周知的类型的方法拥有相同的含义，那就使用相同的命名。 请将字符串转换方法命名为 String 而非 ToString。 最后，Go 中约定使用驼峰记法 MixedCaps 或 mixedCaps 而非下划线的方式来对多单词名称进行命名。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:4:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"分号 和 C 一样，Go 的正式语法使用分号来结束语句；和 C 不同的是，这些分号并不在源码中出现。 取而代之，词法分析器会使用一条简单的规则来自动插入分号，因此源码中基本就不用分号了。 规则是这样的：若在新行前的最后一个标记为标识符（包括 int 和 float64 这类的单词）、数值或字符串常量之类的基本字面或以下标记之一 break continue fallthrough return ++ -- ) } 则词法分析将始终在该标记后面插入分号。这点可以概括为： “如果新行前的最后一个标记可以结束该段语句，则插入分号”。 分号也可在闭合的大括号之前直接省略，因此像 go func() { for { dst \u003c- \u003c-src } }() 这样的语句无需分号。通常Go程序只在诸如 for 循环子句这样的地方使用分号， 以此来将初始化器、条件及增量元素分开。如果你在一行中写多个语句，也需要用分号隔开。 警告：无论如何，你都不应将一个控制结构（if、for、switch 或 select）的左大括号放在下一行。如果这样做，就会在大括号前面插入一个分号，这可能引起不需要的效果。 你应该这样写 if i \u003c f() { g() } 而不是 if i \u003c f() // wrong! { // wrong! g() } ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:5:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"控制结构 Go 中的结构控制与 C 有许多相似之处，但其不同之处才是独到之处。 Go 不再使用 do 或 while 循环，只有一个更通用的 for；switch 要更灵活一点；if 和 switch 像 for 一样可接受可选的初始化语句； 此外，还有一个包含类型选择和多路通信复用器的新控制结构：select。 其语法也有些许不同：没有圆括号，而其主体必须始终使用大括号括住。 由于 if 和 switch 可接受初始化语句， 因此用它们来设置局部变量十分常见。 在 Go 的库中，你会发现若 if 语句不会执行到下一条语句时，亦即其执行体 以 break、continue、goto 或 return 结束时，不必要的 else 会被省略。由于出错时将以 return 结束， 之后的代码也就无需 else 了。 重复声明： 在满足下列条件时，已被声明的变量 v 可出现在:= 声明中： 本次声明与已声明的 v 处于同一作用域中（若 v 已在外层作用域中声明过，则此次声明会创建一个新的变量 §）， 在初始化中与其类型相应的值才能赋予 v，且在此次声明中至少另有一个变量是新声明的。 Go 的 for 循环类似于 C，但却不尽相同。它统一了 for 和 while，不再有 do-while 了。它有三种形式，但只有一种需要分号。 若你想遍历数组、切片、字符串或者映射，或从信道中读取消息， range 子句能够帮你轻松实现循环。 对于字符串，range 能够提供更多便利。它能通过解析 UTF-8， 将每个独立的 Unicode 码点分离出来。错误的编码将占用一个字节，并以符文 U+FFFD 来代替。 （名称 “符文” 和内建类型 rune 是 Go 对单个 Unicode 码点的称谓。 详情见语言规范）。循环 for pos, char := range \"日本 \\ x80 語\" { // \\x80 is an illegal UTF-8 encoding fmt.Printf(\"character %#U starts at byte position %d\\n\", char, pos) } character U+65E5 '日' starts at byte position 0 character U+672C '本' starts at byte position 3 character U+FFFD '�' starts at byte position 6 character U+8A9E '語' starts at byte position 7 最后，Go 没有逗号操作符，而 ++ 和 – 为语句而非表达式。 因此，若你想要在 for 中使用多个变量，应采用平行赋值的方式 （因为它会拒绝 ++ 和 –）. Go 的 switch 比 C 的更通用。其表达式无需为常量或整数，case 语句会自上而下逐一进行求值直到匹配为止。若 switch 后面没有表达式，它将匹配 true，因此，我们可以将 if-else-if-else 链写成一个 switch，这也更符合 Go 的风格。 func unhex(c byte) byte { switch { case '0' \u003c= c \u0026\u0026 c \u003c= '9': return c - '0' case 'a' \u003c= c \u0026\u0026 c \u003c= 'f': return c - 'a' + 10 case 'A' \u003c= c \u0026\u0026 c \u003c= 'F': return c - 'A' + 10 } return 0 } switch 并不会自动下溯(fallthrough)，但 case 可通过逗号分隔来列举相同的处理条件。 func shouldEscape(c byte) bool { switch c { case ' ', '?', '\u0026', '=', '#', '+', '%': return true } return false } 尽管它们在 Go 中的用法和其它类 C 语言差不多，但 break 语句可以使 switch 提前终止。不仅是 switch， 有时候也必须打破层层的循环。在 Go 中，我们只需将标签放置到循环外，然后 “蹦” 到那里即可。下面的例子展示了二者的用法。 Loop: for n := 0; n \u003c len(src); n += size { switch { case src[n] \u003c sizeOne: if validateOnly { break } size = 1 update(src[n]) case src[n] \u003c sizeTwo: if n+1 \u003e= len(src) { err = errShortInput break Loop } if validateOnly { break } size = 2 update(src[n] + src[n+1]\u003c\u003cshift) } } 当然，continue 语句也能接受一个可选的标签，不过它只能在循环中使用。 类型选择 var t interface{} t = functionOfSomeType() switch t := t.(type) { default: fmt.Printf(\"unexpected type %T\", t) // %T prints whatever type t has case bool: fmt.Printf(\"boolean %t\\n\", t) // t has type bool case int: fmt.Printf(\"integer %d\\n\", t) // t has type int case *bool: fmt.Printf(\"pointer to boolean %t\\n\", *t) // t has type *bool case *int: fmt.Printf(\"pointer to integer %d\\n\", *t) // t has type *int } switch 也可用于判断接口变量的动态类型。如 类型选择 通过圆括号中的关键字 type 使用类型断言语法。若 switch 在表达式中声明了一个变量，那么该变量的每个子句中都将有该变量对应的类型。在这些 case 中重用一个名字也是符合语义的，实际上是在每个 case 里声明了一个不同类型但同名的新变量。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:6:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"函数 Go 与众不同的特性之一就是函数和方法可返回多个值。这种形式可以改善 C 中一些笨拙的习惯： 将错误值返回（例如用 -1 表示 EOF）和修改通过地址传入的实参。 在 C 中，写入操作发生的错误会用一个负数标记，而错误码会隐藏在某个不确定的位置。 而在 Go 中，Write 会返回写入的字节数以及一个错误： “是的，您写入了一些字节，但并未全部写入，因为设备已满”。 在 os 包中，File.Write 的签名为： func (file *File) Write(b []byte) (n int, err error) 正如文档所述，它返回写入的字节数，并在 n != len(b) 时返回一个非 nil 的 error 错误值。 这是一种常见的编码风格，更多示例见错误处理一节。 我们可以采用一种简单的方法。来避免为模拟引用参数而传入指针。 以下简单的函数可从字节数组中的特定位置获取其值，并返回该数值和下一个位置。 func nextInt(b []byte, i int) (int, int) { for ; i \u003c len(b) \u0026\u0026 !isDigit(b[i]); i++ { } x := 0 for ; i \u003c len(b) \u0026\u0026 isDigit(b[i]); i++ { x = x*10 + int(b[i]) - '0' } return x, i } 你可以像下面这样，通过它扫描输入的切片 b 来获取数字。 for i := 0; i \u003c len(b); { x, i = nextInt(b, i) fmt.Println(x) } 可命名结果形参 Go 函数的返回值或结果 “形参” 可被命名，并作为常规变量使用，就像传入的形参一样。 命名后，一旦该函数开始执行，它们就会被初始化为与其类型相应的零值； 若该函数执行了一条不带实参的 return 语句，则结果形参的当前值将被返回。 此名称不是强制性的，但它们能使代码更加简短清晰：它们就是文档。若我们命名了 nextInt 的结果，那么它返回的 int 就值如其意了。 func nextInt(b []byte, pos int) (value, nextPos int) { 由于被命名的结果已经初始化，且已经关联至无参数的返回，它们就能让代码简单而清晰。 下面的 io.ReadFull 就是个很好的例子： func ReadFull(r Reader, buf []byte) (n int, err error) { for len(buf) \u003e 0 \u0026\u0026 err == nil { var nr int nr, err = r.Read(buf) n += nr buf = buf[nr:] } return } Defer Go 的 defer 语句用于预设一个函数调用（即推迟执行函数）， 该函数会在执行 defer 的函数返回之前立即执行。它显得非比寻常， 但却是处理一些事情的有效方式，例如无论以何种路径返回，都必须释放资源的函数。 典型的例子就是解锁互斥和关闭文件。 // Contents returns the file's contents as a string. func Contents(filename string) (string, error) { f, err := os.Open(filename) if err != nil { return \"\", err } defer f.Close() // f.Close will run when we're finished. var result []byte buf := make([]byte, 100) for { n, err := f.Read(buf[0:]) result = append(result, buf[0:n]...) // append is discussed later. if err != nil { if err == io.EOF { break } return \"\", err // f will be closed if we return here. } } return string(result), nil // f will be closed if we return here. } 推迟诸如 Close 之类的函数调用有两点好处：第一， 它能确保你不会忘记关闭文件。如果你以后又为该函数添加了新的返回路径时， 这种情况往往就会发生。第二，它意味着 “关闭” 离 “打开” 很近， 这总比将它放在函数结尾处要清晰明了。 被推迟函数的实参（如果该函数为方法则还包括接收者）在推迟执行时就会被求值， 而不是在调用执行时才求值。这样不仅无需担心变量值在函数执行时被改变， 同时还意味着单个被推迟的调用可推迟多个函数的执行。下面是个简单的例子。 for i := 0; i \u003c 5; i++ { defer fmt.Printf(\"%d \", i) } 被推迟的函数按照后进先出（LIFO）的顺序执行，因此以上代码在函数返回时会打印 4 3 2 1 0。一个更具实际意义的例子是通过一种简单的方法， 用程序来跟踪函数的执行。我们可以编写一对简单的跟踪例程： func trace(s string) { fmt.Println(\"entering:\", s) } func untrace(s string) { fmt.Println(\"leaving:\", s) } // Use them like this: func a() { trace(\"a\") defer untrace(\"a\") // do something.... } 我们可以充分利用这个特点，即被推迟函数的实参在 defer 执行时就会被求值。 跟踪例程可针对反跟踪例程设置实参。以下例子： func trace(s string) string { fmt.Println(\"entering:\", s) return s } func un(s string) { fmt.Println(\"leaving:\", s) } func a() { defer un(trace(\"a\")) fmt.Println(\"in a\") } func b() { defer un(trace(\"b\")) fmt.Println(\"in b\") a() } func main() { b() } entering: b in b entering: a in a leaving: a leaving: b 对于习惯其它语言中块级资源管理的程序员，defer 似乎有点怪异， 但它最有趣而强大的应用恰恰来自于其基于函数而非块的特点。在 panic 和 recover 这两节中，我们将看到关于它可能性的其它例子。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:7:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"数据 new分配 Go 提供了两种分配原语，即内建函数 new 和 make。 它们所做的事情不同，所应用的类型也不同。它们可能会引起混淆，但规则却很简单。 让我们先来看看 new。这是个用来分配内存的内建函数， 但与其它语言中的同名函数不同，它不会初始化内存，只会将内存置零。 也就是说，new(T) 会为类型为 T 的新项分配已置零的内存空间， 并返回它的地址，也就是一个类型为 *T 的值。用 Go 的术语来说，它返回一个指针， 该指针指向新分配的，类型为 T 的零值。 既然 new 返回的内存已置零，那么当你设计数据结构时， 每种类型的零值就不必进一步初始化了，这意味着该数据结构的使用者只需用 new 创建一个新的对象就能正常工作。例如，bytes.Buffer 的文档中提到 “零值的 Buffer 就是已准备就绪的缓冲区。\" 同样，sync.Mutex 并没有显式的构造函数或 Init 方法， 而是零值的 sync.Mutex 就已经被定义为已解锁的互斥锁了。 “零值属性” 是传递性的。考虑以下类型声明。 type SyncedBuffer struct { lock sync.Mutex buffer bytes.Buffer } SyncedBuffer 类型的值也是在声明时就分配好内存就绪了。后续代码中， p 和 v 无需进一步处理即可正确工作。 p := new(SyncedBuffer) // type *SyncedBuffer var v SyncedBuffer // type SyncedBuffer 构造函数和复合字面量 有时零值还不够好，这时就需要一个初始化构造函数，如来自 os 包中的这段代码所示。 func NewFile(fd int, name string) *File { if fd \u003c 0 { return nil } f := new(File) f.fd = fd f.name = name f.dirinfo = nil f.nepipe = 0 return f } 这里显得代码过于冗长。我们可通过复合字面量来简化它， 该表达式在每次求值时都会创建新的实例。 func NewFile(fd int, name string) *File { if fd \u003c 0 { return nil } f := File{fd, name, nil, 0} return \u0026f } 请注意，返回一个局部变量的地址完全没有问题，这点与 C 不同。该局部变量对应的数据 在函数返回后依然有效。实际上，每当获取一个复合字面量的地址时，都将为一个新的实例分配内存， 因此我们可以将上面的最后两行代码合并： return \u0026File{fd, name, nil, 0} 复合字面量的字段必须按顺序全部列出。但如果以 字段: 值 对的形式明确地标出元素，初始化字段时就可以按任何顺序出现，未给出的字段值将赋予零值，推荐列出键值对。 因此，我们可以用如下形式： return \u0026File{fd: fd, name: name} 少数情况下，若复合字面量不包括任何字段，它将创建该类型的零值。表达式 new(File) 和 \u0026File{} 是等价的。 复合字面量同样可用于创建数组、切片以及映射，字段标签是索引还是映射键则视情况而定。 在下例初始化过程中，无论 Enone、Eio 和 Einval 的值是什么，只要它们的标签不同就行。 make分配 再回到内存分配上来。内建函数 make(T, args) 的目的不同于 new(T)。它只用于创建切片、映射和信道，并返回类型为 T（而非 *T）的一个已初始化 （而非置零）的值。 出现这种差异的原因在于，这三种类型本质上为引用数据类型，它们在使用前必须初始化。 例如，切片是一个具有三项内容的描述符，包含一个指向（数组内部）数据的指针、长度以及容量， 在这三项被初始化之前，该切片为 nil。对于切片、映射和信道，make 用于初始化其内部的数据结构并准备好将要使用的值。例如， make([]int, 10, 100) 会分配一个具有 100 个 int 的数组空间，接着创建一个长度为 10， 容量为 100 并指向该数组中前 10 个元素的切片结构。（生成切片时，其容量可以省略，更多信息见切片一节。） 与此相反，new([]int) 会返回一个指向新分配的，已置零的切片结构， 即一个指向 nil 切片值的指针。 下面的例子阐明了 new 和 make 之间的区别： var p *[]int = new([]int) // allocates slice structure; *p == nil; rarely useful var v []int = make([]int, 100) // the slice v now refers to a new array of 100 ints // Unnecessarily complex: var p *[]int = new([]int) *p = make([]int, 100, 100) // Idiomatic: v := make([]int, 100) 请记住，make 只适用于映射、切片和信道且不返回指针。若要获得明确的指针， 请使用 new 分配内存或显式地获取一个变量的地址。 数组 在详细规划内存布局时，数组是非常有用的，有时还能避免过多的内存分配， 但它们主要用作切片的构件。这是下一节的主题了，不过要先说上几句来为它做铺垫。 以下为数组在 Go 和 C 中的主要区别。在 Go 中， 数组是值。将一个数组赋予另一个数组会复制其所有元素。 特别地，若将某个数组传入某个函数，它将接收到该数组的一份副本而非指针。 数组的大小是其类型的一部分。类型 [10]int 和 [20]int 是不同的。 数组为值的属性很有用，但代价高昂；若你想要 C 那样的行为和效率，你可以传递一个指向该数组的指针。 func Sum(a *[3]float64) (sum float64) { for _, v := range *a { sum += v } return } array := [...]float64{7.0, 8.5, 9.1} x := Sum(\u0026array) // Note the explicit address-of operator 但这并不是 Go 的习惯用法，切片才是。 切片 切片通过对数组进行封装，为数据序列提供了更通用、强大而方便的接口。 除了矩阵变换这类需要明确维度的情况外，Go 中的大部分数组编程都是通过切片来完成的。 切片保存了对底层数组的引用，若你将某个切片赋予另一个切片，它们会引用同一个数组。 若某个函数将一个切片作为参数传入，则它对该切片元素的修改对调用者而言同样可见， 这可以理解为传递了底层数组的指针。因此，Read 函数可接受一个切片实参 而非一个指针和一个计数；切片的长度决定了可读取数据的上限。以下为 os 包中 File 类型的 Read 方法签名: func (file *File) Read(buf []byte) (n int, err error) 该方法返回读取的字节数和一个错误值（若有的话）。若要从更大的缓冲区 b 中读取前 32 个字节，只需对其进行切片即可。 n, err := f.Read(buf[0:32]) 这种切片的方法常用且高效。若不谈效率，以下片段同样能读取该缓冲区的前 32 个字节。 var n int var err error for i := 0; i \u003c 32; i++ { nbytes, e := f.Read(buf[i:i+1]) // Read one byte. if nbytes == 0 || e != nil { err = e break } n += nbytes } 只要切片不超出底层数组的限制，它的长度就是可变的，只需将它赋予其自身的切片即可。 切片的容量可通过内建函数 cap 获得，它将给出该切片可取得的最大长度。 以下是将数据追加到切片的函数。若数据超出其容量，则会重新分配该切片。返回值即为所得的切片。 该函数中所使用的 len 和 cap 在应用于 nil 切片时是合法的，它会返回 0. func Append(slice, data[]byte) []byte { l := len(slice) if l + len(data) \u003e cap(slice) { // reallocate // Allocate double what's needed, for future growth. newSlice := make([]byte, (l+len(data))*2) // The copy function is predeclared and works for any slice type. copy(newSlice, slice) slice = newSlice } slice = slice[0:l+len(data)] for i, c := range data { slice[l+i] = c } return slice } 最终我们必须返回切片，因为尽管 Append 可修改 slice 的元素，但切片自身（其运行时数据结构包含指针、长度和容量）是通过值传递的。 向切片追加东西的想法非常有用，因此有专门的内建函数 append。 要理解该函数的设计，我们还需要一些额外的信息，我们将稍后再介绍它。 二维切片","date":"2022-04-30 09:22:50","objectID":"/effective_go/:8:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"初始化 尽管从表面上看，Go 的初始化过程与 C 或 C++ 相比并无太大差别，但它确实更为强大。 在初始化过程中，不仅可以构建复杂的结构，还能正确处理不同包对象间的初始化顺序。 常量 Go 中的常量就是不变量。它们在编译时创建，即便它们可能是函数中定义的局部变量。 常量只能是数字、字符（符文）、字符串或布尔值。由于编译时的限制， 定义它们的表达式必须也是可被编译器求值的常量表达式。例如 1«3 就是一个常量表达式，而 math.Sin(math.Pi/4) 则不是，因为对 math.Sin 的函数调用在运行时才会发生。 在 Go 中，枚举常量使用枚举器 iota 创建。由于 iota 可为表达式的一部分，而表达式可以被隐式地重复，这样也就更容易构建复杂的值的集合了。 type ByteSize float64 const ( // 通过赋予空白标识符来忽略第一个值 _ = iota // ignore first value by assigning to blank identifier KB ByteSize = 1 \u003c\u003c (10 * iota) MB GB TB PB EB ZB YB ) 由于可将 String 之类的方法附加在用户定义的类型上， 因此它就为打印时自动格式化任意值提供了可能性，即便是作为一个通用类型的一部分。 尽管你常常会看到这种技术应用于结构体，但它对于像 ByteSize 之类的浮点数标量等类型也是有用的。 func (b ByteSize) String() string { switch { case b \u003e= YB: return fmt.Sprintf(\"%.2fYB\", b/YB) case b \u003e= ZB: return fmt.Sprintf(\"%.2fZB\", b/ZB) case b \u003e= EB: return fmt.Sprintf(\"%.2fEB\", b/EB) case b \u003e= PB: return fmt.Sprintf(\"%.2fPB\", b/PB) case b \u003e= TB: return fmt.Sprintf(\"%.2fTB\", b/TB) case b \u003e= GB: return fmt.Sprintf(\"%.2fGB\", b/GB) case b \u003e= MB: return fmt.Sprintf(\"%.2fMB\", b/MB) case b \u003e= KB: return fmt.Sprintf(\"%.2fKB\", b/KB) } return fmt.Sprintf(\"%.2fB\", b) } 表达式 YB 会打印出 1.00YB，而 ByteSize(1e13) 则会打印出 9.09TB。 在这里用 Sprintf 实现 ByteSize 的 String 方法很安全（不会无限递归），这倒不是因为类型转换，而是它以 %f 调用了 Sprintf，它并不是一种字符串格式：Sprintf 只会在它需要字符串时才调用 String 方法，而 %f 需要一个浮点数值。 变量 变量的初始化与常量类似，但其初始值也可以是在运行时才被计算的一般表达式。 var ( home = os.Getenv(\"HOME\") user = os.Getenv(\"USER\") gopath = os.Getenv(\"GOPATH\") ) init 函数 最后，每个源文件都可以通过定义自己的无参数 init 函数来设置一些必要的状态。 （其实每个文件都可以拥有多个 init 函数。）而它的结束就意味着初始化结束： 只有该包中的所有变量声明都通过它们的初始化器求值后 init 才会被调用， 而那些 init 只有在所有已导入的包都被初始化后才会被求值。 除了那些不能被表示成声明的初始化外，init 函数还常被用在程序真正开始执行前，检验或校正程序的状态。 func init() { if user == \"\" { log.Fatal(\"$USER not set\") } if home == \"\" { home = \"/home/\" + user } if gopath == \"\" { gopath = home + \"/go\" } // gopath may be overridden by --gopath flag on command line. flag.StringVar(\u0026gopath, \"gopath\", gopath, \"override default GOPATH\") } ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:9:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"方法 指针vs值 正如 ByteSize 那样，我们可以为任何已命名的类型（除了指针或接口）定义方法； 接收者可不必为结构体。 在之前讨论切片时，我们编写了一个 Append 函数。 我们也可将其定义为切片的方法。为此，我们首先要声明一个已命名的类型来绑定该方法， 然后使该方法的接收者成为该类型的值。 type ByteSlice []byte func (slice ByteSlice) Append(data []byte) []byte { // Body exactly the same as above } 我们仍然需要该方法返回更新后的切片。为了消除这种不便，我们可通过重新定义该方法， 将一个指向 ByteSlice 的指针作为该方法的接收者， 这样该方法就能重写调用者提供的切片了。 func (p *ByteSlice) Append(data []byte) { slice := *p // Body as above, without the return. *p = slice } 其实我们做得更好。若我们将函数修改为与标准 Write 类似的方法，就像这样， func (p *ByteSlice) Write(data []byte) (n int, err error) { slice := *p // Again as above. *p = slice return len(data), nil } 那么类型 *ByteSlice 就满足了标准的 io.Writer 接口，这将非常实用。 例如，我们可以通过打印将内容写入。 var b ByteSlice fmt.Fprintf(\u0026b, \"This hour has %d days\\n\", 7) 我们将 ByteSlice 的地址传入，因为只有 *ByteSlice 才满足 io.Writer。以指针或值为接收者的区别在于：值方法可通过指针和值调用， 而指针方法只能通过指针来调用。 之所以会有这条规则是因为指针方法可以修改接收者；通过值调用它们会导致方法接收到该值的副本， 因此任何修改都将被丢弃，因此该语言不允许这种错误。不过有个方便的例外：若该值是可寻址的， 那么该语言就会自动插入取址操作符来对付一般的通过值调用的指针方法。在我们的例子中，变量 b 是可寻址的，因此我们只需通过 b.Write 来调用它的 Write 方法，编译器会将它重写为 (\u0026b).Write。 顺便一提，在字节切片上使用 Write 的想法已被 bytes.Buffer 所实现。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:10:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"接口与其他类型 接口 Go 中的接口为指定对象的行为提供了一种方法：如果某样东西可以完成这个， 那么它就可以用在这里。我们已经见过许多简单的示例了；通过实现 String 方法，我们可以自定义打印函数，而通过 Write 方法，Fprintf 则能对任何对象产生输出。在 Go 代码中， 仅包含一两种方法的接口很常见，且其名称通常来自于实现它的方法， 如 io.Writer 就是实现了 Write 的一类对象。 每种类型都能实现多个接口。例如一个实现了 sort.Interface 接口的集合就可通过 sort 包中的例程进行排序。该接口包括 Len()、Less(i, j int) bool 以及 Swap(i, j int)，另外，该集合仍然可以有一个自定义的格式化器。 以下特意构建的例子 Sequence 就同时满足这两种情况。 type Sequence []int // Methods required by sort.Interface. // sort.Interface 所需的方法。 func (s Sequence) Len() int { return len(s) } func (s Sequence) Less(i, j int) bool { return s[i] \u003c s[j] } func (s Sequence) Swap(i, j int) { s[i], s[j] = s[j], s[i] } // Method for printing - sorts the elements before printing. // 用于打印的方法 - 在打印前对元素进行排序。 func (s Sequence) String() string { sort.Sort(s) str := \"[\" for i, elem := range s { if i \u003e 0 { str += \" \" } str += fmt.Sprint(elem) } return str + \"]\" } 类型转换 Sequence 的 String 方法重新实现了 Sprint 为切片实现的功能。若我们在调用 Sprint 之前将 Sequence 转换为纯粹的 []int，就能共享已实现的功能。 func (s Sequence) String() string { sort.Sort(s) return fmt.Sprint([]int(s)) } 该方法是通过类型转换技术，在 String 方法中安全调用 Sprintf 的另个一例子。若我们忽略类型名的话，这两种类型（Sequence 和 []int）其实是相同的，因此在二者之间进行转换是合法的。 转换过程并不会创建新值，它只是暂时让现有的值看起来有个新类型而已。 （还有些合法转换则会创建新值，如从整数转换为浮点数等。） 在 Go 程序中，为访问不同的方法集而进行类型转换的情况非常常见。 例如，我们可使用现有的 sort.IntSlice 类型来简化整个示例： type Sequence []int // Method for printing - sorts the elements before printing func (s Sequence) String() string { sort.IntSlice(s).Sort() return fmt.Sprint([]int(s)) } 现在，不必让 Sequence 实现多个接口（排序和打印）， 我们可通过将数据条目转换为多种类型（Sequence、sort.IntSlice 和 []int）来使用相应的功能，每次转换都完成一部分工作。 这在实践中虽然有些不同寻常，但往往却很有效。 接口转换与类型断言 类型选择 是类型转换的一种形式：它接受一个接口，在选择 （switch）中根据其判断选择对应的情况（case）， 并在某种意义上将其转换为该种类型。以下代码为 fmt.Printf 通过类型选择将值转换为字符串的简化版。若它已经为字符串，我们需要该接口中实际的字符串值； 若它有 String 方法，我们则需要调用该方法所得的结果。 type Stringer interface { String() string } var value interface{} // Value provided by caller. switch str := value.(type) { case string: return str case Stringer: return str.String() } 第一种情况获取具体的值，第二种将该接口转换为另一个接口。这种方式对于混合类型来说非常完美。 若我们只关心一种类型呢？若我们知道该值拥有一个 string 而想要提取它呢？ 只需一种情况的类型选择就行，但它需要类型断言。类型断言接受一个接口值， 并从中提取指定的明确类型的值。其语法借鉴自类型选择开头的子句，但它需要一个明确的类型， 而非 type 关键字： value.(typeName) 而其结果则是拥有静态类型 typeName 的新值。该类型必须为该接口所拥有的具体类型， 或者该值可转换成的第二种接口类型。要提取我们知道在该值中的字符串，可以这样： str := value.(string) 但若它所转换的值中不包含字符串，该程序就会以运行时错误崩溃。为避免这种情况， 需使用 “逗号, ok” 惯用法来测试它能安全地判断该值是否为字符串： str, ok := value.(string) if ok { fmt.Printf(\"string value is: %q\\n\", str) } else { fmt.Printf(\"value is not a string\\n\") } 若类型断言失败，str 将继续存在且为字符串类型，但它将拥有零值，即空字符串。 作为对这种能力的说明，这里有个 if-else 语句，它等价于本节开头的类型选择。 if str, ok := value.(string); ok { return str } else if str, ok := value.(Stringer); ok { return str.String() } 通用性 若某种现有的类型仅实现了一个接口，且除此之外并无可导出的方法，则该类型本身就无需导出。 仅导出该接口能让我们更专注于其行为而非实现，其它属性不同的实现则能反映该原始类型的行为。 这也能够避免为每个通用接口的实例重复编写文档。 在这种情况下，构造函数应当返回一个接口值而非实现的类型。例如在 hash 库中，crc32.NewIEEE 和 adler32.New 都返回接口类型 hash.Hash32。要在 Go 程序中用 Adler-32 算法替代 CRC-32， 只需修改构造函数调用即可，其余代码则不受算法改变的影响。 同样的方式能将 crypto 包中多种联系在一起的流密码算法与块密码算法分开。 crypto/cipher 包中的 Block 接口指定了块密码算法的行为， 它为单独的数据块提供加密。接着，和 bufio 包类似，任何实现了该接口的密码包都能被用于构造以 Stream 为接口表示的流密码，而无需知道块加密的细节。 crypto/cipher 接口看其来就像这样： type Block interface { BlockSize() int Encrypt(src, dst []byte) Decrypt(src, dst []byte) } type Stream interface { XORKeyStream(dst, src []byte) } 这是计数器模式 CTR 流的定义，它将块加密改为流加密，注意块加密的细节已被抽象化了。 // NewCTR returns a Stream that encrypts/decrypts using the given Block in // counter mode. The length of iv must be the same as the Block's block size. func NewCTR(block Block, iv []byte) Stream NewCTR 的应用并不仅限于特定的加密算法和数据源，它适用于任何对 Block 接口和 Stream 的实现。因为它们返回接口值， 所以用其它加密模式来代替 CTR 只需做局部的更改。构造函数的调用过程必须被修改， 但由于其周围的代码只能将它看做 Stream，因此它们不会注意到其中的区别。 接口和方法 由于几乎任何类型都能添加方法，因此几乎任何类型都能满足一个接口。一个很直观的例子就是 http 包中定义的 Handler 接口。任何实现了 Handler 的对象都能够处理 HTTP 请求。 type Handler interface { ServeHTTP(ResponseWriter, *Request) } ResponseWriter 接口提供了对方法的访问，这些方法需要响应客户端的请求。 由于这些方法包含了标准的 Write 方法，因此 http.ResponseWriter 可用于任何 io.Writer 适用的场景。Request 结构体包含已解析的客户端请求。 为简单起见，我们假设所有的 HTTP 请求都是 GET 方法，而忽略 POST 方法， 这种简化不会影响处理程序的建立方式。这里有个短小却完整的处理程序实现， 它用","date":"2022-04-30 09:22:50","objectID":"/effective_go/:11:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"空白标识符 我们在 for-range 循环和 映射 中提过几次空白标识符。 空白标识符可被赋予或声明为任何类型的任何值，而其值会被无害地丢弃。它有点像 Unix 中的 /dev/null 文件：它表示只写的值，在需要变量但不需要实际值的地方用作占位符。 我们在前面已经见过它的用法了。 多重赋值中的空白标识符 for range 循环中对空白标识符的用法是一种具体情况，更一般的情况即为多重赋值。 若某次赋值需要匹配多个左值，但其中某个变量不会被程序使用， 那么用空白标识符来代替该变量可避免创建无用的变量，并能清楚地表明该值将被丢弃。 例如，当调用某个函数时，它会返回一个值和一个错误，但只有错误很重要， 那么可使用空白标识符来丢弃无关的值。 if _, err := os.Stat(path); os.IsNotExist(err) { fmt.Printf(\"%s does not exist\\n\", path) } 你偶尔会看见为忽略错误而丢弃错误值的代码，这是种糟糕的实践。请务必检查错误返回， 它们会提供错误的理由。 // Bad! This code will crash if path does not exist. fi, _ := os.Stat(path) if fi.IsDir() { fmt.Printf(\"%s is a directory\\n\", path) } 未使用的导入和变量 若导入某个包或声明某个变量而不使用它就会产生错误。未使用的包会让程序膨胀并拖慢编译速度， 而已初始化但未使用的变量不仅会浪费计算能力，还有可能暗藏着更大的 Bug。 然而在程序开发过程中，经常会产生未使用的导入和变量。虽然以后会用到它们， 但为了完成编译又不得不删除它们才行，这很让人烦恼。空白标识符就能提供一个临时解决方案。 这个写了一半的程序有两个未使用的导入（fmt 和 io）以及一个未使用的变量（fd），因此它不能编译， 但若到目前为止代码还是正确的，我们还是很乐意看到它们的。 package main import ( \"fmt\" \"io\" \"log\" \"os\" ) func main() { fd, err := os.Open(\"test.go\") if err != nil { log.Fatal(err) } // TODO: use fd. } 要让编译器停止关于未使用导入的抱怨，需要空白标识符来引用已导入包中的符号。 同样，将未使用的变量 fd 赋予空白标识符也能关闭未使用变量错误。 该程序的以下版本可以编译。 package main import ( \"fmt\" \"io\" \"log\" \"os\" ) var _ = fmt.Printf // For debugging; delete when done. // 用于调试，结束时删除。 var _ io.Reader // For debugging; delete when done. // 用于调试，结束时删除。 func main() { fd, err := os.Open(\"test.go\") if err != nil { log.Fatal(err) } // TODO: use fd. _ = fd } 按照惯例，我们应在导入并加以注释后，再使全局声明导入错误静默，这样可以让它们更易找到， 并作为以后清理它的提醒。 为副作用而导入 像前例中 fmt 或 io 这种未使用的导入总应在最后被使用或移除： 空白赋值会将代码标识为工作正在进行中。但有时导入某个包只是为了其副作用， 而没有任何明确的使用。例如，在 net/http/pprof 包的 init 函数中记录了 HTTP 处理程序的调试信息。它有个可导出的 API， 但大部分客户端只需要该处理程序的记录和通过 Web 页面访问数据。欲导入一个只使用其副作用的包， 只需将该包重命名为空白标识符： import _ \"net/http/pprof\" 这种导入格式能明确表示该包是为其副作用而导入的，因为没有其它使用该包的可能： 在此文件中，它没有名字。（若它有名字而我们没有使用，编译器就会拒绝该程序。） 接口检查 就像我们在前面 接口 中讨论的那样， 一个类型无需显式地声明它实现了某个接口。取而代之，该类型只要实现了某个接口的方法， 其实就实现了该接口。在实践中，大部分接口转换都是静态的，因此会在编译时检测。 例如，将一个 *os.File 传入一个接收 io.Reader 的函数将不会被编译， 除非 *os.File 实现了 io.Reader 接口。 尽管如此，有些接口检查会在运行时进行。例如，encoding/json 包定义了一个 Marshaler 接口。当 JSON 编码器接收到一个实现了该接口的值，那么该编码器就会调用该值的编组方法， 将其转换为 JSON，而非进行标准的类型转换。 编码器在运行时通过 类型断言 检查其属性，就像这样： m, ok := val.(json.Marshaler) 若只需要判断某个类型是否是实现了某个接口，而不需要实际使用接口本身 （可能是错误检查部分），就使用空白标识符来忽略类型断言的值： if _, ok := val.(json.Marshaler); ok { fmt.Printf(\"value %v of type %T implements json.Marshaler\\n\", val, val) } 当需要确保某个包中实现的类型一定满足该接口时，就会遇到这种情况。 若某个类型（例如 json.RawMessage） 需要一种定制的 JSON 表现时，它应当实现 json.Marshaler， 不过现在没有静态转换可以让编译器去自动验证它。若该类型通过忽略转换失败来满足该接口， 那么 JSON 编码器仍可工作，但它却不会使用定制的实现。为确保其实现正确， 可在该包中用空白标识符声明一个全局变量： var _ json.Marshaler = (*RawMessage)(nil) 在此声明中，我们调用了一个 *RawMessage 转换并将其赋予了 Marshaler，以此来要求 *RawMessage 实现 Marshaler，这时其属性就会在编译时被检测。 若 json.Marshaler 接口被更改，此包将无法通过编译， 而我们则会注意到它需要更新。 在这种结构中出现空白标识符，即表示该声明的存在只是为了类型检查。 不过请不要为满足接口就将它用于任何类型。作为约定， 仅当代码中不存在静态类型转换时才能这种声明，毕竟这是种罕见的情况。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:12:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"内嵌 Go 并不提供典型的，类型驱动的子类化概念，但通过将类型内嵌到结构体或接口中， 它就能 “借鉴” 部分实现。 接口内嵌非常简单。我们之前提到过 io.Reader 和 io.Writer 接口，这里是它们的定义。 type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } io 包也导出了一些其它接口，以此来阐明对象所需实现的方法。 例如 io.ReadWriter 就是个包含 Read 和 Write 的接口。我们可以通过显示地列出这两个方法来指明 io.ReadWriter， 但通过将这两个接口内嵌到新的接口中显然更容易且更具启发性，就像这样： // ReadWriter is the interface that combines the Reader and Writer interfaces. type ReadWriter interface { Reader Writer } 正如它看起来那样：ReadWriter 能够做任何 Reader 和 Writer 可以做到的事情，它是内嵌接口的联合体 （它们必须是不相交的方法集）。只有接口能被嵌入到接口中。 同样的基本想法可以应用在结构体中，但其意义更加深远。bufio 包中有 bufio.Reader 和 bufio.Writer 这两个结构体类型， 它们每一个都实现了与 io 包中相同意义的接口。此外，bufio 还通过结合 reader/writer 并将其内嵌到结构体中，实现了带缓冲的 reader/writer：它在结构体中列出了这些类型，但并未给予它们字段名。 // ReadWriter stores pointers to a Reader and a Writer. // It implements io.ReadWriter. type ReadWriter struct { *Reader // *bufio.Reader *Writer // *bufio.Writer } 内嵌的元素为指向结构体的指针，当然它们在使用前必须被初始化为指向有效结构体的指针。 ReadWriter 结构体可通过如下方式定义： type ReadWriter struct { reader *Reader writer *Writer } 但为了提升该字段的方法并满足 io 接口，我们同样需要提供转发的方法， 就像这样： func (rw *ReadWriter) Read(p []byte) (n int, err error) { return rw.reader.Read(p) } 而通过直接内嵌结构体，我们就能避免如此繁琐。 内嵌类型的方法可以直接引用，这意味着 bufio.ReadWriter 不仅包括 bufio.Reader 和 bufio.Writer 的方法，它还同时满足下列三个接口： io.Reader、io.Writer 以及 io.ReadWriter。 还有种区分内嵌与子类的重要手段。当内嵌一个类型时，该类型的方法会成为外部类型的方法， 但当它们被调用时，该方法的接收者是内部类型，而非外部的。在我们的例子中，当 bufio.ReadWriter 的 Read 方法被调用时， 它与之前写的转发方法具有同样的效果；接收者是 ReadWriter 的 reader 字段，而非 ReadWriter 本身。 内嵌同样可以提供便利。这个例子展示了一个内嵌字段和一个常规的命名字段。 type Job struct { Command string *log.Logger } Job 类型现在有了 Log、Logf 和 *log.Logger 的其它方法。我们当然可以为 Logger 提供一个字段名，但完全不必这么做。现在，一旦初始化后，我们就能记录 Job 了： job.Log(\"starting now...\") Logger 是 Job 结构体的常规字段， 因此我们可在 Job 的构造函数中，通过一般的方式来初始化它，就像这样： func NewJob(command string, logger *log.Logger) *Job { return \u0026Job{command, logger} } 或通过复合字面： job := \u0026Job{command, log.New(os.Stderr, \"Job: \", log.Ldate)} 若我们需要直接引用内嵌字段，可以忽略包限定名，直接将该字段的类型名作为字段名， 就像我们在 ReaderWriter 结构体的 Read 方法中做的那样。 若我们需要访问 Job 类型的变量 job 的 *log.Logger， 可以直接写作 job.Logger。若我们想精炼 Logger 的方法时， 这会非常有用。 func (job *Job) Logf(format string, args ...interface{}) { job.Logger.Logf(\"%q: %s\", job.Command, fmt.Sprintf(format, args...)) } 内嵌类型会引入命名冲突的问题，但解决规则却很简单。首先，字段或方法 X 会隐藏该类型中更深层嵌套的其它项 X。若 log.Logger 包含一个名为 Command 的字段或方法，Job 的 Command 字段会覆盖它。 其次，若相同的嵌套层级上出现同名冲突，通常会产生一个错误。若 Job 结构体中包含名为 Logger 的字段或方法，再将 log.Logger 内嵌到其中的话就会产生错误。然而，若重名永远不会在该类型定义之外的程序中使用，那就不会出错。 这种限定能够在外部嵌套类型发生修改时提供某种保护。 因此，就算添加的字段与另一个子类型中的字段相冲突，只要这两个相同的字段永远不会被使用就没问题。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:13:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"并发 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"通过通信共享内存 并发编程是个很大的论题。但限于篇幅，这里仅讨论一些 Go 特有的东西。 在并发编程中，为实现对共享变量的正确访问需要精确的控制，这在多数环境下都很困难。 Go 语言另辟蹊径，它将共享的值通过信道传递，实际上，多个独立执行的线程从不会主动共享。 在任意给定的时间点，只有一个 goroutine 能够访问该值。数据竞争从设计上就被杜绝了。 为了提倡这种思考方式，我们将它简化为一句口号： 不要通过共享内存来通信，而应通过通信来共享内存。 这种方法意义深远。例如，引用计数通过为整数变量添加互斥锁来很好地实现。 但作为一种高级方法，通过信道来控制访问能够让你写出更简洁，正确的程序。 我们可以从典型的单线程运行在单 CPU 之上的情形来审视这种模型。它无需提供同步原语。 现在再运行一个线程，它也无需同步。现在让它们俩进行通信。若将通信过程看做同步着， 那就完全不需要其它同步了。例如，Unix 管道就与这种模型完美契合。 尽管 Go 的并发处理方式来源于 Hoare 的通信顺序处理（CSP）， 它依然可以看做是类型安全的 Unix 管道的实现。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:1","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Goroutines 我们称之为 ** goroutine ** ，是因为现有的术语—线程、协程、进程等等—无法准确传达它的含义。 Goroutine 具有简单的模型：它是与其它 goroutine 并发运行在同一地址空间的函数。它是轻量级的， 所有消耗几乎就只有栈空间的分配。而且栈最开始是非常小的，所以它们很廉价， 仅在需要时才会随着堆空间的分配（和释放）而变化。 Goroutine 在多线程操作系统上可实现多路复用，因此若一个线程阻塞，比如说等待 I/O， 那么其它的线程就会运行。Goroutine 的设计隐藏了线程创建和管理的诸多复杂性。 在函数或方法前添加 go 关键字能够在新的 goroutine 中调用它。当调用完成后， 该 goroutine 也会安静地退出。（效果有点像 Unix Shell 中的 \u0026 符号，它能让命令在后台运行。） go list.Sort() // 并发运行 list.Sort，无需等它结束。 函数字面在 goroutine 调用中非常有用。 func Announce(message string, delay time.Duration) { go func() { time.Sleep(delay) fmt.Println(message) }() // Note the parentheses - must call the function. } 在 Go 中，函数字面都是闭包：其实现在保证了函数内引用变量的生命周期与函数的活动时间相同。 这些函数没什么实用性，因为它们没有实现完成时的信号处理。因此，我们需要信道。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:2","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Channels 信道与映射一样，也需要通过 make 来分配内存。其结果值充当了对底层数据结构的引用。 若提供了一个可选的整数形参，它就会为该信道设置缓冲区大小。默认值是零，表示不带缓冲的或同步的信道。 ci := make(chan int) // unbuffered channel of integers cj := make(chan int, 0) // unbuffered channel of integers cs := make(chan *os.File, 100) // buffered channel of pointers to Files 无缓冲信道在通信时会同步交换数据，它能确保（两个 goroutine）计算处于确定状态。 信道有很多惯用法，我们从这里开始了解。在上一节中，我们在后台启动了排序操作。 信道使得启动的 goroutine 等待排序完成。 c := make(chan int) // Allocate a channel. // Start the sort in a goroutine; when it completes, signal on the channel. go func() { list.Sort() c \u003c- 1 // Send a signal; value does not matter. }() doSomethingForAWhile() \u003c-c // Wait for sort to finish; discard sent value. 接收者在收到数据前会一直阻塞。若信道是不带缓冲的，那么在接收者收到值前， 发送者会一直阻塞；若信道是带缓冲的，则发送者直到值被复制到缓冲区才开始阻塞； 若缓冲区已满，发送者会一直等待直到某个接收者取出一个值为止。 带缓冲的信道可被用作信号量，例如限制吞吐量。在此例中，进入的请求会被传递给 handle，它从信道中接收值，处理请求后将值发回该信道中，以便让该 “信号量” 准备迎接下一次请求。信道缓冲区的容量决定了同时调用 process 的数量上限。 var sem = make(chan int, MaxOutstanding) func handle(r *Request) { sem \u003c- 1 // Wait for active queue to drain. process(r) // May take a long time. \u003c-sem // Done; enable next request to run. } func Serve(queue chan *Request) { for { req := \u003c-queue go handle(req) // Don't wait for handle to finish. } } 一旦有 MaxOutstanding 个处理器进入运行状态，其他的所有处理器都会在试图发送值到信道缓冲区的时候阻塞，直到某个处理器完成处理并从缓冲区取回一个值为止。 然而，它却有个设计问题：尽管只有 MaxOutstanding 个 goroutine 能同时运行，但 Serve 还是为每个进入的请求都创建了新的 goroutine。其结果就是，若请求来得很快， 该程序就会无限地消耗资源。为了弥补这种不足，我们可以通过修改 Serve 来限制创建 Go 程，这是个明显的解决方案，但要当心我们修复后出现的 Bug。 func Serve(queue chan *Request) { for req := range queue { sem \u003c- 1 go func() { process(req) // Buggy; see explanation below. \u003c-sem }() } } Bug 出现在 Go 的 for 循环中，该循环变量在每次迭代时会被重用，因此 req 变量会在所有的 goroutine 间共享，这不是我们想要的。我们需要确保 req 对于每个 goroutine 来说都是唯一的。有一种方法能够做到，就是将 req 的值作为实参传入到该 goroutine 的闭包中： func Serve(queue chan *Request) { for req := range queue { sem \u003c- 1 go func(req *Request) { process(req) \u003c-sem }(req) } } 比较前后两个版本，观察该闭包声明和运行中的差别。 另一种解决方案就是以相同的名字创建新的变量，如例中所示： func Serve(queue chan *Request) { for req := range queue { req := req // Create new instance of req for the goroutine. sem \u003c- 1 go func() { process(req) \u003c-sem }() } } 它的写法看起来有点奇怪 req := req 但在 Go 中这样做是合法且惯用的。你用相同的名字获得了该变量的一个新的版本， 以此来局部地刻意屏蔽循环变量，使它对每个 goroutine 保持唯一。 回到编写服务器的一般问题上来。另一种管理资源的好方法就是启动固定数量的 handle goroutine，一起从请求信道中读取数据。Goroutine 的数量限制了同时调用 process 的数量。Serve 同样会接收一个通知退出的信道， 在启动所有 goroutine 后，它将阻塞并暂停从信道中接收消息。 func handle(queue chan *Request) { for r := range queue { process(r) } } func Serve(clientRequests chan *Request, quit chan bool) { // Start handlers for i := 0; i \u003c MaxOutstanding; i++ { go handle(clientRequests) } \u003c-quit // Wait to be told to exit. } ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:3","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Channels of channels Go 最重要的特性就是信道是一等值，它可以被分配并像其它值到处传递。 这种特性通常被用来实现安全、并行的多路分解。 在上一节的例子中，handle 是个非常理想化的请求处理程序， 但我们并未定义它所处理的请求类型。若该类型包含一个可用于回复的信道， 那么每一个客户端都能为其回应提供自己的路径。以下为 Request 类型的大概定义。 type Request struct { args []int f func([]int) int resultChan chan int } 客户端提供了一个函数及其实参，此外在请求对象中还有个接收应答的信道。 func sum(a []int) (s int) { for _, v := range a { s += v } return } request := \u0026Request{[]int{3, 4, 5}, sum, make(chan int)} // Send request clientRequests \u003c- request // Wait for response. fmt.Printf(\"answer: %d\\n\", \u003c-request.resultChan) 在服务端，只需改动 handler 函数。 func handle(queue chan *Request) { for req := range queue { req.resultChan \u003c- req.f(req.args) } } 要使其实际可用还有很多工作要做，这些代码仅能实现一个速率有限、并行、非阻塞 RPC 系统的框架，而且它并不包含互斥锁。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:4","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Parallelization 这些设计的另一个应用是在多 CPU 核心上实现并行计算。如果计算过程能够被分为几块可独立执行的过程，它就可以在每块计算结束时向信道发送信号，从而实现并行处理。 让我们看看这个理想化的例子。我们在对一系列向量项进行极耗资源的操作， 而每个项的值计算是完全独立的。 type Vector []float64 // Apply the operation to v[i], v[i+1] ... up to v[n-1]. func (v Vector) DoSome(i, n int, u Vector, c chan int) { for ; i \u003c n; i++ { v[i] += u.Op(v[i]) } c \u003c- 1 // signal that this piece is done } 我们在循环中启动了独立的处理块，每个 CPU 将执行一个处理。 它们有可能以乱序的形式完成并结束，但这没有关系； 我们只需在所有 goroutine 开始后接收，并统计信道中的完成信号即可。 const NCPU = 4 // number of CPU cores func (v Vector) DoAll(u Vector) { c := make(chan int, NCPU) // Buffering optional but sensible. for i := 0; i \u003c NCPU; i++ { go v.DoSome(i*len(v)/NCPU, (i+1)*len(v)/NCPU, u, c) } // Drain the channel. for i := 0; i \u003c NCPU; i++ { \u003c-c // wait for one task to complete } // All done. } 目前 Go 运行时的实现默认并不会并行执行代码，它只为用户层代码提供单一的处理核心。 任意数量的 goroutine 都可能在系统调用中被阻塞，而在任意时刻默认只有一个会执行用户层代码。 它应当变得更智能，而且它将来肯定会变得更智能。但现在，若你希望 CPU 并行执行， 就必须告诉运行时你希望同时有多少 goroutine 能执行代码。有两种途径可达到这一目的，要么 在运行你的工作时将 GOMAXPROCS 环境变量设为你要使用的核心数， 要么导入 runtime 包并调用 runtime.GOMAXPROCS(NCPU)。 runtime.NumCPU() 的值可能很有用，它会返回当前机器的逻辑 CPU 核心数。 当然，随着调度算法和运行时的改进，将来会不再需要这种方法。 注意不要混淆并发和并行的概念：并发是用可独立执行的组件构造程序的方法， 而并行则是为了效率在多 CPU 上平行地进行计算。尽管 Go 的并发特性能够让某些问题更易构造成并行计算， 但 Go 仍然是种并发而非并行的语言，且 Go 的模型并不适合所有的并行问题。 关于其中区别的讨论，见 此博文。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:5","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"A leaky buffer 并发编程的工具甚至能很容易地表达非并发的思想。这里有个提取自 RPC 包的例子。 客户端 Go 程从某些来源，可能是网络中循环接收数据。为避免分配和释放缓冲区， 它保存了一个空闲链表，使用一个带缓冲信道表示。若信道为空，就会分配新的缓冲区。 一旦消息缓冲区就绪，它将通过 serverChan 被发送到服务器。 var freeList = make(chan *Buffer, 100) var serverChan = make(chan *Buffer) func client() { for { var b *Buffer // Grab a buffer if available; allocate if not. select { case b = \u003c-freeList: // Got one; nothing more to do. default: // None free, so allocate a new one. b = new(Buffer) } load(b) // Read next message from the net. serverChan \u003c- b // Send to server. } } 服务器从客户端循环接收每个消息，处理它们，并将缓冲区返回给空闲列表。 func server() { for { b := \u003c-serverChan // Wait for work. process(b) // Reuse buffer if there's room. select { case freeList \u003c- b: // Buffer on free list; nothing more to do. default: // Free list full, just carry on. } } } 客户端试图从 freeList 中获取缓冲区；若没有缓冲区可用， 它就将分配一个新的。服务器将 b 放回空闲列表 freeList 中直到列表已满，此时缓冲区将被丢弃，并被垃圾回收器回收。（select 语句中的 default 子句在没有条件符合时执行，这也就意味着 selects 永远不会被阻塞。）依靠带缓冲的信道和垃圾回收器的记录， 我们仅用短短几行代码就构建了一个可能导致缓冲区槽位泄露的空闲列表。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:14:6","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Errors 库例程通常需要向调用者返回某种类型的错误提示。之前提到过，Go 语言的多值返回特性， 使得它在返回常规的值时，还能轻松地返回详细的错误描述。使用这个特性来提供详细的错误信息是一种良好的风格。 例如，我们稍后会看到， os.Open 在失败时不仅返回一个 nil 指针，还返回一个详细描述错误的 error 值。 按照约定，错误的类型通常为 error，这是一个内建的简单接口。 type error interface { Error() string } 库的编写者通过更丰富的底层模型可以轻松实现这个接口，这样不仅能看见错误，还能提供一些上下文。前已述及，除了通常的 *os.File 返回值， os.Open 还返回一个 error 值。若该文件被成功打开， error 值就是 nil ，而如果出了问题，该值就是一个 os.PathError。 // PathError records an error and the operation and // file path that caused it. type PathError struct { Op string // \"open\", \"unlink\", etc. Path string // The associated file. Err error // Returned by the system call. } func (e *PathError) Error() string { return e.Op + \" \" + e.Path + \": \" + e.Err.Error() } PathError 的 Error 会生成如下错误信息： open /etc/passwx: no such file or directory 这种错误包含了出错的文件名、操作和触发的操作系统错误，即便在产生该错误的调用和输出的错误信息相距甚远时，它也会非常有用，这比苍白的 “不存在该文件或目录” 更具说明性。 错误字符串应尽可能地指明它们的来源，例如产生该错误的包名前缀。例如在 image 包中，由于未知格式导致解码错误的字符串为 “image: unknown format”。 若调用者关心错误的完整细节，可使用类型选择或者类型断言来查看特定错误，并抽取其细节。 对于 PathErrors，它应该还包含检查内部的 Err 字段以进行可能的错误恢复。 for try := 0; try \u003c 2; try++ { file, err = os.Create(filename) if err == nil { return } if e, ok := err.(*os.PathError); ok \u0026\u0026 e.Err == syscall.ENOSPC { deleteTempFiles() // Recover some space. continue } return } 这里的第二条 if 是另一种 类型断言。若它失败， ok 将为 false，而 e 则为 nil. 若它成功，ok 将为 true，这意味着该错误属于 *os.PathError 类型，而 e 能够检测关于该错误的更多信息。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:15:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Panic 向调用者报告错误的一般方式就是将 error 作为额外的值返回。 标准的 Read 方法就是个众所周知的实例，它返回一个字节计数和一个 error。但如果错误是不可恢复的呢？有时程序就是不能继续运行。 为此，我们提供了内建的 panic 函数，它会产生一个运行时错误并终止程序 （但请继续看下一节）。该函数接受一个任意类型的实参（一般为字符串），并在程序终止时打印。 它还能表明发生了意料之外的事情，比如从无限循环中退出了。 // A toy implementation of cube root using Newton's method. func CubeRoot(x float64) float64 { z := x/3 // Arbitrary initial value for i := 0; i \u003c 1e6; i++ { prevz := z z -= (z*z*z-x) / (3*z*z) if veryClose(z, prevz) { return z } } // A million iterations has not converged; something is wrong. panic(fmt.Sprintf(\"CubeRoot(%g) did not converge\", x)) } 这仅仅是个示例，实际的库函数应避免 panic。若问题可以被屏蔽或解决， 最好就是让程序继续运行而不是终止整个程序。一个可能的反例就是初始化： 若某个库真的不能让自己工作，且有足够理由产生 Panic，那就由它去吧。 var user = os.Getenv(\"USER\") func init() { if user == \"\" { panic(\"no value for $USER\") } } ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:15:1","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"Recover 当 panic 被调用后（包括不明确的运行时错误，例如切片检索越界或类型断言失败）， 程序将立刻终止当前函数的执行，并开始回溯 goroutine 的栈，运行任何被推迟的函数。 若回溯到达 goroutine 栈的顶端，程序就会终止。不过我们可以用内建的 recover 函数来重新取回 goroutine 的控制权限并使其恢复正常执行。 调用 recover 将停止回溯过程，并返回传入 panic 的实参。 由于在回溯时只有被推迟函数中的代码在运行，因此 recover 只能在被推迟的函数中才有效。 recover 的一个应用就是在服务器中终止失败的 goroutine 而无需杀死其它正在执行的 goroutine。 func server(workChan \u003c-chan *Work) { for work := range workChan { go safelyDo(work) } } func safelyDo(work *Work) { defer func() { if err := recover(); err != nil { log.Println(\"work failed:\", err) } }() do(work) } 在此例中，若 do(work) 触发了 Panic，其结果就会被记录， 而该 Go 程会被干净利落地结束，不会干扰到其它 goroutine。我们无需在推迟的闭包中做任何事情， recover 会处理好这一切。 由于直接从被推迟函数中调用 recover 时不会返回 nil， 因此被推迟的代码能够调用本身使用了 panic 和 recover 的库函数而不会失败。例如在 safelyDo 中，被推迟的函数可能在调用 recover 前先调用记录函数，而该记录函数应当不受 Panic 状态的代码的影响。 通过恰当地使用恢复模式，do 函数（及其调用的任何代码）可通过调用 panic 来避免更坏的结果。我们可以利用这种思想来简化复杂软件中的错误处理。 让我们看看 regexp 包的理想化版本，它会以局部的错误类型调用 panic 来报告解析错误。以下是一个 error 类型的 Error 方法和一个 Compile 函数的定义： // Error is the type of a parse error; it satisfies the error interface. type Error string func (e Error) Error() string { return string(e) } // error is a method of *Regexp that reports parsing errors by // panicking with an Error. func (regexp *Regexp) error(err string) { panic(Error(err)) } // Compile returns a parsed representation of the regular expression. func Compile(str string) (regexp *Regexp, err error) { regexp = new(Regexp) // doParse will panic if there is a parse error. defer func() { if e := recover(); e != nil { regexp = nil // Clear return value. err = e.(Error) // Will re-panic if not a parse error. } }() return regexp.doParse(str), nil } 若 doParse 触发了 Panic，恢复块会将返回值设为 nil —被推迟的函数能够修改已命名的返回值。在 err 的赋值过程中， 我们将通过断言它是否拥有局部类型 Error 来检查它。若它没有， 类型断言将会失败，此时会产生运行时错误，并继续栈的回溯，仿佛一切从未中断过一样。 该检查意味着若发生了一些像索引越界之类的意外，那么即便我们使用了 panic 和 recover 来处理解析错误，代码仍然会失败。 通过适当的错误处理，error 方法（由于它是个绑定到具体类型的方法， 因此即便它与内建的 error 类型名字相同也没有关系） 能让报告解析错误变得更容易，而无需手动处理回溯的解析栈： if pos == 0 { re.error(\"'*' illegal at start of expression\") } 尽管这种模式很有用，但它应当仅在包内使用。Parse 会将其内部的 panic 调用转为 error 值，它并不会向调用者暴露出 panic。这是个值得遵守的良好规则。 顺便一提，这种重新触发Panic的惯用法会在产生实际错误时改变Panic的值。 然而，不管是原始的还是新的错误都会在崩溃报告中显示，因此问题的根源仍然是可见的。 这种简单的重新触发Panic的模型已经够用了，毕竟他只是一次崩溃。 但若你只想显示原始的值，也可以多写一点代码来过滤掉不需要的问题，然后用原始值再次触发Panic。 这里就将这个练习留给读者了。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:15:2","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Go"],"content":"A web server 让我们以一个完整的 Go 程序作为结束吧，一个 Web 服务器。该程序其实只是个 Web 服务器的重用。 Google 在 http://chart.apis.google.com 上提供了一个将表单数据自动转换为图表的服务。不过，该服务很难交互， 因为你需要将数据作为查询放到 URL 中。此程序为一种数据格式提供了更好的的接口： 给定一小段文本，它将调用图表服务器来生成二维码（QR 码），这是一种编码文本的点格矩阵。 该图像可被你的手机摄像头捕获，并解释为一个字符串，比如 URL， 这样就免去了你在狭小的手机键盘上键入 URL 的麻烦。 以下为完整的程序，随后有一段解释。 package main import ( \"flag\" \"html/template\" \"log\" \"net/http\" ) var addr = flag.String(\"addr\", \":1718\", \"http service address\") // Q=17, R=18 var templ = template.Must(template.New(\"qr\").Parse(templateStr)) func main() { flag.Parse() http.Handle(\"/\", http.HandlerFunc(QR)) err := http.ListenAndServe(*addr, nil) if err != nil { log.Fatal(\"ListenAndServe:\", err) } } func QR(w http.ResponseWriter, req *http.Request) { templ.Execute(w, req.FormValue(\"s\")) } const templateStr = ` \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eQR Link Generator\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e {{if.}}\u003cimg src=\"http://chart.apis.google.com/chart?chs=300x300\u0026cht=qr\u0026choe=UTF-8\u0026chl={{.}}\" /\u003e \u003cbr\u003e {{.}}\u003cbr\u003e \u003cbr\u003e {{end}}\u003cform action=\"/\" name=f method=\"GET\"\u003e\u003cinput maxLength=1024 size=70 name=s value=\"\" title=\"Text to QR Encode\"\u003e\u003cinput type=submit value=\"Show QR\" name=qr\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e ` main 之前的代码应该比较容易理解。我们通过一个标志为服务器设置了默认端口。 模板变量 templ 正是有趣的地方。它构建的 HTML 模版将会被服务器执行并显示在页面中。 稍后我们将详细讨论。 main 函数解析了参数标志并使用我们讨论过的机制将 QR 函数绑定到服务器的根路径。然后调用 http.ListenAndServe 启动服务器；它将在服务器运行时处于阻塞状态。 QR 仅接受包含表单数据的请求，并为表单值 s 中的数据执行模板。 模板包 html/template 非常强大；该程序只是浅尝辄止。 本质上，它通过在运行时将数据项中提取的元素（在这里是表单值）传给 templ.Execute 执行因而重写了 HTML 文本。 在模板文本（templateStr）中，双大括号界定的文本表示模板的动作。 从 {{if .}} 到 {{end}} 的代码段仅在当前数据项（这里是点 .）的值非空时才会执行。 也就是说，当字符串为空时，此部分模板段会被忽略。 其中两段 {{.}} 表示要将数据显示在模板中 （即将查询字符串显示在 Web 页面上）。HTML 模板包将自动对文本进行转义， 因此文本的显示是安全的。 余下的模板字符串只是页面加载时将要显示的 HTML。如果这段解释你无法理解，请参考 文档 获得更多有关模板包的解释。 你终于如愿以偿了：以几行代码实现的，包含一些数据驱动的HTML文本的Web服务器。 Go语言强大到能让很多事情以短小精悍的方式解决。 ","date":"2022-04-30 09:22:50","objectID":"/effective_go/:16:0","tags":["go"],"title":"Effective_go","uri":"/effective_go/"},{"categories":["Advanced learning"],"content":"HTTP 用 Nginx 搭建 Web 服务器，照着网上的文章配好了，但里面那么多的指令，什么 keepalive、rewrite、proxy_pass 都是怎么回事？为什么要这么配置？用 Python 写爬虫，URI、URL“傻傻分不清”，有时里面还会加一些奇怪的字符，怎么处理才好？都说 HTTP 缓存很有用，可以大幅度提升系统性能，可它是怎么做到的？又应该用在何时何地？HTTP 和 HTTPS 是什么关系？还经常听说有 SSL/TLS/SNI/OCSP/ALPN……这么多稀奇古怪的缩写，头都大了，实在是搞不懂。 我们还有一个终极的学习资料，那就是 RFC 文档。RFC 是互联网工程组（IETF）发布的官方文件，是对 HTTP 最权威的定义和解释。但它也是最难懂的，全英文看着费劲，理解起来更是难上加难，文档之间还会互相关联引用，“劝退率”极高。 rfc8040 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:0:0","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"基础 因特网、互联网、万维网： 互联网 \u003e 因特网 \u003e 万维网 凡是能彼此通信的设备组成的网络就叫互联网 因特网是网络与网络之间所串连成的庞大网络，这些网络以一组标准的网络TCP/IP协议族相连 万维网是文件、图片、多媒体和其他资源的集合，资源通过超链接互相连接形成网络，并使用统一资源标志符（URL）标识。HTTP是万维网的主要访问协议。 据 NetCraft 公司统计，目前全球至少有 16 亿个网站、2 亿多个独立域名，而这个庞大网络世界的底层运转机制就是 HTTP。 本文内容： 广度上从 HTTP 尽量向外扩展，不只讲协议本身，与它相关的 TCP/IP、DNS、SSL/TLS、Web Server 等 基于最新RFC标准文档 分析 HTTPS时用 Wireshark 从建立 TCP 连接时就开始抓包，从二进制最底层来分析里面的 Record、Cipher Suite、Extension，讲 ECDHE、AES、SHA384，再画出详细的流程图，做到“一览无余” ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:0","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"历史 20 世纪 60 年代，美国国防部高等研究计划署（ARPA）建立了 ARPA 网，它有四个分布在各地的节点，被认为是如今互联网的“始祖”。 然后在 70 年代，基于对 ARPA 网的实践和思考，研究人员发明出了著名的 TCP/IP 协议。由于具有良好的分层结构和稳定的性能，TCP/IP 协议迅速战胜其他竞争对手流行起来，并在 80 年代中期进入了 UNIX 系统内核，促使更多的计算机接入了互联网。 1989 年，任职于欧洲核子研究中心（CERN）的蒂姆·伯纳斯 - 李（Tim Berners-Lee）发表了一篇论文，提出了在互联网上构建超链接文档系统的构想。这篇论文中他确立了三项关键技术。 URI：即统一资源标识符，作为互联网上资源的唯一身份； HTML：即超文本标记语言，描述超文本文档； HTTP：即超文本传输协议，用来传输超文本。 （超文本有超链接，是网状结构，而普通文本是线性结构。） 蒂姆把这个系统称为“万维网”（World Wide Web），也就是我们现在所熟知的 Web。 HTTP/0.9 20 世纪 90 年代初期的互联网世界非常简陋，计算机处理能力低，存储容量小，网速很慢。网络上绝大多数的资源都是纯文本，很多通信协议也都使用纯文本。这一时期的 HTTP 被定义为 0.9 版，结构比较简单，为了便于服务器和客户端处理，它也采用了纯文本格式。蒂姆·伯纳斯 - 李最初设想的系统里的文档都是只读的，所以只允许用“GET”动作从服务器上获取 HTML 文档，并且在响应请求之后立即关闭连接，功能非常有限。 HTTP/1.0 1993 年，NCSA（美国国家超级计算应用中心）开发出了 Mosaic，是第一个可以图文混排的浏览器，随后又在 1995 年开发出了服务器软件 Apache，简化了 HTTP 服务器的搭建工作。同一时期，计算机多媒体技术也有了新的发展：1992 年发明了 JPEG 图像格式，1995 年发明了 MP3 音乐格式。HTTP/1.0 版本在 1996 年正式发布。它在多方面增强了 0.9 版，形式上已经和我们现在的 HTTP 差别不大了，例如： 增加了 HEAD、POST 等新方法； 增加了响应状态码，标记可能的错误原因； 引入了协议版本号概念； 引入了 HTTP Header（头部）的概念，让 HTTP 处理请求和响应更加灵活； 传输的数据不再仅限于文本。 但 HTTP/1.0 并不是一个“标准”，只是记录已有实践和模式的一份参考文档，不具有实际的约束力，相当于一个“备忘录”。 HTTP/1.1 1995 年，网景的 Netscape Navigator 和微软的 Internet Explorer 开始了著名的“浏览器大战”，都希望在互联网上占据主导地位。最终微软的 IE 取得了决定性的胜利，而网景则“败走麦城”（但后来却凭借 Mozilla Firefox 又扳回一局） 在“浏览器大战”结束之后的 1999 年，HTTP/1.1 发布了 RFC 文档，编号为 2616，正式确立了延续十余年的传奇。 HTTP/1.1 与 HTTP/1.0 的一个重要的区别是：它是一个“正式的标准”，而不是一份可有可无的“参考文档”。这意味着今后互联网上所有的浏览器、服务器、网关、代理等等，只要用到 HTTP 协议，就必须严格遵守这个标准，相当于是互联网世界的一个“立法”。 HTTP/1.1 主要的变更点有： 增加了 PUT、DELETE 等新的方法； 增加了缓存管理和控制； 明确了连接管理，允许持久连接； 允许响应数据分块（chunked），利于传输大文件； 强制要求 Host 头，让互联网主机托管成为可能 只要是HTTP/1.1，就都是文本格式，虽然里面的数据可能是二进制，但分隔符还是文本。 现在许多的知名网站都是在HTTP/1.1这个时间点左右创立的，例如 Google、新浪、搜狐、网易、腾讯等，互联网开始爆发式增长。不过由于 HTTP/1.1 太过庞大和复杂，所以在 2014 年又做了一次修订，原来的一个大文档被拆分成了六份较小的文档，编号为 7230-7235，优化了一些细节，但此外没有任何实质性的改动。 HTTP/2 当时也有一些弊病：主要是连接慢。但标准固定人们只能耍技巧，比如切图、js合并等网页优化手段。 Google 首先开发了自己的浏览器 Chrome，然后推出了新的 SPDY 协议，并在 Chrome 里应用于自家的服务器，如同十多年前的网景与微软一样，从实际的用户方来“倒逼”HTTP 协议的变革，这也开启了第二次的“浏览器大战”。 历史再次重演，不过这次的胜利者是 Google，Chrome 目前的全球的占有率超过了 60%。Google 借此顺势把 SPDY 推上了标准的宝座，互联网标准化组织以 SPDY 为基础开始制定新版本的 HTTP 协议，最终在 2015 年发布了 HTTP/2，RFC 编号 7540。 HTTP/2 的制定充分考虑了现今互联网的现状：宽带、移动、不安全，在高度兼容 HTTP/1.1 的同时在性能改善方面做了很大努力，主要的特点有： 二进制协议，不再是纯文本； 可发起多个请求，废弃了 1.1 里的管道； 使用专用算法压缩头部，减少数据传输量； 允许服务器主动向客户端推送数据； 增强了安全性，“事实上”要求加密通信。 HTTP/3 在 HTTP/2 还处于草案之时，Google 又发明了一个新的协议，叫做 QUIC，而且还是相同的“套路”，继续在 Chrome 和自家服务器里试验着“玩”，依托它的庞大用户量和数据量，持续地推动 QUIC 协议成为互联网上的“既成事实”。 2018 年，互联网标准化组织 IETF 提议将“HTTP over QUIC”更名为“HTTP/3”并获得批准，HTTP/3 正式进入了标准化制订阶段。HTTP/3 现在还没正式推出，不过自 2017 年起， HTTP/3 已经更新到 30 多个草案了。 小结 HTTP 协议始于三十年前蒂姆·伯纳斯 - 李的一篇论文； HTTP/0.9 是个简单的文本协议，只能获取文本资源； HTTP/1.0 确立了大部分现在使用的技术，但它不是正式标准； HTTP/1.1 是目前互联网上使用最广泛的协议，功能也非常完善； HTTP/2 基于 Google 的 SPDY 协议，注重性能改善，但还未普及； HTTP/3 基于 Google 的 QUIC 协议，是将来的发展方向。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:1","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP的定义 HyperText Transfer Protocol。 协议意味着有多个参与者为了达成某个共同的目的而站在了一起，除了要无疑义地沟通交流之外，还必须明确地规定各方的“责、权、利”，约定该做什么不该做什么，先做什么后做什么，做错了怎么办，有没有补救措施等等。 HTTP 是一个用在计算机世界里的协议。它使用计算机能够理解的语言确立了一种计算机之间交流通信的规范，以及相关的各种控制和错误处理方式。 HTTP 是一个在计算机世界里专门用来在两点之间传输数据的约定和规范 最终定义： HTTP 是一个在计算机世界里专门在两点之间传输超文本数据的约定和规范” 互联网（Internet）是遍布于全球的许多网络互相连接而形成的一个巨大的国际网络，在它上面存放着各式各样的资源，也对应着各式各样的协议，例如超文本资源使用 HTTP，普通文件使用 FTP，电子邮件使用 SMTP 和 POP3 等。 HTML 是超文本的载体，是一种标记语言，使用各种标签描述文字、图片、超链接等资源，并且可以嵌入 CSS、JavaScript 等技术实现复杂的动态效果。 在互联网世界里，HTTP 通常跑在 TCP/IP 协议栈之上，依靠 IP 协议实现寻址和路由、TCP 协议实现可靠数据传输、DNS 协议实现域名查找、SSL/TLS 协议实现安全通信。此外，还有一些协议依赖于 HTTP，例如 WebSocket、HTTPDNS 等。这些协议相互交织，构成了一个协议网，而 HTTP 则处于中心地位。 与HTTP相关的协议与技术（左边偏理论，右边偏应用）总图：","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:2","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP相关的概念 见总图右边部分。 网络世界： 实际的互联网是由许许多多个规模略小的网络连接而成的，这些“小网络”可能是只有几百台电脑的局域网，可能是有几万、几十万台电脑的广域网，可能是用电缆、光纤构成的固定网络，也可能是用基站、热点构成的移动网络…… 我们通常所说的“上网”实际上访问的只是互联网的一个子集“万维网”（World Wide Web），它基于 HTTP 协议，传输 HTML 等超文本资源，能力也就被限制在 HTTP 协议之内。 互联网上还有许多万维网之外的资源，例如常用的电子邮件、BT（BitTorrent，一种内容分发协议，上传速度越快，下载速度越快；但BT下载速度不够稳定，当中断时则无法完整下载。） 和 Magnet（磁力链接，磁力链接是一种特殊链接，但是它与传统基于文件的位置或名称的普通链接（如http://xxx）不一样，它只是通过不同文件内容的Hash结果生成一个纯文本的“数字指纹”，并用它来识别文件。） 点对点下载、FTP 文件下载、SSH 安全登录、各种即时通信服务等等，它们需要用各自的专有协议来访问。 由于 HTTP 协议非常灵活、易于扩展，而且“超文本”的表述能力很强，所以很多其他原本不属于 HTTP 的资源也可以“包装”成 HTTP 来访问，这就是我们为什么能够总看到各种“网页应用”——例如“微信网页版”“邮箱网页版”——的原因。 浏览器： Google 的 Chrome、Mozilla 的 Firefox、Apple 的 Safari、Microsoft 的 IE 和 Edge（后者是前者的替代品，Edge是win10正式推出的，支持更多插件拓展，而IE在16年停止了更新），还有小众的 Opera 以及国内的各种“换壳”的“极速”“安全”浏览器。 浏览器本质上是一个 HTTP 协议中的请求方，使用 HTTP 协议获取网络上的各种资源。当然，为了让我们更好地检索查看网页，它还集成了很多额外的功能。（例如，HTML 排版引擎用来展示页面，JavaScript 引擎用来实现动态化效果，甚至还有开发者工具用来调试网页，以及五花八门的各种插件和扩展。） 在 HTTP 协议里，浏览器的角色被称为“User Agent”即“用户代理”，意思是作为访问者的“代理”来发起 HTTP 请求。不过在不引起混淆的情况下，我们通常都简单地称之为“客户端”。 web服务器： 作为HTTP 协议里响应请求的主体，通常也把控着绝大多数的网络资源 web服务器的组成：硬件、软件 硬件含义就是物理形式或“云”形式的机器，在大多数情况下它可能不是一台服务器，而是利用反向代理、负载均衡等技术组成的庞大集群。但从外界看来，它仍然表现为一台机器，但这个形象是“虚拟的”。 软件含义的 Web 服务器可能我们更为关心，它就是提供 Web 服务的应用程序，通常会运行在硬件含义的服务器上。它利用强大的硬件能力响应海量的客户端 HTTP 请求，处理磁盘上的网页、图片等静态文件，或者把请求转发给后面的 Tomcat、Node.js 等业务应用，返回动态的信息。 Apache成熟且适合入门。Nginx作为后起之秀，具备高性能、高稳定性、易拓展等优势，受高流量网站的青睐。此外，还有 Windows 上的 IIS、Java 的 Jetty/Tomcat 等，因为性能不是很高，所以在互联网上应用得较少。 CDN: 浏览器和服务器是 HTTP 协议的两个端点。但浏览器通常不会直接连到服务器，中间会经过“重重关卡”，其中的一个重要角色就叫做 CDN。 CDN，全称是“Content Delivery Network”，翻译过来就是“内容分发网络”。它应用了 HTTP 协议里的缓存和代理技术，代替源站响应客户端的请求。 可以缓存源站的数据，让浏览器的请求不用“千里迢迢”地到达源站服务器，直接在“半路”就可以获取响应。如果 CDN 的调度算法很优秀，更可以找到离用户最近的节点，大幅度缩短响应时间。 除了基本的网络加速外，还提供负载均衡、安全防护、边缘计算、跨运营商网络等功能，能够成倍地“放大”源站服务器的服务能力，很多云服务商都把 CDN 作为产品的一部分。 作为透明代理与反向代理。 爬虫： 自动访问web资源的应用程序 据估计，互联网上至少有 50% 的流量都是由爬虫产生的，某些特定领域的比例还会更高 绝大多数爬虫是由各大搜索引擎“放”出来的，抓取网页存入庞大的数据库，再建立关键字索引，这样我们才能够在搜索引擎中快速地搜索到互联网角落里的页面 不好的地方：过度消耗网络资源，占用服务器和带宽，影响网站对真实数据的分析，甚至导致敏感信息泄漏 反爬虫：网站自身的反爬机制以及“君子协定”robots.txt（约定哪些该爬，哪些不该爬）等。 爬与反爬都只用到了两个技术：HTTP,HTML ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:3","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP相关的协议 见总图左边部分。 TCP/IP： TCP/IP 协议实际上是一系列网络通信协议的统称，其中最核心的两个协议是 TCP 和 IP，其他的还有 UDP、ICMP、ARP 等等，共同构成了一个复杂但有层次的协议栈。 这个协议栈有四层，最上层是“应用层”，最下层是“链接层”，TCP 和 IP 则在中间：TCP 属于“传输层”，IP 属于“网际层”。协议的层级关系模型非常重要。 IP 协议是“Internet Protocol”的缩写，主要目的是解决寻址和路由问题，以及如何在两点间传送数据包。IP 协议使用“IP 地址”的概念来定位互联网上的每一台计算机。 现在我们使用的 IP 协议大多数是 v4 版，地址是四个用“.”分隔的数字，例如“192.168.0.1”，总共有 2^32，大约 42 亿个可以分配的地址。看上去好像很多，但互联网的快速发展让地址的分配管理很快就“捉襟见肘”。所以，就又出现了 v6 版，使用 8 组“:”分隔的数字作为地址，容量扩大了很多，有 2^128 个，在未来的几十年里应该是足够用了。 TCP 协议是“Transmission Control Protocol”的缩写，意思是“传输控制协议”，它位于 IP 协议之上，基于 IP 协议提供可靠的、字节流形式的通信，是 HTTP 协议得以实现的基础。“可靠”是指保证数据不丢失，“字节流”是指保证数据完整，所以在 TCP 协议的两端可以如同操作文件一样访问传输的数据，就像是读写在一个密闭的管道里“流动”的字节。 因此，HTTP协议运行在TCP/IP之上。 DNS： 域名系统”（Domain Name System） 在 DNS 中，“域名”（Domain Name）又称为“主机名”（Host），为了更好地标记不同国家或组织的主机，让名字更好记，所以被设计成了一个有层次的结构。 域名用“.”分隔成多个单词，级别从左到右逐级升高，最右边的被称为“顶级域名”。对于顶级域名，可能你随口就能说出几个，例如表示商业公司的“com”、表示教育机构的“edu”，表示国家的“cn”“uk”等 目前全世界有 13 组根 DNS 服务器，下面再有许多的顶级 DNS、权威 DNS 和更小的本地 DNS，逐层递归地实现域名查询。 HTTP 协议中并没有明确要求必须使用 DNS，但实际上为了方便访问互联网上的 Web 服务器，通常都会使用 DNS 来定位或标记主机名，间接地把 DNS 与 HTTP 绑在了一起。 URI、URL： DNS 和 IP 地址只是标记了互联网上的主机，并没有确定要访问的资源。 URI（Uniform Resource Identifier），中文名称是 统一资源标识符，使用它就能够唯一地标记互联网上资源。 URI 另一个更常用的表现形式是 URL（Uniform Resource Locator）， 统一资源定位符，也就是我们俗称的“网址”，它实际上是 URI 的一个子集，不过因为这两者几乎是相同的，差异不大，所以通常不会做严格的区分。 http://nginx.org/en/download.html URI 主要有三个基本部分构成： 协议名：即访问该资源应当使用的协议，在这里是“http”； 主机名：即互联网上主机的标记，可以是域名或 IP 地址，在这里是“nginx.org”； 路径：即资源在主机上的位置，使用“/”分隔多级目录，在这里是“/en/download.html”。 HTTPS（HTTP over SSL/TLS）： SSL/TLS是负责加密通信的安全协议，可以被用作HTTP的下层 SSL 的全称是 “Secure Socket Layer” ，由网景公司发明，当发展到 3.0 时被标准化，改名为 TLS，即“Transport Layer Security”，但由于历史的原因还是有很多人称之为 SSL/TLS，或者直接简称为 SSL。 SSL 使用了许多密码学最先进的研究成果，综合了对称加密、非对称加密、摘要算法、数字签名、数字证书等技术，能够在不安全的环境中为通信的双方创建出一个秘密的、安全的传输通道。 浏览器地址栏，如果有一个小锁头标志，那就表明网站启用了安全的 HTTPS 协议，而 URI 里的协议名，也从“http”变成了“https” 代理： 代理（Proxy）是 HTTP 协议中请求方和应答方中间的一个环节，作为“中转站”，既可以转发客户端的请求，也可以转发服务器的应答。 常见的代理： 匿名代理：完全“隐匿”了被代理的机器，外界看到的只是代理服务器； 透明代理：顾名思义，它在传输过程中是“透明开放”的，外界既知道代理，也知道客户端； 正向代理：靠近客户端，代表客户端向服务器发送请求； 反向代理：靠近服务器端，代表服务器响应客户端的请求； 由于代理在传输过程中插入了一个“中间层”，所以可以在这个环节做很多有意思的事情，比如： 负载均衡：把访问请求均匀分散到多台机器，实现访问集群化； 内容缓存：暂存上下行的数据，减轻后端的压力； 安全防护：隐匿 IP, 使用 WAF 等工具抵御网络攻击，保护被代理的机器； 数据处理：提供压缩、加密等额外的功能。 关于 HTTP 的代理还有一个特殊的“代理协议”（proxy protocol），它由知名的代理软件 HAProxy 制订，但并不是 RFC 标准。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:4","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"四层？七层？ 什么四层负载均衡”“七层负载均衡”，什么“二层转发”“三层路由” 都是啥？ TCP/IP网络分层模型： 把复杂的网络通信划分出多个层次，再给每一个层次分配不同的职责，层次内只专心做自己的事情就好，典型的分而治之的思想。 TCP/IP协议栈层次图： ---------------------- application layer /HTTP ----OSI:L5,L6,L7 ---------------------- transport layer /TCP/UDP ----OSI:L4 ---------------------- internet layer /IP ----OSI:L3 ---------------------- link layer /MAC (first layer) ----OSI:L2 ---------------------- 第一层叫“链接层”（link layer），负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址（MAC地址（英语：Media Access Control Address），直译为媒体存取控制位址，也称为局域网地址（LAN Address），MAC位址，以太网地址（Ethernet Address）或物理地址（Physical Address），它是一个用来确认网络设备位置的位址。在OSI模型中，第三层网络层负责IP地址，第二层数据链路层则负责MAC位址 。MAC地址用于在网络中唯一标示一个网卡，一台设备若有一或多个网卡，则每个网卡都需要并会有一个唯一的MAC地址，摘自百度百科）来标记网络上的设备，所以有时候也叫 MAC 层。 第二层叫“网际层”或者“网络互连层”（internet layer），IP 协议就处在这一层。因为 IP 协议定义了“IP 地址”的概念，所以就可以在“链接层”的基础上，用 IP 地址取代 MAC 地址，把许许多多的局域网、广域网连接成一个虚拟的巨大网络，在这个网络里找设备时只要把 IP 地址再“翻译”成 MAC 地址就可以了。 第三层叫“传输层”（transport layer），这个层次协议的职责是保证数据在 IP 地址标记的两点之间“可靠”地传输，是 TCP 协议工作的层次，另外还有它的一个“小伙伴”UDP。（User Datagram Protocol） ，都是二进制协议。 TCP 是一个有状态的协议，需要先与对方建立连接然后才能发送数据，而且保证数据不丢失不重复。而 UDP 则比较简单，它无状态，不用事先建立连接就可以任意发送数据，但不保证数据一定会发到对方。两个协议的另一个重要区别在于数据的形式。TCP 的数据是连续的“字节流”，有先后顺序，而 UDP 则是分散的小数据包，是顺序发，乱序收。 关于 TCP 和 UDP 可以展开讨论的话题还有很多，比如最经典的“三次握手”和“四次挥手”，一时半会很难说完，好在与 HTTP 的关系不是太大，以后遇到了再详细讲解。 协议栈的第四层叫“应用层”（application layer），由于下面的三层把基础打得非常好，所以在这一层就“百花齐放”了，有各种面向具体应用的协议。例如 Telnet、SSH、FTP、SMTP 等等，当然还有我们的 HTTP。 MAC 层的传输单位是帧（frame），IP 层的传输单位是包（packet），TCP 层的传输单位是段（segment），HTTP 的传输单位则是消息或报文（message）。但这些名词并没有什么本质的区分，可以统称为数据包。 OSI网络分层模型： OSI，全称是“开放式系统互联通信参考模型”（Open System Interconnection Reference Model）。 背景：TCP/IP 发明于 1970 年代，当时除了它还有很多其他的网络协议，整个网络世界比较混乱。这个时候国际标准组织（ISO）注意到了这种现象，就想要来个“大一统”。于是设计出了一个新的网络分层模型，想用这个新框架来统一既存的各种网络协议。 OSI层次模型： ------------------ application layer L7 ------------------ presentation layer L6 ------------------ session layer L5 ------------------ transport layer L4 ------------------ network layer L3 ------------------ data link layer L2 ------------------ physical layer L1 ------------------ 第一层：物理层，网络的物理形式，例如电缆、光纤、网卡、集线器等等； 第二层：数据链路层，它基本相当于 TCP/IP 的链接层； 第三层：网络层，相当于 TCP/IP 里的网际层； 第四层：传输层，相当于 TCP/IP 里的传输层； 第五层：会话层，维护网络中的连接状态，即保持会话和同步； 第六层：表示层，把数据转换为合适、可理解的语法和语义； 第七层：应用层，面向具体的应用传输数据。 对比一下就可以看出，TCP/IP 是一个纯软件的栈，没有网络应有的最根基的电缆、网卡等物理设备的位置。而 OSI 则补足了这个缺失，在理论层面上描述网络更加完整。 OSI 的分层模型在四层以上分的太细，而 TCP/IP 实际应用时的会话管理、编码转换、压缩等和具体应用经常联系的很紧密，很难分开。例如，HTTP 协议就同时包含了连接管理和数据格式定义。 所谓的“四层负载均衡”就是指工作在传输层上，基于 TCP/IP 协议的特性，例如 IP 地址、端口号等实现对后端服务器的负载均衡。 所谓的“七层负载均衡”就是指工作在应用层上，看到的是 HTTP 协议，解析 HTTP 报文里的 URI、主机名、资源类型等数据，再用适当的策略转发给后端服务器。 TCP/IP协议栈的工作方式 HTTP 协议的传输过程就是通过协议栈逐层向下，每一层都添加本层的专有数据，层层打包，然后通过下层发送出去。 接收数据则是相反的操作，从下往上穿过协议栈，逐层拆包，每层去掉本层的专有头，上层就会拿到自己的数据。 但下层的传输过程对于上层是完全“透明”的，上层也不需要关心下层的具体实现细节，所以就 HTTP 层次来看，它不管下层是不是 TCP/IP 协议，看到的只是一个可靠的传输链路，只要把数据加上自己的头，对方就能原样收到。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:5","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"域名相关 IP 协议的职责是“网际互连”，它在 MAC 层之上，使用 IP 地址把 MAC 编号转换成了四位数字，这就对物理网卡的 MAC 地址做了一层抽象，发展出了许多的“新玩法”。 但IP地址难记忆，从而发展出了DNS域名系统。 域名是一个有层次的结构，是一串用“.”分隔的多个单词，最右边的被称为“顶级域名”，然后是“二级域名”，层级关系向左依次降低。最左边的是主机名，通常用来表明主机的用途，比如“www”表示提供万维网服务、“mail”表示提供邮件服务，不过这也不是绝对的，名字的关键是要让我们容易记忆。 域名除了代替IP地址的其他用途： 在 Apache、Nginx 这样的 Web 服务器里，域名可以用来标识虚拟主机，决定由哪个虚拟主机来对外提供服务，比如在 Nginx 里就会使用“server_name”指令： server { listen 80; #监听80端口 server_name www.baidu.com; #主机名是www.baidu.com ... } 域名本质上还是个名字空间系统，使用多级域名就可以划分出不同的国家、地区、组织、公司、部门，每个域名都是独一无二的，可以作为一种身份的标识。 域名解析 就像 IP 地址必须转换成 MAC 地址才能访问主机一样，域名也必须要转换成 IP 地址，这个过程就是“域名解析”。 DNS 的核心系统是一个三层的树状、分布式服务，基本对应域名的结构： 根域名服务器（Root DNS Server）：管理顶级域名服务器，返回“com”“net”“cn”等顶级域名服务器的 IP 地址；（目前全世界共有 13 组根域名服务器，又有数百台的镜像，10个在美国，2个在欧洲，1个在日本。而中国只有3个根域名镜像服务器，DNS解析的结果最终还会汇总到根域名服务器上。） 顶级域名服务器（Top-level DNS Server）：管理各自域名下的权威域名服务器，比如 com 顶级域名服务器可以返回 apple.com 域名服务器的 IP 地址； 权威域名服务器（Authoritative DNS Server）：管理自己域名下主机的 IP 地址，比如 apple.com 权威域名服务器可以返回 www.apple.com 的 IP 地址。 例如，你要访问“www.apple.com”，就要进行下面的三次查询：访问根域名服务器，它会告诉你“com”顶级域名服务器的地址；访问“com”顶级域名服务器，它再告诉你“apple.com”域名服务器的地址；最后访问“apple.com”域名服务器，就得到了“www.apple.com”的地址。 减轻域名解析的压力： 缓存 许多大公司、网络运行商都会建立自己的 DNS 服务器，作为用户 DNS 查询的代理，代替用户访问核心 DNS 系统。这些“野生”服务器被称为“非权威域名服务器”，可以缓存之前的查询结果，如果已经有了记录，就无需再向根服务器发起查询，直接返回对应的 IP 地址。 这些 DNS 服务器的数量要比核心系统的服务器多很多，而且大多部署在离用户很近的地方。比较知名的 DNS 有 Google 的“8.8.8.8”，Microsoft 的“4.2.2.1”，还有 CloudFlare 的“1.1.1.1”等等。 操作系统里也会对 DNS 解析结果做缓存，如果你之前访问过“www.apple.com”，那么下一次在浏览器里再输入这个网址的时候就不会再跑到 DNS 那里去问了，直接在操作系统里就可以拿到 IP 地址。 操作系统里还有一个特殊的“主机映射”文件，通常是一个可编辑的文本，在 Linux 里是“/etc/hosts”，在 Windows 里是“C:\\WINDOWS\\system32\\drivers\\etc\\hosts”，如果操作系统在缓存里找不到 DNS 记录，就会找这个文件。 在 Nginx 里有这么一条配置指令“resolver”，它就是用来配置 DNS 服务器的，如果没有它，那么 Nginx 就无法查询域名对应的 IP，也就无法反向代理到外部的网站。 resolver 8.8.8.8 valid=30s; #指定Google的DNS，缓存30秒 域名新玩法 重定向 当主机有情况需要下线、迁移时，可以更改 DNS 记录，让域名指向其他的机器。比如，你有一台“buy.tv”的服务器要临时停机维护，那你就可以通知 DNS 服务器：“我这个 buy.tv 域名的地址变了啊，原先是 1.2.3.4，现在是 5.6.7.8，麻烦你改一下。”DNS 于是就修改内部的 IP 地址映射关系，之后再有访问 buy.tv 的请求就不走 1.2.3.4 这台主机，改由 5.6.7.8 来处理，这样就可以保证业务服务不中断。 搭建内部DNS 因为域名是一个名字空间，所以可以使用 bind9 等开源软件搭建一个在内部使用的 DNS，作为名字服务器。这样我们开发的各种内部服务就都用域名来标记，比如数据库服务都用域名“mysql.inner.app”，商品服务都用“goods.inner.app”，发起网络通信时也就不必再使用写死的 IP 地址了，可以直接用域名，而且这种方式也兼具了第一种“玩法”的优势。 基于域名实现的负载均衡 包含前两种玩法，有两种方式，且可以混用 第一种方式，因为域名解析可以返回多个 IP 地址，所以一个域名可以对应多台主机，客户端收到多个 IP 地址后，就可以自己使用轮询算法依次向服务器发起请求，实现负载均衡 第二种方式，域名解析可以配置内部的策略，返回离客户端最近的主机，或者返回当前服务质量最好的主机，这样在 DNS 端把请求分发到不同的服务器，实现负载均衡。 恶意DNS: “域名屏蔽”，对域名直接不解析，返回错误，让你无法拿到 IP 地址，也就无法访问网站； “域名劫持”，也叫“域名污染”，你要访问 A 网站，但 DNS 给了你 B 网站。 比如你有一个网站要上线，你在域名注册商那里申请了abc.com,那么你的域名A记录就保存在这个域名注册商的DNS服务器上，该DNS服务器称为权威域名服务器。当客户端访问abc.com时，先查找浏览器DNS缓存，没有则查找操作系统DNS缓存，在这一阶段是操作系统dnscache clinet 服务进行DNS缓存的（你在任务管理器里面可以看到一个dns客户端进程，就是这玩意实现缓存的），如果还是没有则查找hosts文件中的域名记录。然后依然没有的话则访问电脑上设置的DNS服务器IP，比如三大营运商的dns服务器或者谷歌的8.8.8.8，此时这一层的DNS服务器称为“野生DNS缓存服务器”，也就是非权威域名服务器。 如果还是没有则非权威域名服务器会去查找 根域名服务器-顶级域名服务器-二级域名服务器-权威域名服务器 ，这样客户端就在权威域名服务器上找到了abc.com对应的IP了，这个IP可以是多个，每次客户端请求的时候域名服务器会根据负载均衡算法分配一个IP给你。当DNS缓存失效了，则重新开始新一轮的域名请求。 总结如下： 浏览器缓存-\u003e操作系统dnscache -\u003ehosts文件-\u003e非权威域名服务器-\u003e根域名服务器-\u003e顶级域名服务器-\u003e（二级域名服务器）-\u003e权威域名服务器。 其中非权威域名服务器还包括LDNS（企业内网DNS服务器），三大营运商DNS，谷歌公开的DNS，微软公开的DNS等。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:6","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"搭建HTTP实验环境 Wireshark 著名的网络抓包工具，能够截获在 TCP/IP 协议栈中传输的所有流量，并按协议类型、地址、端口等任意过滤，功能非常强大，是学习网络协议的必备工具。 Chrome 它不仅上网方便，也是一个很好的调试器，对 HTTP/1.1、HTTPS、HTTP/2、QUIC 等的协议都支持得非常好，用 F12 打开“开发者工具”还可以非常详细地观测 HTTP 传输全过程的各种数据。不能观测 HTTP 传输的过程，只能看到结果。 Telnet 一个经典的虚拟终端，基于 TCP 协议远程登录主机，我们可以使用它来模拟浏览器的行为，连接服务器后手动发送 HTTP 请求，把浏览器的干扰也彻底排除，能够从最原始的层面去研究 HTTP 协议。 OpenResty 它是基于 Nginx 的一个“强化包”，里面除了 Nginx 还有一大堆有用的功能模块，不仅支持 HTTP/HTTPS，还特别集成了脚本语言 Lua 简化 Nginx 二次开发，方便快速地搭建动态网关，更能够当成应用容器来编写业务逻辑。它相当于 Nginx 的“超集”，功能更丰富，安装部署更方便。可以用 Lua 编写一些服务端脚本，实现简单的 Web 服务器响应逻辑，方便实验。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:7","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"在浏览器上访问网址 解析域名 浏览器从地址栏的输入中获得服务器的 IP 地址和端口号； 浏览器用 TCP 的三次握手与服务器建立连接； 浏览器向服务器发送拼好的报文； 服务器收到报文后处理请求，同样拼好报文再发给浏览器； 浏览器解析报文，渲染输出页面。 域名解析的过程中会有多级的缓存，浏览器首先看一下自己的缓存里有没有，如果没有就向操作系统的缓存要，还没有就检查本机域名解析文件 hosts 真实网络场景： CDN 会在 DNS 的解析过程中“插上一脚”。DNS 解析可能会给出 CDN 服务器的 IP 地址，这样你拿到的就会是 CDN 服务器而不是目标网站的实际地址。因为 CDN 会缓存网站的大部分资源，比如图片、CSS 样式表。由 PHP、Java 等后台服务动态生成的页面属于“动态资源”，CDN 无法缓存，只能从目标网站获取。 目标网站的服务器对外表现的是一个 IP 地址，但为了能够扛住高并发，在内部也是一套复杂的架构。通常在入口是负载均衡设备，例如四层的 LVS 或者七层的 Nginx，在后面是许多的服务器，构成一个更强更稳定的集群。 负载均衡设备会先访问系统里的缓存服务器，通常有 memory 级缓存 Redis 和 disk 级缓存 Varnish，它们的作用与 CDN 类似，不过是工作在内部网络里，把最频繁访问的数据缓存几秒钟或几分钟，减轻后端应用服务器的压力。 如果缓存服务器里也没有，那么负载均衡设备就要把请求转发给应用服务器了。这里就是各种开发框架大显神通的地方了，例如 Java 的 Tomcat/Netty/Jetty，Python 的 Django，还有 PHP、Node.js、Golang 等等。它们又会再访问后面的 MySQL、PostgreSQL、MongoDB 等数据库服务，实现用户登录、商品查询、购物下单、扣款支付等业务操作，然后把执行的结果返回给负载均衡设备，同时也可能给缓存服务器里也放一份 应用服务器的输出到了负载均衡设备这里，请求的处理就算是完成了，就要按照原路再走回去，还是要经过许多的路由器、网关、代理。如果这个资源允许缓存，那么经过 CDN 的时候它也会做缓存，这样下次同样的请求就不会到达源站了 最后网站的响应数据回到了你的设备，它可能是 HTML、JSON、图片或者其他格式的数据，需要由浏览器解析处理才能显示出来，如果数据里面还有超链接，指向别的资源，那么就又要重走一遍整个流程，直到所有的资源都下载完。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:8","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP报文 HTTP协议的核心部分在于它传输的报文内容。 报文结构 TCP 报文在实际要传输的数据之前附加了一个 20 字节的头部数据，存储 TCP 协议必须的额外信息，例如发送方的端口号、接收方的端口号、包序号、标志位等等。有了这个附加的 TCP 头，数据包才能够正确传输，到了目的地后把头部去掉，就可以拿到真正的数据。 HTTP 协议也是与 TCP/UDP 类似，同样也需要在实际传输的数据前附加一些头数据，不过与 TCP/UDP 不同的是，它是一个“纯文本”的协议，所以头数据都是 ASCII 码的文本，可以很容易地用肉眼阅读 HTTP 协议的请求报文和响应报文的结构基本相同，由三大部分组成： 起始行（start line）：描述请求或响应的基本信息； 头部字段集合（header）：使用 key-value 形式更详细地说明报文； 消息正文（entity）：实际传输的数据，它不一定是纯文本，可以是图片、视频等二进制数据。 这其中前两部分起始行和头部字段经常又合称为“请求头”或“响应头”，消息正文又称为“实体”，但与“header”对应，很多时候就直接称为“body” HTTP 协议规定报文必须有 header，但可以没有 body，而且在 header 之后必须要有一个“空行”，也就是“CRLF”，十六进制的“0D0A”。 请求行 请求行简要地描述了客户端想要如何操作服务器端的资源。 构成： 请求方法：是一个动词，如 GET/POST，表示对资源的操作； 请求目标：通常是一个 URI，标记了请求方法要操作的资源； 版本号：表示报文使用的 HTTP 协议版本。 这三个部分通常使用空格（space）来分隔，最后要用 CRLF 换行表示结束 状态行 状态行，顾名思义，服务器响应的状态。 构成： 版本号：表示报文使用的 HTTP 协议版本； 状态码：一个三位数，用代码的形式表示处理的结果，比如 200 是成功，500 是服务器错误，详细见文章附录； 原因：作为数字状态码补充，是更详细的解释文字，帮助人理解原因。 以空格分割，CRLF换行结束。 头部字段 请求行或状态行再加上头部字段集合就构成了 HTTP 报文里完整的请求头或响应头 头部字段是 key-value 的形式，key 和 value 之间用“:”分隔，最后用 CRLF 换行表示字段结束 HTTP 头字段非常灵活，不仅可以使用标准里的 Host、Connection 等已有头，也可以任意添加自定义头 使用头字段需要注意下面几点： 字段名不区分大小写，例如“Host”也可以写成“host”，但首字母大写的可读性更好； 字段名里不允许出现空格，可以使用连字符“-”，但不能使用下划线“_”。例如，“test-name”是合法的字段名，而“test name”“test_name”是不正确的字段名； 字段名后面必须紧接着“:”，不能有空格，而“:”后的字段值前可以有多个空格； 字段的顺序是没有意义的，可以任意排列不影响语义； 字段原则上不能重复，除非这个字段本身的语义允许，例如 Set-Cookie。 常用头字段 基本上可以分为四大类： 通用字段：在请求头和响应头里都可以出现； 请求字段：仅能出现在请求头里，进一步说明请求信息或者额外的附加条件； 响应字段：仅能出现在响应头里，补充说明响应报文的信息； 实体字段：它实际上属于通用字段，但专门描述 body 的额外信息。 对 HTTP 报文的解析和处理实际上主要就是对头字段的处理，理解了头字段也就理解了 HTTP 报文。 Host字段 属于请求字段，只能出现在请求头里，它同时也是唯一一个 HTTP/1.1 规范里要求必须出现的字段。Host 字段告诉服务器这个请求应该由哪个主机来处理，当一台计算机上托管了多个虚拟主机的时候，服务器端就需要用 Host 字段来选择，有点像是一个简单的“路由重定向” User-Agent 是请求字段，只出现在请求头里。它使用一个字符串来描述发起 HTTP 请求的客户端，服务器可以依据它来返回最合适此浏览器显示的页面。“诚实”的爬虫会在 User-Agent 里用“spider”标明自己是爬虫，所以可以利用这个字段实现简单的反爬虫策略 Date 字段 是一个通用字段，但通常出现在响应头里，表示 HTTP 报文创建的时间，客户端可以使用这个时间再搭配其他字段决定缓存策略。 Server 字段 是响应字段，只能出现在响应头里。它告诉客户端当前正在提供 Web 服务的软件名称和版本号 Server 字段也不是必须要出现的，因为这会把服务器的一部分信息暴露给外界，如果这个版本恰好存在 bug，那么黑客就有可能利用 bug 攻陷服务器 比如 GitHub，它的 Server 字段里就看不出是使用了 Apache 还是 Nginx，只是显示为“GitHub.com”。 Content-Length 实体字段，它表示报文里 body 的长度，也就是请求头或响应头空行后面数据的长度。服务器看到这个字段，就知道了后续有多少数据，可以直接接收。如果没有这个字段，那么 body 就是不定长的，需要使用 chunked 方式分段传输 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:9","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"请求方法 目前 HTTP/1.1 规定了八种方法，前四个比较常用，单词都必须是大写的形式： GET：获取资源，可以理解为读取或者下载数据； 请求从服务器获取资源，这个资源既可以是静态的文本、页面、图片、视频，也可以是由 PHP、Java 动态生成的页面或者其他格式的数据 搭配 URI 和其他头字段就能实现对资源更精细的操作： 在 URI 后使用“#”，就可以在获取页面后直接定位到某个标签所在的位置；使用 If-Modified-Since 字段就变成了“有条件的请求”，仅当资源被修改时才会执行获取动作；使用 Range 字段就是“范围请求”，只获取资源的一部分数据。 HEAD：获取资源的元信息； 服务器不会返回请求的实体数据，只会传回响应头，也就是资源的“元信息” 比如，想要检查一个文件是否存在，只要发个 HEAD 请求就可以了，没有必要用 GET 把整个文件都取下来。再比如，要检查文件是否有最新版本，同样也应该用 HEAD，服务器会在响应头里把文件的修改时间传回来。 POST：向资源提交数据，相当于写入或上传数据； 只要向服务器发送数据，用的大多数都是 POST PUT：类似 POST，但较少使用； 与 POST 存在微妙的不同，通常 POST 表示的是“新建”“create”的含义，而 PUT 则是“修改”“update”的含义。 DELETE：删除资源； 这个动作危险性太大，所以通常服务器不会执行真正的删除操作，而是对资源做一个删除标记。当然，更多的时候服务器就直接不处理 DELETE 请求。 CONNECT：建立特殊的连接隧道； 要求服务器为客户端和另一台远程服务器建立一条特殊的连接隧道，这时 Web 服务器在中间充当了代理的角色。 OPTIONS：列出可对资源实行的方法； 要求服务器列出可对资源实行的操作方法，在响应头的 Allow 字段里返回。它的功能很有限，用处也不大，有的服务器（例如 Nginx）干脆就没有实现对它的支持。 TRACE：追踪请求 - 响应的传输路径。 多用于对 HTTP 链路的测试或诊断，可以显示出请求 - 响应的传输路径。它的本意是好的，但存在漏洞，会泄漏网站的信息，所以 Web 服务器通常也是禁止使用。 也可以自定义请求方法。 在 HTTP 协议里，所谓的“安全”是指请求方法不会“破坏”服务器上的资源，即不会对服务器上的资源造成实质的修改。只有GET/HEAD是安全的。 所谓的“幂等”实际上是一个数学用语（一个运算*，如果对任意元x，x与自身运算等于自身，即x * x=x,则称该运算*满足幂等律），被借用到了 HTTP 协议里，意思是多次执行相同的操作，结果也都是相同的，即多次“幂”后结果“相等”。GET可以多次请求同一个资源，DELETE可以多次删除同一个资源，所以它们都是幂等的。POST不是幂等的，而PUT是幂等的。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:10","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"正确的网址 URI最常用形式： scheme，“方案名”或者“协议名”，表示资源应该使用哪种协议来访问。 :// “authority”，表示资源所在的主机名，通常的形式是“host:port”，即主机名加端口号 标记资源所在位置的 path /是UNIX的目录风格，也是path的一部分，Win的目录风格是\\ 客户端和服务器看到的URI是不一样的。客户端看到的必须是完整的 URI，使用特定的协议去连接特定的主机，而服务器看到的只是报文请求行里被删除了协议名和主机名的 URI。 query，用一个“?”开始，但不包含“?”，表示对资源附加的额外要求 查询参数 query 有一套自己的格式，是多个“key=value”的字符串，这些 KV 值用字符“\u0026”连接，浏览器和服务器都可以按照这个格式把长串的查询参数解析成可理解的字典或关联数组形式。 URI完整格式： 协议名之后、主机名之前的身份信息“user:passwd@”，表示登录主机时的用户名和密码，但现在已经不推荐使用这种形式了（RFC7230），不安全 查询参数后的片段标识符“#fragment”，它是 URI 所定位的资源内部的一个“锚点”或者说是“标签”，浏览器可以在获取资源后直接跳转到它指示的位置。 片段标识符仅能由浏览器这样的客户端使用，服务器是看不到的 URI会把ASCII 码以外的字符集和特殊字符进行转义，将其转换成十六进制字节值，然后前面再加上一个“%”。 eg: 空格被转义成“%20”，“?”被转义成“%3F”。而中文、日文等则通常使用 UTF-8 编码后再转义，例如“银河”会被转义成“%E9%93%B6%E6%B2%B3” ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:11","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"响应状态码 RFC 标准把状态码分成了五类，用数字的第一位表示分类，而 0~99 不用，这样状态码的实际可用范围就大大缩小了，由 000~999 变成了 100~599。 1××： 1××类状态码属于提示信息，是协议处理的中间状态，实际能够用到的时候很少。 我们偶尔能够见到的是“101 Switching Protocols”。它的意思是客户端使用 Upgrade 头字段，要求在 HTTP 协议的基础上改成其他的协议继续通信，比如 WebSocket。而如果服务器也同意变更协议，就会发送状态码 101，但这之后的数据传输就不会再使用 HTTP 了。 2××： 2××类状态码表示服务器收到并成功处理了客户端的请求。 “200 OK”是最常见的成功状态码，表示一切正常，如果是非 HEAD 请求，通常在响应头后都会有 body 数据。 “204 No Content”是另一个很常见的成功状态码，它的含义与“200 OK”基本相同，但响应头后没有 body 数据。所以对于 Web 服务器来说，正确地区分 200 和 204 是很必要的。 “206 Partial Content”是 HTTP 分块下载或断点续传的基础，在客户端发送“范围请求”、要求获取资源的部分数据时出现，它与 200 一样，也是服务器成功处理了请求，但 body 里的数据不是资源的全部，而是其中的一部分。 状态码 206 通常还会伴随着头字段“Content-Range”，表示响应报文里 body 数据的具体范围，供客户端确认，例如“Content-Range: bytes 0-99/2000”，意思是此次获取的是总计 2000 个字节的前 100 个字节。 3××： 3××类状态码表示客户端请求的资源发生了变动，客户端必须用新的 URI 重新发送请求获取资源，也就是通常所说的“重定向”，包括著名的 301、302 跳转。 “301 Moved Permanently”俗称“永久重定向”，含义是此次请求的资源已经不存在了，需要改用新的 URI 再次访问。 与它类似的是“302 Found”，曾经的描述短语是“Moved Temporarily”，俗称“临时重定向”，意思是请求的资源还在，但需要暂时用另一个 URI 来访问。 301 和 302 都会在响应头里使用字段 Location 指明后续要跳转的 URI，最终的效果很相似，浏览器都会重定向到新的 URI。两者的根本区别在于语义，一个是“永久”，一个是“临时”，所以在场景、用法上差距很大。 比如，你的网站升级到了 HTTPS，原来的 HTTP 不打算用了，这就是“永久”的，所以要配置 301 跳转，把所有的 HTTP 流量都切换到 HTTPS。 再比如，今天夜里网站后台要系统维护，服务暂时不可用，这就属于“临时”的，可以配置成 302 跳转，把流量临时切换到一个静态通知页面，浏览器看到这个 302 就知道这只是暂时的情况，不会做缓存优化，第二天还会访问原来的地址。 “304 Not Modified” 是一个比较有意思的状态码，它用于 If-Modified-Since 等条件请求，表示资源未修改，用于缓存控制。它不具有通常的跳转含义，但可以理解成“重定向已到缓存的文件”（即“缓存重定向”）。 301、302 和 304 分别涉及了 HTTP 协议里重要的“重定向跳转”和“缓存控制” 4××： 4××类状态码表示客户端发送的请求报文有误，服务器无法处理，它就是真正的“错误码”含义了。 “400 Bad Request”是一个通用的错误码，表示请求报文有错误，但具体是数据格式错误、缺少请求头还是 URI 超长它没有明确说，只是一个笼统的错误，客户端看到 400 只会是“一头雾水”“不知所措”。所以，在开发 Web 应用时应当尽量避免给客户端返回 400，而是要用其他更有明确含义的状态码。 “403 Forbidden”实际上不是客户端的请求出错，而是表示服务器禁止访问资源。原因可能多种多样，例如信息敏感、法律禁止等，如果服务器友好一点，可以在 body 里详细说明拒绝请求的原因，不过现实中通常都是直接给一个“闭门羹”。 “404 Not Found”可能是我们最常看见也是最不愿意看到的一个状态码，它的原意是资源在本服务器上未找到，所以无法提供给客户端。但现在已经被“用滥了”，只要服务器“不高兴”就可以给出个 404，而我们也无从得知后面到底是真的未找到，还是有什么别的原因，某种程度上它比 403 还要令人讨厌。 4××里剩下的一些代码较明确地说明了错误的原因，都很好理解，开发中常用的有： 405 Method Not Allowed：不允许使用某些方法操作资源，例如不允许 POST 只能 GET； 406 Not Acceptable：资源无法满足客户端请求的条件，例如请求中文但只有英文； 408 Request Timeout：请求超时，服务器等待了过长的时间； 409 Conflict：多个请求发生了冲突，可以理解为多线程并发时的竞态； 413 Request Entity Too Large：请求报文里的 body 太大； 414 Request-URI Too Long：请求行里的 URI 太大； 429 Too Many Requests：客户端发送了太多的请求，通常是由于服务器的限连策略； 431 Request Header Fields Too Large：请求头某个字段或总体太大； 5××： 表示客户端请求报文正确，但服务器在处理时内部发生了错误，无法返回应有的响应数据，是服务器端的“错误码”。 “500 Internal Server Error”与 400 类似，也是一个通用的错误码，服务器究竟发生了什么错误我们是不知道的。不过对于服务器来说这应该算是好事，通常不应该把服务器内部的详细信息，例如出错的函数调用栈告诉外界。虽然不利于调试，但能够防止黑客的窥探或者分析。 “501 Not Implemented”表示客户端请求的功能还不支持，这个错误码比 500 要“温和”一些，和“即将开业，敬请期待”的意思差不多，不过具体什么时候“开业”就不好说了。 “502 Bad Gateway”通常是服务器作为网关或者代理时返回的错误码，表示服务器自身工作正常，访问后端服务器时发生了错误，但具体的错误原因也是不知道的。 “503 Service Unavailable”表示服务器当前很忙，暂时无法响应服务，我们上网时有时候遇到的“网络服务正忙，请稍后重试”的提示信息就是状态码 503。 503 是一个“临时”的状态，很可能过几秒钟后服务器就不那么忙了，可以继续提供服务，所以 503 响应报文里通常还会有一个“Retry-After”字段，指示客户端可以在多久以后再次尝试发送请求。 目前 RFC 标准里总共有 41 个状态码，但状态码的定义是开放的，允许自行扩展。所以 Apache、Nginx 等 Web 服务器都定义了一些专有的状态码。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:12","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP/1.1的特点 灵活可拓展 比如自定义头部字段、传输的实体数据可缓存可压缩、可分段获取数据、支持身份认证、支持国际化语言等 可靠传输 HTTP是基于TCP/IP的，TCP是可靠的 具体做法与 TCP/UDP 差不多，都是对实际传输的数据（entity）做了一层包装，加上一个头，然后调用 Socket API，通过 TCP/IP 协议栈发送或者接收。 应用层协议 自TCP/IP诞生后，出现了不少应用较为局限的应用层协议，如FTP 只能传输文件、SMTP 只能发送邮件、SSH 只能远程登录等 HTTP 凭借着可携带任意头字段和实体数据的报文结构，以及连接控制、缓存代理等方便易用的特性，在通用数据传输方面技压群雄 请求-应答 这是HTTP最根本的通信模型，契合了传统的 C/S（Client/Server）系统架构以及后来的B/S架构。 请求 - 应答模式也完全符合 RPC（Remote Procedure Call）的工作模式，可以把 HTTP 请求处理封装成远程函数调用，导致了 WebService、RESTful 和 gRPC 等的出现。 无状态 客户端和服务器永远是处在一种“无知”的状态。建立连接前两者互不知情，每次收发的报文也都是互相独立的，没有任何的联系 “没有记忆能力”,不需要额外的资源来记录状态信息，能减轻服务器的负担 不能记忆，就无法支持需要连续多个步骤的“事务”操作。例如电商购物，首先要登录，然后添加购物车，再下单、结算、支付，这一系列操作都需要知道用户的身份才行，但“无状态”服务器是不知道这些请求是相互关联的，每次都得问一遍身份信息，不仅麻烦，而且还增加了不必要的数据传输量，当然现在以及有好些解决方式了。 “无状态”也表示服务器都是相同的，没有“状态”的差异，所以可以很容易地组成集群，让负载均衡把请求转发到任意一台服务器 对比一下 UDP 协议，不过它是无连接也无状态的，顺序发包乱序收包，数据包发出去后就不管了，收到后也不会顺序整理。而 HTTP 是有连接无状态，顺序发包顺序收包，按照收发的顺序管理报文。 明文 协议里的报文（准确地说是 header 部分）不使用二进制数据，而是用简单可阅读的文本形式。对比 TCP、UDP 这样的二进制协议，它的优点显而易见，不需要借助任何外部工具，用浏览器、Wireshark 或者 tcpdump 抓包后，直接用肉眼就可以很容易地查看或者修改 HTTP 报文的所有信息会暴露。免费WIFI陷阱，一旦你连上了这个 WiFi 热点，所有的流量都会被截获保存。 不安全 与明文有重合 明文只是“机密”方面的一个缺点，在“身份认证”和“完整性校验”这两方面 HTTP 也是欠缺的。 为了解决 HTTP 不安全的缺点，所以就出现了 HTTPS 性能 “队头阻塞”（Head-of-line blocking），当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一并被阻塞，会导致客户端迟迟收不到数据。解决方案：HTTP/2 和 HTTP/3 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:1:13","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"进阶 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:2:0","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP的实体数据 数据类型与编码 TCP、UDP 是传输层的协议，它们不会关心 body 数据是什么，只要把数据发送到对方就算是完成了任务。 HTTP作为应用层的协议需要告诉上层接收到的是什么数据。HTTP 协议诞生之前就已经有了针对这种问题的解决方案，不过它是用在电子邮件系统里的，让电子邮件可以发送 ASCII 码以外的任意数据，方案的名字叫做“多用途互联网邮件扩展”（Multipurpose Internet Mail Extensions），简称为 MIME。MIME 把数据分成了八大类，每个大类下再细分出多个子类，形式是“type/subtype”的字符串。 HTTP取了MIME标准规范中的一部分，用来标记 body 的数据类型，这就是我们平常总能听到的“MIME type”。 在 HTTP 里经常遇到的几个类别： text：即文本格式的可读数据，我们最熟悉的应该就是 text/html 了，表示超文本文档，此外还有纯文本 text/plain、样式表 text/css 等。 image：即图像文件，有 image/gif、image/jpeg、image/png 等。 audio/video：音频和视频数据，例如 audio/mpeg、video/mp4 等。 application：数据格式不固定，可能是文本也可能是二进制，必须由上层应用程序来解释。常见的有 application/json，application/javascript、application/pdf 等，另外，如果实在是不知道数据是什么类型，就会是 application/octet-stream，即不透明的二进制数据。 HTTP 在传输时为了节约带宽，有时候还会压缩数据，需要有一个“Encoding type”，告诉数据是用的什么编码格式，这样对方才能正确解压缩，还原出原始的数据。 比MIME type要少，常用的Encoding type只有下面三种： gzip：GNU zip 压缩格式，也是互联网上最流行的压缩格式； deflate：zlib（deflate）压缩格式，流行程度仅次于 gzip； br：一种专门为 HTTP 优化的新压缩算法（Brotli）。 数据类型使用的头字段 HTTP 协议定义了两个 Accept 请求头字段和两个 Content 实体头字段，用于客户端和服务器进行“内容协商”。也就是说，客户端用 Accept 头告诉服务器希望接收什么样的数据，而服务器用 Content 头告诉客户端实际发送了什么样的数据。 Accept 字段标记的是客户端可理解的 MIME type，可以用“,”做分隔符列出多个类型，让服务器有更多的选择，例如: Accept: text/html,application/xml,image/webp,image/png 服务器会在响应报文里用头字段 Content-Type 告诉实体数据的真实类型：（注意：content-type是通用字段，请求头也可以用，比如post时会用） Content-Type: text/html Content-Type: image/png 浏览器看到报文里的类型是“text/html”就知道是 HTML 文件，会调用排版引擎渲染出页面，看到“image/png”就知道是一个 PNG 文件，就会在页面上显示出图像 Accept-Encoding 字段标记的是客户端支持的压缩格式，例如上面说的 gzip、deflate 等，同样也可以用“,”列出多个，服务器可以选择其中一种来压缩数据，实际使用的压缩格式放在响应头字段 Content-Encoding 里。这两个字段是可以省略的，如果请求报文里没有 Accept-Encoding 字段，就表示客户端不支持压缩数据；如果响应报文里没有 Content-Encoding 字段，就表示响应数据没有被压缩。 语言类型与编码 en 表示任意的英语，en-US 表示美式英语，en-GB 表示英式英语，而 zh-CN 表示汉语。 遵循 UTF-8 字符编码方式的 Unicode 字符集是互联网上的标准字符集。 Accept-Language 字段标记了客户端可理解的自然语言，也允许用“,”做分隔符列出多个类型，例如： Accept-Language: zh-CN, zh, en 同样的（一般不会发，因为语言完全可以由字符集推断出来）： Content-Language: zh-CN 需要注意的是，字符集在 HTTP 里使用的请求头字段是 Accept-Charset（虽然这个头字段基本用不着，现在浏览器基本支持多种字符集），但响应头里却没有对应的 Content-Charset，而是在 Content-Type 字段的数据类型后面用“charset=xxx”来表示： Accept-Charset: gbk, utf-8 Content-Type: text/html; charset=utf-8 用以上请求头请求头字段进行内容协商的时候，还可以用一种特殊的“q”参数表示权重来设定优先级，这里的“q”是“quality factor”的意思。最大值是 1，最小值是 0.01，默认值是 1，如果值是 0 就表示拒绝。具体的形式是在数据类型或语言代码后面加一个“;”，然后是“q=value” 注意：在大多数编程语言里“;”的断句语气要强于“,”，而在 HTTP 的内容协商里却恰好反了过来，“;”的意义是小于“,”的。 Accept: text/html,application/xml;q=0.9,*/*;q=0.8 内容协商的结果 内容协商的过程是不透明的，每个 Web 服务器使用的算法都不一样。 服务器可以在响应头里多加一个 Vary 字段，记录服务器在内容协商时参考的请求头字段，给出一点信息，例如： Vary: Accept-Encoding,User-Agent,Accept ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:2:1","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP传输大文件的方法 数据压缩： 有个缺点，gzip 等压缩算法通常只对文本文件有较好的压缩率，而图片、音频视频等多媒体数据本身就已经是高度压缩的，再用 gzip 处理也不会变小（甚至还有可能会增大一点），所以它就失效了 压缩文本效果不错，在 Nginx 里就会使用“gzip on”指令，启用对“text/html”的压缩。 分块传输： HTTP 协议里的“chunked”分块传输编码，在响应报文里用头字段“Transfer-Encoding: chunked”来表示（注意：和和“Content-Length”不能同时出现），意思是报文里的 body 部分不是一次性发过来的，而是分成了许多的块（chunk）逐个发送。 分块传输也可以用于“流式数据”，例如由数据库动态生成的表单页面，这种情况下 body 数据的长度是未知的，无法在头字段“Content-Length”里给出确切的长度，所以也只能用 chunked 方式分块发送。 分块传输的编码规则： 分块传输也有局限性，用看视频快进来理解就明白了。为了满足需求，HTTP协议提出了“范围请求”（这个功能是非必须的，web服务器支持范围请求便需要在响应头里添加字段“Accept-Ranges: bytes”，不支持可以用“Accept-Ranges: none”或者不用该字段），允许客户端在请求头里使用专用字段来表示只获取文件的一部分 请求头 Range 是 HTTP 范围请求的专用字段，格式是“bytes=x-y”，其中的 x 和 y 是以字节为单位的数据范围。（也可以请求多个范围，即多个x-y，见后文） 关于Range的格式：假设文件是 100 个字节，那么： “0-”表示从文档起点到文档终点，相当于“0-99”，即整个文件； “10-”是从第 10 个字节开始到文档末尾，相当于“10-99”； “-1”是文档的最后一个字节，相当于“99-99”； “-10”是从文档末尾倒数 10 个字节，相当于“90-99” 服务器收到 Range 字段后，需要做四件事: 检查范围是否合法 范围正确，服务器就可以根据 Range 头计算偏移量，读取文件的片段了，返回状态码“206 Partial Content”，和 200 的意思差不多，但表示 body 只是原数据的一部分。 添加一个响应头字段 Content-Range，告诉片段的实际偏移量和资源的总大小，格式是“bytes x-y/length”，与 Range 头区别在没有“=”，范围后多了总长度。例如，对于“0-10”的范围请求，值就是“bytes 0-10/100”。 发送数据，直接把片段用 TCP 发给客户端 除了视频拖拽，常用的下载工具里的多段下载、断点续传也是基于范围请求实现的。 多范围请求要使用一种特殊的 MIME 类型：“multipart/byteranges”，表示报文的 body 是由多段字节序列组成的，并且还要用一个参数“boundary=xxx”给出段之间的分隔标记。见下图： 比如： …… Range: bytes=0-9, 20-29 …… --00000000001 Content-Type: text/plain Content-Range: bytes 0-9/96 // this is --00000000001 Content-Type: text/plain Content-Range: bytes 20-29/96 ext json d --00000000001-- ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:2:2","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP的连接管理 短连接： 早期的HTTP（0.9/1.0）是短连接的，数据传输基于 TCP/IP，每次发送请求前需要先与服务器建立连接，收到响应报文后会立即关闭连接。 在 TCP 协议里，建立连接和关闭连接都是非常“昂贵”的操作。TCP 建立连接要有“三次握手”，发送 3 个数据包，需要 1 个 RTT；关闭连接是“四次挥手”，4 个数据包需要 2 个 RTT（RTT（round-trip time）：一个小的分组从客户端到服务器，在回到客户端的时间（传输时间忽略）） 长连接： 也叫“持久连接”（persistent connections）、“连接保活”（keep alive）、“连接复用”（connection reuse）。 在 HTTP/1.1 中的连接都会默认启用长连接。不需要用什么特殊的头字段指定，只要向服务器发送了第一次请求，后续的请求都会重复利用第一次打开的 TCP 连接，也就是长连接，在这个连接上收发数据。 也可以在请求头里明确地要求使用长连接机制，使用的字段是 Connection，值是“keep-alive”。 如果服务器支持长连接，它总会在响应报文里放一个“Connection: keep-alive”字段 关闭连接：在客户端，可以在请求头里加上“Connection: close”字段，服务端看到后在响应报文里也加上这个字段，发送之后就调用 Socket API 关闭 TCP 连接。 服务器端通常不会主动关闭连接，拿 Nginx 来举例，它有两种方式关闭连接： 使用“keepalive_timeout”指令，设置长连接的超时时间，如果在一段时间内连接上没有任何数据收发就主动断开连接 使用“keepalive_requests”指令，设置长连接上可发送的最大请求次数。 队头阻塞：head-of-line blocking 与短连接和长连接无关，而是由 HTTP 基本的“请求 - 应答”模型所导致的。 HTTP 规定报文必须是“一发一收”，这就形成了一个先进先出的“串行”队列。队列里的请求没有轻重缓急的优先级，只有入队的先后顺序，排在最前面的请求被最优先处理。 性能优化： “并发连接”（concurrent connections），也就是同时对一个域名发起多个长连接 缺陷：每个客户端都建立多个连接，会将资源服务器榨干，或者“拒绝服务” RFC2616 里明确限制每个客户端最多并发 2 个连接。不过实践证明这个数字实在是太小了，众多浏览器都无视标准，把这个上限提高到了 6~8。后来修订的 RFC7230 也就取消了这个“2”的限制。 “域名分片”（domain sharding） 多个域名指向同一个服务器。避免被HTTP和浏览器限制，但依旧在增加服务器负担。 注意：上面是指HTTP层次的队首阻塞，而tcp层次的队首阻塞的原因：引自《web性能权威指南》 每个 TCP 分组都会带着一个唯一的序列号被发出，而所有分组必须按顺序传送到接收端。 如果中途有一个分组没能到达接收端，那么后续分组必须保存到接收端的 TCP 缓冲区，等待丢失的分组重发并到达接收端。 这一切都发生在 TCP 层，应用程序对 TCP 重发和缓冲区中排队的分组一无所知，必须等待分组全部到达才能访问数据。 在此之前，应用程序只能在通过套接字读数据时感觉到延迟交互。这种效应称为 TCP 的队首阻塞。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:2:3","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP的重定向以及跳转 主动跳转：点击文档里的一个链接 浏览器发起，浏览器首先要解析链接文字里的 URI 再用这个 URI 发起一个新的 HTTP 请求，获取响应报文后就会切换显示内容，渲染出新 URI 指向的页面。 被动跳转： 服务器发起，在HTTP里也叫做“重定向”（Redirection） 重定向是“用户无感知”的。 重定向： “Location”字段属于响应字段，必须出现在响应报文里。但只有配合 301/302 状态码才有意义，它标记了服务器要求重定向的 URI，这里就是要求浏览器跳转到“index.html”。 浏览器收到 301/302 报文，会检查响应头里有没有“Location”。如果有，就从字段值里提取出 URI，发出新的 HTTP 请求 在“Location”里的 URI 既可以使用绝对 URI，也可以使用相对 URI（如果是站内跳转的话），相对 URI 从从请求上下文里计算得到。 重定向状态码： 内部重定向对用户来说是透明的，所以HTTP状态码是200 301 永久重定向（Moved Permanently），意思是原 URI 已经“永久”性地不存在了，今后的所有请求都必须改用新的 URI。 302 临时重定向 其他 303 See Other：类似 302，但要求重定向后的请求改为 GET 方法，访问一个结果页面，避免 POST/PUT 重复操作； 307 Temporary Redirect：类似 302，但重定向后请求里的方法和实体不允许变动，含义比 302 更明确； 308 Permanent Redirect：类似 307，不允许重定向后的请求变动，但它是 301“永久重定向”的含义。 重定向应用场景： “资源不可用”，需要用另一个新的 URI 来代替。例如域名变更、服务器变更、网站改版、系统维护，这些都会导致原 URI 指向的资源无法访问，为了避免出现 404，就需要用重定向跳转到新的 URI。 “避免重复”，让多个网址都跳转到一个 URI，增加访问入口的同时还不会增加额外的工作量。 永久和临时的选择。 重定向的一些问题： “性能损耗”。重定向的机制决定了一个跳转会有两次请求 - 应答，比正常的访问多了一次。（这里默认了外部重定向，内部重定向只有一次） “循环跳转” HTTP 协议特别规定，浏览器必须具有检测“循环跳转”的能力，在发现这种情况时应当停止发送请求并给出错误提示。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:2:4","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP的cookie机制 Cookie的工作过程： 响应头字段 Set-Cookie 和请求头字段 Cookie 当用户通过浏览器第一次访问服务器的时候，要创建一个独特的身份标识数据，格式是“key=value”，然后放进 Set-Cookie 字段里，随着响应报文一同发给浏览器 浏览器收到响应报文，看到里面有 Set-Cookie，知道这是服务器给的身份标识，保存起来，下次再请求的时候就自动把这个值放进 Cookie 字段里发给服务器 第二次请求里面有了 Cookie 字段，就可以拿出 Cookie 里的值，识别出用户的身份 服务器有时会在响应头里添加多个 Set-Cookie，存储多个“key=value”。浏览器发送时不需要用多个 Cookie 字段，只要在一行里用“;”隔开就行。 Cookie 是由浏览器负责存储的。 可见，Cookie 就是服务器委托浏览器存储在客户端里的一些数据，而这些数据通常都会记录用户的关键识别信息。 设置Cookie属性： 设置 Cookie 的生存周期 使用 Expires 和 Max-Age 两个属性来设置。 “Expires”俗称“过期时间”，用的是绝对时间点，可以理解为“截止日期”（deadline）。“Max-Age”用的是相对时间，单位是秒，浏览器用收到报文的时间点再加上 Max-Age，就可以得到失效的绝对时间。二者可以同时出现，浏览器会优先采用 Max-Age 计算失效期。 设置 Cookie 的作用域 “Domain”和“Path”指定了 Cookie 所属的域名和路径，浏览器在发送 Cookie 前会从 URI 中提取出 host 和 path 部分，对比 Cookie 的属性。如果不满足条件，就不会在请求头里发送 Cookie。 Cookie 的安全性 属性“HttpOnly”会告诉浏览器，此 Cookie 只能通过浏览器 HTTP 协议传输，禁止其他方式访问，浏览器的 JS 引擎就会禁用 document.cookie 等一切相关的 API。 属性“SameSite”可以防范“跨站请求伪造”（XSRF）攻击，设置成“SameSite=Strict”可以严格限定 Cookie 不能随着跳转链接跨站发送，而“SameSite=Lax”则略宽松一点，允许 GET/HEAD 等安全方法，但禁止 POST 跨站发送。 “Secure”，表示这个 Cookie 仅能用 HTTPS 协议加密传输，明文的 HTTP 协议会禁止发送。但 Cookie 本身不是加密的，浏览器里还是以明文的形式存在 Cookie应用： 身份识别 广告跟踪 一些网站的页面里会嵌入很多广告代码，里面就会访问广告商，传给浏览器的cookie都带有一定的行为分析，从而你访问其他网站时，那些广告商网站也能拿到你具有行为分析的的Cookie，从而个性化广告。 虽然现在已经出现了多种 Local Web Storage 技术，能够比 Cookie 存储更多的数据，但 Cookie 仍然是最通用、兼容性最强的客户端数据存储手段。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:2:5","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP的缓存控制 实际上，HTTP 传输的每一个环节基本上都会有缓存，非常复杂。 基于“请求 - 应答”模式的特点，可以大致分为客户端缓存和服务器端缓存，因为服务器端缓存经常与代理服务“混搭”在一起 服务器的缓存控制： 浏览器发现缓存无数据，于是发送请求，向服务器获取资源； 服务器响应请求，返回资源，同时标记资源的有效期； 服务器标记资源有效期使用的头字段是“Cache-Control”，里面的值“max-age=30”就是资源的有效时间，告诉浏览器，“这个页面只能缓存 30 秒”，这个max-age是生存时间，从服务器发出资源就开始计算，区别于Cookie的生存周期。 除了max-age，在响应报文里还可以用其他的属性来更精确地指示浏览器应该如何使用缓存： no-store：不允许缓存，用于某些变化非常频繁的数据，例如秒杀页面； no-cache：它的字面含义容易与 no-store 搞混，实际的意思并不是不允许缓存，而是可以缓存，但在使用之前必须要去服务器验证是否过期，是否有最新的版本； must-revalidate：又是一个和 no-cache 相似的词，它的意思是如果缓存不过期就可以继续使用，但过期了如果还想用就必须去服务器验证。 浏览器缓存资源，等待下次重用。 浏览器的缓存控制： Cache-Control也可以使用在请求报文的头字段里面，比如你点击刷新的时候 当你点“刷新”按钮的时候，浏览器会在请求头里加一个“Cache-Control: max-age=0” Ctrl+F5 的“强制刷新”其实就是发了一个“Cache-Control: no-cache”，含义和“max-age=0”基本一样 点一下浏览器的“前进”“后退”按钮，再看开发者工具，发现“from disk cache”的字样，意思是没有发送网络请求，而是读取的磁盘上的缓存。 在“前进”“后退”“跳转”这些重定向动作中浏览器只用最基本的请求头，没有“Cache-Control”，所以就会检查缓存，直接利用本地缓存资源，不再进行网络通信。 条件请求： 浏览器可以用两个连续的请求组成“验证动作”：先是一个 HEAD，获取资源的修改时间等元信息，然后与缓存数据比较，如果没有改动就使用缓存，节省网络流量，否则就再发一个 GET 请求，获取最新的版本。 但是，这两个请求网络成本太高了，所以 HTTP 协议就定义了一系列“If”开头的“条件请求”字段，专门用来检查验证资源是否过期，把两个请求才能完成的工作合并在一个请求里做。而且，验证的责任也交给服务器。 条件请求一共有 5 个头字段，我们最常用的是“if-Modified-Since”和“If-None-Match”这两个。需要第一次的响应报文预先提供“Last-modified”和“ETag”，然后第二次请求时就可以带上缓存里的原值，验证资源是否是最新的。如果资源没有变，服务器就回应一个“304 Not Modified”，表示缓存依然有效，浏览器就可以更新一下有效期，然后继续使用缓存了。 ETag 是“实体标签”（Entity Tag）的缩写，是资源的一个唯一标识，主要是用来解决修改时间无法准确区分文件变化的问题。有些文件修改了时间变化了，但内容并未改变，便无需传给浏览器。因此使用 ETag 可以精确地识别资源的变动情况 强 ETag 要求资源在字节级别必须完全相符，弱 ETag 在值前有个“W/”标记，只要求资源在语义上没有变化，但内部可能会有部分发生了改变（例如 HTML 里的标签顺序调整，或者多了几个空格）。 强制刷新会清空请求头里的 If-Modified-Since 和 If-None-Match 所以会返回最新数据 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:2:6","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP的代理服务 代理有很多的种类，例如匿名代理、透明代理、正向代理和反向代理。 代理的作用： 负载均衡 代理服务器在面向客户端时屏蔽了源服务器，客户端看到的只是代理服务器，源服务器究竟有多少台、是哪些 IP 地址都不知道。于是代理服务器就可以掌握请求分发的“大权”，决定由后面的哪台服务器来响应请求。 常用的负载均衡算法比如轮询、一致性哈希、随机、最近最少使用、链接最少，它们的目标都是尽量把外部的流量合理地分散到多台源服务器，提高系统的整体资源利用率和性能。 健康检查：使用“心跳”等机制监控后端服务器，发现有故障就及时“踢出”集群，保证服务高可用； 安全防护：保护被代理的后端服务器，限制 IP 地址或流量，抵御网络攻击和过载； 加密卸载：对外网使用 SSL/TLS 加密通信认证，而在安全的内网不加密，消除加解密成本； 数据过滤：拦截上下行的数据，任意指定策略修改请求或者响应； 内容缓存：暂存、复用服务器响应 代理相关头字段： 代理服务器需要用字段“Via”标明代理的身份。Via 是一个通用字段，请求头或响应头里都可以出现。每当报文经过一个代理节点，代理服务器就会把自身的信息追加到字段的末尾。“Via: proxy1, proxy2” 服务器的 IP 地址应该是保密的，关系到企业的内网安全，所以一般不会让客户端知道，而通常服务器需要知道客户端的真实 IP 地址，方便做访问控制、用户画像、统计分析。 HTTP 标准里并没有为此定义头字段，但已经出现了很多“事实标准”，最常用的两个头字段是“X-Forwarded-For”和“X-Real-IP”。 “X-Forwarded-For”的字面意思是“为谁而转发”，形式上和“Via”差不多，也是每经过一个代理节点就会在字段里追加一个信息。但“Via”追加的是代理主机名（或者域名），而“X-Forwarded-For”追加的是请求方的 IP 地址。所以，在字段里最左边的 IP 地址就是客户端的地址。 “X-Real-IP”是另一种获取客户端真实 IP 的手段，它的作用很简单，就是记录客户端 IP 地址，没有中间的代理信息，相当于是“X-Forwarded-For”的简化版。如果客户端和源服务器之间只有一个代理，那么这两个字段的值就是相同的。 X-Forwarded-Host”和“X-Forwarded-Proto”，它们的作用与“X-Real-IP”类似，只记录客户端的信息，分别是客户端请求的原始域名和原始协议名。 代理协议： 有了“X-Forwarded-For”等头字段，源服务器就可以拿到准确的客户端信息了。但对于代理服务器来说它并不是一个最佳的解决方案。 因为通过“X-Forwarded-For”操作代理信息必须要解析 HTTP 报文头，这对于代理来说成本比较高，原本只需要简单地转发消息就好，而现在却必须要费力解析数据再修改数据，会降低代理的转发性能。 另一个问题是“X-Forwarded-For”等头必须要修改原始报文，而有些情况下是不允许甚至不可能的（比如使用 HTTPS 通信被加密）。 所以就出现了一个专门的“代理协议”（The PROXY protocol），它由知名的代理软件 HAProxy 所定义，也是一个“事实标准”，被广泛采用（注意并不是 RFC）。 “代理协议”有 v1 和 v2 两个版本，v1 和 HTTP 差不多，也是明文，而 v2 是二进制格式。 v1在 HTTP 报文前增加了一行 ASCII 码文本，相当于又多了一个头。这一行文本其实非常简单，开头必须是“PROXY”五个大写字母，然后是“TCP4”或者“TCP6”，表示客户端的 IP 地址类型，再后面是请求方地址、应答方地址、请求方端口号、应答方端口号，最后用一个回车换行（\\r\\n）结束。 服务器看到这样的报文，只要解析第一行就可以拿到客户端地址,不过代理协议并不支持“X-Forwarded-For”的链式地址形式，所以拿到客户端地址后再如何处理就需要代理服务器与后端自行约定。 PROXY TCP4 1.1.1.1 2.2.2.2 55555 80\\r\\n GET / HTTP/1.1\\r\\n Host: www.xxx.com\\r\\n \\r\\n ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:2:7","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP的缓存代理 支持缓存控制的代理服务。 HTTP 传输链路上，不只是客户端有缓存，服务器上的缓存也是非常有价值的，可以让请求不必走完整个后续处理流程，“就近”获得响应结果。特别是对于那些“读多写少”的数据，例如突发热点新闻、爆款商品的详情页，一秒钟内可能有成千上万次的请求。即使仅仅缓存数秒钟，也能够把巨大的访问流量挡在外面，让 RPS（request per second）降低好几个数量级，减轻应用服务器的并发压力，对性能的改善是非常显著的。 HTTP 的服务器缓存功能主要由代理服务器来实现（即缓存代理），而源服务器系统内部虽然也经常有各种缓存（如 Memcache、Redis、Varnish 等），但与 HTTP 没有太多关系。 缓存代理服务收到源服务器发来的响应数据后需要做两件事。第一个是把报文转发给客户端，而第二个是把报文存入自己的 Cache 里。下一次再有相同的请求，代理服务器就可以直接发送 304 或者缓存数据，不必再从源服务器那里获取 源服务器的缓存控制： 服务器端的“Cache-Control”属性：max-age、no-store、no-cache 和 must-revalidate，这 4 种缓存属性可以约束客户端，也可以约束代理。 要区分客户端上的缓存和代理上的缓存，可以使用两个新属性“private”和“public”。 比如你登录论坛，返回的响应报文里用“Set-Cookie”添加了论坛 ID，这就属于私人数据，不能存在代理上。 缓存失效后的重新验证也要区分开（即使用条件请求“Last-modified”和“ETag”），“must-revalidate”是只要过期就必须回源服务器验证，而新的“proxy-revalidate”只要求代理的缓存过期后必须验证，客户端不必回源，只验证到代理这个环节就行了。 缓存的生存时间可以使用新的“s-maxage”（s 是 share 的意思，注意 maxage 中间没有“-”），只限定在代理上能够存多久，而客户端仍然使用“max-age” 一个代理专用的属性“no-transform”。代理有时候会对缓存下来的数据做一些优化，比如把图片生成 png、webp 等几种格式，方便今后的请求处理，而“no-transform”就会禁止这样做 源服务器在设置完“Cache-Control”后必须要为报文加上“Last-modified”或“ETag”字段。否则，客户端和代理后面就无法使用条件请求来验证缓存是否有效，也就不会有 304 缓存重定向。 完整服务器端缓存控制策略： 客户端的缓存控制： 关于缓存的生存时间，多了两个新属性“max-stale”和“min-fresh” “max-stale”的意思是如果代理上的缓存过期了也可以接受，但不能过期太多，超过 x 秒也会不要。 比如，草莓上贴着标签“max-age=5”，现在已经在冰柜里存了 7 天。如果有请求“max-stale=2”，意思是过期两天也能接受。 “min-fresh”的意思是缓存必须有效，而且必须在 x 秒后依然有效。 有的时候客户端还会发出一个特别的“only-if-cached”属性，表示只接受代理缓存的数据，不接受源服务器的响应。如果代理上没有缓存或者缓存过期，就应该给客户端返回一个 504（Gateway Timeout） ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:2:8","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTPS ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:3:0","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTPS? SSL/TLS? HTTP的“明文”以及“不安全”需要HTTPS来解决。 在HTTP协议下，代理服务可以在数据上下行的时候可以添加或删除部分头字段，也可以使用黑白名单过滤 body 里的关键字，甚至直接发送虚假的请求、响应，而浏览器和源服务器都没有办法判断报文的真伪。 安全的通信过程： 机密性 完整性 身份认证 不可否认 HTTPS 把 HTTP 下层的传输协议由 TCP/IP 换成了 SSL/TLS，由“HTTP over TCP/IP”变成了“HTTP over SSL/TLS”，让 HTTP 运行在了安全的 SSL/TLS 协议上，收发报文不再使用 Socket API，而是调用专门的安全接口。而原先的TCP/IP层位于SSL/TLS层的下一层。 SSL/TLS SSL 即安全套接层（Secure Sockets Layer），在 OSI 模型中处于第 5 层（会话层） （SSL 发展到 v3 时已经证明了它自身是一个非常好的安全通信协议，于是互联网工程组 IETF 在 1999 年把它改名为 TLS（传输层安全，Transport Layer Security），正式标准化，版本号从 1.0 重新算起，所以 TLS1.0 实际上就是 SSLv3.1。） TLS 由记录协议、握手协议、警告协议、变更密码规范协议、扩展协议等几个子协议组成，综合使用了对称加密、非对称加密、身份认证等许多密码学前沿技术。 浏览器和服务器在使用 TLS 建立连接时需要选择一组恰当的加密算法来实现安全通信，这些算法的组合被称为“密码套件”（cipher suite，也叫加密套件）。 eg:“ECDHE-RSA-AES256-GCM-SHA384” 解释：“握手时使用 ECDHE 算法进行密钥交换，用 RSA 签名和身份认证，握手后的通信使用 AES 对称算法，密钥长度 256 位，分组模式是 GCM，摘要算法 SHA384 用于消息认证和产生随机数。” TLS 的密码套件命名非常规范，格式很固定。基本的形式是“密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法” OpenSSL OpenSSL是一个著名的开源密码学程序库和工具包,几乎支持所有公开的加密算法和协议，已经成为了事实上的标准，许多应用软件都会使用它作为底层库来实现 TLS 功能，包括常用的 Web 服务器 Apache、Nginx 等。 由于 OpenSSL 是开源的，所以它还有一些代码分支，比如 Google 的 BoringSSL、OpenBSD 的 LibreSSL，这些分支在 OpenSSL 的基础上删除了一些老旧代码，也增加了一些新特性 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:3:1","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"对称加密与非对称加密 对称加密： TLS 里有非常多的对称加密算法可供选择，比如 RC4、DES、3DES、AES、ChaCha20 等，但前三种算法都被认为是不安全的，通常都禁止使用，目前常用的只有 AES 和 ChaCha20。 AES 的意思是“高级加密标准”（Advanced Encryption Standard），密钥长度可以是 128、192 或 256。它是 DES 算法的替代者，安全强度很高，性能也很好，而且有的硬件还会做特殊优化，所以非常流行，是应用最广泛的对称加密算法。 ChaCha20 是 Google 设计的另一种加密算法，密钥长度固定为 256 位，纯软件运行性能要超过 AES，曾经在移动客户端上比较流行，但 ARMv8 之后也加入了 AES 硬件优化，所以现在不再具有明显的优势，但仍然算得上是一个不错的算法。 分组模式： 可以让算法用固定长度的密钥加密任意长度的明文 最新的分组模式被称为 AEAD（Authenticated Encryption with Associated Data），在加密的同时增加了认证的功能，常用的是 GCM、CCM 和 Poly1305。 eg: AES128-GCM，意思是密钥长度为 128 位的 AES 算法，使用的分组模式是 GCM； ChaCha20-Poly1305 的意思是 ChaCha20 算法，使用的分组模式是 Poly1305 非对称加密： 如何把密钥安全地传递给对方，术语叫“密钥交换”。这是非对称加密的优势所在。 非对称加密算法的设计要比对称算法难得多，在 TLS 里只有很少的几种，比如 DH、DSA、RSA、ECC 等。 RSA 可能是其中最著名的一个，它的安全性基于“整数分解”的数学难题，使用两个超大素数的乘积作为生成密钥的材料，想要从公钥推算出私钥是非常困难的。目前RSA密钥推荐长度是2048 ECC（Elliptic Curve Cryptography）是非对称加密里的“后起之秀”，它基于“椭圆曲线离散对数”的数学难题，使用特定的曲线方程和基点生成公钥和私钥，子算法 ECDHE 用于密钥交换，ECDSA 用于数字签名。 目前比较常用的两个曲线是 P-256（secp256r1，在 OpenSSL 称为 prime256v1）和 x25519。P-256 是 NIST（美国国家标准技术研究所）和 NSA（美国国家安全局）推荐使用的曲线，而 x25519 被认为是最安全、最快速的曲线。 ECC 曲线并不是椭圆形，只是因为方程很类似计算椭圆周长的公式，实际的形状更像抛物线 比起 RSA，ECC 在安全强度和性能上都有明显的优势。160 位的 ECC 相当于 1024 位的 RSA，而 224 位的 ECC 则相当于 2048 位的 RSA。因为密钥短，所以相应的计算量、消耗的内存和带宽也就少，加密解密的性能就上去了 混合加密： 虽然非对称加密没有“密钥交换”的问题，但因为它们都是基于复杂的数学难题，运算速度很慢，即使是 ECC 也要比 AES 差上好几个数量级 在通信刚开始的时候使用非对称算法，比如 RSA、ECDHE，首先解决密钥交换的问题。然后用随机数产生对称算法使用的“会话密钥”（session key），再用公钥加密。因为会话密钥很短，通常只有 16 字节或 32 字节，所以慢一点也无所谓。对方拿到密文后用私钥解密，取出会话密钥。这样，双方就实现了对称密钥的安全交换，后续就不再使用非对称加密，全都使用对称加密。 机密性得到的解决。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:3:2","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"数字签名与证书 摘要算法（具有完整性，但单纯使用不具有机密性，在混合加密系统里用会话密钥加密消息和摘要才真正满足完整性）：散列函数、哈希函数（Hash Function）。 可以理解成一种特殊的压缩算法，它能够把任意长度的数据“压缩”成固定长度、而且独一无二的“摘要”字符串 摘要算法对输入具有“单向性”和“雪崩效应”，输入的微小不同会导致输出的剧烈变化，所以也被 TLS 用来生成伪随机数（PRF，pseudo random function）。 MD5（Message-Digest 5）、SHA-1（Secure Hash Algorithm 1），最常用的两个摘要算法，能够生成 16 字节和 20 字节长度的数字摘要。但这两个算法不够安全，在 TLS 里已经被禁止使用了。目前 TLS 推荐使用的是 SHA-1 的后继者：SHA-2。 SHA-2 实际上是一系列摘要算法的统称，总共有 6 种，常用的有 SHA224、SHA256、SHA384，分别能够生成 28 字节、32 字节、48 字节的摘要。 数字签名： 加密算法结合摘要算法，我们的通信过程可以说是比较安全了。但这里还有漏洞，就是通信的两个端点（endpoint）。 使用私钥再加上摘要算法，就能够实现“数字签名”，同时实现“身份认证”和“不可否认”。 因为非对称加密效率太低，所以私钥只加密原文的摘要，这样运算量就小的多，而且得到的数字签名也方便保管和传输。 数字证书和CA: 怎么来判断这个公钥就是你的？ 那就找一个公认的可信第三方吧~ 这个“第三方”就是我们常说的 CA（Certificate Authority，证书认证机构）。它就像网络世界里的公安局、教育部、公证中心，具有极高的可信度，由它来给各个公钥签名 CA签发的证书分 DV、OV、EV 三种 DV 是最低的，只是域名级别的可信，背后是谁不知道。EV 是最高的，经过了法律和审计的严格核查，可以证明网站拥有者的身份（在浏览器地址栏会显示出公司的名字，例如 Apple、GitHub 的网站）。 小一点的 CA 可以让大 CA 签名认证，Root CA只能用自签名证书/根证书自己证明自己。 证书体系的缺陷： CA可能被欺骗签发了错误的证书，或者CA被黑客攻陷都会导致混乱。 对证书体系的缺陷的处理： CRL（证书吊销列表，Certificate revocation list）和 OCSP（在线证书状态协议，Online Certificate Status Protocol），及时废止有问题的证书。 操作系统或者浏览器从根上“下狠手”了，撤销对 CA 的信任，列入“黑名单”，这样它颁发的所有证书就都会被认为是不安全的。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:3:3","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"TLS1.2连接过程解析 HTTPS建立连接： 在 HTTP 协议里，建立连接后，浏览器会立即发送请求报文。但现在是 HTTPS 协议，它需要再用另外一个“握手”过程，在 TCP 上建立安全连接，之后才是收发 HTTP 报文 TLS协议的组成： 记录协议、警报协议、握手协议、变更密码规范协议等。 记录协议（Record Protocol） 规定了 TLS 收发数据的基本单位：记录（record）。它有点像是 TCP 里的 segment，所有的其他子协议都需要通过记录协议发出。但多个记录数据可以在一个 TCP 包里一次性发出，也并不需要像 TCP 那样返回 ACK。 警报协议（Alert Protocol） 这个协议的职责是向对方发出警报信息，有点像是 HTTP 协议里的状态码。比如，protocol_version 就是不支持旧版本，bad_certificate 就是证书有问题，收到警报后另一方可以选择继续，也可以立即终止连接。 握手协议（Handshake Protocol） 这是 TLS 里最复杂的子协议，要比 TCP 的 SYN/ACK 复杂的多，浏览器和服务器会在握手过程中协商 TLS 版本号、随机数、密码套件等信息，然后交换证书和密钥参数，最终双方协商得到会话密钥，用于后续的混合加密系统。 变更密码规范协议（Change Cipher Spec Protocol） 它非常简单，就是一个“通知”，告诉对方后续的数据都将使用加密保护。那也说明在它之前数据都是明文的。 简要的TLS握手过程： 详细的TLS握手过程： 在 TCP 建立连接之后，浏览器会首先发一个“Client Hello”消息，也就是跟服务器“打招呼”。里面有客户端的版本号、支持的密码套件，还有一个随机数（Client Random），用于后续生成会话密钥。 Handshake Protocol: Client Hello Version: TLS 1.2 (0x0303) Random: 1cbf803321fd2623408dfe… Cipher Suites (17 suites) Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f) Cipher Suite: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (0xc030) 服务器收到“Client Hello”后，会返回一个“Server Hello”消息。把版本号对一下，也给出一个随机数（Server Random），然后从客户端的列表里选一个作为本次通信使用的密码套件。比如：“TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384”。 Handshake Protocol: Server Hello Version: TLS 1.2 (0x0303) Random: 0e6320f21bae50842e96… Cipher Suite: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (0xc030) 然后，服务器为了证明自己的身份，就把证书也发给了客户端（Server Certificate） 接下来，因为服务器选择了 ECDHE 算法，所以它会在证书后发送“Server Key Exchange”消息，里面是椭圆曲线的公钥（Server Params），用来实现密钥交换算法，再加上自己的私钥签名认证。 Handshake Protocol: Server Key Exchange EC Diffie-Hellman Server Params Curve Type: named_curve (0x03) Named Curve: x25519 (0x001d) Pubkey: 3b39deaf00217894e... Signature Algorithm: rsa_pkcs1_sha512 (0x0601) Signature: 37141adac38ea4... 这相当于说：“刚才我选的密码套件有点复杂，所以再给你个算法的参数，和刚才的随机数一样有用，别丢了。为了防止别人冒充，我又盖了个章。”之后是“Server Hello Done”消息，服务器说：“我的信息就是这些，打招呼完毕。”这样第一个消息往返就结束了（两个 TCP 包），结果是客户端和服务器通过明文共享了三个信息：Client Random、Server Random 和 Server Params。 客户端这时也拿到了服务器的证书，那这个证书是不是真实有效的呢？这就要用到第 25 讲里的知识了，开始走证书链逐级验证，确认证书的真实性，再用证书公钥验证签名，就确认了服务器的身份：“刚才跟我打招呼的不是骗子，可以接着往下走。”然后，客户端按照密码套件的要求，也生成一个椭圆曲线的公钥（Client Params），用“Client Key Exchange”消息发给服务器。 Handshake Protocol: Client Key Exchange EC Diffie-Hellman Client Params Pubkey: 8c674d0e08dc27b5eaa… 现在客户端和服务器手里都拿到了密钥交换算法的两个参数（Client Params、Server Params），就用 ECDHE 算法一阵算，算出了一个新的东西，叫“Pre-Master”，其实也是一个随机数。至于具体的计算原理和过程，因为太复杂就不细说了，但算法可以保证即使黑客截获了之前的参数，也是绝对算不出这个随机数的。 现在客户端和服务器手里有了三个随机数：Client Random、Server Random 和 Pre-Master。用这三个作为原始材料，就可以生成用于加密会话的主密钥，叫“Master Secret”。而黑客因为拿不到“Pre-Master”，所以也就得不到主密钥。为什么非得这么麻烦，非要三个随机数呢？ 这就必须说 TLS 的设计者考虑得非常周到了，他们不信任客户端或服务器伪随机数的可靠性，为了保证真正的“完全随机”“不可预测”，把三个不可靠的随机数混合起来，那么“随机”的程度就非常高了，足够让黑客难以猜测。你一定很想知道“Master Secret”究竟是怎么算出来的吧，贴一下 RFC 里的公式： master_secret = PRF(pre_master_secret, \"master secret\", ClientHello.random + ServerHello.random) 这里的“PRF”就是伪随机数函数，它基于密码套件里的最后一个参数，比如这次的 SHA384，通过摘要算法来再一次强化“Master Secret”的随机性。 主密钥有 48 字节，但它也不是最终用于通信的会话密钥，还会再用 PRF 扩展出更多的密钥，比如客户端发送用的会话密钥（client_write_key）、服务器发送用的会话密钥（server_write_key）等等，避免只用一个密钥带来的安全隐患。 有了主密钥和派生的会话密钥，握手就快结束了。客户端发一个“Change Cipher Spec”，然后再发一个“Finished”消息，把之前所有发送的数据做个摘要，再加密一下，让服务器做个验证。 意思就是告诉服务器：“后面都改用对称算法加密通信了啊，用的就是打招呼时说的 AES，加密对不对还得你测一下。”服务器也是同样的操作，发“Change Cipher Spec”和“Finished”消息，双方都验证加密解密 OK，握手正式结束，后面就收发被加密的 HTTP 请求和响应了。 RSA 握手过程 整个握手过程可真是够复杂的，但你可能会问了，好像这个过程和其他地方看到的不一样呢？刚才说的其实是如今主流的 TLS 握手过程，这与传统的握手有两点不同。第一个，使用 ECDHE 实现密钥交换，而不是 RSA，所以会在服务器端发出“Server Key Exchange”消息。第二个，因为使用了 ECDHE，客户端可以不用等到服务器发回“Finished”确认握手完毕，立即就发出 HTTP 报文，省去了一个消息往返的时间浪费。这个叫“TLS False Start”，意思就是“抢跑”，和“TCP Fast Open”有点像，都是不等连接完全建立就提前发应用数据，提高传输的效率。实验环境在 440 端口（https://www.chrono.com:440/26-1）实现了传统的 RSA 密钥交换，没有“False Start”，你可以课后自己抓包看一下，这里我也画了个图。 大体的流程没有变，只是“Pre-Master”不再需要用算法生成，而是客户端直接生成随机数，然后用服务器的公钥加密，通过“Client Key Exchange”消息发给服务器。服务器再用私钥解密，这样双方也实现了共享三个随机数，就可以生成主密钥。 双向认证 到这里 TLS 握手就基本讲完了。 不过上面说的是“单向认证”握手过程，只认证了服务器的身份，而没有认证客户端的身份。这是因为通常单向认证通过后已经建立了安全通信，用账号、密码等简单的手段就能够确认用户的真实身份。但为了防止账号、密码被盗，有的时候（比如网上银行）还会使用 U 盾给用户颁发客户端证书，实现“双向认证”，这样会更加安全。 双向认证的流程也没有太多变化，只是在“Server Hello Done”之后，“Client Key Exchange”之前，客户端要发送“Client Certifi","date":"2022-01-24 20:52:12","objectID":"/cn_http/:3:4","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"TLS1.3特性解析 TLS1.2 已经是 10 年前（2008 年）的“老”协议了，虽然历经考验，但毕竟“岁月不饶人”，在安全、性能等方面已经跟不上如今的互联网了。于是经过四年、近 30 个草案的反复打磨，TLS1.3 终于在去年（2018 年）“粉墨登场”，再次确立了信息安全领域的新标准。在抓包分析握手之前，我们先来快速浏览一下 TLS1.3 的三个主要改进目标：兼容、安全与性能。 最大化兼容性 由于 1.1、1.2 等协议已经出现了很多年，很多应用软件、中间代理（官方称为“MiddleBox”）只认老的记录协议格式，更新改造很困难，甚至是不可行（设备僵化）。在早期的试验中发现，一旦变更了记录头字段里的版本号，也就是由 0x303（TLS1.2）改为 0x304（TLS1.3）的话，大量的代理服务器、网关都无法正确处理，最终导致 TLS 握手失败。为了保证这些被广泛部署的“老设备”能够继续使用，避免新协议带来的“冲击”，TLS1.3 不得不做出妥协，保持现有的记录格式不变，通过“伪装”来实现兼容，使得 TLS1.3 看上去“像是”TLS1.2。 那么，该怎么区分 1.2 和 1.3 呢？这要用到一个新的扩展协议（Extension Protocol），它有点“补充条款”的意思，通过在记录末尾添加一系列的“扩展字段”来增加新的功能，老版本的 TLS 不认识它可以直接忽略，这就实现了“后向兼容”。在记录头的 Version 字段被兼容性“固定”的情况下，只要是 TLS1.3 协议，握手的“Hello”消息后面就必须有“supported_versions”扩展，它标记了 TLS 的版本号，使用它就能区分新旧协议。其实上一讲 Chrome 在握手时发的就是 TLS1.3 协议，你可以看一下“Client Hello”消息后面的扩展，只是因为服务器不支持 1.3，所以就“后向兼容”降级成了 1.2。 Handshake Protocol: Client Hello Version: TLS 1.2 (0x0303) Extension: supported_versions (len=11) Supported Version: TLS 1.3 (0x0304) Supported Version: TLS 1.2 (0x0303) TLS1.3 利用扩展实现了许多重要的功能，比如“supported_groups”“key_share”“signature_algorithms”“server_name”等，这些等后面用到的时候再说。 强化安全 TLS1.2 在十来年的应用中获得了许多宝贵的经验，陆续发现了很多的漏洞和加密算法的弱点，所以 TLS1.3 就在协议里修补了这些不安全因素。比如： 伪随机数函数由 PRF 升级为 HKDF（HMAC-based Extract-and-Expand Key Derivation Function）； 明确禁止在记录协议里使用压缩； 废除了 RC4、DES 对称加密算法； 废除了 ECB、CBC 等传统分组模式； 废除了 MD5、SHA1、SHA-224 摘要算法； 废除了 RSA、DH 密钥交换算法和许多命名曲线。 经过这一番“减肥瘦身”之后，TLS1.3 里只保留了 AES、ChaCha20 对称加密算法，分组模式只能用 AEAD 的 GCM、CCM 和 Poly1305，摘要算法只能用 SHA256、SHA384，密钥交换算法只有 ECDHE 和 DHE，椭圆曲线也被“砍”到只剩 P-256 和 x25519 等 5 种。减肥可以让人变得更轻巧灵活，TLS 也是这样。算法精简后带来了一个意料之中的好处：原来众多的算法、参数组合导致密码套件非常复杂，难以选择，而现在的 TLS1.3 里只有 5 个套件，无论是客户端还是服务器都不会再犯“选择困难症”了。 这里还要特别说一下废除 RSA 和 DH 密钥交换算法的原因。上一讲用 Wireshark 抓包时你一定看到了，浏览器默认会使用 ECDHE 而不是 RSA 做密钥交换，这是因为它不具有“前向安全”（Forward Secrecy）。假设有这么一个很有耐心的黑客，一直在长期收集混合加密系统收发的所有报文。如果加密系统使用服务器证书里的 RSA 做密钥交换，一旦私钥泄露或被破解（使用社会工程学或者巨型计算机），那么黑客就能够使用私钥解密出之前所有报文的“Pre-Master”，再算出会话密钥，破解所有密文。这就是所谓的“今日截获，明日破解”。 而 ECDHE 算法在每次握手时都会生成一对临时的公钥和私钥，每次通信的密钥对都是不同的，也就是“一次一密”，即使黑客花大力气破解了这一次的会话密钥，也只是这次通信被攻击，之前的历史消息不会受到影响，仍然是安全的。所以现在主流的服务器和浏览器在握手阶段都已经不再使用 RSA，改用 ECDHE，而 TLS1.3 在协议里明确废除 RSA 和 DH 则在标准层面保证了“前向安全”。 提升性能 HTTPS 建立连接时除了要做 TCP 握手，还要做 TLS 握手，在 1.2 中会多花两个消息往返（2-RTT），可能导致几十毫秒甚至上百毫秒的延迟，在移动网络中延迟还会更严重。现在因为密码套件大幅度简化，也就没有必要再像以前那样走复杂的协商流程了。TLS1.3 压缩了以前的“Hello”协商过程，删除了“Key Exchange”消息，把握手时间减少到了“1-RTT”，效率提高了一倍。那么它是怎么做的呢？ 其实具体的做法还是利用了扩展。客户端在“Client Hello”消息里直接用“supported_groups”带上支持的曲线，比如 P-256、x25519，用“key_share”带上曲线对应的客户端公钥参数，用“signature_algorithms”带上签名算法。服务器收到后在这些扩展里选定一个曲线和参数，再用“key_share”扩展返回服务器这边的公钥参数，就实现了双方的密钥交换，后面的流程就和 1.2 基本一样了。我为 1.3 的握手过程画了一张图，你可以对比 1.2 看看区别在哪里。 除了标准的“1-RTT”握手，TLS1.3 还引入了“0-RTT”握手，用“pre_shared_key”和“early_data”扩展，在 TCP 连接后立即就建立安全连接发送加密消息，不过这需要有一些前提条件，今天暂且不说。 握手分析 目前 Nginx 等 Web 服务器都能够很好地支持 TLS1.3，但要求底层的 OpenSSL 必须是 1.1.1，而我们实验环境里用的 OpenSSL 是 1.1.0，所以暂时无法直接测试 TLS1.3。不过我在 Linux 上用 OpenSSL1.1.1 编译了一个支持 TLS1.3 的 Nginx，用 Wireshark 抓包存到了 GitHub 上，用它就可以分析 TLS1.3 的握手过程。 在 TCP 建立连接之后，浏览器首先还是发一个“Client Hello”。因为 1.3 的消息兼容 1.2，所以开头的版本号、支持的密码套件和随机数（Client Random）结构都是一样的（不过这时的随机数是 32 个字节）。 Handshake Protocol: Client Hello Version: TLS 1.2 (0x0303) Random: cebeb6c05403654d66c2329… Cipher Suites (18 suites) Cipher Suite: TLS_AES_128_GCM_SHA256 (0x1301) Cipher Suite: TLS_CHACHA20_POLY1305_SHA256 (0x1303) Cipher Suite: TLS_AES_256_GCM_SHA384 (0x1302) Extension: supported_versions (len=9) Supported Version: TLS 1.3 (0x0304) Supported Version: TLS 1.2 (0x0303) Extension: supported_groups (len=14) Supported Groups (6 groups) Supported Group: x25519 (0x001d) Supported Group: secp256r1 (0x0017) Extension: key_share (len=107) Key Share extension Client Key Share Length: 105 Key Share Entry: Group: x25519 Key Share Entry: Group: secp256r1 注意“Client Hello”里的扩展，“supported_versions”表示这是 TLS1.3，“supported_groups”是支持的曲线，“key_share”是曲线对应的参数。这就好像是说：“还是照老规矩打招呼，这边有这些这些信息。但我猜你可能会升级，所以再多给你一些东西，也许后面用的上，咱们有话尽量一口气说完。”服务器收到“Client Hello”同样返回“Server Hello”消息，还是要给出一个随机数（Server Random）和选定密码套件。 Handshake Protocol: Server Hello Version: TLS 1.2 (0x0303) Random: 12d2bce6568b063d3dee2… Cipher Suite: TLS","date":"2022-01-24 20:52:12","objectID":"/cn_http/:3:5","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTPS的优化 通过前两讲的学习，你可以看到，HTTPS 连接大致上可以划分为两个部分，第一个是建立连接时的非对称加密握手，第二个是握手后的对称加密报文传输。由于目前流行的 AES、ChaCha20 性能都很好，还有硬件优化，报文传输的性能损耗可以说是非常地小，小到几乎可以忽略不计了。所以，通常所说的“HTTPS 连接慢”指的就是刚开始建立连接的那段时间。 在 TCP 建连之后，正式数据传输之前，HTTPS 比 HTTP 增加了一个 TLS 握手的步骤，这个步骤最长可以花费两个消息往返，也就是 2-RTT。而且在握手消息的网络耗时之外，还会有其他的一些“隐形”消耗，比如： 产生用于密钥交换的临时公私钥对（ECDHE）； 验证证书时访问 CA 获取 CRL 或者 OCSP； 非对称加密解密处理“Pre-Master”。 在最差的情况下，也就是不做任何的优化措施，HTTPS 建立连接可能会比 HTTP 慢上几百毫秒甚至几秒，这其中既有网络耗时，也有计算耗时，就会让人产生“打开一个 HTTPS 网站好慢啊”的感觉。不过刚才说的情况早就是“过去时”了，现在已经有了很多行之有效的 HTTPS 优化手段，运用得好可以把连接的额外耗时降低到几十毫秒甚至是“零”。我画了一张图，把 TLS 握手过程中影响性能的部分都标记了出来，对照着它就可以“有的放矢”地来优化 HTTPS。 硬件优化 在计算机世界里的“优化”可以分成“硬件优化”和“软件优化”两种方式，先来看看有哪些硬件的手段。硬件优化，说白了就是“花钱”。但花钱也是有门道的，要“有钱用在刀刃上”，不能大把的银子撒出去“只听见响”。HTTPS 连接是计算密集型，而不是 I/O 密集型。所以，如果你花大价钱去买网卡、带宽、SSD 存储就是“南辕北辙”了，起不到优化的效果。 那该用什么样的硬件来做优化呢？首先，你可以选择更快的 CPU，最好还内建 AES 优化，这样即可以加速握手，也可以加速传输。其次，你可以选择“SSL 加速卡”，加解密时调用它的 API，让专门的硬件来做非对称加解密，分担 CPU 的计算压力。不过“SSL 加速卡”也有一些缺点，比如升级慢、支持算法有限，不能灵活定制解决方案等。 所以，就出现了第三种硬件加速方式：“SSL 加速服务器”，用专门的服务器集群来彻底“卸载”TLS 握手时的加密解密计算，性能自然要比单纯的“加速卡”要强大的多。 软件优化 不过硬件优化方式中除了 CPU，其他的通常可不是靠简单花钱就能买到的，还要有一些开发适配工作，有一定的实施难度。比如，“加速服务器”中关键的一点是通信必须是“异步”的，不能阻塞应用服务器，否则加速就没有意义了。所以，软件优化的方式相对来说更可行一些，性价比高，能够“少花钱，多办事”。软件方面的优化还可以再分成两部分：一个是软件升级，一个是协议优化。 软件升级实施起来比较简单，就是把现在正在使用的软件尽量升级到最新版本，比如把 Linux 内核由 2.x 升级到 4.x，把 Nginx 由 1.6 升级到 1.16，把 OpenSSL 由 1.0.1 升级到 1.1.0/1.1.1。由于这些软件在更新版本的时候都会做性能优化、修复错误，只要运维能够主动配合，这种软件优化是最容易做的，也是最容易达成优化效果的。但对于很多大中型公司来说，硬件升级或软件升级都是个棘手的问题，有成千上万台各种型号的机器遍布各个机房，逐一升级不仅需要大量人手，而且有较高的风险，可能会影响正常的线上服务。所以，在软硬件升级都不可行的情况下，我们最常用的优化方式就是在现有的环境下挖掘协议自身的潜力。 协议优化 从刚才的 TLS 握手图中你可以看到影响性能的一些环节，协议优化就要从这些方面着手，先来看看核心的密钥交换过程。如果有可能，应当尽量采用 TLS1.3，它大幅度简化了握手的过程，完全握手只要 1-RTT，而且更加安全。如果暂时不能升级到 1.3，只能用 1.2，那么握手时使用的密钥交换协议应当尽量选用椭圆曲线的 ECDHE 算法。它不仅运算速度快，安全性高，还支持“False Start”，能够把握手的消息往返由 2-RTT 减少到 1-RTT，达到与 TLS1.3 类似的效果。 另外，椭圆曲线也要选择高性能的曲线，最好是 x25519，次优选择是 P-256。对称加密算法方面，也可以选用“AES_128_GCM”，它能比“AES_256_GCM”略快一点点。在 Nginx 里可以用“ssl_ciphers”“ssl_ecdh_curve”等指令配置服务器使用的密码套件和椭圆曲线，把优先使用的放在前面，例如： ssl_ciphers TLS13-AES-256-GCM-SHA384:TLS13-CHACHA20-POLY1305-SHA256:EECDH+CHACHA20； ssl_ecdh_curve X25519:P-256; 证书优化 除了密钥交换，握手过程中的证书验证也是一个比较耗时的操作，服务器需要把自己的证书链全发给客户端，然后客户端接收后再逐一验证。这里就有两个优化点，一个是证书传输，一个是证书验证。 服务器的证书可以选择椭圆曲线（ECDSA）证书而不是 RSA 证书，因为 224 位的 ECC 相当于 2048 位的 RSA，所以椭圆曲线证书的“个头”要比 RSA 小很多，即能够节约带宽也能减少客户端的运算量，可谓“一举两得”。客户端的证书验证其实是个很复杂的操作，除了要公钥解密验证多个证书签名外，因为证书还有可能会被撤销失效，客户端有时还会再去访问 CA，下载 CRL 或者 OCSP 数据，这又会产生 DNS 查询、建立连接、收发数据等一系列网络通信，增加好几个 RTT。 CRL（Certificate revocation list，证书吊销列表）由 CA 定期发布，里面是所有被撤销信任的证书序号，查询这个列表就可以知道证书是否有效。但 CRL 因为是“定期”发布，就有“时间窗口”的安全隐患，而且随着吊销证书的增多，列表会越来越大，一个 CRL 经常会上 MB。想象一下，每次需要预先下载几 M 的“无用数据”才能连接网站，实用性实在是太低了。所以，现在 CRL 基本上不用了，取而代之的是 OCSP（在线证书状态协议，Online Certificate Status Protocol），向 CA 发送查询请求，让 CA 返回证书的有效状态。但 OCSP 也要多出一次网络请求的消耗，而且还依赖于 CA 服务器，如果 CA 服务器很忙，那响应延迟也是等不起的。 于是又出来了一个“补丁”，叫“OCSP Stapling”（OCSP 装订），它可以让服务器预先访问 CA 获取 OCSP 响应，然后在握手时随着证书一起发给客户端，免去了客户端连接 CA 服务器查询的时间。 会话复用 到这里，我们已经讨论了四种 HTTPS 优化手段（硬件优化、软件优化、协议优化、证书优化），那么，还有没有其他更好的方式呢？我们再回想一下 HTTPS 建立连接的过程：先是 TCP 三次握手，然后是 TLS 一次握手。这后一次握手的重点是算出主密钥“Master Secret”，而主密钥每次连接都要重新计算，未免有点太浪费了，如果能够把“辛辛苦苦”算出来的主密钥缓存一下“重用”，不就可以免去了握手和计算的成本了吗？ 这种做法就叫“会话复用”（TLS session resumption），和 HTTP Cache 一样，也是提高 HTTPS 性能的“大杀器”，被浏览器和服务器广泛应用。会话复用分两种，第一种叫“Session ID”，就是客户端和服务器首次连接后各自保存一个会话的 ID 号，内存里存储主密钥和其他相关的信息。当客户端再次连接时发一个 ID 过来，服务器就在内存里找，找到就直接用主密钥恢复会话状态，跳过证书验证和密钥交换，只用一个消息往返就可以建立安全通信。 实验环境的端口 441 实现了“Session ID”的会话复用，你可以访问 URI“https://www.chrono.com:441/28-1”，刷新几次，用 Wireshark 抓包看看实际的效果。 Handshake Protocol: Client Hello Version: TLS 1.2 (0x0303) Session ID: 13564734eeec0a658830cd… Cipher Suites Length: 34 Handshake Protocol: Server Hello Version: TLS 1.2 (0x0303) Session ID: 13564734eeec0a658830cd… Cipher Suite: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (0xc030) 通过抓包可以看到，服务器在“ServerHello”消息后直接发送了“Change Cipher Spec”和“Finished”消息，复用会话完成了握手。 会话票证 “Session ID”是最早出现的会话复用技术，也是应用最广的，但它也有缺点，服务器必须保存每一个客户端的会话数据，对于拥有百万、千万级别用户的网站来说存储量就成了大问题，加重了服务器的负担。于是，又出现了第二种“Session Ticket”方案。 它有点类似 HTTP 的 Cookie，存储的责任由服务器转移到了客户端，服务器加密会话信息，用“New Session Ticket”消息发给客户端，让客户端保存。重连的时候，客户端使用扩展“session_ticket”发送“Ticket”而不是“Sessio","date":"2022-01-24 20:52:12","objectID":"/cn_http/:3:6","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"该不该迁移到HTTPS 我们已经学完了 HTTPS、TLS 相关的大部分知识。不过，或许你心里还会有一些困惑：“HTTPS 这么复杂，我是否应该迁移到 HTTPS 呢？它能带来哪些好处呢？具体又应该怎么实施迁移呢？”这些问题不单是你，也是其他很多人，还有当初的我的真实想法，所以今天我就来跟你聊聊这方面的事情。 迁移的必要性 如果你做移动应用开发的话，那么就一定知道，Apple、Android、某信等开发平台在 2017 年就相继发出通知，要求所有的应用必须使用 HTTPS 连接，禁止不安全的 HTTP。在台式机上，主流的浏览器 Chrome、Firefox 等也早就开始“强推”HTTPS，把 HTTP 站点打上“不安全”的标签，给用户以“心理压力”。Google 等搜索巨头还利用自身的“话语权”优势，降低 HTTP 站点的排名，而给 HTTPS 更大的权重，力图让网民只访问到 HTTPS 网站。 这些手段都逐渐“挤压”了纯明文 HTTP 的生存空间，“迁移到 HTTPS”已经不是“要不要做”的问题，而是“要怎么做”的问题了。HTTPS 的大潮无法阻挡，如果还是死守着 HTTP，那么无疑会被冲刷到互联网的角落里。目前国内外的许多知名大站都已经实现了“全站 HTTPS”，打开常用的某宝、某东、某浪，都可以在浏览器的地址栏里看到“小锁头”，如果你正在维护的网站还没有实施 HTTPS，那可要抓点紧了。 迁移的顾虑 据我观察，阻碍 HTTPS 实施的因素还有一些这样那样的顾虑，我总结出了三个比较流行的观点：“慢、贵、难”。 所谓“慢”，是指惯性思维，拿以前的数据来评估 HTTPS 的性能，认为 HTTPS 会增加服务器的成本，增加客户端的时延，影响用户体验。其实现在服务器和客户端的运算能力都已经有了很大的提升，性能方面完全没有担心的必要，而且还可以应用很多的优化解决方案（参见第 28 讲）。根据 Google 等公司的评估，在经过适当优化之后，HTTPS 的额外 CPU 成本小于 1%，额外的网络成本小于 2%，可以说是与无加密的 HTTP 相差无几。 所谓“贵”，主要是指证书申请和维护的成本太高，网站难以承担。这也属于惯性思维，在早几年的确是个问题，向 CA 申请证书的过程不仅麻烦，而且价格昂贵，每年要交几千甚至几万元。但现在就不一样了，为了推广 HTTPS，很多云服务厂商都提供了一键申请、价格低廉的证书，而且还出现了专门颁发免费证书的 CA，其中最著名的就是“Let’s Encrypt”。 所谓的“难”，是指 HTTPS 涉及的知识点太多、太复杂，有一定的技术门槛，不能很快上手。这第三个顾虑比较现实，HTTPS 背后关联到了密码学、TLS、PKI 等许多领域，不是短短几周、几个月就能够精通的。但实施 HTTPS 也并不需要把这些完全掌握，只要抓住少数几个要点就好，下面我就来帮你逐个解决一些关键的“难点”。 申请证书 要把网站从 HTTP 切换到 HTTPS，首先要做的就是为网站申请一张证书。大型网站出于信誉、公司形象的考虑，通常会选择向传统的 CA 申请证书，例如 DigiCert、GlobalSign，而中小型网站完全可以选择使用“Let’s Encrypt”这样的免费证书，效果也完全不输于那些收费的证书。 “Let’s Encrypt”一直在推动证书的自动化部署，为此还实现了专门的 ACME 协议（RFC8555）。有很多的客户端软件可以完成申请、验证、下载、更新的“一条龙”操作，比如 Certbot、acme.sh 等等，都可以在“Let’s Encrypt”网站上找到，用法很简单，相关的文档也很详细，几分钟就能完成申请，所以我在这里就不细说了。 不过我必须提醒你几个注意事项。 第一，申请证书时应当同时申请 RSA 和 ECDSA 两种证书，在 Nginx 里配置成双证书验证，这样服务器可以自动选择快速的椭圆曲线证书，同时也兼容只支持 RSA 的客户端。 第二，如果申请 RSA 证书，私钥至少要 2048 位，摘要算法应该选用 SHA-2，例如 SHA256、SHA384 等。 第三，出于安全的考虑，“Let’s Encrypt”证书的有效期很短，只有 90 天，时间一到就会过期失效，所以必须要定期更新。你可以在 crontab 里加个每周或每月任务，发送更新请求，不过很多 ACME 客户端会自动添加这样的定期任务，完全不用你操心。 配置 HTTPS 搞定了证书，接下来就是配置 Web 服务器，在 443 端口上开启 HTTPS 服务了。这在 Nginx 上非常简单，只要在“listen”指令后面加上参数“ssl”，再配上刚才的证书文件就可以实现最基本的 HTTPS。 listen 443 ssl; ssl_certificate xxx_rsa.crt; #rsa2048 cert ssl_certificate_key xxx_rsa.key; #rsa2048 private key ssl_certificate xxx_ecc.crt; #ecdsa cert ssl_certificate_key xxx_ecc.key; #ecdsa private ke 为了提高 HTTPS 的安全系数和性能，你还可以强制 Nginx 只支持 TLS1.2 以上的协议，打开“Session Ticket”会话复用： ssl_protocols TLSv1.2 TLSv1.3; ssl_session_timeout 5m; ssl_session_tickets on; ssl_session_ticket_key ticket.key; 密码套件的选择方面，我给你的建议是以服务器的套件优先。这样可以避免恶意客户端故意选择较弱的套件、降低安全等级，然后密码套件向 TLS1.3“看齐”，只使用 ECDHE、AES 和 ChaCha20，支持“False Start”。 ssl_prefer_server_ciphers on; ssl_ciphers ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-CHACHA20-POLY1305:ECDHE+AES128:!MD5:!SHA1; 如果你的服务器上使用了 OpenSSL 的分支 BorringSSL，那么还可以使用一个特殊的“等价密码组”（Equal preference cipher groups）特性，它可以让服务器配置一组“等价”的密码套件，在这些套件里允许客户端优先选择，比如这么配置： ssl_ciphers [ECDHE-ECDSA-AES128-GCM-SHA256|ECDHE-ECDSA-CHACHA20-POLY1305]; 如果客户端硬件没有 AES 优化，服务器就会顺着客户端的意思，优先选择与 AES“等价”的 ChaCha20 算法，让客户端能够快一点。全部配置完成后，你可以访问“SSLLabs”网站，测试网站的安全程度，它会模拟多种客户端发起测试，打出一个综合的评分。下图就是 GitHub 网站的评分结果： 服务器名称指示 配置 HTTPS 服务时还有一个“虚拟主机”的问题需要解决。在 HTTP 协议里，多个域名可以同时在一个 IP 地址上运行，这就是“虚拟主机”，Web 服务器会使用请求头里的 Host 字段（参见第 9 讲）来选择。但在 HTTPS 里，因为请求头只有在 TLS 握手之后才能发送，在握手时就必须选择“虚拟主机”对应的证书，TLS 无法得知域名的信息，就只能用 IP 地址来区分。所以，最早的时候每个 HTTPS 域名必须使用独立的 IP 地址，非常不方便。 那么怎么解决这个问题呢？这还是得用到 TLS 的“扩展”，给协议加个 SNI（Server Name Indication）的“补充条款”。它的作用和 Host 字段差不多，客户端会在“Client Hello”时带上域名信息，这样服务器就可以根据名字而不是 IP 地址来选择证书。 Extension: server_name (len=19) Server Name Indication extension Server Name Type: host_name (0) Server Name: www.chrono.com Nginx 很早就基于 SNI 特性支持了 HTTPS 的虚拟主机，但在 OpenResty 里可还以编写 Lua 脚本，利用 Redis、MySQL 等数据库更灵活快速地加载证书。 重定向跳转 现在有了 HTTPS 服务，但原来的 HTTP 站点也不能马上弃用，还是会有很多网民习惯在地址栏里直接敲域名（或者是旧的书签、超链接），默认使用 HTTP 协议访问。所以，我们就需要用到第 18 讲里的“重定向跳转”技术了，把不安全的 HTTP 网址用 301 或 302“重定向”到新的 HTTPS 网站，这在 Nginx 里也很容易做到，使用“return”或“rewrite”都可以。 return 301 https://$host$request_uri; #永久重定向 rewrite ^ https://$host$request_uri permanent; #永久重定向 但这种方式有两个问题。一个是重定向增加了网络成本，多出了一次请求；另一个是存在安全隐患，重定向的响应可能会被“中间人”窜改，实现“会话劫持”，跳转到恶意网站。 不过有一","date":"2022-01-24 20:52:12","objectID":"/cn_http/:3:7","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP/2 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:4:0","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP/2特性概览 我们看到 HTTP 有两个主要的缺点：安全不足和性能不高。刚结束的“安全篇”里的 HTTPS，通过引入 SSL/TLS 在安全上达到了“极致”，但在性能提升方面却是乏善可陈，只优化了握手加密的环节，对于整体的数据传输没有提出更好的改进方案，还只能依赖于“长连接”这种“落后”的技术（参见第 17 讲）。所以，在 HTTPS 逐渐成熟之后，HTTP 就向着性能方面开始“发力”，走出了另一条进化的道路。 在 HTTP 历史中你也看到了，“秦失其鹿，天下共逐之”，Google 率先发明了 SPDY 协议，并应用于自家的浏览器 Chrome，打响了 HTTP 性能优化的“第一枪”。随后互联网标准化组织 IETF 以 SPDY 为基础，综合其他多方的意见，终于推出了 HTTP/1 的继任者，也就是今天的主角“HTTP/2”，在性能方面有了一个大的飞跃。 为什么不是 HTTP/2.0 你一定很想知道，为什么 HTTP/2 不像之前的“1.0”“1.1”那样叫“2.0”呢？这个也是很多初次接触 HTTP/2 的人问的最多的一个问题，对此 HTTP/2 工作组特别给出了解释。他们认为以前的“1.0”“1.1”造成了很多的混乱和误解，让人在实际的使用中难以区分差异，所以就决定 HTTP 协议不再使用小版本号（minor version），只使用大版本号（major version），从今往后 HTTP 协议不会出现 HTTP/2.0、2.1，只会有“HTTP/2”“HTTP/3”……这样就可以明确无误地辨别出协议版本的“跃进程度”，让协议在一段较长的时期内保持稳定，每当发布新版本的 HTTP 协议都会有本质的不同，绝不会有“零敲碎打”的小改良。 兼容 HTTP/1 由于 HTTPS 已经在安全方面做的非常好了，所以 HTTP/2 的唯一目标就是改进性能。但它不仅背负着众多的期待，同时还背负着 HTTP/1 庞大的历史包袱，所以协议的修改必须小心谨慎，兼容性是首要考虑的目标，否则就会破坏互联网上无数现有的资产，这方面 TLS 已经有了先例（为了兼容 TLS1.2 不得不进行“伪装”）。那么，HTTP/2 是怎么做的呢？ 因为必须要保持功能上的兼容，所以 HTTP/2 把 HTTP 分解成了“语义”和“语法”两个部分，“语义”层不做改动，与 HTTP/1 完全一致（即 RFC7231）。比如请求方法、URI、状态码、头字段等概念都保留不变，这样就消除了再学习的成本，基于 HTTP 的上层应用也不需要做任何修改，可以无缝转换到 HTTP/2。特别要说的是，与 HTTPS 不同，HTTP/2 没有在 URI 里引入新的协议名，仍然用“http”表示明文协议，用“https”表示加密协议。这是一个非常了不起的决定，可以让浏览器或者服务器去自动升级或降级协议，免去了选择的麻烦，让用户在上网的时候都意识不到协议的切换，实现平滑过渡。在“语义”保持稳定之后，HTTP/2 在“语法”层做了“天翻地覆”的改造，完全变更了 HTTP 报文的传输格式。 头部压缩 首先，HTTP/2 对报文的头部做了一个“大手术”。通过“进阶篇”的学习你应该知道，HTTP/1 里可以用头字段“Content-Encoding”指定 Body 的编码方式，比如用 gzip 压缩来节约带宽，但报文的另一个组成部分——Header 却被无视了，没有针对它的优化手段。由于报文 Header 一般会携带“User Agent”“Cookie”“Accept”“Server”等许多固定的头字段，多达几百字节甚至上千字节，但 Body 却经常只有几十字节（比如 GET 请求、204/301/304 响应），成了不折不扣的“大头儿子”。更要命的是，成千上万的请求响应报文里有很多字段值都是重复的，非常浪费，“长尾效应”导致大量带宽消耗在了这些冗余度极高的数据上。 所以，HTTP/2 把“头部压缩”作为性能改进的一个重点，优化的方式你也肯定能想到，还是“压缩”。不过 HTTP/2 并没有使用传统的压缩算法，而是开发了专门的“HPACK”算法，在客户端和服务器两端建立“字典”，用索引号表示重复的字符串，还釆用哈夫曼编码来压缩整数和字符串，可以达到 50%~90% 的高压缩率。 二进制格式 你可能已经很习惯于 HTTP/1 里纯文本形式的报文了，它的优点是“一目了然”，用最简单的工具就可以开发调试，非常方便。但 HTTP/2 在这方面没有“妥协”，决定改变延续了十多年的现状，不再使用肉眼可见的 ASCII 码，而是向下层的 TCP/IP 协议“靠拢”，全面采用二进制格式。 这样虽然对人不友好，但却大大方便了计算机的解析。原来使用纯文本的时候容易出现多义性，比如大小写、空白字符、回车换行、多字少字等等，程序在处理时必须用复杂的状态机，效率低，还麻烦。而二进制里只有“0”和“1”，可以严格规定字段大小、顺序、标志位等格式，“对就是对，错就是错”，解析起来没有歧义，实现简单，而且体积小、速度快，做到“内部提效”。以二进制格式为基础，HTTP/2 就开始了“大刀阔斧”的改革。 它把 TCP 协议的部分特性挪到了应用层，把原来的“Header+Body”的消息“打散”为数个小片的二进制“帧”（Frame），用“HEADERS”帧存放头数据、“DATA”帧存放实体数据。这种做法有点像是“Chunked”分块编码的方式（参见第 16 讲），也是“化整为零”的思路，但 HTTP/2 数据分帧后“Header+Body”的报文结构就完全消失了，协议看到的只是一个个的“碎片”。 虚拟的“流” 消息的“碎片”到达目的地后应该怎么组装起来呢？ HTTP/2 为此定义了一个“流”（Stream）的概念，它是二进制帧的双向传输序列，同一个消息往返的帧会分配一个唯一的流 ID。你可以把它想象成是一个虚拟的“数据流”，在里面流动的是一串有先后顺序的数据帧，这些数据帧按照次序组装起来就是 HTTP/1 里的请求报文和响应报文。 因为“流”是虚拟的，实际上并不存在，所以 HTTP/2 就可以在一个 TCP 连接上用“流”同时发送多个“碎片化”的消息，这就是常说的“多路复用”（ Multiplexing）——多个往返通信都复用一个连接来处理。 在“流”的层面上看，消息是一些有序的“帧”序列，而在“连接”的层面上看，消息却是乱序收发的“帧”。多个请求 / 响应之间没有了顺序关系，不需要排队等待，也就不会再出现“队头阻塞”问题，降低了延迟，大幅度提高了连接的利用率。 为了更好地利用连接，加大吞吐量，HTTP/2 还添加了一些控制帧来管理虚拟的“流”，实现了优先级和流量控制，这些特性也和 TCP 协议非常相似。HTTP/2 还在一定程度上改变了传统的“请求 - 应答”工作模式，服务器不再是完全被动地响应请求，也可以新建“流”主动向客户端发送消息。比如，在浏览器刚请求 HTML 的时候就提前把可能会用到的 JS、CSS 文件发给客户端，减少等待的延迟，这被称为“服务器推送”（Server Push，也叫 Cache Push）。 强化安全 出于兼容的考虑，HTTP/2 延续了 HTTP/1 的“明文”特点，可以像以前一样使用明文传输数据，不强制使用加密通信，不过格式还是二进制，只是不需要解密。但由于 HTTPS 已经是大势所趋，而且主流的浏览器 Chrome、Firefox 等都公开宣布只支持加密的 HTTP/2，所以“事实上”的 HTTP/2 是加密的。也就是说，互联网上通常所能见到的 HTTP/2 都是使用“https”协议名，跑在 TLS 上面。 为了区分“加密”和“明文”这两个不同的版本，HTTP/2 协议定义了两个字符串标识符：“h2”表示加密的 HTTP/2，“h2c”表示明文的 HTTP/2，多出的那个字母“c”的意思是“clear text”。 在 HTTP/2 标准制定的时候（2015 年）已经发现了很多 SSL/TLS 的弱点，而新的 TLS1.3 还未发布，所以加密版本的 HTTP/2 在安全方面做了强化，要求下层的通信协议必须是 TLS1.2 以上，还要支持前向安全和 SNI，并且把几百个弱密码套件列入了“黑名单”，比如 DES、RC4、CBC、SHA-1 都不能在 HTTP/2 里使用，相当于底层用的是“TLS1.25”。 协议栈 下面的这张图对比了 HTTP/1、HTTPS 和 HTTP/2 的协议栈，你可以清晰地看到，HTTP/2 是建立在“HPack”“Stream”“TLS1.2”基础之上的，比 HTTP/1、HTTPS 复杂了一些。 虽然 HTTP/2 的底层实现很复杂，但它的“语义”还是简单的 HTTP/1，之前学习的知识不会过时，仍然能够用得上。我们的实验环境在新的域名“www.metroid.net”上启用了 HTTP/2 协议，你可以把之前“进阶篇”“安全篇”的测试用例都走一遍，再用 Wireshark 抓一下包，实际看看 HTTP/2 的效果和对老协议的兼容性（例如“http://www.metroid.net/11-1”）。在今天这节课专用的 URI“/30-1”里，你还可以看到服务器输出了 HTTP 的版本号“2”和标识符“h2”，表示这是加密的 HTTP/2，如果改用“https://www.chrono.com/30-1”访问就会是“1.1”和空。 你可能还会注意到 URI 里的一个小变化，端口使用的是“8443”而不是“443”。这是因为 443 端口已经被“www.chrono.com”的 HTTP","date":"2022-01-24 20:52:12","objectID":"/cn_http/:4:1","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP/2内核剖析 继续上一讲的话题，深入 HTTP/2 协议的内部，看看它的实现细节。 这次实验环境的 URI 是“/31-1”，我用 Wireshark 把请求响应的过程抓包存了下来，文件放在 GitHub 的“wireshark”目录。今天我们就对照着抓包来实地讲解 HTTP/2 的头部压缩、二进制帧等特性。 连接前言 由于 HTTP/2“事实上”是基于 TLS，所以在正式收发数据之前，会有 TCP 握手和 TLS 握手，这两个步骤相信你一定已经很熟悉了，所以这里就略过去不再细说。TLS 握手成功之后，客户端必须要发送一个“连接前言”（connection preface），用来确认建立 HTTP/2 连接。这个“连接前言”是标准的 HTTP/1 请求报文，使用纯文本的 ASCII 码格式，请求方法是特别注册的一个关键字“PRI”，全文只有 24 个字节： PRI * HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n 在 Wireshark 里，HTTP/2 的“连接前言”被称为“Magic”，意思就是“不可知的魔法”。所以，就不要问“为什么会是这样”了，只要服务器收到这个“有魔力的字符串”，就知道客户端在 TLS 上想要的是 HTTP/2 协议，而不是其他别的协议，后面就会都使用 HTTP/2 的数据格式。 头部压缩 确立了连接之后，HTTP/2 就开始准备请求报文。因为语义上它与 HTTP/1 兼容，所以报文还是由“Header+Body”构成的，但在请求发送前，必须要用“HPACK”算法来压缩头部数据。“HPACK”算法是专门为压缩 HTTP 头部定制的算法，与 gzip、zlib 等压缩算法不同，它是一个“有状态”的算法，需要客户端和服务器各自维护一份“索引表”，也可以说是“字典”（这有点类似 brotli），压缩和解压缩就是查表和更新表的操作。 为了方便管理和压缩，HTTP/2 废除了原有的起始行概念，把起始行里面的请求方法、URI、状态码等统一转换成了头字段的形式，并且给这些“不是头字段的头字段”起了个特别的名字——“伪头字段”（pseudo-header fields）。而起始行里的版本号和错误原因短语因为没什么大用，顺便也给废除了。为了与“真头字段”区分开来，这些“伪头字段”会在名字前加一个“:”，比如“:authority” “:method” “:status”，分别表示的是域名、请求方法和状态码。 现在 HTTP 报文头就简单了，全都是“Key-Value”形式的字段，于是 HTTP/2 就为一些最常用的头字段定义了一个只读的“静态表”（Static Table）。下面的这个表格列出了“静态表”的一部分，这样只要查表就可以知道字段名和对应的值，比如数字“2”代表“GET”，数字“8”代表状态码 200。 但如果表里只有 Key 没有 Value，或者是自定义字段根本找不到该怎么办呢？这就要用到“动态表”（Dynamic Table），它添加在静态表后面，结构相同，但会在编码解码的时候随时更新。比如说，第一次发送请求时的“user-agent”字段长是一百多个字节，用哈夫曼压缩编码发送之后，客户端和服务器都更新自己的动态表，添加一个新的索引号“65”。那么下一次发送的时候就不用再重复发那么多字节了，只要用一个字节发送编号就好。 你可以想象得出来，随着在 HTTP/2 连接上发送的报文越来越多，两边的“字典”也会越来越丰富，最终每次的头部字段都会变成一两个字节的代码，原来上千字节的头用几十个字节就可以表示了，压缩效果比 gzip 要好得多。 二进制帧 头部数据压缩之后，HTTP/2 就要把报文拆成二进制的帧准备发送。HTTP/2 的帧结构有点类似 TCP 的段或者 TLS 里的记录，但报头很小，只有 9 字节，非常地节省（可以对比一下 TCP 头，它最少是 20 个字节）。二进制的格式也保证了不会有歧义，而且使用位运算能够非常简单高效地解析。 帧开头是 3 个字节的长度（但不包括头的 9 个字节），默认上限是 2^14，最大是 2^24，也就是说 HTTP/2 的帧通常不超过 16K，最大是 16M。长度后面的一个字节是帧类型，大致可以分成数据帧和控制帧两类，HEADERS 帧和 DATA 帧属于数据帧，存放的是 HTTP 报文，而 SETTINGS、PING、PRIORITY 等则是用来管理流的控制帧。HTTP/2 总共定义了 10 种类型的帧，但一个字节可以表示最多 256 种，所以也允许在标准之外定义其他类型实现功能扩展。这就有点像 TLS 里扩展协议的意思了，比如 Google 的 gRPC 就利用了这个特点，定义了几种自用的新帧类型。 第 5 个字节是非常重要的帧标志信息，可以保存 8 个标志位，携带简单的控制信息。常用的标志位有 END_HEADERS 表示头数据结束，相当于 HTTP/1 里头后的空行（“\\r\\n”），END_STREAM 表示单方向数据发送结束（即 EOS，End of Stream），相当于 HTTP/1 里 Chunked 分块结束标志（“0\\r\\n\\r\\n”）。报文头里最后 4 个字节是流标识符，也就是帧所属的“流”，接收方使用它就可以从乱序的帧里识别出具有相同流 ID 的帧序列，按顺序组装起来就实现了虚拟的“流”。流标识符虽然有 4 个字节，但最高位被保留不用，所以只有 31 位可以使用，也就是说，流标识符的上限是 2^31，大约是 21 亿。好了，把二进制头理清楚后，我们来看一下 Wireshark 抓包的帧实例： 在这个帧里，开头的三个字节是“00010a”，表示数据长度是 266 字节。帧类型是 1，表示 HEADERS 帧，负载（payload）里面存放的是被 HPACK 算法压缩的头部信息。标志位是 0x25，转换成二进制有 3 个位被置 1。PRIORITY 表示设置了流的优先级，END_HEADERS 表示这一个帧就是完整的头数据，END_STREAM 表示单方向数据发送结束，后续再不会有数据帧（即请求报文完毕，不会再有 DATA 帧 /Body 数据）。最后 4 个字节的流标识符是整数 1，表示这是客户端发起的第一个流，后面的响应数据帧也会是这个 ID，也就是说在 stream[1]里完成这个请求响应。 流与多路复用 弄清楚了帧结构后我们就来看 HTTP/2 的流与多路复用，它是 HTTP/2 最核心的部分。在上一讲里我简单介绍了流的概念，不知道你“悟”得怎么样了？这里我再重复一遍：流是二进制帧的双向传输序列。 要搞明白流，关键是要理解帧头里的流 ID。在 HTTP/2 连接上，虽然帧是乱序收发的，但只要它们都拥有相同的流 ID，就都属于一个流，而且在这个流里帧不是无序的，而是有着严格的先后顺序。比如在这次的 Wireshark 抓包里，就有“0、1、3”一共三个流，实际上就是分配了三个流 ID 号，把这些帧按编号分组，再排一下队，就成了流。 在概念上，一个 HTTP/2 的流就等同于一个 HTTP/1 里的“请求 - 应答”。在 HTTP/1 里一个“请求 - 响应”报文来回是一次 HTTP 通信，在 HTTP/2 里一个流也承载了相同的功能。你还可以对照着 TCP 来理解。TCP 运行在 IP 之上，其实从 MAC 层、IP 层的角度来看，TCP 的“连接”概念也是“虚拟”的。但从功能上看，无论是 HTTP/2 的流，还是 TCP 的连接，都是实际存在的，所以你以后大可不必再纠结于流的“虚拟”性，把它当做是一个真实存在的实体来理解就好。 HTTP/2 的流有哪些特点呢？我给你简单列了一下： 流是可并发的，一个 HTTP/2 连接上可以同时发出多个流传输数据，也就是并发多请求，实现“多路复用”； 客户端和服务器都可以创建流，双方互不干扰； 流是双向的，一个流里面客户端和服务器都可以发送或接收数据帧，也就是一个“请求 - 应答”来回； 流之间没有固定关系，彼此独立，但流内部的帧是有严格顺序的； 流可以设置优先级，让服务器优先处理，比如先传 HTML/CSS，后传图片，优化用户体验； 流 ID 不能重用，只能顺序递增，客户端发起的 ID 是奇数，服务器端发起的 ID 是偶数； 在流上发送“RST_STREAM”帧可以随时终止流，取消接收或发送； 第 0 号流比较特殊，不能关闭，也不能发送数据帧，只能发送控制帧，用于流量控制 这里我又画了一张图，把上次的图略改了一下，显示了连接中无序的帧是如何依据流 ID 重组成流的。 从这些特性中，我们还可以推理出一些深层次的知识点。比如说，HTTP/2 在一个连接上使用多个流收发数据，那么它本身默认就会是长连接，所以永远不需要“Connection”头字段（keepalive 或 close）。你可以再看一下 Wireshark 的抓包，里面发送了两个请求“/31-1”和“/favicon.ico”，始终用的是“56095\u003c-\u003e8443”这个连接，对比一下第 8 讲，你就能够看出差异了。又比如，下载大文件的时候想取消接收，在 HTTP/1 里只能断开 TCP 连接重新“三次握手”，成本很高，而在 HTTP/2 里就可以简单地发送一个“RST_STREAM”中断流，而长连接会继续保持。再比如，因为客户端和服务器两端都可以创建流，而流 ID 有奇数偶数和上限的区分，所以大多数的流 ID 都会是奇数，而且客户端在一个连接里最多只能发出 2^30，也就是 10 亿个请求。 所以就要问了：ID 用完了该怎么办呢？这个时候可以再发一个控制帧“","date":"2022-01-24 20:52:12","objectID":"/cn_http/:4:2","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP/3展望 在前面的两讲里，我们一起学习了 HTTP/2，你也应该看到了 HTTP/2 做出的许多努力，比如头部压缩、二进制分帧、虚拟的“流”与多路复用，性能方面比 HTTP/1 有了很大的提升，“基本上”解决了“队头阻塞”这个“老大难”问题。 HTTP/2 的“队头阻塞” 等等，你可能要发出疑问了：为什么说是“基本上”，而不是“完全”解决了呢？这是因为 HTTP/2 虽然使用“帧”“流”“多路复用”，没有了“队头阻塞”，但这些手段都是在应用层里，而在下层，也就是 TCP 协议里，还是会发生“队头阻塞”。这是怎么回事呢？ 让我们从协议栈的角度来仔细看一下。在 HTTP/2 把多个“请求 - 响应”分解成流，交给 TCP 后，TCP 会再拆成更小的包依次发送（其实在 TCP 里应该叫 segment，也就是“段”）。在网络良好的情况下，包可以很快送达目的地。但如果网络质量比较差，像手机上网的时候，就有可能会丢包。而 TCP 为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认，其他的包即使已经收到了，也只能放在缓冲区里，上层的应用拿不出来，只能“干着急”。 我举个简单的例子：客户端用 TCP 发送了三个包，但服务器所在的操作系统只收到了后两个包，第一个包丢了。那么内核里的 TCP 协议栈就只能把已经收到的包暂存起来，“停下”等着客户端重传那个丢失的包，这样就又出现了“队头阻塞”。由于这种“队头阻塞”是 TCP 协议固有的，所以 HTTP/2 即使设计出再多的“花样”也无法解决。 Google 在推 SPDY 的时候就已经意识到了这个问题，于是就又发明了一个新的“QUIC”协议，让 HTTP 跑在 QUIC 上而不是 TCP 上。而这个“HTTP over QUIC”就是 HTTP 协议的下一个大版本，HTTP/3。它在 HTTP/2 的基础上又实现了质的飞跃，真正“完美”地解决了“队头阻塞”问题。不过 HTTP/3 目前还处于草案阶段，正式发布前可能会有变动，所以今天我尽量不谈那些不稳定的细节。这里先贴一下 HTTP/3 的协议栈图，让你对它有个大概的了解。 QUIC 协议 从这张图里，你可以看到 HTTP/3 有一个关键的改变，那就是它把下层的 TCP“抽掉”了，换成了 UDP。因为 UDP 是无序的，包之间没有依赖关系，所以就从根本上解决了“队头阻塞”。你一定知道，UDP 是一个简单、不可靠的传输协议，只是对 IP 协议的一层很薄的包装，和 TCP 相比，它实际应用的较少。不过正是因为它简单，不需要建连和断连，通信成本低，也就非常灵活、高效，“可塑性”很强。所以，QUIC 就选定了 UDP，在它之上把 TCP 的那一套连接管理、拥塞窗口、流量控制等“搬”了过来，“去其糟粕，取其精华”，打造出了一个全新的可靠传输协议，可以认为是“新时代的 TCP”。 QUIC 最早是由 Google 发明的，被称为 gQUIC。而当前正在由 IETF 标准化的 QUIC 被称为 iQUIC。两者的差异非常大，甚至比当年的 SPDY 与 HTTP/2 的差异还要大。gQUIC 混合了 UDP、TLS、HTTP，是一个应用层的协议。而 IETF 则对 gQUIC 做了“清理”，把应用部分分离出来，形成了 HTTP/3，原来的 UDP 部分“下放”到了传输层，所以 iQUIC 有时候也叫“QUIC-transport”。接下来要说的 QUIC 都是指 iQUIC，要记住，它与早期的 gQUIC 不同，是一个传输层的协议，和 TCP 是平级的。 QUIC 的特点 QUIC 基于 UDP，而 UDP 是“无连接”的，根本就不需要“握手”和“挥手”，所以天生就要比 TCP 快。就像 TCP 在 IP 的基础上实现了可靠传输一样，QUIC 也基于 UDP 实现了可靠传输，保证数据一定能够抵达目的地。它还引入了类似 HTTP/2 的“流”和“多路复用”，单个“流”是有序的，可能会因为丢包而阻塞，但其他“流”不会受到影响。为了防止网络上的中间设备（Middle Box）识别协议的细节，QUIC 全面采用加密通信，可以很好地抵御窜改和“协议僵化”（ossification）。 而且，因为 TLS1.3 已经在去年（2018）正式发布，所以 QUIC 就直接应用了 TLS1.3，顺便也就获得了 0-RTT、1-RTT 连接的好处。但 QUIC 并不是建立在 TLS 之上，而是内部“包含”了 TLS。它使用自己的帧“接管”了 TLS 里的“记录”，握手消息、警报消息都不使用 TLS 记录，直接封装成 QUIC 的帧发送，省掉了一次开销。 QUIC 内部细节 由于 QUIC 在协议栈里比较偏底层，所以我只简略介绍两个内部的关键知识点。QUIC 的基本数据传输单位是包（packet）和帧（frame），一个包由多个帧组成，包面向的是“连接”，帧面向的是“流”。QUIC 使用不透明的“连接 ID”来标记通信的两个端点，客户端和服务器可以自行选择一组 ID 来标记自己，这样就解除了 TCP 里连接对“IP 地址 + 端口”（即常说的四元组）的强绑定，支持“连接迁移”（Connection Migration）。 比如你下班回家，手机会自动由 4G 切换到 WiFi。这时 IP 地址会发生变化，TCP 就必须重新建立连接。而 QUIC 连接里的两端连接 ID 不会变，所以连接在“逻辑上”没有中断，它就可以在新的 IP 地址上继续使用之前的连接，消除重连的成本，实现连接的无缝迁移。QUIC 的帧里有多种类型，PING、ACK 等帧用于管理连接，而 STREAM 帧专门用来实现流。QUIC 里的流与 HTTP/2 的流非常相似，也是帧的序列，你可以对比着来理解。但 HTTP/2 里的流都是双向的，而 QUIC 则分为双向流和单向流。 QUIC 帧普遍采用变长编码，最少只要 1 个字节，最多有 8 个字节。流 ID 的最大可用位数是 62，数量上比 HTTP/2 的 2^31 大大增加。流 ID 还保留了最低两位用作标志，第 1 位标记流的发起者，0 表示客户端，1 表示服务器；第 2 位标记流的方向，0 表示双向流，1 表示单向流。所以 QUIC 流 ID 的奇偶性质和 HTTP/2 刚好相反，客户端的 ID 是偶数，从 0 开始计数。 HTTP/3 协议 了解了 QUIC 之后，再来看 HTTP/3 就容易多了。因为 QUIC 本身就已经支持了加密、流和多路复用，所以 HTTP/3 的工作减轻了很多，把流控制都交给 QUIC 去做。调用的不再是 TLS 的安全接口，也不是 Socket API，而是专门的 QUIC 函数。不过这个“QUIC 函数”还没有形成标准，必须要绑定到某一个具体的实现库。 HTTP/3 里仍然使用流来发送“请求 - 响应”，但它自身不需要像 HTTP/2 那样再去定义流，而是直接使用 QUIC 的流，相当于做了一个“概念映射”。HTTP/3 里的“双向流”可以完全对应到 HTTP/2 的流，而“单向流”在 HTTP/3 里用来实现控制和推送，近似地对应 HTTP/2 的 0 号流。由于流管理被“下放”到了 QUIC，所以 HTTP/3 里帧的结构也变简单了。帧头只有两个字段：类型和长度，而且同样都采用变长编码，最小只需要两个字节。 HTTP/3 里的帧仍然分成数据帧和控制帧两类，HEADERS 帧和 DATA 帧传输数据，但其他一些帧因为在下层的 QUIC 里有了替代，所以在 HTTP/3 里就都消失了，比如 RST_STREAM、WINDOW_UPDATE、PING 等。头部压缩算法在 HTTP/3 里升级成了“QPACK”，使用方式上也做了改变。虽然也分成静态表和动态表，但在流上发送 HEADERS 帧时不能更新字段，只能引用，索引表的更新需要在专门的单向流上发送指令来管理，解决了 HPACK 的“队头阻塞”问题。另外，QPACK 的字典也做了优化，静态表由之前的 61 个增加到了 98 个，而且序号从 0 开始，也就是说“:authority”的编号是 0。 HTTP/3 服务 发现讲了这么多，不知道你注意到了没有：HTTP/3 没有指定默认的端口号，也就是说不一定非要在 UDP 的 80 或者 443 上提供 HTTP/3 服务。那么，该怎么“发现”HTTP/3 呢？这就要用到 HTTP/2 里的“扩展帧”了。浏览器需要先用 HTTP/2 协议连接服务器，然后服务器可以在启动 HTTP/2 连接后发送一个“Alt-Svc”帧，包含一个“h3=host:port”的字符串，告诉浏览器在另一个端点上提供等价的 HTTP/3 服务。浏览器收到“Alt-Svc”帧，会使用 QUIC 异步连接指定的端口，如果连接成功，就会断开 HTTP/2 连接，改用新的 HTTP/3 收发数据。 小结 HTTP/3 综合了我们之前讲的所有技术（HTTP/1、SSL/TLS、HTTP/2），包含知识点很多，比如队头阻塞、0-RTT 握手、虚拟的“流”、多路复用，算得上是“集大成之作”，需要多下些功夫好好体会。 HTTP/3 基于 QUIC 协议，完全解决了“队头阻塞”问题，弱网环境下的表现会优于 HTTP/2； QUIC 是一个新的传输层协议，建立在 UDP 之上，实现了可靠传输； QUIC 内含了 TLS1.3，只能加密通信，支持 0-RTT 快速建连； QUIC 的连接使用“不透明”的连接 ID，不绑定在“IP 地址 + 端口”上，支持“连接迁移”； QUIC","date":"2022-01-24 20:52:12","objectID":"/cn_http/:4:3","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"该不该迁移到HTTP/2 前面你已经看到了新的 HTTP/2 和 HTTP/3 协议，了解了它们的特点和工作原理，如果再联系上前几天“安全篇”的 HTTPS，你可能又会发出疑问：“刚费了好大的力气升级到 HTTPS，这又出了一个 HTTP/2，还有再次升级的必要吗？”与各大浏览器“强推”HTTPS 的待遇不一样，HTTP/2 的公布可谓是“波澜不惊”。虽然它是 HTTP 协议的一个重大升级，但 Apple、Google 等科技巨头并没有像 HTTPS 那样给予大量资源的支持。直到今天，HTTP/2 在互联网上还是处于“不温不火”的状态，虽然已经有了不少的网站改造升级到了 HTTP/2，但普及的速度远不及 HTTPS。所以，你有这样的疑问也是很自然的，升级到 HTTP/2 究竟能给我们带来多少好处呢？到底“值不值”呢？ HTTP/2 的优点 前面的几讲主要关注了 HTTP/2 的内部实现，今天我们就来看看它有哪些优点和缺点。首先要说的是，HTTP/2 最大的一个优点是完全保持了与 HTTP/1 的兼容，在语义上没有任何变化，之前在 HTTP 上的所有投入都不会浪费。因为兼容 HTTP/1，所以 HTTP/2 也具有 HTTP/1 的所有优点，并且“基本”解决了 HTTP/1 的所有缺点，安全与性能兼顾，可以认为是“更安全的 HTTP、更快的 HTTPS”。 在安全上，HTTP/2 对 HTTPS 在各方面都做了强化。下层的 TLS 至少是 1.2，而且只能使用前向安全的密码套件（即 ECDHE），这同时也就默认实现了“TLS False Start”，支持 1-RTT 握手，所以不需要再加额外的配置就可以自动实现 HTTPS 加速。安全有了保障，再来看 HTTP/2 在性能方面的改进。 你应该知道，影响网络速度的两个关键因素是“带宽”和“延迟”，HTTP/2 的头部压缩、多路复用、流优先级、服务器推送等手段其实都是针对这两个要点。所谓的“带宽”就是网络的传输速度。从最早的 56K/s，到如今的 100M/s，虽然网速已经是“今非昔比”，比从前快了几十倍、几百倍，但仍然是“稀缺资源”，图片、视频这样的多媒体数据很容易会把带宽用尽。节约带宽的基本手段就是压缩，在 HTTP/1 里只能压缩 body，而 HTTP/2 则可以用 HPACK 算法压缩 header，这对高流量的网站非常有价值，有数据表明能节省大概 5%~10% 的流量，这是实实在在的“真金白银”。 与 HTTP/1“并发多个连接”不同，HTTP/2 的“多路复用”特性要求对一个域名（或者 IP）只用一个 TCP 连接，所有的数据都在这一个连接上传输，这样不仅节约了客户端、服务器和网络的资源，还可以把带宽跑满，让 TCP 充分“吃饱”。 这是为什么呢？我们来看一下在 HTTP/1 里的长连接，虽然是双向通信，但任意一个时间点实际上还是单向的：上行请求时下行空闲，下行响应时上行空闲，再加上“队头阻塞”，实际的带宽打了个“对折”还不止（可参考第 17 讲）。而在 HTTP/2 里，“多路复用”则让 TCP 开足了马力，“全速狂奔”，多个请求响应并发，每时每刻上下行方向上都有流在传输数据，没有空闲的时候，带宽的利用率能够接近 100%。所以，HTTP/2 只使用一个连接，就能抵得过 HTTP/1 里的五六个连接。不过流也可能会有依赖关系，可能会存在等待导致的阻塞，这就是“延迟”，所以 HTTP/2 的其他特性就派上了用场。“优先级”可以让客户端告诉服务器，哪个文件更重要，更需要优先传输，服务器就可以调高流的优先级，合理地分配有限的带宽资源，让高优先级的 HTML、图片更快地到达客户端，尽早加载显示。“服务器推送”也是降低延迟的有效手段，它不需要客户端预先请求，服务器直接就发给客户端，这就省去了客户端解析 HTML 再请求的时间。 HTTP/2 的缺点 说了一大堆 HTTP/2 的优点，再来看看它有什么缺点吧。听过上一讲 HTTP/3 的介绍，你就知道 HTTP/2 在 TCP 级别还是存在“队头阻塞”的问题。所以，如果网络连接质量差，发生丢包，那么 TCP 会等待重传，传输速度就会降低。另外，在移动网络中发生 IP 地址切换的时候，下层的 TCP 必须重新建连，要再次“握手”，经历“慢启动”，而且之前连接里积累的 HPACK 字典也都消失了，必须重头开始计算，导致带宽浪费和时延。刚才也说了，HTTP/2 对一个域名只开一个连接，所以一旦这个连接出问题，那么整个网站的体验也就变差了。而这些情况下 HTTP/1 反而不会受到影响，因为它“本来就慢”，而且还会对一个域名开 6~8 个连接，顶多其中的一两个连接会“更慢”，其他的连接不会受到影响。 应该迁移到 HTTP/2 吗？ 说到这里，你对迁移到 HTTP/2 是否已经有了自己的判断呢？在我看来，HTTP/2 处于一个略“尴尬”的位置，前面有“老前辈”HTTP/1，后面有“新来者”HTTP/3，即有“老前辈”的“打压”，又有“新来者”的“追赶”，也就难怪没有获得市场的大力“吹捧”了。但这绝不是说 HTTP/2“一无是处”，实际上 HTTP/2 的性能改进效果是非常明显的，**Top 1000 的网站中已经有超过 40% 运行在了 HTTP/2 上，包括知名的 Apple、Facebook、Google、Twitter 等等。**仅用了四年的时间，HTTP/2 就拥有了这么大的市场份额和巨头的认可，足以证明它的价值。因为 HTTP/2 的侧重点是“性能”，所以“是否迁移”就需要在这方面进行评估。如果网站的流量很大，那么 HTTP/2 就可以带来可观的收益；反之，如果网站流量比较小，那么升级到 HTTP/2 就没有太多必要了，只要利用现有的 HTTP 再优化就足矣。不过如果你是新建网站，我觉得完全可以跳过 HTTP/1、HTTPS，直接“一步到位”，上 HTTP/2，这样不仅可以获得性能提升，还免去了老旧的“历史包袱”，日后也不会再有迁移的烦恼。顺便再多嘴一句，HTTP/2 毕竟是“下一代”HTTP 协议，它的很多特性也延续到了 HTTP/3，提早升级到 HTTP/2 还可以让你在 HTTP/3 到来时有更多的技术积累和储备，不至于落后于时代。 配置 HTTP/2 假设你已经决定要使用 HTTP/2，应该如何搭建服务呢？因为 HTTP/2“事实上”是加密的，所以如果你已经在“安全篇”里成功迁移到了 HTTPS，那么在 Nginx 里启用 HTTP/2 简直可以说是“不费吹灰之力”，只需要在 server 配置里再多加一个参数就可以搞定了。 server { listen 443 ssl http2; server_name www.xxx.net; ssl_certificate xxx.crt; ssl_certificate_key xxx.key; 注意“listen”指令，在“ssl”后面多了一个“http2”，这就表示在 443 端口上开启了 SSL 加密，然后再启用 HTTP/2。配置服务器推送特性可以使用指令“http2_push”和“http2_push_preload”： http2_push /style/xxx.css; http2_push_preload on; 不过如何合理地配置推送是个难题，如果推送给浏览器不需要的资源，反而浪费了带宽。这方面暂时没有一般性的原则指导，你必须根据自己网站的实际情况去“猜测”客户端最需要的数据。优化方面，HTTPS 的一些策略依然适用，比如精简密码套件、ECC 证书、会话复用、HSTS 减少重定向跳转等等。但还有一些优化手段在 HTTP/2 里是不适用的，而且还会有反效果，比如说常见的精灵图（Spriting）、资源内联（inlining）、域名分片（Sharding）等，至于原因是什么，我把它留给你自己去思考（提示，与缓存有关）。还要注意一点，HTTP/2 默认启用 header 压缩（HPACK），但并没有默认启用 body 压缩，所以不要忘了在 Nginx 配置文件里加上“gzip”指令，压缩 HTML、JS 等文本数据。 应用层协议协商（ALPN） 最后说一下 HTTP/2 的“服务发现”吧。你有没有想过，在 URI 里用的都是 HTTPS 协议名，没有版本标记，浏览器怎么知道服务器支持 HTTP/2 呢？为什么上来就能用 HTTP/2，而不是用 HTTP/1 通信呢？答案在 TLS 的扩展里，有一个叫“ALPN”（Application Layer Protocol Negotiation）的东西，用来与服务器就 TLS 上跑的应用协议进行“协商”。客户端在发起“Client Hello”握手的时候，后面会带上一个“ALPN”扩展，里面按照优先顺序列出客户端支持的应用协议。就像下图这样，最优先的是“h2”，其次是“http/1.1”，以前还有“spdy”，以后还可能会有“h3”。 服务器看到 ALPN 扩展以后就可以从列表里选择一种应用协议，在“Server Hello”里也带上“ALPN”扩展，告诉客户端服务器决定使用的是哪一种。因为我们在 Nginx 配置里使用了 HTTP/2 协议，所以在这里它选择的就是“h2”。 这样在 TLS 握手结束后，客户端和服务器就通过“ALPN”完成了应用层的协议协商，后面就可以使用 HTTP/2 通信了。 小结 今天我们讨论了是否应该迁移到 HTTP/2，还有应该如何迁移到 HTTP/2。 HTTP/2 完全兼容 HTTP/1，是“更安全的 HTTP、","date":"2022-01-24 20:52:12","objectID":"/cn_http/:4:4","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"web服务器 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:5:0","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"Nginx：高性能web服务器 经过前面几大模块的学习，你已经完全掌握了 HTTP 的所有知识，那么接下来请收拾一下行囊，整理一下装备，跟我一起去探索 HTTP 之外的广阔天地。现在的互联网非常发达，用户越来越多，网速越来越快，HTTPS 的安全加密、HTTP/2 的多路复用等特性都对 Web 服务器提出了非常高的要求。一个好的 Web 服务器必须要具备稳定、快速、易扩展、易维护等特性，才能够让网站“立于不败之地”。那么，在搭建网站的时候，应该选择什么样的服务器软件呢？在开头的几讲里我也提到过，Web 服务器就那么几款，目前市面上主流的只有两个：Apache 和 Nginx，两者合计占据了近 90% 的市场份额。今天我要说的就是其中的 Nginx，它是 Web 服务器的“后起之秀”，虽然比 Apache 小了 10 岁，但增长速度十分迅猛，已经达到了与 Apache“平起平坐”的地位，而在“Top Million”网站中更是超过了 Apache，拥有超过 50% 的用户（参考数据）。 在这里必须要说一下 Nginx 的正确发音，它应该读成“Engine X”，但我个人感觉“X”念起来太“拗口”，还是比较倾向于读做“Engine ks”，这也与 UNIX、Linux 的发音一致。作为一个 Web 服务器，Nginx 的功能非常完善，完美支持 HTTP/1、HTTPS 和 HTTP/2，而且还在不断进步。当前的主线版本已经发展到了 1.17，正在进行 HTTP/3 的研发，或许一年之后就能在 Nginx 上跑 HTTP/3 了。Nginx 也是我个人的主要研究领域，我也写过相关的书，按理来说今天的课程应该是“手拿把攥”，但真正动笔的时候还是有些犹豫的：很多要点都已经在书里写过了，这次的专栏如果再重复相同的内容就不免有“骗稿费”的嫌疑，应该有些“不一样的东西”。所以我决定抛开书本，换个角度，结合 HTTP 协议来讲 Nginx，带你窥视一下 HTTP 处理的内幕，看看 Web 服务器的工作原理。 进程池 你也许听说过，Nginx 是个“轻量级”的 Web 服务器，那么这个所谓的“轻量级”是什么意思呢？“轻量级”是相对于“重量级”而言的。“重量级”就是指服务器进程很“重”，占用很多资源，当处理 HTTP 请求时会消耗大量的 CPU 和内存，受到这些资源的限制很难提高性能。而 Nginx 作为“轻量级”的服务器，它的 CPU、内存占用都非常少，同样的资源配置下就能够为更多的用户提供服务，其奥秘在于它独特的工作模式。 在 Nginx 之前，Web 服务器的工作模式大多是“Per-Process”或者“Per-Thread”，对每一个请求使用单独的进程或者线程处理。这就存在创建进程或线程的成本，还会有进程、线程“上下文切换”的额外开销。如果请求数量很多，CPU 就会在多个进程、线程之间切换时“疲于奔命”，平白地浪费了计算时间。Nginx 则完全不同，“一反惯例”地没有使用多线程，而是使用了“进程池 + 单线程”的工作模式。Nginx 在启动的时候会预先创建好固定数量的 worker 进程，在之后的运行过程中不会再 fork 出新进程，这就是进程池，而且可以自动把进程“绑定”到独立的 CPU 上，这样就完全消除了进程创建和切换的成本，能够充分利用多核 CPU 的计算能力。在进程池之上，还有一个“master”进程，专门用来管理进程池。它的作用有点像是 supervisor（一个用 Python 编写的进程管理工具），用来监控进程，自动恢复发生异常的 worker，保持进程池的稳定和服务能力。不过 master 进程完全是 Nginx 自行用 C 语言实现的，这就摆脱了外部的依赖，简化了 Nginx 的部署和配置。 I/O 多路复用 如果你用 Java、C 等语言写过程序，一定很熟悉“多线程”的概念，使用多线程能够很容易实现并发处理。但多线程也有一些缺点，除了刚才说到的“上下文切换”成本，还有编程模型复杂、数据竞争、同步等问题，写出正确、快速的多线程程序并不是一件容易的事情。所以 Nginx 就选择了单线程的方式，带来的好处就是开发简单，没有互斥锁的成本，减少系统消耗。那么，疑问也就产生了：为什么单线程的 Nginx，处理能力却能够超越其他多线程的服务器呢？这要归功于 Nginx 利用了 Linux 内核里的一件“神兵利器”，I/O 多路复用接口，“大名鼎鼎”的 epoll。 “多路复用”这个词我们已经在之前的 HTTP/2、HTTP/3 里遇到过好几次，如果你理解了那里的“多路复用”，那么面对 Nginx 的 epoll“多路复用”也就好办了。Web 服务器从根本上来说是“I/O 密集型”而不是“CPU 密集型”，处理能力的关键在于网络收发而不是 CPU 计算（这里暂时不考虑 HTTPS 的加解密），而网络 I/O 会因为各式各样的原因不得不等待，比如数据还没到达、对端没有响应、缓冲区满发不出去等等。这种情形就有点像是 HTTP 里的“队头阻塞”。对于一般的单线程来说 CPU 就会“停下来”，造成浪费。而多线程的解决思路有点类似“并发连接”，虽然有的线程可能阻塞，但由于多个线程并行，总体上看阻塞的情况就不会太严重了。 Nginx 里使用的 epoll，就好像是 HTTP/2 里的“多路复用”技术，它把多个 HTTP 请求处理打散成碎片，都“复用”到一个单线程里，不按照先来后到的顺序处理，而是只当连接上真正可读、可写的时候才处理，如果可能发生阻塞就立刻切换出去，处理其他的请求。通过这种方式，Nginx 就完全消除了 I/O 阻塞，把 CPU 利用得“满满当当”，又因为网络收发并不会消耗太多 CPU 计算能力，也不需要切换进程、线程，所以整体的 CPU 负载是相当低的。这里我画了一张 Nginx“I/O 多路复用”的示意图，你可以看到，它的形式与 HTTP/2 的流非常相似，每个请求处理单独来看是分散、阻塞的，但因为都复用到了一个线程里，所以资源的利用率非常高。 epoll 还有一个特点，大量的连接管理工作都是在操作系统内核里做的，这就减轻了应用程序的负担，所以 Nginx 可以为每个连接只分配很小的内存维护状态，即使有几万、几十万的并发连接也只会消耗几百 M 内存，而其他的 Web 服务器这个时候早就“Memory not enough”了。 多阶段处理 有了“进程池”和“I/O 多路复用”，Nginx 是如何处理 HTTP 请求的呢？Nginx 在内部也采用的是“化整为零”的思路，把整个 Web 服务器分解成了多个“功能模块”，就好像是乐高积木，可以在配置文件里任意拼接搭建，从而实现了高度的灵活性和扩展性。Nginx 的 HTTP 处理有四大类模块： handler 模块：直接处理 HTTP 请求； filter 模块：不直接处理请求，而是加工过滤响应报文； upstream 模块：实现反向代理功能，转发请求到其他服务器； balance 模块：实现反向代理时的负载均衡算法。 因为 upstream 模块和 balance 模块实现的是代理功能，Nginx 作为“中间人”，运行机制比较复杂，所以我今天只讲 handler 模块和 filter 模块。不知道你有没有了解过“设计模式”这方面的知识，其中有一个非常有用的模式叫做“职责链”。它就好像是工厂里的流水线，原料从一头流入，线上有许多工人会进行各种加工处理，最后从另一头出来的就是完整的产品。Nginx 里的 handler 模块和 filter 模块就是按照“职责链”模式设计和组织的，HTTP 请求报文就是“原材料”，各种模块就是工厂里的工人，走完模块构成的“流水线”，出来的就是处理完成的响应报文。下面的这张图显示了 Nginx 的“流水线”，在 Nginx 里的术语叫“阶段式处理”（Phases），一共有 11 个阶段，每个阶段里又有许多各司其职的模块。 我简单列几个与我们的课程相关的模块吧： charset 模块实现了字符集编码转换；（第 15 讲） chunked 模块实现了响应数据的分块传输；（第 16 讲） range 模块实现了范围请求，只返回数据的一部分；（第 16 讲） rewrite 模块实现了重定向和跳转，还可以使用内置变量自定义跳转的 URI；（第 18 讲） not_modified 模块检查头字段“if-Modified-Since”和“If-None-Match”，处理条件请求；（第 20 讲） realip 模块处理“X-Real-IP”“X-Forwarded-For”等字段，获取客户端的真实 IP 地址；（第 21 讲） ssl 模块实现了 SSL/TLS 协议支持，读取磁盘上的证书和私钥，实现 TLS 握手和 SNI、ALPN 等扩展功能；（安全篇） http_v2 模块实现了完整的 HTTP/2 协议。（飞翔篇） 在这张图里，你还可以看到 limit_conn、limit_req、access、log 等其他模块，它们实现的是限流限速、访问控制、日志等功能，不在 HTTP 协议规定之内，但对于运行在现实世界的 Web 服务器却是必备的。如果你有 C 语言基础，感兴趣的话可以下载 Nginx 的源码，在代码级别仔细看看 HTTP 的处理过程。 小结 Nginx 是一个高性能的 Web 服务器，它非常的轻量级，消耗的 CPU、内存很少； Nginx 采用“master/workers”进程池架构，不使用多线程，消除了进程、线程切换的成本； Nginx 基于 epoll 实现了“I","date":"2022-01-24 20:52:12","objectID":"/cn_http/:5:1","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"OpenResty：更灵活的web服务器 在上一讲里，我们看到了高性能的 Web 服务器 Nginx，它资源占用少，处理能力高，是搭建网站的首选。虽然 Nginx 成为了 Web 服务器领域无可争议的“王者”，但它也并不是没有缺点的，毕竟它已经 15 岁了。“一个人很难超越时代，而时代却可以轻易超越所有人”，Nginx 当初设计时针对的应用场景已经发生了变化，它的一些缺点也就暴露出来了。 Nginx 的服务管理思路延续了当时的流行做法，使用磁盘上的静态配置文件，所以每次修改后必须重启才能生效。这在业务频繁变动的时候是非常致命的（例如流行的微服务架构），特别是对于拥有成千上万台服务器的网站来说，仅仅增加或者删除一行配置就要分发、重启所有的机器，对运维是一个非常大的挑战，要耗费很多的时间和精力，成本很高，很不灵活，难以“随需应变”。那么，有没有这样的一个 Web 服务器，它有 Nginx 的优点却没有 Nginx 的缺点，既轻量级、高性能，又灵活、可动态配置呢？这就是我今天要说的 OpenResty，它是一个“更好更灵活的 Nginx”。 cOpenResty 是什么？ 其实你对 OpenResty 并不陌生，这个专栏的实验环境就是用 OpenResty 搭建的，这么多节课程下来，你应该或多或少对它有了一些印象吧。OpenResty 诞生于 2009 年，到现在刚好满 10 周岁。它的创造者是当时就职于某宝的“神级”程序员章亦春，网名叫“agentzh”。OpenResty 并不是一个全新的 Web 服务器，而是基于 Nginx，它利用了 Nginx 模块化、可扩展的特性，开发了一系列的增强模块，并把它们打包整合，形成了一个“一站式”的 Web 开发平台。 虽然 OpenResty 的核心是 Nginx，但它又超越了 Nginx，关键就在于其中的 ngx_lua 模块，把小巧灵活的 Lua 语言嵌入了 Nginx，可以用脚本的方式操作 Nginx 内部的进程、多路复用、阶段式处理等各种构件。脚本语言的好处你一定知道，它不需要编译，随写随执行，这就免去了 C 语言编写模块漫长的开发周期。而且 OpenResty 还把 Lua 自身的协程与 Nginx 的事件机制完美结合在一起，优雅地实现了许多其他语言所没有的“同步非阻塞”编程范式，能够轻松开发出高性能的 Web 应用。目前 OpenResty 有两个分支，分别是开源、免费的“OpenResty”和闭源、商业产品的“OpenResty+”，运作方式有社区支持、OpenResty 基金会、OpenResty.Inc 公司，还有其他的一些外界赞助（例如 Kong、CloudFlare），正在蓬勃发展。 顺便说一下 OpenResty 的官方 logo，是一只展翅飞翔的海鸥，选择海鸥是因为“鸥”与 OpenResty 的发音相同。另外，这个 logo 的形状也像是左手比出的一个“OK”姿势，正好也是一个“O”。 动态的 Lua 刚才说了，OpenResty 里的一个关键模块是 ngx_lua，它为 Nginx 引入了脚本语言 Lua。Lua 是一个比较“小众”的语言，虽然历史比较悠久，但名气却没有 PHP、Python、JavaScript 大，这主要与它的自身定位有关。 Lua 的设计目标是嵌入到其他应用程序里运行，为其他编程语言带来“脚本化”能力，所以它的“个头”比较小，功能集有限，不追求“大而全”，而是“小而美”，大多数时间都“隐匿”在其他应用程序的后面，是“无名英雄”。你或许玩过或者听说过《魔兽世界》《愤怒的小鸟》吧，它们就在内部嵌入了 Lua，使用 Lua 来调用底层接口，充当“胶水语言”（glue language），编写游戏逻辑脚本，提高开发效率。OpenResty 选择 Lua 作为“工作语言”也是基于同样的考虑。因为 Nginx C 开发实在是太麻烦了，限制了 Nginx 的真正实力。而 Lua 作为“最快的脚本语言”恰好可以成为 Nginx 的完美搭档，既可以简化开发，性能上又不会有太多的损耗。 作为脚本语言，Lua 还有一个重要的“代码热加载”特性，不需要重启进程，就能够从磁盘、Redis 或者任何其他地方加载数据，随时替换内存里的代码片段。这就带来了“动态配置”，让 OpenResty 能够永不停机，在微秒、毫秒级别实现配置和业务逻辑的实时更新，比起 Nginx 秒级的重启是一个极大的进步。你可以看一下实验环境的“www/lua”目录，里面存放了我写的一些测试 HTTP 特性的 Lua 脚本，代码都非常简单易懂，就像是普通的英语“阅读理解”，这也是 Lua 的另一个优势：易学习、易上手。 你可以看一下实验环境的“www/lua”目录，里面存放了我写的一些测试 HTTP 特性的 Lua 脚本，代码都非常简单易懂，就像是普通的英语“阅读理解”，这也是 Lua 的另一个优势：易学习、易上手。 高效率的 Lua OpenResty 能够高效运行的一大“秘技”是它的“同步非阻塞”编程范式，如果你要开发 OpenResty 应用就必须时刻铭记于心。“同步非阻塞”本质上还是一种“多路复用”，我拿上一讲的 Nginx epoll 来对比解释一下。epoll 是操作系统级别的“多路复用”，运行在内核空间。而 OpenResty 的“同步非阻塞”则是基于 Lua 内建的“协程”，是应用程序级别的“多路复用”，运行在用户空间，所以它的资源消耗要更少。OpenResty 里每一段 Lua 程序都由协程来调度运行。和 Linux 的 epoll 一样，每当可能发生阻塞的时候“协程”就会立刻切换出去，执行其他的程序。这样单个处理流程是“阻塞”的，但整个 OpenResty 却是“非阻塞的”，多个程序都“复用”在一个 Lua 虚拟机里运行。 下面的代码是一个简单的例子，读取 POST 发送的 body 数据，然后再发回客户端： ngx.req.read_body() -- 同步非阻塞(1) local data = ngx.req.get_body_data() if data then ngx.print(\"body: \", data) -- 同步非阻塞(2) end 代码中的“ngx.req.read_body”和“ngx.print”分别是数据的收发动作，只有收到数据才能发送数据，所以是“同步”的。但即使因为网络原因没收到或者发不出去，OpenResty 也不会在这里阻塞“干等着”，而是做个“记号”，把等待的这段 CPU 时间用来处理其他的请求，等网络可读或者可写时再“回来”接着运行。 假设收发数据的等待时间是 10 毫秒，而真正 CPU 处理的时间是 0.1 毫秒，那么 OpenResty 就可以在这 10 毫秒内同时处理 100 个请求，而不是把这 100 个请求阻塞排队，用 1000 毫秒来处理。 除了“同步非阻塞”，OpenResty 还选用了 LuaJIT 作为 Lua 语言的“运行时（Runtime）”，进一步“挖潜增效”。 LuaJIT 是一个高效的 Lua 虚拟机，支持 JIT（Just In Time）技术，可以把 Lua 代码即时编译成“本地机器码”，这样就消除了脚本语言解释运行的劣势，让 Lua 脚本跑得和原生 C 代码一样快。另外，LuaJIT 还为 Lua 语言添加了一些特别的增强，比如二进制位运算库 bit，内存优化库 table，还有 FFI（Foreign Function Interface），让 Lua 直接调用底层 C 函数，比原生的压栈调用快很多。 阶段式处理 和 Nginx 一样，OpenResty 也使用“流水线”来处理 HTTP 请求，底层的运行基础是 Nginx 的“阶段式处理”，但它又有自己的特色。Nginx 的“流水线”是由一个个 C 模块组成的，只能在静态文件里配置，开发困难，配置麻烦（相对而言）。而 OpenResty 的“流水线”则是由一个个的 Lua 脚本组成的，不仅可以从磁盘上加载，也可以从 Redis、MySQL 里加载，而且编写、调试的过程非常方便快捷。下面我画了一张图，列出了 OpenResty 的阶段，比起 Nginx，OpenResty 的阶段更注重对 HTTP 请求响应报文的加工和处理。 OpenResty 里有几个阶段与 Nginx 是相同的，比如 rewrite、access、content、filter，这些都是标准的 HTTP 处理。在这几个阶段里可以用“xxx_by_lua”指令嵌入 Lua 代码，执行重定向跳转、访问控制、产生响应、负载均衡、过滤报文等功能。因为 Lua 的脚本语言特性，不用考虑内存分配、资源回收释放等底层的细节问题，可以专注于编写非常复杂的业务逻辑，比 C 模块的开发效率高很多，即易于扩展又易于维护。 OpenResty 里还有两个不同于 Nginx 的特殊阶段。一个是“init 阶段”，它又分成“master init”和“worker init”，在 master 进程和 worker 进程启动的时候运行。这个阶段还没有开始提供服务，所以慢一点也没关系，可以调用一些阻塞的接口初始化服务器，比如读取磁盘、MySQL，加载黑白名单或者数据模型，然后放进共享内存里供运行时使用。另一个是“ssl 阶段”，这算得上是 OpenResty 的一大创举，可以在 TLS 握手时动态加载证书，或者发送“OCSP Stapling”。 还记得第 29 讲里说的“SNI 扩展”吗？Nginx 可以依据“服务器名称指示”来选择证","date":"2022-01-24 20:52:12","objectID":"/cn_http/:5:2","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"WAF：保护我们的网络服务 在前些天的“安全篇”里，我谈到了 HTTPS，它使用了 SSL/TLS 协议，加密整个通信过程，能够防止恶意窃听和窜改，保护我们的数据安全。但 HTTPS 只是网络安全中很小的一部分，仅仅保证了“通信链路安全”，让第三方无法得知传输的内容。在通信链路的两端，也就是客户端和服务器，它是无法提供保护的。因为 HTTP 是一个开放的协议，Web 服务都运行在公网上，任何人都可以访问，所以天然就会成为黑客的攻击目标。而且黑客的本领比我们想象的还要大得多。虽然不能在传输过程中做手脚，但他们还可以“假扮”成合法的用户访问系统，然后伺机搞破坏。 Web 服务遇到的威胁 黑客都有哪些手段来攻击 Web 服务呢？我给你大概列出几种常见的方式。第一种叫“DDoS”攻击（distributed denial-of-service attack），有时候也叫“洪水攻击”。黑客会控制许多“僵尸”计算机，向目标服务器发起大量无效请求。因为服务器无法区分正常用户和黑客，只能“照单全收”，这样就挤占了正常用户所应有的资源。如果黑客的攻击强度很大，就会像“洪水”一样对网站的服务能力造成冲击，耗尽带宽、CPU 和内存，导致网站完全无法提供正常服务。 “DDoS”攻击方式比较“简单粗暴”，虽然很有效，但不涉及 HTTP 协议内部的细节，“技术含量”比较低，不过下面要说的几种手段就不一样了。 网站后台的 Web 服务经常会提取出 HTTP 报文里的各种信息，应用于业务，有时会缺乏严格的检查。因为 HTTP 报文在语义结构上非常松散、灵活，URI 里的 query 字符串、头字段、body 数据都可以任意设置，这就带来了安全隐患，给了黑客“代码注入”的可能性。黑客可以精心编制 HTTP 请求报文，发送给服务器，服务程序如果没有做防备，就会“上当受骗”，执行黑客设定的代码。 “SQL 注入”（SQL injection）应该算是最著名的一种“代码注入”攻击了，它利用了服务器字符串拼接形成 SQL 语句的漏洞，构造出非正常的 SQL 语句，获取数据库内部的敏感信息。 另一种“HTTP 头注入”攻击的方式也是类似的原理，它在“Host”“User-Agent”“X-Forwarded-For”等字段里加入了恶意数据或代码，服务端程序如果解析不当，就会执行预设的恶意代码。在之前的第 19 讲里，也说过一种利用 Cookie 的攻击手段，“跨站脚本”（XSS）攻击，它属于“JS 代码注入”，利用 JavaScript 脚本获取未设防的 Cookie。 网络应用防火墙 面对这么多的黑客攻击手段，我们应该怎么防御呢？这就要用到“网络应用防火墙”（Web Application Firewall）了，简称为“WAF”。你可能对传统的“防火墙”比较熟悉。传统“防火墙”工作在三层或者四层，隔离了外网和内网，使用预设的规则，只允许某些特定 IP 地址和端口号的数据包通过，拒绝不符合条件的数据流入或流出内网，实质上是一种网络数据过滤设备。WAF 也是一种“防火墙”，但它工作在七层，看到的不仅是 IP 地址和端口号，还能看到整个 HTTP 报文，所以就能够对报文内容做更深入细致的审核，使用更复杂的条件、规则来过滤数据。说白了，WAF 就是一种“HTTP 入侵检测和防御系统”。 WAF 都能干什么呢？通常一款产品能够称为 WAF，要具备下面的一些功能： IP 黑名单和白名单，拒绝黑名单上地址的访问，或者只允许白名单上的用户访问； URI 黑名单和白名单，与 IP 黑白名单类似，允许或禁止对某些 URI 的访问； 防护 DDoS 攻击，对特定的 IP 地址限连限速； 过滤请求报文，防御“代码注入”攻击； 过滤响应报文，防御敏感信息外泄； 审计日志，记录所有检测到的入侵操作。 听起来 WAF 好像很高深，但如果你理解了它的工作原理，其实也不难。它就像是平时编写程序时必须要做的函数入口参数检查，拿到 HTTP 请求、响应报文，用字符串处理函数看看有没有关键字、敏感词，或者用正则表达式做一下模式匹配，命中了规则就执行对应的动作，比如返回 403/404。 如果你比较熟悉 Apache、Nginx、OpenResty，可以自己改改配置文件，写点 JS 或者 Lua 代码，就能够实现基本的 WAF 功能。比如说，在 Nginx 里实现 IP 地址黑名单，可以利用“map”指令，从变量 $remote_addr 获取 IP 地址，在黑名单上就映射为值 1，然后在“if”指令里判断： map $remote_addr $blocked { default 0; \"1.2.3.4\" 1; \"5.6.7.8\" 1; } if ($blocked) { return 403 \"you are blocked.\"; } Nginx 的配置文件只能静态加载，改名单必须重启，比较麻烦。如果换成 OpenResty 就会非常方便，在 access 阶段进行判断，IP 地址列表可以使用 cosocket 连接外部的 Redis、MySQL 等数据库，实现动态更新： local ip_addr = ngx.var.remote_addr local rds = redis:new() if rds:get(ip_addr) == 1 then ngx.exit(403) end 看了上面的两个例子，你是不是有种“跃跃欲试”的冲动了，想自己动手开发一个 WAF？ 不过我必须要提醒你，在网络安全领域必须时刻记得“木桶效应”（也叫“短板效应”）。网站的整体安全不在于你加固的最强的那个方向，而是在于你可能都没有意识到的“短板”。黑客往往会“避重就轻”，只要发现了网站的一个弱点，就可以“一点突破”，其他方面的安全措施也就都成了“无用功”。 所以，使用 WAF 最好“不要重新发明轮子”，而是使用现有的、比较成熟的、经过实际考验的 WAF 产品。 全面的 WAF 解决方案 这里我就要“隆重”介绍一下 WAF 领域里的最顶级产品了：ModSecurity，它可以说是 WAF 界“事实上的标准”。ModSecurity 是一个开源的、生产级的 WAF 工具包，历史很悠久，比 Nginx 还要大几岁。它开始于一个私人项目，后来被商业公司 Breach Security 收购，现在则是由 TrustWave 公司的 SpiderLabs 团队负责维护。 ModSecurity 最早是 Apache 的一个模块，只能运行在 Apache 上。因为其品质出众，大受欢迎，后来的 2.x 版添加了 Nginx 和 IIS 支持，但因为底层架构存在差异，不够稳定。所以，这两年 SpiderLabs 团队就开发了全新的 3.0 版本，移除了对 Apache 架构的依赖，使用新的“连接器”来集成进 Apache 或者 Nginx，比 2.x 版更加稳定和快速，误报率也更低。 ModSecurity 有两个核心组件。第一个是“规则引擎”，它实现了自定义的“SecRule”语言，有自己特定的语法。但“SecRule”主要基于正则表达式，还是不够灵活，所以后来也引入了 Lua，实现了脚本化配置。 ModSecurity 的规则引擎使用 C++11 实现，可以从GitHub上下载源码，然后集成进 Nginx。因为它比较庞大，编译很费时间，所以最好编译成动态模块，在配置文件里用指令“load_module”加载： load_module modules/ngx_http_modsecurity_module.so; 只有引擎还不够，要让引擎运转起来，还需要完善的防御规则，所以 ModSecurity 的第二个核心组件就是它的“规则集”。ModSecurity 源码提供一个基本的规则配置文件“modsecurity.conf-recommended”，使用前要把它的后缀改成“conf”。 有了规则集，就可以在 Nginx 配置文件里加载，然后启动规则引擎： modsecurity on; modsecurity_rules_file /path/to/modsecurity.conf; “modsecurity.conf”文件默认只有检测功能，不提供入侵阻断，这是为了防止误杀误报，把“SecRuleEngine”后面改成“On”就可以开启完全的防护： #SecRuleEngine DetectionOnly SecRuleEngine On 基本的规则集之外，ModSecurity 还额外提供一个更完善的规则集，为网站提供全面可靠的保护。这个规则集的全名叫“OWASP ModSecurity 核心规则集”（Open Web Application Security Project ModSecurity Core Rule Set），因为名字太长了，所以有时候会简称为“核心规则集”或者“CRS”。 CRS 也是完全开源、免费的，可以从 GitHub 上下载： git clone https://github.com/SpiderLabs/owasp-modsecurity-crs.git 其中有一个“crs-setup.conf.example”的文件，它是 CRS 的基本配置，可以用“Include”命令添加到“modsecurity.conf”里，然后再添加“rules”里的各种规则。 Include /path/to/crs-setup.conf Include /path/to/rules/*.conf 你如果有兴趣可以看一下这些配置文件，里面用“SecRule","date":"2022-01-24 20:52:12","objectID":"/cn_http/:5:3","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"CDN：加速我们的网络服务 我们先来看看到现在为止 HTTP 手头都有了哪些“武器”。协议方面，HTTPS 强化通信链路安全、HTTP/2 优化传输效率；应用方面，Nginx/OpenResty 提升网站服务能力，WAF 抵御网站入侵攻击，讲到这里，你是不是感觉还少了点什么？没错，在应用领域，还缺一个在外部加速 HTTP 协议的服务，这个就是我们今天要说的 CDN（Content Delivery Network 或 Content Distribution Network），中文名叫“内容分发网络”。 为什么要有网络加速？ 你可能要问了，HTTP 的传输速度也不算差啊，而且还有更好的 HTTP/2，为什么还要再有一个额外的 CDN 来加速呢？是不是有点“多此一举”呢？这里我们就必须要考虑现实中会遇到的问题了。你一定知道，光速是有限的，虽然每秒 30 万公里，但这只是真空中的上限，在实际的电缆、光缆中的速度会下降到原本的三分之二左右，也就是 20 万公里 / 秒，这样一来，地理位置的距离导致的传输延迟就会变得比较明显了。 比如，北京到广州直线距离大约是 2000 公里，按照刚才的 20 万公里 / 秒来算的话，发送一个请求单程就要 10 毫秒，往返要 20 毫秒，即使什么都不干，这个“硬性”的时延也是躲不过的。另外不要忘了， 互联网从逻辑上看是一张大网，但实际上是由许多小网络组成的，这其中就有小网络“互连互通”的问题，典型的就是各个电信运营商的网络，比如国内的电信、联通、移动三大家。 这些小网络内部的沟通很顺畅，但网络之间却只有很少的联通点。如果你在 A 网络，而网站在 C 网络，那么就必须“跨网”传输，和成千上万的其他用户一起去“挤”连接点的“独木桥”。而带宽终究是有限的，能抢到多少只能看你的运气。 还有，网络中还存在许多的路由器、网关，数据每经过一个节点，都要停顿一下，在二层、三层解析转发，这也会消耗一定的时间，带来延迟。把这些因素再放到全球来看，地理距离、运营商网络、路由转发的影响就会成倍增加。想象一下，你在北京，访问旧金山的网站，要跨越半个地球，中间会有多少环节，会增加多少时延？最终结果就是，如果仅用现有的 HTTP 传输方式，大多数网站都会访问速度缓慢、用户体验糟糕。 什么是 CDN？ 这个时候 CDN 就出现了，它就是专门为解决“长距离”上网络访问速度慢而诞生的一种网络应用服务。从名字上看，CDN 有三个关键词：“内容”“分发”和“网络”。 先看一下“网络”的含义。CDN 的最核心原则是“就近访问”，如果用户能够在本地几十公里的距离之内获取到数据，那么时延就基本上变成 0 了。所以 CDN 投入了大笔资金，在全国、乃至全球的各个大枢纽城市都建立了机房，部署了大量拥有高存储高带宽的节点，构建了一个专用网络。这个网络是跨运营商、跨地域的，虽然内部也划分成多个小网络，但它们之间用高速专有线路连接，是真正的“信息高速公路”，基本上可以认为不存在网络拥堵。 有了这个高速的专用网之后，CDN 就要“分发”源站的“内容”了，用到的就是在第 22 讲说过的“缓存代理”技术。使用“推”或者“拉”的手段，把源站的内容逐级缓存到网络的每一个节点上。于是，用户在上网的时候就不直接访问源站，而是访问离他“最近的”一个 CDN 节点，术语叫“边缘节点”（edge node），其实就是缓存了源站内容的代理服务器，这样一来就省去了“长途跋涉”的时间成本，实现了“网络加速”。 那么，CDN 都能加速什么样的“内容”呢？在 CDN 领域里，“内容”其实就是 HTTP 协议里的“资源”，比如超文本、图片、视频、应用程序安装包等等。 资源按照是否可缓存又分为“静态资源”和“动态资源”。所谓的“静态资源”是指数据内容“静态不变”，任何时候来访问都是一样的，比如图片、音频。所谓的“动态资源”是指数据内容是“动态变化”的，也就是由后台服务计算生成的，每次访问都不一样，比如商品的库存、微博的粉丝数等。很显然，只有静态资源才能够被缓存加速、就近访问，而动态资源只能由源站实时生成，即使缓存了也没有意义。不过，如果动态资源指定了“Cache-Control”，允许缓存短暂的时间，那它在这段时间里也就变成了“静态资源”，可以被 CDN 缓存加速。 套用一句广告词来形容 CDN 吧，我觉得非常恰当：“我们不生产内容，我们只是内容的搬运工。”CDN，正是把“数据传输”这件看似简单的事情“做大做强”“做专做精”，就像专门的快递公司一样，在互联网世界里实现了它的价值。 CDN 的负载均衡 我们再来看看 CDN 是具体怎么运行的，它有两个关键组成部分：全局负载均衡和缓存系统，对应的是 DNS（第 6 讲）和缓存代理（第 21 讲、第 22 讲）技术。 全局负载均衡（Global Sever Load Balance）一般简称为 GSLB，它是 CDN 的“大脑”，主要的职责是当用户接入网络的时候在 CDN 专网中挑选出一个“最佳”节点提供服务，解决的是用户如何找到“最近的”边缘节点，对整个 CDN 网络进行“负载均衡”。 GSLB 最常见的实现方式是“DNS 负载均衡”，这个在第 6 讲里也说过，不过 GSLB 的方式要略微复杂一些。原来没有 CDN 的时候，权威 DNS 返回的是网站自己服务器的实际 IP 地址，浏览器收到 DNS 解析结果后直连网站。但加入 CDN 后就不一样了，权威 DNS 返回的不是 IP 地址，而是一个 CNAME( Canonical Name ) 别名记录，指向的就是 CDN 的 GSLB。它有点像是 HTTP/2 里“Alt-Svc”的意思，告诉外面：“我这里暂时没法给你真正的地址，你去另外一个地方再查查看吧。” 因为没拿到 IP 地址，于是本地 DNS 就会向 GSLB 再发起请求，这样就进入了 CDN 的全局负载均衡系统，开始“智能调度”，主要的依据有这么几个： 看用户的 IP 地址，查表得知地理位置，找相对最近的边缘节点； 看用户所在的运营商网络，找相同网络的边缘节点； 检查边缘节点的负载情况，找负载较轻的节点； 其他，比如节点的“健康状况”、服务能力、带宽、响应时间等。 GSLB 把这些因素综合起来，用一个复杂的算法，最后找出一台“最合适”的边缘节点，把这个节点的 IP 地址返回给用户，用户就可以“就近”访问 CDN 的缓存代理了。 CDN 的缓存代理 缓存系统是 CDN 的另一个关键组成部分，相当于 CDN 的“心脏”。如果缓存系统的服务能力不够，不能很好地满足用户的需求，那 GSLB 调度算法再优秀也没有用。但互联网上的资源是无穷无尽的，不管 CDN 厂商有多大的实力，也不可能把所有资源都缓存起来。所以，缓存系统只能有选择地缓存那些最常用的那些资源。这里就有两个 CDN 的关键概念：“命中”和“回源”。 “命中”就是指用户访问的资源恰好在缓存系统里，可以直接返回给用户；“回源”则正相反，缓存里没有，必须用代理的方式回源站取。相应地，也就有了两个衡量 CDN 服务质量的指标：“命中率”和“回源率”。命中率就是命中次数与所有访问次数之比，回源率是回源次数与所有访问次数之比。显然，好的 CDN 应该是命中率越高越好，回源率越低越好。现在的商业 CDN 命中率都在 90% 以上，相当于把源站的服务能力放大了 10 倍以上。 怎么样才能尽可能地提高命中率、降低回源率呢？ 首先，最基本的方式就是在存储系统上下功夫，硬件用高速 CPU、大内存、万兆网卡，再搭配 TB 级别的硬盘和快速的 SSD。软件方面则不断“求新求变”，各种新的存储软件都会拿来尝试，比如 Memcache、Redis、Ceph，尽可能地高效利用存储，存下更多的内容。其次，缓存系统也可以划分出层次，分成一级缓存节点和二级缓存节点。一级缓存配置高一些，直连源站，二级缓存配置低一些，直连用户。回源的时候二级缓存只找一级缓存，一级缓存没有才回源站，这样最终“扇入度”就缩小了，可以有效地减少真正的回源。第三个就是使用高性能的缓存服务，据我所知，目前国内的 CDN 厂商内部都是基于开源软件定制的。最常用的是专门的缓存代理软件 Squid、Varnish，还有新兴的 ATS（Apache Traffic Server），而 Nginx 和 OpenResty 作为 Web 服务器领域的“多面手”，凭借着强大的反向代理能力和模块化、易于扩展的优点，也在 CDN 里占据了不少的份额。 小结 CDN 发展到现在已经有二十来年的历史了，早期的 CDN 功能比较简单，只能加速静态资源。随着这些年 Web 2.0、HTTPS、视频、直播等新技术、新业务的崛起，它也在不断进步，增加了很多的新功能，比如 SSL 加速、内容优化（数据压缩、图片格式转换、视频转码）、资源防盗链、WAF 安全防护等等。 现在，再说 CDN 是“搬运工”已经不太准确了，它更像是一个“无微不至”的“网站保姆”，让网站只安心生产优质的内容，其他的“杂事”都由它去代劳。 由于客观地理距离的存在，直连网站访问速度会很慢，所以就出现了 CDN； CDN 构建了全国、全球级别的专网，让用户就近访问专网里的边缘节点，降低了传输延迟，实现了网站加速； GSLB 是 CDN 的“大脑”，使用 DNS 负载均衡技术，智能调度边缘节点提供服务； 缓存系统是 CDN 的“心脏”，使用 HTTP 缓存代理技术，缓存命中就返回给用户，否则就要回源。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:5:4","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"WebSocket：沙盒里的TCP 在之前讲 TCP/IP 协议栈的时候，我说过有“TCP Socket”，它实际上是一种功能接口，通过这些接口就可以使用 TCP/IP 协议栈在传输层收发数据。那么，你知道还有一种东西叫“WebSocket”吗？ 单从名字上看，“Web”指的是 HTTP，“Socket”是套接字调用，那么这两个连起来又是什么意思呢？所谓“望文生义”，大概你也能猜出来，“WebSocket”就是运行在“Web”，也就是 HTTP 上的 Socket 通信规范，提供与“TCP Socket”类似的功能，使用它就可以像“TCP Socket”一样调用下层协议栈，任意地收发数据。 更准确地说，“WebSocket”是一种基于 TCP 的轻量级网络通信协议，在地位上是与 HTTP“平级”的。 为什么要有 WebSocket 不过，已经有了被广泛应用的 HTTP 协议，为什么要再出一个 WebSocket 呢？它有哪些好处呢？其实 WebSocket 与 HTTP/2 一样，都是为了解决 HTTP 某方面的缺陷而诞生的。HTTP/2 针对的是“队头阻塞”，而 WebSocket 针对的是“请求 - 应答”通信模式。那么，“请求 - 应答”有什么不好的地方呢？ “请求 - 应答”是一种“半双工”的通信模式，虽然可以双向收发数据，但同一时刻只能一个方向上有动作，传输效率低。更关键的一点，它是一种“被动”通信模式，服务器只能“被动”响应客户端的请求，无法主动向客户端发送数据。虽然后来的 HTTP/2、HTTP/3 新增了 Stream、Server Push 等特性，但“请求 - 应答”依然是主要的工作方式。这就导致 HTTP 难以应用在动态页面、即时消息、网络游戏等要求“实时通信”的领域。 在 WebSocket 出现之前，在浏览器环境里用 JavaScript 开发实时 Web 应用很麻烦。因为浏览器是一个“受限的沙盒”，不能用 TCP，只有 HTTP 协议可用，所以就出现了很多“变通”的技术，“轮询”（polling）就是比较常用的的一种。 简单地说，轮询就是不停地向服务器发送 HTTP 请求，问有没有数据，有数据的话服务器就用响应报文回应。如果轮询的频率比较高，那么就可以近似地实现“实时通信”的效果。但轮询的缺点也很明显，反复发送无效查询请求耗费了大量的带宽和 CPU 资源，非常不经济。 所以，为了克服 HTTP“请求 - 应答”模式的缺点，WebSocket 就“应运而生”了。它原来是 HTML5 的一部分，后来“自立门户”，形成了一个单独的标准，RFC 文档编号是 6455。 WebSocket 的特点 WebSocket 是一个真正“全双工”的通信协议，与 TCP 一样，客户端和服务器都可以随时向对方发送数据，而不用像 HTTP“你拍一，我拍一”那么“客套”。于是，服务器就可以变得更加“主动”了。一旦后台有新的数据，就可以立即“推送”给客户端，不需要客户端轮询，“实时通信”的效率也就提高了。 WebSocket 采用了二进制帧结构，语法、语义与 HTTP 完全不兼容，但因为它的主要运行环境是浏览器，为了便于推广和应用，就不得不“搭便车”，在使用习惯上尽量向 HTTP 靠拢，这就是它名字里“Web”的含义。 WebSocket 的默认端口也选择了 80 和 443，因为现在互联网上的防火墙屏蔽了绝大多数的端口，只对 HTTP 的 80、443 端口“放行”，所以 WebSocket 就可以“伪装”成 HTTP 协议，比较容易地“穿透”防火墙，与服务器建立连接。具体是怎么“伪装”的，我稍后再讲。 下面我举几个 WebSocket 服务的例子，你看看，是不是和 HTTP 几乎一模一样： ws://www.chrono.com ws://www.chrono.com:8080/srv wss://www.chrono.com:445/im?user_id=xxx 要注意的一点是，WebSocket 的名字容易让人产生误解，虽然大多数情况下我们会在浏览器里调用 API 来使用 WebSocket，但它不是一个“调用接口的集合”，而是一个通信协议，所以我觉得把它理解成“TCP over Web”会更恰当一些。 WebSocket 的帧结构 刚才说了，WebSocket 用的也是二进制帧，有之前 HTTP/2、HTTP/3 的经验，相信你这次也能很快掌握 WebSocket 的报文结构。不过 WebSocket 和 HTTP/2 的关注点不同，WebSocket 更侧重于“实时通信”，而 HTTP/2 更侧重于提高传输效率，所以两者的帧结构也有很大的区别。WebSocket 虽然有“帧”，但却没有像 HTTP/2 那样定义“流”，也就不存在“多路复用”“优先级”等复杂的特性，而它自身就是“全双工”的，也就不需要“服务器推送”。所以综合起来，WebSocket 的帧学习起来会简单一些。下图就是 WebSocket 的帧结构定义，长度不固定，最少 2 个字节，最多 14 字节，看着好像很复杂，实际非常简单。 开头的两个字节是必须的，也是最关键的。第一个字节的第一位“FIN”是消息结束的标志位，相当于 HTTP/2 里的“END_STREAM”，表示数据发送完毕。一个消息可以拆成多个帧，接收方看到“FIN”后，就可以把前面的帧拼起来，组成完整的消息。“FIN”后面的三个位是保留位，目前没有任何意义，但必须是 0。 第一个字节的后 4 位很重要，叫“Opcode”，操作码，其实就是帧类型，比如 1 表示帧内容是纯文本，2 表示帧内容是二进制数据，8 是关闭连接，9 和 10 分别是连接保活的 PING 和 PONG。 第二个字节第一位是掩码标志位“MASK”，表示帧内容是否使用异或操作（xor）做简单的加密。目前的 WebSocket 标准规定，客户端发送数据必须使用掩码，而服务器发送则必须不使用掩码。第二个字节后 7 位是“Payload len”，表示帧内容的长度。它是另一种变长编码，最少 7 位，最多是 7+64 位，也就是额外增加 8 个字节，所以一个 WebSocket 帧最大是 2^64。 长度字段后面是“Masking-key”，掩码密钥，它是由上面的标志位“MASK”决定的，如果使用掩码就是 4 个字节的随机数，否则就不存在。这么分析下来，其实 WebSocket 的帧头就四个部分：“结束标志位 + 操作码 + 帧长度 + 掩码”，只是使用了变长编码的“小花招”，不像 HTTP/2 定长报文头那么简单明了。我们的实验环境利用 OpenResty 的“lua-resty-websocket”库，实现了一个简单的 WebSocket 通信，你可以访问 URI“/38-1”，它会连接后端的 WebSocket 服务“ws://127.0.0.1/38-0”，用 Wireshark 抓包就可以看到 WebSocket 的整个通信过程。下面的截图是其中的一个文本帧，因为它是客户端发出的，所以需要掩码，报文头就在两个字节之外多了四个字节的“Masking-key”，总共是 6 个字节。 而报文内容经过掩码，不是直接可见的明文，但掩码的安全强度几乎是零，用“Masking-key”简单地异或一下就可以转换出明文。 WebSocket 的握手 和 TCP、TLS 一样，WebSocket 也要有一个握手过程，然后才能正式收发数据。这里它还是搭上了 HTTP 的“便车”，利用了 HTTP 本身的“协议升级”特性，“伪装”成 HTTP，这样就能绕过浏览器沙盒、网络防火墙等等限制，这也是 WebSocket 与 HTTP 的另一个重要关联点。WebSocket 的握手是一个标准的 HTTP GET 请求，但要带上两个协议升级的专用头字段： “Connection: Upgrade”，表示要求协议“升级”；“Upgrade: websocket”，表示要“升级”成 WebSocket 协议。 另外，为了防止普通的 HTTP 消息被“意外”识别成 WebSocket，握手消息还增加了两个额外的认证用头字段（所谓的“挑战”，Challenge）： Sec-WebSocket-Key：一个 Base64 编码的 16 字节随机数，作为简单的认证密钥；Sec-WebSocket-Version：协议的版本号，当前必须是 13。 服务器收到 HTTP 请求报文，看到上面的四个字段，就知道这不是一个普通的 GET 请求，而是 WebSocket 的升级请求，于是就不走普通的 HTTP 处理流程，而是构造一个特殊的“101 Switching Protocols”响应报文，通知客户端，接下来就不用 HTTP 了，全改用 WebSocket 协议通信。（有点像 TLS 的“Change Cipher Spec”）WebSocket 的握手响应报文也是有特殊格式的，要用字段“Sec-WebSocket-Accept”验证客户端请求报文，同样也是为了防止误连接。具体的做法是把请求头里“Sec-WebSocket-Key”的值，加上一个专用的 UUID “258EAFA5-E914-47DA-95CA-C5AB0DC85B11”，再计算 SHA-1 摘要。 encode_base64( sha1( Sec-WebSocket-Key + '258EAFA5-E914-47DA-95CA-C5AB0DC85B11' )) 客户端收到响应报文，就可以用同样的算法，比对值是否相等，如果相等，","date":"2022-01-24 20:52:12","objectID":"/cn_http/:5:5","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"总结 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:6:0","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"HTTP性能优化 由于 HTTPS（SSL/TLS）的优化已经在第 28 讲里介绍的比较详细了，所以这次就暂时略过不谈，你可以课后再找机会复习。既然要做性能优化，那么，我们就需要知道：什么是性能？它都有哪些指标，又应该如何度量，进而采取哪些手段去优化？“性能”其实是一个复杂的概念。不同的人、不同的应用场景都会对它有不同的定义。对于 HTTP 来说，它又是一个非常复杂的系统，里面有非常多的角色，所以很难用一两个简单的词就能把性能描述清楚。还是从 HTTP 最基本的“请求 - 应答”模型来着手吧。在这个模型里有两个角色：客户端和服务器，还有中间的传输链路，考查性能就可以看这三个部分。 HTTP 服务器 我们先来看看服务器，它一般运行在 Linux 操作系统上，用 Apache、Nginx 等 Web 服务器软件对外提供服务，所以，性能的含义就是它的服务能力，也就是尽可能多、尽可能快地处理用户的请求。衡量服务器性能的主要指标有三个：吞吐量（requests per second）、并发数（concurrency）和响应时间（time per request）。 吞吐量就是我们常说的 RPS，每秒的请求次数，也有叫 TPS、QPS，它是服务器最基本的性能指标，RPS 越高就说明服务器的性能越好。并发数反映的是服务器的负载能力，也就是服务器能够同时支持的客户端数量，当然也是越多越好，能够服务更多的用户。响应时间反映的是服务器的处理能力，也就是快慢程度，响应时间越短，单位时间内服务器就能够给越多的用户提供服务，提高吞吐量和并发数。 除了上面的三个基本性能指标，服务器还要考虑 CPU、内存、硬盘和网卡等系统资源的占用程度，利用率过高或者过低都可能有问题。在 HTTP 多年的发展过程中，已经出现了很多成熟的工具来测量这些服务器的性能指标，开源的、商业的、命令行的、图形化的都有。在 Linux 上，最常用的性能测试工具可能就是 ab（Apache Bench）了，比如，下面的命令指定了并发数 100，总共发送 10000 个请求： ab -c 100 -n 10000 'http://www.xxx.com' 系统资源监控方面，Linux 自带的工具也非常多，常用的有 uptime、top、vmstat、netstat、sar 等等，可能你比我还要熟悉，我就列几个简单的例子吧： top #查看CPU和内存占用情况 vmstat 2 #每2秒检查一次系统状态 sar -n DEV 2 #看所有网卡的流量，定时2秒检查 理解了这些性能指标，我们就知道了服务器的性能优化方向：合理利用系统资源，提高服务器的吞吐量和并发数，降低响应时间。 HTTP 客户端 看完了服务器的性能指标，我们再来看看如何度量客户端的性能。客户端是信息的消费者，一切数据都要通过网络从服务器获取，所以它最基本的性能指标就是“延迟”（latency）。 之前在讲 HTTP/2 的时候就简单介绍过延迟。所谓的“延迟”其实就是“等待”，等待数据到达客户端时所花费的时间。但因为 HTTP 的传输链路很复杂，所以延迟的原因也就多种多样。 首先，我们必须谨记有一个“不可逾越”的障碍——光速，因为地理距离而导致的延迟是无法克服的，访问数千公里外的网站显然会有更大的延迟。其次，第二个因素是带宽，它又包括接入互联网时的电缆、WiFi、4G 和运营商内部网络、运营商之间网络的各种带宽，每一处都有可能成为数据传输的瓶颈，降低传输速度，增加延迟。第三个因素是 DNS 查询，如果域名在本地没有缓存，就必须向 DNS 系统发起查询，引发一连串的网络通信成本，而在获取 IP 地址之前客户端只能等待，无法访问网站。第四个因素是 TCP 握手，你应该对它比较熟悉了吧，必须要经过 SYN、SYN/ACK、ACK 三个包之后才能建立连接，它带来的延迟由光速和带宽共同决定。 建立 TCP 连接之后，就是正常的数据收发了，后面还有解析 HTML、执行 JavaScript、排版渲染等等，这些也会耗费一些时间。不过它们已经不属于 HTTP 了，所以不在今天的讨论范围之内。之前讲 HTTPS 时介绍过一个专门的网站“SSLLabs”，而对于 HTTP 性能优化，也有一个专门的测试网站“WebPageTest”。它的特点是在世界各地建立了很多的测试点，可以任意选择地理位置、机型、操作系统和浏览器发起测试，非常方便，用法也很简单。网站测试的最终结果是一个直观的“瀑布图”（Waterfall Chart），清晰地列出了页面中所有资源加载的先后顺序和时间消耗，比如下图就是对 GitHub 首页的一次测试。 Chrome 等浏览器自带的开发者工具也可以很好地观察客户端延迟指标，面板左边有每个 URI 具体消耗的时间，面板的右边也是类似的瀑布图。点击某个 URI，在 Timing 页里会显示出一个小型的“瀑布图”，是这个资源消耗时间的详细分解，延迟的原因都列的清清楚楚，比如下面的这张图： 图里面的这些指标都是什么含义呢？我给你解释一下： 因为有“队头阻塞”，浏览器对每个域名最多开 6 个并发连接（HTTP/1.1），当页面里链接很多的时候就必须排队等待（Queued、Queueing），这里它就等待了 1.62 秒，然后才被浏览器正式处理； 浏览器要预先分配资源，调度连接，花费了 11.56 毫秒（Stalled）; 连接前必须要解析域名，这里因为有本地缓存，所以只消耗了 0.41 毫秒（DNS Lookup）； 与网站服务器建立连接的成本很高，总共花费了 270.87 毫秒，其中有 134.89 毫秒用于 TLS 握手，那么 TCP 握手的时间就是 135.98 毫秒（Initial connection、SSL）； 实际发送数据非常快，只用了 0.11 毫秒（Request sent）； 之后就是等待服务器的响应，专有名词叫 TTFB（Time To First Byte），也就是“首字节响应时间”，里面包括了服务器的处理时间和网络传输时间，花了 124.2 毫秒； 接收数据也是非常快的，用了 3.58 毫秒（Content Dowload）。 从这张图你可以看到，一次 HTTP“请求 - 响应”的过程中延迟的时间是非常惊人的，总时间 415.04 毫秒里占了差不多 99%。所以，客户端 HTTP 性能优化的关键就是：降低延迟。 HTTP 传输链路 以 HTTP 基本的“请求 - 应答”模型为出发点，刚才我们得到了 HTTP 性能优化的一些指标，现在，我们来把视角放大到“真实的世界”，看看客户端和服务器之间的传输链路，它也是影响 HTTP 性能的关键。还记得第 8 讲里的互联网示意图吗？我把它略微改了一下，划分出了几个区域，这就是所谓的“第一公里”“中间一公里”和“最后一公里”（在英语原文中是 mile，英里）。 “第一公里”是指网站的出口，也就是服务器接入互联网的传输线路，它的带宽直接决定了网站对外的服务能力，也就是吞吐量等指标。显然，优化性能应该在这“第一公里”加大投入，尽量购买大带宽，接入更多的运营商网络。“中间一公里”就是由许多小网络组成的实际的互联网，其实它远不止“一公里”，而是非常非常庞大和复杂的网络，地理距离、网络互通都严重影响了传输速度。好在这里面有一个 HTTP 的“好帮手”——CDN，它可以帮助网站跨越“千山万水”，让这段距离看起来真的就好像只有“一公里”。“最后一公里”是用户访问互联网的入口，对于固网用户就是光纤、网线，对于移动用户就是 WiFi、基站。以前它是客户端性能的主要瓶颈，延迟大带宽小，但随着近几年 4G 和高速宽带的普及，“最后一公里”的情况已经好了很多，不再是制约性能的主要因素了。 除了这“三公里”，我个人认为还有一个“第零公里”， 就是网站内部的 Web 服务系统。它其实也是一个小型的网络（当然也可能会非常大），中间的数据处理、传输会导致延迟，增加服务器的响应时间，也是一个不可忽视的优化点。在上面整个互联网传输链路中，末端的“最后一公里”我们是无法控制的，所以我们只能在“第零公里”“第一公里”和“中间一公里”这几个部分下功夫，增加带宽降低延迟，优化传输速度。 小结 性能优化是一个复杂的概念，在 HTTP 里可以分解为服务器性能优化、客户端性能优化和传输链路优化； 服务器有三个主要的性能指标：吞吐量、并发数和响应时间，此外还需要考虑资源利用率； 客户端的基本性能指标是延迟，影响因素有地理距离、带宽、DNS 查询、TCP 握手等； 从服务器到客户端的传输链路可以分为三个部分，我们能够优化的是前两个部分，也就是“第一公里”和“中间一公里”； 有很多工具可以测量这些指标，服务器端有 ab、top、sar 等，客户端可以使用测试网站，浏览器的开发者工具。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:6:1","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"继续优化 上一讲里我说到了，在整个 HTTP 系统里有三个可优化的环节，分别是服务器、客户端和传输链路（“第一公里”和“中间一公里”）。但因为我们是无法完全控制客户端的，所以实际上的优化工作通常是在服务器端。这里又可以细分为后端和前端，后端是指网站的后台服务，而前端就是 HTML、CSS、图片等展现在客户端的代码和数据。知道了大致的方向，HTTP 性能优化具体应该怎么做呢？总的来说，任何计算机系统的优化都可以分成这么几类：硬件软件、内部外部、花钱不花钱。 投资购买现成的硬件最简单的优化方式，比如换上更强的 CPU、更快的网卡、更大的带宽、更多的服务器，效果也会“立竿见影”，直接提升网站的服务能力，也就实现了 HTTP 优化。另外，花钱购买外部的软件或者服务也是一种行之有效的优化方式，最“物有所值”的应该算是 CDN 了（参见第 37 讲）。CDN 专注于网络内容交付，帮助网站解决“中间一公里”的问题，还有很多其他非常专业的优化功能。把网站交给 CDN 运营，就好像是“让网站坐上了喷气飞机”，能够直达用户，几乎不需要费什么力气就能够达成很好的优化效果。不过这些“花钱”的手段实在是太没有“技术含量”了，属于“懒人”（无贬义）的做法，所以我就不再细说，接下来重点就讲讲在网站内部、“不花钱”的软件优化。 我把这方面的 HTTP 性能优化概括为三个关键词：开源、节流、缓存。 开源 这个“开源”可不是 Open Source，而是指抓“源头”，开发网站服务器自身的潜力，在现有条件不变的情况下尽量挖掘出更多的服务能力。首先，我们应该选用高性能的 Web 服务器，最佳选择当然就是 Nginx/OpenResty 了，尽量不要选择基于 Java、Python、Ruby 的其他服务器，它们用来做后面的业务逻辑服务器更好。利用 Nginx 强大的反向代理能力实现“动静分离”，动态页面交给 Tomcat、Django、Rails，图片、样式表等静态资源交给 Nginx。 Nginx 或者 OpenResty 自身也有很多配置参数可以用来进一步调优，举几个例子，比如说禁用负载均衡锁、增大连接池，绑定 CPU 等等，相关的资料有很多。特别要说的是，对于 HTTP 协议一定要启用长连接。在第 39 讲里你也看到了，TCP 和 SSL 建立新连接的成本是非常高的，有可能会占到客户端总延迟的一半以上。长连接虽然不能优化连接握手，但可以把成本“均摊”到多次请求里，这样只有第一次请求会有延迟，之后的请求就不会有连接延迟，总体的延迟也就降低了。 另外，在现代操作系统上都已经支持 TCP 的新特性“TCP Fast Open”（Win10、iOS9、Linux 4.1），它的效果类似 TLS 的“False Start”，可以在初次握手的时候就传输数据，也就是 0-RTT，所以我们应该尽可能在操作系统和 Nginx 里开启这个特性，减少外网和内网里的握手延迟。下面给出一个简短的 Nginx 配置示例，启用了长连接等优化参数，实现了动静分离。 server { listen 80 deferred reuseport backlog=4096 fastopen=1024; keepalive_timeout 60; keepalive_requests 10000; location ~* \\.(png)$ { root /var/images/png/; } location ~* \\.(php)$ { proxy_pass http://php_back_end; } } 节流 “节流”是指减少客户端和服务器之间收发的数据量，在有限的带宽里传输更多的内容。“节流”最基本的做法就是使用 HTTP 协议内置的“数据压缩”编码，不仅可以选择标准的 gzip，还可以积极尝试新的压缩算法 br，它有更好的压缩效果。不过在数据压缩的时候应当注意选择适当的压缩率，不要追求最高压缩比，否则会耗费服务器的计算资源，增加响应时间，降低服务能力，反而会“得不偿失”。gzip 和 br 是通用的压缩算法，对于 HTTP 协议传输的各种格式数据，我们还可以有针对性地采用特殊的压缩方式。 HTML/CSS/JavaScript 属于纯文本，就可以采用特殊的“压缩”，去掉源码里多余的空格、换行、注释等元素。这样“压缩”之后的文本虽然看起来很混乱，对“人类”不友好，但计算机仍然能够毫无障碍地阅读，不影响浏览器上的运行效果。图片在 HTTP 传输里占有非常高的比例，虽然它本身已经被压缩过了，不能被 gzip、br 处理，但仍然有优化的空间。比如说，去除图片里的拍摄时间、地点、机型等元数据，适当降低分辨率，缩小尺寸。图片的格式也很关键，尽量选择高压缩率的格式，有损格式应该用 JPEG，无损格式应该用 Webp 格式。对于小文本或者小图片，还有一种叫做“资源合并”（Concatenation）的优化方式，就是把许多小资源合并成一个大资源，用一个请求全下载到客户端，然后客户端再用 JavaScript、CSS 切分后使用，好处是节省了请求次数，但缺点是处理比较麻烦。 刚才说的几种数据压缩针对的都是 HTTP 报文里的 body，在 HTTP/1 里没有办法可以压缩 header，但我们也可以采取一些手段来减少 header 的大小，不必要的字段就尽量不发（例如 Server、X-Powered-By）。网站经常会使用 Cookie 来记录用户的数据，浏览器访问网站时每次都会带上 Cookie，冗余度很高。所以应当少使用 Cookie，减少 Cookie 记录的数据量，总使用 domain 和 path 属性限定 Cookie 的作用域，尽可能减少 Cookie 的传输。如果客户端是现代浏览器，还可以使用 HTML5 里定义的 Web Local Storage，避免使用 Cookie。 压缩之外，“节流”还有两个优化点，就是域名和重定向。DNS 解析域名会耗费不少的时间，如果网站拥有多个域名，那么域名解析获取 IP 地址就是一个不小的成本，所以应当适当“收缩”域名，限制在两三个左右，减少解析完整域名所需的时间，让客户端尽快从系统缓存里获取解析结果。重定向引发的客户端延迟也很高，它不仅增加了一次请求往返，还有可能导致新域名的 DNS 解析，是 HTTP 前端性能优化的“大忌”。除非必要，应当尽量不使用重定向，或者使用 Web 服务器的“内部重定向”。 缓存 在第 20 讲里，我就说到了“缓存”，它不仅是 HTTP，也是任何计算机系统性能优化的“法宝”，把它和上面的“开源”“节流”搭配起来应用于传输链路，就能够让 HTTP 的性能再上一个台阶。在“第零公里”，也就是网站系统内部，可以使用 Memcache、Redis、Varnish 等专门的缓存服务，把计算的中间结果和资源存储在内存或者硬盘里，Web 服务器首先检查缓存系统，如果有数据就立即返回给客户端，省去了访问后台服务的时间。 在“中间一公里”，缓存更是性能优化的重要手段，CDN 的网络加速功能就是建立在缓存的基础之上的，可以这么说，如果没有缓存，那就没有 CDN。利用好缓存功能的关键是理解它的工作原理（参见第 20 讲和第 22 讲），为每个资源都添加 ETag 和 Last-modified 字段，再用 Cache-Control、Expires 设置好缓存控制属性。其中最基本的是 max-age 有效期，标记资源可缓存的时间。对于图片、CSS 等静态资源可以设置较长的时间，比如一天或者一个月，对于动态资源，除非是实时性非常高，也可以设置一个较短的时间，比如 1 秒或者 5 秒。这样一旦资源到达客户端，就会被缓存起来，在有效期内都不会再向服务器发送请求，也就是：“没有请求的请求，才是最快的请求。” HTTP/2 在“开源”“节流”和“缓存”这三大策略之外，HTTP 性能优化还有一个选择，那就是把协议由 HTTP/1 升级到 HTTP/2。通过“飞翔篇”的学习，你已经知道了 HTTP/2 的很多优点，它消除了应用层的队头阻塞，拥有头部压缩、二进制帧、多路复用、流量控制、服务器推送等许多新特性，大幅度提升了 HTTP 的传输效率。 实际上这些特性也是在“开源”和“节流”这两点上做文章，但因为这些都已经内置在了协议内，所以只要换上 HTTP/2，网站就能够立刻获得显著的性能提升。不过你要注意，一些在 HTTP/1 里的优化手段到了 HTTP/2 里会有“反效果”。对于 HTTP/2 来说，一个域名使用一个 TCP 连接才能够获得最佳性能，如果开多个域名，就会浪费带宽和服务器资源，也会降低 HTTP/2 的效率，所以“域名收缩”在 HTTP/2 里是必须要做的。 “资源合并”在 HTTP/1 里减少了多次请求的成本，但在 HTTP/2 里因为有头部压缩和多路复用，传输小文件的成本很低，所以合并就失去了意义。而且“资源合并”还有一个缺点，就是降低了缓存的可用性，只要一个小文件更新，整个缓存就完全失效，必须重新下载。所以在现在的大带宽和 CDN 应用场景下，应当尽量少用资源合并（JavaScript、CSS 图片合并，数据内嵌），让资源的粒度尽可能地小，才能更好地发挥缓存的作用。 小结 花钱购买硬件、软件或者服务可以直接提升网站的服务能力，其中最有价值的是 CDN； 不花钱也可以优化 HTTP，三个关键词是“开源”“节流”和“缓存”； 后端应该选用高性能的 Web 服务器，开启长连接，提升 TCP 的传输效率； 前端应该启用 gzip、br 压缩，减小文本、图片的体积，尽量少传不必要","date":"2022-01-24 20:52:12","objectID":"/cn_http/:6:2","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"docker 启动 Docker 容器 有了镜像文件，你就可以用“docker run”命令，从镜像启动一个容器了。 这里面就是我们完整的 HTTP 实验环境，不需要再操心这样、那样的问题了，做到了真正的“开箱即用”。 docker pull chronolaw/http_study docker run -it --rm chronolaw/http_study 对于上面这条命令，我还要稍微解释一下：“-it”参数表示开启一个交互式的 Shell，默认使用的是 bash；“–rm”参数表示容器是“用完即扔”，不保存容器实例，一旦退出 Shell 就会自动删除容器（但不会删除镜像），免去了管理容器的麻烦。“docker run”之后，你就会像虚拟机一样进入容器的运行环境，这里就是 Ubuntu 18.04，身份也自动变成了 root 用户 项目的源码我放在了 root 用户目录下，你可以直接进入“http_study/www”目录，然后执行“run.sh”启动 OpenResty 服务（可参考第 41 讲）。 cd ~/http_study/www ./run.sh start 不过因为 Docker 自身的限制，镜像里的 hosts 文件不能直接添加“www.chrono.com”等实验域名的解析。如果你想要在 URI 里使用域名，就必须在容器启动后手动修改 hosts 文件，用 Vim 或者 cat 都可以。 vim /etc/hosts #手动编辑hosts文件 cat ~/http_study/hosts \u003e\u003e /etc/hosts #cat追加到hosts末尾 另一种方式是在“docker run”的时候用“–add-host”参数，手动指定域名 /IP 的映射关系。 docker run -it --rm --add-host=www.chrono.com:127.0.0.1 chronolaw/http_study 保险起见，我建议你还是用第一种方式比较好。也就是启动容器后，用“cat”命令，把实验域名的解析追加到 hosts 文件里，然后再启动 OpenResty 服务。 docker run -it --rm chronolaw/http_study cat ~/http_study/hosts \u003e\u003e /etc/hosts cd ~/http_study/www ./run.sh start 在 Docker 容器里做实验 把上面的工作都做完之后，我们的实验环境就算是完美地运行起来了，现在你就可以在里面任意验证各节课里的示例了，我来举几个例子。不过在开始之前，我要提醒你一点，因为这个 Docker 镜像是基于 Linux 的，没有图形界面，所以只能用命令行（比如 telnet、curl）来访问 HTTP 服务。当然你也可以查一下资料，让容器对外暴露 80 等端口（比如使用参数“–net=host”），在外部用浏览器来访问，这里我就不细说了。先来看最简单的，第 7 讲里的测试实验环境，用 curl 来访问 localhost，会输出一个文本形式的 HTML 文件内容。 curl http://localhost #访问本机的HTTP服务 然后我们来看第 9 讲，用 telnet 来访问 HTTP 服务，输入“telnet 127.0.0.1 80”，回车，就进入了 telnet 界面。Linux 下的 telnet 操作要比 Windows 的容易一些，你可以直接把 HTTP 请求报文复制粘贴进去，再按两下回车就行了，结束 telnet 可以用“Ctrl+C”。 GET /09-1 HTTP/1.1 Host: www.chrono.com 实验环境里测试 HTTPS 和 HTTP/2 也是毫无问题的，只要你按之前说的，正确修改了 hosts 域名解析，就可以用 curl 来访问，但要加上“-k”参数来忽略证书验证。 curl https://www.chrono.com/23-1 -vk curl https://www.metroid.net:8443/30-1 -vk 这里要注意一点，因为 Docker 镜像里的 Openresty 1.17.8.2 内置了 OpenSSL1.1.1g，默认使用的是 TLS1.3，所以如果你想要测试 TLS1.2 的话，需要使用参数“–tlsv1.2”。 curl https://www.chrono.com/30-1 -k --tlsv1.2 在 Docker 容器里抓包 到这里，课程中的大部分示例都可以运行了。最后我再示范一下在 Docker 容器里 tcpdump 抓包的用法。首先，你要指定抓取的协议、地址和端口号，再用“-w”指定存储位置，启动 tcpdump 后按“Ctrl+Z”让它在后台运行。比如为了测试 TLS1.3，就可以用下面的命令行，抓取 HTTPS 的 443 端口，存放到“/tmp”目录。 tcpdump tcp port 443 -i lo -w /tmp/a.pcap 然后，我们执行任意的 telnet 或者 curl 命令，完成 HTTP 请求之后，输入“fg”恢复 tcpdump，再按“Ctrl+C”，这样抓包就结束了。对于 HTTPS 需要导出密钥的情形，你必须在 curl 请求的同时指定环境变量“SSLKEYLOGFILE”，不然抓包获取的数据无法解密，你就只能看到乱码了。 SSLKEYLOGFILE=/tmp/a.log curl https://www.chrono.com/11-1 -k 抓包生成的文件在容器关闭后就会消失，所以还要用“docker cp”命令及时从容器里拷出来（指定容器的 ID，看提示符，或者用“docker ps -a”查看，也可以从 GitHub 仓库里获取 43-1.pcap/43-1.log）。 docker cp xxx:/tmp/a.pcap . #需要指定容器的ID docker cp xxx:/tmp/a.log . #需要指定容器的ID 现在有了 pcap 文件和 log 文件，我们就可以用 Wireshark 来看网络数据，细致地分析 HTTP/HTTPS 通信过程了（HTTPS 还需要设置一下 Wireshark，见第 26 讲）。 在这个包里，你可以清楚地看到，通信时使用的是 TLS1.3 协议，服务器选择的密码套件是 TLS_AES_256_GCM_SHA384。掌握了 tcpdump 的用法之后，你也可以再参考第 27 讲，改改 Nginx 配置文件，自己抓包仔细研究 TLS1.3 协议的“supported_versions”“key_share”“server_name”等各个扩展协议。 小结 今天讲了 Docker 实验环境的搭建，我再小结一下要点。 Docker 是一种非常流行的虚拟化技术，可以近似地把它理解成是一个“轻量级的虚拟机”； 可以用“docker pull”命令从 Docker Hub 上获取课程相应的 Docker 镜像文件； 可以用“docker run”命令从镜像启动一个容器，里面是完整的 HTTP 实验环境，支持 TLS1.3； 可以在 Docker 容器里任意验证各节课里的示例，但需要使用命令行形式的 telnet 或者 curl； 抓包需要使用 tcpdump，指定抓取的协议、地址和端口号； 对于 HTTPS，需要指定环境变量“SSLKEYLOGFILE”导出密钥，再发送 curl 请求。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:6:3","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"DHE/ECDHE算法的原理 在第 26 讲里，我介绍了 TLS 1.2 的握手过程，在 Client Hello 和 Server Hello 里用到了 ECDHE 算法做密钥交换，参数完全公开，但却能够防止黑客攻击，算出只有通信双方才能知道的秘密 Pre-Master。这是 TLS 握手的关键步骤，也让很多同学不太理解，“为什么数据都是不保密的，但中间人却无法破解呢？”解答这个问题必须要涉及密码学，我原本觉得有点太深了，不想展开细讲，但后来发现大家都对这个很关心，有点“打破砂锅问到底”的精神。所以，这次我就试着从底层来解释一下。不过你要有点心理准备，这不是那么好懂的。先从 ECDHE 算法的名字说起。ECDHE 就是“短暂 - 椭圆曲线 - 迪菲 - 赫尔曼”算法（ephemeral Elliptic Curve Diffie–Hellman），里面的关键字是“短暂”“椭圆曲线”和“迪菲 - 赫尔曼”，我先来讲“迪菲 - 赫尔曼”，也就是 DH 算法。 离散对数 DH 算法是一种非对称加密算法，只能用于密钥交换，它的数学基础是“离散对数”（Discrete logarithm）。那么，什么是离散对数呢？上中学的时候我们都学过初等代数，知道指数和对数，指数就是幂运算，对数是指数的逆运算，是已知底数和真数（幂结果），反推出指数。例如，如果以 10 作为底数，那么指数运算是 y=10^x，对数运算是 y=logx，100 的对数是 2（10^2=100，log100=2），2 的对数是 0.301（log2≈0.301）。对数运算的域是实数，取值是连续的，而“离散对数”顾名思义，取值是不连续的，数值都是整数，但运算具有与实数对数相似的性质。离散对数里的一个核心操作是模运算，也就是取余数（mod，在 C、Java、Lua 等语言里的操作符是“%”）。 假设有模数 17，底数 5，那么“5 的 3 次方再对 17 取余数得 6”（5 ^ 3 % 17 = 6）就是在离散整数域上的一次指数运算（5 ^ 3 (mod 17) = 6）。反过来，以 5 为底，17 为模数，6 的离散对数就是 3（Ind(5, 6) = 3 ( mod 17)）。这里的（17，5）是离散对数的公共参数，6 是真数，3 是对数。知道了对数，就可以用幂运算很容易地得到真数，但反过来，知道真数却很难推断出对数，于是就形成了一个“单向函数”。在这个例子里，选择的模数 17 很小，使用穷举法从 1 到 17 暴力破解也能够计算得到 6 的离散对数是 3。但如果我们选择的是一个非常非常大的数，比如说是有 1024 位的超大素数，那么暴力破解的成本就非常高了，几乎没有什么有效的方法能够快速计算出离散对数，这就是 DH 算法的数学基础。 DH 算法 知道了离散对数，我们来看 DH 算法，假设 Alice 和 Bob 约定使用 DH 算法来交换密钥。基于离散对数，Alice 和 Bob 需要首先确定模数和底数作为算法的参数，这两个参数是公开的，用 P 和 G 来代称，简单起见我们还是用 17 和 5（P=17，G=5）。然后 Alice 和 Bob 各自选择一个随机整数作为私钥（必须在 1 和 P-2 之间），严格保密。比如 Alice 选择 a=10，Bob 选择 b=5。 有了 DH 的私钥，Alice 和 Bob 再计算幂作为公钥，也就是 A = (G ^ a % P) = 9，B = (G ^ b % P) = 14，这里的 A 和 B 完全可以公开，因为根据离散对数的原理，从真数反向计算对数 a 和 b 是非常困难的。交换 DH 公钥之后，Alice 手里有五个数：P=17，G=5，a=10，A=9，B=14，然后执行一个运算：(B ^ a % P)= 8。因为离散对数的幂运算有交换律，B ^ a = (G ^ b ) ^ a = (G ^ a) ^ b = A ^ b，所以 Bob 计算 A ^ b % P 也会得到同样的结果 8，这个就是 Alice 和 Bob 之间的共享秘密，可以作为会话密钥使用，也就是 TLS 里的 Pre-Master。 那么黑客在这个密钥交换的通信过程中能否实现攻击呢？整个通信过程中，Alice 和 Bob 公开了 4 个信息：P、G、A、B，其中 P、G 是算法的参数，A 和 B 是公钥，而 a、b 是各自秘密保管的私钥，无法获取，所以黑客只能从已知的 P、G、A、B 下手，计算 9 或 14 的离散对数。由离散对数的性质就可以知道，如果 P 非常大，那么他很难在短时间里破解出私钥 a、b，所以 Alice 和 Bob 的通信是安全的（但在本例中数字小，计算难度很低）。实验环境的 URI“/42-1”演示了这个简单 DH 密钥交换过程，可以用浏览器直接访问，命令行下也可以用“resty www/lua/42-1.lua”直接运行。 DHE 算法 DH 算法有两种实现形式，一种是已经被废弃的 DH 算法，也叫 static DH 算法，另一种是现在常用的 DHE 算法（有时候也叫 EDH）。static DH 算法里有一方的私钥是静态的，通常是服务器方固定，即 a 不变。而另一方（也就是客户端）随机选择私钥，即 b 采用随机数。于是 DH 交换密钥时就只有客户端的公钥会变，而服务器公钥不变，在长期通信时就增加了被破解的风险，使得拥有海量计算资源的攻击者获得了足够的时间，最终能够暴力破解出服务器私钥，然后计算得到所有的共享秘密 Pre-Master，不具有“前向安全”。而 DHE 算法的关键在于“E”表示的临时性上（ephemeral），每次交换密钥时双方的私钥都是随机选择、临时生成的，用完就扔掉，下次通信不会再使用，相当于“一次一密”。所以，即使攻击者破解了某一次的私钥，其他通信过程的私钥仍然是安全的，不会被解密，实现了“前向安全”。 ECDHE 算法 现在如果你理解了 DHE，那么理解 ECDHE 也就不那么困难了。ECDHE 算法，就是把 DHE 算法里整数域的离散对数，替换成了椭圆曲线上的离散对数。 原来 DHE 算法里的是任意整数，而 ECDHE 则是把连续的椭圆曲线给“离散化”成整数，用椭圆曲线上的“倍运算”替换了 DHE 里的幂运算。在 ECDHE 里，算法的公开参数是椭圆曲线 C、基点 G 和模数 P，私钥是倍数 x，公钥是倍点 xG，已知倍点 xG 要想计算出离散对数 x 是非常困难的。在通信时 Alice 和 Bob 各自随机选择两个数字 a 和 b 作为私钥，计算 A=aG、B=bG 作为公钥，然后互相交换，用与 DHE 相同的算法，计算得到 aB=abG=Ab，就是共享秘密 Pre-Master。因为椭圆曲线离散对数的计算难度比普通的离散对数更大，所以 ECDHE 的安全性比 DHE 还要高，更能够抵御黑客的攻击。 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:6:4","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Advanced learning"],"content":"附录 HTTP错误状态码： HTTP 400 – 请求无效 HTTP 401.1 – 未授权：登录失败 HTTP 401.2 – 未授权：服务器配置问题导致登录失败 HTTP 401.3 – ACL 禁止访问资源 HTTP 401.4 – 未授权：授权被筛选器拒绝 HTTP 401.5 – 未授权：ISAPI 或 CGI 授权失败 HTTP 403 – 禁止访问 HTTP 403 – 对 Internet 服务管理器 的访问仅限于 Localhost HTTP 403.1 禁止访问：禁止可执行访问 HTTP 403.2 – 禁止访问：禁止读访问 HTTP 403.3 – 禁止访问：禁止写访问 HTTP 403.4 – 禁止访问：要求 SSL HTTP 403.5 – 禁止访问：要求 SSL 128 HTTP 403.6 – 禁止访问：IP 地址被拒绝 HTTP 403.7 – 禁止访问：要求客户证书 HTTP 403.8 – 禁止访问：禁止站点访问 HTTP 403.9 – 禁止访问：连接的用户过多 HTTP 403.10 – 禁止访问：配置无效 HTTP 403.11 – 禁止访问：密码更改 HTTP 403.12 – 禁止访问：映射器拒绝访问 HTTP 403.13 – 禁止访问：客户证书已被吊销 HTTP 403.15 – 禁止访问：客户访问许可过多 HTTP 403.16 – 禁止访问：客户证书不可信或者无效 HTTP 403.17 – 禁止访问：客户证书已经到期或者尚未生效 HTTP 404.1 - 无法找到 Web 站点 HTTP 404- 无法找到文件 HTTP 405 – 资源被禁止 HTTP 406 – 无法接受 HTTP 407 – 要求代理身份验证 HTTP 410 – 永远不可用 HTTP 412 – 先决条件失败 HTTP 414 – 请求 – URI 太长 HTTP 500 – 内部服务器错误 HTTP 500.100 – 内部服务器错误 – ASP 错误 HTTP 500-11 服务器关闭 HTTP 500-12 应用程序重新启动 HTTP 500-13 – 服务器太忙 HTTP 500-14 – 应用程序无效 HTTP 500-15 – 不允许请求 global.asa Error 501 – 未实现 HTTP 502 – 网关错误 ","date":"2022-01-24 20:52:12","objectID":"/cn_http/:7:0","tags":["computer network"],"title":"CN_HTTP","uri":"/cn_http/"},{"categories":["Coding"],"content":"结构型设计模式 常用的有：代理模式、桥接模式、装饰者模式、适配器模式。不常用的有：门面模式、组合模式、享元模式。 代理模式：代理模式在不改变原始类接口的条件下，为原始类定义一个代理类，主要目的是控制访问，而非加强功能，这是它跟装饰器模式最大的不同。 桥接模式：桥接模式的目的是将接口部分和实现部分分离，从而让它们可以较为容易、也相对独立地加以改变。 装饰器模式：装饰者模式在不改变原始类接口的情况下，对原始类功能进行增强，并且支持多个装饰器的嵌套使用。 适配器模式：适配器模式是一种事后的补救策略。适配器提供跟原始类不同的接口，而代理模式、装饰器模式提供的都是跟原始类相同的接口。 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:0:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"adapter 适配器模式的英文翻译是 Adapter Design Pattern。顾名思义，这个模式就是用来做适配的，它将不兼容的接口转换为可兼容的接口，让原本由于接口不兼容而不能一起工作的类可以一起工作。对于这个模式，有一个经常被拿来解释它的例子，就是 USB 转接头充当适配器，把两种不兼容的接口，通过转接变得可以一起工作。 适配器模式用于转换一种接口适配另一种接口。 实际使用中Adaptee一般为接口，并且使用工厂函数生成实例。 在Adapter中匿名组合Adaptee接口，所以Adapter类也拥有SpecificRequest实例方法，又因为Go语言中非入侵式接口特征，其实Adapter也适配Adaptee接口。 两种实现方式，类适配器和对象适配器 其中，类适配器使用继承关系来实现，对象适配器使用组合关系来实现。 一般来说，适配器模式可以看作一种“补偿模式”，用来补救设计上的缺陷。应用这种模式算是“无奈之举”。如果在设计初期，我们就能协调规避接口不兼容的问题，那这种模式就没有应用的机会了。 封装有缺陷的接口设计 假设我们依赖的外部系统在接口设计方面有缺陷（比如包含大量静态方法），引入之后会影响到我们自身代码的可测试性。为了隔离设计上的缺陷，我们希望对外部系统提供的接口进行二次封装，抽象出更好的接口设计，这个时候就可以使用适配器模式了。 统一多个类的接口设计 某个功能的实现依赖多个外部系统（或者说类）。通过适配器模式，将它们的接口适配为统一的接口定义，然后我们就可以使用多态的特性来复用代码逻辑。 替换依赖的外部系统 当我们把项目中依赖的一个外部系统替换为另一个外部系统的时候，利用适配器模式，可以减少对代码的改动。 兼容老版本接口 在做版本升级的时候，对于一些要废弃的接口，我们不直接将其删除，而是暂时保留，并且标注为 deprecated，并将内部实现逻辑委托为新的接口实现。这样做的好处是，让使用它的项目有个过渡期，而不是强制进行代码修改。这也可以粗略地看作适配器模式的一个应用场景。 适配不同格式的数据 前面我们讲到，适配器模式主要用于接口的适配，实际上，它还可以用在不同格式的数据之间的适配。比如，把从不同征信系统拉取的不同格式的征信数据，统一为相同的格式，以方便存储和使用。再比如，Java 中的 Arrays.asList() 也可以看作一种数据适配器，将数组类型的数据转化为集合容器类型。 package adapter //Target 是适配的目标接口 type Target interface { Request() string } //Adaptee 是被适配的目标接口 type Adaptee interface { SpecificRequest() string } //NewAdaptee 是被适配接口的工厂函数 func NewAdaptee() Adaptee { return \u0026adapteeImpl{} } //AdapteeImpl 是被适配的目标类 type adapteeImpl struct{} //SpecificRequest 是目标类的一个方法 func (*adapteeImpl) SpecificRequest() string { return \"adaptee method\" } //NewAdapter 是Adapter的工厂函数 func NewAdapter(adaptee Adaptee) Target { return \u0026adapter{ Adaptee: adaptee, } } //Adapter 是转换Adaptee为Target接口的适配器 type adapter struct { Adaptee } //Request 实现Target接口 func (a *adapter) Request() string { return a.SpecificRequest() } package adapter import \"testing\" var expect = \"adaptee method\" func TestAdapter(t *testing.T) { adaptee := NewAdaptee() target := NewAdapter(adaptee) res := target.Request() if res != expect { t.Fatalf(\"expect: %s, actual: %s\", expect, res) } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:1:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"proxy 代理模式用于延迟处理操作或者在进行实际操作前后进行其它处理。 代理模式的常见用法有 虚代理 COW代理 远程代理 保护代理 Cache 代理 防火墙代理 同步代理 智能指引 等。。。 package proxy type Subject interface { Do() string } type RealSubject struct{} func (RealSubject) Do() string { return \"real\" } type Proxy struct { real RealSubject } func (p Proxy) Do() string { var res string // 在调用真实对象之前的工作，检查缓存，判断权限，实例化真实对象等。。 res += \"pre:\" // 调用真实对象 res += p.real.Do() // 调用之后的操作，如缓存结果，对结果进行处理等。。 res += \":after\" return res } package proxy import \"testing\" func TestProxy(t *testing.T) { var sub Subject sub = \u0026Proxy{} res := sub.Do() if res != \"pre:real:after\" { t.Fail() } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:2:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"代理在RPC、缓存、监控等场景中的应用 代理模式的原理解析 代理模式（Proxy Design Pattern）的原理和代码实现都不难掌握。它在不改变原始类（或叫被代理类）代码的情况下，通过引入代理类来给原始类附加功能。 动态代理的原理解析 所谓动态代理（Dynamic Proxy），就是我们不事先为每个原始类编写代理类，而是在运行的时候，动态地创建原始类对应的代理类，然后在系统中用代理类替换掉原始类。 代理模式的应用场景 代理模式的应用场景非常多，我这里列举一些比较常见的用法，希望你能举一反三地应用在你的项目开发中。 业务系统的非功能性需求开发代理模式最常用的一个应用场景就是，在业务系统中开发一些非功能性需求，比如：监控、统计、鉴权、限流、事务、幂等、日志。我们将这些附加功能与业务功能解耦，放到代理类中统一处理，让程序员只需要关注业务方面的开发。实际上，前面举的搜集接口请求信息的例子，就是这个应用场景的一个典型例子。如果你熟悉 Java 语言和 Spring 开发框架，这部分工作都是可以在 Spring AOP 切面中完成的。前面我们也提到，Spring AOP 底层的实现原理就是基于动态代理。 代理模式在 RPC、缓存中的应用实际上，RPC 框架也可以看作一种代理模式，GoF 的《设计模式》一书中把它称作远程代理。通过远程代理，将网络通信、数据编解码等细节隐藏起来。客户端在使用 RPC 服务的时候，就像使用本地函数一样，无需了解跟服务器交互的细节。除此之外，RPC 服务的开发者也只需要开发业务逻辑，就像开发本地使用的函数一样，不需要关注跟客户端的交互细节。关于远程代理的代码示例，我自己实现了一个简单的 RPC 框架 Demo，放到了 GitHub 中，你可以点击这里的链接查看。 我们再来看代理模式在缓存中的应用。假设我们要开发一个接口请求的缓存功能，对于某些接口请求，如果入参相同，在设定的过期时间内，直接返回缓存结果，而不用重新进行逻辑处理。比如，针对获取用户个人信息的需求，我们可以开发两个接口，一个支持缓存，一个支持实时查询。对于需要实时数据的需求，我们让其调用实时查询接口，对于不需要实时数据的需求，我们让其调用支持缓存的接口。那如何来实现接口请求的缓存功能呢？最简单的实现方法就是刚刚我们讲到的，给每个需要支持缓存的查询需求都开发两个不同的接口，一个支持缓存，一个支持实时查询。但是，这样做显然增加了开发成本，而且会让代码看起来非常臃肿（接口个数成倍增加），也不方便缓存接口的集中管理（增加、删除缓存接口）、集中配置（比如配置每个接口缓存过期时间）。针对这些问题，代理模式就能派上用场了，确切地说，应该是动态代理。如果是基于 Spring 框架来开发的话，那就可以在 AOP 切面中完成接口缓存的功能。在应用启动的时候，我们从配置文件中加载需要支持缓存的接口，以及相应的缓存策略（比如过期时间）等。当请求到来的时候，我们在 AOP 切面中拦截请求，如果请求中带有支持缓存的字段（比如 http://…?..\u0026cached=true），我们便从缓存（内存缓存或者 Redis 缓存等）中获取数据直接返回。 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:2:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"decorator 装饰模式使用对象组合的方式动态改变或增加对象行为。 Go语言借助于匿名组合和非入侵式接口可以很方便实现装饰模式。 使用匿名组合，在装饰器中不必显式定义转调原对象方法。 装饰器模式主要解决继承关系过于复杂的问题，通过组合来替代继承。它主要的作用是给原始类添加增强功能。这也是判断是否该用装饰器模式的一个重要的依据。除此之外，装饰器模式还有一个特点，那就是可以对原始类嵌套使用多个装饰器。为了满足这个应用场景，在设计的时候，装饰器类需要跟原始类继承相同的抽象类或者接口。 package decorator type Component interface { Calc() int } type ConcreteComponent struct{} func (*ConcreteComponent) Calc() int { return 0 } type MulDecorator struct { Component num int } func WarpMulDecorator(c Component, num int) Component { return \u0026MulDecorator{ Component: c, num: num, } } func (d *MulDecorator) Calc() int { return d.Component.Calc() * d.num } type AddDecorator struct { Component num int } func WarpAddDecorator(c Component, num int) Component { return \u0026AddDecorator{ Component: c, num: num, } } func (d *AddDecorator) Calc() int { return d.Component.Calc() + d.num } package decorator import \"fmt\" func ExampleDecorator() { var c Component = \u0026ConcreteComponent{} c = WarpAddDecorator(c, 10) c = WarpMulDecorator(c, 8) res := c.Calc() fmt.Printf(\"res %d\\n\", res) // Output: // res 80 } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:3:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"bridge 桥接模式分离抽象部分和实现部分。使得两部分独立扩展。 桥接模式类似于策略模式，区别在于策略模式封装一系列算法使得算法可以互相替换。 策略模式使抽象部分和实现部分分离，可以独立变化。 桥接模式有两种理解方式。第一种理解方式是“将抽象和实现解耦，让它们能独立开发”。这种理解方式比较特别，应用场景也不多。另一种理解方式更加简单，类似“组合优于继承”设计原则，这种理解方式更加通用，应用场景比较多。不管是哪种理解方式，它们的代码结构都是相同的，都是一种类之间的组合关系。 package bridge import \"fmt\" type AbstractMessage interface { SendMessage(text, to string) } type MessageImplementer interface { Send(text, to string) } type MessageSMS struct{} func ViaSMS() MessageImplementer { return \u0026MessageSMS{} } func (*MessageSMS) Send(text, to string) { fmt.Printf(\"send %s to %s via SMS\", text, to) } type MessageEmail struct{} func ViaEmail() MessageImplementer { return \u0026MessageEmail{} } func (*MessageEmail) Send(text, to string) { fmt.Printf(\"send %s to %s via Email\", text, to) } type CommonMessage struct { method MessageImplementer } func NewCommonMessage(method MessageImplementer) *CommonMessage { return \u0026CommonMessage{ method: method, } } func (m *CommonMessage) SendMessage(text, to string) { m.method.Send(text, to) } type UrgencyMessage struct { method MessageImplementer } func NewUrgencyMessage(method MessageImplementer) *UrgencyMessage { return \u0026UrgencyMessage{ method: method, } } func (m *UrgencyMessage) SendMessage(text, to string) { m.method.Send(fmt.Sprintf(\"[Urgency] %s\", text), to) } package bridge func ExampleCommonSMS() { m := NewCommonMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via SMS } func ExampleCommonEmail() { m := NewCommonMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via Email } func ExampleUrgencySMS() { m := NewUrgencyMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via SMS } func ExampleUrgencyEmail() { m := NewUrgencyMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via Email } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:4:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"如何实现支持不同类型和渠道的消息推送系统？ 相对于代理模式来说，桥接模式在实际的项目中并没有那么常用，你只需要简单了解 桥接模式的原理解析 桥接模式，也叫作桥梁模式，英文是 Bridge Design Pattern。这个模式可以说是 23 种设计模式中最难理解的模式之一了。我查阅了比较多的书籍和资料之后发现，对于这个模式有两种不同的理解方式。当然，这其中“最纯正”的理解方式，当属 GoF 的《设计模式》一书中对桥接模式的定义。毕竟，这 23 种经典的设计模式，最初就是由这本书总结出来的。在 GoF 的《设计模式》一书中，桥接模式是这么定义的：“Decouple an abstraction from its implementation so that the two can vary independently。”翻译成中文就是：“将抽象和实现解耦，让它们可以独立变化。”关于桥接模式，很多书籍、资料中，还有另外一种理解方式：“一个类存在两个（或多个）独立变化的维度，我们通过组合的方式，让这两个（或多个）维度可以独立进行扩展。”通过组合关系来替代继承关系，避免继承层次的指数级爆炸。这种理解方式非常类似于，我们之前讲过的“组合优于继承”设计原则，所以，这里我就不多解释了。我们重点看下 GoF 的理解方式。GoF 给出的定义非常的简短，单凭这一句话，估计没几个人能看懂是什么意思。所以，我们通过 JDBC 驱动的例子来解释一下。JDBC 驱动是桥接模式的经典应用。我们先来看一下，如何利用 JDBC 驱动来查询数据库。具体的代码如下所示： Class.forName(\"com.mysql.jdbc.Driver\");//加载及注册JDBC驱动程序 String url = \"jdbc:mysql://localhost:3306/sample_db?user=root\u0026password=your_password\"; Connection con = DriverManager.getConnection(url); Statement stmt = con.createStatement()； String query = \"select * from test\"; ResultSet rs=stmt.executeQuery(query); while(rs.next()) { rs.getString(1); rs.getInt(2); } 如果我们想要把 MySQL 数据库换成 Oracle 数据库，只需要把第一行代码中的 com.mysql.jdbc.Driver 换成 oracle.jdbc.driver.OracleDriver 就可以了。当然，也有更灵活的实现方式，我们可以把需要加载的 Driver 类写到配置文件中，当程序启动的时候，自动从配置文件中加载，这样在切换数据库的时候，我们都不需要修改代码，只需要修改配置文件就可以了。不管是改代码还是改配置，在项目中，从一个数据库切换到另一种数据库，都只需要改动很少的代码，或者完全不需要改动代码，那如此优雅的数据库切换是如何实现的呢？源码之下无秘密。要弄清楚这个问题，我们先从 com.mysql.jdbc.Driver 这个类的代码看起。我摘抄了部分相关代码，放到了这里，你可以看一下。 java package com.mysql.jdbc; import java.sql.SQLException; public class Driver extends NonRegisteringDriver implements java.sql.Driver { static { try { java.sql.DriverManager.registerDriver(new Driver()); } catch (SQLException E) { throw new RuntimeException(\"Can't register driver!\"); } } /** * Construct a new driver and register it with DriverManager * @throws SQLException if a database error occurs. */ public Driver() throws SQLException { // Required for Class.forName().newInstance() } } 结合 com.mysql.jdbc.Driver 的代码实现，我们可以发现，当执行 Class.forName(“com.mysql.jdbc.Driver”) 这条语句的时候，实际上是做了两件事情。第一件事情是要求 JVM 查找并加载指定的 Driver 类，第二件事情是执行该类的静态代码，也就是将 MySQL Driver 注册到 DriverManager 类中。现在，我们再来看一下，DriverManager 类是干什么用的。具体的代码如下所示。当我们把具体的 Driver 实现类（比如，com.mysql.jdbc.Driver）注册到 DriverManager 之后，后续所有对 JDBC 接口的调用，都会委派到对具体的 Driver 实现类来执行。而 Driver 实现类都实现了相同的接口（java.sql.Driver ），这也是可以灵活切换 Driver 的原因。 java public class DriverManager { private final static CopyOnWriteArrayList\u003cDriverInfo\u003e registeredDrivers = new CopyOnWriteArrayList\u003cDriverInfo\u003e(); //... static { loadInitialDrivers(); println(\"JDBC DriverManager initialized\"); } //... public static synchronized void registerDriver(java.sql.Driver driver) throws SQLException { if (driver != null) { registeredDrivers.addIfAbsent(new DriverInfo(driver)); } else { throw new NullPointerException(); } } public static Connection getConnection(String url, String user, String password) throws SQLException { java.util.Properties info = new java.util.Properties(); if (user != null) { info.put(\"user\", user); } if (password != null) { info.put(\"password\", password); } return (getConnection(url, info, Reflection.getCallerClass())); } //... } 桥接模式的定义是“将抽象和实现解耦，让它们可以独立变化”。那弄懂定义中“抽象”和“实现”两个概念，就是理解桥接模式的关键。那在 JDBC 这个例子中，什么是“抽象”？什么是“实现”呢？实际上，JDBC 本身就相当于“抽象”。注意，这里所说的“抽象”，指的并非“抽象类”或“接口”，而是跟具体的数据库无关的、被抽象出来的一套“类库”。具体的 Driver（比如，com.mysql.jdbc.Driver）就相当于“实现”。注意，这里所说的“实现”，也并非指“接口的实现类”，而是跟具体数据库相关的一套“类库”。JDBC 和 Driver 独立开发，通过对象之间的组合关系，组装在一起。JDBC 的所有逻辑操作，最终都委托给 Driver 来执行。我画了一张图帮助你理解，你可以结合着我刚才的讲解一块看。 桥接模式的应用举例 一个 API 接口监控告警的例子：根据不同的告警规则，触发不同类型的告警。告警支持多种通知渠道，包括：邮件、短信、微信、自动语音电话。通知的紧急程度有多种类型，包括：SEVERE（严重）、URGENCY（紧急）、NORMAL（普通）、TRIVIAL（无关紧要）。不同的紧急程度对应不同的通知渠道。比如，SERVE（严重）级别的消息会通过“自动语音电话”告知相关人员。在当时的代码实现中，关于发送告警信息那部分代码，我们只给出了粗略的设计，现在我们来一块实现一下。我们先来看最简单、最直接的一种实现方式。代码如下所示： java public enum NotificationEmergencyLevel { SEVERE, URGENCY, NORMAL, TRIVIAL } public class Notification { private List\u003cString\u003e emailAddresses; private List\u003cString\u003e telephones; private List\u003cString\u003e wechatIds; public Notification() {} public void setEmailAddress(List\u003cString\u003e emailAddress) { this.em","date":"2022-01-22 09:21:00","objectID":"/structural_type/:4:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"facade 门面模式，一个很典型的例子，Socket其实就是应用层与TCP/IP协议族通信的中间软件抽象层，就是一个门面模式，把复杂的TCP/IP协议族隐藏在Socket后面，对用户来说，只需要调用Socket规定的相关函数，让Socket去组织符合制定的协议数据然后进行通信。 API 为facade 模块的外观接口，大部分代码使用此接口简化对facade类的访问。 facade模块同时暴露了a和b 两个Module 的NewXXX和interface，其它代码如果需要使用细节功能时可以直接调用。 package facade import \"fmt\" func NewAPI() API { return \u0026apiImpl{ a: NewAModuleAPI(), b: NewBModuleAPI(), } } //API is facade interface of facade package type API interface { Test() string } //facade implement type apiImpl struct { a AModuleAPI b BModuleAPI } func (a *apiImpl) Test() string { aRet := a.a.TestA() bRet := a.b.TestB() return fmt.Sprintf(\"%s\\n%s\", aRet, bRet) } //NewAModuleAPI return new AModuleAPI func NewAModuleAPI() AModuleAPI { return \u0026aModuleImpl{} } //AModuleAPI ... type AModuleAPI interface { TestA() string } type aModuleImpl struct{} func (*aModuleImpl) TestA() string { return \"A module running\" } //NewBModuleAPI return new BModuleAPI func NewBModuleAPI() BModuleAPI { return \u0026bModuleImpl{} } //BModuleAPI ... type BModuleAPI interface { TestB() string } type bModuleImpl struct{} func (*bModuleImpl) TestB() string { return \"B module running\" } package facade import \"testing\" var expect = \"A module running\\nB module running\" // TestFacadeAPI ... func TestFacadeAPI(t *testing.T) { api := NewAPI() ret := api.Test() if ret != expect { t.Fatalf(\"expect %s, return %s\", expect, ret) } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:5:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"如何设计合理的接口粒度以兼顾接口的易用性和通用性？ 门面模式原理和实现都特别简单，应用场景也比较明确，主要在接口设计方面使用。 如果你平时的工作涉及接口开发，不知道你有没有遇到关于接口粒度的问题呢？ 为了保证接口的可复用性（或者叫通用性），我们需要将接口尽量设计得细粒度一点，职责单一一点。但是，如果接口的粒度过小，在接口的使用者开发一个业务功能时，就会导致需要调用 n 多细粒度的接口才能完成。调用者肯定会抱怨接口不好用。 相反，如果接口粒度设计得太大，一个接口返回 n 多数据，要做 n 多事情，就会导致接口不够通用、可复用性不好。接口不可复用，那针对不同的调用者的业务需求，我们就需要开发不同的接口来满足，这就会导致系统的接口无限膨胀。 门面模式的原理与实现 门面模式，也叫外观模式，英文全称是 Facade Design Pattern。在 GoF 的《设计模式》一书中，门面模式是这样定义的： Provide a unified interface to a set of interfaces in a subsystem. Facade Pattern defines a higher-level interface that makes the subsystem easier to use.翻译成中文就是：门面模式为子系统提供一组统一的接口，定义一组高层接口让子系统更易用。 门面模式的应用场景举例 解决易用性问题门面模式可以用来封装系统的底层实现，隐藏系统的复杂性，提供一组更加简单易用、更高层的接口。比如，**Linux 系统调用函数就可以看作一种“门面”。**它是 Linux 操作系统暴露给开发者的一组“特殊”的编程接口，它封装了底层更基础的 Linux 内核调用。再比如，Linux 的 Shell 命令，实际上也可以看作一种门面模式的应用。它继续封装系统调用，提供更加友好、简单的命令，让我们可以直接通过执行命令来跟操作系统交互。我们前面也多次讲过，设计原则、思想、模式很多都是相通的，是同一个道理不同角度的表述。实际上，从隐藏实现复杂性，提供更易用接口这个意图来看，门面模式有点类似之前讲到的迪米特法则（最少知识原则）和接口隔离原则：两个有交互的系统，只暴露有限的必要的接口。除此之外，门面模式还有点类似之前提到封装、抽象的设计思想，提供更抽象的接口，封装底层实现细节。 解决性能问题关于利用门面模式解决性能问题这一点，刚刚我们已经讲过了。我们通过将多个接口调用替换为一个门面接口调用，减少网络通信成本，提高 App 客户端的响应速度。所以，关于这点，我就不再举例说明了。我们来讨论一下这样一个问题：从代码实现的角度来看，该如何组织门面接口和非门面接口？如果门面接口不多，我们完全可以将它跟非门面接口放到一块，也不需要特殊标记，当作普通接口来用即可。如果门面接口很多，我们可以在已有的接口之上，再重新抽象出一层，专门放置门面接口，从类、包的命名上跟原来的接口层做区分。如果门面接口特别多，并且很多都是跨多个子系统的，我们可以将门面接口放到一个新的子系统中。 解决分布式事务问题关于利用门面模式来解决分布式事务问题，我们通过一个例子来解释一下。在一个金融系统中，有两个业务领域模型，用户和钱包。这两个业务领域模型都对外暴露了一系列接口，比如用户的增删改查接口、钱包的增删改查接口。假设有这样一个业务场景：在用户注册的时候，我们不仅会创建用户（在数据库 User 表中），还会给用户创建一个钱包（在数据库的 Wallet 表中）。对于这样一个简单的业务需求，我们可以通过依次调用用户的创建接口和钱包的创建接口来完成。但是，用户注册需要支持事务，也就是说，创建用户和钱包的两个操作，要么都成功，要么都失败，不能一个成功、一个失败。要支持两个接口调用在一个事务中执行，是比较难实现的，这涉及分布式事务问题。虽然我们可以通过引入分布式事务框架或者事后补偿的机制来解决，但代码实现都比较复杂。而最简单的解决方案是，利用数据库事务或者 Spring 框架提供的事务（如果是 Java 语言的话），在一个事务中，执行创建用户和创建钱包这两个 SQL 操作。这就要求两个 SQL 操作要在一个接口中完成，所以，我们可以借鉴门面模式的思想，再设计一个包裹这两个操作的新接口，让新接口在一个事务中执行两个 SQL 操作。 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:5:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"composite k8s的Job对象有应用。 组合模式统一对象和对象集，使得使用相同接口使用对象和对象集。 组合模式常用于树状结构，用于统一叶子节点和树节点的访问，并且可以用于应用某一操作到所有子节点。 package composite import \"fmt\" type Component interface { Parent() Component SetParent(Component) Name() string SetName(string) AddChild(Component) Print(string) } const ( LeafNode = iota CompositeNode ) func NewComponent(kind int, name string) Component { var c Component switch kind { case LeafNode: c = NewLeaf() case CompositeNode: c = NewComposite() } c.SetName(name) return c } type component struct { parent Component name string } func (c *component) Parent() Component { return c.parent } func (c *component) SetParent(parent Component) { c.parent = parent } func (c *component) Name() string { return c.name } func (c *component) SetName(name string) { c.name = name } func (c *component) AddChild(Component) {} func (c *component) Print(string) {} type Leaf struct { component } func NewLeaf() *Leaf { return \u0026Leaf{} } func (c *Leaf) Print(pre string) { fmt.Printf(\"%s-%s\\n\", pre, c.Name()) } type Composite struct { component childs []Component } func NewComposite() *Composite { return \u0026Composite{ childs: make([]Component, 0), } } func (c *Composite) AddChild(child Component) { child.SetParent(c) c.childs = append(c.childs, child) } func (c *Composite) Print(pre string) { fmt.Printf(\"%s+%s\\n\", pre, c.Name()) pre += \" \" for _, comp := range c.childs { comp.Print(pre) } } package composite func ExampleComposite() { root := NewComponent(CompositeNode, \"root\") c1 := NewComponent(CompositeNode, \"c1\") c2 := NewComponent(CompositeNode, \"c2\") c3 := NewComponent(CompositeNode, \"c3\") l1 := NewComponent(LeafNode, \"l1\") l2 := NewComponent(LeafNode, \"l2\") l3 := NewComponent(LeafNode, \"l3\") root.AddChild(c1) root.AddChild(c2) c1.AddChild(c3) c1.AddChild(l1) c2.AddChild(l2) c2.AddChild(l3) root.Print(\"\") // Output: // +root // +c1 // +c3 // -l1 // +c2 // -l2 // -l3 } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:6:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"如何设计实现支持递归遍历的文件系统目录树结构？ 组合模式跟我们之前讲的面向对象设计中的“组合关系（通过组合来组装两个类）”，完全是两码事。这里讲的“组合模式”，主要是用来处理树形结构数据。这里的“数据”，你可以简单理解为一组对象集合，待会我们会详细讲解。正因为其应用场景的特殊性，数据必须能表示成树形结构，这也导致了这种模式在实际的项目开发中并不那么常用。但是，一旦数据满足树形结构，应用这种模式就能发挥很大的作用，能让代码变得非常简洁。 组合模式的原理与实现 在 GoF 的《设计模式》一书中，组合模式是这样定义的：Compose objects into tree structure to represent part-whole hierarchies.Composite lets client treat individual objects and compositions of objects uniformly.翻译成中文就是：将一组对象组织（Compose）成树形结构，以表示一种“部分 - 整体”的层次结构。组合让客户端（在很多设计模式书籍中，“客户端”代指代码的使用者。）可以统一单个对象和组合对象的处理逻辑。 “将一组对象（文件和目录）组织成树形结构，以表示一种‘部分 - 整体’的层次结构（目录与子目录的嵌套结构）。组合模式让客户端可以统一单个对象（文件）和组合对象（目录）的处理逻辑（递归遍历）。” 实际上，刚才讲的这种组合模式的设计思路，与其说是一种设计模式，倒不如说是对业务场景的一种数据结构和算法的抽象。其中，数据可以表示成树这种数据结构，业务需求可以通过在树上的递归遍历算法来实现。 组合模式的应用场景举例 在实际的项目中，遇到类似的可以表示成树形结构的业务场景，你只要“照葫芦画瓢”去设计就可以了。 假设我们在开发一个 OA 系统（办公自动化系统）。公司的组织结构包含部门和员工两种数据类型。其中，部门又可以包含子部门和员工。 我们希望在内存中构建整个公司的人员架构图（部门、子部门、员工的隶属关系），并且提供接口计算出部门的薪资成本（隶属于这个部门的所有员工的薪资和）。部门包含子部门和员工，这是一种嵌套结构，可以表示成树这种数据结构。计算每个部门的薪资开支这样一个需求，也可以通过在树上的遍历算法来实现。所以，从这个角度来看，这个应用场景可以使用组合模式来设计和实现。 java public abstract class HumanResource { protected long id; protected double salary; public HumanResource(long id) { this.id = id; } public long getId() { return id; } public abstract double calculateSalary(); } public class Employee extends HumanResource { public Employee(long id, double salary) { super(id); this.salary = salary; } @Override public double calculateSalary() { return salary; } } public class Department extends HumanResource { private List\u003cHumanResource\u003e subNodes = new ArrayList\u003c\u003e(); public Department(long id) { super(id); } @Override public double calculateSalary() { double totalSalary = 0; for (HumanResource hr : subNodes) { totalSalary += hr.calculateSalary(); } this.salary = totalSalary; return totalSalary; } public void addSubNode(HumanResource hr) { subNodes.add(hr); } } // 构建组织架构的代码 public class Demo { private static final long ORGANIZATION_ROOT_ID = 1001; private DepartmentRepo departmentRepo; // 依赖注入 private EmployeeRepo employeeRepo; // 依赖注入 public void buildOrganization() { Department rootDepartment = new Department(ORGANIZATION_ROOT_ID); buildOrganization(rootDepartment); } private void buildOrganization(Department department) { List\u003cLong\u003e subDepartmentIds = departmentRepo.getSubDepartmentIds(department.getId()); for (Long subDepartmentId : subDepartmentIds) { Department subDepartment = new Department(subDepartmentId); department.addSubNode(subDepartment); buildOrganization(subDepartment); } List\u003cLong\u003e employeeIds = employeeRepo.getDepartmentEmployeeIds(department.getId()); for (Long employeeId : employeeIds) { double salary = employeeRepo.getEmployeeSalary(employeeId); department.addSubNode(new Employee(employeeId, salary)); } } } 组合模式的设计思路，与其说是一种设计模式，倒不如说是对业务场景的一种数据结构和算法的抽象。其中，数据可以表示成树这种数据结构，业务需求可以通过在树上的递归遍历算法来实现。组合模式，将一组对象组织成树形结构，将单个对象和组合对象都看做树中的节点，以统一处理逻辑，并且它利用树形结构的特点，递归地处理每个子树，依次简化代码实现。使用组合模式的前提在于，你的业务场景必须能够表示成树形结构。所以，组合模式的应用场景也比较局限，它并不是一种很常用的设计模式。 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:6:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"flyweight 享元模式从对象中剥离出不发生改变且多个实例需要的重复数据，独立出一个享元，使多个对象共享，从而节省内存以及减少对象数量。 package flyweight import \"fmt\" type ImageFlyweightFactory struct { maps map[string]*ImageFlyweight } var imageFactory *ImageFlyweightFactory func GetImageFlyweightFactory() *ImageFlyweightFactory { if imageFactory == nil { imageFactory = \u0026ImageFlyweightFactory{ maps: make(map[string]*ImageFlyweight), } } return imageFactory } func (f *ImageFlyweightFactory) Get(filename string) *ImageFlyweight { image := f.maps[filename] if image == nil { image = NewImageFlyweight(filename) f.maps[filename] = image } return image } type ImageFlyweight struct { data string } func NewImageFlyweight(filename string) *ImageFlyweight { // Load image file data := fmt.Sprintf(\"image data %s\", filename) return \u0026ImageFlyweight{ data: data, } } func (i *ImageFlyweight) Data() string { return i.data } type ImageViewer struct { *ImageFlyweight } func NewImageViewer(filename string) *ImageViewer { image := GetImageFlyweightFactory().Get(filename) return \u0026ImageViewer{ ImageFlyweight: image, } } func (i *ImageViewer) Display() { fmt.Printf(\"Display: %s\\n\", i.Data()) } package flyweight import \"testing\" func ExampleFlyweight() { viewer := NewImageViewer(\"image1.png\") viewer.Display() // Output: // Display: image data image1.png } func TestFlyweight(t *testing.T) { viewer1 := NewImageViewer(\"image1.png\") viewer2 := NewImageViewer(\"image1.png\") if viewer1.ImageFlyweight != viewer2.ImageFlyweight { t.Fail() } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:7:0","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"如何利用享元模式优化文本编辑器的内存占用？ 跟其他所有的设计模式类似，享元模式的原理和实现也非常简单。今天，我会通过棋牌游戏和文本编辑器两个实际的例子来讲解。除此之外，我还会讲到它跟单例、缓存、对象池的区别和联系 享元模式原理与实现 所谓“享元”，顾名思义就是被共享的单元。享元模式的意图是复用对象，节省内存，前提是享元对象是不可变对象。 具体来讲，当一个系统中存在大量重复对象的时候，如果这些重复的对象是不可变对象，我们就可以利用享元模式将对象设计成享元，在内存中只保留一份实例，供多处代码引用。这样可以减少内存中对象的数量，起到节省内存的目的。实际上，不仅仅相同对象可以设计成享元，对于相似对象，我们也可以将这些对象中相同的部分（字段）提取出来，设计成享元，让这些大量相似对象引用这些享元。 这里我稍微解释一下，定义中的“不可变对象”指的是，一旦通过构造函数初始化完成之后，它的状态（对象的成员变量或者属性）就不会再被修改了。所以，不可变对象不能暴露任何 set() 等修改内部状态的方法。之所以要求享元是不可变对象，那是因为它会被多处代码共享使用，避免一处代码对享元进行了修改，影响到其他使用它的代码。 享元模式 vs 单例、缓存、对象池 在上面的讲解中，我们多次提到“共享”“缓存”“复用”这些字眼，那它跟单例、缓存、对象池这些概念有什么区别呢？我们来简单对比一下。 我们先来看享元模式跟单例的区别。在单例模式中，一个类只能创建一个对象，而在享元模式中，一个类可以创建多个对象，每个对象被多处代码引用共享。实际上，享元模式有点类似于之前讲到的单例的变体：多例。我们前面也多次提到，区别两种设计模式，不能光看代码实现，而是要看设计意图，也就是要解决的问题。尽管从代码实现上来看，享元模式和多例有很多相似之处，但从设计意图上来看，它们是完全不同的。应用享元模式是为了对象复用，节省内存，而应用多例模式是为了限制对象的个数。 我们再来看享元模式跟缓存的区别。在享元模式的实现中，我们通过工厂类来“缓存”已经创建好的对象。这里的“缓存”实际上是“存储”的意思，跟我们平时所说的“数据库缓存”“CPU 缓存”“MemCache 缓存”是两回事。我们平时所讲的缓存，主要是为了提高访问效率，而非复用。 最后我们来看享元模式跟对象池的区别。对象池、连接池（比如数据库连接池）、线程池等也是为了复用，那它们跟享元模式有什么区别呢？你可能对连接池、线程池比较熟悉，对对象池比较陌生，所以，这里我简单解释一下对象池。像 C++ 这样的编程语言，内存的管理是由程序员负责的。为了避免频繁地进行对象创建和释放导致内存碎片，我们可以预先申请一片连续的内存空间，也就是这里说的对象池。每次创建对象时，我们从对象池中直接取出一个空闲对象来使用，对象使用完成之后，再放回到对象池中以供后续复用，而非直接释放掉。虽然对象池、连接池、线程池、享元模式都是为了复用，但是，如果我们再细致地抠一抠“复用”这个字眼的话，对象池、连接池、线程池等池化技术中的“复用”和享元模式中的“复用”实际上是不同的概念。池化技术中的“复用”可以理解为“重复使用”，主要目的是节省时间（比如从数据库池中取一个连接，不需要重新创建）。在任意时刻，每一个对象、连接、线程，并不会被多处使用，而是被一个使用者独占，当使用完成之后，放回到池中，再由其他使用者重复利用。享元模式中的“复用”可以理解为“共享使用”，在整个生命周期中，都是被所有使用者共享的，主要目的是节省空间。 总结 享元模式的原理所谓“享元”，顾名思义就是被共享的单元。享元模式的意图是复用对象，节省内存，前提是享元对象是不可变对象。具体来讲，当一个系统中存在大量重复对象的时候，我们就可以利用享元模式，将对象设计成享元，在内存中只保留一份实例，供多处代码引用，这样可以减少内存中对象的数量，以起到节省内存的目的。实际上，不仅仅相同对象可以设计成享元，对于相似对象，我们也可以将这些对象中相同的部分（字段），提取出来设计成享元，让这些大量相似对象引用这些享元。 享元模式的实现享元模式的代码实现非常简单，主要是通过工厂模式，在工厂类中，通过一个 Map 或者 List 来缓存已经创建好的享元对象，以达到复用的目的。 享元模式 VS 单例、缓存、对象池我们前面也多次提到，区别两种设计模式，不能光看代码实现，而是要看设计意图，也就是要解决的问题。这里的区别也不例外。我们可以用简单几句话来概括一下它们之间的区别。应用单例模式是为了保证对象全局唯一。应用享元模式是为了实现对象复用，节省内存。缓存是为了提高访问效率，而非复用。池化技术中的“复用”理解为“重复使用”，主要是为了节省时间。 参考 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:7:1","tags":["design pattern"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"行为型设计模式 常用的有：观察者模式、模板模式、策略模式、职责链模式、迭代器模式、状态模式。 不常用的有：命令模式、备忘录模式、解释器模式、访问者模式、中介模式。 ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:0:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"observer 观察者模式用于触发联动。 一个对象的改变会触发其它观察者的相关动作，而此对象无需关心连动对象的具体实现。 package observer import \"fmt\" type Subject struct { observers []Observer context string } func NewSubject() *Subject { return \u0026Subject{ observers: make([]Observer, 0), } } func (s *Subject) Attach(o Observer) { s.observers = append(s.observers, o) } func (s *Subject) notify() { for _, o := range s.observers { o.Update(s) } } func (s *Subject) UpdateContext(context string) { s.context = context s.notify() } type Observer interface { Update(*Subject) } type Reader struct { name string } func NewReader(name string) *Reader { return \u0026Reader{ name: name, } } func (r *Reader) Update(s *Subject) { fmt.Printf(\"%s receive %s\\n\", r.name, s.context) } package observer func ExampleObserver() { subject := NewSubject() reader1 := NewReader(\"reader1\") reader2 := NewReader(\"reader2\") reader3 := NewReader(\"reader3\") subject.Attach(reader1) subject.Attach(reader2) subject.Attach(reader3) subject.UpdateContext(\"observer mode\") // Output: // reader1 receive observer mode // reader2 receive observer mode // reader3 receive observer mode } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:1:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"template method 模版方法模式使用继承机制，把通用步骤和通用方法放到父类中，把具体实现延迟到子类中实现。使得实现符合开闭原则。 如实例代码中通用步骤在父类中实现（准备、下载、保存、收尾）下载和保存的具体实现留到子类中，并且提供 保存方法的默认实现。 因为Golang不提供继承机制，需要使用匿名组合模拟实现继承。 此处需要注意：因为父类需要调用子类方法，所以子类需要匿名组合父类的同时，父类需要持有子类的引用。 package templatemethod import \"fmt\" type Downloader interface { Download(uri string) } type template struct { implement uri string } type implement interface { download() save() } func newTemplate(impl implement) *template { return \u0026template{ implement: impl, } } func (t *template) Download(uri string) { t.uri = uri fmt.Print(\"prepare downloading\\n\") t.implement.download() t.implement.save() fmt.Print(\"finish downloading\\n\") } func (t *template) save() { fmt.Print(\"default save\\n\") } type HTTPDownloader struct { *template } func NewHTTPDownloader() Downloader { downloader := \u0026HTTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *HTTPDownloader) download() { fmt.Printf(\"download %s via http\\n\", d.uri) } func (*HTTPDownloader) save() { fmt.Printf(\"http save\\n\") } type FTPDownloader struct { *template } func NewFTPDownloader() Downloader { downloader := \u0026FTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *FTPDownloader) download() { fmt.Printf(\"download %s via ftp\\n\", d.uri) } package templatemethod func ExampleHTTPDownloader() { var downloader Downloader = NewHTTPDownloader() downloader.Download(\"http://example.com/abc.zip\") // Output: // prepare downloading // download http://example.com/abc.zip via http // http save // finish downloading } func ExampleFTPDownloader() { var downloader Downloader = NewFTPDownloader() downloader.Download(\"ftp://example.com/abc.zip\") // Output: // prepare downloading // download ftp://example.com/abc.zip via ftp // default save // finish downloading } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:2:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"strategy 定义一系列算法，让这些算法在运行时可以互换，使得分离算法，符合开闭原则。 package strategy import \"fmt\" type Payment struct { context *PaymentContext strategy PaymentStrategy } type PaymentContext struct { Name, CardID string Money int } func NewPayment(name, cardid string, money int, strategy PaymentStrategy) *Payment { return \u0026Payment{ context: \u0026PaymentContext{ Name: name, CardID: cardid, Money: money, }, strategy: strategy, } } func (p *Payment) Pay() { p.strategy.Pay(p.context) } type PaymentStrategy interface { Pay(*PaymentContext) } type Cash struct{} func (*Cash) Pay(ctx *PaymentContext) { fmt.Printf(\"Pay $%d to %s by cash\", ctx.Money, ctx.Name) } type Bank struct{} func (*Bank) Pay(ctx *PaymentContext) { fmt.Printf(\"Pay $%d to %s by bank account %s\", ctx.Money, ctx.Name, ctx.CardID) } package strategy func ExamplePayByCash() { payment := NewPayment(\"Ada\", \"\", 123, \u0026Cash{}) payment.Pay() // Output: // Pay $123 to Ada by cash } func ExamplePayByBank() { payment := NewPayment(\"Bob\", \"0002\", 888, \u0026Bank{}) payment.Pay() // Output: // Pay $888 to Bob by bank account 0002 } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:3:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"chain of responsibility 职责链模式用于分离不同职责，并且动态组合相关职责。 Golang实现职责链模式时候，因为没有继承的支持，使用链对象包涵职责的方式，即： 链对象包含当前职责对象以及下一个职责链。 职责对象提供接口表示是否能处理对应请求。 职责对象提供处理函数处理相关职责。 同时可在职责链类中实现职责接口相关函数，使职责链对象可以当做一般职责对象是用。 package chain import \"fmt\" type Manager interface { HaveRight(money int) bool HandleFeeRequest(name string, money int) bool } type RequestChain struct { Manager successor *RequestChain } func (r *RequestChain) SetSuccessor(m *RequestChain) { r.successor = m } func (r *RequestChain) HandleFeeRequest(name string, money int) bool { if r.Manager.HaveRight(money) { return r.Manager.HandleFeeRequest(name, money) } if r.successor != nil { return r.successor.HandleFeeRequest(name, money) } return false } func (r *RequestChain) HaveRight(money int) bool { return true } type ProjectManager struct{} func NewProjectManagerChain() *RequestChain { return \u0026RequestChain{ Manager: \u0026ProjectManager{}, } } func (*ProjectManager) HaveRight(money int) bool { return money \u003c 500 } func (*ProjectManager) HandleFeeRequest(name string, money int) bool { if name == \"bob\" { fmt.Printf(\"Project manager permit %s %d fee request\\n\", name, money) return true } fmt.Printf(\"Project manager don't permit %s %d fee request\\n\", name, money) return false } type DepManager struct{} func NewDepManagerChain() *RequestChain { return \u0026RequestChain{ Manager: \u0026DepManager{}, } } func (*DepManager) HaveRight(money int) bool { return money \u003c 5000 } func (*DepManager) HandleFeeRequest(name string, money int) bool { if name == \"tom\" { fmt.Printf(\"Dep manager permit %s %d fee request\\n\", name, money) return true } fmt.Printf(\"Dep manager don't permit %s %d fee request\\n\", name, money) return false } type GeneralManager struct{} func NewGeneralManagerChain() *RequestChain { return \u0026RequestChain{ Manager: \u0026GeneralManager{}, } } func (*GeneralManager) HaveRight(money int) bool { return true } func (*GeneralManager) HandleFeeRequest(name string, money int) bool { if name == \"ada\" { fmt.Printf(\"General manager permit %s %d fee request\\n\", name, money) return true } fmt.Printf(\"General manager don't permit %s %d fee request\\n\", name, money) return false } package chain func ExampleChain() { c1 := NewProjectManagerChain() c2 := NewDepManagerChain() c3 := NewGeneralManagerChain() c1.SetSuccessor(c2) c2.SetSuccessor(c3) var c Manager = c1 c.HandleFeeRequest(\"bob\", 400) c.HandleFeeRequest(\"tom\", 1400) c.HandleFeeRequest(\"ada\", 10000) c.HandleFeeRequest(\"floar\", 400) // Output: // Project manager permit bob 400 fee request // Dep manager permit tom 1400 fee request // General manager permit ada 10000 fee request // Project manager don't permit floar 400 fee request } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:4:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"iterator 迭代器模式用于使用相同方式送代不同类型集合或者隐藏集合类型的具体实现。 可以使用迭代器模式使遍历同时应用送代策略，如请求新对象、过滤、处理对象等。 package iterator import \"fmt\" type Aggregate interface { Iterator() Iterator } type Iterator interface { First() IsDone() bool Next() interface{} } type Numbers struct { start, end int } func NewNumbers(start, end int) *Numbers { return \u0026Numbers{ start: start, end: end, } } func (n *Numbers) Iterator() Iterator { return \u0026NumbersIterator{ numbers: n, next: n.start, } } type NumbersIterator struct { numbers *Numbers next int } func (i *NumbersIterator) First() { i.next = i.numbers.start } func (i *NumbersIterator) IsDone() bool { return i.next \u003e i.numbers.end } func (i *NumbersIterator) Next() interface{} { if !i.IsDone() { next := i.next i.next++ return next } return nil } func IteratorPrint(i Iterator) { for i.First(); !i.IsDone(); { c := i.Next() fmt.Printf(\"%#v\\n\", c) } } package iterator func ExampleIterator() { var aggregate Aggregate aggregate = NewNumbers(1, 10) IteratorPrint(aggregate.Iterator()) // Output: // 1 // 2 // 3 // 4 // 5 // 6 // 7 // 8 // 9 // 10 } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:5:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"state 状态模式用于分离状态和行为。 package state import \"fmt\" type Week interface { Today() Next(*DayContext) } type DayContext struct { today Week } func NewDayContext() *DayContext { return \u0026DayContext{ today: \u0026Sunday{}, } } func (d *DayContext) Today() { d.today.Today() } func (d *DayContext) Next() { d.today.Next(d) } type Sunday struct{} func (*Sunday) Today() { fmt.Printf(\"Sunday\\n\") } func (*Sunday) Next(ctx *DayContext) { ctx.today = \u0026Monday{} } type Monday struct{} func (*Monday) Today() { fmt.Printf(\"Monday\\n\") } func (*Monday) Next(ctx *DayContext) { ctx.today = \u0026Tuesday{} } type Tuesday struct{} func (*Tuesday) Today() { fmt.Printf(\"Tuesday\\n\") } func (*Tuesday) Next(ctx *DayContext) { ctx.today = \u0026Wednesday{} } type Wednesday struct{} func (*Wednesday) Today() { fmt.Printf(\"Wednesday\\n\") } func (*Wednesday) Next(ctx *DayContext) { ctx.today = \u0026Thursday{} } type Thursday struct{} func (*Thursday) Today() { fmt.Printf(\"Thursday\\n\") } func (*Thursday) Next(ctx *DayContext) { ctx.today = \u0026Friday{} } type Friday struct{} func (*Friday) Today() { fmt.Printf(\"Friday\\n\") } func (*Friday) Next(ctx *DayContext) { ctx.today = \u0026Saturday{} } type Saturday struct{} func (*Saturday) Today() { fmt.Printf(\"Saturday\\n\") } func (*Saturday) Next(ctx *DayContext) { ctx.today = \u0026Sunday{} } package state func ExampleWeek() { ctx := NewDayContext() todayAndNext := func() { ctx.Today() ctx.Next() } for i := 0; i \u003c 8; i++ { todayAndNext() } // Output: // Sunday // Monday // Tuesday // Wednesday // Thursday // Friday // Saturday // Sunday } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:6:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"不常用 ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:7:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"command 命令模式本质是把某个对象的方法调用封装到对象中，方便传递、存储、调用。 示例中把主板单中的启动(start)方法和重启(reboot)方法封装为命令对象，再传递到主机(box)对象中。于两个按钮进行绑定： 第一个机箱(box1)设置按钮1(button1) 为开机按钮2(button2)为重启。 第二个机箱(box1)设置按钮2(button2) 为开机按钮1(button1)为重启。 从而得到配置灵活性。 除了配置灵活外，使用命令模式还可以用作： 批处理 任务队列 undo, redo 等把具体命令封装到对象中使用的场合 package command import \"fmt\" type Command interface { Execute() } type StartCommand struct { mb *MotherBoard } func NewStartCommand(mb *MotherBoard) *StartCommand { return \u0026StartCommand{ mb: mb, } } func (c *StartCommand) Execute() { c.mb.Start() } type RebootCommand struct { mb *MotherBoard } func NewRebootCommand(mb *MotherBoard) *RebootCommand { return \u0026RebootCommand{ mb: mb, } } func (c *RebootCommand) Execute() { c.mb.Reboot() } type MotherBoard struct{} func (*MotherBoard) Start() { fmt.Print(\"system starting\\n\") } func (*MotherBoard) Reboot() { fmt.Print(\"system rebooting\\n\") } type Box struct { button1 Command button2 Command } func NewBox(button1, button2 Command) *Box { return \u0026Box{ button1: button1, button2: button2, } } func (b *Box) PressButton1() { b.button1.Execute() } func (b *Box) PressButton2() { b.button2.Execute() } package command func ExampleCommand() { mb := \u0026MotherBoard{} startCommand := NewStartCommand(mb) rebootCommand := NewRebootCommand(mb) box1 := NewBox(startCommand, rebootCommand) box1.PressButton1() box1.PressButton2() box2 := NewBox(rebootCommand, startCommand) box2.PressButton1() box2.PressButton2() // Output: // system starting // system rebooting // system rebooting // system starting } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:8:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"memento 备忘录模式用于保存程序内部状态到外部，又不希望暴露内部状态的情形。 程序内部状态使用窄接口传递给外部进行存储，从而不暴露程序实现细节。 备忘录模式同时可以离线保存内部状态，如保存到数据库，文件等。 package memento import \"fmt\" type Memento interface{} type Game struct { hp, mp int } type gameMemento struct { hp, mp int } func (g *Game) Play(mpDelta, hpDelta int) { g.mp += mpDelta g.hp += hpDelta } func (g *Game) Save() Memento { return \u0026gameMemento{ hp: g.hp, mp: g.mp, } } func (g *Game) Load(m Memento) { gm := m.(*gameMemento) g.mp = gm.mp g.hp = gm.hp } func (g *Game) Status() { fmt.Printf(\"Current HP:%d, MP:%d\\n\", g.hp, g.mp) } package memento func ExampleGame() { game := \u0026Game{ hp: 10, mp: 10, } game.Status() progress := game.Save() game.Play(-2, -3) game.Status() game.Load(progress) game.Status() // Output: // Current HP:10, MP:10 // Current HP:7, MP:8 // Current HP:10, MP:10 } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:9:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"interpreter 解释器模式定义一套语言文法，并设计该语言解释器，使用户能使用特定文法控制解释器行为。 解释器模式的意义在于，它分离多种复杂功能的实现，每个功能只需关注自身的解释。 对于调用者不用关心内部的解释器的工作，只需要用简单的方式组合命令就可以。 package interpreter import ( \"strconv\" \"strings\" ) type Node interface { Interpret() int } type ValNode struct { val int } func (n *ValNode) Interpret() int { return n.val } type AddNode struct { left, right Node } func (n *AddNode) Interpret() int { return n.left.Interpret() + n.right.Interpret() } type MinNode struct { left, right Node } func (n *MinNode) Interpret() int { return n.left.Interpret() - n.right.Interpret() } type Parser struct { exp []string index int prev Node } func (p *Parser) Parse(exp string) { p.exp = strings.Split(exp, \" \") for { if p.index \u003e= len(p.exp) { return } switch p.exp[p.index] { case \"+\": p.prev = p.newAddNode() case \"-\": p.prev = p.newMinNode() default: p.prev = p.newValNode() } } } func (p *Parser) newAddNode() Node { p.index++ return \u0026AddNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newMinNode() Node { p.index++ return \u0026MinNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newValNode() Node { v, _ := strconv.Atoi(p.exp[p.index]) p.index++ return \u0026ValNode{ val: v, } } func (p *Parser) Result() Node { return p.prev } package interpreter import \"testing\" func TestInterpreter(t *testing.T) { p := \u0026Parser{} p.Parse(\"1 + 2 + 3 - 4 + 5 - 6\") res := p.Result().Interpret() expect := 1 if res != expect { t.Fatalf(\"expect %d got %d\", expect, res) } } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:10:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"visitor 访问者模式可以给一系列对象透明的添加功能，并且把相关代码封装到一个类中。 对象只要预留访问者接口Accept则后期为对象添加功能的时候就不需要改动对象。 package visitor import \"fmt\" type Customer interface { Accept(Visitor) } type Visitor interface { Visit(Customer) } type EnterpriseCustomer struct { name string } type CustomerCol struct { customers []Customer } func (c *CustomerCol) Add(customer Customer) { c.customers = append(c.customers, customer) } func (c *CustomerCol) Accept(visitor Visitor) { for _, customer := range c.customers { customer.Accept(visitor) } } func NewEnterpriseCustomer(name string) *EnterpriseCustomer { return \u0026EnterpriseCustomer{ name: name, } } func (c *EnterpriseCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type IndividualCustomer struct { name string } func NewIndividualCustomer(name string) *IndividualCustomer { return \u0026IndividualCustomer{ name: name, } } func (c *IndividualCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type ServiceRequestVisitor struct{} func (*ServiceRequestVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\"serving enterprise customer %s\\n\", c.name) case *IndividualCustomer: fmt.Printf(\"serving individual customer %s\\n\", c.name) } } // only for enterprise type AnalysisVisitor struct{} func (*AnalysisVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\"analysis enterprise customer %s\\n\", c.name) } } package visitor func ExampleRequestVisitor() { c := \u0026CustomerCol{} c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Accept(\u0026ServiceRequestVisitor{}) // Output: // serving enterprise customer A company // serving enterprise customer B company // serving individual customer bob } func ExampleAnalysis() { c := \u0026CustomerCol{} c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Accept(\u0026AnalysisVisitor{}) // Output: // analysis enterprise customer A company // analysis enterprise customer B company } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:11:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"mediator 中介者模式封装对象之间互交，使依赖变的简单，并且使复杂互交简单化，封装在中介者中。 例子中的中介者使用单例模式生成中介者。 中介者的change使用switch判断类型。 package mediator import ( \"fmt\" \"strings\" ) type CDDriver struct { Data string } func (c *CDDriver) ReadData() { c.Data = \"music,image\" fmt.Printf(\"CDDriver: reading data %s\\n\", c.Data) GetMediatorInstance().changed(c) } type CPU struct { Video string Sound string } func (c *CPU) Process(data string) { sp := strings.Split(data, \",\") c.Sound = sp[0] c.Video = sp[1] fmt.Printf(\"CPU: split data with Sound %s, Video %s\\n\", c.Sound, c.Video) GetMediatorInstance().changed(c) } type VideoCard struct { Data string } func (v *VideoCard) Display(data string) { v.Data = data fmt.Printf(\"VideoCard: display %s\\n\", v.Data) GetMediatorInstance().changed(v) } type SoundCard struct { Data string } func (s *SoundCard) Play(data string) { s.Data = data fmt.Printf(\"SoundCard: play %s\\n\", s.Data) GetMediatorInstance().changed(s) } type Mediator struct { CD *CDDriver CPU *CPU Video *VideoCard Sound *SoundCard } var mediator *Mediator func GetMediatorInstance() *Mediator { if mediator == nil { mediator = \u0026Mediator{} } return mediator } func (m *Mediator) changed(i interface{}) { switch inst := i.(type) { case *CDDriver: m.CPU.Process(inst.Data) case *CPU: m.Sound.Play(inst.Sound) m.Video.Display(inst.Video) } } package mediator import \"testing\" func TestMediator(t *testing.T) { mediator := GetMediatorInstance() mediator.CD = \u0026CDDriver{} mediator.CPU = \u0026CPU{} mediator.Video = \u0026VideoCard{} mediator.Sound = \u0026SoundCard{} //Tiggle mediator.CD.ReadData() if mediator.CD.Data != \"music,image\" { t.Fatalf(\"CD unexpect data %s\", mediator.CD.Data) } if mediator.CPU.Sound != \"music\" { t.Fatalf(\"CPU unexpect sound data %s\", mediator.CPU.Sound) } if mediator.CPU.Video != \"image\" { t.Fatalf(\"CPU unexpect video data %s\", mediator.CPU.Video) } if mediator.Video.Data != \"image\" { t.Fatalf(\"VidoeCard unexpect data %s\", mediator.Video.Data) } if mediator.Sound.Data != \"music\" { t.Fatalf(\"SoundCard unexpect data %s\", mediator.Sound.Data) } } 参考 ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:12:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"设计模式七大原则 我们在设计一些设计模式时，一般遵循如下七项基本原则，它们分别是: 单一职责原则 (Single Responsibility Principle) 开放-关闭原则 (Open-Closed Principle) 里氏替换原则 (Liskov Substitution Principle) 依赖倒转原则 (Dependence Inversion Principle) 接口隔离原则 (Interface Segregation Principle) 迪米特法则（Law Of Demeter） 组合/聚合复用原则 (Composite/Aggregate Reuse Principle) ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:13:0","tags":["design pattern"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"创建型设计模式 常用的有：单例模式、工厂模式（工厂方法和抽象工厂）、建造者模式。不常用的有：原型模式。 创建型模式主要解决对象的创建问题，封装复杂的创建过程，解耦对象的创建代码和使用代码。其中，单例模式用来创建全局唯一的对象。工厂模式用来创建不同但是相关类型的对象（继承同一父类或者接口的一组子类），由给定的参数来决定创建哪种类型的对象。建造者模式是用来创建复杂对象，可以通过设置不同的可选参数，“定制化”地创建不同的对象。原型模式针对创建成本比较大的对象，利用对已有对象进行复制的方式进行创建，以达到节省创建时间的目的。 设计模式总览：http://c.biancheng.net/view/1320.html ","date":"2022-01-22 09:19:30","objectID":"/create_type/:0:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"simple factory go 语言没有构造函数一说，所以一般会定义NewXXX函数来初始化相关类。 NewXXX 函数根据参数返回不同接口时就是简单工厂模式。 在这个simplefactory包中只有API 接口和NewAPI函数为包外可见，封装了实现细节。 package simplefactory import \"fmt\" //API is interface type API interface { Say(name string) string } //NewAPI return Api instance by type func NewAPI(t int) API { if t == 1 { return \u0026hiAPI{} } else if t == 2 { return \u0026helloAPI{} } return nil } //hiAPI is one of API implement type hiAPI struct{} //Say hi to name func (*hiAPI) Say(name string) string { return fmt.Sprintf(\"Hi, %s\", name) } //HelloAPI is another API implement type helloAPI struct{} //Say hello to name func (*helloAPI) Say(name string) string { return fmt.Sprintf(\"Hello, %s\", name) } package simplefactory import \"testing\" //TestType1 test get hiapi with factory func TestType1(t *testing.T) { api := NewAPI(1) s := api.Say(\"Tom\") if s != \"Hi, Tom\" { t.Fatal(\"Type1 test fail\") } } func TestType2(t *testing.T) { api := NewAPI(2) s := api.Say(\"Tom\") if s != \"Hello, Tom\" { t.Fatal(\"Type2 test fail\") } } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:1:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"factory method 工厂方法模式使用子类的方式延迟生成对象到子类中实现。 Go中不存在继承 所以使用匿名组合来实现 package factorymethod //Operator 是被封装的实际类接口 type Operator interface { SetA(int) SetB(int) Result() int } //OperatorFactory 是工厂接口 type OperatorFactory interface { Create() Operator } //OperatorBase 是Operator 接口实现的基类，封装公用方法 type OperatorBase struct { a, b int } //SetA 设置 A func (o *OperatorBase) SetA(a int) { o.a = a } //SetB 设置 B func (o *OperatorBase) SetB(b int) { o.b = b } //PlusOperatorFactory 是 PlusOperator 的工厂类 type PlusOperatorFactory struct{} func (PlusOperatorFactory) Create() Operator { return \u0026PlusOperator{ OperatorBase: \u0026OperatorBase{}, } } //PlusOperator Operator 的实际加法实现 type PlusOperator struct { *OperatorBase } //Result 获取结果 func (o PlusOperator) Result() int { return o.a + o.b } //MinusOperatorFactory 是 MinusOperator 的工厂类 type MinusOperatorFactory struct{} func (MinusOperatorFactory) Create() Operator { return \u0026MinusOperator{ OperatorBase: \u0026OperatorBase{}, } } //MinusOperator Operator 的实际减法实现 type MinusOperator struct { *OperatorBase } //Result 获取结果 func (o MinusOperator) Result() int { return o.a - o.b } package factorymethod import \"testing\" func compute(factory OperatorFactory, a, b int) int { op := factory.Create() op.SetA(a) op.SetB(b) return op.Result() } func TestOperator(t *testing.T) { var ( factory OperatorFactory ) factory = PlusOperatorFactory{} if compute(factory, 1, 2) != 3 { t.Fatal(\"error with factory method pattern\") } factory = MinusOperatorFactory{} if compute(factory, 4, 2) != 2 { t.Fatal(\"error with factory method pattern\") } } 一般情况下，工厂模式分为三种更加细分的类型：简单工厂、工厂方法和抽象工厂。不过，在 GoF 的《设计模式》一书中，它将简单工厂模式看作是工厂方法模式的一种特例，所以工厂模式只被分成了工厂方法和抽象工厂两类。实际上，前面一种分类方法更加常见，所以，在今天的讲解中，我们沿用第一种分类方法。在这三种细分的工厂模式中，简单工厂、工厂方法原理比较简单，在实际的项目中也比较常用。而抽象工厂的原理稍微复杂点，在实际的项目中相对也不常用。所以，我们今天讲解的重点是前两种工厂模式。对于抽象工厂，你稍微了解一下即可。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:2:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"工厂方法 工厂方法模式比起简单工厂模式更加符合开闭原则（开闭原则，在面向对象编程领域中，规定“软件中的对象（类，模块，函数等等）应该对于扩展是开放的，但是对于修改是封闭的”，这意味着一个实体是允许在不改变它的源代码的前提下变更它的行为。该特性在产品化的环境中是特别有价值的，在这种环境中，改变源代码需要代码审查，单元测试以及诸如此类的用以确保产品使用质量的过程。遵循这种原则的代码在扩展时并不发生改变，因此无需上述的过程。——–百度百科）。 我们可以为工厂类再创建一个简单工厂，也就是工厂的工厂，用来创建工厂类对象 实际上，对于规则配置文件解析这个应用场景来说，工厂模式需要额外创建诸多 Factory 类，也会增加代码的复杂性，而且，每个 Factory 类只是做简单的 new 操作，功能非常单薄（只有一行代码），也没必要设计成独立的类，所以，在这个应用场景下，简单工厂模式简单好用，比工厂方法模式更加合适。 那什么时候该用工厂方法模式，而非简单工厂模式呢？ 我们前面提到，之所以将某个代码块剥离出来，独立为函数或者类，原因是这个代码块的逻辑过于复杂，剥离之后能让代码更加清晰，更加可读、可维护。但是，如果代码块本身并不复杂，就几行代码而已，我们完全没必要将它拆分成单独的函数或者类。 基于这个设计思想，当对象的创建逻辑比较复杂，不只是简单的 new 一下就可以，而是要组合其他类对象，做各种初始化操作的时候，我们推荐使用工厂方法模式，将复杂的创建逻辑拆分到多个工厂类中，让每个工厂类都不至于过于复杂。而使用简单工厂模式，将所有的创建逻辑都放到一个工厂类中，会导致这个工厂类变得很复杂。 除此之外，在某些场景下，如果对象不可复用，那工厂类每次都要返回不同的对象。如果我们使用简单工厂模式来实现，就只能选择第一种包含 if 分支逻辑的实现方式。如果我们还想避免烦人的 if-else 分支逻辑，这个时候，我们就推荐使用工厂方法模式。 总结 当创建逻辑比较复杂，是一个“大工程”的时候，我们就考虑使用工厂模式，封装对象的创建过程，将对象的创建和使用相分离。何为创建逻辑比较复杂呢？我总结了下面两种情况。第一种情况：类似规则配置解析的例子，代码中存在 if-else 分支判断，动态地根据不同的类型创建不同的对象。针对这种情况，我们就考虑使用工厂模式，将这一大坨 if-else 创建对象的代码抽离出来，放到工厂类中。还有一种情况，尽管我们不需要根据不同的类型创建不同的对象，但是，单个对象本身的创建过程比较复杂，比如前面提到的要组合其他类对象，做各种初始化操作。在这种情况下，我们也可以考虑使用工厂模式，将对象的创建过程封装到工厂类中。 对于第一种情况，当每个对象的创建逻辑都比较简单的时候，我推荐使用简单工厂模式，将多个对象的创建逻辑放到一个工厂类中。当每个对象的创建逻辑都比较复杂的时候，为了避免设计一个过于庞大的简单工厂类，我推荐使用工厂方法模式，将创建逻辑拆分得更细，每个对象的创建逻辑独立到各自的工厂类中。同理，对于第二种情况，因为单个对象本身的创建逻辑就比较复杂，所以，我建议使用工厂方法模式。 除了刚刚提到的这几种情况之外，如果创建对象的逻辑并不复杂，那我们就直接通过 new 来创建对象就可以了，不需要使用工厂模式。 现在，我们上升一个思维层面来看工厂模式，它的作用无外乎下面这四个。这也是判断要不要使用工厂模式的最本质的参考标准。 封装变化：创建逻辑有可能变化，封装成工厂类之后，创建逻辑的变更对调用者透明。 代码复用：创建代码抽离到独立的工厂类之后可以复用。 隔离复杂性：封装复杂的创建逻辑，调用者无需了解如何创建对象。 控制复杂度：将创建代码抽离出来，让原本的函数或类职责更单一，代码更简洁。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:2:1","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"如何设计实现一个Dependency Injection框架？ 总结 DI 容器在一些软件开发中已经成为了标配，比如 Spring IOC、Google Guice。但是，大部分人可能只是把它当作一个黑盒子来使用，并未真正去了解它的底层是如何实现的。当然，如果只是做一些简单的小项目，简单会用就足够了，但是，如果我们面对的是非常复杂的系统，当系统出现问题的时候，对底层原理的掌握程度，决定了我们排查问题的能力，直接影响到我们排查问题的效率。今天，我们讲解了一个简单的 DI 容器的实现原理，其核心逻辑主要包括：配置文件解析，以及根据配置文件通过“反射”语法来创建对象。其中，创建对象的过程就应用到了我们在学的工厂模式。对象创建、组装、管理完全有 DI 容器来负责，跟具体业务代码解耦，让程序员聚焦在业务代码的开发上。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:2:2","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"abstract factory 抽象工厂模式用于生成产品族的工厂，所生成的对象是有关联的。 如果抽象工厂退化成生成的对象无关联则成为工厂函数模式。 比如本例子中使用RDB和XML存储订单信息，抽象工厂分别能生成相关的主订单信息和订单详情信息。 如果业务逻辑中需要替换使用的时候只需要改动工厂函数相关的类就能替换使用不同的存储方式了。 package abstractfactory import \"fmt\" //OrderMainDAO 为订单主记录 type OrderMainDAO interface { SaveOrderMain() } //OrderDetailDAO 为订单详情纪录 type OrderDetailDAO interface { SaveOrderDetail() } //DAOFactory DAO 抽象模式工厂接口 type DAOFactory interface { CreateOrderMainDAO() OrderMainDAO CreateOrderDetailDAO() OrderDetailDAO } //RDBMainDAP 为关系型数据库的OrderMainDAO实现 type RDBMainDAO struct{} //SaveOrderMain ... func (*RDBMainDAO) SaveOrderMain() { fmt.Print(\"rdb main save\\n\") } //RDBDetailDAO 为关系型数据库的OrderDetailDAO实现 type RDBDetailDAO struct{} // SaveOrderDetail ... func (*RDBDetailDAO) SaveOrderDetail() { fmt.Print(\"rdb detail save\\n\") } //RDBDAOFactory 是RDB 抽象工厂实现 type RDBDAOFactory struct{} func (*RDBDAOFactory) CreateOrderMainDAO() OrderMainDAO { return \u0026RDBMainDAO{} } func (*RDBDAOFactory) CreateOrderDetailDAO() OrderDetailDAO { return \u0026RDBDetailDAO{} } //XMLMainDAO XML存储 type XMLMainDAO struct{} //SaveOrderMain ... func (*XMLMainDAO) SaveOrderMain() { fmt.Print(\"xml main save\\n\") } //XMLDetailDAO XML存储 type XMLDetailDAO struct{} // SaveOrderDetail ... func (*XMLDetailDAO) SaveOrderDetail() { fmt.Print(\"xml detail save\") } //XMLDAOFactory 是XML 抽象工厂实现 type XMLDAOFactory struct{} func (*XMLDAOFactory) CreateOrderMainDAO() OrderMainDAO { return \u0026XMLMainDAO{} } func (*XMLDAOFactory) CreateOrderDetailDAO() OrderDetailDAO { return \u0026XMLDetailDAO{} } package abstractfactory func getMainAndDetail(factory DAOFactory) { factory.CreateOrderMainDAO().SaveOrderMain() factory.CreateOrderDetailDAO().SaveOrderDetail() } func ExampleRdbFactory() { var factory DAOFactory factory = \u0026RDBDAOFactory{} getMainAndDetail(factory) // Output: // rdb main save // rdb detail save } func ExampleXmlFactory() { var factory DAOFactory factory = \u0026XMLDAOFactory{} getMainAndDetail(factory) // Output: // xml main save // xml detail save } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:3:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"builder package builder //Builder 是生成器接口 type Builder interface { Part1() Part2() Part3() } type Director struct { builder Builder } // NewDirector ... func NewDirector(builder Builder) *Director { return \u0026Director{ builder: builder, } } //Construct Product func (d *Director) Construct() { d.builder.Part1() d.builder.Part2() d.builder.Part3() } type Builder1 struct { result string } func (b *Builder1) Part1() { b.result += \"1\" } func (b *Builder1) Part2() { b.result += \"2\" } func (b *Builder1) Part3() { b.result += \"3\" } func (b *Builder1) GetResult() string { return b.result } type Builder2 struct { result int } func (b *Builder2) Part1() { b.result += 1 } func (b *Builder2) Part2() { b.result += 2 } func (b *Builder2) Part3() { b.result += 3 } func (b *Builder2) GetResult() int { return b.result } package builder import \"testing\" func TestBuilder1(t *testing.T) { builder := \u0026Builder1{} director := NewDirector(builder) director.Construct() res := builder.GetResult() if res != \"123\" { t.Fatalf(\"Builder1 fail expect 123 acture %s\", res) } } func TestBuilder2(t *testing.T) { builder := \u0026Builder2{} director := NewDirector(builder) director.Construct() res := builder.GetResult() if res != 6 { t.Fatalf(\"Builder2 fail expect 6 acture %d\", res) } } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:4:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"详解构造函数、set方法、建造者模式三种对象创建方式 实际上，建造者模式的原理和代码实现非常简单，掌握起来并不难，难点在于应用场景。比如，你有没有考虑过这样几个问题：直接使用构造函数或者配合 set 方法就能创建对象，为什么还需要建造者模式来创建呢？建造者模式和工厂模式都可以创建对象，那它们两个的区别在哪里呢？ ","date":"2022-01-22 09:19:30","objectID":"/create_type/:4:1","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"为什么需要建造者模式？ 在平时的开发中，创建一个对象最常用的方式是，使用 new 关键字调用类的构造函数来完成。我的问题是，什么情况下这种方式就不适用了，就需要采用建造者模式来创建对象呢？你可以先思考一下，下面我通过一个例子来带你看一下。假设有这样一道设计面试题：我们需要定义一个资源池配置类 ResourcePoolConfig。这里的资源池，你可以简单理解为线程池、连接池、对象池等。在这个资源池配置类中，有以下几个成员变量，也就是可配置项。现在，请你编写代码实现这个 ResourcePoolConfig 类。 只要你稍微有点开发经验，那实现这样一个类对你来说并不是件难事。最常见、最容易想到的实现思路如下代码所示。因为 maxTotal、maxIdle、minIdle 不是必填变量，所以在创建 ResourcePoolConfig 对象的时候，我们通过往构造函数中，给这几个参数传递 null 值，来表示使用默认值。 java public class ResourcePoolConfig { private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig(String name, Integer maxTotal, Integer maxIdle, Integer minIdle) { if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(\"name should not be empty.\"); } this.name = name; if (maxTotal != null) { if (maxTotal \u003c= 0) { throw new IllegalArgumentException(\"maxTotal should be positive.\"); } this.maxTotal = maxTotal; } if (maxIdle != null) { if (maxIdle \u003c 0) { throw new IllegalArgumentException(\"maxIdle should not be negative.\"); } this.maxIdle = maxIdle; } if (minIdle != null) { if (minIdle \u003c 0) { throw new IllegalArgumentException(\"minIdle should not be negative.\"); } this.minIdle = minIdle; } } //...省略getter方法... } 现在，ResourcePoolConfig 只有 4 个可配置项，对应到构造函数中，也只有 4 个参数，参数的个数不多。但是，如果可配置项逐渐增多，变成了 8 个、10 个，甚至更多，那继续沿用现在的设计思路，构造函数的参数列表会变得很长，代码在可读性和易用性上都会变差。在使用构造函数的时候，我们就容易搞错各参数的顺序，传递进错误的参数值，导致非常隐蔽的 bug。 // 参数太多，导致可读性差、参数可能传递错误 ResourcePoolConfig config = new ResourcePoolConfig(\"dbconnectionpool\", 16, null, 8, null, false , true, 10, 20，false， true); 解决这个问题的办法你应该也已经想到了，那就是用 set() 函数来给成员变量赋值，以替代冗长的构造函数。我们直接看代码，具体如下所示。其中，配置项 name 是必填的，所以我们把它放到构造函数中设置，强制创建类对象的时候就要填写。其他配置项 maxTotal、maxIdle、minIdle 都不是必填的，所以我们通过 set() 函数来设置，让使用者自主选择填写或者不填写。 java public class ResourcePoolConfig { private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig(String name) { if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(\"name should not be empty.\"); } this.name = name; } public void setMaxTotal(int maxTotal) { if (maxTotal \u003c= 0) { throw new IllegalArgumentException(\"maxTotal should be positive.\"); } this.maxTotal = maxTotal; } public void setMaxIdle(int maxIdle) { if (maxIdle \u003c 0) { throw new IllegalArgumentException(\"maxIdle should not be negative.\"); } this.maxIdle = maxIdle; } public void setMinIdle(int minIdle) { if (minIdle \u003c 0) { throw new IllegalArgumentException(\"minIdle should not be negative.\"); } this.minIdle = minIdle; } //...省略getter方法... } 接下来，我们来看新的 ResourcePoolConfig 类该如何使用。我写了一个示例代码，如下所示。没有了冗长的函数调用和参数列表，代码在可读性和易用性上提高了很多。 // ResourcePoolConfig使用举例 ResourcePoolConfig config = new ResourcePoolConfig(\"dbconnectionpool\"); config.setMaxTotal(16); config.setMaxIdle(8); 至此，我们仍然没有用到建造者模式，通过构造函数设置必填项，通过 set() 方法设置可选配置项，就能实现我们的设计需求。如果我们把问题的难度再加大点，比如，还需要解决下面这三个问题，那现在的设计思路就不能满足了。 我们刚刚讲到，name 是必填的，所以，我们把它放到构造函数中，强制创建对象的时候就设置。如果必填的配置项有很多，把这些必填配置项都放到构造函数中设置，那构造函数就又会出现参数列表很长的问题。如果我们把必填项也通过 set() 方法设置，那校验这些必填项是否已经填写的逻辑就无处安放了。除此之外，假设配置项之间有一定的依赖关系，比如，如果用户设置了 maxTotal、maxIdle、minIdle 其中一个，就必须显式地设置另外两个；或者配置项之间有一定的约束条件，比如，maxIdle 和 minIdle 要小于等于 maxTotal。如果我们继续使用现在的设计思路，那这些配置项之间的依赖关系或者约束条件的校验逻辑就无处安放了。如果我们希望 ResourcePoolConfig 类对象是不可变对象，也就是说，对象在创建好之后，就不能再修改内部的属性值。要实现这个功能，我们就不能在 ResourcePoolConfig 类中暴露 set() 方法。 为了解决这些问题，建造者模式就派上用场了。 我们可以把校验逻辑放置到 Builder 类中，先创建建造者，并且通过 set() 方法设置建造者的变量值，然后在使用 build() 方法真正创建对象之前，做集中的校验，校验通过之后才会创建对象。除此之外，我们把 ResourcePoolConfig 的构造函数改为 private 私有权限。这样我们就只能通过建造者来创建 ResourcePoolConfig 类对象。并且，ResourcePoolConfig 没有提供任何 set() 方法，这样我们创建出来的对象就是不可变对象了。我们用建造者模式重新实现了上面的需求，具体的代码如下所示： java public class ResourcePoolConfig { pr","date":"2022-01-22 09:19:30","objectID":"/create_type/:4:2","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"与工厂模式有何区别？ 从上面的讲解中，我们可以看出，建造者模式是让建造者类来负责对象的创建工作。上一节课中讲到的工厂模式，是由工厂类来负责对象创建的工作。那它们之间有什么区别呢？实际上，工厂模式是用来创建不同但是相关类型的对象（继承同一父类或者接口的一组子类），由给定的参数来决定创建哪种类型的对象。建造者模式是用来创建一种类型的复杂对象，通过设置不同的可选参数，“定制化”地创建不同的对象。网上有一个经典的例子很好地解释了两者的区别。顾客走进一家餐馆点餐，我们利用工厂模式，根据用户不同的选择，来制作不同的食物，比如披萨、汉堡、沙拉。对于披萨来说，用户又有各种配料可以定制，比如奶酪、西红柿、起司，我们通过建造者模式根据用户选择的不同配料来制作披萨。实际上，我们也不要太学院派，非得把工厂模式、建造者模式分得那么清楚，我们需要知道的是，每个模式为什么这么设计，能解决什么问题。只有了解了这些最本质的东西，我们才能不生搬硬套，才能灵活应用，甚至可以混用各种模式创造出新的模式，来解决特定场景的问题。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:4:3","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"prototype 原型模式使对象能复制自身，并且暴露到接口中，使客户端面向接口编程时，不知道接口实际对象的情况下生成新的对象。 原型模式配合原型管理器使用，使得客户端在不知道具体类的情况下，通过接口管理器得到新的实例，并且包含部分预设定配置。 package prototype //Cloneable 是原型对象需要实现的接口 type Cloneable interface { Clone() Cloneable } type PrototypeManager struct { prototypes map[string]Cloneable } func NewPrototypeManager() *PrototypeManager { return \u0026PrototypeManager{ prototypes: make(map[string]Cloneable), } } func (p *PrototypeManager) Get(name string) Cloneable { return p.prototypes[name].Clone() } func (p *PrototypeManager) Set(name string, prototype Cloneable) { p.prototypes[name] = prototype } package prototype import \"testing\" var manager *PrototypeManager type Type1 struct { name string } func (t *Type1) Clone() Cloneable { tc := *t return \u0026tc } type Type2 struct { name string } func (t *Type2) Clone() Cloneable { tc := *t return \u0026tc } func TestClone(t *testing.T) { t1 := manager.Get(\"t1\") t2 := t1.Clone() if t1 == t2 { t.Fatal(\"error! get clone not working\") } } func TestCloneFromManager(t *testing.T) { c := manager.Get(\"t1\").Clone() t1 := c.(*Type1) if t1.name != \"type1\" { t.Fatal(\"error\") } } func init() { manager = NewPrototypeManager() t1 := \u0026Type1{ name: \"type1\", } manager.Set(\"t1\", t1) } 对于熟悉 JavaScript 语言的前端程序员来说，原型模式是一种比较常用的开发模式。这是因为，有别于 Java、C++ 等基于类的面向对象编程语言，JavaScript 是一种基于原型的面向对象编程语言。即便 JavaScript 现在也引入了类的概念，但它也只是基于原型的语法糖而已。不过，如果你熟悉的是 Java、C++ 等这些编程语言，那在实际的开发中，就很少用到原型模式了。 通过一个 clone 散列表的例子带你搞清楚：原型模式的应用场景，以及它的两种实现方式：深拷贝和浅拷贝。 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:5:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"原型模式的原理与应用 如果对象的创建成本比较大，而同一个类的不同对象之间差别不大（大部分字段都相同），在这种情况下，我们可以利用对已有对象（原型）进行复制（或者叫拷贝）的方式来创建新对象，以达到节省创建时间的目的。这种基于原型来创建对象的方式就叫作原型设计模式（Prototype Design Pattern），简称原型模式。 …… ","date":"2022-01-22 09:19:30","objectID":"/create_type/:5:1","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"原型模式的实现方式：深拷贝和浅拷贝 我们来看，在内存中，用散列表组织的搜索关键词信息是如何存储的。我画了一张示意图，大致结构如下所示。从图中我们可以发现，散列表索引中，每个结点存储的 key 是搜索关键词，value 是 SearchWord 对象的内存地址。SearchWord 对象本身存储在散列表之外的内存空间中。 浅拷贝和深拷贝的区别在于，浅拷贝只会复制图中的索引（散列表），不会复制数据（SearchWord 对象）本身。相反，深拷贝不仅仅会复制索引，还会复制数据本身。浅拷贝得到的对象（newKeywords）跟原始对象（currentKeywords）共享数据（SearchWord 对象），而深拷贝得到的是一份完完全全独立的对象。具体的对比如下图所示： …… ","date":"2022-01-22 09:19:30","objectID":"/create_type/:5:2","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"singleton 使用懒汉模式的单例模式，使用双重检查加锁保证线程安全 package singleton import \"sync\" // Singleton 是单例模式接口，导出的 // 通过该接口可以避免 GetInstance 返回一个包私有类型的指针 type Singleton interface { foo() } // singleton 是单例模式类，包私有的 type singleton struct{} func (s singleton) foo() {} var ( instance *singleton once sync.Once ) //GetInstance 用于获取单例模式对象 func GetInstance() Singleton { once.Do(func() { instance = \u0026singleton{} }) return instance } package singleton import ( \"sync\" \"testing\" ) const parCount = 100 func TestSingleton(t *testing.T) { ins1 := GetInstance() ins2 := GetInstance() if ins1 != ins2 { t.Fatal(\"instance is not equal\") } } func TestParallelSingleton(t *testing.T) { start := make(chan struct{}) wg := sync.WaitGroup{} wg.Add(parCount) instances := [parCount]Singleton{} for i := 0; i \u003c parCount; i++ { go func(index int) { //协程阻塞，等待channel被关闭才能继续运行 \u003c-start instances[index] = GetInstance() wg.Done() }(i) } //关闭channel，所有协程同时开始运行，实现并行(parallel) close(start) wg.Wait() for i := 1; i \u003c parCount; i++ { if instances[i] != instances[i-1] { t.Fatal(\"instance is not equal\") } } } 单例存在哪些问题？单例与静态类的区别？有何替代的解决方案？ ","date":"2022-01-22 09:19:30","objectID":"/create_type/:6:0","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"为什么要使用单例？ 单例设计模式（Singleton Design Pattern）理解起来非常简单。一个类只允许创建一个对象（或者实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。 除了使用类级别锁之外，实际上，解决资源竞争问题的办法还有很多，分布式锁是最常听到的一种解决方案。不过，实现一个安全可靠、无 bug、高性能的分布式锁，并不是件容易的事情。除此之外，并发队列（比如 Java 中的 BlockingQueue）也可以解决这个问题：多个线程同时往并发队列里写日志，一个单独的线程负责将并发队列中的数据，写入到日志文件。这种方式实现起来也稍微有点复杂。 相对于这两种解决方案，单例模式的解决思路就简单一些了。单例模式相对于之前类级别锁的好处是，不用创建那么多 Logger 对象，一方面节省内存空间，另一方面节省系统文件句柄（对于操作系统来说，文件句柄也是一种资源，不能随便浪费）。我们将 Logger 设计成一个单例类，程序中只允许创建一个 Logger 对象，所有的线程共享使用的这一个 Logger 对象，共享一个 FileWriter 对象，而 FileWriter 本身是对象级别线程安全的，也就避免了多线程情况下写日志会互相覆盖的问题。 我们将 Logger 设计成一个单例类，程序中只允许创建一个 Logger 对象，所有的线程共享使用的这一个 Logger 对象，共享一个 FileWriter 对象，而 FileWriter 本身是对象级别线程安全的，也就避免了多线程情况下写日志会互相覆盖的问题。 表示全局唯一类（或者只保存一份的数据） 比如：配置信息类、唯一递增ID号码生成器（如果程序中有两个对象，那就会存在生成重复 ID 的情况） 如何实现一个单例 要实现一个单例，我们需要关注的点无外乎下面几个：构造函数需要是 private 访问权限的，这样才能避免外部通过 new 创建实例；考虑对象创建时的线程安全问题；考虑是否支持延迟加载；考虑 getInstance() 性能是否高（是否加锁）。 饿汉式（一开始就很饿） 饿汉式的实现方式比较简单。在类加载的时候，instance 静态实例就已经创建并初始化好了，所以，instance 实例的创建过程是线程安全的。不过，这样的实现方式不支持延迟加载（在真正用到 IdGenerator 的时候，再创建实例），从名字中我们也可以看出这一点。具体的代码实现如下所示： java public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static final IdGenerator instance = new IdGenerator(); private IdGenerator() {} public static IdGenerator getInstance() { return instance; } public long getId() { return id.incrementAndGet(); } } 有人觉得这种实现方式不好，因为不支持延迟加载，如果实例占用资源多（比如占用内存多）或初始化耗时长（比如需要加载各种配置文件），提前初始化实例是一种浪费资源的行为。最好的方法应该在用到的时候再去初始化。不过，我个人并不认同这样的观点。 如果初始化耗时长，那我们最好不要等到真正要用它的时候，才去执行这个耗时长的初始化过程，这会影响到系统的性能（比如，在响应客户端接口请求的时候，做这个初始化操作，会导致此请求的响应时间变长，甚至超时）。采用饿汉式实现方式，将耗时的初始化操作，提前到程序启动的时候完成，这样就能避免在程序运行的时候，再去初始化导致的性能问题。 懒汉式 有饿汉式，对应的，就有懒汉式。懒汉式相对于饿汉式的优势是支持延迟加载。具体的代码实现如下所示： java public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() {} public static synchronized IdGenerator getInstance() { if (instance == null) { instance = new IdGenerator(); } return instance; } public long getId() { return id.incrementAndGet(); } } 不过懒汉式的缺点也很明显，我们给 getInstance() 这个方法加了一把大锁（synchronzed），导致这个函数的并发度很低。量化一下的话，并发度是 1，也就相当于串行操作了。而这个函数是在单例使用期间，一直会被调用。如果这个单例类偶尔会被用到，那这种实现方式还可以接受。但是，如果频繁地用到，那频繁加锁、释放锁及并发度低等问题，会导致性能瓶颈，这种实现方式就不可取了。 双重检测 饿汉式不支持延迟加载，懒汉式有性能问题，不支持高并发。那我们再来看一种既支持延迟加载、又支持高并发的单例实现方式，也就是双重检测实现方式。在这种实现方式中，只要 instance 被创建之后，即便再调用 getInstance() 函数也不会再进入到加锁逻辑中了。所以，这种实现方式解决了懒汉式并发度低的问题。具体的代码实现如下所示： java public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() {} public static IdGenerator getInstance() { if (instance == null) { synchronized(IdGenerator.class) { // 此处为类级别的锁 if (instance == null) { instance = new IdGenerator(); } } } return instance; } public long getId() { return id.incrementAndGet(); } } 网上有人说，这种实现方式有些问题。因为指令重排序，可能会导致 IdGenerator 对象被 new 出来，并且赋值给 instance 之后，还没来得及初始化（执行构造函数中的代码逻辑），就被另一个线程使用了。 要解决这个问题，我们需要给 instance 成员变量加上 volatile 关键字，禁止指令重排序才行。实际上，只有很低版本的 Java 才会有这个问题。我们现在用的高版本的 Java 已经在 JDK 内部实现中解决了这个问题（解决的方法很简单，只要把对象 new 操作和初始化操作设计为原子操作，就自然能禁止重排序）。 静态内部类 我们再来看一种比双重检测更加简单的实现方法，那就是利用 Java 的静态内部类。它有点类似饿汉式，但又能做到了延迟加载。具体是怎么做到的呢？我们先来看它的代码实现。 java public class IdGenerator { private AtomicLong id = new AtomicLong(0); private IdGenerator() {} private static class SingletonHolder{ private static final IdGenerator instance = new IdGenerator(); } public static IdGenerator getInstance() { return SingletonHolder.instance; } public long getId() { return id.incrementAndGet(); } } SingletonHolder 是一个静态内部类，当外部类 IdGenerator 被加载的时候，并不会创建 SingletonHolder 实例对象。只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance。instance 的唯一性、创建过程的线程安全性，都由 JVM 来保证。所以，这种实现方法既保证了线程安全，又能做到延迟加载。 枚举 最后，我们介绍一种最简单的实现方式，基于枚举类型的单例实现。这种实现方式通过 Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性。具体的代码如下所示： java public enum IdGenerator { INSTANCE; private AtomicLong id = new AtomicLong(0); public long getId() { return id.incrementAndGet(); } } 总结 单例有下面几种经典的实现方式。 饿汉式 饿汉式的实现方式，在类加载的期间，就已经将 instance 静态实例初始化好了，所以，instance 实例的创建是线程安全的。不过，这样的实现方式不支持延迟加载实例。 懒汉式 懒汉式相对于饿汉式的优势是支持延迟加载。这种实现方式会导致频繁加锁、释放锁，以及并发度低等","date":"2022-01-22 09:19:30","objectID":"/create_type/:6:1","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"单例这种设计模式存在哪些问题？ 为什么会被称为反模式？如果不用单例，该如何表示全局唯一类？有何替代的解决方案？ 大部分情况下，我们在项目中使用单例，都是用它来表示一些全局唯一类，比如配置信息类、连接池类、ID 生成器类。单例模式书写简洁、使用方便，在代码中，我们不需要创建对象，直接通过类似 IdGenerator.getInstance().getId() 这样的方法来调用就可以了。但是，这种使用方法有点类似硬编码（hard code），会带来诸多问题。 单例对 OOP 特性的支持不友好我们知道，OOP 的四大特性是封装、抽象、继承、多态。单例这种设计模式对于其中的抽象、继承、多态都支持得不好。 java public class Order { public void create(...) { //... long id = IdGenerator.getInstance().getId(); //... } } public class User { public void create(...) { // ... long id = IdGenerator.getInstance().getId(); //... } } IdGenerator 的使用方式违背了基于接口而非实现的设计原则，也就违背了广义上理解的 OOP 的抽象特性。如果未来某一天，我们希望针对不同的业务采用不同的 ID 生成算法。比如，订单 ID 和用户 ID 采用不同的 ID 生成器来生成。为了应对这个需求变化，我们需要修改所有用到 IdGenerator 类的地方，这样代码的改动就会比较大。 java public class Order { public void create(...) { //... long id = IdGenerator.getInstance().getId(); // 需要将上面一行代码，替换为下面一行代码 long id = OrderIdGenerator.getIntance().getId(); //... } } public class User { public void create(...) { // ... long id = IdGenerator.getInstance().getId(); // 需要将上面一行代码，替换为下面一行代码 long id = UserIdGenerator.getIntance().getId(); } } 除此之外，单例对继承、多态特性的支持也不友好。这里我之所以会用“不友好”这个词，而非“完全不支持”，是因为从理论上来讲，单例类也可以被继承、也可以实现多态，只是实现起来会非常奇怪，会导致代码的可读性变差。不明白设计意图的人，看到这样的设计，会觉得莫名其妙。所以，一旦你选择将某个类设计成到单例类，也就意味着放弃了继承和多态这两个强有力的面向对象特性，也就相当于损失了可以应对未来需求变化的扩展性。 单例会隐藏类之间的依赖关系 我们知道，代码的可读性非常重要。在阅读代码的时候，我们希望一眼就能看出类与类之间的依赖关系，搞清楚这个类依赖了哪些外部类。通过构造函数、参数传递等方式声明的类之间的依赖关系，我们通过查看函数的定义，就能很容易识别出来。但是，单例类不需要显示创建、不需要依赖参数传递，在函数中直接调用就可以了。如果代码比较复杂，这种调用关系就会非常隐蔽。在阅读代码的时候，我们就需要仔细查看每个函数的代码实现，才能知道这个类到底依赖了哪些单例类。 单例对代码的扩展性不友好 我们知道，单例类只能有一个对象实例。如果未来某一天，我们需要在代码中创建两个实例或多个实例，那就要对代码有比较大的改动。你可能会说，会有这样的需求吗？既然单例类大部分情况下都用来表示全局类，怎么会需要两个或者多个实例呢？实际上，这样的需求并不少见。我们拿数据库连接池来举例解释一下。在系统设计初期，我们觉得系统中只应该有一个数据库连接池，这样能方便我们控制对数据库连接资源的消耗。所以，我们把数据库连接池类设计成了单例类。但之后我们发现，系统中有些 SQL 语句运行得非常慢。这些 SQL 语句在执行的时候，长时间占用数据库连接资源，导致其他 SQL 请求无法响应。为了解决这个问题，我们希望将慢 SQL 与其他 SQL 隔离开来执行。为了实现这样的目的，我们可以在系统中创建两个数据库连接池，慢 SQL 独享一个数据库连接池，其他 SQL 独享另外一个数据库连接池，这样就能避免慢 SQL 影响到其他 SQL 的执行。如果我们将数据库连接池设计成单例类，显然就无法适应这样的需求变更，也就是说，单例类在某些情况下会影响代码的扩展性、灵活性。所以，数据库连接池、线程池这类的资源池，最好还是不要设计成单例类。实际上，一些开源的数据库连接池、线程池也确实没有设计成单例类。 单例对代码的可测试性不友好 单例模式的使用会影响到代码的可测试性。如果单例类依赖比较重的外部资源，比如 DB，我们在写单元测试的时候，希望能通过 mock 的方式将它替换掉。而单例类这种硬编码式的使用方式，导致无法实现 mock 替换。除此之外，如果单例类持有成员变量（比如 IdGenerator 中的 id 成员变量），那它实际上相当于一种全局变量，被所有的代码共享。如果这个全局变量是一个可变全局变量，也就是说，它的成员变量是可以被修改的，那我们在编写单元测试的时候，还需要注意不同测试用例之间，修改了单例类中的同一个成员变量的值，从而导致测试结果互相影响的问题。 单例不支持有参数的构造函数 单例不支持有参数的构造函数，比如我们创建一个连接池的单例对象，我们没法通过参数来指定连接池的大小。针对这个问题，我们来看下都有哪些解决方案。第一种解决思路是：创建完实例之后，再调用 init() 函数传递参数。需要注意的是，我们在使用这个单例类的时候，要先调用 init() 方法，然后才能调用 getInstance() 方法，否则代码会抛出异常。具体的代码实现如下所示： java public class Singleton { private static Singleton instance = null; private final int paramA; private final int paramB; private Singleton(int paramA, int paramB) { this.paramA = paramA; this.paramB = paramB; } public static Singleton getInstance() { if (instance == null) { throw new RuntimeException(\"Run init() first.\"); } return instance; } public synchronized static Singleton init(int paramA, int paramB) { if (instance != null){ throw new RuntimeException(\"Singleton has been created!\"); } instance = new Singleton(paramA, paramB); return instance; } } Singleton.init(10, 50); // 先init，再使用 Singleton singleton = Singleton.getInstance(); 第二种解决思路是：将参数放到 getIntance() 方法中。具体的代码实现如下所示： java public class Singleton { private static Singleton instance = null; private final int paramA; private final int paramB; private Singleton(int paramA, int paramB) { this.paramA = paramA; this.paramB = paramB; } public synchronized static Singleton getInstance(int paramA, int paramB) { if (instance == null) { instance = new Singleton(paramA, paramB); } return instance; } } Singleton singleton = Singleton.getInstance(10, 50); 不知道你有没有发现，上面的代码实现稍微有点问题。如果我们如下两次执行 getInstance() 方法，那获取到的 singleton1 和 signleton2 的 paramA 和 paramB 都是 10 和 50。也就是说，第二次的参数（20，30）没有起作用，而构建的过程也没有给与提示，这样就会误导用户。 java Singleton singleton1 = Singleton.getInstance(10, 50); Singleton singleton2 = Singleton.getInstance(20, 30); 第三种解决思路是：将参数放到另外一个全局变量中。具体的代码实现如下。Config 是一个存储了 paramA 和 paramB 值的全局变量。里面的值既可以像下面的代码那","date":"2022-01-22 09:19:30","objectID":"/create_type/:6:2","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"如何设计实现一个集群环境下的分布式单例模式？ 参考 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:6:3","tags":["design pattern"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:0:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"数组 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:1:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"数组基础 数组是存放在连续内存空间上的相同类型数据的集合。 因为数组的在内存空间的地址是连续的，所以我们在删除或者增添元素的时候，就难免要移动其他元素的地址。 数组元素无法删除，只能覆盖。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:1:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"二分查找 给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target ，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。 有序数组、无重复元素便可想到用二分法查找。注意区间左闭右闭还是左闭右开。 简单实现： package main func search(nums []int, target int) int { left := 0 right := len(nums) - 1 for left \u003c= right { middle := left + (right - left)/2 if nums[middle] \u003e target { right = middle - 1 }else if nums[middle] \u003c target { left = middle + 1 }else{ return middle; } } return -1 } func search(nums []int,t int)int{ left,middle:=0 right:=len(nums) for left\u003c=right{ num=(left+right)/2 if nums[num]==t{ return num } if nums[num]\u003et{ right=num-1 }else{ left=num+1 } } } func search(nums []int,t int)int{ if middle:=len(nums)/2;nums[middle]==t{ return middle } if len(nums)==1{ return -1 } if nums[middle]\u003et{ return search(nums[:middle],t) }else{ return search(nums[middle+1:]) } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:1:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"移除元素 给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。 不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并原地修改输入数组。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 示例 1: 给定 nums = [3,2,2,3], val = 3, 函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 给定 nums = [0,1,2,2,3,0,4,2], val = 2, 函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。 你不需要考虑数组中超出新长度后面的元素 双指针法（快慢指针法）： 通过一个快指针和慢指针在一个for循环下完成两个for循环的工作。 func removeElement(nums []int, val int) int { length:=len(nums) res:=0 for i:=0;i\u003clength;i++{ if nums[i]!=val { nums[res]=nums[i] res++ } } return res } func removeElement(nums []int, val int) int { num := 0 for i, v := range nums { if v == val { num++ } else { nums[i-num] = v } } return len(nums)-num } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:1:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"有序数组的平方 给你一个按 非递减顺序 排序的整数数组 nums，返回 每个数字的平方 组成的新数组，要求也按 非递减顺序 排序。 示例 1： 输入：nums = [-4,-1,0,3,10] 输出：[0,1,9,16,100] 解释：平方后，数组变为 [16,1,0,9,100]，排序后，数组变为 [0,1,9,16,100] 示例 2： 输入：nums = [-7,-3,2,3,11] 输出：[4,9,9,49,121] 双指针法： func sortedSquares(nums []int) []int { n := len(nums) i, j, k := 0, n-1, n-1 ans := make([]int, n) for i \u003c= j { lm, rm := nums[i]*nums[i], nums[j]*nums[j] if lm \u003e rm { ans[k] = lm i++ } else { ans[k] = rm j-- } k-- } return ans } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:1:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"长度最小的子数组 给定一个含有 n 个正整数的数组和一个正整数 s ，找出该数组中满足其和 ≥ s 的长度最小的 连续 子数组，并返回其长度。如果不存在符合条件的子数组，返回 0。 示例： 输入：s = 7, nums = [2,3,1,2,4,3] 输出：2 解释：子数组 [4,3] 是该条件下的长度最小的子数组 滑动窗口 func minSubArrayLen(target int, nums []int) int { i := 0 l := len(nums) // 数组长度 sum := 0 // 子数组之和 result := l + 1 // 初始化返回长度为l+1，目的是为了判断“不存在符合条件的子数组，返回0”的情况 for j := 0; j \u003c l; j++ { sum += nums[j] for sum \u003e= target { subLength := j - i + 1 if subLength \u003c result { result = subLength } sum -= nums[i] i++ } } if result == l+1 { return 0 } else { return result } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:1:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"螺旋矩阵II 给定一个正整数 n，生成一个包含 1 到 n^2 所有元素，且元素按顺时针顺序螺旋排列的正方形矩阵。 示例: 输入: 3 输出: [ [ 1, 2, 3 ], [ 8, 9, 4 ], [ 7, 6, 5 ] ] #思路 模拟行为： func generateMatrix(n int) [][]int { top, bottom := 0, n-1 left, right := 0, n-1 num := 1 tar := n * n matrix := make([][]int, n) for i := 0; i \u003c n; i++ { matrix[i] = make([]int, n) } for num \u003c= tar { for i := left; i \u003c= right; i++ { matrix[top][i] = num num++ } top++ for i := top; i \u003c= bottom; i++ { matrix[i][right] = num num++ } right-- for i := right; i \u003e= left; i-- { matrix[bottom][i] = num num++ } bottom-- for i := bottom; i \u003e= top; i-- { matrix[i][left] = num num++ } left++ } return matrix } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:1:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"总结 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:1:7","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"链表 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"链表理论基础 循环链表可以用来解决约瑟夫环问题。 链表和数组对比： 数组 插入删除时间复杂度：$O(n)$ 查询时间复杂度：$O(1)$ 适用场景：数据量固定，频繁查询，较少增删 链表 插入删除时间复杂度：$O(1)$ 查询时间复杂度：$O(n)$ 适用场景：数据量不固定，频繁增删，较少查询 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"移除链表元素 题意：删除链表中等于给定值 val 的所有节点。 示例 1： 输入：head = [1,2,6,3,4,5,6], val = 6 输出：[1,2,3,4,5] 示例 2： 输入：head = [], val = 1 输出：[] 示例 3： 输入：head = [7,7,7,7], val = 7 输出：[] /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeElements(head *ListNode, val int) *ListNode { dummyHead := \u0026ListNode{} dummyHead.Next = head cur := dummyHead for cur != nil \u0026\u0026 cur.Next != nil { if cur.Next.Val == val { cur.Next = cur.Next.Next } else { cur = cur.Next } } return dummyHead.Next } //如果是c或者c++要在删除后free或者delete掉节点释放内存，java,python,go都会自动回收 另外，移除头节点的话，如果没有虚拟头节点，将头节点往后移一个节点，有虚拟头节点就一般删除节点即可。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"设计链表 设计五个接口： 获取链表第index个节点的数值 在链表的最前面插入一个节点 在链表的最后面插入一个节点 在链表第index个节点前面插入一个节点 删除链表的第index个节点 链表操作的两种方式： 直接使用原来的链表来进行操作。 设置一个虚拟头结点在进行操作。（更方便一点） 题意： 在链表类中实现这些功能： get(index)：获取链表中第 index 个节点的值。如果索引无效，则返回-1。 addAtHead(val)：在链表的第一个元素之前添加一个值为 val 的节点。插入后，新节点将成为链表的第一个节点。 addAtTail(val)：将值为 val 的节点追加到链表的最后一个元素。 addAtIndex(index,val)：在链表中的第 index 个节点之前添加值为 val 的节点。如果 index 等于链表的长度，则该节点将附加到链表的末尾。如果 index 大于链表长度，则不会插入节点。如果index小于0，则在头部插入节点。 deleteAtIndex(index)：如果索引 index 有效，则删除链表中的第 index 个节点。 //循环双链表 type MyLinkedList struct { dummy *Node } type Node struct { Val int Next *Node Pre *Node } //仅保存哑节点，pre-\u003e rear, next-\u003e head /** Initialize your data structure here. */ func Constructor() MyLinkedList { rear := \u0026Node{ Val: -1, Next: nil, Pre: nil, } rear.Next = rear rear.Pre = rear return MyLinkedList{rear} } /** Get the value of the index-th node in the linked list. If the index is invalid, return -1. */ func (this *MyLinkedList) Get(index int) int { head := this.dummy.Next //head == this, 遍历完全 for head != this.dummy \u0026\u0026 index \u003e 0 { index-- head = head.Next } //否则, head == this, 索引无效 if 0 != index { return -1 } return head.Val } /** Add a node of value val before the first element of the linked list. After the insertion, the new node will be the first node of the linked list. */ func (this *MyLinkedList) AddAtHead(val int) { dummy := this.dummy node := \u0026Node{ Val: val, //head.Next指向原头节点 Next: dummy.Next, //head.Pre 指向哑节点 Pre: dummy, } //更新原头节点 dummy.Next.Pre = node //更新哑节点 dummy.Next = node //以上两步不能反 } /** Append a node of value val to the last element of the linked list. */ func (this *MyLinkedList) AddAtTail(val int) { dummy := this.dummy rear := \u0026Node{ Val: val, //rear.Next = dummy(哑节点) Next: dummy, //rear.Pre = ori_rear Pre: dummy.Pre, } //ori_rear.Next = rear dummy.Pre.Next = rear //update dummy dummy.Pre = rear //以上两步不能反 } /** Add a node of value val before the index-th node in the linked list. If index equals to the length of linked list, the node will be appended to the end of linked list. If index is greater than the length, the node will not be inserted. */ func (this *MyLinkedList) AddAtIndex(index int, val int) { head := this.dummy.Next //head = MyLinkedList[index] for head != this.dummy \u0026\u0026 index \u003e 0 { head = head.Next index-- } if index \u003e 0 { return } node := \u0026Node{ Val: val, //node.Next = MyLinkedList[index] Next: head, //node.Pre = MyLinkedList[index-1] Pre: head.Pre, } //MyLinkedList[index-1].Next = node head.Pre.Next = node //MyLinkedList[index].Pre = node head.Pre = node //以上两步不能反 } /** Delete the index-th node in the linked list, if the index is valid. */ func (this *MyLinkedList) DeleteAtIndex(index int) { //链表为空 if this.dummy.Next == this.dummy { return } head := this.dummy.Next //head = MyLinkedList[index] for head.Next != this.dummy \u0026\u0026 index \u003e 0 { head = head.Next index-- } //验证index有效 if index == 0 { //MyLinkedList[index].Pre = index[index-2] head.Next.Pre = head.Pre //MyLinedList[index-2].Next = index[index] head.Pre.Next = head.Next //以上两步顺序无所谓 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"翻转链表 //双指针 func reverseList(head *ListNode) *ListNode { var pre *ListNode cur := head for cur != nil { next := cur.Next cur.Next = pre pre = cur cur = next } return pre } //递归 func reverseList(head *ListNode) *ListNode { return help(nil, head) } func help(pre, head *ListNode)*ListNode{ if head == nil { return pre } next := head.Next head.Next = pre return help(head, next) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"两两交换链表中的节点 给定一个链表，两两交换其中相邻的节点，并返回交换后的链表。 你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。 正常模拟，使用虚拟头节点。 func swapPairs(head *ListNode) *ListNode { dummy := \u0026ListNode{ Next: head, } //head=list[i] //pre=list[i-1] pre := dummy for head != nil \u0026\u0026 head.Next != nil { pre.Next = head.Next next := head.Next.Next head.Next.Next = head head.Next = next //pre=list[(i+2)-1] pre = head //head=list[(i+2)] head = next } return dummy.Next } // 递归版本 func swapPairs(head *ListNode) *ListNode { if head == nil || head.Next == nil { return head } next := head.Next head.Next = swapPairs(next.Next) next.Next = head return next } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"删除链表的倒数第N个节点 给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点。 进阶：你能尝试使用一趟扫描实现吗？ 双指针很好做： /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { dummyHead := \u0026ListNode{} dummyHead.Next = head cur := head prev := dummyHead i := 1 for cur != nil { cur = cur.Next if i \u003e n { prev = prev.Next } i++ } prev.Next = prev.Next.Next return dummyHead.Next } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"链表相交 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 null 。 图示两个链表在节点 c1 开始相交： 题目数据 保证 整个链式结构中不存在环。 注意，函数返回结果后，链表必须 保持其原始结构 。 func getIntersectionNode(headA, headB *ListNode) *ListNode { curA := headA curB := headB lenA, lenB := 0, 0 // 求A，B的长度 for curA != nil { curA = curA.Next lenA++ } for curB != nil { curB = curB.Next lenB++ } var step int var fast, slow *ListNode // 请求长度差，并且让更长的链表先走相差的长度 if lenA \u003e lenB { step = lenA - lenB fast, slow = headA, headB } else { step = lenB - lenA fast, slow = headB, headA } for i:=0; i \u003c step; i++ { fast = fast.Next } // 遍历两个链表遇到相同则跳出遍历 for fast != slow { fast = fast.Next slow = slow.Next } return fast } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:7","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"环形链表II 题意： 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 为了表示给定链表中的环，使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。 说明：不允许修改给定的链表。 判断是否有环：快慢指针法 如何确定环的入口：一个简单的数学题 func detectCycle(head *ListNode) *ListNode { slow, fast := head, head for fast != nil \u0026\u0026 fast.Next != nil { slow = slow.Next fast = fast.Next.Next if slow == fast { for slow != head { slow = slow.Next head = head.Next } return head } } return nil } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:8","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"如何实现LRU缓存淘汰算法? 缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 CPU 缓存、数据库缓存、浏览器缓存等等。 缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用策略 LRU（Least Recently Used）。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:9","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"链表结构 我们先从底层的存储结构上来看一看。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。 链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。我们首先来看最简单、最常用的单链表。 其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。 循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。 接下来我们再来看一个稍微复杂的，在实际的软件开发中，也更加常用的链表结构：双向链表。 相比单链表，双向链表适合解决哪种问题呢？ 从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。 对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗 如何基于链表实现 LRU 缓存淘汰算法？我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。 如果此数据没有在缓存链表中，又可以分为两种情况： 如果此时缓存未满，则将此结点直接插入到链表的头部；如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。 现在我们来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为 O(n)。实际上，我们可以继续优化这个实现思路，比如引入散列表（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)。因为要涉及我们还没有讲到的数据结构，所以这个优化方案，我现在就不详细说了，等讲到散列表的时候，我会再拿出来讲。 除了基于链表的实现思路，实际上还可以用数组来实现 LRU 缓存淘汰策略。如何利用数组实现 LRU 缓存淘汰策略呢？ ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:2:10","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"哈希表 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"哈希表理论基础 哈希表是根据关键码的值而直接进行访问的数据结构，比如数组、map（映射）、set（集合）。 一般用来快速判断一个元素是否出现在集合里。 比如把字符串映射为索引的例子： 通过哈希函数/hashCode将字符串转化为数值 如果得到的数值大于哈希表的大小了，取模，保证所有字符串映射到哈希表上。 如果字符串数量都大于哈希表的大小了，会出现同一索引不同字符串的情况。也称，哈希碰撞。 解决哈希碰撞： 拉链法 发生冲突的元素用链表存储 要选择适当的哈希表的大小，这样既不会因为数组空值而浪费大量内存，也不会因为链表太长而在查找上浪费太多时间。 线性探测法 保证tableSize大于dataSize，避免碰撞。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"有效的字母异位词 给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。 示例 1: 输入: s = “anagram”, t = “nagaram” 输出: true 示例 2: 输入: s = “rat”, t = “car” 输出: false 说明: 你可以假设字符串只包含小写字母。 func isAnagram(s string, t string) bool { if len(s)!=len(t){ return false } exists := make(map[byte]int) for i:=0;i\u003clen(s);i++{ if v,ok:=exists[s[i]];v\u003e=0\u0026\u0026ok{ exists[s[i]]=v+1 }else{ exists[s[i]]=1 } } for i:=0;i\u003clen(t);i++{ if v,ok:=exists[t[i]];v\u003e=1\u0026\u0026ok{ exists[t[i]]=v-1 }else{ return false } } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"两个数组的交集 使用数组来做哈希的题目，是因为题目都限制了数值的大小。 而这道题目没有限制数值的大小，就无法使用数组来做哈希表了。 func intersection(nums1 []int, nums2 []int) []int { m := make(map[int]int) for _, v := range nums1 { m[v] = 1 } var res []int // 利用count\u003e0，实现重复值只拿一次放入返回结果中 for _, v := range nums2 { if count, ok := m[v]; ok \u0026\u0026 count \u003e 0 { res = append(res, v) m[v]-- } } return res } //优化版，利用set，减少count统计 func intersection(nums1 []int, nums2 []int) []int { set:=make(map[int]struct{},0) res:=make([]int,0) for _,v:=range nums1{ if _,ok:=set[v];!ok{ set[v]=struct{}{} } } for _,v:=range nums2{ //如果存在于上一个数组中，则加入结果集，并清空该set值 if _,ok:=set[v];ok{ res=append(res,v) delete(set, v) } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"快乐数 编写一个算法来判断一个数 n 是不是快乐数。 「快乐数」定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是 无限循环 但始终变不到 1。如果 可以变为 1，那么这个数就是快乐数。 如果 n 是快乐数就返回 True ；不是，则返回 False 。 示例： 输入：19 输出：true 解释： 1^2 + 9^2 = 82 8^2 + 2^2 = 68 6^2 + 8^2 = 100 1^2 + 0^2 + 0^2 = 1 func isHappy(n int) bool { m := make(map[int]bool) for n != 1 \u0026\u0026 !m[n] { n, m[n] = getSum(n), true } return n == 1 } func getSum(n int) int { sum := 0 for n \u003e 0 { sum += (n % 10) * (n % 10) n = n / 10 } return sum } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"两数之和 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素不能使用两遍。 示例: 给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9 所以返回 [0, 1] func twoSum(nums []int, target int) []int { for k1, _ := range nums { for k2 := k1 + 1; k2 \u003c len(nums); k2++ { if target == nums[k1] + nums[k2] { return []int{k1, k2} } } } return []int{} } // 使用map方式解题，降低时间复杂度 func twoSum(nums []int, target int) []int { m := make(map[int]int) for index, val := range nums { if preIndex, ok := m[target-val]; ok { return []int{preIndex, index} } else { m[val] = index } } return []int{} } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"四数相加II 给定四个包含整数的数组列表 A , B , C , D ,计算有多少个元组 (i, j, k, l) ，使得 A[i] + B[j] + C[k] + D[l] = 0。 为了使问题简单化，所有的 A, B, C, D 具有相同的长度 N，且 0 ≤ N ≤ 500 。所有整数的范围在 -2^28 到 2^28 - 1 之间，最终结果不会超过 2^31 - 1 。 例如: 输入: A = [ 1, 2] B = [-2,-1] C = [-1, 2] D = [ 0, 2] 输出: 2 解释: 两个元组如下: (0, 0, 0, 1) -\u003e A[0] + B[0] + C[0] + D[1] = 1 + (-2) + (-1) + 2 = 0 (1, 1, 0, 0) -\u003e A[1] + B[1] + C[0] + D[0] = 2 + (-1) + (-1) + 0 = 0 func fourSumCount(nums1 []int, nums2 []int, nums3 []int, nums4 []int) int { m := make(map[int]int) count := 0 for _, v1 := range nums1 { for _, v2 := range nums2 { m[v1+v2]++ } } for _, v3 := range nums3 { for _, v4 := range nums4 { count += m[-v3-v4] } } return count } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"赎金信 给定一个赎金信 (ransom) 字符串和一个杂志(magazine)字符串，判断第一个字符串 ransom 能不能由第二个字符串 magazines 里面的字符构成。如果可以构成，返回 true ；否则返回 false。 (题目说明：为了不暴露赎金信字迹，要从杂志上搜索各个需要的字母，组成单词来表达意思。杂志字符串中的每个字符只能在赎金信字符串中使用一次。) 注意： 你可以假设两个字符串均只含有小写字母。 canConstruct(“a”, “b”) -\u003e false canConstruct(“aa”, “ab”) -\u003e false canConstruct(“aa”, “aab”) -\u003e true func canConstruct(ransomNote string, magazine string) bool { record := make([]int, 26) for _, v := range magazine { record[v-'a']++ } for _, v := range ransomNote { record[v-'a']-- if record[v-'a'] \u003c 0 { return false } } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:7","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"三数之和 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有满足条件且不重复的三元组。 注意： 答案中不可以包含重复的三元组。 示例： 给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ] 解法：哈希、双指针（更高效） func threeSum(nums []int)[][]int{ sort.Ints(nums) res:=[][]int{} for i:=0;i\u003clen(nums)-2;i++{ n1:=nums[i] if n1\u003e0{ break } if i\u003e0\u0026\u0026n1==nums[i-1]{ continue } l,r:=i+1,len(nums)-1 for l\u003cr{ n2,n3:=nums[l],nums[r] if n1+n2+n3==0{ res=append(res,[]int{n1,n2,n3}) for l\u003cr\u0026\u0026nums[l]==n2{ l++ } for l\u003cr\u0026\u0026nums[r]==n3{ r-- } }else if n1+n2+n3\u003c0{ l++ }else { r-- } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:8","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"四数之和 题意：给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组。 注意： 答案中不可以包含重复的四元组。 示例： 给定数组 nums = [1, 0, -1, 0, -2, 2]，和 target = 0。 满足要求的四元组集合为： [ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2] ] func fourSum(nums []int, target int) [][]int { if len(nums) \u003c 4 { return nil } sort.Ints(nums) var res [][]int for i := 0; i \u003c len(nums)-3; i++ { n1 := nums[i] // if n1 \u003e target { // 不能这样写,因为可能是负数 // break // } if i \u003e 0 \u0026\u0026 n1 == nums[i-1] { continue } for j := i + 1; j \u003c len(nums)-2; j++ { n2 := nums[j] if j \u003e i+1 \u0026\u0026 n2 == nums[j-1] { continue } l := j + 1 r := len(nums) - 1 for l \u003c r { n3 := nums[l] n4 := nums[r] sum := n1 + n2 + n3 + n4 if sum \u003c target { l++ } else if sum \u003e target { r-- } else { res = append(res, []int{n1, n2, n3, n4}) for l \u003c r \u0026\u0026 n3 == nums[l+1] { // 去重 l++ } for l \u003c r \u0026\u0026 n4 == nums[r-1] { // 去重 r-- } // 找到答案时,双指针同时靠近 r-- l++ } } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:3:9","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"字符串 库函数是工具，基础更重要。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:4:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"反转字符串 I 和反转链表相同，都用双指针法。 func reverseString(s []byte) { left:=0 right:=len(s)-1 for left\u003cright{ s[left],s[right]=s[right],s[left] left++ right-- } } II 给定一个字符串 s 和一个整数 k，你需要对从字符串开头算起的每隔 2k 个字符的前 k 个字符进行反转。 如果剩余字符少于 k 个，则将剩余字符全部反转。 如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。 示例: 输入: s = \"abcdefg\", k = 2 输出: \"bacdfeg\" func reverseStr(s string, k int) string { ss := []byte(s) length := len(s) for i := 0; i \u003c length; i += 2 * k { if i + k \u003c= length { reverse(ss[i:i+k]) } else { reverse(ss[i:length]) } } return string(ss) } func reverse(b []byte) { left := 0 right := len(b) - 1 for left \u003c right { b[left], b[right] = b[right], b[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:4:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"替换空格 请实现一个函数，把字符串 s 中的每个空格替换成\"%20\"。 示例 1： 输入：s = “We are happy.” 输出：“We%20are%20happy.” // 遍历添加 func replaceSpace(s string) string { b := []byte(s) result := make([]byte, 0) for i := 0; i \u003c len(b); i++ { if b[i] == ' ' { result = append(result, []byte(\"%20\")...) } else { result = append(result, b[i]) } } return string(result) } // 原地修改 func replaceSpace(s string) string { b := []byte(s) length := len(b) spaceCount := 0 // 计算空格数量 for _, v := range b { if v == ' ' { spaceCount++ } } // 扩展原有切片 resizeCount := spaceCount * 2 tmp := make([]byte, resizeCount) b = append(b, tmp...) i := length - 1 j := len(b) - 1 for i \u003e= 0 { if b[i] != ' ' { b[j] = b[i] i-- j-- } else { b[j] = '0' b[j-1] = '2' b[j-2] = '%' i-- j = j - 3 } } return string(b) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:4:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"翻转字符串里的单词 给定一个字符串，逐个翻转字符串中的每个单词。 输入: \"the sky is blue\" 输出: \"blue is sky the\" import ( \"fmt\" ) func reverseWords(s string) string { //1.使用双指针删除冗余的空格 slowIndex, fastIndex := 0, 0 b := []byte(s) //删除头部冗余空格 for len(b) \u003e 0 \u0026\u0026 fastIndex \u003c len(b) \u0026\u0026 b[fastIndex] == ' ' { fastIndex++ } //删除单词间冗余空格 for ; fastIndex \u003c len(b); fastIndex++ { if fastIndex-1 \u003e 0 \u0026\u0026 b[fastIndex-1] == b[fastIndex] \u0026\u0026 b[fastIndex] == ' ' { continue } b[slowIndex] = b[fastIndex] slowIndex++ } //删除尾部冗余空格 if slowIndex-1 \u003e 0 \u0026\u0026 b[slowIndex-1] == ' ' { b = b[:slowIndex-1] } else { b = b[:slowIndex] } //2.反转整个字符串 reverse(\u0026b, 0, len(b)-1) //3.反转单个单词 i单词开始位置，j单词结束位置 i := 0 for i \u003c len(b) { j := i for ; j \u003c len(b) \u0026\u0026 b[j] != ' '; j++ { } reverse(\u0026b, i, j-1) i = j i++ } return string(b) } func reverse(b *[]byte, left, right int) { for left \u003c right { (*b)[left], (*b)[right] = (*b)[right], (*b)[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:4:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"左旋转字符串 输入: s = \"abcdefg\", k = 2 输出: \"cdefgab\" func reverseLeftWords(s string, n int) string { b := []byte(s) // 1. 反转前n个字符 // 2. 反转第n到end字符 // 3. 反转整个字符 reverse(b, 0, n-1) reverse(b, n, len(b)-1) reverse(b, 0, len(b)-1) return string(b) } // 切片是引用传递 func reverse(b []byte, left, right int){ for left \u003c right{ b[left], b[right] = b[right],b[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:4:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"实现strStr() 给定一个 haystack 字符串和一个 needle 字符串，在 haystack 字符串中找出 needle 字符串出现的第一个位置 (从0开始)。如果不存在，则返回 -1。 示例 1: 输入: haystack = “hello”, needle = “ll” 输出: 2 示例 2: 输入: haystack = “aaaaa”, needle = “bba” 输出: -1 说明: 当 needle 是空字符串时，我们应当返回什么值呢？这是一个在面试中很好的问题。 对于本题而言，当 needle 是空字符串时我们应当返回 0 。这与C语言的 strstr() 以及 Java的 indexOf() 定义相符。 什么是KMP KMP算法是一种改进的字符串匹配算法，由D.E.Knuth，J.H.Morris和V.R.Pratt提出的，因此人们称它为克努特—莫里斯—普拉特操作（简称KMP算法）。KMP算法的核心是利用匹配失败后的信息，尽量减少模式串与主串的匹配次数以达到快速匹配的目的。具体实现就是通过一个next()函数实现，函数本身包含了模式串的局部匹配信息。KMP算法的时间复杂度O(m+n) [1] 。（来自百度百科） KMP的经典思想就是:当出现字符串不匹配时，可以记录一部分之前已经匹配的文本内容，利用这些信息避免从头再去做匹配。 如何记录已经匹配的文本内容，是KMP的重点，也是next数组的任务。 什么是前缀表与next数组 next数组就是一个前缀表。 前缀表是用来回退的，它记录了模式串与主串(文本串)不匹配的时候，模式串应该从哪里开始重新匹配。 什么是前缀表：记录下标i之前（包括i）的字符串中，有多大长度的相同前缀后缀。 最长公共前后缀：字符串aa的最长相等前后缀为1。 字符串aaa的最长相等前后缀为2。 等等….. 前缀表要求的就是最长相同前后缀的长度。它能告诉我们上次匹配的位置。 下标5之前这部分的字符串（也就是字符串aabaa）的最长相等的前缀和后缀字符串是子字符串aa ，因为找到了最长相等的前缀和后缀，匹配失败的位置是后缀子串的后面，那么我们找到与其相同的前缀的后面从新匹配就可以了。 前缀表与next数组有什么关系： next数组即可以就是前缀表，也可以是前缀表统一减一（右移一位，初始位置为-1）。 前缀表统一减一之后的next数组： 时间复杂度分析： 其中n为文本串长度，m为模式串长度，因为在匹配的过程中，根据前缀表不断调整匹配的位置，可以看出匹配的过程是$O(n)$，之前还要单独生成next数组，时间复杂度是$O(m)$。所以整个KMP算法的时间复杂度是$O(n+m)$的。 暴力的解法显而易见是$O(n × m)$，所以KMP在字符串匹配中极大的提高的搜索的效率。 为了和力扣题目28.实现strStr保持一致，方便大家理解，以下文章统称haystack为文本串, needle为模式串。 都知道使用KMP算法，一定要构造next数组 构造next数组： 我们定义一个函数getNext来构建next数组，函数参数为指向next数组的指针，和一个字符串。 代码如下： void getNext(int* next, const string\u0026 s) 构造next数组其实就是计算模式串s，前缀表的过程。 主要有如下三步： 初始化 定义两个指针i和j，j指向前缀起始位置，i指向后缀起始位置。 然后还要对next数组进行初始化赋值，如下： int j = -1; next[0] = j; j 为什么要初始化为 -1呢，因为之前说过 前缀表要统一减一的操作仅仅是其中的一种实现，我们这里选择j初始化为-1，下文我还会给出j不初始化为-1的实现代码。 next[i] 表示 i（包括i）之前最长相等的前后缀长度（其实就是j） 所以初始化next[0] = j 。 处理前后缀不相同的情况 因为j初始化为-1，那么i就从1开始，进行s[i] 与 s[j+1]的比较。 所以遍历模式串s的循环下标i 要从 1开始，代码如下： for(int i = 1; i \u003c s.size(); i++) { 如果 s[i] 与 s[j+1]不相同，也就是遇到 前后缀末尾不相同的情况，就要向前回退。 怎么回退呢？ next[j]就是记录着j（包括j）之前的子串的相同前后缀的长度。 那么 s[i] 与 s[j+1] 不相同，就要找 j+1前一个元素在next数组里的值（就是next[j]）。 所以，处理前后缀不相同的情况代码如下： while (j \u003e= 0 \u0026\u0026 s[i] != s[j + 1]) { // 前后缀不相同了 j = next[j]; // 向前回退 } 处理前后缀相同的情况 如果s[i] 与 s[j + 1] 相同，那么就同时向后移动i 和j 说明找到了相同的前后缀，同时还要将j（前缀的长度）赋给next[i], 因为next[i]要记录相同前后缀的长度。 代码如下： if (s[i] == s[j + 1]) { // 找到相同的前后缀 j++; } next[i] = j; 最后整体构建next数组的函数代码如下： void getNext(int* next, const string\u0026 s){ int j = -1; next[0] = j; for(int i = 1; i \u003c s.size(); i++) { // **注意**i从1开始 while (j \u003e= 0 \u0026\u0026 s[i] != s[j + 1]) { // 前后缀不相同了 j = next[j]; // 向前回退 } if (s[i] == s[j + 1]) { // 找到相同的前后缀 j++; } next[i] = j; // 将j（前缀的长度）赋给next[i] } } 使用next数组进行匹配 在文本串s里 找是否出现过模式串t。 定义两个下标j 指向模式串起始位置，i指向文本串起始位置。 那么j初始值依然为-1，为什么呢？ 依然因为next数组里记录的起始位置为-1。 i就从0开始，遍历文本串，代码如下： for (int i = 0; i \u003c s.size(); i++) 接下来就是 s[i] 与 t[j + 1] （因为j从-1开始的） 进行比较。 如果 s[i] 与 t[j + 1] 不相同，j就要从next数组里寻找下一个匹配的位置。 代码如下： while(j \u003e= 0 \u0026\u0026 s[i] != t[j + 1]) { j = next[j]; } 如果 s[i] 与 t[j + 1] 相同，那么i 和 j 同时向后移动， 代码如下： if (s[i] == t[j + 1]) { j++; // i的增加在for循环里 } 如何判断在文本串s里出现了模式串t呢，如果j指向了模式串t的末尾，那么就说明模式串t完全匹配文本串s里的某个子串了。 本题要在文本串字符串中找出模式串出现的第一个位置 (从0开始)，所以返回当前在文本串匹配模式串的位置i 减去 模式串的长度，就是文本串字符串中出现模式串的第一个位置。 代码如下： if (j == (t.size() - 1) ) { return (i - t.size() + 1); } 那么使用next数组，用模式串匹配文本串的整体代码如下： int j = -1; // 因为next数组里记录的起始位置为-1 for (int i = 0; i \u003c s.size(); i++) { // **注意**i就从0开始 while(j \u003e= 0 \u0026\u0026 s[i] != t[j + 1]) { // 不匹配 j = next[j]; // j 寻找之前匹配的位置 } if (s[i] == t[j + 1]) { // 匹配，j和i同时向后移动 j++; // i的增加在for循环里 } if (j == (t.size() - 1) ) { // 文本串s里出现了模式串t return (i - t.size() + 1); } } 此时所有逻辑的代码都已经写出来了，力扣 28.实现strStr 题目的整体代码如下： class Solution { public: void getNext(int* next, const string\u0026 s) { int j = -1; next[0] = j; for(int i = 1; i \u003c s.size(); i++) { // **注意**i从1开始 while (j \u003e= 0 \u0026\u0026 s[i] != s[j + 1]) { // 前后缀不相同了 j = next[j]; // 向前回退 } if (s[i] == s[j + 1]) { // 找到相同的前后缀 j++; } next[i] = j; // 将j（前缀的长度）赋给next[i] } } int strStr(string haystack, string needle) { if (needle.size() == 0) { return 0; } int next[needle.size()]; getNext(next, needle); int j = -1; // // 因为next数组里记录的起始位置为-1 for (int i = 0; i \u003c haystack.size(); i++) { // **注意**i就从0开始 while(j \u003e= 0 \u0026\u0026 haystack[i] != needle[j + 1]) { // 不匹配 j = next[j]; // j 寻找之前匹配","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:4:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"重复的子字符串 给定一个非空的字符串，判断它是否可以由它的一个子串重复多次构成。给定的字符串只含有小写英文字母，并且长度不超过10000。 示例 1: 输入: \"abab\" 输出: True 解释: 可由子字符串 \"ab\" 重复两次构成。 示例 2: 输入: \"aba\" 输出: False 示例 3: 输入: \"abcabcabcabc\" 输出: True 解释: 可由子字符串 \"abc\" 重复四次构成。 (或者子字符串 \"abcabc\" 重复两次构成。) 标准的KMP题目~ 数组长度减去最长相同前后缀的长度相当于是第一个周期的长度，也就是一个周期的长度，如果这个周期可以被整除，就说明整个数组就是这个周期的循环。 强烈建议大家把next数组打印出来，看看next数组里的规律，有助于理解KMP算法 代码实现： //前缀表统一减一的实现 func repeatedSubstringPattern(s string) bool { n := len(s) if n == 0 { return false } next := make([]int, n) j := -1 next[0] = j for i := 1; i \u003c n; i++ { for j \u003e= 0 \u0026\u0026 s[i] != s[j+1] { j = next[j] } if s[i] == s[j+1] { j++ } next[i] = j } // next[n-1]+1 最长相同前后缀的长度 if next[n-1] != -1 \u0026\u0026 n%(n-(next[n-1]+1)) == 0 { return true } return false } //前缀表不减一 func repeatedSubstringPattern(s string) bool { n := len(s) if n == 0 { return false } j := 0 next := make([]int, n) next[0] = j for i := 1; i \u003c n; i++ { for j \u003e 0 \u0026\u0026 s[i] != s[j] { j = next[j-1] } if s[i] == s[j] { j++ } next[i] = j } // next[n-1] 最长相同前后缀的长度 if next[n-1] != 0 \u0026\u0026 n%(n-next[n-1]) == 0 { return true } return false } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:4:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"双指针法 双指针法（快慢指针法）在数组和链表的操作中是非常常见的，很多考察数组、链表、字符串等操作的面试题，都使用双指针法。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"移除元素 给你一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，并返回移除后数组的新长度。 不要使用额外的数组空间，你必须仅使用 $O(1)$ 额外空间并原地修改输入数组。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 示例 1: 给定 nums = [3,2,2,3], val = 3, 函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 给定 nums = [0,1,2,2,3,0,4,2], val = 2, 函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。 你不需要考虑数组中超出新长度后面的元素。 func removeElement(nums []int, val int) int { length:=len(nums) res:=0 for i:=0;i\u003clength;i++{ if nums[i]!=val { nums[res]=nums[i] res++ } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"反转字符串 func reverseString(s []byte) { left:=0 right:=len(s)-1 for left\u003cright{ s[left],s[right]=s[right],s[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"替换空格 // 遍历添加 func replaceSpace(s string) string { b := []byte(s) result := make([]byte, 0) for i := 0; i \u003c len(b); i++ { if b[i] == ' ' { result = append(result, []byte(\"%20\")...) } else { result = append(result, b[i]) } } return string(result) } // 原地修改 func replaceSpace(s string) string { b := []byte(s) length := len(b) spaceCount := 0 // 计算空格数量 for _, v := range b { if v == ' ' { spaceCount++ } } // 扩展原有切片 resizeCount := spaceCount * 2 tmp := make([]byte, resizeCount) b = append(b, tmp...) i := length - 1 j := len(b) - 1 for i \u003e= 0 { if b[i] != ' ' { b[j] = b[i] i-- j-- } else { b[j] = '0' b[j-1] = '2' b[j-2] = '%' i-- j = j - 3 } } return string(b) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"翻转字符串里的单词 import ( \"fmt\" ) func reverseWords(s string) string { //1.使用双指针删除冗余的空格 slowIndex, fastIndex := 0, 0 b := []byte(s) //删除头部冗余空格 for len(b) \u003e 0 \u0026\u0026 fastIndex \u003c len(b) \u0026\u0026 b[fastIndex] == ' ' { fastIndex++ } //删除单词间冗余空格 for ; fastIndex \u003c len(b); fastIndex++ { if fastIndex-1 \u003e 0 \u0026\u0026 b[fastIndex-1] == b[fastIndex] \u0026\u0026 b[fastIndex] == ' ' { continue } b[slowIndex] = b[fastIndex] slowIndex++ } //删除尾部冗余空格 if slowIndex-1 \u003e 0 \u0026\u0026 b[slowIndex-1] == ' ' { b = b[:slowIndex-1] } else { b = b[:slowIndex] } //2.反转整个字符串 reverse(\u0026b, 0, len(b)-1) //3.反转单个单词 i单词开始位置，j单词结束位置 i := 0 for i \u003c len(b) { j := i for ; j \u003c len(b) \u0026\u0026 b[j] != ' '; j++ { } reverse(\u0026b, i, j-1) i = j i++ } return string(b) } func reverse(b *[]byte, left, right int) { for left \u003c right { (*b)[left], (*b)[right] = (*b)[right], (*b)[left] left++ right-- } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"翻转链表 //双指针 func reverseList(head *ListNode) *ListNode { var pre *ListNode cur := head for cur != nil { next := cur.Next cur.Next = pre pre = cur cur = next } return pre } //递归 func reverseList(head *ListNode) *ListNode { return help(nil, head) } func help(pre, head *ListNode)*ListNode{ if head == nil { return pre } next := head.Next head.Next = pre return help(head, next) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"删除链表的倒数第N个节点 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { dummyHead := \u0026ListNode{} dummyHead.Next = head cur := head prev := dummyHead i := 1 for cur != nil { cur = cur.Next if i \u003e n { prev = prev.Next } i++ } prev.Next = prev.Next.Next return dummyHead.Next } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"链表相交 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 null 。 func getIntersectionNode(headA, headB *ListNode) *ListNode { curA := headA curB := headB lenA, lenB := 0, 0 // 求A，B的长度 for curA != nil { curA = curA.Next lenA++ } for curB != nil { curB = curB.Next lenB++ } var step int var fast, slow *ListNode // 请求长度差，并且让更长的链表先走相差的长度 if lenA \u003e lenB { step = lenA - lenB fast, slow = headA, headB } else { step = lenB - lenA fast, slow = headB, headA } for i:=0; i \u003c step; i++ { fast = fast.Next } // 遍历两个链表遇到相同则跳出遍历 for fast != slow { fast = fast.Next slow = slow.Next } return fast } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:7","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"环形链表II 题意： 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 func detectCycle(head *ListNode) *ListNode { slow, fast := head, head for fast != nil \u0026\u0026 fast.Next != nil { slow = slow.Next fast = fast.Next.Next if slow == fast { for slow != head { slow = slow.Next head = head.Next } return head } } return nil } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:8","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"三数之和 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有满足条件且不重复的三元组。 注意： 答案中不可以包含重复的三元组。 示例： 给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ] func threeSum(nums []int)[][]int{ sort.Ints(nums) res:=[][]int{} for i:=0;i\u003clen(nums)-2;i++{ n1:=nums[i] if n1\u003e0{ break } if i\u003e0\u0026\u0026n1==nums[i-1]{ continue } l,r:=i+1,len(nums)-1 for l\u003cr{ n2,n3:=nums[l],nums[r] if n1+n2+n3==0{ res=append(res,[]int{n1,n2,n3}) for l\u003cr\u0026\u0026nums[l]==n2{ l++ } for l\u003cr\u0026\u0026nums[r]==n3{ r-- } }else if n1+n2+n3\u003c0{ l++ }else { r-- } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:9","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"四数之和 题意：给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组。 注意： 答案中不可以包含重复的四元组。 示例： 给定数组 nums = [1, 0, -1, 0, -2, 2]，和 target = 0。 满足要求的四元组集合为： [ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2] ] func fourSum(nums []int, target int) [][]int { if len(nums) \u003c 4 { return nil } sort.Ints(nums) var res [][]int for i := 0; i \u003c len(nums)-3; i++ { n1 := nums[i] // if n1 \u003e target { // 不能这样写,因为可能是负数 // break // } if i \u003e 0 \u0026\u0026 n1 == nums[i-1] { continue } for j := i + 1; j \u003c len(nums)-2; j++ { n2 := nums[j] if j \u003e i+1 \u0026\u0026 n2 == nums[j-1] { continue } l := j + 1 r := len(nums) - 1 for l \u003c r { n3 := nums[l] n4 := nums[r] sum := n1 + n2 + n3 + n4 if sum \u003c target { l++ } else if sum \u003e target { r-- } else { res = append(res, []int{n1, n2, n3, n4}) for l \u003c r \u0026\u0026 n3 == nums[l+1] { // 去重 l++ } for l \u003c r \u0026\u0026 n4 == nums[r-1] { // 去重 r-- } // 找到答案时,双指针同时靠近 r-- l++ } } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:10","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"双指针总结 总结 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:5:11","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"栈和队列 需要知道栈和队列的底层实现，不同编程语言不同STL的实现原理都是不尽相同的。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:6:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"理论基础(c++) 栈其实就是递归的一种实现结构。 栈（本身可以说是一个容器适配器）是以底层容器（数组或者链表）完成其所有的工作，对外提供统一的接口，底层容器是可插拔的（也就是说我们可以控制使用哪种容器来实现栈的功能）。 SGI -- Silicon Graphics [Computer System] Inc. 硅图[计算机系统] 公司. STL -- Standard Template Library 标准模板库。 SGI STL -- SGI的标准模板库。 SGI的全称 -- 硅图[计算机系统] 公司。 我们常用的SGI STL，如果没有指定底层实现的话，默认是以deque为缺省情况下栈的低层结构。 deque是一个双向队列，只要封住一段，只开通另一端就可以实现栈的逻辑了。 SGI STL中 队列底层实现缺省情况下一样使用deque实现的。 我们也可以指定vector为栈的底层实现，初始化语句如下： std::stack\u003cint, std::vector\u003cint\u003e \u003e third; // 使用vector为底层容器的栈 对应的队列的情况是一样的。 队列中先进先出的数据结构，同样不允许有遍历行为，不提供迭代器, SGI STL中队列一样是以deque为缺省情况下的底部结构。 也可以指定list 为起底层实现，初始化queue的语句如下： std::queue\u003cint, std::list\u003cint\u003e\u003e third; // 定义以list为底层容器的队列 所以STL 队列也不被归类为容器，而被归类为container adapter（ 容器适配器）。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:6:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"用栈实现队列 使用栈实现队列的下列操作： push(x) – 将一个元素放入队列的尾部。 pop() – 从队列首部移除元素。 peek() – 返回队列首部的元素。 empty() – 返回队列是否为空。 示例: MyQueue queue = new MyQueue(); queue.push(1); queue.push(2); queue.peek(); // 返回 1 queue.pop(); // 返回 1 queue.empty(); // 返回 false 说明: 你只能使用标准的栈操作 – 也就是只有 push to top, peek/pop from top, size, 和 is empty 操作是合法的。 你所使用的语言也许不支持栈。你可以使用 list 或者 deque（双端队列）来模拟一个栈，只要是标准的栈操作即可。 假设所有操作都是有效的 （例如，一个空的队列不会调用 pop 或者 peek 操作） type MyQueue struct { stack []int back []int } /** Initialize your data structure here. */ func Constructor() MyQueue { return MyQueue{ stack: make([]int, 0), back: make([]int, 0), } } /** Push element x to the back of queue. */ func (this *MyQueue) Push(x int) { for len(this.back) != 0 { val := this.back[len(this.back)-1] this.back = this.back[:len(this.back)-1] this.stack = append(this.stack, val) } this.stack = append(this.stack, x) } /** Removes the element from in front of queue and returns that element. */ func (this *MyQueue) Pop() int { for len(this.stack) != 0 { val := this.stack[len(this.stack)-1] this.stack = this.stack[:len(this.stack)-1] this.back = append(this.back, val) } if len(this.back) == 0 { return 0 } val := this.back[len(this.back)-1] this.back = this.back[:len(this.back)-1] return val } /** Get the front element. */ func (this *MyQueue) Peek() int { for len(this.stack) != 0 { val := this.stack[len(this.stack)-1] this.stack = this.stack[:len(this.stack)-1] this.back = append(this.back, val) } if len(this.back) == 0 { return 0 } val := this.back[len(this.back)-1] return val } /** Returns whether the queue is empty. */ func (this *MyQueue) Empty() bool { return len(this.stack) == 0 \u0026\u0026 len(this.back) == 0 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:6:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"用队列实现栈 使用队列实现栈的下列操作： push(x) -- 元素 x 入栈 pop() -- 移除栈顶元素 top() -- 获取栈顶元素 empty() -- 返回栈是否为空 注意: 你只能使用队列的基本操作– 也就是 push to back, peek/pop from front, size, 和 is empty 这些操作是合法的。 你所使用的语言也许不支持队列。 你可以使用 list 或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。 你可以假设所有操作都是有效的（例如, 对一个空的栈不会调用 pop 或者 top 操作） 用两个队列实现： type MyStack struct { //创建两个队列 queue1 []int queue2 []int } func Constructor() MyStack { return MyStack{ //初始化 queue1:make([]int,0), queue2:make([]int,0), } } func (this *MyStack) Push(x int) { //先将数据存在queue2中 this.queue2 = append(this.queue2,x) //将queue1中所有元素移到queue2中，再将两个队列进行交换 this.Move() } func (this *MyStack) Move(){ if len(this.queue1) == 0{ //交换，queue1置为queue2,queue2置为空 this.queue1,this.queue2 = this.queue2,this.queue1 }else{ //queue1元素从头开始一个一个追加到queue2中 this.queue2 = append(this.queue2,this.queue1[0]) this.queue1 = this.queue1[1:] //去除第一个元素 this.Move() //重复 } } func (this *MyStack) Pop() int { val := this.queue1[0] this.queue1 = this.queue1[1:] //去除第一个元素 return val } func (this *MyStack) Top() int { return this.queue1[0] //直接返回 } func (this *MyStack) Empty() bool { return len(this.queue1) == 0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ 用一个队列实现： type MyStack struct { queue []int//创建一个队列 } /** Initialize your data structure here. */ func Constructor() MyStack { return MyStack{ //初始化 queue:make([]int,0), } } /** Push element x onto stack. */ func (this *MyStack) Push(x int) { //添加元素 this.queue=append(this.queue,x) } /** Removes the element on top of the stack and returns that element. */ func (this *MyStack) Pop() int { n:=len(this.queue)-1//判断长度 for n!=0{ //除了最后一个，其余的都重新添加到队列里 val:=this.queue[0] this.queue=this.queue[1:] this.queue=append(this.queue,val) n-- } //弹出元素 val:=this.queue[0] this.queue=this.queue[1:] return val } /** Get the top element. */ func (this *MyStack) Top() int { //利用Pop函数，弹出来的元素重新添加 val:=this.Pop() this.queue=append(this.queue,val) return val } /** Returns whether the stack is empty. */ func (this *MyStack) Empty() bool { return len(this.queue)==0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:6:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"有效的括号 给定一个只包括 ‘('，')'，'{'，'}'，'['，']’ 的字符串，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 注意空字符串可被认为是有效字符串。 栈的经典问题。 func isValid(s string) bool { hash := map[byte]byte{')':'(', ']':'[', '}':'{'} stack := make([]byte, 0) if s == \"\" { return true } for i := 0; i \u003c len(s); i++ { if s[i] == '(' || s[i] == '[' || s[i] == '{' { stack = append(stack, s[i]) } else if len(stack) \u003e 0 \u0026\u0026 stack[len(stack)-1] == hash[s[i]] { stack = stack[:len(stack)-1] } else { return false } } return len(stack) == 0 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:6:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"删除字符串中的所有相邻重复项 给出由小写字母组成的字符串 S，重复项删除操作会选择两个相邻且相同的字母，并删除它们。 在 S 上反复执行重复项删除操作，直到无法继续删除。 在完成所有重复项删除操作后返回最终的字符串。答案保证唯一。 示例： 输入：\"abbaca\" 输出：\"ca\" 解释：例如，在 \"abbaca\" 中，我们可以删除 \"bb\" 由于两字母相邻且相同，这是此时唯一可以执行删除操作的重复项。之后我们得到字符串 \"aaca\"，其中又只有 \"aa\" 可以执行重复项删除操作，所以最后的字符串为 \"ca\"。 提示： 1 \u003c= S.length \u003c= 20000 S 仅由小写英文字母组成 func removeDuplicates(s string) string { var stack []byte for i := 0; i \u003c len(s);i++ { // 栈不空 且 与栈顶元素不等 if len(stack) \u003e 0 \u0026\u0026 stack[len(stack)-1] == s[i] { // 弹出栈顶元素 并 忽略当前元素(s[i]) stack = stack[:len(stack)-1] }else{ // 入栈 stack = append(stack, s[i]) } } return string(stack) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:6:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"逆波兰表达式求值 根据 逆波兰表示法，求表达式的值。 有效的运算符包括 + , - , * , / 。每个运算对象可以是整数，也可以是另一个逆波兰表达式。 说明： 整数除法只保留整数部分。 给定逆波兰表达式总是有效的。换句话说，表达式总会得出有效数值且不存在除数为 0 的情况。 示例 1： 输入: [\"2\", \"1\", \"+\", \"3\", \" * \"] 输出: 9 解释: 该算式转化为常见的中缀算术表达式为：((2 + 1) * 3) = 9 逆波兰表达式：是一种后缀表达式，所谓后缀就是指算符写在后面。 平常使用的算式则是一种中缀表达式，如 ( 1 + 2 ) * ( 3 + 4 ) 。 该算式的逆波兰表达式写法为 ( ( 1 2 + ) ( 3 4 + ) * ) 。 逆波兰表达式主要有以下两个优点： 去掉括号后表达式无歧义，上式即便写成 1 2 + 3 4 + * 也可以依据次序计算出正确结果。 适合用栈操作运算：遇到数字则入栈；遇到算符则取出栈顶两个数字进行计算，并将结果压入栈中 func evalRPN(tokens []string) int { stack := []int{} for _, token := range tokens { val, err := strconv.Atoi(token) if err == nil { stack = append(stack, val) } else { num1, num2 := stack[len(stack)-2], stack[(len(stack))-1] stack = stack[:len(stack)-2] switch token { case \"+\": stack = append(stack, num1+num2) case \"-\": stack = append(stack, num1-num2) case \"*\": stack = append(stack, num1*num2) case \"/\": stack = append(stack, num1/num2) } } } return stack[0] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:6:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"滑动窗口最大值 给定一个数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。 每个窗口都有一个最大值，返回滑动窗口中的最大值数组。 进阶：你能在线性时间复杂度内解决此题吗？ // 封装单调队列的方式解题 type MyQueue struct { queue []int } func NewMyQueue() *MyQueue { return \u0026MyQueue{ queue: make([]int, 0), } } func (m *MyQueue) Front() int { return m.queue[0] } func (m *MyQueue) Back() int { return m.queue[len(m.queue)-1] } func (m *MyQueue) Empty() bool { return len(m.queue) == 0 } func (m *MyQueue) Push(val int) { for !m.Empty() \u0026\u0026 val \u003e m.Back() { m.queue = m.queue[:len(m.queue)-1] } m.queue = append(m.queue, val) } func (m *MyQueue) Pop(val int) { if !m.Empty() \u0026\u0026 val == m.Front() { m.queue = m.queue[1:] } } func maxSlidingWindow(nums []int, k int) []int { queue := NewMyQueue() length := len(nums) res := make([]int, 0) // 先将前k个元素放入队列 for i := 0; i \u003c k; i++ { queue.Push(nums[i]) } // 记录前k个元素的最大值 res = append(res, queue.Front()) for i := k; i \u003c length; i++ { // 滑动窗口移除最前面的元素 queue.Pop(nums[i-k]) // 滑动窗口添加最后面的元素 queue.Push(nums[i]) // 记录最大值 res = append(res, queue.Front()) } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:6:7","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"前K个高频元素 给定一个非空的整数数组，返回其中出现频率前 k 高的元素。 示例 1: 输入: nums = [1,1,1,2,2,3], k = 2 输出: [1,2] //方法一：小顶堆 func topKFrequent(nums []int, k int) []int { map_num:=map[int]int{} //记录每个元素出现的次数 for _,item:=range nums{ map_num[item]++ } h:=\u0026IHeap{} heap.Init(h) //所有元素入堆，堆的长度为k for key,value:=range map_num{ heap.Push(h,[2]int{key,value}) if h.Len()\u003ek{ heap.Pop(h) } } res:=make([]int,k) //按顺序返回堆中的元素 for i:=0;i\u003ck;i++{ res[k-i-1]=heap.Pop(h).([2]int)[0] } return res } //构建小顶堆 type IHeap [][2]int func (h IHeap) Len()int { return len(h) } func (h IHeap) Less (i,j int) bool { return h[i][1]\u003ch[j][1] } func (h IHeap) Swap(i,j int) { h[i],h[j]=h[j],h[i] } func (h *IHeap) Push(x interface{}){ *h=append(*h,x.([2]int)) } func (h *IHeap) Pop() interface{}{ old:=*h n:=len(old) x:=old[n-1] *h=old[0:n-1] return x } //方法二:利用O(logn)排序 func topKFrequent(nums []int, k int) []int { ans:=[]int{} map_num:=map[int]int{} for _,item:=range nums { map_num[item]++ } for key,_:=range map_num{ ans=append(ans,key) } //核心思想：排序 //可以不用包函数，自己实现快排 sort.Slice(ans,func (a,b int)bool{ return map_num[ans[a]]\u003emap_num[ans[b]] }) return ans[:k] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:6:8","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"二叉树 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"理论基础 一般主要会碰到满二叉树以及完全二叉树。 完全二叉树的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2^h -1 个节点。 优先级队列其实是一个堆，堆就是一棵完全二叉树，同时保证父子节点的顺序关系。 二叉搜索树： 与前面两个树不同，该树有节点权值。 有序树，左节点 \u003c 中节点 \u003c 右节点 平衡二叉搜索树：又被称为AVL（Adelson-Velsky and Landis）树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 二叉树可以链式存储，也可以顺序存储。 深度优先遍历 前序遍历（递归法，迭代法） 中序遍历（递归法，迭代法） 后序遍历（递归法，迭代法） 广度优先遍历 层次遍历（迭代法） type TreeNode struct { Val int Left *TreeNode Right *TreeNode } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"递归遍历 递归的实现就是：每一次递归调用都会把函数的局部变量、参数值和返回地址等压入调用栈中，然后递归返回的时候，从栈顶弹出上一次递归的各项参数，所以这就是递归为什么可以返回上一层位置的原因。 写递归还是得有方法论。 确定递归函数的参数和返回值： 确定哪些参数是递归的过程中需要处理的，那么就在递归函数里加上这个参数， 并且还要明确每次递归的返回值是什么进而确定递归函数的返回类型。 确定终止条件： 写完了递归算法, 运行的时候，经常会遇到栈溢出的错误，就是没写终止条件或者终止条件写的不对，操作系统也是用一个栈的结构来保存每一层递归的信息，如果递归没有终止，操作系统的内存栈必然就会溢出。 确定单层递归的逻辑： 确定每一层递归需要处理的信息。在这里也就会重复调用自己来实现递归的过程。 前序遍历： func preorderTraversal(root *TreeNode) (res []int) { var traversal func(node *TreeNode) traversal = func(node *TreeNode) { if node == nil { return } res = append(res,node.Val) traversal(node.Left) traversal(node.Right) } traversal(root) return res } func pre(r *TreeNode)(res []int){ } 中序遍历： func inorderTraversal(root *TreeNode) (res []int) { var traversal func(node *TreeNode) traversal = func(node *TreeNode) { if node == nil { return } traversal(node.Left) res = append(res,node.Val) traversal(node.Right) } traversal(root) return res } 后序遍历: func postorderTraversal(root *TreeNode) (res []int) { var traversal func(node *TreeNode) traversal = func(node *TreeNode) { if node == nil { return } traversal(node.Left) traversal(node.Right) res = append(res,node.Val) } traversal(root) return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"迭代遍历 迭代法前序遍历 func preorderTraversal(root *TreeNode) []int { ans := []int{} if root == nil { return ans } st := list.New() st.PushBack(root) for st.Len() \u003e 0 { node := st.Remove(st.Back()).(*TreeNode) ans = append(ans, node.Val) if node.Right != nil { st.PushBack(node.Right) } if node.Left != nil { st.PushBack(node.Left) } } return ans } func preorderTraver(root *TreeNode) []int { } 迭代法后序遍历 func postorderTraversal(root *TreeNode) []int { ans := []int{} if root == nil { return ans } st := list.New() st.PushBack(root) for st.Len() \u003e 0 { node := st.Remove(st.Back()).(*TreeNode) ans = append(ans, node.Val) if node.Left != nil { st.PushBack(node.Left) } if node.Right != nil { st.PushBack(node.Right) } } reverse(ans) return ans } func reverse(a []int) { l, r := 0, len(a) - 1 for l \u003c r { a[l], a[r] = a[r], a[l] l, r = l+1, r-1 } } 迭代法中序遍历 func inorderTraversal(root *TreeNode) []int { ans := []int{} if root == nil { return ans } st := list.New() cur := root for cur != nil || st.Len() \u003e 0 { if cur != nil { st.PushBack(cur) cur = cur.Left } else { cur = st.Remove(st.Back()).(*TreeNode) ans = append(ans, cur.Val) cur = cur.Right } } return ans } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"统一迭代 迭代法实现的先中后序，其实风格也不是那么统一，除了先序和后序，有关联，中序完全就是另一个风格了，一会用栈遍历，一会又用指针来遍历。 使用迭代法实现先中后序遍历，很难写出统一的代码，不像是递归法，实现了其中的一种遍历方式，其他两种只要稍稍改一下节点顺序就可以了。 要处理的节点放入栈之后，紧接着放入一个空指针作为标记。 这种方法也可以叫做标记法 前序遍历统一迭代法 /** type Element struct { // 元素保管的值 Value interface{} // 内含隐藏或非导出字段 } func (l *List) Back() *Element 前序遍历：中左右 压栈顺序：右左中 **/ func preorderTraversal(root *TreeNode) []int { if root == nil { return nil } var stack = list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u003e0{ e := stack.Back() stack.Remove(e)//弹出元素 if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：右左中 if node.Right!=nil{ stack.PushBack(node.Right) } if node.Left!=nil{ stack.PushBack(node.Left) } stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) } return res } 中序遍历统一迭代法 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ //中序遍历：左中右 //压栈顺序：右中左 func inorderTraversal(root *TreeNode) []int { if root==nil{ return nil } stack:=list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u003e0{ e := stack.Back() stack.Remove(e) if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：右中左 if node.Right!=nil{ stack.PushBack(node.Right) } stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) if node.Left!=nil{ stack.PushBack(node.Left) } } return res } 后序遍历统一迭代法 //后续遍历：左右中 //压栈顺序：中右左 func postorderTraversal(root *TreeNode) []int { if root == nil { return nil } var stack = list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u003e0{ e := stack.Back() stack.Remove(e) if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：中右左 stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) if node.Right!=nil{ stack.PushBack(node.Right) } if node.Left!=nil{ stack.PushBack(node.Left) } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"层序遍历 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"102.二叉树的层序遍历 https://leetcode-cn.com/problems/binary-tree-level-order-traversal/ /** 102. 二叉树的层序遍历 */ func levelOrder(root *TreeNode) [][]int { res:=[][]int{} if root==nil{//防止为空 return res } queue:=list.New() queue.PushBack(root) var tmpArr []int for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node.Val)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]int{}//清空层的数据 } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"107.二叉树的层次遍历II https://leetcode-cn.com/problems/binary-tree-level-order-traversal-ii/ /** 107. 二叉树的层序遍历 II */ func levelOrderBottom(root *TreeNode) [][]int { queue:=list.New() res:=[][]int{} if root==nil{ return res } queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() tmp:=[]int{} for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmp=append(tmp,node.Val) } res=append(res,tmp) } //反转结果集 for i:=0;i\u003clen(res)/2;i++{ res[i],res[len(res)-i-1]=res[len(res)-i-1],res[i] } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:7","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"199.二叉树的右视图 https://leetcode-cn.com/problems/binary-tree-right-side-view/ func rightSideView(root *TreeNode) []int { queue:=list.New() res:=[][]int{} var finaRes []int if root==nil{ return finaRes } queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() tmp:=[]int{} for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmp=append(tmp,node.Val) } res=append(res,tmp) } //取每一层的最后一个元素 for i:=0;i\u003clen(res);i++{ finaRes=append(finaRes,res[i][len(res[i])-1]) } return finaRes } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:8","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"637.二叉树的层平均值 https://leetcode-cn.com/problems/average-of-levels-in-binary-tree/ /** 637. 二叉树的层平均值 */ func averageOfLevels(root *TreeNode) []float64 { res:=[][]int{} var finRes []float64 if root==nil{//防止为空 return finRes } queue:=list.New() queue.PushBack(root) var tmpArr []int for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node.Val)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]int{}//清空层的数据 } //计算每层的平均值 length:=len(res) for i:=0;i\u003clength;i++{ var sum int for j:=0;j\u003clen(res[i]);j++{ sum+=res[i][j] } tmp:=float64(sum)/float64(len(res[i])) finRes=append(finRes,tmp)//将平均值放入结果集合 } return finRes } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:9","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"429.N叉树的层序遍历 https://leetcode-cn.com/problems/n-ary-tree-level-order-traversal/ func levelOrder(root *Node) [][]int { queue:=list.New() res:=[][]int{}//结果集 if root==nil{ return res } queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len()//记录当前层的数量 var tmp []int for T:=0;T\u003clength;T++{//该层的每个元素：一添加到该层的结果集中；二找到该元素的下层元素加入到队列中，方便下次使用 myNode:=queue.Remove(queue.Front()).(*Node) tmp=append(tmp,myNode.Val) for i:=0;i\u003clen(myNode.Children);i++{ queue.PushBack(myNode.Children[i]) } } res=append(res,tmp) } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:10","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"515.在每个树行中找最大值 https://leetcode-cn.com/problems/find-largest-value-in-each-tree-row/ /** 515. 在每个树行中找最大值 */ func largestValues(root *TreeNode) []int { res:=[][]int{} var finRes []int if root==nil{//防止为空 return finRes } queue:=list.New() queue.PushBack(root) var tmpArr []int //层次遍历 for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node.Val)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]int{}//清空层的数据 } //找到每层的最大值 for i:=0;i\u003clen(res);i++{ finRes=append(finRes,max(res[i]...)) } return finRes } func max(vals...int) int { max:=int(math.Inf(-1))//负无穷 for _, val := range vals { if val \u003e max { max = val } } return max } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:11","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"116.填充每个节点的下一个右侧节点指针 https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node/ /** 116. 填充每个节点的下一个右侧节点指针 117. 填充每个节点的下一个右侧节点指针 II */ func connect(root *Node) *Node { res:=[][]*Node{} if root==nil{//防止为空 return root } queue:=list.New() queue.PushBack(root) var tmpArr []*Node for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*Node)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]*Node{}//清空层的数据 } //遍历每层元素,指定next for i:=0;i\u003clen(res);i++{ for j:=0;j\u003clen(res[i])-1;j++{ res[i][j].Next=res[i][j+1] } } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:12","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"117.填充每个节点的下一个右侧节点指针II https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node-ii/ /** 116. 填充每个节点的下一个右侧节点指针 117. 填充每个节点的下一个右侧节点指针 II */ func connect(root *Node) *Node { res:=[][]*Node{} if root==nil{//防止为空 return root } queue:=list.New() queue.PushBack(root) var tmpArr []*Node for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*Node)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]*Node{}//清空层的数据 } //遍历每层元素,指定next for i:=0;i\u003clen(res);i++{ for j:=0;j\u003clen(res[i])-1;j++{ res[i][j].Next=res[i][j+1] } } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:13","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"104.二叉树的最大深度 https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/ /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func maxDepth(root *TreeNode) int { ans:=0 if root==nil{ return 0 } queue:=list.New() queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } ans++//记录深度，其他的是层序遍历的板子 } return ans } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:14","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"111.二叉树的最小深度 https://leetcode-cn.com/problems/minimum-depth-of-binary-tree/ go： /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func minDepth(root *TreeNode) int { ans:=0 if root==nil{ return 0 } queue:=list.New() queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left==nil\u0026\u0026node.Right==nil{//当前节点没有左右节点，则代表此层是最小层 return ans+1//返回当前层 ans代表是上一层 } if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } ans++//记录层数 } return ans+1 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:15","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"翻转二叉树 二叉树，当然是左右翻转。 递归版本的前序遍历 func invertTree(root *TreeNode) *TreeNode { if root ==nil{ return nil } temp:=root.Left root.Left=root.Right root.Right=temp invertTree(root.Left) invertTree(root.Right) return root } 递归版本的后序遍历 func invertTree(root *TreeNode) *TreeNode { if root==nil{ return root } invertTree(root.Left)//遍历左节点 invertTree(root.Right)//遍历右节点 root.Left,root.Right=root.Right,root.Left//交换 return root } 迭代版本的前序遍历 func invertTree(root *TreeNode) *TreeNode { stack:=[]*TreeNode{} node:=root for node!=nil||len(stack)\u003e0{ for node!=nil{ node.Left,node.Right=node.Right,node.Left//交换 stack=append(stack,node) node=node.Left } node=stack[len(stack)-1] stack=stack[:len(stack)-1] node=node.Right } return root } 迭代版本的后序遍历 func invertTree(root *TreeNode) *TreeNode { stack:=[]*TreeNode{} node:=root var prev *TreeNode for node!=nil||len(stack)\u003e0{ for node!=nil{ stack=append(stack,node) node=node.Left } node=stack[len(stack)-1] stack=stack[:len(stack)-1] if node.Right==nil||node.Right==prev{ node.Left,node.Right=node.Right,node.Left//交换 prev=node node=nil }else { stack=append(stack,node) node=node.Right } } return root } 层序遍历 func invertTree(root *TreeNode) *TreeNode { if root==nil{ return root } queue:=list.New() node:=root queue.PushBack(node) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ e:=queue.Remove(queue.Front()).(*TreeNode) e.Left,e.Right=e.Right,e.Left//交换 if e.Left!=nil{ queue.PushBack(e.Left) } if e.Right!=nil{ queue.PushBack(e.Right) } } } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:16","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"对称二叉树 检查二叉树是否镜像对称。 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ // 递归 func defs(left *TreeNode, right *TreeNode) bool { if left == nil \u0026\u0026 right == nil { return true; }; if left == nil || right == nil { return false; }; if left.Val != right.Val { return false; } return defs(left.Left, right.Right) \u0026\u0026 defs(right.Left, left.Right); } func isSymmetric(root *TreeNode) bool { return defs(root.Left, root.Right); } // 迭代 func isSymmetric(root *TreeNode) bool { var queue []*TreeNode; if root != nil { queue = append(queue, root.Left, root.Right); } for len(queue) \u003e 0 { left := queue[0]; right := queue[1]; queue = queue[2:]; if left == nil \u0026\u0026 right == nil { continue; } if left == nil || right == nil || left.Val != right.Val { return false; }; queue = append(queue, left.Left, right.Right, right.Left, left.Right); } return true; } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:17","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最大深度 /** * definition for a binary tree node. * type treenode struct { * val int * left *treenode * right *treenode * } */ func max (a, b int) int { if a \u003e b { return a; } return b; } // 递归 func maxdepth(root *treenode) int { if root == nil { return 0; } return max(maxdepth(root.left), maxdepth(root.right)) + 1; } // 遍历 func maxdepth(root *treenode) int { levl := 0; queue := make([]*treenode, 0); if root != nil { queue = append(queue, root); } for l := len(queue); l \u003e 0; { for ;l \u003e 0;l-- { node := queue[0]; if node.left != nil { queue = append(queue, node.left); } if node.right != nil { queue = append(queue, node.right); } queue = queue[1:]; } levl++; l = len(queue); } return levl; } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:18","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最小深度 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func min(a, b int) int { if a \u003c b { return a; } return b; } // 递归 func minDepth(root *TreeNode) int { if root == nil { return 0; } if root.Left == nil \u0026\u0026 root.Right != nil { return 1 + minDepth(root.Right); } if root.Right == nil \u0026\u0026 root.Left != nil { return 1 + minDepth(root.Left); } return min(minDepth(root.Left), minDepth(root.Right)) + 1; } // 迭代 func minDepth(root *TreeNode) int { dep := 0; queue := make([]*TreeNode, 0); if root != nil { queue = append(queue, root); } for l := len(queue); l \u003e 0; { dep++; for ; l \u003e 0; l-- { node := queue[0]; if node.Left == nil \u0026\u0026 node.Right == nil { return dep; } if node.Left != nil { queue = append(queue, node.Left); } if node.Right != nil { queue = append(queue, node.Right); } queue = queue[1:]; } l = len(queue); } return dep; } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:19","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"完全二叉树的节点个数 递归版本 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ //本题直接就是求有多少个节点，无脑存进数组算长度就行了。 func countNodes(root *TreeNode) int { if root == nil { return 0 } res := 1 if root.Right != nil { res += countNodes(root.Right) } if root.Left != nil { res += countNodes(root.Left) } return res } 利用完全二叉树特性的递归解法 func countNodes(root *TreeNode) int { if root == nil { return 0 } leftH, rightH := 0, 0 leftNode := root.Left rightNode := root.Right for leftNode != nil { leftNode = leftNode.Left leftH++ } for rightNode != nil { rightNode = rightNode.Right rightH++ } if leftH == rightH { return (2 \u003c\u003c leftH) - 1 } return countNodes(root.Left) + countNodes(root.Right) + 1 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:20","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"平衡二叉树 给定一个二叉树，判断它是否是高度平衡的二叉树。 本题中，一棵高度平衡二叉树定义为：一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过1。 func isBalanced(root *TreeNode) bool { if root==nil{ return true } if !isBalanced(root.Left) || !isBalanced(root.Right){ return false } LeftH:=maxdepth(root.Left)+1 RightH:=maxdepth(root.Right)+1 if abs(LeftH-RightH)\u003e1{ return false } return true } func maxdepth(root *TreeNode)int{ if root==nil{ return 0 } return max(maxdepth(root.Left),maxdepth(root.Right))+1 } func max(a,b int)int{ if a\u003eb{ return a } return b } func abs(a int)int{ if a\u003c0{ return -a } return a } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:21","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"二叉树的所有路径 递归法： func binaryTreePaths(root *TreeNode) []string { res := make([]string, 0) var travel func(node *TreeNode, s string) travel = func(node *TreeNode, s string) { if node.Left == nil \u0026\u0026 node.Right == nil { v := s + strconv.Itoa(node.Val) res = append(res, v) return } s = s + strconv.Itoa(node.Val) + \"-\u003e\" if node.Left != nil { travel(node.Left, s) } if node.Right != nil { travel(node.Right, s) } } travel(root, \"\") return res } 迭代法： func binaryTreePaths(root *TreeNode) []string { stack := []*TreeNode{} paths := make([]string, 0) res := make([]string, 0) if root != nil { stack = append(stack, root) paths = append(paths, \"\") } for len(stack) \u003e 0 { l := len(stack) node := stack[l-1] path := paths[l-1] stack = stack[:l-1] paths = paths[:l-1] if node.Left == nil \u0026\u0026 node.Right == nil { res = append(res, path+strconv.Itoa(node.Val)) continue } if node.Right != nil { stack = append(stack, node.Right) paths = append(paths, path+strconv.Itoa(node.Val)+\"-\u003e\") } if node.Left != nil { stack = append(stack, node.Left) paths = append(paths, path+strconv.Itoa(node.Val)+\"-\u003e\") } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:22","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"二叉树的递归+回溯 100.相同的树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func isSameTree(p *TreeNode, q *TreeNode) bool { switch { case p == nil \u0026\u0026 q == nil: return true case p == nil || q == nil: fallthrough case p.Val != q.Val: return false } return isSameTree(p.Left, q.Left) \u0026\u0026 isSameTree(p.Right, q.Right) } 257.二叉的所有路径 递归法 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func binaryTreePaths(root *TreeNode) []string { var result []string traversal(root,\u0026result,\"\") return result } func traversal(root *TreeNode,result *[]string,pathStr string){ //判断是否为第一个元素 if len(pathStr)!=0{ pathStr=pathStr+\"-\u003e\"+strconv.Itoa(root.Val) }else{ pathStr=strconv.Itoa(root.Val) } //判断是否为叶子节点 if root.Left==nil\u0026\u0026root.Right==nil{ *result=append(*result,pathStr) return } //左右 if root.Left!=nil{ traversal(root.Left,result,pathStr) } if root.Right!=nil{ traversal(root.Right,result,pathStr) } } 回溯法 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func binaryTreePaths(root *TreeNode) []string { var result []string var path []int traversal(root,\u0026result,\u0026path) return result } func traversal(root *TreeNode,result *[]string,path *[]int){ *path=append(*path,root.Val) //判断是否为叶子节点 if root.Left==nil\u0026\u0026root.Right==nil{ pathStr:=strconv.Itoa((*path)[0]) for i:=1;i\u003clen(*path);i++{ pathStr=pathStr+\"-\u003e\"+strconv.Itoa((*path)[i]) } *result=append(*result,pathStr) return } //左右 if root.Left!=nil{ traversal(root.Left,result,path) *path=(*path)[:len(*path)-1]//回溯到上一个节点（因为traversal会加下一个节点值到path中） } if root.Right!=nil{ traversal(root.Right,result,path) *path=(*path)[:len(*path)-1]//回溯 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:23","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"左叶子之和 递归法 func sumOfLeftLeaves(root *TreeNode) int { var res int findLeft(root,\u0026res) return res } func findLeft(root *TreeNode,res *int){ //左节点 if root.Left!=nil\u0026\u0026root.Left.Left==nil\u0026\u0026root.Left.Right==nil{ *res=*res+root.Left.Val } if root.Left!=nil{ findLeft(root.Left,res) } if root.Right!=nil{ findLeft(root.Right,res) } } 迭代法 func sumOfLeftLeaves(root *TreeNode) int { var res int queue:=list.New() queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil\u0026\u0026node.Left.Left==nil\u0026\u0026node.Left.Right==nil{ res=res+node.Left.Val } if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:24","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"找树左下角的值 给定一个二叉树，在树的最后一行找到最左边的值。 递归法： var maxDeep int // 全局变量 深度 var value int //全局变量 最终值 func findBottomLeftValue(root *TreeNode) int { if root.Left==nil\u0026\u0026root.Right==nil{//需要提前判断一下（不要这个if的话提交结果会出错，但执行代码不会。防止这种情况出现，故先判断是否只有一个节点） return root.Val } findLeftValue (root,maxDeep) return value } func findLeftValue (root *TreeNode,deep int){ //最左边的值在左边 if root.Left==nil\u0026\u0026root.Right==nil{ if deep\u003emaxDeep{ value=root.Val maxDeep=deep } } //递归 if root.Left!=nil{ deep++ findLeftValue(root.Left,deep) deep--//回溯 } if root.Right!=nil{ deep++ findLeftValue(root.Right,deep) deep--//回溯 } } 迭代法： func findBottomLeftValue(root *TreeNode) int { queue:=list.New() var gradation int queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if i==0{gradation=node.Val} if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } } return gradation } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:25","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"路径总和 给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和 路径总和 //递归法 /** * definition for a binary tree node. * type treenode struct { * val int * left *treenode * right *treenode * } */ func haspathsum(root *treenode, targetsum int) bool { var flage bool //找没找到的标志 if root==nil{ return flage } pathsum(root,0,targetsum,\u0026flage) return flage } func pathsum(root *treenode, sum int,targetsum int,flage *bool){ sum+=root.val if root.left==nil\u0026\u0026root.right==nil\u0026\u0026sum==targetsum{ *flage=true return } if root.left!=nil\u0026\u0026!(*flage){//左节点不为空且还没找到 pathsum(root.left,sum,targetsum,flage) } if root.right!=nil\u0026\u0026!(*flage){//右节点不为空且没找到 pathsum(root.right,sum,targetsum,flage) } } 113 递归法 /** * definition for a binary tree node. * type treenode struct { * val int * left *treenode * right *treenode * } */ func pathsum(root *treenode, targetsum int) [][]int { var result [][]int//最终结果 if root==nil{ return result } var sumnodes []int//经过路径的节点集合 haspathsum(root,\u0026sumnodes,targetsum,\u0026result) return result } func haspathsum(root *treenode,sumnodes *[]int,targetsum int,result *[][]int){ *sumnodes=append(*sumnodes,root.val) if root.left==nil\u0026\u0026root.right==nil{//叶子节点 fmt.println(*sumnodes) var sum int var number int for k,v:=range *sumnodes{//求该路径节点的和 sum+=v number=k } tempnodes:=make([]int,number+1)//新的nodes接受指针里的值，防止最终指针里的值发生变动，导致最后的结果都是最后一个sumnodes的值 for k,v:=range *sumnodes{ tempnodes[k]=v } if sum==targetsum{ *result=append(*result,tempnodes) } } if root.left!=nil{ haspathsum(root.left,sumnodes,targetsum,result) *sumnodes=(*sumnodes)[:len(*sumnodes)-1]//回溯 } if root.right!=nil{ haspathsum(root.right,sumnodes,targetsum,result) *sumnodes=(*sumnodes)[:len(*sumnodes)-1]//回溯 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:26","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"从中序、后序遍历序列构造二叉树 106 从中序与后序遍历序列构造二叉树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func buildTree(inorder []int, postorder []int) *TreeNode { if len(inorder)\u003c1||len(postorder)\u003c1{return nil} //先找到根节点（后续遍历的最后一个就是根节点） nodeValue:=postorder[len(postorder)-1] //从中序遍历中找到一分为二的点，左边为左子树，右边为右子树 left:=findRootIndex(inorder,nodeValue) //构造root root:=\u0026TreeNode{Val: nodeValue, Left: buildTree(inorder[:left],postorder[:left]),//将后续遍历一分为二，左边为左子树，右边为右子树 Right: buildTree(inorder[left+1:],postorder[left:len(postorder)-1])} return root } func findRootIndex(inorder []int,target int) (index int){ for i:=0;i\u003clen(inorder);i++{ if target==inorder[i]{ return i } } return -1 } 105 从前序与中序遍历序列构造二叉树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func buildTree(preorder []int, inorder []int) *TreeNode { if len(preorder)\u003c1||len(inorder)\u003c1{return nil} left:=findRootIndex(preorder[0],inorder) root:=\u0026TreeNode{ Val: preorder[0], Left: buildTree(preorder[1:left+1],inorder[:left]), Right: buildTree(preorder[left+1:],inorder[left+1:])} return root } func findRootIndex(target int,inorder []int) int{ for i:=0;i\u003clen(inorder);i++{ if target==inorder[i]{ return i } } return -1 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:27","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最大二叉树 给定一个不含重复元素的整数数组。一个以此数组构建的最大二叉树定义如下： 二叉树的根是数组中的最大元素。 左子树是通过数组中最大值左边部分构造出的最大二叉树。 右子树是通过数组中最大值右边部分构造出的最大二叉树。 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func constructMaximumBinaryTree(nums []int) *TreeNode { if len(nums)\u003c1{return nil} //首选找到最大值 index:=findMax(nums) //其次构造二叉树 root:=\u0026TreeNode{ Val: nums[index], Left:constructMaximumBinaryTree(nums[:index]),//左半边 Right:constructMaximumBinaryTree(nums[index+1:]),//右半边 } return root } func findMax(nums []int) (index int){ for i:=0;i\u003clen(nums);i++{ if nums[i]\u003enums[index]{ index=i } } return } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:28","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"合并二叉树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ //前序遍历（递归遍历，跟105 106差不多的思路） func mergeTrees(t1 *TreeNode, t2 *TreeNode) *TreeNode { var value int var nullNode *TreeNode//空node，便于遍历 nullNode=\u0026TreeNode{ Val:0, Left:nil, Right:nil} switch { case t1==nil\u0026\u0026t2==nil: return nil//终止条件 default : //如果其中一个节点为空，则将该节点置为nullNode，方便下次遍历 if t1==nil{ value=t2.Val t1=nullNode }else if t2==nil{ value=t1.Val t2=nullNode }else { value=t1.Val+t2.Val } } root:=\u0026TreeNode{//构造新的二叉树 Val: value, Left: mergeTrees(t1.Left,t2.Left), Right: mergeTrees(t1.Right,t2.Right)} return root } // 前序遍历简洁版 func mergeTrees(root1 *TreeNode, root2 *TreeNode) *TreeNode { if root1 == nil { return root2 } if root2 == nil { return root1 } root1.Val += root2.Val root1.Left = mergeTrees(root1.Left, root2.Left) root1.Right = mergeTrees(root1.Right, root2.Right) return root1 } // 迭代版本 func mergeTrees(root1 *TreeNode, root2 *TreeNode) *TreeNode { queue := make([]*TreeNode,0) if root1 == nil{ return root2 } if root2 == nil{ return root1 } queue = append(queue,root1) queue = append(queue,root2) for size:=len(queue);size\u003e0;size=len(queue){ node1 := queue[0] queue = queue[1:] node2 := queue[0] queue = queue[1:] node1.Val += node2.Val // 左子树都不为空 if node1.Left != nil \u0026\u0026 node2.Left != nil{ queue = append(queue,node1.Left) queue = append(queue,node2.Left) } // 右子树都不为空 if node1.Right !=nil \u0026\u0026 node2.Right !=nil{ queue = append(queue,node1.Right) queue = append(queue,node2.Right) } // 树 1 的左子树为 nil，直接接上树 2 的左子树 if node1.Left == nil{ node1.Left = node2.Left } // 树 1 的右子树为 nil，直接接上树 2 的右子树 if node1.Right == nil{ node1.Right = node2.Right } } return root1 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:29","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"二叉搜索树 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:30","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"搜索 给定二叉搜索树（BST）的根节点和一个值。 你需要在BST中找到节点值等于给定值的节点。 返回以该节点为根的子树。 如果节点不存在，则返回 NULL。 递归法： //递归法 func searchBST(root *TreeNode, val int) *TreeNode { if root==nil||root.Val==val{ return root } if root.Val\u003eval{ return searchBST(root.Left,val) } return searchBST(root.Right,val) } 迭代法： //迭代法 func searchBST(root *TreeNode, val int) *TreeNode { for root!=nil{ if root.Val\u003eval{ root=root.Left }else if root.Val\u003cval{ root=root.Right }else{ break } } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:31","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"验证 给定一个二叉树，判断其是否是一个有效的二叉搜索树。 假设一个二叉搜索树具有如下特征： 节点的左子树只包含小于当前节点的数。 节点的右子树只包含大于当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 import \"math\" func isValidBST(root *TreeNode) bool { // 二叉搜索树也可以是空树 if root == nil { return true } // 由题目中的数据限制可以得出min和max return check(root,math.MinInt64,math.MaxInt64) } func check(node *TreeNode,min,max int64) bool { if node == nil { return true } if min \u003e= int64(node.Val) || max \u003c= int64(node.Val) { return false } // 分别对左子树和右子树递归判断，如果左子树和右子树都符合则返回true return check(node.Right,int64(node.Val),max) \u0026\u0026 check(node.Left,min,int64(node.Val)) } // 中序遍历解法 func isValidBST(root *TreeNode) bool { // 保存上一个指针 var prev *TreeNode var travel func(node *TreeNode) bool travel = func(node *TreeNode) bool { if node == nil { return true } leftRes := travel(node.Left) // 当前值小于等于前一个节点的值，返回false if prev != nil \u0026\u0026 node.Val \u003c= prev.Val { return false } prev = node rightRes := travel(node.Right) return leftRes \u0026\u0026 rightRes } return travel(root) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:32","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最小绝对差 给你一棵所有节点为非负值的二叉搜索树，请你计算树中任意两节点的差的绝对值的最小值。 中序遍历，然后计算最小差值 func getMinimumDifference(root *TreeNode) int { var res []int findMIn(root,\u0026res) min:=1000000//一个比较大的值 for i:=1;i\u003clen(res);i++{ tempValue:=res[i]-res[i-1] if tempValue\u003cmin{ min=tempValue } } return min } //中序遍历 func findMIn(root *TreeNode,res *[]int){ if root==nil{return} findMIn(root.Left,res) *res=append(*res,root.Val) findMIn(root.Right,res) } // 中序遍历的同时计算最小值 func getMinimumDifference(root *TreeNode) int { // 保留前一个节点的指针 var prev *TreeNode // 定义一个比较大的值 min := math.MaxInt64 var travel func(node *TreeNode) travel = func(node *TreeNode) { if node == nil { return } travel(node.Left) if prev != nil \u0026\u0026 node.Val - prev.Val \u003c min { min = node.Val - prev.Val } prev = node travel(node.Right) } travel(root) return min } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:33","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"众数 给定一个有相同值的二叉搜索树（BST），找出 BST 中的所有众数（出现频率最高的元素）。 暴力法 func findMode(root *TreeNode) []int { var history map[int]int var maxValue int var maxIndex int var result []int history=make(map[int]int) traversal(root,history) for k,value:=range history{ if value\u003emaxValue{ maxValue=value maxIndex=k } } for k,value:=range history{ if value==history[maxIndex]{ result=append(result,k) } } return result } func traversal(root *TreeNode,history map[int]int){ if root.Left!=nil{ traversal(root.Left,history) } if value,ok:=history[root.Val];ok{ history[root.Val]=value+1 }else{ history[root.Val]=1 } if root.Right!=nil{ traversal(root.Right,history) } } 计数法，不使用额外空间，利用二叉树性质，中序遍历 func findMode(root *TreeNode) []int { res := make([]int, 0) count := 1 max := 1 var prev *TreeNode var travel func(node *TreeNode) travel = func(node *TreeNode) { if node == nil { return } travel(node.Left) if prev != nil \u0026\u0026 prev.Val == node.Val { count++ } else { count = 1 } if count \u003e= max { if count \u003e max \u0026\u0026 len(res) \u003e 0 { res = []int{node.Val} } else { res = append(res, node.Val) } max = count } prev = node travel(node.Right) } travel(root) return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:34","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"二叉树最近公共祖先 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先（可以是自己） func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { // check if root == nil { return root } // 相等 直接返回root节点即可 if root == p || root == q { return root } // Divide left := lowestCommonAncestor(root.Left, p, q) right := lowestCommonAncestor(root.Right, p, q) // Conquer // 左右两边都不为空，则根节点为祖先 if left != nil \u0026\u0026 right != nil { return root } if left != nil { return left } if right != nil { return right } return nil } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:35","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"二叉搜索树的最近公共祖先 递归法： //利用BSL的性质（前序遍历有序） func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { if root==nil{return nil} if root.Val\u003ep.Val\u0026\u0026root.Val\u003eq.Val{//当前节点的值大于给定的值，则说明满足条件的在左边 return lowestCommonAncestor(root.Left,p,q) }else if root.Val\u003cp.Val\u0026\u0026root.Val\u003cq.Val{//当前节点的值小于各点的值，则说明满足条件的在右边 return lowestCommonAncestor(root.Right,p,q) }else {return root}//当前节点的值在给定值的中间（或者等于），即为最深的祖先 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:36","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"插入操作 给定二叉搜索树（BST）的根节点和要插入树中的值，将值插入二叉搜索树。 返回插入后二叉搜索树的根节点。 输入数据保证，新值和原始二叉搜索树中的任意节点值都不同。 递归法 func insertIntoBST(root *TreeNode, val int) *TreeNode { if root == nil { root = \u0026TreeNode{Val: val} return root } if root.Val \u003e val { root.Left = insertIntoBST(root.Left, val) } else { root.Right = insertIntoBST(root.Right, val) } return root } 迭代法 func insertIntoBST(root *TreeNode, val int) *TreeNode { if root == nil { return \u0026TreeNode{Val:val} } node := root var pnode *TreeNode for node != nil { if val \u003e node.Val { pnode = node node = node.Right } else { pnode = node node = node.Left } } if val \u003e pnode.Val { pnode.Right = \u0026TreeNode{Val: val} } else { pnode.Left = \u0026TreeNode{Val: val} } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:37","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"删除节点 搜索树的节点删除要比节点增加复杂的多。 // 递归版本 func deleteNode(root *TreeNode, key int) *TreeNode { if root==nil{ return nil } if key\u003croot.Val{ root.Left=deleteNode(root.Left,key) return root } if key\u003eroot.Val{ root.Right=deleteNode(root.Right,key) return root } if root.Right==nil{ return root.Left } if root.Left==nil{ return root.Right } minnode:=root.Right for minnode.Left!=nil{ minnode=minnode.Left } root.Val=minnode.Val root.Right=deleteNode1(root.Right) return root } func deleteNode1(root *TreeNode)*TreeNode{ if root.Left==nil{ pRight:=root.Right root.Right=nil return pRight } root.Left=deleteNode1(root.Left) return root } // 迭代版本 func deleteOneNode(target *TreeNode) *TreeNode { if target == nil { return target } if target.Right == nil { return target.Left } cur := target.Right for cur.Left != nil { cur = cur.Left } cur.Left = target.Left return target.Right } func deleteNode(root *TreeNode, key int) *TreeNode { // 特殊情况处理 if root == nil { return root } cur := root var pre *TreeNode for cur != nil { if cur.Val == key { break } pre = cur if cur.Val \u003e key { cur = cur.Left } else { cur = cur.Right } } if pre == nil { return deleteOneNode(cur) } // pre 要知道是删除左孩子还有右孩子 if pre.Left != nil \u0026\u0026 pre.Left.Val == key { pre.Left = deleteOneNode(cur) } if pre.Right != nil \u0026\u0026 pre.Right.Val == key { pre.Right = deleteOneNode(cur) } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:38","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"修剪 给定一个二叉搜索树，同时给定最小边界L 和最大边界 R。通过修剪二叉搜索树，使得所有节点的值在[L, R]中 (R\u003e=L) 。你可能需要改变树的根节点，所以结果应当返回修剪好的二叉搜索树的新的根节点。 // 递归 func trimBST(root *TreeNode, low int, high int) *TreeNode { if root==nil{ return nil } if root.Val\u003clow{//如果该节点值小于最小值，则该节点更换为该节点的右节点值，继续遍历 right:=trimBST(root.Right,low,high) return right } if root.Val\u003ehigh{//如果该节点的值大于最大值，则该节点更换为该节点的左节点值，继续遍历 left:=trimBST(root.Left,low,high) return left } root.Left=trimBST(root.Left,low,high) root.Right=trimBST(root.Right,low,high) return root } // 迭代 func trimBST(root *TreeNode, low int, high int) *TreeNode { if root == nil { return nil } // 处理 root，让 root 移动到[low, high] 范围内，**注意**是左闭右闭 for root != nil \u0026\u0026 (root.Val\u003clow||root.Val\u003ehigh){ if root.Val \u003c low{ root = root.Right }else{ root = root.Left } } cur := root // 此时 root 已经在[low, high] 范围内，处理左孩子元素小于 low 的情况（左节点是一定小于 root.Val，因此天然小于 high） for cur != nil{ for cur.Left!=nil \u0026\u0026 cur.Left.Val \u003c low{ cur.Left = cur.Left.Right } cur = cur.Left } cur = root // 此时 root 已经在[low, high] 范围内，处理右孩子大于 high 的情况 for cur != nil{ for cur.Right!=nil \u0026\u0026 cur.Right.Val \u003e high{ cur.Right = cur.Right.Left } cur = cur.Right } return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:39","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"将有序数组转化为二叉搜索树 将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。 本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。 递归（隐含回溯） func sortedArrayToBST(nums []int) *TreeNode { if len(nums)==0{return nil}//终止条件，最后数组为空则可以返回 root:=\u0026TreeNode{nums[len(nums)/2],nil,nil}//按照BSL的特点，从中间构造节点 root.Left=sortedArrayToBST(nums[:len(nums)/2])//数组的左边为左子树 root.Right=sortedArrayToBST(nums[len(nums)/2+1:])//数字的右边为右子树 return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:40","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"把二叉搜索树转化为累加树 给出二叉 搜索 树的根节点，该树的节点值各不相同，请你将其转换为累加树（Greater Sum Tree），使每个节点 node 的新值等于原树中大于或等于 node.val 的值之和。 弄一个sum暂存其和值 //右中左 func bstToGst(root *TreeNode) *TreeNode { var sum int RightMLeft(root,\u0026sum) return root } func RightMLeft(root *TreeNode,sum *int) *TreeNode { if root==nil{return nil}//终止条件，遇到空节点就返回 RightMLeft(root.Right,sum)//先遍历右边 temp:=*sum//暂存总和值 *sum+=root.Val//将总和值变更 root.Val+=temp//更新节点值 RightMLeft(root.Left,sum)//遍历左节点 return root } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:7:41","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"回溯算法 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"理论基础 也叫回溯搜索算法。 回溯是递归的副产品，只要有递归就会有回溯 回溯的本质是穷举，穷举所有可能，然后选出我们想要的答案，并不算高效。加一些剪枝操作或许会高效一点。 一般用来解决除了暴力搜索无可奈何的情况。 回溯法，一般可以解决如下几种问题： 组合问题：N个数里面按一定规则找出k个数的集合 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 排列问题：N个数按一定规则全排列，有几种排列方式 棋盘问题：N皇后，解数独等等 回溯法解决的问题都可以抽象为树形结构 因为回溯法解决的都是在集合中递归查找子集，集合的大小就构成了树的宽度，递归的深度，都构成的树的深度。 回溯模板： void backtracking(参数) { if (终止条件) { 存放结果; return; } for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) { 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"*组合问题及其优化 给定两个整数 n 和 k，返回 1 … n 中所有可能的 k 个数的组合。 回溯法三部曲：函数参数、终止条件和单层搜索 剪枝优化： 可以剪枝的地方就在递归中每一层的for循环所选择的起始位置。 如果for循环选择的起始位置之后的元素个数已经不足我们需要的元素个数了，那么就没有必要搜索了。 var res [][]int func combine(n int, k int) [][]int { res=[][]int{} if n \u003c= 0 || k \u003c= 0 || k \u003e n { return res } backtrack(n, k, 1, []int{}) return res } func backtrack(n,k,start int,track []int){ if len(track)==k{ temp:=make([]int,k) copy(temp,track) res=append(res,temp) } if len(track)+n-start+1 \u003c k { return } for i:=start;i\u003c=n;i++{ track=append(track,i) backtrack(n,k,i+1,track) track=track[:len(track)-1] } } 剪枝：go语言的剪枝优化会爆内存溢出，不知道是为啥…… var res [][]int func combine(n int, k int) [][]int { res=[][]int{} if n \u003c= 0 || k \u003c= 0 || k \u003e n { return res } backtrack(n, k, 1, []int{}) return res } func backtrack(n,k,start int,track []int){ if len(track)==k{ temp:=make([]int,k) copy(temp,track) res=append(res,temp) } if len(track)+n-start+1 \u003c k { return } for i:=start;i\u003c=(n-k+len(track)+1);i++{ track=append(track,i) backtrack(n,k,i+1,track) track=track[:len(track)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"组合总和III 找出所有相加之和为 n 的 k 个数的组合。组合中只允许含有 1 - 9 的正整数，并且每种组合中不存在重复的数字。 说明： 所有数字都是正整数。 解集不能包含重复的组合。 回溯+减枝 func combinationSum3(k int, n int) [][]int { var track []int// 遍历路径 var result [][]int// 存放结果集 backTree(n,k,1,\u0026track,\u0026result) return result } func backTree(n,k,startIndex int,track *[]int,result *[][]int){ if len(*track)==k{ var sum int tmp:=make([]int,k) for k,v:=range *track{ sum+=v tmp[k]=v } if sum==n{ *result=append(*result,tmp) } return } for i:=startIndex;i\u003c=9-(k-len(*track))+1;i++{//减枝（k-len(*track)表示还剩多少个可填充的元素） *track=append(*track,i)//记录路径 backTree(n,k,i+1,track,result)//递归 *track=(*track)[:len(*track)-1]//回溯 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"电话号码的字母组合 给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。 给出数字到字母的映射如电话按键。注意 1 不对应任何字母。 主要在于递归中传递下一个数字 func letterCombinations(digits string) []string { lenth:=len(digits) if lenth==0 ||lenth\u003e4{ return nil } digitsMap:= [10]string{ \"\", // 0 \"\", // 1 \"abc\", // 2 \"def\", // 3 \"ghi\", // 4 \"jkl\", // 5 \"mno\", // 6 \"pqrs\", // 7 \"tuv\", // 8 \"wxyz\", // 9 } res:=make([]string,0) recursion(\"\",digits,0,digitsMap,\u0026res) return res } func recursion(tempString ,digits string, Index int,digitsMap [10]string, res *[]string) {//index表示第几个数字 if len(tempString)==len(digits){//终止条件，字符串长度等于digits的长度 *res=append(*res,tempString) return } tmpK:=digits[Index]-'0' // 将index指向的数字转为int（确定下一个数字） letter:=digitsMap[tmpK]// 取数字对应的字符集 for i:=0;i\u003clen(letter);i++{ tempString=tempString+string(letter[i])//拼接结果 recursion(tempString,digits,Index+1,digitsMap,res) tempString=tempString[:len(tempString)-1]//回溯 } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"组合总和 给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。candidates 中的数字可以无限制重复被选取。 主要在于递归中传递下一个数字 func combinationSum(candidates []int, target int) [][]int { var trcak []int var res [][]int backtracking(0,0,target,candidates,trcak,\u0026res) return res } func backtracking(startIndex,sum,target int,candidates,trcak []int,res *[][]int){ //终止条件 if sum==target{ tmp:=make([]int,len(trcak)) copy(tmp,trcak)//拷贝 *res=append(*res,tmp)//放入结果集 return } if sum\u003etarget{return} //回溯 for i:=startIndex;i\u003clen(candidates);i++{ //更新路径集合和sum trcak=append(trcak,candidates[i]) sum+=candidates[i] //递归 backtracking(i,sum,target,candidates,trcak,res) //回溯 trcak=trcak[:len(trcak)-1] sum-=candidates[i] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"组合总和II 给定一个数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。 candidates 中的每个数字在每个组合中只能使用一次。 主要在于如何在回溯中去重 使用used数组 func combinationSum2(candidates []int, target int) [][]int { var trcak []int var res [][]int var history map[int]bool history=make(map[int]bool) sort.Ints(candidates) backtracking(0,0,target,candidates,trcak,\u0026res,history) return res } func backtracking(startIndex,sum,target int,candidates,trcak []int,res *[][]int,history map[int]bool){ //终止条件 if sum==target{ tmp:=make([]int,len(trcak)) copy(tmp,trcak)//拷贝 *res=append(*res,tmp)//放入结果集 return } if sum\u003etarget{return} //回溯 // used[i - 1] == true，说明同一树枝candidates[i - 1]使用过 // used[i - 1] == false，说明同一树层candidates[i - 1]使用过 for i:=startIndex;i\u003clen(candidates);i++{ if i\u003e0\u0026\u0026candidates[i]==candidates[i-1]\u0026\u0026history[i-1]==false{ continue } //更新路径集合和sum trcak=append(trcak,candidates[i]) sum+=candidates[i] history[i]=true //递归 backtracking(i+1,sum,target,candidates,trcak,res,history) //回溯 trcak=trcak[:len(trcak)-1] sum-=candidates[i] history[i]=false } } 不使用used数组 func combinationSum2(candidates []int, target int) [][]int { var trcak []int var res [][]int sort.Ints(candidates) backtracking(0,0,target,candidates,trcak,\u0026res) return res } func backtracking(startIndex,sum,target int,candidates,trcak []int,res *[][]int){ //终止条件 if sum==target{ tmp:=make([]int,len(trcak)) //拷贝 copy(tmp,trcak) //放入结果集 *res=append(*res,tmp) return } //回溯 for i:=startIndex;i\u003clen(candidates) \u0026\u0026 sum+candidates[i]\u003c=target;i++{ // 若当前树层有使用过相同的元素，则跳过 if i\u003estartIndex\u0026\u0026candidates[i]==candidates[i-1]{ continue } //更新路径集合和sum trcak=append(trcak,candidates[i]) sum+=candidates[i] backtracking(i+1,sum,target,candidates,trcak,res) //回溯 trcak=trcak[:len(trcak)-1] sum-=candidates[i] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"分割回文串 给定一个字符串 s，将 s 分割成一些子串，使每个子串都是回文串。 返回 s 所有可能的分割方案。 示例: 输入: \"aab\" 输出: [ [\"aa\",\"b\"], [\"a\",\"a\",\"b\"] ] 注意切片（go切片是披着值类型外衣的引用类型） func partition(s string) [][]string { var tmpString []string//切割字符串集合 var res [][]string//结果集合 backTracking(s,tmpString,0,\u0026res) return res } func backTracking(s string,tmpString []string,startIndex int,res *[][]string){ if startIndex==len(s){//到达字符串末尾了 //进行一次切片拷贝，怕之后的操作影响tmpString切片内的值 t := make([]string, len(tmpString)) copy(t, tmpString) *res=append(*res,t) } for i:=startIndex;i\u003clen(s);i++{ //处理（首先通过startIndex和i判断切割的区间，进而判断该区间的字符串是否为回文，若为回文，则加入到tmpString，否则继续后移，找到回文区间）（这里为一层处理） if isPartition(s,startIndex,i){ tmpString=append(tmpString,s[startIndex:i+1]) }else{ continue } //递归 backTracking(s,tmpString,i+1,res) //回溯 tmpString=tmpString[:len(tmpString)-1] } } //判断是否为回文 func isPartition(s string,startIndex,end int)bool{ left:=startIndex right:=end for ;left\u003cright;{ if s[left]!=s[right]{ return false } //移动左右指针 left++ right-- } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:7","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"复原IP地址 给定一个只包含数字的字符串，复原它并返回所有可能的 IP 地址格式。 有效的 IP 地址 正好由四个整数（每个整数位于 0 到 255 之间组成，且不能含有前导 0），整数之间用 ‘.’ 分隔。 例如：“0.1.2.201” 和 “192.168.1.1” 是 有效的 IP 地址，但是 “0.011.255.245”、“192.168.1.312” 和 “192.168@1.1” 是 无效的 IP 地址。 示例 1： 输入：s = \"25525511135\" 输出：[\"255.255.11.135\",\"255.255.111.35\"] 示例 2： 输入：s = \"0000\" 输出：[\"0.0.0.0\"] 回溯（对于前导 0的IP（特别注意s[startIndex]==‘0’的判断，不应该写成s[startIndex]==0，因为s截取出来不是数字）） func restoreIpAddresses(s string) []string { var res,path []string backTracking(s,path,0,\u0026res) return res } func backTracking(s string,path []string,startIndex int,res *[]string){ //终止条件 if startIndex==len(s)\u0026\u0026len(path)==4{ tmpIpString:=path[0]+\".\"+path[1]+\".\"+path[2]+\".\"+path[3] *res=append(*res,tmpIpString) } for i:=startIndex;i\u003clen(s);i++{ //处理 path:=append(path,s[startIndex:i+1]) if i-startIndex+1\u003c=3\u0026\u0026len(path)\u003c=4\u0026\u0026isNormalIp(s,startIndex,i){ //递归 backTracking(s,path,i+1,res) }else {//如果首尾超过了3个，或路径多余4个，或前导为0，或大于255，直接回退 return } //回溯 path=path[:len(path)-1] } } func isNormalIp(s string,startIndex,end int)bool{ checkInt,_:=strconv.Atoi(s[startIndex:end+1]) if end-startIndex+1\u003e1\u0026\u0026s[startIndex]=='0'{//对于前导 0的IP（特别**注意**s[startIndex]=='0'的判断，不应该写成s[startIndex]==0，因为s截取出来不是数字） return false } if checkInt\u003e255{ return false } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:8","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"子集问题 给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 var res [][]int func subset(nums []int) [][]int { res = make([][]int, 0) sort.Ints(nums) Dfs([]int{}, nums, 0) return res } func Dfs(temp, nums []int, start int){ tmp := make([]int, len(temp)) copy(tmp, temp) res = append(res, tmp) for i := start; i \u003c len(nums); i++{ //if i\u003estart\u0026\u0026nums[i]==nums[i-1]{ // continue //} temp = append(temp, nums[i]) Dfs(temp, nums, i+1) temp = temp[:len(temp)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:9","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"子集II 给定一个可能包含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 var res[][]int func subsetsWithDup(nums []int)[][]int { res=make([][]int,0) sort.Ints(nums) dfs([]int{},nums,0) return res } func dfs(temp, num []int, start int) { tmp:=make([]int,len(temp)) copy(tmp,temp) res=append(res,tmp) for i:=start;i\u003clen(num);i++{ if i\u003estart\u0026\u0026num[i]==num[i-1]{ continue } temp=append(temp,num[i]) dfs(temp,num,i+1) temp=temp[:len(temp)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:10","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"递增子序列 给定一个整型数组, 你的任务是找到所有该数组的递增子序列，递增子序列的长度至少是2。给定数组中可能包含重复数字，相等的数字应该被视为递增的一种情况 func findSubsequences(nums []int) [][]int { var subRes []int var res [][]int backTring(0,nums,subRes,\u0026res) return res } func backTring(startIndex int,nums,subRes []int,res *[][]int){ if len(subRes)\u003e1{ tmp:=make([]int,len(subRes)) copy(tmp,subRes) *res=append(*res,tmp) } history:=[201]int{}//记录本层元素使用记录 for i:=startIndex;i\u003clen(nums);i++{ //分两种情况判断：一，当前取的元素小于子集的最后一个元素，则继续寻找下一个适合的元素 // 或者二，当前取的元素在本层已经出现过了，所以跳过该元素，继续寻找 if len(subRes)\u003e0\u0026\u0026nums[i]\u003csubRes[len(subRes)-1]||history[nums[i] + 100]==1{ continue } history[nums[i] + 100]=1//表示本层该元素使用过了 subRes=append(subRes,nums[i]) backTring(i+1,nums,subRes,res) subRes=subRes[:len(subRes)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:11","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"全排列 给定一个 没有重复 数字的序列，返回其所有可能的全排列。 var res [][]int func permute(nums []int) [][]int { res = [][]int{} backTrack(nums,len(nums),[]int{}) return res } func backTrack(nums []int,numsLen int,path []int) { if len(nums)==0{ p:=make([]int,len(path)) copy(p,path) res = append(res,p) } for i:=0;i\u003cnumsLen;i++{ cur:=nums[i] path = append(path,cur) nums = append(nums[:i],nums[i+1:]...)//直接使用切片 backTrack(nums,len(nums),path) nums = append(nums[:i],append([]int{cur},nums[i:]...)...)//回溯的时候切片也要复原，元素位置不能变 path = path[:len(path)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:12","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"全排列II 给定一个可包含重复数字的序列 nums ，按任意顺序 返回所有不重复的全排列。 var res [][]int func permute(nums []int) [][]int { res = [][]int{} backTrack(nums,len(nums),[]int{}) return res } func backTrack(nums []int,numsLen int,path []int) { if len(nums)==0{ p:=make([]int,len(path)) copy(p,path) res = append(res,p) } used := [21]int{}//跟前一题唯一的区别，同一层不使用重复的数。关于used的思想carl在递增子序列那一题中提到过 for i:=0;i\u003cnumsLen;i++{ if used[nums[i]+10]==1{ continue } cur:=nums[i] path = append(path,cur) used[nums[i]+10]=1 nums = append(nums[:i],nums[i+1:]...) backTrack(nums,len(nums),path) nums = append(nums[:i],append([]int{cur},nums[i:]...)...) path = path[:len(path)-1] } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:13","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"回溯算法去重问题的另一种写法 https://programmercarl.com/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E5%8E%BB%E9%87%8D%E9%97%AE%E9%A2%98%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%86%99%E6%B3%95.html#_90-%E5%AD%90%E9%9B%86ii 看c++版的 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:14","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"重新安排行程 深搜和回溯也是相辅相成的，都用了递归。 给定一个机票的字符串二维数组 [from, to]，子数组中的两个成员分别表示飞机出发和降落的机场地点，对该行程进行重新规划排序。所有这些机票都属于一个从 JFK（肯尼迪国际机场）出发的先生，所以该行程必须从 JFK 开始。 提示： 如果存在多种有效的行程，请你按字符自然排序返回最小的行程组合。例如，行程 [“JFK”, “LGA”] 与 [“JFK”, “LGB”] 相比就更小，排序更靠前 所有的机场都用三个大写字母表示（机场代码）。 假定所有机票至少存在一种合理的行程。 所有的机票必须都用一次 且 只能用一次。 示例 1： 输入：[[\"MUC\", \"LHR\"], [\"JFK\", \"MUC\"], [\"SFO\", \"SJC\"], [\"LHR\", \"SFO\"]] 输出：[\"JFK\", \"MUC\", \"LHR\", \"SFO\", \"SJC\"] 示例 2： 输入：[[\"JFK\",\"SFO\"],[\"JFK\",\"ATL\"],[\"SFO\",\"ATL\"],[\"ATL\",\"JFK\"],[\"ATL\",\"SFO\"]] 输出：[\"JFK\",\"ATL\",\"JFK\",\"SFO\",\"ATL\",\"SFO\"] 解释：另一种有效的行程是 [\"JFK\",\"SFO\",\"ATL\",\"JFK\",\"ATL\",\"SFO\"]。但是它自然排序更大更靠后 class Solution { private: // unordered_map\u003c出发机场, map\u003c到达机场, 航班次数\u003e\u003e targets unordered_map\u003cstring, map\u003cstring, int\u003e\u003e targets; bool backtracking(int ticketNum, vector\u003cstring\u003e\u0026 result) { if (result.size() == ticketNum + 1) { return true; } for (pair\u003cconst string, int\u003e\u0026 target : targets[result[result.size() - 1]]) { if (target.second \u003e 0 ) { // 记录到达机场是否飞过了 result.push_back(target.first); target.second--; if (backtracking(ticketNum, result)) return true; result.pop_back(); target.second++; } } return false; } public: vector\u003cstring\u003e findItinerary(vector\u003cvector\u003cstring\u003e\u003e\u0026 tickets) { targets.clear(); vector\u003cstring\u003e result; for (const vector\u003cstring\u003e\u0026 vec : tickets) { targets[vec[0]][vec[1]]++; // 记录映射关系 } result.push_back(\"JFK\"); // 起始机场 backtracking(tickets.size(), result); return result; } }; ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:15","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"N皇后 n 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。 给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。 每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 ‘Q’ 和 ‘.’ 分别代表了皇后和空位。 import \"strings\" var res [][]string func isValid(board [][]string, row, col int) (res bool){ n := len(board) for i:=0; i \u003c row; i++ { if board[i][col] == \"Q\" { return false } } for i := 0; i \u003c n; i++{ if board[row][i] == \"Q\" { return false } } for i ,j := row, col; i \u003e= 0 \u0026\u0026 j \u003e=0 ; i, j = i - 1, j- 1{ if board[i][j] == \"Q\"{ return false } } for i, j := row, col; i \u003e=0 \u0026\u0026 j \u003c n; i,j = i-1, j+1 { if board[i][j] == \"Q\" { return false } } return true } func backtrack(board [][]string, row int) { size := len(board) if row == size{ temp := make([]string, size) for i := 0; i\u003csize;i++{ temp[i] = strings.Join(board[i],\"\") } res =append(res,temp) return } for col := 0; col \u003c size; col++ { if !isValid(board, row, col){ continue } board[row][col] = \"Q\" backtrack(board, row+1) board[row][col] = \".\" } } func solveNQueens(n int) [][]string { res = [][]string{} board := make([][]string, n) for i := 0; i \u003c n; i++{ board[i] = make([]string, n) } for i := 0; i \u003c n; i++{ for j := 0; j\u003cn;j++{ board[i][j] = \".\" } } backtrack(board, 0) return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:16","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"解数独 编写一个程序，通过填充空格来解决数独问题。 func solveSudoku(board [][]byte) { var backtracking func(board [][]byte) bool backtracking=func(board [][]byte) bool{ for i:=0;i\u003c9;i++{ for j:=0;j\u003c9;j++{ //判断此位置是否适合填数字 if board[i][j]!='.'{ continue } //尝试填1-9 for k:='1';k\u003c='9';k++{ if isvalid(i,j,byte(k),board)==true{//如果满足要求就填 board[i][j]=byte(k) if backtracking(board)==true{ return true } board[i][j]='.' } } return false } } return true } backtracking(board) } //判断填入数字是否满足要求 func isvalid(row,col int,k byte,board [][]byte)bool{ for i:=0;i\u003c9;i++{//行 if board[row][i]==k{ return false } } for i:=0;i\u003c9;i++{//列 if board[i][col]==k{ return false } } //方格 startrow:=(row/3)*3 startcol:=(col/3)*3 for i:=startrow;i\u003cstartrow+3;i++{ for j:=startcol;j\u003cstartcol+3;j++{ if board[i][j]==k{ return false } } } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:8:17","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"贪心算法 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"理论基础 贪心算法一般分为如下四步： 将问题分解为若干个子问题 找出适合的贪心策略 求解每一个子问题的最优解 将局部最优解堆叠成全局最优解 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"分发饼干 假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。 对每个孩子 i，都有一个胃口值 g[i]，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j，都有一个尺寸 s[j] 。如果 s[j] \u003e= g[i]，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。 示例 1: 输入: g = [1,2,3], s = [1,1] 输出: 1 解释:你有三个孩子和两块小饼干，3个孩子的胃口值分别是：1,2,3。虽然你有两块小饼干，由于他们的尺寸都是1，你只能让胃口值是1的孩子满足。所以你应该输出1。 //排序后，局部最优 func findContentChildren(g []int, s []int) int { sort.Ints(g) sort.Ints(s) // 从小到大 child := 0 for sIdx := 0; child \u003c len(g) \u0026\u0026 sIdx \u003c len(s); sIdx++ { if s[sIdx] \u003e= g[child] {//如果饼干的大小大于或等于孩子的为空则给与，否则不给予，继续寻找选一个饼干是否符合 child++ } } return child } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"摆动序列 如果连续数字之间的差严格地在正数和负数之间交替，则数字序列称为摆动序列。第一个差（如果存在的话）可能是正数或负数。少于两个元素的序列也是摆动序列。 例如， [1,7,4,9,2,5] 是一个摆动序列，因为差值 (6,-3,5,-7,3) 是正负交替出现的。相反, [1,4,7,2,5] 和 [1,7,4,5,5] 不是摆动序列，第一个序列是因为它的前两个差值都是正数，第二个序列是因为它的最后一个差值为零。 给定一个整数序列，返回作为摆动序列的最长子序列的长度。 通过从原始序列中删除一些（也可以不删除）元素来获得子序列，剩下的元素保持其原始顺序。 示例 1: 输入: [1,7,4,9,2,5] 输出: 6 解释: 整个序列均为摆动序列。 示例 2: 输入: [1,17,5,10,13,15,10,5,16,8] 输出: 7 解释: 这个序列包含几个长度为 7 摆动序列，其中一个可为[1,17,10,13,10,16,8]。 贪心或者dp func wiggleMaxLength(nums []int) int { var count,preDiff,curDiff int count=1 if len(nums)\u003c2{ return count } for i:=0;i\u003clen(nums)-1;i++{ curDiff=nums[i+1]-nums[i] //如果有正有负则更新下标值||或者只有前一个元素为0（针对两个不等元素的序列也视作摆动序列，且摆动长度为2） if (curDiff \u003e 0 \u0026\u0026 preDiff \u003c= 0) || (preDiff \u003e= 0 \u0026\u0026 curDiff \u003c 0){ preDiff=curDiff count++ } } return count } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最大子序和 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例: 输入: [-2,1,-3,4,-1,2,1,-5,4] 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6 贪心或者dp func maxSubArray(nums []int) int { maxSum := nums[0] for i := 1; i \u003c len(nums); i++ { if nums[i] + nums[i-1] \u003e nums[i] { nums[i] += nums[i-1] } if nums[i] \u003e maxSum { maxSum = nums[i] } } return maxSum } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"买卖股票的最佳时机II 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: [7,1,5,3,6,4] 输出: 7 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4。随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 示例 2: 输入: [1,2,3,4,5] 输出: 4 解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。**注意**你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3: 输入: [7,6,4,3,1] 输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0 贪心或者dp //贪心算法 func maxProfit(prices []int) int { var sum int for i := 1; i \u003c len(prices); i++ { // 累加每次大于0的交易 if prices[i]-prices[i-1] \u003e 0 { sum += prices[i]-prices[i-1] } } return sum } //确定售卖点 func maxProfit(prices []int) int { var result,buy int prices=append(prices,0)//在price末尾加个0，防止price一直递增 /** 思路：检查后一个元素是否大于当前元素，如果小于，则表明这是一个售卖点，当前元素的值减去购买时候的值 如果不小于，说明后面有更好的售卖点， **/ for i:=0;i\u003clen(prices)-1;i++{ if prices[i]\u003eprices[i+1]{ result+=prices[i]-prices[buy] buy=i+1 }else if prices[buy]\u003eprices[i]{//更改最低购买点 buy=i } } return result } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"跳跃游戏 I 给定一个非负整数数组，你最初位于数组的第一个位置。 数组中的每个元素代表你在该位置可以跳跃的最大长度。 判断你是否能够到达最后一个位置。 func canJUmp(nums []int) bool { if len(nums)\u003c=1{ return true } dp:=make([]bool,len(nums)) dp[0]=true for i:=1;i\u003clen(nums);i++{ for j:=i-1;j\u003e=0;j--{ if dp[j]\u0026\u0026nums[j]+j\u003e=i{ dp[i]=true break } } } return dp[len(nums)-1] } II 给定一个非负整数数组，你最初位于数组的第一个位置。 数组中的每个元素代表你在该位置可以跳跃的最大长度。 你的目标是使用最少的跳跃次数到达数组的最后一个位置。 func jump(nums []int) int { dp:=make([]int ,len(nums)) dp[0]=0 for i:=1;i\u003clen(nums);i++{ dp[i]=i for j:=0;j\u003ci;j++{ if nums[j]+j\u003ei{ dp[i]=min(dp[j]+1,dp[i]) } } } return dp[len(nums)-1] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"K次取反后最大化的数组和 给定一个整数数组 A，我们只能用以下方法修改该数组：我们选择某个索引 i 并将 A[i] 替换为 -A[i]，然后总共重复这个过程 K 次。（我们可以多次选择同一个索引 i。） 以这种方式修改数组后，返回数组可能的最大和。 func largestSumAfterKNegations(nums []int, K int) int { sort.Slice(nums, func(i, j int) bool { return math.Abs(float64(nums[i])) \u003e math.Abs(float64(nums[j])) }) for i := 0; i \u003c len(nums); i++ { if K \u003e 0 \u0026\u0026 nums[i] \u003c 0 { nums[i] = -nums[i] K-- } } if K%2 == 1 { nums[len(nums)-1] = -nums[len(nums)-1] } result := 0 for i := 0; i \u003c len(nums); i++ { result += nums[i] } return result } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:7","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"加油站 在一条环路上有 N 个加油站，其中第 i 个加油站有汽油 gas[i] 升。 你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。 如果你可以绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1。 如果题目有解，该答案即为唯一答案。 输入数组均为非空数组，且长度相同。 输入数组中的元素均为非负数。 func canCompleteCircuit(gas []int, cost []int) int { curSum := 0 totalSum := 0 start := 0 for i := 0; i \u003c len(gas); i++ { curSum += gas[i] - cost[i] totalSum += gas[i] - cost[i] if curSum \u003c 0 { start = i+1 curSum = 0 } } if totalSum \u003c 0 { return -1 } return start } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:8","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"分发糖果 老师想给孩子们分发糖果，有 N 个孩子站成了一条直线，老师会根据每个孩子的表现，预先给他们评分。 你需要按照以下要求，帮助老师给这些孩子分发糖果： 每个孩子至少分配到 1 个糖果。 相邻的孩子中，评分高的孩子必须获得更多的糖果。 那么这样下来，老师至少需要准备多少颗糖果呢？ 示例 1: 输入: [1,0,2] 输出: 5 解释: 你可以分别给这三个孩子分发 2、1、2 颗糖果。 示例 2: 输入: [1,2,2] 输出: 4 解释: 你可以分别给这三个孩子分发 1、2、1 颗糖果。第三个孩子只得到 1 颗糖果，这已满足上述两个条件。 func candy(ratings []int) int { /**先确定一边，再确定另外一边 1.先从左到右，当右边的大于左边的就加1 2.再从右到左，当左边的大于右边的就再加1 **/ need:=make([]int,len(ratings)) sum:=0 //初始化(每个人至少一个糖果) for i:=0;i\u003clen(ratings);i++{ need[i]=1 } //1.先从左到右，当右边的大于左边的就加1 for i:=0;i\u003clen(ratings)-1;i++{ if ratings[i]\u003cratings[i+1]{ need[i+1]=need[i]+1 } } //2.再从右到左，当左边的大于右边的就右边加1，但要花费糖果最少，所以需要做下判断 for i:=len(ratings)-1;i\u003e0;i--{ if ratings[i-1]\u003eratings[i]{ need[i-1]=findMax(need[i-1],need[i]+1) } } //计算总共糖果 for i:=0;i\u003clen(ratings);i++{ sum+=need[i] } return sum } func findMax(num1 int ,num2 int) int{ if num1\u003enum2{ return num1 } return num2 } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:9","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"柠檬水找零 在柠檬水摊上，每一杯柠檬水的售价为 5 美元。 顾客排队购买你的产品，（按账单 bills 支付的顺序）一次购买一杯。 每位顾客只买一杯柠檬水，然后向你付 5 美元、10 美元或 20 美元。你必须给每个顾客正确找零，也就是说净交易是每位顾客向你支付 5 美元。 注意，一开始你手头没有任何零钱。 如果你能给每位顾客正确找零，返回 true ，否则返回 false 。 示例 1： 输入：[5,5,5,10,20] 输出：true 解释： 前 3 位顾客那里，我们按顺序收取 3 张 5 美元的钞票。 第 4 位顾客那里，我们收取一张 10 美元的钞票，并返还 5 美元。 第 5 位顾客那里，我们找还一张 10 美元的钞票和一张 5 美元的钞票。 由于所有客户都得到了正确的找零，所以我们输出 true。 示例 2： 输入：[5,5,10] 输出：true func lemonadeChange(bills []int) bool { //left表示还剩多少 下标0位5元的个数 ，下标1为10元的个数 left:=[2]int{0,0} //第一个元素不为5，直接退出 if bills[0]!=5{ return false } for i:=0;i\u003clen(bills);i++{ //先统计5元和10元的个数 if bills[i]==5{ left[0]+=1 } if bills[i]==10{ left[1]+=1 } //接着处理找零的 tmp:=bills[i]-5 if tmp==5{ if left[0]\u003e0{ left[0]-=1 }else { return false } } if tmp==15{ if left[1]\u003e0\u0026\u0026left[0]\u003e0{ left[0]-=1 left[1]-=1 }else if left[1]==0\u0026\u0026left[0]\u003e2{ left[0]-=3 }else{ return false } } } return true } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:10","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"根据身高重建队列 假设有打乱顺序的一群人站成一个队列，数组 people 表示队列中一些人的属性（不一定按顺序）。每个 people[i] = [hi, ki] 表示第 i 个人的身高为 hi ，前面 正好 有 ki 个身高大于或等于 hi 的人。 请你重新构造并返回输入数组 people 所表示的队列。返回的队列应该格式化为数组 queue ，其中 queue[j] = [hj, kj] 是队列中第 j 个人的属性（queue[0] 是排在队列前面的人）。 示例 1： 输入：people = [[7,0],[4,4],[7,1],[5,0],[6,1],[5,2]] 输出：[[5,0],[7,0],[5,2],[6,1],[4,4],[7,1]] 解释： 编号为 0 的人身高为 5 ，没有身高更高或者相同的人排在他前面。 编号为 1 的人身高为 7 ，没有身高更高或者相同的人排在他前面。 编号为 2 的人身高为 5 ，有 2 个身高更高或者相同的人排在他前面，即编号为 0 和 1 的人。 编号为 3 的人身高为 6 ，有 1 个身高更高或者相同的人排在他前面，即编号为 1 的人。 编号为 4 的人身高为 4 ，有 4 个身高更高或者相同的人排在他前面，即编号为 0、1、2、3 的人。 编号为 5 的人身高为 7 ，有 1 个身高更高或者相同的人排在他前面，即编号为 1 的人。 因此 [[5,0],[7,0],[5,2],[6,1],[4,4],[7,1]] 是重新构造后的队列。 func reconstructQueue(people [][]int) [][]int { //先将身高从大到小排序，确定最大个子的相对位置 sort.Slice(people,func(i,j int)bool{ if people[i][0]==people[j][0]{ return people[i][1]\u003cpeople[j][1]//这个才是当身高相同时，将K按照从小到大排序 } return people[i][0]\u003epeople[j][0]//这个只是确保身高按照由大到小的顺序来排，并不确定K是按照从小到大排序的 }) //再按照K进行插入排序，优先插入K小的 result := make([][]int, 0) for _, info := range people { result = append(result, info) copy(result[info[1] +1:], result[info[1]:])//将插入位置之后的元素后移动一位（意思是腾出空间） result[info[1]] = info//将插入元素位置插入元素 } return result } //链表法 func reconstructQueue(people [][]int) [][]int { sort.Slice(people,func (i,j int) bool { if people[i][0]==people[j][0]{ return people[i][1]\u003cpeople[j][1]//当身高相同时，将K按照从小到大排序 } //先将身高从大到小排序，确定最大个子的相对位置 return people[i][0]\u003epeople[j][0] }) l:=list.New()//创建链表 for i:=0;i\u003clen(people);i++{ position:=people[i][1] mark:=l.PushBack(people[i])//插入元素 e:=l.Front() for position!=0{//获取相对位置 position-- e=e.Next() } l.MoveBefore(mark,e)//移动位置 } res:=[][]int{} for e:=l.Front();e!=nil;e=e.Next(){ res=append(res,e.Value.([]int)) } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:11","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"用最少数量的箭引爆气球 在二维空间中有许多球形的气球。对于每个气球，提供的输入是水平方向上，气球直径的开始和结束坐标。由于它是水平的，所以纵坐标并不重要，因此只要知道开始和结束的横坐标就足够了。开始坐标总是小于结束坐标。 一支弓箭可以沿着 x 轴从不同点完全垂直地射出。在坐标 x 处射出一支箭，若有一个气球的直径的开始和结束坐标为 xstart，xend， 且满足 xstart ≤ x ≤ xend，则该气球会被引爆。可以射出的弓箭的数量没有限制。 弓箭一旦被射出之后，可以无限地前进。我们想找到使得所有气球全部被引爆，所需的弓箭的最小数量。 给你一个数组 points ，其中 points [i] = [xstart,xend] ，返回引爆所有气球所必须射出的最小弓箭数。 示例 1： 输入：points = [[10,16],[2,8],[1,6],[7,12]] 输出：2 解释：对于该样例，x = 6 可以射爆 [2,8],[1,6] 两个气球，以及 x = 11 射爆另外两个气球 示例 2： 输入：points = [[1,2],[3,4],[5,6],[7,8]] 输出：4 示例 3： 输入：points = [[1,2],[2,3],[3,4],[4,5]] 输出：2 示例 4： 输入：points = [[1,2]] 输出：1 示例 5： 输入：points = [[2,3],[2,3]] 输出：1 提示： 0 \u003c= points.length \u003c= 10^4 points[i].length == 2 -2^31 \u003c= xstart \u003c xend \u003c= 2^31 - 1 func findMinArrowShots(points [][]int) int { var res int =1//弓箭数 //先按照第一位排序 sort.Slice(points,func (i,j int) bool{ return points[i][0]\u003cpoints[j][0] }) for i:=1;i\u003clen(points);i++{ if points[i-1][1]\u003cpoints[i][0]{//如果前一位的右边界小于后一位的左边界，则一定不重合 res++ }else{ points[i][1] = min(points[i - 1][1], points[i][1]); // 更新重叠气球最小右边界,覆盖该位置的值，留到下一步使用 } } return res } func min(a,b int) int{ if a\u003eb{ return b } return a } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:12","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"无重叠区间 给定一个区间的集合，找到需要移除区间的最小数量，使剩余区间互不重叠。 注意: 可以认为区间的终点总是大于它的起点。 区间 [1,2] 和 [2,3] 的边界相互“接触”，但没有相互重叠。 示例 1: 输入: [ [1,2], [2,3], [3,4], [1,3] ] 输出: 1 解释: 移除 [1,3] 后，剩下的区间没有重叠。 func eraseOverlapIntervals(intervals [][]int) int { var flag int //先排序 sort.Slice(intervals,func(i,j int)bool{ return intervals[i][0]\u003cintervals[j][0] }) fmt.Println(intervals) for i:=1;i\u003clen(intervals);i++{ if intervals[i-1][1]\u003eintervals[i][0]{ flag++ intervals[i][1]=min(intervals[i-1][1],intervals[i][1])//由于是先排序的，所以，第一位是递增顺序，故只需要将临近两个元素的第二个值最小值更新到该元素的第二个值即可作之后的判断 } } return flag } func min(a,b int)int{ if a\u003eb{ return b } return a } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:13","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"划分字母区间 字符串 S 由小写字母组成。我们要把这个字符串划分为尽可能多的片段，同一字母最多出现在一个片段中。返回一个表示每个字符串片段的长度的列表。 示例： 输入：S = “ababcbacadefegdehijhklij” 输出：[9,7,8] 解释： 划分结果为 “ababcbaca”, “defegde”, “hijhklij”。 每个字母最多出现在一个片段中。 像 “ababcbacadefegde”, “hijhklij” 的划分是错误的，因为划分的片段数较少。 func partitionLabels(s string) []int { var res []int; var marks [26]int; size, left, right := len(s), 0, 0; for i := 0; i \u003c size; i++ { marks[s[i] - 'a'] = i; } for i := 0; i \u003c size; i++ { right = max(right, marks[s[i] - 'a']); if i == right { res = append(res, right - left + 1); left = i + 1; } } return res; } func max(a, b int) int { if a \u003c b { a = b; } return a; } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:14","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"合并区间 给出一个区间的集合，请合并所有重叠的区间。 func merge(intervals [][]int) [][]int { //先从小到大排序 sort.Slice(intervals,func(i,j int)bool{ return intervals[i][0]\u003cintervals[j][0] }) //再弄重复的 for i:=0;i\u003clen(intervals)-1;i++{ if intervals[i][1]\u003e=intervals[i+1][0]{ intervals[i][1]=max(intervals[i][1],intervals[i+1][1])//赋值最大值 intervals=append(intervals[:i+1],intervals[i+2:]...) i-- } } return intervals } func max(a,b int)int{ if a\u003eb{ return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:15","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"单调递增的数字 给定一个非负整数 N，找出小于或等于 N 的最大的整数，同时这个整数需要满足其各个位数上的数字是单调递增 示例： 输入: N = 332 输出: 299 func monotoneIncreasingDigits(N int) int { s := strconv.Itoa(N)//将数字转为字符串，方便使用下标 ss := []byte(s)//将字符串转为byte数组，方便更改。 n := len(ss) if n \u003c= 1 { return N } for i:=n-1 ; i\u003e0; i-- { if ss[i-1] \u003e ss[i] {//前一个大于后一位,前一位减1，后面的全部置为9 ss[i-1] -= 1 for j := i ; j \u003c n; j++ {//后面的全部置为9 ss[j] = '9' } } } res, _ := strconv.Atoi(string(ss)) return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:16","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"买卖股票的最佳时机含手续费 给定一个整数数组 prices，其中第 i 个元素代表了第 i 天的股票价格 ；非负整数 fee 代表了交易股票的手续费用。 你可以无限次地完成交易，但是你每笔交易都需要付手续费。如果你已经购买了一个股票，在卖出它之前你就不能再继续购买股票了。 返回获得利润的最大值。 注意：这里的一笔交易指买入持有并卖出股票的整个过程，每笔交易你只需要为支付一次手续费。 示例 1: 输入: prices = [1, 3, 2, 8, 4, 9], fee = 2 输出: 8 解释: 能够达到的最大利润: 在此处买入 prices[0] = 1 在此处卖出 prices[3] = 8 在此处买入 prices[4] = 4 在此处卖出 prices[5] = 9 总利润: ((8 - 1) - 2) + ((9 - 4) - 2) = 8. 贪心或者dp func maxProfit(prices []int, fee int) int { var minBuy int = prices[0] //第一天买入 var res int for i:=0;i\u003clen(prices);i++{ //如果当前价格小于最低价，则在此处买入 if prices[i]\u003cminBuy{ minBuy=prices[i] } //如果以当前价格卖出亏本，则不卖，继续找下一个可卖点 if prices[i]\u003e=minBuy\u0026\u0026prices[i]-fee-minBuy\u003c=0{ continue } //可以售卖了 if prices[i]\u003eminBuy+fee{ //累加每天的收益 res+=prices[i]-minBuy-fee //更新最小值（如果还在收获利润的区间里，表示并不是真正的卖出，而计算利润每次都要减去手续费，所以要让minBuy = prices[i] - fee;，这样在明天收获利润的时候，才不会多减一次手续费！） minBuy=prices[i]-fee } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:17","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"监控二叉树 给定一个二叉树，我们在树的节点上安装摄像头。 节点上的每个摄影头都可以监视其父对象、自身及其直接子对象。 计算监控树的所有节点所需的最小摄像头数量。 const inf = math.MaxInt64 / 2 func minCameraCover(root *TreeNode) int { var dfs func(*TreeNode) (a, b, c int) dfs = func(node *TreeNode) (a, b, c int) { if node == nil { return inf, 0, 0 } lefta, leftb, leftc := dfs(node.Left) righta, rightb, rightc := dfs(node.Right) a = leftc + rightc + 1 b = min(a, min(lefta+rightb, righta+leftb)) c = min(a, leftb+rightb) return } _, ans, _ := dfs(root) return ans } func min(a, b int) int { if a \u003c= b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:9:18","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"动态规划 有很多重叠子问题，优先考虑使用动态规划。 与贪心的区别：贪心不会考虑之前的状态而只考虑局部最优。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:0","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"理论基础 dp步骤： 确定dp数组（dp table）以及下标的含义 确定递推公式 dp数组如何初始化 确定遍历顺序 举例推导dp数组 debug:把dp数组打印出来 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:1","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"斐波那契数 func fib(n int) int { if n \u003c 2 { return n } a, b, c := 0, 1, 0 for i := 1; i \u003c n; i++ { c = a + b a, b = b, c } return c } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:2","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"爬楼梯 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： 输入： 2 输出： 2 解释： 有两种方法可以爬到楼顶。 1 阶 + 1 阶 2 阶 func climbStairs(n int) int { if n==1{ return 1 } dp:=make([]int,n+1) dp[1]=1 dp[2]=2 for i:=3;i\u003c=n;i++{ dp[i]=dp[i-1]+dp[i-2] } return dp[n] } 使用最小花费爬楼梯 数组的每个下标作为一个阶梯，第 i 个阶梯对应着一个非负数的体力花费值 cost[i]（下标从 0 开始）。 每当你爬上一个阶梯你都要花费对应的体力值，一旦支付了相应的体力值，你就可以选择向上爬一个阶梯或者爬两个阶梯。 请你找出达到楼层顶部的最低花费。在开始时，你可以选择从下标为 0 或 1 的元素作为初始阶梯。 示例 1： 输入：cost = [10, 15, 20] 输出：15 解释：最低花费是从 cost[1] 开始，然后走两步即可到阶梯顶，一共花费 15 func minCostClimbingStairs(cost []int) int { dp := make([]int, len(cost)) dp[0], dp[1] = cost[0], cost[1] for i := 2; i \u003c len(cost); i++ { dp[i] = min(dp[i-1], dp[i-2]) + cost[i] } return min(dp[len(cost)-1], dp[len(cost)-2]) } func min(a, b int) int { if a \u003c b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:3","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"不同路径 I 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。 问总共有多少条不同的路径？ func uniquePaths(m int, n int) int { dp := make([][]int, m) for i := range dp { dp[i] = make([]int, n) dp[i][0] = 1 } for j := 0; j \u003c n; j++ { dp[0][j] = 1 } for i := 1; i \u003c m; i++ { for j := 1; j \u003c n; j++ { dp[i][j] = dp[i-1][j] + dp[i][j-1] } } return dp[m-1][n-1] } II 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。 现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？ func uniquePathsWithObstacles(obstacleGrid [][]int) int { m,n:= len(obstacleGrid),len(obstacleGrid[0]) // 定义一个dp数组 dp := make([][]int,m) for i,_ := range dp { dp[i] = make([]int,n) } // 初始化 for i:=0;i\u003cm;i++ { // 如果是障碍物, 后面的就都是0, 不用循环了 if obstacleGrid[i][0] == 1 { break } dp[i][0]=1 } for i:=0;i\u003cn;i++ { if obstacleGrid[0][i] == 1 { break } dp[0][i]=1 } // dp数组推导过程 for i:=1;i\u003cm;i++ { for j:=1;j\u003cn;j++ { // 如果obstacleGrid[i][j]这个点是障碍物, 那么我们的dp[i][j]保持为0 if obstacleGrid[i][j] != 1 { // 否则我们需要计算当前点可以到达的路径数 dp[i][j] = dp[i-1][j]+dp[i][j-1] } } } // debug遍历dp //for i,_ := range dp { // for j,_ := range dp[i] { // fmt.Printf(\"%.2v,\",dp[i][j]) // } // fmt.Println() //} return dp[m-1][n-1] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:4","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"整数拆分 给定一个正整数 n，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回你可以获得的最大乘积。 func integerBreak(n int) int { /** 动态五部曲 1.确定dp下标及其含义 2.确定递推公式 3.确定dp初始化 4.确定遍历顺序 5.打印dp **/ dp:=make([]int,n+1) dp[1]=1 dp[2]=1 for i:=3;i\u003cn+1;i++{ for j:=1;j\u003ci-1;j++{ // i可以差分为i-j和j。由于需要最大值，故需要通过j遍历所有存在的值，取其中最大的值作为当前i的最大值，在求最大值的时候，一个是j与i-j相乘，一个是j与dp[i-j]. dp[i]=max(dp[i],max(j*(i-j),j*dp[i-j])) } } return dp[n] } func max(a,b int) int{ if a\u003eb{ return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:5","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"不同的二叉搜索树 给定一个整数 n，求以 1 … n 为节点组成的二叉搜索树有多少种？ func numTrees(n int)int{ dp:=make([]int,n+1) dp[0]=1 for i:=1;i\u003c=n;i++{ for j:=1;j\u003c=i;j++{ dp[i]+=dp[j-1]*dp[i-j] } } return dp[n] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:6","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"0-1背包理论基础 I 暴力的解法是指数级别的时间复杂度。进而才需要动态规划的解法来进行优化 代码随想录详解 func test_2_wei_bag_problem1(weight, value []int, bagweight int) int { // 定义dp数组 dp := make([][]int, len(weight)) for i, _ := range dp { dp[i] = make([]int, bagweight+1) } // 初始化 for j := bagweight; j \u003e= weight[0]; j-- { dp[0][j] = dp[0][j-weight[0]] + value[0] } // 递推公式 for i := 1; i \u003c len(weight); i++ { //正序,也可以倒序 for j := weight[i];j\u003c= bagweight ; j++ { dp[i][j] = max(dp[i-1][j], dp[i-1][j-weight[i]]+value[i]) } } return dp[len(weight)-1][bagweight] } func max(a,b int) int { if a \u003e b { return a } return b } func main() { weight := []int{1,3,4} value := []int{15,20,30} test_2_wei_bag_problem1(weight,value,4) } II 代码随想录详解 func test_1_wei_bag_problem(weight, value []int, bagWeight int) int { // 定义 and 初始化 dp := make([]int,bagWeight+1) // 递推顺序 for i := 0 ;i \u003c len(weight) ; i++ { // 这里必须倒序,区别二维,因为二维dp保存了i的状态 for j:= bagWeight; j \u003e= weight[i] ; j-- { // 递推公式 dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) } } //fmt.Println(dp) return dp[bagWeight] } func max(a,b int) int { if a \u003e b { return a } return b } func main() { weight := []int{1,3,4} value := []int{15,20,30} test_1_wei_bag_problem(weight,value,4) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:7","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"分割等和子集 给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 注意: 每个数组中的元素不会超过 100 数组的大小不会超过 200 示例 1: 输入: [1, 5, 11, 5] 输出: true 解释: 数组可以分割成 [1, 5, 5] 和 [11]. 示例 2: 输入: [1, 2, 3, 5] 输出: false 解释: 数组不能分割成两个元素和相等的子集. // 分割等和子集 动态规划 // 时间复杂度O(n^2) 空间复杂度O(n) func canPartition(nums []int) bool { sum := 0 for _, num := range nums { sum += num } // 如果 nums 的总和为奇数则不可能平分成两个子集 if sum % 2 == 1 { return false } target := sum / 2 dp := make([]int, target + 1) for _, num := range nums { for j := target; j \u003e= num; j-- { if dp[j] \u003c dp[j - num] + num { dp[j] = dp[j - num] + num } } } return dp[target] == target } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:8","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最后一块石头的重量II 有一堆石头，每块石头的重量都是正整数。 每一回合，从中选出任意两块石头，然后将它们一起粉碎。假设石头的重量分别为 x 和 y，且 x \u003c= y。那么粉碎的可能结果如下： 如果 x == y，那么两块石头都会被完全粉碎； 如果 x != y，那么重量为 x 的石头将会完全粉碎，而重量为 y 的石头新重量为 y-x。 最后，最多只会剩下一块石头。返回此石头最小的可能重量。如果没有石头剩下，就返回 0。 示例： 输入：[2,7,4,1,8,1] 输出：1 解释： 组合 2 和 4，得到 2，所以数组转化为 [2,7,1,8,1]， 组合 7 和 8，得到 1，所以数组转化为 [2,1,1,1]， 组合 2 和 1，得到 1，所以数组转化为 [1,1,1]， 组合 1 和 1，得到 0，所以数组转化为 [1]，这就是最优值。 func lastStoneWeightII(stones []int) int { // 15001 = 30 * 1000 /2 +1 dp := make([]int, 15001) // 求target sum := 0 for _, v := range stones { sum += v } target := sum / 2 // 遍历顺序 for i := 0; i \u003c len(stones); i++ { for j := target; j \u003e= stones[i]; j-- { // 推导公式 dp[j] = max(dp[j], dp[j-stones[i]]+stones[i]) } } return sum - 2 * dp[target] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:9","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"目标和 给定一个非负整数数组，a1, a2, …, an, 和一个目标数，S。现在你有两个符号 + 和 -。对于数组中的任意一个整数，你都可以从 + 或 -中选择一个符号添加在前面。 返回可以使最终数组和为目标数 S 的所有添加符号的方法数。 示例： 输入：nums: [1, 1, 1, 1, 1], S: 3 输出：5 解释： -1+1+1+1+1 = 3 +1-1+1+1+1 = 3 +1+1-1+1+1 = 3 +1+1+1-1+1 = 3 +1+1+1+1-1 = 3 一共有5种方法让最终目标和为3。 可回溯可dp func findTargetSumWays(nums []int, target int) int { sum := 0 for _, v := range nums { sum += v } if target \u003e sum { return 0 } if (sum+target)%2 == 1 { return 0 } // 计算背包大小 bag := (sum + target) / 2 // 定义dp数组 dp := make([]int, bag+1) // 初始化 dp[0] = 1 // 遍历顺序 for i := 0; i \u003c len(nums); i++ { for j := bag; j \u003e= nums[i]; j-- { //推导公式 dp[j] += dp[j-nums[i]] //fmt.Println(dp) } } return dp[bag] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:10","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"一和零 给你一个二进制字符串数组 strs 和两个整数 m 和 n 。 请你找出并返回 strs 的最大子集的大小，该子集中 最多 有 m 个 0 和 n 个 1 。 如果 x 的所有元素也是 y 的元素，集合 x 是集合 y 的 子集 。 示例 1： 输入：strs = [“10”, “0001”, “111001”, “1”, “0”], m = 5, n = 3 输出：4 解释：最多有 5 个 0 和 3 个 1 的最大子集是 {“10”,“0001”,“1”,“0”} ，因此答案是 4 。 其他满足题意但较小的子集包括 {“0001”,“1”} 和 {“10”,“1”,“0”} 。{“111001”} 不满足题意，因为它含 4 个 1 ，大于 n 的值 3 。 func findMaxForm(strs []string, m int, n int) int { // 定义数组 dp := make([][]int, m+1) for i,_ := range dp { dp[i] = make([]int, n+1 ) } // 遍历 for i:=0;i\u003clen(strs);i++ { zeroNum,oneNum := 0 , 0 //计算0,1 个数 //或者直接strings.Count(strs[i],\"0\") for _,v := range strs[i] { if v == '0' { zeroNum++ } } oneNum = len(strs[i])-zeroNum // 从后往前 遍历背包容量 for j:= m ; j \u003e= zeroNum;j-- { for k:=n ; k \u003e= oneNum;k-- { // 推导公式 dp[j][k] = max(dp[j][k],dp[j-zeroNum][k-oneNum]+1) } } //fmt.Println(dp) } return dp[m][n] } func max(a,b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:11","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"完全背包理论基础 每件物品都有无限个（也就是可以放入背包多次） // test_CompletePack1 先遍历物品, 在遍历背包 func test_CompletePack1(weight, value []int, bagWeight int) int { // 定义dp数组 和初始化 dp := make([]int, bagWeight+1) // 遍历顺序 for i := 0; i \u003c len(weight); i++ { // 正序会多次添加 value[i] for j := weight[i]; j \u003c= bagWeight; j++ { // 推导公式 dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) // debug //fmt.Println(dp) } } return dp[bagWeight] } // test_CompletePack2 先遍历背包, 在遍历物品 func test_CompletePack2(weight, value []int, bagWeight int) int { // 定义dp数组 和初始化 dp := make([]int, bagWeight+1) // 遍历顺序 // j从0 开始 for j := 0; j \u003c= bagWeight; j++ { for i := 0; i \u003c len(weight); i++ { if j \u003e= weight[i] { // 推导公式 dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) } // debug //fmt.Println(dp) } } return dp[bagWeight] } func max(a, b int) int { if a \u003e b { return a } return b } func main() { weight := []int{1, 3, 4} price := []int{15, 20, 30} fmt.Println(test_CompletePack1(weight, price, 4)) fmt.Println(test_CompletePack2(weight, price, 4)) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:12","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"零钱兑换II 给定不同面额的硬币和一个总金额。写出函数来计算可以凑成总金额的硬币组合数。假设每一种面额的硬币有无限个。 示例 1: 输入: amount = 5, coins = [1, 2, 5] 输出: 4 解释: 有四种方式可以凑成总金额: 5=5 5=2+2+1 5=2+1+1+1 5=1+1+1+1+1 func change(amount int, coins []int) int { // 定义dp数组 dp := make([]int, amount+1) // 初始化,0大小的背包, 当然是不装任何东西了, 就是1种方法 dp[0] = 1 // 遍历顺序 // 遍历物品 for i := 0 ;i \u003c len(coins);i++ { // 遍历背包 for j:= coins[i] ; j \u003c= amount ;j++ { // 推导公式 dp[j] += dp[j-coins[i]] } } return dp[amount] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:13","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"组合总和（IV） 给定一个由正整数组成且不存在重复数字的数组，找出和为给定目标正整数的组合的个数。 示例: nums = [1, 2, 3] target = 4 所有可能的组合为： (1, 1, 1, 1) (1, 1, 2) (1, 2, 1) (1, 3) (2, 1, 1) (2, 2) (3, 1) 请注意，顺序不同的序列被视作不同的组合。 因此输出为 7 func combinationSum4(nums []int, target int) int { //定义dp数组 dp := make([]int, target+1) // 初始化 dp[0] = 1 // 遍历顺序, 先遍历背包,再循环遍历物品 for j:=0;j\u003c=target;j++ { for i:=0 ;i \u003c len(nums);i++ { if j \u003e= nums[i] { dp[j] += dp[j-nums[i]] } } } return dp[target] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:14","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"爬楼梯（进阶） 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： 输入： 2 输出： 2 解释： 有两种方法可以爬到楼顶。 1 阶 + 1 阶 2 阶 func climbStairs(n int) int { //定义 dp := make([]int, n+1) //初始化 dp[0] = 1 // 本题物品只有两个1,2 m := 2 // 遍历顺序 for j := 1; j \u003c= n; j++ { //先遍历背包 for i := 1; i \u003c= m; i++ { //再遍历物品 if j \u003e= i { dp[j] += dp[j-i] } //fmt.Println(dp) } } return dp[n] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:15","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"零钱兑换 给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。 你可以认为每种硬币的数量是无限的。 示例 1： 输入：coins = [1, 2, 5], amount = 11 输出：3 解释：11 = 5 + 5 + 1 // 版本一, 先遍历物品,再遍历背包 func coinChange1(coins []int, amount int) int { dp := make([]int, amount+1) // 初始化dp[0] dp[0] = 0 // 初始化为math.MaxInt32 for j := 1; j \u003c= amount; j++ { dp[j] = math.MaxInt32 } // 遍历物品 for i := 0; i \u003c len(coins); i++ { // 遍历背包 for j := coins[i]; j \u003c= amount; j++ { if dp[j-coins[i]] != math.MaxInt32 { // 推导公式 dp[j] = min(dp[j], dp[j-coins[i]]+1) //fmt.Println(dp,j,i) } } } // 没找到能装满背包的, 就返回-1 if dp[amount] == math.MaxInt32 { return -1 } return dp[amount] } // 版本二,先遍历背包,再遍历物品 func coinChange2(coins []int, amount int) int { dp := make([]int, amount+1) // 初始化dp[0] dp[0] = 0 // 遍历背包,从1开始 for j := 1; j \u003c= amount; j++ { // 初始化为math.MaxInt32 dp[j] = math.MaxInt32 // 遍历物品 for i := 0; i \u003c len(coins); i++ { if j \u003e= coins[i] \u0026\u0026 dp[j-coins[i]] != math.MaxInt32 { // 推导公式 dp[j] = min(dp[j], dp[j-coins[i]]+1) //fmt.Println(dp) } } } // 没找到能装满背包的, 就返回-1 if dp[amount] == math.MaxInt32 { return -1 } return dp[amount] } func min(a, b int) int { if a \u003c b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:16","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"完全平方数 给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, …）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。 给你一个整数 n ，返回和为 n 的完全平方数的 最少数量 。 完全平方数 是一个整数，其值等于另一个整数的平方；换句话说，其值等于一个整数自乘的积。例如，1、4、9 和 16 都是完全平方数，而 3 和 11 不是。 示例 1： 输入：n = 12 输出：3 解释：12 = 4 + 4 + 4 // 版本一,先遍历物品, 再遍历背包 func numSquares1(n int) int { //定义 dp := make([]int, n+1) // 初始化 dp[0] = 0 for i := 1; i \u003c= n; i++ { dp[i] = math.MaxInt32 } // 遍历物品 for i := 1; i \u003c= n; i++ { // 遍历背包 for j := i*i; j \u003c= n; j++ { dp[j] = min(dp[j], dp[j-i*i]+1) } } return dp[n] } // 版本二,先遍历背包, 再遍历物品 func numSquares2(n int) int { //定义 dp := make([]int, n+1) // 初始化 dp[0] = 0 // 遍历背包 for j := 1; j \u003c= n; j++ { //初始化 dp[j] = math.MaxInt32 // 遍历物品 for i := 1; i \u003c= n; i++ { if j \u003e= i*i { dp[j] = min(dp[j], dp[j-i*i]+1) } } } return dp[n] } func min(a, b int) int { if a \u003c b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:17","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"单词拆分 给定一个非空字符串 s 和一个包含非空单词的列表 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。 说明： 拆分时可以重复使用字典中的单词。 你可以假设字典中没有重复的单词。 示例 1： 输入: s = “leetcode”, wordDict = [“leet”, “code”] 输出: true 解释: 返回 true 因为 “leetcode” 可以被拆分成 “leet code”。 func wordBreak(s string,wordDict []string) bool { wordDictSet:=make(map[string]bool) for _,w:=range wordDict{ wordDictSet[w]=true } dp:=make([]bool,len(s)+1) dp[0]=true for i:=1;i\u003c=len(s);i++{ for j:=0;j\u003ci;j++{ if dp[j]\u0026\u0026 wordDictSet[s[j:i]]{ dp[i]=true break } } } return dp[len(s)] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:18","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"打家劫舍 I 你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。 示例 1： 输入：[1,2,3,1] 输出：4 解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 func rob(nums []int) int { if len(nums)\u003c1{ return 0 } if len(nums)==1{ return nums[0] } if len(nums)==2{ return max(nums[0],nums[1]) } dp :=make([]int,len(nums)) dp[0]=nums[0] dp[1]=max(nums[0],nums[1]) for i:=2;i\u003clen(nums);i++{ dp[i]=max(dp[i-2]+nums[i],dp[i-1]) } return dp[len(dp)-1] } func max(a, b int) int { if a\u003eb{ return a } return b } II 你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都 围成一圈 ，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警 。 给定一个代表每个房屋存放金额的非负整数数组，计算你 在不触动警报装置的情况下 ，能够偷窃到的最高金额。 示例 1： 输入：nums = [2,3,2] 输出：3 解释：你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。 // 打家劫舍Ⅱ 动态规划 // 时间复杂度O(n) 空间复杂度O(n) func rob(nums []int) int { if len(nums) == 1 { return nums[0] } if len(nums) == 2 { return max(nums[0], nums[1]) } result1 := robRange(nums, 0) result2 := robRange(nums, 1) return max(result1, result2) } // 偷盗指定的范围 func robRange(nums []int, start int) int { dp := make([]int, len(nums)) dp[1] = nums[start] for i := 2; i \u003c len(nums); i++ { dp[i] = max(dp[i - 2] + nums[i - 1 + start], dp[i - 1]) } return dp[len(nums) - 1] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:19","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"III 在上次打劫完一条街道之后和一圈房屋后，小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为“根”。 除了“根”之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。 计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。 动态规划 func rob(root *TreeNode) int { res := robTree(root) return max(res[0], res[1]) } func max(a, b int) int { if a \u003e b { return a } return b } func robTree(cur *TreeNode) []int { if cur == nil { return []int{0, 0} } // 后序遍历 left := robTree(cur.Left) right := robTree(cur.Right) // 考虑去偷当前的屋子 robCur := cur.Val + left[0] + right[0] // 考虑不去偷当前的屋子 notRobCur := max(left[0], left[1]) + max(right[0], right[1]) // **注意**顺序：0:不偷，1:去偷 return []int{notRobCur, robCur} } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:20","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"买卖股票的最佳时机 I 给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。 你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。 返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。 示例 1： 输入：[7,1,5,3,6,4] 输出：5 解释：在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。**注意**利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格；同时，你不能在买入前卖出股票。 func maxProfit(prices []int) int { length:=len(prices) if length==0{return 0} dp:=make([][]int,length) for i:=0;i\u003clength;i++{ dp[i]=make([]int,2) } dp[0][0]=-prices[0] dp[0][1]=0 for i:=1;i\u003clength;i++{ dp[i][0]=max(dp[i-1][0],-prices[i]) dp[i][1]=max(dp[i-1][1],dp[i-1][0]+prices[i]) } return dp[length-1][1] } func max(a,b int)int { if a\u003eb{ return a } return b } II 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: [7,1,5,3,6,4] 输出: 7 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4。随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 // 买卖股票的最佳时机Ⅱ 动态规划 // 时间复杂度：O(n) 空间复杂度：O(n) func maxProfit(prices []int) int { dp := make([][]int, len(prices)) status := make([]int, len(prices) * 2) for i := range dp { dp[i] = status[:2] status = status[2:] } dp[0][0] = -prices[0] for i := 1; i \u003c len(prices); i++ { dp[i][0] = max(dp[i - 1][0], dp[i - 1][1] - prices[i]) dp[i][1] = max(dp[i - 1][1], dp[i - 1][0] + prices[i]) } return dp[len(prices) - 1][1] } func max(a, b int) int { if a \u003e b { return a } return b } func maxProfit(prices []int) int { //创建数组 dp:=make([][]int,len(prices)) for i:=0;i\u003clen(prices);i++{ dp[i]=make([]int,2) } dp[0][0]=-prices[0] dp[0][1]=0 for i:=1;i\u003clen(prices);i++{ dp[i][0]=max(dp[i-1][0],dp[i-1][1]-prices[i]) dp[i][1]=max(dp[i-1][1],dp[i-1][0]+prices[i]) } return dp[len(prices)-1][1] } func max(a,b int)int{ if a\u003cb{ return b } return a } III 给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入：prices = [3,3,5,0,0,3,1,4] 输出：6 解释：在第 4 天（股票价格 = 0）的时候买入，在第 6 天（股票价格 = 3）的时候卖出，这笔交易所能获得利润 = 3-0 = 3 。随后，在第 7 天（股票价格 = 1）的时候买入，在第 8 天 （股票价格 = 4）的时候卖出，这笔交易所能获得利润 = 4-1 = 3。 func maxProfit(prices []int) int { dp:=make([][]int,len(prices)) for i:=0;i\u003clen(prices);i++{ dp[i]=make([]int,5) } dp[0][0]=0 dp[0][1]=-prices[0] dp[0][2]=0 dp[0][3]=-prices[0] dp[0][4]=0 for i:=1;i\u003clen(prices);i++{ dp[i][0]=dp[i-1][0] dp[i][1]=max(dp[i-1][1],dp[i-1][0]-prices[i]) dp[i][2]=max(dp[i-1][2],dp[i-1][1]+prices[i]) dp[i][3]=max(dp[i-1][3],dp[i-1][2]-prices[i]) dp[i][4]=max(dp[i-1][4],dp[i-1][3]+prices[i]) } return dp[len(prices)-1][4] } func max(a,b int)int{ if a\u003eb{ return a } return b } IV 给定一个整数数组 prices ，它的第 i 个元素 prices[i] 是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 k 笔交易。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1： 输入：k = 2, prices = [2,4,1] 输出：2 解释：在第 1 天 (股票价格 = 2) 的时候买入，在第 2 天 (股票价格 = 4) 的时候卖出，这笔交易所能获得利润 = 4-2 = 2。 版本一： // 买卖股票的最佳时机IV 动态规划 // 时间复杂度O(kn) 空间复杂度O(kn) func maxProfit(k int, prices []int) int { if k == 0 || len(prices) == 0 { return 0 } dp := make([][]int, len(prices)) status := make([]int, (2 * k + 1) * len(prices)) for i := range dp { dp[i] = status[:2 * k + 1] status = status[2 * k + 1:] } for j := 1; j \u003c 2 * k; j += 2 { dp[0][j] = -prices[0] } for i := 1; i \u003c len(prices); i++ { for j := 0; j \u003c 2 * k; j += 2 { dp[i][j + 1] = max(dp[i - 1][j + 1], dp[i - 1][j] - prices[i]) dp[i][j + 2] = max(dp[i - 1][j + 2], dp[i - 1][j + 1] + prices[i]) } } return dp[len(prices) - 1][2 * k] } func max(a, b int) int { if a \u003e b { return a } return b } func maxProfit(k int, prices []int) int { if len(prices)==0{ return 0 } dp:=make([][]int,len(prices)) for i:=0;i\u003clen(prices);i++{ dp[i]=make([]int,2*k+1) } for i:=1;i\u003clen(dp[0]);i++{ if i%2!=0{ dp[0][i]=-prices[0] } } for i:=1;i\u003clen(prices);i++{ dp[i][0]=dp[i-1][0] for j:=1;j\u003clen(dp[0]);j++{ if j%2!=0{ dp[i][j]=max(dp[i-1][j],dp[i-1][j-1]-prices[i]) }else { dp[i][j]=max(dp[i-1][j],dp[i-1][j-1]+prices[i]) } } } return dp[len(prices)-1][2*k] } func max(a,b int)int{ if a\u003eb{ return a } return b } 最佳买卖股票时机含冷冻期 给定一个整数数组，其中第 i 个元素代表了第 i","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:21","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"股票问题总结 代码随想录 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:22","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最长上升子序列 给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 子序列是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 示例 1： 输入：nums = [10,9,2,5,3,7,101,18] 输出：4 解释：最长递增子序列是 [2,3,7,101]，因此长度为 4 。 func lengthOfLIS(nums []int ) int { dp := []int{} for _, num := range nums { if len(dp) ==0 || dp[len(dp) - 1] \u003c num { dp = append(dp, num) } else { l, r := 0, len(dp) - 1 pos := r for l \u003c= r { mid := (l + r) \u003e\u003e 1 if dp[mid] \u003e= num { pos = mid; r = mid - 1 } else { l = mid + 1 } } dp[pos] = num }//二分查找 } return len(dp) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:23","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最长连续递增序列 给定一个未经排序的整数数组，找到最长且 连续递增的子序列，并返回该序列的长度。 连续递增的子序列 可以由两个下标 l 和 r（l \u003c r）确定，如果对于每个 l \u003c= i \u003c r，都有 nums[i] \u003c nums[i + 1] ，那么子序列 [nums[l], nums[l + 1], …, nums[r - 1], nums[r]] 就是连续递增子序列。 示例 1： 输入：nums = [1,3,5,4,7] 输出：3 解释：最长连续递增序列是 [1,3,5], 长度为3。 尽管 [1,3,5,7] 也是升序的子序列, 但它不是连续的，因为 5 和 7 在原数组里被 4 隔开。 动态规划： py class Solution: def findLengthOfLCIS(self, nums: List[int]) -\u003e int: if len(nums) == 0: return 0 result = 1 dp = [1] * len(nums) for i in range(len(nums)-1): if nums[i+1] \u003e nums[i]: #连续记录 dp[i+1] = dp[i] + 1 result = max(result, dp[i+1]) return result 贪心法： py class Solution: def findLengthOfLCIS(self, nums: List[int]) -\u003e int: if len(nums) == 0: return 0 result = 1 #连续子序列最少也是1 count = 1 for i in range(len(nums)-1): if nums[i+1] \u003e nums[i]: #连续记录 count += 1 else: #不连续，count从头开始 count = 1 result = max(result, count) return result ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:24","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最长重复子数组 给两个整数数组 A 和 B ，返回两个数组中公共的、长度最长的子数组的长度。 示例： 输入： A: [1,2,3,2,1] B: [3,2,1,4,7] 输出：3 解释： 长度最长的公共子数组是 [3, 2, 1] 。 func findLength(A []int, B []int) int { m, n := len(A), len(B) res := 0 dp := make([][]int, m+1) for i := 0; i \u003c= m; i++ { dp[i] = make([]int, n+1) } for i := 1; i \u003c= m; i++ { for j := 1; j \u003c= n; j++ { if A[i-1] == B[j-1] { dp[i][j] = dp[i-1][j-1] + 1 } if dp[i][j] \u003e res { res = dp[i][j] } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:25","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最长公共子序列 给定两个字符串 text1 和 text2，返回这两个字符串的最长公共子序列的长度。 一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。 例如，“ace” 是 “abcde” 的子序列，但 “aec” 不是 “abcde” 的子序列。两个字符串的「公共子序列」是这两个字符串所共同拥有的子序列。 若这两个字符串没有公共子序列，则返回 0。 示例 1: 输入：text1 = \"abcde\", text2 = \"ace\" 输出：3 解释：最长公共子序列是 \"ace\"，它的长度为 3。 func longestCommonSubsequence(text1 string, text2 string) int { t1 := len(text1) t2 := len(text2) dp:=make([][]int,t1+1) for i:=range dp{ dp[i]=make([]int,t2+1) } for i := 1; i \u003c= t1; i++ { for j := 1; j \u003c=t2; j++ { if text1[i-1]==text2[j-1]{ dp[i][j]=dp[i-1][j-1]+1 }else{ dp[i][j]=max(dp[i-1][j],dp[i][j-1]) } } } return dp[t1][t2] } func max(a,b int)int { if a\u003eb{ return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:26","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"不相交的线 我们在两条独立的水平线上按给定的顺序写下 A 和 B 中的整数。 现在，我们可以绘制一些连接两个数字 A[i] 和 B[j] 的直线，只要 A[i] == B[j]，且我们绘制的直线不与任何其他连线（非水平线）相交。 以这种方法绘制线条，并返回我们可以绘制的最大连线数。 func maxUncrossedLines(A []int, B []int) int { m, n := len(A), len(B) dp := make([][]int, m+1) for i := range dp { dp[i] = make([]int, n+1) } for i := 1; i \u003c= len(A); i++ { for j := 1; j \u003c= len(B); j++ { if (A[i - 1] == B[j - 1]) { dp[i][j] = dp[i - 1][j - 1] + 1 } else { dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]) } } } return dp[m][n] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:27","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最大子序和 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例: 输入: [-2,1,-3,4,-1,2,1,-5,4] 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6 // solution // 1, dp // 2, 贪心 func maxSubArray(nums []int) int { n := len(nums) // 这里的dp[i] 表示，最大的连续子数组和，包含num[i] 元素 dp := make([]int,n) // 初始化，由于dp 状态转移方程依赖dp[0] dp[0] = nums[0] // 初始化最大的和 mx := nums[0] for i:=1;i\u003cn;i++ { // 这里的状态转移方程就是：求最大和 // 会面临2种情况，一个是带前面的和，一个是不带前面的和 dp[i] = max(dp[i-1]+nums[i],nums[i]) mx = max(mx,dp[i]) } return mx } func max(a,b int) int{ if a\u003eb { return a } return b } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:28","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"判断子序列 给定字符串 s 和 t ，判断 s 是否为 t 的子序列。 字符串的一个子序列是原始字符串删除一些（也可以不删除）字符而不改变剩余字符相对位置形成的新字符串。（例如，“ace\"是\"abcde\"的一个子序列，而\"aec\"不是）。 示例 1： 输入：s = “abc”, t = “ahbgdc” 输出：true 示例 2： 输入：s = “axc”, t = “ahbgdc” 输出：false func isSubsequence(s string, t string) bool { dp := make([][]int,len(s)+1) for i:=0;i\u003clen(dp);i++{ dp[i] = make([]int,len(t)+1) } for i:=1;i\u003clen(dp);i++{ for j:=1;j\u003clen(dp[i]);j++{ if s[i-1] == t[j-1]{ dp[i][j] = dp[i-1][j-1] +1 }else{ dp[i][j] = dp[i][j-1] } } } return dp[len(s)][len(t)]==len(s) } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:29","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"不同的子序列 给定一个字符串 s 和一个字符串 t ，计算在 s 的子序列中 t 出现的个数。 字符串的一个 子序列 是指，通过删除一些（也可以不删除）字符且不干扰剩余字符相对位置所组成的新字符串。（例如，“ACE” 是 “ABCDE” 的一个子序列，而 “AEC” 不是） 题目数据保证答案符合 32 位带符号整数范围。 func numDistinct(s string, t string) int { dp:= make([][]int,len(s)+1) for i:=0;i\u003clen(dp);i++{ dp[i] = make([]int,len(t)+1) } // 初始化 for i:=0;i\u003clen(dp);i++{ dp[i][0] = 1 } // dp[0][j] 为 0，默认值，因此不需要初始化 for i:=1;i\u003clen(dp);i++{ for j:=1;j\u003clen(dp[i]);j++{ if s[i-1] == t[j-1]{ dp[i][j] = dp[i-1][j-1] + dp[i-1][j] }else{ dp[i][j] = dp[i-1][j] } } } return dp[len(dp)-1][len(dp[0])-1] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:30","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"两个字符串的删除操作 给定两个单词 word1 和 word2，找到使得 word1 和 word2 相同所需的最小步数，每步可以删除任意一个字符串中的一个字符。 示例： 输入: \"sea\", \"eat\" 输出: 2 解释: 第一步将\"sea\"变为\"ea\"，第二步将\"eat\"变为\"ea\" ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:31","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"编辑距离 给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符 删除一个字符 替换一个字符 示例 1： 输入：word1 = “horse”, word2 = “ros” 输出：3 解释： horse -\u003e rorse (将 ‘h’ 替换为 ‘r’) rorse -\u003e rose (删除 ‘r’) rose -\u003e ros (删除 ‘e’) func minDistance(word1 string, word2 string) int { m, n := len(word1), len(word2) dp := make([][]int, m+1) for i := range dp { dp[i] = make([]int, n+1) } for i := 0; i \u003c m+1; i++ { dp[i][0] = i // word1[i] 变成 word2[0], 删掉 word1[i], 需要 i 部操作 } for j := 0; j \u003c n+1; j++ { dp[0][j] = j // word1[0] 变成 word2[j], 插入 word1[j]，需要 j 部操作 } for i := 1; i \u003c m+1; i++ { for j := 1; j \u003c n+1; j++ { if word1[i-1] == word2[j-1] { dp[i][j] = dp[i-1][j-1] } else { // Min(插入，删除，替换) dp[i][j] = Min(dp[i][j-1], dp[i-1][j], dp[i-1][j-1]) + 1 } } } return dp[m][n] } func Min(args ...int) int { min := args[0] for _, item := range args { if item \u003c min { min = item } } return min } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:32","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"编辑距离总结 代码随想录 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:33","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"回文子串 给定一个字符串，你的任务是计算这个字符串中有多少个回文子串。 具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被视作不同的子串。 示例 1： 输入：“abc” 输出：3 解释：三个回文子串: “a”, “b”, “c” func countSubstrings(s string) int { res:=0 dp:=make([][]bool,len(s)) for i:=0;i\u003clen(s);i++{ dp[i]=make([]bool,len(s)) } for i:=len(s)-1;i\u003e=0;i--{ for j:=i;j\u003clen(s);j++{ if s[i]==s[j]{ if j-i\u003c=1{ res++ dp[i][j]=true }else if dp[i+1][j-1]{ res++ dp[i][j]=true } } } } return res } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:34","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"最长回文子序列 给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度。可以假设 s 的最大长度为 1000 。 示例 1: 输入: “bbbab” 输出: 4 一个可能的最长回文子序列为 “bbbb”。 示例 2: 输入:“cbbd” 输出: 2 一个可能的最长回文子序列为 “bb”。 func longestPalindromeSubseq(s string) int { lenth:=len(s) dp:=make([][]int,lenth) for i:=0;i\u003clenth;i++{ for j:=0;j\u003clenth;j++{ if dp[i]==nil{ dp[i]=make([]int,lenth) } if i==j{ dp[i][j]=1 } } } for i:=lenth-1;i\u003e=0;i--{ for j:=i+1;j\u003clenth;j++{ if s[i]==s[j]{ dp[i][j]=dp[i+1][j-1]+2 }else { dp[i][j]=max(dp[i+1][j],dp[i][j-1]) } } } return dp[0][lenth-1] } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:35","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"DP总结 代码随想录 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:36","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"开篇词｜为什么大厂都爱考动态规划？ 你好，我是卢誉声，很高兴能在这个专栏与你见面，和你一起搞定动态规划。开门见山，我先做一个自我介绍。最开始，我在思科系统（Cisco Systems）工作，曾参与设计和开发了下一代视频会议系统的核心数据交换服务。我的工作涵盖了协议栈开发、微服务设计、分布式系统编配以及弹性算法设计。这段经历让我形成了一个认知：算法对设计关键服务来说十分重要，它决定了系统的稳定性、弹性以及可扩展性。后来，我加入了 Autodesk，成为了一款三维设计旗舰软件的框架和平台软件工程师。负责开发了基于大规模结构化数据的高性能搜索引擎，首次将灵活的多线程和异步框架带入产品框架层面，在原有的底层内存模型上采用了改进后的检索引擎，相较于原有的搜索功能，实现了超过 300 倍的性能提升。除此之外，我还改进并维护了用于改进用户体验的数据处理系统，在平台框架层面的工作，让我积累了大量的工程实践经验。现在，我在 Autodesk 数据平台就职，负责设计和开发大规模数据的分析、丰富化以及流化分布式服务。我发现自己的职业发展一直围绕着数据在不断前进。基于此，我常说的一句话是：“数据即是正义”。那直到今天，我的态度依然没有变。数据为媒，算法为介，而在极其重要的算法中，动态规划其实占了很大的比重。 事实上，如果你平常关注大厂面试的话，你会发现，但凡是研发岗位，无论是招聘初级还是高级工程师，大厂都倾向于安排一轮或多轮专门的算法面试环节，而且在面试环节提出动态规划相关问题的这种趋势已经愈发明显。这是为什么呢？我来谈谈我的看法。 先说算法这件事吧。我想请你回想一下，当处理数据结构相关的问题时，你有没有这样的经历？你本能地到工具函数或者库函数中寻找有没有现成的工具。如果问题得到快速解决，它是不是迅速就成了过眼云烟？如果这个问题看起来比较棘手，它不是一个典型的算法问题，那么就寻求搜索引擎的帮助，或者干脆访问 Stack Overflow 这样的“智库”寻找前人留下的解决方案？虽然平时工作中表现优异，但当你想换工作参加大厂面试时，又发现自己难以解决面试官提出的算法问题，无从下手，面对白板“望洋兴叹”？ 相信我，你不是一个人！这种现象很普遍。其实，对于开发人员来说，算法和数据结构就是我们的基本功。我们常常自嘲软件研发人员的工作就是复制粘贴，搬砖就是日常工作的全部。但当公司或部门要求你去研究一个全新的技术，或者快速阅读一份开发多年且成熟的开源项目代码，并对其改造来服务于自己的产品功能时，你的压力会让你明白基本功到底有多重要！关于基本功这事儿，我要插个故事进来，再多说几句。我曾有幸与 C++ 之父 Bjarne Stroustrup 先生进行过面对面的交流。我问了他一个问题：“如今新生代技术人员倾向于学习 Java、Go 或 Python 这些更容易上手的编程语言，您是如何看待这个现象的？”Stroustrup 先生的回答大概是这样的：“如果一个人只了解一种编程语言，那么他不能称自己是专业人士，而从我的角度上看，将 C++ 作为基础，能让你深入洞察各种各样编程语言背后的思想和设计思路。” 我作为面试官曾接触过许多优秀的候选人，他们有着各种各样的背景，既有潜力又非常努力，但在面对算法问题和解决问题时没有太多思路，始终无法更上一层楼，十分遗憾。而动态规划恰恰是解决问题的重要方法论，面对很多数据处理的应用场景，它在降低时间复杂度上极具优势，因此成为了考察重点。不仅如此，动态规划问题还能很好地考察面试者的数学模型抽象能力和逻辑思维能力，可以反应个人在算法上的综合能力。所以我觉得，大厂之所以如此看中一个面试者的算法基础，特别是动态规划问题的解决能力，是因为他们更加看中一位面试者解决问题的思路与逻辑思维能力，而不只是工具与技能的熟练程度。 不同于普通算法，如排序或递归，动态规划从名字上看就显得很特别，有些“高端、大气、上档次”的味道在里面。但其实它离我们很近。我举个例子你就明白了，在云计算平台上一个解决方案的计算能力（容量）肯定是有限的，那么为了高效服务那些重要程度或优先级最高的客户，同时又不想浪费计算资源（说白了为了省钱），我们该怎么办？这个问题其实可以通过队列这样的分发方式来进行一个简单的编配。但是这不够好，如果我们能够事先知道一个计算任务的重要程度和所需的计算时长，就可以通过动态规划算法来进行预演算，从数学角度推导出一个严谨的编排结果，实现有限资源的最大化利用。 你看，似乎遥不可及的动态规划问题，其实就是求最优解问题，它无时无刻都在我们身边，总是戏剧般地提高了最优化问题的性能！这再一次凸显出大厂为何青睐于动态规划问题，而且成为了区别面试者的一个隐形门槛。甚至可以说，掌握动态规划思想，在工作面试、技术等级晋升上都扮演了核心角色。总之一句话，动归必学。 模块一：初识动态规划我会为你讲解复杂面试题的思考和解决方式。从贪心算法开始，一步步阐述动态规划的由来，并通过一个贯穿全篇的例子来展现动态规划的强大之处。学习和掌握这些经典的处理方法，能够为你后续掌握动态规划打下一个坚实基础。通过这部分内容，你会系统了解到动态规划问题的特点和解题经验。模块二：动态规划的套路我会为你讲解动态规划问题的解题框架和套路，你可以把这个套路理解成是解决动归问题的模板。在此模板的基础上，我会向你讲解面试真题，有针对性地套用解题框架。而应对面试题的纷繁复杂，我会为你进行有效的分类，并针对每一种动态规划问题进行深入而全面的讲解。通过这部分内容，你会快速掌握常见面试题的解题套路。模块三：举一反三，突破套路我会针对几种特别易考的动态规划面试题进行总结，帮助你攻破套路。并在这些高级话题的基础上，提出设计动态规划算法的关键问题。另外，还有刷题指南，所谓孰能生巧，必要的练习我们还是要的。通过这部分内容，你会快速掌握动态规划面试题的进阶法门。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:37","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"什么样的问题应该使用动态规划？ 动态规划问题的典型特点：求“最”优解问题（最大值和最小值）、求可行性（True 或 False）、求方案总数、数据结构不可排序（Unsortable）、算法不可使用交换（Non-swappable）。 数据不可排序（Unsortable） 假设我们有一个无序数列，希望求出这个数列中最大的两个数字之和。很多初学者刚刚学完动态规划会走火入魔到看到最优化问题就想用动态规划来求解，嗯，那么这样应该也是可以的吧……不，等等，这个问题不是简单做一个排序或者做一个遍历就可以求解出来了吗？ 数据不可交换（Non-swapable） 还有一类问题，可以归类到我们总结的几类问题里去，但是不存在动态规划要求的重叠子问题（比如经典的八皇后问题），那么这类问题就无法通过动态规划求解。这种情况需要避免被套进去。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:38","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["Coding"],"content":"常见的动态规划面试题串烧 简单的路径规划、带障碍的路径规划、跳跃游戏（题目：给出一个非负整数数组 A，你最初定位在数组的第一个位置。数组中的每个元素代表你在那个位置可以跳跃的最大长度。判断你是否能到达数组的最后一个位置。） ","date":"2022-01-06 08:17:33","objectID":"/algorithm_carl/:10:39","tags":["data structure"],"title":"Algorithm_carl","uri":"/algorithm_carl/"},{"categories":["School courses"],"content":" 笔记来自：计算机体系结构基础（第三版）胡伟武等 引言 造计算机： 硬件方面：计算机组成原理和计算机体系结构 软件方面：操作系统和编译原理 由CPU和OS构建起来的平台举例： Wintel(Windows+X86Cpu) AA(Android+ARMCpu) ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:0:0","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":["School courses"],"content":"计算机体系结构研究内容 ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:1:0","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":["School courses"],"content":"按一下键盘，PPT就能翻一页？ 为了说明计算机体系结构研究涉及的领域，我们看一个很简单平常的问题：为什么我按一下键盘，PPT会翻一页？这是一个什么样的过程？在这个过程中，应用程序（WPS）、操作系统（Windows或Linux）、硬件系统、CPU、晶体管是怎么协同工作的？ 下面介绍用龙芯CPU构建的系统实现上述功能的原理性过程。 按一下键盘，键盘会产生一个信号送到南桥芯片，南桥芯片把键盘的编码保存在南桥内部的一个寄存器中，并向处理器发出一个外部中断信号。该外部中断信号传到CPU内部后把CPU中一个控制寄存器的某一位置为“1”，表示收到了外部中断。CPU中另外一个控制寄存器有屏蔽位来确定是否处理这个外部中断信号。 屏蔽处理后的中断信号被附在一条译码后的指令上送到重排序缓冲（Re-Order Buffer，简称ROB）。外部中断是例外（Exception，也称“异常”）的一种，发生例外的指令不会被送到功能部件执行。当这条指令成为重排序缓冲的第一条指令时CPU处理例外。重排序缓冲为了给操作系统一个精确的例外现场，处理例外前要把例外指令前面的指令都执行完，后面的指令都取消掉。 重排序缓冲向所有的模块发出一个取消信号，取消该指令后面的所有指令；修改控制寄存器，把系统状态设为核心态；保存例外原因、发生例外的程序计数器（Program Counter，简称PC）等到指定的控制寄存器中；然后把程序计数器的值置为相应的例外处理入口地址进行取指（LoongArch中例外的入口地址计算规则可以参见其体系结构手册）。 处理器跳转到相应的例外处理器入口后执行操作系统代码，操作系统首先保存处理器现场，包括寄存器内容等。保存现场后，操作系统向CPU的控制寄存器读例外原因，发现是外部中断例外，就向南桥的中断控制器读中断原因，读的同时清除南桥的中断位。读回来后发现中断原因是有人敲了空格键。 操作系统接下来要查找读到的空格是给谁的：有没有进程处在阻塞状态等键盘输入。大家都学过操作系统的进程调度，知道进程至少有三个状态：运行态、阻塞态、睡眠态，进程在等IO输入时处在阻塞态。操作系统发现有一个名为WPS的进程处于阻塞态，这个进程对空格键会有所响应，就把WPS唤醒。 WPS被唤醒后处在运行状态。发现操作系统传过来的数据是个键盘输入空格，表示要翻页。WPS就把下一页要显示的内容准备好，调用操作系统中的显示驱动程序，把要显示的内容送到显存，由图形处理器（Graphic Processing Unit，简称GPU）通过访问显存空间刷新屏幕。达到了翻一页的效果 再看一个问题：如果在翻页的过程中，发现翻页过程非常卡顿，即该计算机在WPS翻页时性能较低，可能是什么原因呢？首先得看看系统中有没有其他任务在运行，如果有很多任务在运行，这些任务会占用CPU、内存带宽、IO带宽等资源，使得WPS分到的资源不够，造成卡顿。如果系统中没有其他应用与WPS抢资源，还会卡顿，那是什么原因呢？多数人会认为是CPU太慢，需要升级。实际上，在WPS翻页时，CPU干的活不多。一种可能是下一页包含很多图形，尤其是很多矢量图，需要GPU画出来，GPU忙不过来了。另外一种可能是要显示的内容数据量大，要把大量数据从WPS的应用程序空间传给GPU使用的专门空间，内存带宽不足导致不能及时传输。在独立显存的情况下，数据如何从内存传输到显存有两种不同的机制：由CPU从内存读出来再写到显存需要CPU具有专门的IO加速功能，因为显存一般是映射在CPU的IO空间；不通过CPU，通过直接内存访问（Direct Memory Access，简称DMA）的方式直接从内存传输到显存会快得多。 “计算机体系结构”课程是研究怎么造计算机，而不是怎么用计算机。我们不是学习驾驶汽车，而是学习如何造汽车。一个计算机体系结构设计人员就像一个带兵打仗的将领，要学会排兵布阵。要上知天文、下知地理，否则就不会排兵布阵，或者只会纸上谈兵地排兵布阵，只能贻误军国大事。对计算机体系结构设计来说，“排兵布阵”就是体系结构设计，“上知天文”就是了解应用程序、操作系统、编译器的行为特征，“下知地理”就是了解逻辑、电路、工艺的特点。永远不要就体系结构论体系结构，要做到应用、系统、结构、逻辑、电路、器件的融会贯通。就像《论语》中说的“吾道一以贯之”。 通用计算机系统的层次结构： 该图把计算机系统分成应用程序、操作系统、硬件系统、晶体管四个大的层次。注意把这四个层次联系起来的三个界面。第一个界面是应用程序编程接口API（Application Programming Interface），也可以称作“操作系统的指令系统”，介于应用程序和操作系统之间。API是应用程序的高级语言编程接口，在编写程序的源代码时使用。常见的API包括C语言、Fortran语言、Java语言、JavaScript语言接口以及OpenGL图形编程接口等。使用一种API编写的应用程序经重新编译后可以在支持该API的不同计算机上运行。所有应用程序都是通过API编出来的，在IT产业，谁控制了API谁就控制了生态，API做得好，APP（Application）就多。API是建生态的起点。第二个界面是指令系统ISA（Instruction Set Architecture），介于操作系统和硬件系统之间。常见的指令系统包括X86、ARM、MIPS、RISC-V和LoongArch等。指令系统是实现目标码兼容的关键，由于IT产业的主要应用都是通过目标码的形态发布的，因此ISA是软件兼容的关键，是生态建设的终点。指令系统除了实现加减乘除等操作的指令外，还包括系统状态的切换、地址空间的安排、寄存器的设置、中断的传递等运行时环境的内容。第三个界面是工艺模型，介于硬件系统与晶体管之间。工艺模型是芯片生产厂家提供给芯片设计者的界面，除了表达晶体管和连线等基本参数的SPICE（Simulation Program with Integrated Circuit Emphasis）模型外，该工艺所能提供的各种IP也非常重要，如实现PCIE接口的物理层（简称PHY）等。 需要指出的是，在API和ISA之间还有一层应用程序二进制接口（Application Binary Interface，简称ABI）。ABI是应用程序访问计算机硬件及操作系统服务的接口，由计算机的用户态指令和操作系统的系统调用组成。为了实现多进程访问共享资源的安全性，处理器设有“用户态”与“核心态”。用户程序在用户态下执行，操作系统向用户程序提供具有预定功能的系统调用函数来访问只有核心态才能访问的硬件资源。当用户程序调用系统调用函数时，处理器进入核心态执行诸如访问IO设备、修改处理器状态等只有核心态才能执行的指令。处理完系统调用后，处理器返回用户态执行用户代码。相同的应用程序二进制代码可以在相同ABI的不同计算机上运行。 ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:1:1","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":["School courses"],"content":"计算机只是computer? 什么是计算机？大多数人认为计算机就是我们桌面的电脑，实际上计算机已经深入到我们信息化生活的方方面面。除了大家熟知的个人电脑、服务器和工作站等通用计算机外，像手机、数码相机、数字电视、游戏机、打印机、路由器等设备的核心部件都是计算机，都是计算机体系结构研究的范围。也许此刻你的身上就有好几台计算机。 计算机不光是桌面上摆的个人计算机，它可以大到一个厅都放不下，需要专门为它建一个电站来供电，也可以小到揣在我们的兜里，充电两个小时就能用一整天。不管这个计算机的规模有多大，都是计算机体系结构的研究对象。**计算机是为了满足人们各种不同的计算需求设计的自动化计算设备。**随着人类科技的进步和新需求的提出，最快的计算机会越来越大，最小的计算机会越来越小。 ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:1:2","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":["School courses"],"content":"计算机基本组成 计算机的组成非常复杂，但其基本单元非常简单。打开一台PC的机箱，可以发现电路板上有很多芯片。一个芯片就是一个系统，由很多模块组成，如加法器、乘法器等；而一个模块由很多逻辑门组成，如非门、与门、或门等；逻辑门由晶体管组成，如PMOS管和NMOS管等；晶体管则通过复杂的工艺过程形成。所以计算机是一个很复杂的系统，由很多可以存储和处理二进制运算的基本元件组成。就像盖房子一样，再宏伟、高大的建筑都是由基本的砖瓦、钢筋水泥等材料搭建而成的。在CPU芯片内部，一根头发的宽度可以并排走上千根导线；购买一粒大米的钱可以买上千个晶体管。 冯·诺依曼结构的基本思想：数据和程序都在存储器中，CPU从内存中取指令和数据进行运算并把结果也放到内存中。把指令和数据都存在内存中可以让计算机按照事先规定的程序自动地完成运算，是实现图灵机的一种简单方法。冯·诺依曼结构很好地解决了自动化的问题：把程序放在内存里，一条条取进来，自己就做起来了，不用人来干预。如果没有这样一种自动执行的机制，让人去控制计算机做什么运算，拨一下开关算一下，程序没有保存在内存中而是保存在人脑中，就成算盘了。计算机的发展日新月异，但70多年过去了还是使用冯·诺依曼结构。尽管冯·诺依曼结构有很多缺点，例如什么都保存在内存中使访存成为性能瓶颈，但我们还是摆脱不了它。 虽然经过了长期的发展，以存储程序和指令驱动执行为主要特点的冯·诺依曼结构仍是现代计算机的主流结构。笔者面试研究生的时候经常问一个问题：冯·诺依曼结构最核心的思想是什么？结果很多研究生都会答错。有人说是由计算器、运算器、存储器、输入、输出五个部分组成；有人说是程序计数器导致串行执行；等等。实际上，冯·诺依曼结构就是数据和程序都存在存储器中，CPU从内存中取指令和数据进行运算，并且把结果也放在内存中。概括起来就是存储程序和指令驱动执行。 ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:1:3","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":["School courses"],"content":"衡量计算机的指标 ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:2:0","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":["School courses"],"content":"性能 计算机的第一个重要指标就是性能。前面说的用来进行核模拟的高性能计算机对一个国家来说具有战略意义，算得越快越好。又如中央气象台用于天气预报的计算机每天需要根据云图数据解很复杂的偏微分方程，要是计算机太慢，明天的天气预报后天才算出来，那就叫天气后报，没用了。所以性能是计算机的首要指标。 什么叫性能？性能的最本质定义是“完成一个任务所需要的时间”。对中央气象台的台长来说，性能就是算明天的天气预报需要多长时间。如果甲计算机两个小时能算完24小时的天气预报，乙计算机一个小时就算完，显然乙的性能比甲好。完成一个任务所需要的时间可以由完成该任务需要的指令数、完成每条指令需要的拍数以及每拍需要的时间三个量相乘得到。完成任务需要的指令数与算法、编译器和指令的功能有关；每条指令需要的拍数与编译器、指令功能、微结构设计相关；每拍需要的时间，也就是时钟周期，与结构、电路设计、工艺等因素有关。 完成一个任务的指令数首先取决于算法。我们刚开始做龙芯的时候，计算所的一个老研究员讲过一个故事。说20世纪六七十年代的时候，美国的计算机每秒可以算一亿次，苏联的计算机每秒算一百万次，结果算同一个题目，苏联的计算机反而先算完，因为苏联的算法厉害。以对N个数进行排序的排序算法为例，冒泡排序算法的运算复杂度为O(NN)，快速排序算法的运算复杂度为O(Nlog2(N))，如果N为1024，则二者执行的指令数差100倍。 编译器负责把用户用高级语言（如C、Java、JavaScript等）写的代码转换成计算机硬件能识别的、由一条条指令组成的二进制码。转换出来的目标码的质量的好坏在很大程度上影响完成一个任务的指令数。在同一台计算机上运行同一个应用程序，用不同的编译器或不同的编译选项，运行时间可能有几倍的差距。 指令系统的设计对完成一个任务的指令数影响也很大。例如要不要设计一条指令直接完成一个FFT函数，还是让用户通过软件的方法来实现FFT函数，这是结构设计的一个取舍，直接影响完成一个任务的指令数。体系结构有一个常用的指标叫MIPS（Million Instructions Per Second）,即每秒执行多少百万条指令。看起来很合理的一个指标，关键是一条指令能干多少事讲不清楚。如果甲计算机一条指令就能做一个1024点的FFT，而乙计算机一条指令就算一个加法。两台计算机比MIPS值就没什么意义。因此后来有人把MIPS解释为Meaningless Indication of Processor Speed。现在常用一个性能指标MFLOPS（Million FLoating point Operations Per Second），即每秒做多少百万浮点运算，也有类似的问题。如果数据供不上，运算能力再强也没有用。 在指令系统确定后，结构设计需要重点考虑如何降低每条指令的平均执行周期（Cycles Per Instruction，简称CPI），或提高每个时钟周期平均执行的指令数（Instructions Per Cycle，简称IPC），这是处理器微结构研究的主要内容。CPI就是一个程序执行所需要的总的时钟周期数除以它所执行的总指令数，反之则是IPC。处理器的微结构设计对IPC的影响很大，采用单发射还是多发射结构，采用何种转移猜测策略以及什么样的存储层次设计都直接影响IPC。表1.2给出了龙芯3A1000和龙芯3A2000处理器运行SPEC CPU2000基准程序的分值。两个CPU均为64位四发射结构，主频均为1GHz，两个处理器运行的二进制码相同，但由于微结构不同，IPC差异很大，总体上说，3A2000的IPC是3A1000的2~3倍。 主频宏观上取决于微结构设计，微观上取决于工艺和电路设计。例如Pentium III的流水线是10级，Pentium IV为了提高主频，一发猛就把流水级做到了20级，还恨不得做到40级。Intel的研究表明，只要把Cache和转移猜测表的容量增加一倍，就能抵消流水线增加一倍引起的流水线效率降低。又如，从电路的角度来说，甲设计做64位加法只要1ns，而乙设计需要2ns，那么甲设计比乙设计主频高一倍。相同的电路设计，用不同的工艺实现出来的主频也不一样，先进工艺晶体管速度快，主频高。 可见在一个系统中不同层次有不同的性能标准，很难用一项单一指标刻画计算机性能的高低。大家可能会说，从应用的角度看性能是最合理的。甲计算机两个小时算完明天的天气预报，乙计算机只要一小时，那乙的性能肯定比甲的好，这总对吧。也对也不对。只能说，针对算明天的天气预报这个应用，乙计算机的性能比甲的好。但对于其他应用，甲的性能可能反而比乙的好。 ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:2:1","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":["School courses"],"content":"价格 计算机的第二个重要指标是价格。20世纪80年代以来电脑越来越普及，就是因为电脑的价格在不断下降，从一味地追求性能（Performance per Second）到追求性能价格比（Performance per Dollar）。现在中关村卖个人电脑的企业利润率比卖猪饲料的还低得多。 不同的计算机对成本有不同的要求。用于核模拟的超级计算机主要追求性能，一个国家只需要一两台这样的高性能计算机，不太需要考虑成本的问题。相反，大量的嵌入式应用为了降低功耗和成本，可能牺牲一部分性能，因为它要降低功耗和成本。而PC、工作站、服务器等介于两者之间，它们追求性能价格比的最优设计。 计算机的成本跟芯片成本紧密相关，计算机中芯片的成本包括该芯片的制造成本和一次性成本NRE（如研发成本）的分摊部分。生产量对于成本很关键。随着不断重复生产，工程经验和工艺水平都不断提高，生产成本可以持续地降低。例如做衣服，刚开始可能做100件就有10件是次品，以后做1000件也不会做坏1件了，衣服的总体成本就降低了。产量的提高能够加速学习过程，提高成品率，还可以降低一次性成本。 随着工艺技术的发展，为了实现相同功能所需要的硅面积指数级降低，使得单个硅片的成本指数级降低。但成本降到一定的程度就不怎么降了，甚至还会有缓慢上升的趋势，这是因为厂家为了保持利润不再生产和销售该产品，转而生产和销售升级产品。现在的计算机工业是一个不断出售升级产品的工业。买一台计算机三到五年后，就需要换一台新的计算机。CPU和操作系统厂家一起，通过一些技术手段让一般用户五年左右就需要换掉电脑。这些手段包括：控制芯片老化寿命，不再更新老版本的操作系统而新操作系统的文档格式不与老的保持兼容，发明新的应用使没有升级的计算机性能不够，等等。主流的桌面计算机CPU刚上市时价格都比较贵，然后逐渐降低，降到200美元以下，就逐步从主流市场中退出。芯片公司必须不断推出新的产品，才能保持盈利。但是总的来说，对同一款产品，成本曲线是不断降低的。 ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:2:2","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":["School courses"],"content":"功耗 计算机的第三个重要指标是功耗。手机等移动设备需要用电池供电。电池怎么用得久呢？低功耗就非常重要。高性能计算机也要低功耗，它们的功耗都以兆瓦（MW）计。兆瓦是什么概念？我们上大学时在宿舍里煮方便面用的电热棒的功率是1000W左右，几个电热棒一起用宿舍就停电了。1MW就是1000个电热棒的功率。曙光5000高性能计算机在中科院计算所的地下室组装调试时，运行一天电费就是一万多块钱，比整栋楼的电费还要高。计算机里产生功耗的地方非常多，CPU有功耗，内存条有功耗，硬盘也有功耗，最后为了把这些热量散发出去，制冷系统也要产生功耗。近几年来，性能功耗比（Performance per Watt）成为计算机非常重要的一个指标。 芯片功耗是计算机功耗的重要组成部分。芯片的功耗主要由晶体管工作产生，所以先来看晶体管的功耗组成。图1.3是一个反相器的功耗模型。反相器由一个PMOS管和一个NMOS管组成。其功耗主要可以分为三类： 开关功耗、短路功耗和漏电功耗。开关功耗主要是电容的充放电，比如当输出端从0变到1时，输出端的负载电容从不带电变为带电，有一个充电的过程；当输出端从1变到0时，电容又有一个放电的过程。在充电、放电的过程中就会产生功耗。开关功耗既和充放电电压、电容值有关，还和反相器开关频率相关。 短路功耗就是P管和N管短路时产生的功耗。当反相器的输出为1时，P管打开，N管关闭；输出为0时，则N管开，P管闭。但在开、闭的转换过程中，电流的变化并不像理论上那样是一个方波，而是有一定的斜率。在这个变化的过程中会出现N管和P管同时部分打开的情况，这时候就产生了短路功耗。 漏电功耗是指MOS管不能严格关闭时发生漏电产生的功耗。以NMOS管为例，如果栅极有电N管就导通；否则N管就关闭。但在纳米级工艺下，MOS管沟道很窄，即使栅极不加电压，源极和漏极之间也有电流；另外栅极下的绝缘层很薄，只有几个原子的厚度，从栅极到沟道也有漏电流。漏电流大小随温度升高呈指数增加，因此温度是集成电路的第一杀手。 优化芯片功耗一般从两个角度入手——动态功耗优化和静态功耗优化。升级工艺是降低动态功耗的有效方法，因为工艺升级可以降低电容和电压，从而成倍地降低动态功耗。芯片工作频率跟电压成正比，在一定范围内（如5%~10%）降低频率可以同比降低电压，因此频率降低10%，动态功耗可以降低30%左右（功耗和电压的平方成正比，和频率成正比）。可以通过选择低功耗工艺降低芯片静态功耗，集成电路生产厂家一般会提供高性能工艺和低功耗工艺，低功耗工艺速度稍慢一些但漏电功耗成数量级降低。在结构和逻辑设计时，避免不必要的逻辑翻转可以有效降低翻转率，例如在某一流水级没有有效工作时，保持该流水级为上一拍的状态不翻转。在物理设计时，可以通过门控时钟降低时钟树翻转功耗。在电路设计时，可以采用低摆幅电路降低功耗，例如工作电压为1V时，用0.4V表示逻辑0，用0.6V表示逻辑1，摆幅就只有0.2V，大大降低了动态功耗。 芯片的功耗是一个全局量，与每一个设计阶段都相关。功耗优化的层次从系统级、算法级、逻辑级、电路级，直至版图和工艺级，是一个全系统工程。近几年在降低功耗方面的研究非常多，和以前片面追求性能不同，降低功耗已经成了芯片设计一个最重要的任务。 信息产业是一个高能耗产业，信息设备耗电越来越多。根据冯·诺依曼的公式，现在一位比特翻转所耗的电是理论值的(10^{10})倍以上。整个信息的运算过程是一个从无序到有序的过程，这个过程中它的熵变小，是一个吸收能量的过程。但事实上，它真正需要的能量很少，因为我们现在用来实现运算的手段不够先进，不够好，所以才造成了(10^{10})倍这么高的能耗，因此我们还有多个数量级的优化空间。这其中需要一些原理性的革命，材料、设计上都需要很大的革新，即使目前在用的晶体管，优化空间也是很大的。 有些应用还需要考虑计算机的其他指标，例如使用寿命、安全性、可靠性等。以可靠性为例，计算机中用的CPU可以分为商用级、工业级、军品级、宇航级等。比如北斗卫星上面的计算机，价格贵点没关系，慢一点也没关系，关键是要可靠，我国放了不少卫星，有的就是由于其中的元器件不可靠报废了。因此在特定领域可靠性要求非常高。再如银行核心业务用的计算机也非常在乎可靠性，只要一年少死机一次，价格贵一千万元也没关系，对银行来说，核心计算机死机，所有的储户就取不了钱，这损失太大了。 因此考评一个计算机好坏的指标非常多。本课程作为本科计算机体系结构基础课程，在以后的章节中主要关注性能指标。 ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:2:3","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":["School courses"],"content":"体系结构设计的基本原则 计算机体系结构发展很快，但在发展过程中遵循一些基本原则，这些原则包括平衡性、局部性、并行性和虚拟化。 ","date":"2021-11-29 14:19:01","objectID":"/ca_base_01/:3:0","tags":["computer architecture"],"title":"CA_base_01","uri":"/ca_base_01/"},{"categories":null,"content":"关于我 某 c9 19级 本科学生 ","date":"2021-11-06 00:00:00","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"菜鸡的编程历程 大一接触编程，学校开设了C语言。 大一至大二为了获得创新学分，我同小组5人用JAVA开发了一个未完全成形的拍照记账APP，当时的话JAVA水平也只能算是入门级别的，没有深入去学习，我当时负责的功能需求是实现记账数据的可视化，我用 LitePal 操作数据库，并利用一个开源的图表库 MPAndroidChart，实现了期望的数据图表化。忙活了几个月，做出了下面的效果（实际截图） 无论是配安卓开发环境还是解决写程序过程中AS产生的“莫名奇妙”的bug，都十分令人头大，不过我还是比较能接受吧，毕竟当你解决掉一个“不简单”的问题的时候，成就感还是挺足的。 大二，学校有软件设计的课程，我同一位同学组队进行游戏开发，当时的设想是用cocos2dx游戏引擎做一款类似Hex War的回合制策略游戏，原游戏截图： 基本功能完成（包括：开拓视野，行军,征兵,调粮，各类资源消耗，交战等）后部分截图： 一个几千行代码，我只写了1千行不到的代码，更多的代码是队友写的，我c++水平也只能算是入门，没有深入学习细节。游戏UI是我做的，效果也就“差强人意”。 大三上，这个学期我接触到了 go 语言，经过JAVA安卓开发和c++游戏开发的双重折磨后，我立马就被 go 的简单高效的特性所吸引，从而开始学习这门语言，11月份的时候学校的数据库大作业需要自己设计数据库并实现前后端，当时有几个选题，我选了实验室软件管理系统，用PowerDesigner设计数据库结构，用Workbench进行数据库的操作与管理，用HTML写前端，用go写后端。贴一下当时的图： ER图： 登陆界面： 其他界面示例： 大三下5月份，参加了ByteDance的后端青训营，课程比较硬核，但那个月课程繁忙，而且要进行毕设导师的双选，所以落下不少青训营课程没听，青训营项目截止日期是6月中旬左右，刚好是期末考试周，因为考虑到时间紧凑，所以一开始强行组队的时候，我就不打算当组长来着，找个组协作开发。虽然最后的结果不太理想，但还是有结果。 六个人开发一个抖音后端项目，不同于以往和认识的同学协作开发，这次和陌生人组队的体验不太行（具体就不细说了），虽然说大家可能技术水平参差不齐，但对于这种入门级企业活动项目来说水平是次要的，只要肯学还是问题不大的。不过这次团队协作让我意识到：对于一个团队来说，态度、责任心是最基本的，团队交流是最重要的，团队负责人是团队最核心的，三者都做不到是万万不能的。青训营官方强迫不能个人组队的良苦用心有部分人是没有意识到的。 7月份泡在极客时间里，一两个礼拜的免费会员，我看了不少课程，然后自己也买了一些课程。包括计算机基础的，golang语言相关的，微服务相关的……不得不说，真香！唯一的不足就是极客时间的网页客户端体验感不好，网页的延迟高而且网站有时还会挂…… 回顾自己的这些项目好像都没什么亮点，然后我就有了用微服务框架重构一下抖音后端项目的想法。我先简单了解了一下，有很多优秀的 RPC 协议，例如腾讯的 Tars、阿里的 Dubbo、微博的 Motan、字节的 Kitex、Facebook 的 Thrift、RPCX 等等，然而我觉得对于我这个初学者来说，还是使用更普及的 gRPC 比较合适。 可能是因为自己急于求成了，没有好好消化知识，现在都忘了一大半了，不过好在做了笔记。 ","date":"2021-11-06 00:00:00","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"github 统计 ","date":"2021-11-06 00:00:00","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"TODO 继续学习计算机基础、go、微服务等 ","date":"2021-11-06 00:00:00","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"【Golang开发面经】米哈游（一轮游。。） ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:0:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"写在前面 米哈游 面试下来感觉还行吧，挺注重基础的，面试官水平也很高。但是感觉不是在招人的样子，我有好多同学都是简历挂（ ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:1:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"笔试 无笔试，很奇怪（ ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:2:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"一面 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:3:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"线程和协程有什么区别？各自有什么优缺点？ 进程 线程 系统中正在运行的一个应用程序 系统分配处理器时间资源的基本单元 程序一旦运行就是进程 进程之内独立执行的一个单元执行流 资源分配的最小单位 程序执行的最小单位 进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。 线程有自己的堆栈和局部变量，但线程没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些 要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:4:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"进程之间如何进行通信？ 管道、消息队列、共享内存、信号量、信号、socket ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:5:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"什么是信号，信号量是如何实现的？ 信号： 在Linux中，为了响应各种事件，提供了几十种信号，可以通过kill -l命令查看。 如果是运行在 shell终端 的进程，可以通过键盘组合键来给进程发送信号，例如使用Ctrl+C 产生 SIGINT 信号，表示终止进程。 如果是运行在后台的进程，可以通过命令来给进程发送信号，例如使用kill -9 PID 产生SIGKILL信号，表示立即结束进程。 信号量： 当使用共享内存的通信方式，如果有多个进程同时往共享内存写入数据，有可能先写的进程的内容被其他进程覆盖了。 因此需要一种保护机制，信号量本质上是一个整型的计数器，用于实现进程间的互斥和同步。 信号量代表着资源的数量，操作信号量的方式有两种： P操作：这个操作会将信号量减一，相减后信号量如果小于0，则表示资源已经被占用了，进程需要阻塞等待；如果大于等于0，则说明还有资源可用，进程可以正常执行。 V操作：这个操作会将信号量加一，相加后信号量如果小于等于0，则表明当前有进程阻塞，于是会将该进程唤醒；如果大于0，则表示当前没有阻塞的进程。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:6:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"讲讲Go里面的GMP模型？ ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:7:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"Go的GMP模型 G：表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P：表示逻辑processor，P 的数量决定了系统内最大可并行的 G 的数量（前提：系统的物理cpu核数 \u003e= P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M：M 代表着真正的执行计算资源，物理 Processor。 G 如果想运行起来必须依赖 P，因为 P 是它的逻辑处理单元，但是 P 要想真正的运行，他也需要与 M 绑定，这样才能真正的运行起来，P 和 M 的这种关系就相当于 Linux 系统中的用户层面的线程和内核的线程是一样的。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:8:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"map用过吧？怎么对map进行排序？ go的map不保证有序性，所以按key排序需要取出key，对key排序，再遍历输出value ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:9:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"讲讲map底层？为什么是无序的？ map 的底层是一个结构体 // Go map 的底层结构体表示 type hmap struct { count int // map中键值对的个数，使用len()可以获取 flags uint8 B uint8 // 哈希桶的数量的log2，比如有8个桶，那么B=3 noverflow uint16 // 溢出桶的数量 hash0 uint32 // 哈希种子 buckets unsafe.Pointer // 指向哈希桶数组的指针，数量为 2^B oldbuckets unsafe.Pointer // 扩容时指向旧桶的指针，当扩容时不为nil nevacuate uintptr extra *mapextra // 可选字段 } const ( bucketCntBits = 3 bucketCnt = 1 \u003c\u003c bucketCntBits // 桶数量 1 \u003c\u003c 3 = 8 ) // Go map 的一个哈希桶，一个桶最多存放8个键值对 type bmap struct { // tophash存放了哈希值的最高字节 tophash [bucketCnt]uint8 // 在这里有几个其它的字段没有显示出来，因为k-v的数量类型是不确定的，编译的时候才会确定 // keys: 是一个数组，大小为bucketCnt=8，存放Key // elems: 是一个数组，大小为bucketCnt=8，存放Value // 你可能会想到为什么不用空接口，空接口可以保存任意类型。但是空接口底层也是个结构体，中间隔了一层。因此在这里没有使用空接口。 // 注意：之所以将所有key存放在一个数组，将value存放在一个数组，而不是键值对的形式，是为了消除例如map[int64]所需的填充整数8（内存对齐） // overflow: 是一个指针，指向溢出桶，当该桶不够用时，就会使用溢出桶 } 当向 map 中存储一个 kv 时，通过 k 的 hash 值与 buckets 长度取余，定位到 key 在哪一个bucket中，hash 值的高8位存储在 bucket 的 tophash[i] 中，用来快速判断 key是否存在。当一个 bucket 满时，通过 overflow 指针链接到下一个 bucket。 在源码runtime.mapiterinit中 func mapiterinit(t *maptype, h *hmap, it *hiter) { ... it.t = t it.h = h it.B = h.B it.buckets = h.buckets if t.bucket.kind\u0026kindNoPointers != 0 { h.createOverflow() it.overflow = h.extra.overflow it.oldoverflow = h.extra.oldoverflow } r := uintptr(fastrand()) if h.B \u003e 31-bucketCntBits { r += uintptr(fastrand()) \u003c\u003c 31 } it.startBucket = r \u0026 bucketMask(h.B) it.offset = uint8(r \u003e\u003e h.B \u0026 (bucketCnt - 1)) it.bucket = it.startBucket ... mapiternext(it) } 通过对 mapiterinit 方法阅读，可得知其主要用途是在 map 进行遍历迭代时进行初始化动作。共有三个形参，用于读取当前哈希表的类型信息、当前哈希表的存储信息和当前遍历迭代的数据 源码中 fastrand ... // decide where to start r := uintptr(fastrand()) if h.B \u003e 31-bucketCntBits { r += uintptr(fastrand()) \u003c\u003c 31 } it.startBucket = r \u0026 bucketMask(h.B) it.offset = uint8(r \u003e\u003e h.B \u0026 (bucketCnt - 1)) // iterator state it.bucket = it.startBucket 在这段代码中，它生成了随机数。用于决定从哪里开始循环迭代。更具体的话就是根据随机数，选择一个桶位置作为起始点进行遍历迭代。 因此每次重新 for range map，你见到的结果都是不一样的。那是因为它的起始位置根本就不固定！ ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:10:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"知道TCP连接吧？三次握手的目的是什么？ 因为通信的前提是确保双方都是接收和发送信息是正常的。 三次握手是为了建立可靠的数据传输通道， 第一次握手就是让 接收方 知道 发送方 有发送信息的能力 第二次握手就是让 发送方 知道 接收方 有发送和接受信息的能力 第三次握手就是让 接收方 知道 发送方 有接受信息的能力 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:11:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"那四次挥手有了解过吗？为什么是四次？ 四次挥手则是为了保证等数据完成的被接收完再关闭连接。 既然提到需要保证数据完整的传输完，那就需要保证双方 都达到关闭连接的条件才能断开。 第一次挥手：客户端发起关闭连接的请求给服务端； 第二次挥手：服务端收到关闭请求的时候可能这个时候数据还没发送完，所以服务端会先回复一个确认报文，表示自己知道客户端想要关闭连接了，但是因为数据还没传输完，所以还需要等待； 第三次挥手：当数据传输完了，服务端会主动发送一个 FIN 报文，告诉客户端，表示数据已经发送完了，服务端这边准备关闭连接了。 第四次挥手：当客户端收到服务端的 FIN 报文过后，会回复一个 ACK 报文，告诉服务端自己知道了，再等待一会就关闭连接。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:12:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"什么是粘包和拆包？为什么会出现？ 粘包就是两个或者多个以上的包粘在一起，拆包就是为什么解决粘包问题。 出现粘包原因有两个，一个是发送方发送不及时，导致多个包在发送端粘黏。另一个是接收方接受不及时，导致包在接收端堆叠导致。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:13:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"怎么解决？ 设计一个带包头的应用层报文结构就能解决。包头定长，以特定标志开头，里带着负载长度，这样接收侧只要以定长尝试读取包头，再按照包头里的负载长度读取负载就行了，多出来的数据都留在缓冲区里即可。其实ip报文和tcp报文都是这么干的。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:14:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"什么是事务？ 事务具有原子性、一致性、隔离性、持久性特性，并且 指将一系列数据操作捆绑成为一个整体进行统一管理，如果某一事务执行成功，则在该事物中进行的所有数据更改均会提交，成为数据库中的永久组成部分。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:15:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"那 redis 支持事务吗？ redis 是不支持事务的，因为 他不支持原子性，但是支持一致性，是最终一致性。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:16:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"算法：手撕链表，要求递归实现。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:17:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"算法：忘记了。。好像是中等题，反正不难，也不简单。 ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:18:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"二面 我以为过了简历，免了笔试，一面算法写出来，八股也能吹了，应该有二面，但还是挂了（ 真的是 海量 hc 吗。。。 后面问了一下，原来基本不招本科生… ","date":"0001-01-01 00:00:00","objectID":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/:19:0","tags":null,"title":"","uri":"/e.-%E7%B1%B3%E5%93%88%E6%B8%B8/"},{"categories":null,"content":"【Golang开发面经】深信服（两轮技术面） ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:0:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"写在前面 深信服面试起来感觉有点偏向应用，没有涉及高并发等等内容，想想也确实，深信服更多偏向B端。业务能力扎实也是应该的。深信服挺好的，但我想找toc的，就拒掉了。。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:1:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"一面 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:2:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"了解过切片和数组吗？有什么区别？ 切片的底层其实是数组，数组是不可变长度的，而切片是可变长度的。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:3:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"那这样初始化可以吗？有什么问题？ var array []int 这种其实不好，因为这样是不能赋予地址的。所以容易报错，我们要么用 make去创建，要么用语法糖 array:=[]int{}去创建。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:4:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"用过map吧？怎么遍历map？ for k,v:=range map{ ... } ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:5:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"那遍历 map 是有序的吗？ 无序的 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:6:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"为什么是无序的？ 不是有序的，使用 range 多次遍历 map 时输出的 key 和 value 的顺序可能不同，map在遍历时，并不是从固定的0号bucket开始遍历的，每次遍历，都会从一个随机值序号的bucket，再从其中随机的 cell 开始遍历。map 遍历时，是按序遍历 bucket，同时按需遍历 bucket 和其 overflow bucket 中 的 cell。 但是 map 在扩容后，会发生 key 的搬迁，这造成原来落在一个 bucket 中的 key，搬迁后，有可能会落到其他 bucket 中了，从这个角度看，遍历 map 的结果就不可能是按照原来的顺序了。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:7:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"用过chan吧？怎么声明一个chan呢？ chan是引用类型，一般我们应该用make去创建一个chan ch:=make(chan int) ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:8:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"怎么发消息给chan呢？ ch \u003c- 1 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:9:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"给一个关闭的chan发消息会怎么样？ 会引起panic ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:10:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"讲讲 GMP 吧 G：表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P：表示逻辑processor，P 的数量决定了系统内最大可并行的 G 的数量（前提：系统的物理cpu核数 \u003e= P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M：M 代表着真正的执行计算资源，物理 Processor。 G 如果想运行起来必须依赖 P，因为 P 是它的逻辑处理单元，但是 P 要想真正的运行，他也需要与 M 绑定，这样才能真正的运行起来，P 和 M 的这种关系就相当于 Linux 系统中的用户层面的线程和内核的线程是一样的。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:11:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"那GC有了解过吗？ Go是采用三色标记法来进行垃圾回收的，是传统 Mark-Sweep 的一个改进，它是一个并发的 GC 算法。on-the-fly 原理如下 整个进程空间里申请每个对象占据的内存可以视为一个图， 初始状态下每个内存对象都是白色标记。 先stop the world，将扫描任务作为多个并发的goroutine立即入队给调度器，进而被CPU处理，第一轮先扫描所有可达的内存对象，标记为灰色放入队列 第二轮可以恢复start the world，将第一步队列中的对象引用的对象置为灰色加入队列，一个对象引用的所有对象都置灰并加入队列后，这个对象才能置为黑色并从队列之中取出。循环往复，最后队列为空时，整个图剩下的白色内存空间即不可到达的对象，即没有被引用的对象； 第三轮再次stop the world，将第二轮过程中新增对象申请的内存进行标记（灰色），这里使用了writebarrier（写屏障）去记录这些内存的身份； 这个算法可以实现 on-the-fly，也就是在程序执行的同时进行收集，并不需要暂停整个程序。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:12:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"mysql 有用过吧？MVCC 是怎么实现的？ MVCC 的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，Read View 来实现的。所以我们先来看看这个三个 point 的概念 隐式字段 每行记录除了我们自定义的字段外，还有数据库隐式定义的 DB_TRX_ID, DB_ROLL_PTR, DB_ROW_ID 等字段 DB_TRX_ID：6 byte，最近修改(修改/插入)事务 ID：记录创建这条记录/最后一次修改该记录的事务 ID DB_ROLL_PTR：7 byte，回滚指针，指向这条记录的上一个版本（存储于 rollback segment 里） DB_ROW_ID：6 byte，隐含的自增 ID（隐藏主键），如果数据表没有主键，InnoDB 会自动以DB_ROW_ID ，产生一个聚簇索引。 undo日志 insert undo log：代表事务在 insert 新记录时产生的 undo log，只在事务回滚时需要，并且在事务提交后可以被立即丢弃。 update undo log：事务在进行 update 或 delete 时产生的 undo log ，不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被 purge线程统一清除。 Read View Read View 就是事务进行 快照读 操作的时候生产的 读视图 (Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的 ID (当每个事务开启时，都会被分配一个 ID , 这个 ID 是递增的，所以最新的事务，ID 值越大)。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:13:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"mysql 的锁是怎么实现的？ 当时确实没了解如何实现的，只知道如何for update这些，可以看这篇博客，很详细了。 Mysql锁机制及原理简析 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:14:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"用过gin是吧？gin是怎么处理请求的？ Gin其实是通过一个context来进行上下文的传递，将这个传递参数，参数返回。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:15:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"如果有一个业务给你，你怎么写这个请求？ 首先肯定是传统的MVC模型来进行操作。 controller层来接受请求，service层来进行请求的处理，dao层来写sql语句。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:16:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"算法：将重复的元素移到最后 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:17:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"二面 二面基本都是跟着项目走。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:18:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"如果你的系统突然多了10w的访问量，你要怎么处理？ nginx来进行负载均衡。 用户验证的信息的过期时间设置长一点，以免发生缓冲雪崩的问题。 设置布尔过滤器。 检查sql语句，是否符合要求，是否是慢sql，如果是则解决。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:19:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"redis用过是吧？说说你在项目里面的排行榜？你说说redis的底层是怎么处理的？ 主要是用redis的zset实现的。 zset主要是跳跃表实现的。 Redis 的跳跃表由 redis.h/zskiplistNode 和 redis.h/zskiplist 两个结构定义，其中 zskiplistNode 结构用于表示跳跃表节点，而 zskiplist 结构则用于保存跳跃表节点的相关信息，比如节点的数量，以及指向表头节点和表尾节点的指针等等。 上图中展示了一个跳跃表示例，最左边的就是 zskiplist 结构。 header：指向跳跃表的表头节点。 tail：指向跳跃表的表尾节点。 level：记录目前跳跃表内，层数最大的那个节点的层数。 记录跳跃表的长度，也就是，跳跃表目前包含节点的数量。 层（level）：节点中用L1、L2、L3等字样标记节点的各个层，L1表示第一层，L2代表第二层，以此类推。每层都带有两个属性：前进指针和跨度。前进指针用于方位位于表尾方向的其他节点。而跨度则记录了前进指针所指向节点和当前节点的距离。 后退（backward）指针： 节点中用BW字样标记的后退指针，他指向当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。 分值(score)：各个节点中的 1.0、2.0、3.0是节点所保存的分值。在跳跃表中，节点按各个所保存的分值从小到大排序。 成员对象(obj)：各个节点中的o1，o2 和 o3 是节点所保存的成员对象。 ","date":"0001-01-01 00:00:00","objectID":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/:20:0","tags":null,"title":"","uri":"/f.-%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"categories":null,"content":"关于RabbitMQ的一些面试题 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:0:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"0. 什么是RabbitMQ RabbitMQ采用AMQP高级新消息队列协议的一种消息队列技术，最大的特点是消费并不需要确保提供方实现，实现了服务之间的高度解耦 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:1:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1. 延时队列底层实现 延迟队列存储的对象肯定是对应的延迟消息，所谓”延迟消息”是指当消息被发送以后，并不想让消费者立即拿到消息，而是等待指定时间后，消费者才拿到这个消息进行消费 订单时间 定时任务 TTL（Time To Live） RabbitMQ可以针对Queue和Message设置 x-message-tt，来控制消息的生存时间，如果超时，则消息变为dead letter RabbitMQ针对队列中的消息过期时间有两种方法可以设置。 A: 通过队列属性设置，队列中所有消息都有相同的过期时间。 B: 对消息进行单独设置，每条消息TTL可以不同。 给对列设置过期时间，将消息加入对列，过期时间之后消息自动进入死信队列，监听死信队列，进行消费操作可以实现延迟队列 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:2:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"2. 使用RabbitMQ需要注意什么 消息丢失，消息重复消费等等 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:3:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"3. RabbitMQ效率 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:4:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"4. 插入延时队列的过期时间是单调的麻？ ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:5:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"5. 如何确保消息正确地发送至RabbitMQ？ 如何确保消息接收方消费了消息？ ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:6:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"5.1 发送方确认模式 将信道设置成confirm模式（发送方确认模式），则所有在信道上发布的消息都会被指派一个唯一ID。一旦消息被投递到目的队列后，或者消息被写入磁盘后，信道会发送一个确认给生产者（包括消息的ID）。 如果RabbitMQ发生内部错误从而导致消息丢失，会发送一条nack消息。 发送方确认模式是异步的，生产者应用程序在等待确认的同时，可以继续发送消息。 当确认消息到达生产者应用程序，生产者应用的回调方法就会被触发来处理确认消息。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:7:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"5.2 接收方确认机制 消费者接受每一条消息后都必须进行确认，只要有消费者确认了消息，MQ才能安全的把消息从队列中删除。 这里并没有用到超时机制，MQ仅通过Consumer的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMQ给了Consumer足够长的时间来处理消息。保证了数据的最终一致性。 还有几种情况： 如果消费者接受到消息，在确认之前断开了连接或取消订阅，RabbitMQ会认为消息没有被分发，然后重新分发给下一个订阅的消费者。（可能存在消费重复的隐患，需要去重） 如果消费者接受到消息却没有确认消息，连接也未断开，则RabbitMQ认为该消费者繁忙。则不会给该消费者分发更多的消息。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:8:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"6. 如何避免消息重复投递或重复消费？ 在消息生产时，MQ内部针对每条生产者发送的消息生成一个inner-msg-id，作为去重的依据（消息投递失败并重传），避免重复的消息进入队列，在消息消费时，要求消息体中 必须要有一个bizID（对于同一个业务全局唯一） 作为去重的依据，避免同一条消息被重复消费。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:9:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"7. 消息基于什么传输？ 由于 TCP 连接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈。RabbitMQ 使用信道的方式来传输数据。信道是建立在真实的 TCP 连接内的虚拟连接，且每条 TCP 连接上的信道数量没有限制。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:10:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"8、消息如何分发？ 一个生产者，多个消费者 多个消费者时，是轮询机制，依次分发给消费者(每个消费者按顺序依次消费) no_act设置是否确认消息处理完？ no_act = True , 消费者不发送确认信息，RabbitMQ从发送消息队列后，不管消费者是否处理完，删除queue 设置no_act=False，RabbitMQ等待消费者的callback处理完，发送确认信息，如果此时消费者down了，则RabbitMQ把消息轮询发送给下一个消费者，等待确认才会删除queue 去掉no_act=True，需要在回调函数中新增代码，手动向RabbitMQ发送确认信息 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:11:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"9、消息怎么路由？ Direct：直连模式 Topic： 转发模式 Topic 模式下可以使用统配符表示bingKey：'*‘表示匹配一个单词， ‘#‘则表示匹配没有或者多个单词。由此可以实现一个queue接收多个路由的消息。 Fanout ：广播模式 广播模式下，不用理会routing key。Fanout Exchange 会将消息传递到 exchange 绑定好的 queue list 上去。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:12:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"10、如何确保消息不丢失？ ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:13:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"10.1 生产者丢失消息 可以选择使用 RabbitMQ 提供事务功能，就是生产者在发送数据之前开启事务，然后发送消息，如果消息没有成功被RabbitMQ接收到，那么生产者会受到异常报错，这时就可以回滚事物，然后尝试重新发送；如果收到了消息，那么就可以提交事物。 缺点： RabbitMQ 事务已开启，就会变为同步阻塞操作，生产者会阻塞等待是否发送成功，太耗性能会造成吞吐量的下降。 还有就是上面的第五点 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:14:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"10.2 RabbitMQ自己丢了数据 持久化 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:15:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"10.3 消费者弄丢了数据 使用 RabbitMQ 提供的 ACK 机制，首先关闭 RabbitMQ 的自动ACK，然后每次在确保处理完这个消息之后，在代码里手动调用 ACK。这样就可以避免消息还没有处理完就ACK。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:16:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"11、RabbitMQ的集群 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:17:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"12、使用RabbitMQ有什么好处？ ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:18:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"12.1 削峰 把消息压入RabbitMQ中可以缓冲系统压力。比如现在系统只能接受2000请求，但是一下子有10000个请求过来，那么这个请求就会压在RabbitMQ中，那么就可以慢慢进行消费了。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:19:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"12.2 异步 以前是先去发短信，再去发邮件。引入RabbitMQ之后，我们就可以进行在发短信的同时再去发邮箱。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:20:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"12.3 解耦 当多个系统耦合在一起的时候，系统的消息会发送给连在一起的系统，但是这个消息有些系统可能是不需要的。所以引入了之后，很方便将这个系统进行解耦，每个系统需要的就在消息队列解耦。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:21:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"13、MQ 的缺点 虽然能提供削峰，异步，解耦，但是这个还是有很多要考虑的问题，消息丢失，重复消费。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:22:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"14、介绍Rabbitmq的手动ACK和自动ACK 当消息一旦被消费者接收，队列中的消息就会被删除。那么问题来了：RabbitMQ怎么知道消息被接收了呢？ 这就要通过消息确认机制（Acknowlege）来实现了。当消费者获取消息后，会向RabbitMQ发送回执ACK，告知消息已经被接收。不过这种回执ACK分两种情况： 自动ACK：消息一旦被接收，消费者自动发送ACK 手动ACK：消息接收后，不会发送ACK，需要手动调用 这两ACK要怎么选择呢？这需要看消息的重要性： 如果消息不太重要，丢失也没有影响，那么自动ACK会比较方便 如果消息非常重要，不容丢失。那么最好在消费完成后手动ACK，否则接收消息后就自动ACK，RabbitMQ就会把消息从队列中删除。如果此时消费者宕机，那么消息就丢失了。 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:23:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"参考连接 分布式消息中间件-RabbitMQ面试题（必问） RabbitMQ-解耦、异步、削峰 ","date":"0001-01-01 00:00:00","objectID":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:24:0","tags":null,"title":"","uri":"/g.-rabbitmq-%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"分布式题目集锦 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:0:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1. 负载均衡算法 随机访问策略。 系统随机访问，缺点：可能造成服务器负载压力不均衡，俗话讲就是撑的撑死，饿的饿死。 轮询策略。 请求均匀分配，如果服务器有性能差异，则无法实现性能好的服务器能够多承担一部分。 权重轮询策略。 权值需要静态配置，无法自动调节，不适合对长连接和命中率有要求的场景。 Hash取模策略。 不稳定，如果列表中某台服务器宕机，则会导致路由算法产生变化，由此导致命中率的急剧下降。 简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 0 ~ 2^32-1（即哈希值是一个32位无符号整形），整个哈希环如下： 多个服务器都通过这种方式进行计算，最后都会各自映射到圆环上的某个点，这样每台机器就能确定其在哈希环上的位置，如下图所示。 那么用户访问，如何分配访问的服务器呢？我们根据用户的 IP 使用上面相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针行走，遇到的第一台服务器就是其应该定位到的服务器。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:1:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"2. 熔断和降级 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:2:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"2.1 熔断 一般是某个服务故障或者是异常引起的，当某个异常条件被触发，直接熔断整个服务，而不是一直等到此服务超时，为了防止防止整个系统的故障。 而采用了一些保护措施。过载保护。比如A服务的X功能依赖B服务的某个接口，当B服务接口响应很慢时，A服务X功能的响应也会被拖慢，进一步导致了A服务的线程都卡在了X功能上，A服务的其它功能也会卡主或拖慢。此时就需要熔断机制，即A服务不在请求B这个接口，而可以直接进行降级处理。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:3:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"2.2 降级 服务器当压力剧增的时候，根据当前业务情况及流量，对一些服务和页面进行有策略的降级。以此缓解服务器资源的的压力，以保证核心业务的正常运行，同时也保持了客户和大部分客户的得到正确的响应。 自动降级：超时、失败次数、故障、限流 （1）配置好超时时间(异步机制探测回复情况)； （2）不稳的api调用次数达到一定数量进行降级(异步机制探测回复情况)； （3）调用的远程服务出现故障(dns、http服务错误状态码、网络故障、Rpc服务异常)，直接进行降级。 人工降级：秒杀、双十一大促降级非重要的服务。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:4:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"3. 幂等 幂等性的核心思想，其实就是保证这个接口的执行结果只影响一次，后续即便再次调用，也不能对数据产生影响，之所以要考虑到幂等性问题，是因为在网络通信中，存在两种行为可能会导致接口被重复执行。 用户的重复提交或者用户的恶意攻击，导致这个请求会被多次重复执行。 在分布式架构中，为了避免网络通信导致的数据丢失，在服务之间进行通信的时候都会设计超时重试的机制，而这种机制有可能导致服务端接口被重复调用。所以在程序设计中，对于数据变更类操作的接口，需要保证接口的幂等性。 使用 redis 里面提供的 setNX 指令，比如对于MQ消费的场景，为了避免MQ重复消费导致数据多次被修改的问题，可以在接受到MQ的消息时，把这个消息通过setNx写入到redis里面，一旦这个消息被消费过，就不会再次消费。 建去重表，将业务中由唯一标识的字段保存到去重表，如果表中存在，则表示已经处理过了。 版本控制，增加版本号，当版本号符合时候，才更新数据。 状态控制，例如订单有状态已支付，未支付，支付中，支付失败，当处于未支付的时候才允许修改成支付中。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:5:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"4. 分布式事务 在分布式系统中，一次业务处理可能需要多个应用来实现，比如用户发送一次下单请求，就涉及到订单系统创建订单，库存系统减库存，而对于一次下单，订单创建与减库存应该是要同时成功或者同时失效，但在分布式系统中，如果不做处理，就很有可能订单创建成功，但是减库存失败，那么解决这类问题，就需要用到分布式事务，常用的解决方案如下： 本地消息表：创建订单时，将减库存消息加入在本地事务中，一起提交到数据库存入本地消息表，然后调用库存系统，如果调用成功则修改本地。 消息状态为成功，如果调用库存系统失败，则由后台定时任务从本地消息表中取出未成功的消息，重试调用库存系统。 消息队列：目前 RocketMQ 中支持事务消息，它的工作原理是： 产订单系统先发送一条 half 消息到 Broker， half 消息对消费者而言是不可见的。 再创建订单，根据创建订单成功与否，向 Broker 发送 commit 或 rollback。 并且生产者订单系统还可以提供 Broker 回调接口，当 Broker 发现一段时间 half 消息没有收到任何操作命令，则会主动调此接口来查询订单是否创建成功。 如果消费失败，则根据重试策略进行重试，最后还失败则进入死信队列，等待进一步处理。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:6:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"5. 分布式锁 在单体架构中，多个线程都是属于同一个进程的，所以在线程并发执行时，遇到资源竞争时，可以利用ReentrantLock、 synchronized等技术来作为锁来共享资源的使用。 而在分布式架构中，多个线程是可能处于不同进程中的，而这些线程并发执行遇到资源竞争时，利用 ReentrantLock synchronized 等技术是没办法，来控制多个进程中的线程的，所以需要分布式锁，意思就是，需要一个分布式锁生成器，分布式系统中的应用程序都可以来使用这个生成器所提供的锁，从而达到多个进程中的线程使用同一把锁。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:7:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"6. 分布式主键id 在开发中，我们通常会需要一个唯一ID来标识数据，如果是单体架构，我们可以通过数据库的主键，或直接在内存中维护一个自增数字来作为ID都是可以的，但对于-个分布式系统，就会有可能会出现ID冲突,此时有以下解决方案: uuid，这种方案复杂度最低，但是会影响存储空间和性能。 利用单机数据库的自增主键，作为分布式ID的生成器，复杂度适中，ID长度较之uuid更短，但是受到单机数据库性能的限制，并发量大的时候，此方案也不是最优方案。 利用redis、zookeeper的特性来生成id，比如redis的自增命令、zookeeper的顺序节点， 这种方案和单机数据库(mysq|)相比，性能有所提高，可以适当选用。 4.雪花算法，一切问题如果能直接用算法解决，那就是最合适的，利用雪花算法也可以生成分布式ID，底层原理就是通过某台机器在某一毫秒内对某一个数字自增，这种方案也能保证分布式架构中的系统 id 唯一，但是只能保证趋势递增。业界存在tinyid、 leaf 等开源中间件实现了雪花算法。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:8:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"7. 池化技术 对象池技术基本原理的核心有两点：缓存和共享，即对于那些被频繁使用的对象，在使用完后，不立即将它们释放，而是将它们缓存起来，以供后续的应用程序重复使用，从而减少创建对象和释放对象的次数，进而改善应用程序的性能。 事实上，由于对象池技术将对象限制在一定的数量，也有效地减少了应用程序内存上的开销。 ","date":"0001-01-01 00:00:00","objectID":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:9:0","tags":null,"title":"","uri":"/h.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"【Golang开发面经】得物（两轮技术面） ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:0:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"写在前面 得物一顿面试下来感觉还行吧，挺注重基础的，面试官水平也很高。就是聊的挺开心的。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:1:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"笔试 略 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:2:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"一面 聊项目。大概20分钟吧，如何优化之类的。。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:3:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"你用过gorm？那我直接用sql不也可以吗？为什么用gorm？ 用 sql 是可以，但是会有sql注入的风险，而gorm是通过占位符的形式来一定程度减少了sql的注入。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:4:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"你用过哪些锁？ 用过读写锁 sync 包下的排斥锁或是读写锁，这两种锁。 一般在出现数据竞争的情况下使用的。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:5:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"那map是线程安全的吗？为什么？ 不是的，map 会发生数据竞争，因为底层结构中 map 是没有锁的支持的。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:6:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"chan呢？ chan的底层是有锁结构，所以是线程安全的。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:7:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"对一个关闭的chan读写会怎么样？ 对一个关闭的chan进行读： 如果chan里面还有值，那么就读出chan里面的东西 如果chan里面没有值了，就会读出这个chan类型的零值。 对一个关闭的chan进行写： 直接panic ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:8:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"算法：一道回溯的题目，不太记得了 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:9:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"二面 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:10:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"redis 用过吧？你知道redis有多少种高可靠集群的模型？ 我目前了解到的是redis的哨兵模式。 哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它独立运行。 其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:11:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"redis 支持事务吗？ 不支持，redis不支持原子性，只是支持最终一致性。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:12:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"mysql 的隔离等级？ 未提交读（READ UNCOMMITTED）：READ UNCOMMITTED 提供了事务之间最小限度的隔离。除了容易产生虚幻的读操作和不能重复的读操作外，处于这个隔离级的事务可以读到其他事务还没有提交的数据，如果这个事务使用其他事务不提交的变化作为计算的基础，然后那些未提交的变化被它们的父事务撤销，这就导致了大量的数据变化。 提交读（READ COMMITTED）：READ COMMITTED 隔离级别的安全性比 REPEATABLE READ 隔离级别的安全性要差。处于 READ COMMITTED 级别的事务可以看到其他事务对数据的修改。也就是说，在事务处理期间，如果其他事务修改了相应的表，那么同一个事务的多个 SELECT 语句可能返回不同的结果。 可重复读（REPEATABLE READ）：在可重复读在这一隔离级别上，事务不会被看成是一个序列。不过，当前正在执行事务的变化仍然不能被外部看到，也就是说，如果用户在另外一个事务中执行同条 SELECT 语句数次，结果总是相同的。（因为正在执行的事务所产生的数据变化不能被外部看到）。 序列化（SERIALIZABLE）：如果隔离级别为序列化，则用户之间通过一个接一个顺序地执行当前的事务，这种隔离级别提供了事务之间最大限度的隔离。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:13:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"最左匹配原则是什么？ 索引的底层是一颗B+树，构建一颗B+树只能根据一个值来构建。此处的索引当然也包括联合索引，当索引类型为联合索引时，数据库会依据联合索引最左的字段来构建B+树，也叫最左匹配原则。 最左匹配原则：最左优先，以最左边的为起点任何连续的索引都能匹配上。 同时遇到范围查询(\u003e、\u003c、between、like)就会停止匹配。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:14:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"我能在性别这个字段上面建索引吗？为什么？ 关于区分度不高的字段，比如性别，比如状态字段，不应该建索引。索引并不是建了就快，唯一性太差的字段不需要创建索引，只有2种取值的字段，建了索引数据库也不一定会用，只会白白增加索引维护的额外开销。因为索引也是需要存储的，所以插入和更新的写入操作，同时需要插入和更新你这个字段的索引的。 我们也可以从索引的选择性这方面去说：索引的选择性是指索引列中不同值的数目和表的记录数的比值。 假如表里面有 1000 条数据，表索引列有 980 个不同的值，这时候索引的选择性就是 980/1000=0.98 。索引的选择性越接近 1，这个索引的效率很高。 性别可以认为是3种，男，女，其他。如果创建索引，查询语句 性别=‘男’的数据，索引的选择性就是3/1000=0.003。索引的选择性值很低，对查询提升不大，所以性别建索引意义不大。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:15:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"go 里面 context 是怎么用的？ context 一般我们用来传递上下文信息，在go中，理解为goroutine的运行状态、现场，存在上下层goroutine context的传递，上层goroutine会把context传递给下层goroutine。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:16:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"rabbitmq 是如何保证消息不丢失的？ ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:17:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"5.1 发送方确认模式 将信道设置成confirm模式（发送方确认模式），则所有在信道上发布的消息都会被指派一个唯一ID。一旦消息被投递到目的队列后，或者消息被写入磁盘后，信道会发送一个确认给生产者（包括消息的ID）。 如果RabbitMQ发生内部错误从而导致消息丢失，会发送一条nack消息。 发送方确认模式是异步的，生产者应用程序在等待确认的同时，可以继续发送消息。 当确认消息到达生产者应用程序，生产者应用的回调方法就会被触发来处理确认消息。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:18:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"5.2 接收方确认机制 消费者接受每一条消息后都必须进行确认，只要有消费者确认了消息，MQ才能安全的把消息从队列中删除。 这里并没有用到超时机制，MQ仅通过Consumer的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMQ给了Consumer足够长的时间来处理消息。保证了数据的最终一致性。 还有几种情况： 如果消费者接受到消息，在确认之前断开了连接或取消订阅，RabbitMQ会认为消息没有被分发，然后重新分发给下一个订阅的消费者。（可能存在消费重复的隐患，需要去重） 如果消费者接受到消息却没有确认消息，连接也未断开，则RabbitMQ认为该消费者繁忙。则不会给该消费者分发更多的消息。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:19:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"9个球，外观一样，但有一个球是重一点，怎么快速找到这个特殊的球？ 最快两次就好了，分三组，第一组和第二组对比，如果相等，那么第三组抽两个出来就选出来了，如果不相等，就在重的那组抽两个出来对比就好了。如果抽出来的这两个一样重，那么就是第三个了。如果倾向另一边，那么就是一边的重了。 没有算法，很奇怪，因为每次面试都要手撕代码，这次没有反而觉得缺了点什么。。 ","date":"0001-01-01 00:00:00","objectID":"/i.-%E5%BE%97%E7%89%A9/:20:0","tags":null,"title":"","uri":"/i.-%E5%BE%97%E7%89%A9/"},{"categories":null,"content":"关于MySQL的一些面试题 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:0:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1. 内，左，右，全连接 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:1:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1.1 内连接 **内连接：inner join ** 返回的是两个表之间的交集 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:2:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1.2 左连接 **内连接：left join ** 返回的是左表的内容，右边中不符合条件的内容不会显示。 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:3:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1.3 右连接 **内连接：right join ** 返回的是右表的内容，左边中不符合条件的内容不会显示。 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:4:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"1.4 全连接 关键词：union / union all 通过union连接的SQL，它们分别取出的列数必须相同； 不要求合并的表列名称相同时，以第一个sql 表列名为准； 使用union 时，完全相等的行，将会被合并，由于合并比较耗时，一般不直接使用 union 进行合并，而是通常采用union all 进行合并； 被union 连接的sql 子句，单个子句中不用写order by ，因为不会有排序的效果。但可以对最终的结果集进行排序； ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:5:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"2. redo，undo，bin 三大日志的用法 redo log：重做日志，用于记录数据修改后的记录，顺序记录 undo log：回滚日志，undo日志用于存放数据被修改前的值。用于回滚，保证了事务的一致性。 当 buffer pool 中的 dirty page 还没有刷新到磁盘的时候，发生crash，启动服务后，可通过redo log 找到需要重新刷新到磁盘文件的记录； buffer pool 中的数据直接flush到disk file，是一个随机IO，效率较差，而把buffer pool中的数据记录到redo log，是一个顺序IO，可以提高事务提交的速度 bin log：用于主从复制的交互的日志文件 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:6:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"3. MVCC的理解 维持一个数据的多版本，使读写操作没有冲突。 首先是获得事务版本号 获取一个readView 查询到数据与readView的事务版本号进行对比 不匹配的话就从undo log里获取历史版本数据 返回符合规则的数据。 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:7:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"4. MySQL 用索引和不用索引的时间复杂度 索引：每个结点 m 分支，log(m)n，另外结点内分 m 区间，二分查找，所以 m*log(m)n。m 是常数，所以 log（m）n。 不用索引：o(n) 底层是双向链表 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:8:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"5. MySQL的锁的实现 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:9:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"6. MySQL 的隔离级别 隔离级别 读数据一致性 脏读 不可重复读 幻读 未提交读（Read uncommitted） 最低级别，只能保证不读取物理上损坏的数据，事务可以看到其他事务没有被提交的数据（脏数据） 是 是 是 已提交度（Read committed） 语句级，事务可以看到其他事务已经提交的数据 否 是 是 可重复读（Repeatable read） 事务级，事务中两次查询的结果相同 否 否 是 可序列化（Serializable） 串行 否 否 否 ","date":"0001-01-01 00:00:00","objectID":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/:10:0","tags":null,"title":"","uri":"/j.-mysql%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/"},{"categories":null,"content":"【Golang开发面经】奇安信（两轮技术面） ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:0:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"写在前面 奇安信一顿面试下来感觉很一般。被面试官嘲讽了一波（ 不是搞安全的就不要去奇安信了。。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:1:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"笔试 略 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:2:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"一面 项目问的比较多 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:3:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"说一下TCP的三次握手和四次挥手 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:4:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"TCP和UDP的区别 TCP 是可靠传输，面向连接，基于流，占用资源多，效率低。 UDP是尽最大努力交付，基于无连接，基于报文，UDP 占用系统资源较少，效率高。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:5:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"怎么让UDP变得可靠 可以的，我们只需要仿照TCP的可靠传输机制就可以了，比如说设置ACK确认机制，一旦没有收到，或是收到三次上一个报文的ACK，我们就立即重传丢失的报文。再比如说设置滑动窗口来保证数据传输的安全性等等… ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:6:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"写一下sql的查询语句，比如说我要查询id=3的user。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:7:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"说一下 InnoDB 的特性 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:8:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"讲讲mysql的索引 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:9:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"如何创建索引？ ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:10:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"画过流程图吗？我们工作要经常画这些图。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:11:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"算法：给你一个字符串 s 和一个整数 m ，请你找出 s 中的最长子串， 要求该子串中的每一字符出现次数都不少于 m 。返回这一子串的长度。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:12:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"二面 有一道概率论的问题卡住了，被面试官嘲讽。 “就你这还xx学校，xx专业啊？” 不想写了，别去什么奇安信了，睿智公司。 ","date":"0001-01-01 00:00:00","objectID":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/:13:0","tags":null,"title":"","uri":"/k.-%E5%A5%87%E5%AE%89%E4%BF%A1/"},{"categories":null,"content":"【Golang开发面经】360（一轮游） ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:0:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"写在前面 这个公司估计是走个形式…. ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:1:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"笔试 略 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:2:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"一面 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:3:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"TCP 和 UDP 区别 TCP 是可靠传输，面向连接，基于流，占用资源多，效率低。 UDP是尽最大努力交付，基于无连接，基于报文，UDP 占用系统资源较少，效率高。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:4:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"UDP能可靠传输吗？ 可以的，我们只需要仿照TCP的可靠传输机制就可以了，比如说设置ACK确认机制，一旦没有收到，或是收到三次上一个报文的ACK，我们就立即重传丢失的报文。再比如说设置滑动窗口来保证数据传输的安全性等等… ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:5:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"MYSQL的隔离等级 未提交读（READ UNCOMMITTED）：READ UNCOMMITTED 提供了事务之间最小限度的隔离。除了容易产生虚幻的读操作和不能重复的读操作外，处于这个隔离级的事务可以读到其他事务还没有提交的数据，如果这个事务使用其他事务不提交的变化作为计算的基础，然后那些未提交的变化被它们的父事务撤销，这就导致了大量的数据变化。 提交读（READ COMMITTED）：READ COMMITTED 隔离级别的安全性比 REPEATABLE READ 隔离级别的安全性要差。处于 READ COMMITTED 级别的事务可以看到其他事务对数据的修改。也就是说，在事务处理期间，如果其他事务修改了相应的表，那么同一个事务的多个 SELECT 语句可能返回不同的结果。 可重复读（REPEATABLE READ）：在可重复读在这一隔离级别上，事务不会被看成是一个序列。不过，当前正在执行事务的变化仍然不能被外部看到，也就是说，如果用户在另外一个事务中执行同条 SELECT 语句数次，结果总是相同的。（因为正在执行的事务所产生的数据变化不能被外部看到）。 序列化（SERIALIZABLE）：如果隔离级别为序列化，则用户之间通过一个接一个顺序地执行当前的事务，这种隔离级别提供了事务之间最大限度的隔离。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:6:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"左连接，右连接有什么区别？ 左连接：只要左边表中有记录，数据就能检索出来，而右边有的记录必要在左边表中有的记录才能被检索出来。 右连接：右连接是只要右边表中有记录，数据就能检索出来。 右连接与左连接相反，左连接A LEFT JOIN B，连接查询的数据，在A中必须有，在B中可以有可以没有。右连接A INNER JOIN B，在A中也有，在B中也有的数据才能查询出来。 左连接是已左边表中的数据为基准，若左表有数据右表没有数据，则显示左表中的数据右表中的数据显示为空。右联接是左向外联接的反向联接，将返回右表的所有行。如果右表的某行在左表中没有匹配行，则将为左表返回空值。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:7:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"JOIN的性能一定好吗？ 小表在前可以提高sql执行效率。 尽量不要使用where语句。 从上面例子可以看出，尽可能满足ON的条件，而少用Where的条件。从执行性能来看第二个显然更加省时。 Inner join 是不分主从表的，结果是取两个表针对 On 条件相匹配的最小集。如果是很大的表，首先针对两张表做Where条件筛选，然后再做 Join。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:8:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"线程的通信方式？ 消息队列、内存共享、信号 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:9:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"快排原理？堆排呢？ 快速排序实现的重点在于数组的拆分，通常我们将数组的第一个元素定义为比较元素，然后将数组中小于比较元素的数放到左边，将大于比较元素的放到右边，这样我们就将数组拆分成了左右两部分：小于比较元素的数组；大于比较元素的数组。我们再对这两个数组进行同样的拆分，直到拆分到不能再拆分，数组就自然而然地以升序排列了。 堆排序的基本思想是：将待排序序列构造成一个大顶堆，此时，整个序列的最大值就是堆顶的根节点。将其与末尾元素进行交换，此时末尾就为最大值。然后将剩余 n-1 个元素重新构造成一个堆，这样会得到n个元素的次小值。如此反复执行，便能得到一个有序序列了。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:10:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"算法：top k 问题 估计是不招人了，一面后一点消息也没有。。。 ","date":"0001-01-01 00:00:00","objectID":"/l.-360/:11:0","tags":null,"title":"","uri":"/l.-360/"},{"categories":null,"content":"Golang-Interview 记录实习、秋招中大小公司的面经，包括字节跳动，腾讯，滴滴，百度等等… ","date":"0001-01-01 00:00:00","objectID":"/readme/:0:0","tags":null,"title":"","uri":"/readme/"},{"categories":null,"content":"公司涉及 字节跳动 B站 百度 滴滴 腾讯 360 深信服 奇安信 得物 蔚来 小米 ","date":"0001-01-01 00:00:00","objectID":"/readme/:1:0","tags":null,"title":"","uri":"/readme/"},{"categories":null,"content":" 就业推荐表，不轻易给原件，就业处盖章不处理急章 成绩单 T3 T4自助打印 签三方 点击发起，企业确认就已经代表签约了。 重要，确认企业档案保管权，不确定就填回生源地 试用期不超过6个月 任何附加内容落于纸质 解约 ","date":"0001-01-01 00:00:00","objectID":"/school/:0:0","tags":null,"title":"","uri":"/school/"}]