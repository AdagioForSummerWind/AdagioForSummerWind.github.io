<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>K8s_base - 绿叶律动</title><meta name="Description" content="绿叶律动"><meta property="og:title" content="K8s_base" />
<meta property="og:description" content="k8s入门实战 开篇词｜迎难而上，做云原生时代的弄潮儿 你好，我是罗剑锋，不过更愿意你称呼我“Chrono”。先来简单介绍一下我自己吧。作为一个" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jefofrank.xyz/k8s_base/" /><meta property="og:image" content="https://jefofrank.xyz/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-14T10:38:08+08:00" />
<meta property="article:modified_time" content="2022-10-29T21:27:42+08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://jefofrank.xyz/logo.png"/>

<meta name="twitter:title" content="K8s_base"/>
<meta name="twitter:description" content="k8s入门实战 开篇词｜迎难而上，做云原生时代的弄潮儿 你好，我是罗剑锋，不过更愿意你称呼我“Chrono”。先来简单介绍一下我自己吧。作为一个"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://jefofrank.xyz/k8s_base/" /><link rel="prev" href="https://jefofrank.xyz/interview/" /><link rel="next" href="https://jefofrank.xyz/k8s_advanced/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "K8s_base",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/jefofrank.xyz\/k8s_base\/"
        },"image": ["https:\/\/jefofrank.xyz\/images\/Apple-Devices-Preview.png"],"genre": "posts","keywords": "k8s","wordcount":  149401 ,
        "url": "https:\/\/jefofrank.xyz\/k8s_base\/","datePublished": "2022-07-14T10:38:08+08:00","dateModified": "2022-10-29T21:27:42+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "Jefo","logo": "https:\/\/jefofrank.xyz\/images\/avatar.png"},"author": {
                "@type": "Person",
                "name": "Jefo"
            },"description": ""
    }
    </script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-193031966-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-193031966-2');
</script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="绿叶律动"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span><span id="id-1" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> All posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><a class="menu-item" href="https://github.com/jf-011101" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="直接搜索更方便^-^" id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="绿叶律动"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span><span id="id-2" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="直接搜索更方便^-^" id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">All posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a class="menu-item" href="https://github.com/jf-011101" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">K8s_base</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://github.com/jf-011101" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>Jefo</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/k8s/"><i class="far fa-folder fa-fw"></i>k8s</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2022-07-14 10:38:08">2022-07-14 10:38:08</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 149401 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 299 分钟&nbsp;<span id="busuanzi_container_page_pv">
                    <i class="far fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>&nbsp;次阅读量</span>
                </span>
            </div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#开篇词迎难而上做云原生时代的弄潮儿">开篇词｜迎难而上，做云原生时代的弄潮儿</a>
      <ul>
        <li><a href="#现在的-kubernetes">现在的 Kubernetes</a></li>
        <li><a href="#学习-kubernetes-有哪些难点">学习 Kubernetes 有哪些难点</a></li>
        <li><a href="#在这个专栏里你会怎么学习-kubernetes">在这个专栏里你会怎么学习 Kubernetes</a></li>
        <li><a href="#专栏的线性结构是什么样的">专栏的线性结构是什么样的</a></li>
      </ul>
    </li>
    <li><a href="#课前准备动手实践才是最好的学习方式">课前准备｜动手实践才是最好的学习方式</a>
      <ul>
        <li><a href="#选择什么样的实验环境">选择什么样的实验环境</a></li>
        <li><a href="#选择什么样的虚拟机软件">选择什么样的虚拟机软件</a></li>
        <li><a href="#选择哪种-linux-发行版">选择哪种 Linux 发行版</a></li>
        <li><a href="#如何配置虚拟机">如何配置虚拟机</a></li>
        <li><a href="#如何安装虚拟机">如何安装虚拟机</a></li>
        <li><a href="#有哪些常用的-linux-操作">有哪些常用的 Linux 操作</a></li>
        <li><a href="#小结">小结</a></li>
      </ul>
    </li>
    <li><a href="#入门">入门</a></li>
    <li><a href="#01初识容器">01｜初识容器</a>
      <ul>
        <li><a href="#docker-的诞生">Docker 的诞生</a></li>
        <li><a href="#docker-的形态">Docker 的形态</a></li>
        <li><a href="#docker-的安装">Docker 的安装</a></li>
        <li><a href="#docker-的使用">Docker 的使用</a></li>
        <li><a href="#docker-的架构">Docker 的架构</a></li>
        <li><a href="#小结-1">小结</a></li>
      </ul>
    </li>
    <li><a href="#02被隔离的进程一起来看看容器的本质">02｜被隔离的进程：一起来看看容器的本质</a>
      <ul>
        <li><a href="#容器到底是什么">容器到底是什么</a></li>
        <li><a href="#为什么要隔离">为什么要隔离</a></li>
        <li><a href="#与虚拟机的区别是什么">与虚拟机的区别是什么</a></li>
        <li><a href="#隔离是怎么实现的">隔离是怎么实现的</a></li>
        <li><a href="#小结-2">小结</a></li>
      </ul>
    </li>
    <li><a href="#03容器化的应用会了这些你就是docker高手">03｜容器化的应用：会了这些你就是Docker高手</a>
      <ul>
        <li><a href="#什么是容器化的应用">什么是容器化的应用</a></li>
        <li><a href="#常用的镜像操作有哪些">常用的镜像操作有哪些</a></li>
        <li><a href="#常用的容器操作">常用的容器操作</a></li>
        <li><a href="#小结-3">小结</a></li>
      </ul>
    </li>
    <li><a href="#04创建容器镜像如何编写正确高效的dockerfile">04｜创建容器镜像：如何编写正确、高效的Dockerfile</a>
      <ul>
        <li><a href="#镜像的内部机制是什么">镜像的内部机制是什么</a></li>
        <li><a href="#dockerfile-是什么">Dockerfile 是什么</a></li>
        <li><a href="#怎样编写正确高效的-dockerfile">怎样编写正确、高效的 Dockerfile</a></li>
        <li><a href="#docker-build-是怎么工作的">docker build 是怎么工作的</a></li>
        <li><a href="#小结-4">小结</a></li>
      </ul>
    </li>
    <li><a href="#05镜像仓库该怎样用好docker-hub这个宝藏">05｜镜像仓库：该怎样用好Docker Hub这个宝藏</a>
      <ul>
        <li><a href="#什么是镜像仓库registry">什么是镜像仓库（Registry）</a></li>
        <li><a href="#什么是-docker-hub">什么是 Docker Hub</a></li>
        <li><a href="#如何在-docker-hub-上挑选镜像">如何在 Docker Hub 上挑选镜像</a></li>
        <li><a href="#docker-hub-上镜像命名的规则是什么">Docker Hub 上镜像命名的规则是什么</a></li>
        <li><a href="#该怎么上传自己的镜像">该怎么上传自己的镜像</a></li>
        <li><a href="#离线环境该怎么办">离线环境该怎么办</a></li>
        <li><a href="#小结-5">小结</a></li>
      </ul>
    </li>
    <li><a href="#06打破次元壁容器该如何与外界互联互通">06｜打破次元壁：容器该如何与外界互联互通</a>
      <ul>
        <li><a href="#如何拷贝容器内的数据">如何拷贝容器内的数据</a></li>
        <li><a href="#如何共享主机上的文件">如何共享主机上的文件</a></li>
        <li><a href="#如何实现网络互通">如何实现网络互通</a></li>
        <li><a href="#如何分配服务端口号">如何分配服务端口号</a></li>
        <li><a href="#小结-6">小结</a></li>
      </ul>
    </li>
    <li><a href="#07实战演练玩转docker">07｜实战演练：玩转Docker</a>
      <ul>
        <li><a href="#容器技术要点回顾">容器技术要点回顾</a></li>
        <li><a href="#搭建私有镜像仓库">搭建私有镜像仓库</a></li>
        <li><a href="#搭建-wordpress-网站">搭建 WordPress 网站</a></li>
        <li><a href="#小结-7">小结</a></li>
      </ul>
    </li>
    <li><a href="#初级">初级</a></li>
    <li><a href="#09走近云原生如何在本机搭建小巧完备的kubernetes环境">09｜走近云原生：如何在本机搭建小巧完备的Kubernetes环境</a>
      <ul>
        <li><a href="#什么是容器编排">什么是容器编排</a></li>
        <li><a href="#什么是-kubernetes">什么是 Kubernetes</a>
          <ul>
            <li><a href="#什么是-minikube">什么是 minikube</a></li>
          </ul>
        </li>
        <li><a href="#如何搭建-minikube-环境">如何搭建 minikube 环境</a></li>
        <li><a href="#实际验证-minikube-环境">实际验证 minikube 环境</a></li>
        <li><a href="#小结-8">小结</a></li>
      </ul>
    </li>
    <li><a href="#10自动化的运维管理探究kubernetes工作机制的奥秘">10｜自动化的运维管理：探究Kubernetes工作机制的奥秘</a>
      <ul>
        <li><a href="#云计算时代的操作系统">云计算时代的操作系统</a></li>
        <li><a href="#kubernetes-的基本架构">Kubernetes 的基本架构</a></li>
        <li><a href="#节点内部的结构">节点内部的结构</a></li>
        <li><a href="#master-里的组件有哪些">Master 里的组件有哪些</a></li>
        <li><a href="#node-里的组件有哪些">Node 里的组件有哪些</a></li>
        <li><a href="#插件addons有哪些">插件（Addons）有哪些</a></li>
        <li><a href="#小结-9">小结</a></li>
      </ul>
    </li>
    <li><a href="#加餐kubernetes弃用docker是怎么回事">加餐｜Kubernetes“弃用Docker”是怎么回事？</a>
      <ul>
        <li><a href="#什么是-cri">什么是 CRI</a></li>
        <li><a href="#什么是-containerd">什么是 containerd</a></li>
        <li><a href="#正式弃用-docker">正式“弃用 Docker”</a></li>
        <li><a href="#docker-的未来">Docker 的未来</a></li>
      </ul>
    </li>
    <li><a href="#11yamlkubernetes世界里的通用语">11｜YAML：Kubernetes世界里的通用语</a>
      <ul>
        <li><a href="#声明式与命令式是怎么回事">声明式与命令式是怎么回事</a></li>
        <li><a href="#什么是-yaml">什么是 YAML</a></li>
        <li><a href="#什么是-api-对象">什么是 API 对象</a></li>
        <li><a href="#如何描述-api-对象">如何描述 API 对象</a></li>
        <li><a href="#如何编写-yaml">如何编写 YAML</a></li>
        <li><a href="#小结-10">小结</a></li>
      </ul>
    </li>
    <li><a href="#12pod如何理解这个kubernetes里最核心的概念">12｜Pod：如何理解这个Kubernetes里最核心的概念？</a>
      <ul>
        <li><a href="#为什么要有-pod">为什么要有 Pod</a></li>
        <li><a href="#为什么-pod-是-kubernetes-的核心对象">为什么 Pod 是 Kubernetes 的核心对象</a></li>
        <li><a href="#如何使用-yaml-描述-pod">如何使用 YAML 描述 Pod</a></li>
        <li><a href="#如何使用-kubectl-操作-pod">如何使用 kubectl 操作 Pod</a></li>
        <li><a href="#小结-11">小结</a></li>
      </ul>
    </li>
    <li><a href="#13jobcronjob为什么不直接用pod来处理业务">13｜Job/CronJob：为什么不直接用Pod来处理业务？</a>
      <ul>
        <li><a href="#为什么不直接使用-pod">为什么不直接使用 Pod</a></li>
        <li><a href="#为什么要有-jobcronjob">为什么要有 Job/CronJob</a></li>
        <li><a href="#如何使用-yaml-描述-job">如何使用 YAML 描述 Job</a></li>
        <li><a href="#如何在-kubernetes-里操作-job">如何在 Kubernetes 里操作 Job</a></li>
        <li><a href="#如何使用-yaml-描述-cronjob">如何使用 YAML 描述 CronJob</a></li>
        <li><a href="#小结-12">小结</a></li>
      </ul>
    </li>
    <li><a href="#14configmapsecret怎样配置定制我的应用">14｜ConfigMap/Secret：怎样配置、定制我的应用</a>
      <ul>
        <li><a href="#configmapsecret">ConfigMap/Secret</a></li>
        <li><a href="#什么是-configmap">什么是 ConfigMap</a></li>
        <li><a href="#什么是-secret">什么是 Secret</a></li>
        <li><a href="#如何使用">如何使用</a></li>
        <li><a href="#如何以环境变量的方式使用-configmapsecret">如何以环境变量的方式使用 ConfigMap/Secret</a></li>
        <li><a href="#如何以-volume-的方式使用-configmapsecret">如何以 Volume 的方式使用 ConfigMap/Secret</a></li>
        <li><a href="#小结-13">小结</a></li>
      </ul>
    </li>
    <li><a href="#15实战演练玩转kubernetes1">15｜实战演练：玩转Kubernetes（1）</a>
      <ul>
        <li><a href="#kubernetes-技术要点回顾">Kubernetes 技术要点回顾</a></li>
        <li><a href="#wordpress-网站基本架构">WordPress 网站基本架构</a></li>
        <li><a href="#wordpress-网站搭建步骤">WordPress 网站搭建步骤</a></li>
        <li><a href="#使用-dashboard-管理-kubernetes">使用 Dashboard 管理 Kubernetes</a></li>
        <li><a href="#小结-14">小结</a></li>
      </ul>
    </li>
    <li><a href="#16视频初级篇实操总结">16｜视频：初级篇实操总结</a></li>
    <li><a href="#17更真实的云原生实际搭建多节点的kubernetes集群">17｜更真实的云原生：实际搭建多节点的Kubernetes集群</a>
      <ul>
        <li><a href="#什么是-kubeadm">什么是 kubeadm</a></li>
        <li><a href="#实验环境的架构是什么样的">实验环境的架构是什么样的</a></li>
        <li><a href="#安装前的准备工作">安装前的准备工作</a></li>
        <li><a href="#安装-kubeadm">安装 kubeadm</a></li>
        <li><a href="#下载-kubernetes-组件镜像">下载 Kubernetes 组件镜像</a></li>
        <li><a href="#安装-master-节点">安装 Master 节点</a></li>
        <li><a href="#安装-flannel-网络插件">安装 Flannel 网络插件</a></li>
        <li><a href="#安装-worker-节点">安装 Worker 节点</a></li>
        <li><a href="#小结-15">小结</a></li>
      </ul>
    </li>
    <li><a href="#18deployment让应用永不宕机">18｜Deployment：让应用永不宕机</a>
      <ul>
        <li><a href="#为什么要有-deployment">为什么要有 Deployment</a></li>
        <li><a href="#如何使用-yaml-描述-deployment">如何使用 YAML 描述 Deployment</a></li>
        <li><a href="#deployment-的关键字段">Deployment 的关键字段</a></li>
        <li><a href="#如何使用-kubectl-操作-deployment">如何使用 kubectl 操作 Deployment</a></li>
        <li><a href="#小结-16">小结</a></li>
      </ul>
    </li>
    <li><a href="#19daemonset忠实可靠的看门狗">19｜Daemonset：忠实可靠的看门狗</a>
      <ul>
        <li><a href="#为什么要有-daemonset">为什么要有 DaemonSet</a></li>
        <li><a href="#如何使用-yaml-描述-daemonset">如何使用 YAML 描述 DaemonSet</a></li>
        <li><a href="#如何在-kubernetes-里使用-daemonset">如何在 Kubernetes 里使用 DaemonSet</a></li>
        <li><a href="#什么是污点taint和容忍度toleration">什么是污点（taint）和容忍度（toleration）</a></li>
        <li><a href="#什么是静态-pod">什么是静态 Pod</a></li>
        <li><a href="#小结-17">小结</a></li>
      </ul>
    </li>
    <li><a href="#20service微服务架构的应对之道">20｜Service：微服务架构的应对之道</a>
      <ul>
        <li><a href="#为什么要有-service">为什么要有 Service</a></li>
        <li><a href="#如何使用-yaml-描述-service">如何使用 YAML 描述 Service</a></li>
        <li><a href="#如何在-kubernetes-里使用-service">如何在 Kubernetes 里使用 Service</a></li>
        <li><a href="#如何以域名的方式使用-service">如何以域名的方式使用 Service</a></li>
        <li><a href="#如何让-service-对外暴露服务">如何让 Service 对外暴露服务</a></li>
        <li><a href="#小结-18">小结</a></li>
      </ul>
    </li>
    <li><a href="#21ingress集群进出流量的总管">21｜Ingress：集群进出流量的总管</a>
      <ul>
        <li><a href="#为什么要有-ingress">为什么要有 Ingress</a></li>
        <li><a href="#为什么要有-ingress-controller">为什么要有 Ingress Controller</a></li>
        <li><a href="#为什么要有-ingressclass">为什么要有 IngressClass</a></li>
        <li><a href="#如何使用-yaml-描述-ingressingress-class">如何使用 YAML 描述 Ingress/Ingress Class</a></li>
        <li><a href="#如何在-kubernetes-里使用-ingressingress-class">如何在 Kubernetes 里使用 Ingress/Ingress Class</a></li>
        <li><a href="#如何在-kubernetes-里使用-ingress-controller">如何在 Kubernetes 里使用 Ingress Controller</a></li>
        <li><a href="#小结-19">小结</a></li>
      </ul>
    </li>
    <li><a href="#22实战演练玩转kubernetes2">22｜实战演练：玩转Kubernetes（2）</a>
      <ul>
        <li><a href="#kubernetes-技术要点回顾-1">Kubernetes 技术要点回顾</a></li>
        <li><a href="#wordpress-网站基本架构-1">WordPress 网站基本架构</a></li>
        <li><a href="#1-wordpress-网站部署-mariadb">1. WordPress 网站部署 MariaDB</a></li>
        <li><a href="#2-wordpress-网站部署-wordpress">2. WordPress 网站部署 WordPress</a></li>
        <li><a href="#3-wordpress-网站部署-nginx-ingress-controller">3. WordPress 网站部署 Nginx Ingress Controller</a></li>
        <li><a href="#小结-20">小结</a></li>
      </ul>
    </li>
    <li><a href="#23视频中级篇实操总结">23｜视频：中级篇实操总结</a></li>
    <li><a href="#加餐docker-compose单机环境下的容器编排工具">加餐｜docker-compose：单机环境下的容器编排工具</a>
      <ul>
        <li><a href="#什么是-docker-compose">什么是 docker-compose</a></li>
        <li><a href="#如何使用-docker-compose">如何使用 docker-compose</a></li>
        <li><a href="#使用-docker-compose-搭建-wordpress-网站">使用 docker-compose 搭建 WordPress 网站</a></li>
        <li><a href="#小结-21">小结</a></li>
      </ul>
    </li>
    <li><a href="#24persistentvolume怎么解决数据持久化的难题">24｜PersistentVolume：怎么解决数据持久化的难题？</a>
      <ul>
        <li><a href="#什么是-persistentvolume">什么是 PersistentVolume</a></li>
        <li><a href="#什么是-persistentvolumeclaimstorageclass">什么是 PersistentVolumeClaim/StorageClass</a></li>
        <li><a href="#如何使用-yaml-描述-persistentvolume">如何使用 YAML 描述 PersistentVolume</a></li>
        <li><a href="#如何使用-yaml-描述-persistentvolumeclaim">如何使用 YAML 描述 PersistentVolumeClaim</a></li>
        <li><a href="#如何在-kubernetes-里使用-persistentvolume">如何在 Kubernetes 里使用 PersistentVolume</a></li>
        <li><a href="#如何为-pod-挂载-persistentvolume">如何为 Pod 挂载 PersistentVolume</a></li>
        <li><a href="#小结-22">小结</a></li>
      </ul>
    </li>
    <li><a href="#25persistentvolume--nfs怎么使用网络共享存储">25｜PersistentVolume + NFS：怎么使用网络共享存储？</a>
      <ul>
        <li><a href="#如何安装-nfs-服务器">如何安装 NFS 服务器</a></li>
        <li><a href="#如何安装-nfs-客户端">如何安装 NFS 客户端</a></li>
        <li><a href="#如何使用-nfs-存储卷">如何使用 NFS 存储卷</a></li>
        <li><a href="#如何部署-nfs-provisoner">如何部署 NFS Provisoner</a></li>
        <li><a href="#如何使用-nfs-动态存储卷">如何使用 NFS 动态存储卷</a></li>
        <li><a href="#小结-23">小结</a></li>
      </ul>
    </li>
    <li><a href="#26statefulset怎么管理有状态的应用">26｜StatefulSet：怎么管理有状态的应用？</a>
      <ul>
        <li><a href="#什么是有状态的应用">什么是有状态的应用</a></li>
        <li><a href="#如何使用-yaml-描述-statefulset">如何使用 YAML 描述 StatefulSet</a></li>
        <li><a href="#如何在-kubernetes-里使用-statefulset">如何在 Kubernetes 里使用 StatefulSet</a></li>
        <li><a href="#如何实现-statefulset-的数据持久化">如何实现 StatefulSet 的数据持久化</a></li>
        <li><a href="#小结-24">小结</a></li>
      </ul>
    </li>
    <li><a href="#27滚动更新如何做到平滑的应用升级降级">27｜滚动更新：如何做到平滑的应用升级降级？</a>
      <ul>
        <li><a href="#kubernetes-如何定义应用版本">Kubernetes 如何定义应用版本</a></li>
        <li><a href="#kubernetes-如何实现应用更新">Kubernetes 如何实现应用更新</a></li>
        <li><a href="#kubernetes-如何管理应用更新">Kubernetes 如何管理应用更新</a></li>
        <li><a href="#kubernetes-如何添加更新描述">Kubernetes 如何添加更新描述</a></li>
        <li><a href="#小结-25">小结</a></li>
      </ul>
    </li>
    <li><a href="#28应用保障如何让pod运行得更健康">28｜应用保障：如何让Pod运行得更健康？</a>
      <ul>
        <li><a href="#容器资源配额">容器资源配额</a></li>
        <li><a href="#什么是容器状态探针">什么是容器状态探针</a></li>
        <li><a href="#如何使用容器状态探针">如何使用容器状态探针</a></li>
        <li><a href="#小结-26">小结</a></li>
      </ul>
    </li>
    <li><a href="#29集群管理如何用名字空间分隔系统资源">29｜集群管理：如何用名字空间分隔系统资源？</a>
      <ul>
        <li><a href="#为什么要有名字空间">为什么要有名字空间</a></li>
        <li><a href="#如何使用名字空间">如何使用名字空间</a></li>
        <li><a href="#什么是资源配额">什么是资源配额</a></li>
        <li><a href="#如何使用资源配额">如何使用资源配额</a></li>
        <li><a href="#默认资源配额">默认资源配额</a></li>
        <li><a href="#小结-27">小结</a></li>
      </ul>
    </li>
    <li><a href="#30系统监控如何使用metrics-server和prometheus">30｜系统监控：如何使用Metrics Server和Prometheus？</a>
      <ul>
        <li><a href="#metrics-server">Metrics Server</a></li>
        <li><a href="#horizontalpodautoscaler">HorizontalPodAutoscaler</a></li>
        <li><a href="#prometheus">Prometheus</a></li>
        <li><a href="#小结-28">小结</a></li>
      </ul>
    </li>
    <li><a href="#31网络通信cni是怎么回事又是怎么工作的">31｜网络通信：CNI是怎么回事？又是怎么工作的？</a>
      <ul>
        <li><a href="#kubernetes-的网络模型">Kubernetes 的网络模型</a></li>
        <li><a href="#什么是-cni">什么是 CNI</a></li>
        <li><a href="#cni-插件是怎么工作的">CNI 插件是怎么工作的</a></li>
        <li><a href="#使用-calico-网络插件">使用 Calico 网络插件</a></li>
        <li><a href="#小结-29">小结</a></li>
      </ul>
    </li>
    <li><a href="#32实战演练玩转kubernetes3">32｜实战演练：玩转Kubernetes（3）</a>
      <ul>
        <li><a href="#要点回顾一api-对象">要点回顾一：API 对象</a></li>
        <li><a href="#要点回顾二应用管理">要点回顾二：应用管理</a></li>
        <li><a href="#要点回顾三集群管理">要点回顾三：集群管理</a></li>
        <li><a href="#搭建-wordpress-网站-1">搭建 WordPress 网站</a></li>
        <li><a href="#部署-dashboard">部署 Dashboard</a></li>
        <li><a href="#部署-ingressingress-controller">部署 Ingress/Ingress Controller</a></li>
        <li><a href="#访问-dashboard">访问 Dashboard</a></li>
        <li><a href="#小结-30">小结</a></li>
      </ul>
    </li>
    <li><a href="#33视频高级篇实操总结">33｜视频：高级篇实操总结</a></li>
    <li><a href="#最后">最后</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="k8s入门实战">k8s入门实战</h1>
<h2 id="开篇词迎难而上做云原生时代的弄潮儿">开篇词｜迎难而上，做云原生时代的弄潮儿</h2>
<p>你好，我是罗剑锋，不过更愿意你称呼我“Chrono”。先来简单介绍一下我自己吧。作为一个有着近二十年工作经验的“技术老兵”，我一直奋斗在开发第一线，从 Windows 到 Linux、从硬件到软件，从单机到集群、云，开发了各种形式的应用，也经历了许多大小不一的公司，现在是在 API 管理和微服务平台公司 Kong，基于 Nginx/OpenResty 研发 Kong Gateway、Kong ingress Controller 等产品。其实我应该算是极客时间的老朋友了，在 2019 年开了《透视 HTTP 协议》的课程，在 2020 年开了《C++ 实战笔记》的课程，然后因为工作上的事情比较多，“消失”了近两年的时间。不过这段日子里我倒没有“两耳不闻窗外事”，而是一直在关注业界的新技术新动向，所以今天，我再次回到了极客时间的这个大讲堂，想和你聊聊如今风头正劲的 Kubernetes。</p>
<h3 id="现在的-kubernetes">现在的 Kubernetes</h3>
<p>你一定听说过 Kubernetes 吧，也许更熟悉一点的，是许多人总挂在嘴边的缩写——“K8s”。自从 2013 年 Docker 诞生以来，容器一跃成为了 IT 界最热门的话题。而 Kubernetes 则趁着容器的“东风”，借助 Google 和 CNCF 的强力“背书”，击败了 Docker Swarm 和 Apache Mesos，成为了“容器编排”领域的王者至尊。</p>
<p>换一个更通俗易懂的说法，那就是：<strong>现在 Kubernetes 已经没有了实际意义上的竞争对手，它的地位就如同 Linux 一样，成为了事实上的云原生操作系统，是构建现代应用的基石</strong>。毕竟，现代应用是什么？是<strong>微服务，是服务网格，这些统统要围绕着容器来开发、部署和运行，而使用容器就必然要用到容器编排技术</strong>，在现在只有唯一的选项，那就是 Kubernetes。不管你是研发、测试、还是运维，不管你是前台、后台、还是中台，不管你用的是 C++、Java 还是 Python，不管你是搞数据库、区块链、还是人工智能，不管你是从事网站、电商、还是音视频，在这个“云原生”时代，Kubernetes 都是一个绕不过去的产品，是我们工作中迟早要面对的“坎儿”。</p>
<p>你也许会有疑惑：我现在的工作和“云”毫不沾边，而且 Kubernetes 都“火”了这么久，现在才开始学，会不会有点晚了？值不值呢？这里我就要引用一句老话了：“艺多不压身”，还有另一句：“机遇总是偏爱有准备的人”。“云原生”已经是现在 IT 界的普遍共识，是未来的大势所趋。也许这个“浪潮”暂时还没有打到你这里来，但一旦它真正来临，只有你提前做好了知识储备，才能够迎难而进，站上浪头成为“弄潮儿”，否则就可能会被“拍在沙滩上”。</p>
<p>我和你说一下我自己的亲身经历吧。早在 Docker 和 Kubernetes 发布之初，我就对它们有过关注。不过因为我的主要工作语言是 C/C++，而 Docker 和 Kubernetes 用的都是 Go，当时 Go 的性能还比较差（比如垃圾回收机制导致的著名 Stop the World），所以我只是简单了解了，没有去特别研究。过了几年，一个偶然的机会，我们要在客户的环境里部署自研应用，但依赖库差异太大，很难搞定。这个时候我又想起了 Docker，经过一个多星期的折腾，艰难地啃下了一大堆资料之后，总算是把系统正常上线了。虽然任务完成了，但也让我意识到自己从前对 Docker 的轻视是非常错误的，于是就痛下决心，开始从头、系统地学习整理容器知识，之后也就很自然地搭上了 Kubernetes 这条“大船”。再后来，我想换新工作，面试的时候 Boss 出了道“偏门”题，讲 Kubernetes 的容器和环境安全。虽然我不熟悉这个方向，但凭借着之前的积累，只用了一个晚上就赶出了 20 多页的 PPT，第二天面对几位评委侃侃而谈，最终顺利拿下了 Offer。</p>
<p>你看，如果我当时一味固执己见，只呆在自己的“舒适区”里，不主动去学习容器技术和 Kubernetes，当机遇不期而至的时候，很可能就会因为手足无措而错失了升职加薪的良机。所以也希望你不要犯我当初的错误，我们应当看清楚时代的走向，尽可能超前于时代，越早掌握 Kubernetes，将来自己成功的几率就越大。</p>
<h3 id="学习-kubernetes-有哪些难点">学习 Kubernetes 有哪些难点</h3>
<p>那么，我们应该怎么来学习 Kubernetes 呢？其实今天学习 Kubernetes 的难度，比起前几年来说，已经是极大地下降了，网上资料非常多，有博客、专题、视频等各种形式，而且 Kubernetes 为了推广自身，在官网上还放出了非常详细的教程和参考手册，只要你肯花时间，完全可以“自学成才”。不过，“理想很丰满，现实很骨感”。理论上讲，学习 Kubernetes 只要看资料就足够了，但实际情况却是学习起来仍然困难重重，我们会遇到很多意想不到的问题。</p>
<p>这是因为 Kubernetes 是一个分布式、集群化、云时代的系统，有许多新概念和新思维方式，和我们以往经验、认知的差异很大。我觉得，Kubernetes 技术栈的特点可以用四个字来概括，那就是“新、广、杂、深”。</p>
<p>“新”是指 Kubernetes 用到的基本上都是比较前沿、陌生的技术，而且版本升级很快，经常变来变去。“广”是指 Kubernetes 涉及的应用领域很多、覆盖面非常广，不太好找到合适的切入点或者突破口。“杂”是指 Kubernetes 的各种实现比较杂乱，谁都可以上来“掺和”一下，让人看的眼晕。“深”是指 Kubernetes 面对的每个具体问题和方向，都需要有很深的技术背景和底蕴，想要吃透很不容易。</p>
<p>这四个特点就导致 Kubernetes 的“门槛”相当高，学习曲线非常陡峭，学习成本非常昂贵，有可能花费了大量的时间和精力却南辕北辙、收效甚微，这点我确实是深有体会。比如在初学的过程中我就遇到过这些疑问，不知道你有没有同感：</p>
<p>Docker、Containerd、K8s、K3s、MicroK8s、Minikube……这么多项目，该如何选择？容器的概念太抽象了，怎么才能够快速准确地理解？镜像的命名稀奇古怪，里面的“bionic”“buster”等都是什么意思？不知道怎么搭建出 Kubernetes 环境，空有理论知识，无法联系实际。YAML 文件又长又乱，到哪里能找到说明，能否遵循什么简单规律写出来？Pod、Deployment、StatefulSet……这么多的对象，有没有什么内在的脉络和联系？</p>
<p>遗憾的是，这些问题很难在现有的 Kubernetes 资料里找到答案。我个人感觉，它们往往“站得太高”，没有为“零基础”的初学者考虑，总会预设一些前提，比如熟悉 Linux 系统、知道编程语言、了解网络技术等等，有时候还会因为版本过时而失效，或者是忽略一些关键的细节。这就让我们初学者经常“卡”在一些看似无关紧要却又非常现实的难点上，这样的点越积越多，最后就让人逐渐丧失了学习 Kubernetes 的信心和勇气。所以，我就想以自己的学习经历为基础，融合个人感悟、经验教训和心得技巧，整理出一个初学者面对 Kubernetes 这门新技术的入门路线和系统思路，让你在学习时有捷径可走，不再有迷茫和困惑，能快速高效地迈入 Kubernetes 的宏伟殿堂。</p>
<h3 id="在这个专栏里你会怎么学习-kubernetes">在这个专栏里你会怎么学习 Kubernetes</h3>
<p>讲到这里，你一定很想知道，这个专栏有什么特点，和别的课程有哪些不一样，结合刚才讲的 Kubernetes 技术栈四个特点“技术新、领域广、实现杂、方向深”，我来仔细说一说我的想法和考量。</p>
<p>第一，没有太多前提，不会 Go 你也可以学。在这门课里，我不会要求你学习 Go 语言，也不会去讲 Kubernetes 的源码。虽然是研发出身，但我并没有特别深入地了解 Go 语言，但是，我认为这反而是一个优势。因为面对 Kubernetes 的时候我和你是“平等”的，不会“下意识”地去从源码层次来讲解它的运行原理，更能够设身处地为你着想。</p>
<p>讲源码虽然会很透彻，但它的前置条件实在是太高了，不是所有的人都具备这个基础的。为了学习 Kubernetes 要先了解 Go 语言，有点“本末倒置”，如同钱钟书老先生所说：“如果你吃了个鸡蛋，觉得味道不错，何必去认识那个下蛋的母鸡呢?”（我觉得这方面也可以对比一下 Linux 操作系统，它是用 C 语言写的，但几乎没有人要求我们在学习 Linux 之前需要事先掌握 C 语言。）不过如果你真想做 Kubernetes 开发，等学会了 Kubernetes 的基本概念和用法，再回头去学 Go 语言也完全来得及。</p>
<p>第二，这个专栏我会定位在“入门”，也就是说，不会去讲那些高深的大道理和复杂的工作流程，语言也尽量朴素平实，少用专业术语和缩略词。毕竟 Kubernetes 系统涉及的领域太过庞大，对于初次接触的人来说直接“抠”内部细节不太合适，那样很容易会“跑偏”“钻牛角尖”。<strong>我觉得学习 Kubernetes 最好的方式是尽快建立一个全局观和大局观，等到你对这个陌生领域的全貌有了粗略但完整的认识之后，再挑选一个自己感兴趣的方向去研究，才是性价比最高的做法。</strong></p>
<p>而且前面也说过，Kubernetes 版本更新很快，有的功能点或许一段时间之后就成了废弃的特性（比如 ComponentStatus 在 1.19 被废弃、PodSecurityPolicy 在 1.21 被废弃），如果讲得太细，万一今后它过时无用，就实在是太尴尬了。</p>
<p>第三，课程会以实战为导向，强调眼手脑结合，鼓励你多动手、多实际操作，我认为这是这个课程最大的特点。Kubernetes 一般每年都会发布一个大版本，大版本又会有很多的小版本，每个版本都会持续改进功能特性，但一味求新，不符合当前的实际情况，毕竟生产环境里稳定是最重要的。所以，我就选择了今年（2022 年）初发布的 Kubernetes 1.23.3，它是最后一个支持 Docker 的大版本，承上启下，具有很多的新特性，同时也保留了足够的历史兼容性，非常适合用来学习 Kubernetes。</p>
<p>在课程里，我会先<strong>从 Docker 开始，陆续用 minikube、kubeadm 搭建单机和多机的 Kubernetes 集群环境</strong>，在讲解概念的同时，还会给出大量的 docker、kubectl 操作命令，让你能够看完课程后立即上手演练，用实际操作来强化学习效果。</p>
<p>第四，具体到每一节课上，我不会“贪大求全”，而是会“短小精悍”，做减法而不是加法，力争每节课只聚焦在一个知识点。</p>
<p>这是因为 Kubernetes 涉及的领域太广了，它的知识结构是网状的，之间的联系很密切，在学习时稍不注意就会跳跃到其他的地方，很容易“发散”“跑题”，导致思维不集中。所以我在讲解的时候会尽量克制，把每节课收束在一个相对独立的范围之内，不会有太多的外延话题，也不会机械地罗列 API、命令参数、属性字段（这些你都可以查阅 Kubernetes 文档），在讲解复杂的知识点时还会配上图片，让你能够精准地理解吸收知识。比如 Pod 等众多 API 对象之间的关系一直是学习 Kubernetes 的难点，单用文字很难解释清楚，所以我画了很多图，帮助你形象地理解它们的联系。就像这张：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661"
        data-srcset="https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661, https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661 1.5x, https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661"
        title="img" /></p>
<p>因为每一讲都聚焦在一个知识点上，专栏的整个结构，我也梳理出了一条独特路线：把 Kubernetes 的知识点由网状结构简化成了线性结构，你可以在这条路线上循序渐进，由浅入深、由易到难地学习完整的 Kubernetes 知识体系。</p>
<h3 id="专栏的线性结构是什么样的">专栏的线性结构是什么样的</h3>
<p>从这些设想出发，我把专栏主要划分成了五个模块。</p>
<p>课前准备 在正式学习前首先有一节课前准备，这也是我写专栏的惯例了，会跟你说一下我们学习的实验环境，用虚拟机软件搭建出一个 Linux 系统，为零基础的同学扫除一些非常简单但是其他地方可能没有讲到的后顾之忧。</p>
<p>入门篇 <strong>我会用最流行的 Docker 来讲解 Kubernetes 的基础技术</strong>：容器，让你了解它的基本原理，熟悉常用的 Docker 命令，能够轻松地拉取、构建镜像，运行容器，能够使用容器在本机搭建开发测试环境。初级篇在具备了容器的知识之后，我们就可以来<strong>学习 Kubernetes 了，用的是单机环境 minikube</strong>。你会了解为什么容器会发展到容器编排，Kubernetes 解决了什么问题，它的基本架构是什么样子的，再学习 YAML 语言、核心对象 Pod，还有离线业务对象 Job/CronJob、配置信息对象 ConfigMap/Secret。</p>
<p>中级篇 我们会在“初级篇”的基础上更进一步，<strong>使用 kubeadm 搭建出一个多节点的集群，模拟真实的生产环境，并介绍 Kubernetes 里的 4 个重要概念：Deployment、DaemonSet、Service、Ingress</strong>。学习了这些对象，你就能够明白 Kubernetes 的优点、特点是什么，知道为什么它能够一统天下，成为云原生时代的操作系统。</p>
<p>高级篇 经过前面几个模块的学习，你就已经对 Kubernetes 有了比较全面的认识了，所以我会再讲解一些深层次知识点和高级应用技巧，包括持久化存储、有状态的对象、应用的滚动更新和自动伸缩、容器和节点的管理等等。</p>
<p>当然，这种纯线性学习也难免会有缺点，我也会用其他的形式来补充完善，让你的学习过程更丰富多样，比如每一讲后面的知识小贴士、互动答疑。在专栏的中后期，我还会为你准备一些“加餐”，讲讲 Kubernetes 相关的一些“花边逸闻”，比如 docker-compose、CNCF、API Gateway 等等，扩展一些虽然是外围但也很有实际意义的知识。前面说过要多动手实践，为了强化实战效果，每个模块的知识点学完后，我都会安排一节实战演练课和一节视频课：</p>
<p>实战课，我们会应用模块中学习的知识，来实战搭建 WordPress 网站，你可以跟着课程一路走下来，看着它如何从单机应用演变到集群里的高可用系统的。视频课，我会把这个模块里大部分重要的知识点都实机操作演示给你看，相信通过“文字 + 图片 + 音频 + 视频”的多种形式，你的学习一定会非常充实而满足。你的 Kubernetes 之旅马上就要开始了，我再送你一张课程的知识地图，希望你能够在今后的三个月里以它为伴，用努力与坚持去浇灌学习之花，收获丰硕的知识和喜悦之果！</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/0b/74/0b1cfcd69fa5fd1f9a0b9dc4c5d92d74.jpg?wh=3000x3803"
        data-srcset="https://static001.geekbang.org/resource/image/0b/74/0b1cfcd69fa5fd1f9a0b9dc4c5d92d74.jpg?wh=3000x3803, https://static001.geekbang.org/resource/image/0b/74/0b1cfcd69fa5fd1f9a0b9dc4c5d92d74.jpg?wh=3000x3803 1.5x, https://static001.geekbang.org/resource/image/0b/74/0b1cfcd69fa5fd1f9a0b9dc4c5d92d74.jpg?wh=3000x3803 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/0b/74/0b1cfcd69fa5fd1f9a0b9dc4c5d92d74.jpg?wh=3000x3803"
        title="img" /></p>
<h2 id="课前准备动手实践才是最好的学习方式">课前准备｜动手实践才是最好的学习方式</h2>
<p>如果你看过我的另外两个极客时间专栏（《透视 HTTP 协议》和《C++ 实战笔记》）就会知道，我一直都很强调实验环境的重要程度，毕竟计算机这门学科的实践性要大于理论性，而且有一个能够上手操作的实际环境，对学习理论也非常有帮助。落到我们的这个 Kubernetes 学习课上，实验环境更是必不可少的，因为和网络协议、编程语言不同，Kubernetes 是一个更贴近于生产环境的庞大系统，如果“光说不练”，即使你掌握了再多的知识，但不能和实际相结合，也只能是“纸上谈兵”。俗话说：“工欲善其事，必先利其器”，所以在正式学习之前，我们必须要有一个基本的实验环境，要能够在环境中熟悉 Kubernetes 的操作命令、验证测试 Kubernetes 的各种特性，有这样的“帮手”作为辅助，我们的学习才能够事半功倍。</p>
<h3 id="选择什么样的实验环境">选择什么样的实验环境</h3>
<p>但想要得到一个完整的 Kubernetes 环境不那么容易，因为它太复杂了，对软硬件的要求都比较高，安装部署过程中还有许多的小细节，这些都会成为学习过程中的“拦路虎”。那么，应该怎么搭建出符合我们要求的实验环境呢？你也许会说：现在的云厂商到处都是，去网上申请一个就好了。这也许是一个比较便捷的获取途径，不过我有一些不同的意见。首先，这些网上的“云主机”很少是免费的，都需要花钱，而且想要好配置还要花更多的钱，对于我们的学习来说性价比不高。其次，“云主机”都是在“云”上，免不了会受网络和厂商的限制，存在不稳定因素。再次，这些“云主机”都是厂商已经为我们配好了的，很多软硬件都是固定的，不能随意定制，特别是很难真正“从零搭建”。</p>
<p>考虑上面的这三点，我建议还是在本地搭建实验环境最好，不会受制于人，完全自主可控。不过，Kubernetes 通常都运行在集群环境下，由多台服务器组成，难道我们还要自己置办几台电脑来组网吗？这倒大可不必。因为现在的虚拟机软件已经非常成熟可靠了，能够在一台电脑里虚拟出多台主机，这些虚拟主机用起来和真实的物理主机几乎没有什么差异，只要你的电脑配置不是太差，组成一个三四台虚拟服务器的小集群是毫无问题的，而且虚拟机的创建删除都非常简单，成本极低。使用虚拟机软件还有一点额外的好处，由于很多云服务商内部也在大量使用虚拟服务器，Kubernetes 里的容器技术也与虚拟机有很多相似之处，通过使用虚拟机，我们还能顺便对比这些技术的异同点，加深对 Kubernetes 的理解。</p>
<p>所以综合来看，我建议你挑选一台配置不算太差的笔记本或者台式机，在里面使用虚拟机来搭建我们这门课程的实验环境。作为宿主机电脑的 CPU 和硬盘的要求不高，4 核、300G 就可以了，关键是内存要足够大，因为虚拟机和 Kubernetes 都很能“吃”内存，最少要有 8G，这样起码能够支持开两个虚拟机组成最小的集群。</p>
<h3 id="选择什么样的虚拟机软件">选择什么样的虚拟机软件</h3>
<p>确定了我们的实验环境大方向——虚拟机之后，我们就要选择虚拟机软件了。目前市面上的主流虚拟机软件屈指可数，所以选择起来并不算困难，我个人推荐的有两个：VirtualBox 和 VMWare Fusion。</p>
<p>我们先讲适用面广的 VirtualBox。VirtualBox 是 Oracle 推出的一款虚拟机软件，历史很悠久，一直坚持免费政策，使用条款上也没有什么限制，是一个难得的精品软件。VirtualBox 支持 Windows 和 macOS，但有一个小缺点，它只能运行在 Intel（x86_64）芯片上，不支持 Apple 新出的 M1（arm64/aarch64）芯片，这导致它无法在新款 Mac 上使用，不得不说是一大遗憾。所以，如果你手里是 Apple M1 Mac，就只能选择其他的虚拟机软件了。在 macOS 上，虚拟机最出名的应该是 Parallel Desktop 和 VMWare Fusion 这两个了，都需要付费。这里我比较推荐 VMWare Fusion。不过对于 VMWare Fusion 来说，它对 M1 的支持进展比较迟缓，所以在正式的付费版出来之前，公布了一个“技术预览版”，是完全免费的，而且功能、性能也比较好，虽然有使用时间的限制（大约 300 天），但对于我们的学习来说是足够了。这里我给出 VirtualBox（https://www.virtualbox.org/wiki/Downloads）和 VMWare Fusion（https://communities.vmware.com/t5/Fusion-for-Apple-Silicon-Tech/ct-p/3022）的网址，你可以课后去看一下，尽快下载。</p>
<h3 id="选择哪种-linux-发行版">选择哪种 Linux 发行版</h3>
<p>有了虚拟机软件之后，我们就要在上面安装操作系统，在这方面毫无疑问只能是 Linux，因为 <strong>Kubernetes 只能运行在 Linux 之上</strong>。不过麻烦的是，Linux 世界又分裂成很多不同的发行版，流行的有 CentOS/Fedora、 Ubuntu/Debian、SUSE 等等，没有一个占据绝对统治地位的系统。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/70/06/701a9e9a7757be4198f0e9d5a7175406.jpg?wh=662x628"
        data-srcset="https://static001.geekbang.org/resource/image/70/06/701a9e9a7757be4198f0e9d5a7175406.jpg?wh=662x628, https://static001.geekbang.org/resource/image/70/06/701a9e9a7757be4198f0e9d5a7175406.jpg?wh=662x628 1.5x, https://static001.geekbang.org/resource/image/70/06/701a9e9a7757be4198f0e9d5a7175406.jpg?wh=662x628 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/70/06/701a9e9a7757be4198f0e9d5a7175406.jpg?wh=662x628"
        title="img" /></p>
<p>那选哪个比较好呢？我们的主要目的是学习，所以易用性应该是首要关注点，另外系统还应该能够同时支持 x86_64 和 arm64。筛选下来我建议选择 Ubuntu 22.04 Jammy Jellyfish 桌面版（https://ubuntu.com/download/desktop），它有足够新的特性，非常适合运行 Kubernetes，而内置的浏览器、终端等工具也很方便我们的调试和测试。但对 Apple M1 用户来说，有一个不太好的消息，Ubuntu 22.04 在内核由 5.13 升级到 5.15 的时候引入了一个小 Bug，导致 VMWare Fusion 无法正常安装启动，这个问题直到 4 月份的正式版发布还没有解决。好在我当初为了测试，下载了一个较早的“daily build”版本，它可以在 VMWare Fusion 里正常安装，我把它上传到了云盘（https://www.aliyundrive.com/s/zzKcAQwQjR9），你可以下载后使用。需要注意一点，由于网站的限制，文件的后缀名被改成了 .mov ，你必须去掉这个后缀，还原成原始的 .iso 才能使用。</p>
<h3 id="如何配置虚拟机">如何配置虚拟机</h3>
<p>准备好虚拟机软件和 Ubuntu 光盘镜像之后，我们就可以来安装虚拟机了。不过在安装之前，我们必须要把虚拟机适当地配置一下。因为 Kubernetes 不是一般的应用软件，而是一个复杂的系统软件，对硬件资源的要求有一点高，好在并不太高，2 核 CPU、2G 内存是最低要求，如果条件允许，我建议把内存增大到 4G，硬盘 40G 以上，这样运行起来会更流畅一些。另外，一些对于服务器来说不必要的设备也可以禁用或者删除，比如声卡、摄像头、软驱等等，可以节约一点系统资源。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3a/1d/3a6a52f38yy431abf4a0625e2532b01d.png?wh=1504x920"
        data-srcset="https://static001.geekbang.org/resource/image/3a/1d/3a6a52f38yy431abf4a0625e2532b01d.png?wh=1504x920, https://static001.geekbang.org/resource/image/3a/1d/3a6a52f38yy431abf4a0625e2532b01d.png?wh=1504x920 1.5x, https://static001.geekbang.org/resource/image/3a/1d/3a6a52f38yy431abf4a0625e2532b01d.png?wh=1504x920 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3a/1d/3a6a52f38yy431abf4a0625e2532b01d.png?wh=1504x920"
        title="img" /></p>
<p>由于 Linux 服务器大多数要以终端登录的方式使用，多台服务器还要联网，所以在网络方面我们还需要特别设置。前面说虚拟机软件首选 VirtualBox，Apple M1 Mac 备选 VMWare Fusion 技术预览版，这里我也分别说下两个软件的不同设置。对于 VirtualBox，首先，你需要在“工具 - 网络”里创建一个“Host-only”的网络，IP 地址段随意，比如这里就使用了它自动分配的“192.168.56.1/24”：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/aa/f1/aacc45aayyc2e2b9dc870c8c233e53f1.png?wh=1764x1368"
        data-srcset="https://static001.geekbang.org/resource/image/aa/f1/aacc45aayyc2e2b9dc870c8c233e53f1.png?wh=1764x1368, https://static001.geekbang.org/resource/image/aa/f1/aacc45aayyc2e2b9dc870c8c233e53f1.png?wh=1764x1368 1.5x, https://static001.geekbang.org/resource/image/aa/f1/aacc45aayyc2e2b9dc870c8c233e53f1.png?wh=1764x1368 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/aa/f1/aacc45aayyc2e2b9dc870c8c233e53f1.png?wh=1764x1368"
        title="img" /></p>
<p>然后，在虚拟机的配置里，你需要启用两个网卡。“网卡 1”就设置成刚才创建的“Host-only”网络，它是我们在本地终端登录和联网时用的；而“网卡 2”是“网络地址转换（NAT）”，用来上外网：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/ba/d8/ba4c453893b38223aa10989b2c3240d8.png?wh=1560x906"
        data-srcset="https://static001.geekbang.org/resource/image/ba/d8/ba4c453893b38223aa10989b2c3240d8.png?wh=1560x906, https://static001.geekbang.org/resource/image/ba/d8/ba4c453893b38223aa10989b2c3240d8.png?wh=1560x906 1.5x, https://static001.geekbang.org/resource/image/ba/d8/ba4c453893b38223aa10989b2c3240d8.png?wh=1560x906 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/ba/d8/ba4c453893b38223aa10989b2c3240d8.png?wh=1560x906"
        title="img" /></p>
<p>对于 VMWare Fusion，你需要在“偏好设置 - 网络”里，添加一个自定义的网络，比如这里的“vmnet3”，网段是“192.168.10.0”，允许使用 NAT 连接外网，然后在虚拟机的网络设置里选用这个网络：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/82/48/829a9212b4b1ac7cb2f2f087ebe7e848.png?wh=1368x1638"
        data-srcset="https://static001.geekbang.org/resource/image/82/48/829a9212b4b1ac7cb2f2f087ebe7e848.png?wh=1368x1638, https://static001.geekbang.org/resource/image/82/48/829a9212b4b1ac7cb2f2f087ebe7e848.png?wh=1368x1638 1.5x, https://static001.geekbang.org/resource/image/82/48/829a9212b4b1ac7cb2f2f087ebe7e848.png?wh=1368x1638 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/82/48/829a9212b4b1ac7cb2f2f087ebe7e848.png?wh=1368x1638"
        title="img" /></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/1f/bf/1f6d264abdfd2ded54c12c57d89971bf.png?wh=1504x1072"
        data-srcset="https://static001.geekbang.org/resource/image/1f/bf/1f6d264abdfd2ded54c12c57d89971bf.png?wh=1504x1072, https://static001.geekbang.org/resource/image/1f/bf/1f6d264abdfd2ded54c12c57d89971bf.png?wh=1504x1072 1.5x, https://static001.geekbang.org/resource/image/1f/bf/1f6d264abdfd2ded54c12c57d89971bf.png?wh=1504x1072 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/1f/bf/1f6d264abdfd2ded54c12c57d89971bf.png?wh=1504x1072"
        title="img" /></p>
<h3 id="如何安装虚拟机">如何安装虚拟机</h3>
<p>把 CPU、内存、硬盘、网络都配置好之后，再加载上 Ubuntu 22.04 的光盘镜像，我们就可以开始安装 Linux 了。在安装的过程中，为了节约时间，建议选择“最小安装”，同时物理断网，避免下载升级包。注意，断网对于 Apple M1 来说特别重要，否则 Ubuntu 会自动更新到 5.15 内核，导致安装后无法正常启动。安装完 Linux 系统之后，我们还要再做一些环境的初始化操作。首先我们需要用 Ctrl + Alt + T 打开命令行窗口，然后用 apt 从 Ubuntu 的官方软件仓库安装 git、vim、curl 等常用工具：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo apt update
sudo apt install -y git vim curl jq
</code></pre></td></tr></table>
</div>
</div><p>Ubuntu 桌面版默认是不支持远程登录的，所以为了让后续的实验更加便利，我们还需要安装“openssh-server”，再使用命令 ip addr ，查看虚拟机的 IP 地址，然后就可以在宿主机上使用 ssh 命令登录虚拟机：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo apt install -y openssh-server
ip addr
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/59/50/59c0c45afe6538a9b6837d5277da6e50.png?wh=936x838"
        data-srcset="https://static001.geekbang.org/resource/image/59/50/59c0c45afe6538a9b6837d5277da6e50.png?wh=936x838, https://static001.geekbang.org/resource/image/59/50/59c0c45afe6538a9b6837d5277da6e50.png?wh=936x838 1.5x, https://static001.geekbang.org/resource/image/59/50/59c0c45afe6538a9b6837d5277da6e50.png?wh=936x838 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/59/50/59c0c45afe6538a9b6837d5277da6e50.png?wh=936x838"
        title="img" /></p>
<p>从这个截图里可以看到，这台 VirtualBox 虚拟机有 3 个网卡，其中名字是“enp0s3”的网卡就是我们之前配置的“192.168.56.1/24”网段，IP 地址是自动分配的“192.168.56.11”。如果你对自动分配的 IP 地址不是很满意，也可以在 Ubuntu 右上角的系统设置里修改网卡，把它从动态地址（DHCP）改成静态地址（Manual），具体的参数可以参考下面的截图，重启后新的 IP 地址就生效了。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a3/88/a3d2749f6ea7f3327c7efd09116b5b88.png?wh=936x838"
        data-srcset="https://static001.geekbang.org/resource/image/a3/88/a3d2749f6ea7f3327c7efd09116b5b88.png?wh=936x838, https://static001.geekbang.org/resource/image/a3/88/a3d2749f6ea7f3327c7efd09116b5b88.png?wh=936x838 1.5x, https://static001.geekbang.org/resource/image/a3/88/a3d2749f6ea7f3327c7efd09116b5b88.png?wh=936x838 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a3/88/a3d2749f6ea7f3327c7efd09116b5b88.png?wh=936x838"
        title="img" /></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/yy/61/yy8d883fe7b55a6f6fdf3cefd8990661.png?wh=1920x1569"
        data-srcset="https://static001.geekbang.org/resource/image/yy/61/yy8d883fe7b55a6f6fdf3cefd8990661.png?wh=1920x1569, https://static001.geekbang.org/resource/image/yy/61/yy8d883fe7b55a6f6fdf3cefd8990661.png?wh=1920x1569 1.5x, https://static001.geekbang.org/resource/image/yy/61/yy8d883fe7b55a6f6fdf3cefd8990661.png?wh=1920x1569 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/yy/61/yy8d883fe7b55a6f6fdf3cefd8990661.png?wh=1920x1569"
        title="img" /></p>
<p>这些工作完成之后，我建议你再给虚拟机拍个快照，做好备份工作，这样万一后面有什么意外发生环境被弄乱了，也可以轻松回滚到拍快照时的正确状态。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b2/f8/b2291c4ed75a8cd6248202c461de4ff8.png?wh=1234x1316"
        data-srcset="https://static001.geekbang.org/resource/image/b2/f8/b2291c4ed75a8cd6248202c461de4ff8.png?wh=1234x1316, https://static001.geekbang.org/resource/image/b2/f8/b2291c4ed75a8cd6248202c461de4ff8.png?wh=1234x1316 1.5x, https://static001.geekbang.org/resource/image/b2/f8/b2291c4ed75a8cd6248202c461de4ff8.png?wh=1234x1316 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b2/f8/b2291c4ed75a8cd6248202c461de4ff8.png?wh=1234x1316"
        title="img" /></p>
<p>现在，让我们启动一个命令行终端（我用的是 Mac 里的“iTerm2”），使用 ssh ，输入用户名、密码和 IP 地址，就能够登录创建好的虚拟机了：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/89/0c/89d4ccc118a6483f94cf9ebde548a30c.png?wh=1920x728"
        data-srcset="https://static001.geekbang.org/resource/image/89/0c/89d4ccc118a6483f94cf9ebde548a30c.png?wh=1920x728, https://static001.geekbang.org/resource/image/89/0c/89d4ccc118a6483f94cf9ebde548a30c.png?wh=1920x728 1.5x, https://static001.geekbang.org/resource/image/89/0c/89d4ccc118a6483f94cf9ebde548a30c.png?wh=1920x728 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/89/0c/89d4ccc118a6483f94cf9ebde548a30c.png?wh=1920x728"
        title="img" /></p>
<h3 id="有哪些常用的-linux-操作">有哪些常用的 Linux 操作</h3>
<p>到这里，我们的实验环境就算是搭建完毕了，虽然目前只有最基本的 Linux 系统，但在后面的“入门篇”“初级篇”“中级篇”里，我们会以它为基础逐步完善，实现完整的 Kubernetes 环境。特别提醒一下，因为 Kubernetes 基于 Linux，虽然也有图形化的 Dashboard，但更多的时候都是在命令行里工作，所以你需要对基本的 Linux 操作有所了解。学习 Linux 操作系统是另外一个很大的话题了，虽然它很重要，但并不是我们这门课的目标，我这里简单列一些比较常用的知识，你可以检测一下自己的掌握程度，如果有不了解的，希望你课后再查找相关资料补上这些点：</p>
<p>命令行界面称为“Shell”，支持交互操作，也支持脚本操作，也就是“Shell 编程”。root 用户有最高权限，但有安全隐患，所以通常我们都只使用普通用户身份，必要的时候使用 sudo 来临时使用 root 权限。查看系统当前进程列表的命令是 ps ，它是 Linux 里最常用的命令之一。查看文件可以使用 cat ，如果内容太多，可以用管道符 | ，后面跟 more 、less 。vim 是 Linux 里最流行的编辑器，但它的使用方式与一般的编辑器不同，学习成本略高。curl 能够以命令行的方式发送 HTTP 请求，多用来测试 HTTP 服务器（例如 Nginx）。</p>
<h3 id="小结">小结</h3>
<p>好了，我们的课前准备就要结束了，我再简单小结一下今天的要点内容：一个完善的实验环境能够很好地辅助我们的学习，建议在本地使用虚拟机从零开始搭建 Kubernetes 环境。虚拟机软件可以选择 VirtualBox（intel 芯片）和 VMWare Fusion（Apple M1 芯片），因为 Kubernetes 只能运行在 Linux 上，建议选择最新的 Ubuntu 22.04。虚拟机要事先配置好内存、网络等参数，安装系统时选最小安装，然后再安装一些常用的工具。虚拟机都支持快照，环境设置好后要及时备份，出现问题可以随时回滚恢复，避免重复安装系统浪费时间。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/31/ec/31aa9e27b25ef630f987ae17de070cec.jpg?wh=1920x1025"
        data-srcset="https://static001.geekbang.org/resource/image/31/ec/31aa9e27b25ef630f987ae17de070cec.jpg?wh=1920x1025, https://static001.geekbang.org/resource/image/31/ec/31aa9e27b25ef630f987ae17de070cec.jpg?wh=1920x1025 1.5x, https://static001.geekbang.org/resource/image/31/ec/31aa9e27b25ef630f987ae17de070cec.jpg?wh=1920x1025 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/31/ec/31aa9e27b25ef630f987ae17de070cec.jpg?wh=1920x1025"
        title="img" /></p>
<p>另外，我写专栏的惯例是在 GitHub 上开一个配套的学习项目，这门课程的仓库就叫“k8s_study”（https://github.com/chronolaw/k8s_study），里面有文档链接、安装脚本、测试命令、YAML 描述文件等等，你可以克隆下来在后续的课程中参照着学习。</p>
<h2 id="入门">入门</h2>
<h2 id="01初识容器">01｜初识容器</h2>
<p>俗话说：“万事开头难”，对于 Kubernetes 这个庞大而陌生的领域来说更是如此，如何迈出学习的第一步非常关键，所以，今天我们先从最简单、最基本的知识入手，聊聊最流行的容器技术 Docker，先搭建实验环境，再动手操作一下，进而破除它的神秘感。</p>
<h3 id="docker-的诞生">Docker 的诞生</h3>
<p>现在我们都已经对 Container、Kubernetes 这些技术名词耳熟能详了，但你知道这一切的开端——Docker，第一次在世界上的亮相是什么样子的吗？九年前，也就是 2013 年 3 月 15 日，在北美的圣克拉拉市召开了一场 Python 开发者社区的主题会议 PyCon，研究和探讨各种 Python 开发技术和应用，与我们常说的“云”“PaaS”“SaaS”根本毫不相关。在当天的会议日程快结束时，有一个“闪电演讲”（lighting talk）的小环节。其中有一位开发者，用了 5 分钟的时间，做了题为 “The future of Linux Containers” 的演讲，不过临近末尾因为超时而被主持人赶下了台，场面略显尴尬（你可以在<a href="https://www.youtube.com/watch?v=wW9CAH9nSLs" target="_blank" rel="noopener noreffer">这里</a>回看这段具有历史意义的视频）。</p>
<p>相信你一定猜到了，这个只有短短 5 分钟的技术演示，就是我们目前所看到的、席卷整个业界的云原生大潮的开端。正是在这段演讲里，Solomon Hykes（dotCloud 公司，也就是 Docker 公司的创始人）首次向全世界展示了 Docker 技术。5 分钟的时间非常短，但演讲里却包含了几个现在已经普及，但当时却非常新奇的概念，比如容器、镜像、隔离运行进程等，信息量非常大。PyCon2013 大会之后，许多人都意识到了容器的价值和重要性，发现它能够解决困扰了云厂商多年的打包、部署、管理、运维等问题，Docker 也就迅速流行起来，成为了 GitHub 上的明星项目。然后在几个月的时间里，Docker 更是吸引了 Amazon、Google、Red Hat 等大公司的关注，这些公司利用自身的技术背景，纷纷在容器概念上大做文章，最终成就了我们今天所看到的至尊王者 Kubernetes 的出现。</p>
<h3 id="docker-的形态">Docker 的形态</h3>
<p>好了，下面我们就要来一个“情境再现”，在我们的 Linux 虚拟机上搭建一个容器运行环境，模拟一下当年 Solomon Hykes 初次展示 Docker 的场景。当然，如今的 Docker 经过了九年的发展，已经远不是当初的“吴下阿蒙”了，不过最核心的那些概念和操作还是保持了一贯性，没有太大的变化。首先，我们需要对 <strong>Docker 的形态</strong>有所了解。目前使用 Docker 基本上有两个选择：<strong>Docker Desktop 和 Docker Engine。</strong></p>
<p>Docker Desktop 是专门针对个人使用而设计的，支持 Mac 和 Windows 快速安装，具有直观的图形界面，还集成了许多周边工具，方便易用。不过，我个人不是太推荐使用 Docker Desktop，原因有两个。第一个，它是<strong>商业产品</strong>，难免会带有 Docker 公司的“私人气息”，有一些自己的、非通用的东西，不利于我们后续的 Kubernetes 学习。第二个，它只是对个人学习免费，<strong>受条款限制不能商用</strong>，我们在日常工作中难免会“踩到雷区”。Docker Engine 则和 Docker Desktop 正好相反，完<strong>全免费，但只能在 Linux 上运行，只能使用命令行操作，缺乏辅助工具，需要我们自己动手 DIY 运行环境</strong>。不过要是较起真来，它才是 Docker 当初的真正形态，“血脉”最纯正，也是现在各个公司在生产环境中实际使用的 Docker 产品，毕竟机房里 99% 的服务器跑的都是 Linux。所以，在接下来的学习过程里，我推荐使用 Docker Engine，之后在本专栏内，如果没有什么特别的声明，Docker 这个词通常指的就是 Docker Engine。</p>
<h3 id="docker-的安装">Docker 的安装</h3>
<p>在课前准备里，我们已经在 Linux 虚拟机里安装了一些常用软件，用的是 Ubuntu 的包管理工具 apt，所以，我们仍然可以使用同样的方式来安装 Docker。先让我们尝试输入命令 docker ，会得到“命令未找到”的提示，还有如何安装的建议：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Command &#39;docker&#39; not found, but can be installed with:
sudo apt install docker.io
</code></pre></td></tr></table>
</div>
</div><p>所以，你只需要按照系统的提示，“照葫芦画瓢”输入命令，安装 docker.io 就可以了。为了方便，你还可以使用 -y 参数来避免确认，实现自动化操作：</p>
<p>极客时间版权所有: <a href="https://time.geekbang.org/column/article/528619" target="_blank" rel="noopener noreffer">https://time.geekbang.org/column/article/528619</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo apt install -y docker.io #安装Docker Engine
</code></pre></td></tr></table>
</div>
</div><p>刚才说过，Docker Engine 不像 Docker Desktop 那样可以安装后就直接使用，必须要做一些手工调整才能用起来，所以你还要在安装完毕后执行下面的两条命令：</p>
<p>极客时间版权所有: <a href="https://time.geekbang.org/column/article/528619" target="_blank" rel="noopener noreffer">https://time.geekbang.org/column/article/528619</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo service docker start         #启动docker服务
sudo usermod -aG docker ${USER}   #当前用户加入docker组
</code></pre></td></tr></table>
</div>
</div><p>第一个 service docker start 是启动 Docker 的后台服务，第二个 usermod -aG 是把当前的用户加入 Docker 的用户组。这是因为操作 Docker 必须要有 root 权限，而直接使用 root 用户不够安全，加入 Docker 用户组是一个比较好的选择，这也是 Docker 官方推荐的做法。当然，如果只是为了图省事，你也可以直接切换到 root 用户来操作 Docker。上面的三条命令执行完之后，我们还需要退出系统（命令 exit ），再重新登录一次，这样才能让修改用户组的命令 usermod 生效。现在我们就可以来验证 Docker 是否安装成功了，使用的命令是 docker version 和 docker info。docker version 会输出 Docker 客户端和服务器各自的版本信息：</p>
<p>极客时间版权所有: <a href="https://time.geekbang.org/column/article/528619" target="_blank" rel="noopener noreffer">https://time.geekbang.org/column/article/528619</a>
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/fa/f9/fa0088c858d63d6b423155f854a1ddf9.png?wh=1262x1746"
        data-srcset="https://static001.geekbang.org/resource/image/fa/f9/fa0088c858d63d6b423155f854a1ddf9.png?wh=1262x1746, https://static001.geekbang.org/resource/image/fa/f9/fa0088c858d63d6b423155f854a1ddf9.png?wh=1262x1746 1.5x, https://static001.geekbang.org/resource/image/fa/f9/fa0088c858d63d6b423155f854a1ddf9.png?wh=1262x1746 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/fa/f9/fa0088c858d63d6b423155f854a1ddf9.png?wh=1262x1746"
        title="https://static001.geekbang.org/resource/image/fa/f9/fa0088c858d63d6b423155f854a1ddf9.png?wh=1262x1746" /></p>
<p>下面是我从中摘出的比较关键的版本号和系统信息。可以看到，我使用的是 Docker Engine 20.10.12，系统是 Linux，硬件架构是 arm64，也就是 Apple M1：</p>
<p>极客时间版权所有: <a href="https://time.geekbang.org/column/article/528619" target="_blank" rel="noopener noreffer">https://time.geekbang.org/column/article/528619</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Client:
 Version:           20.10.12
 OS/Arch:           linux/arm64
Server:
 Engine:
  Version:          20.10.12
  OS/Arch:          linux/arm64
</code></pre></td></tr></table>
</div>
</div><p>docker info 会显示当前 Docker 系统相关的信息，例如 CPU、内存、容器数量、镜像数量、容器运行时、存储文件系统等等，这里我也摘录了一部分：</p>
<p>极客时间版权所有: <a href="https://time.geekbang.org/column/article/528619" target="_blank" rel="noopener noreffer">https://time.geekbang.org/column/article/528619</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">
Server:
 Containers: 1
  Running: 0
  Paused: 0
  Stopped: 1
 Images: 8
 Server Version: 20.10.12
 Storage Driver: overlay2
  Backing Filesystem: extfs
 Cgroup Driver: systemd
 Default Runtime: runc
 Kernel Version: 5.13.0-19-generic
 Operating System: Ubuntu Jammy Jellyfish (development branch)
 OSType: linux
 Architecture: aarch64
 CPUs: 2
 Total Memory: 3.822GiB
 Docker Root Dir: /var/lib/docker
</code></pre></td></tr></table>
</div>
</div><p>docker info 显示的这些信息，对于我们了解 Docker 的内部运行状态非常有用，比如在这里，你就能够看到当前有一个容器处于停止状态，有 8 个镜像，存储用的文件系统是 overlay2，Linux 内核是 5.13，操作系统是 Ubuntu 22.04 Jammy Jellyfish，硬件是 aarch64，两个 CPU，内存 4G。</p>
<p>极客时间版权所有: <a href="https://time.geekbang.org/column/article/528619" target="_blank" rel="noopener noreffer">https://time.geekbang.org/column/article/528619</a></p>
<h3 id="docker-的使用">Docker 的使用</h3>
<p>现在，我们已经有了可用的 Docker 运行环境，就可以来重现 9 年前 Solomon Hykes 的那场简短的技术演示了。首先，我们使用命令 docker ps，它会列出当前系统里运行的容器，就像我们在 Linux 系统里使用 ps 命令列出运行的进程一样。注意，所有的 Docker 操作都是这种形式：以 docker 开始，然后是一个具体的子命令，之前的 docker version 和 docker info 也遵循了这样的规则。你还可以用 help 或者 &ndash;help 来获取帮助信息，查看命令清单和更详细的说明。</p>
<p>因为我们刚刚安装好 Docker 环境，这个时候还没有运行任何容器，所以列表显然是空的。</p>
<p>接下来，让我们尝试另一个非常重要的命令 docker pull ，从外部的镜像仓库（Registry）拉取一个 busybox 镜像（image），你可以把它类比成是 Ubuntu 里的“apt install”下载软件包：</p>
<p>docker pull 会有一些看起来比较奇怪的输出信息，现在我们暂时不用管，后续的课程会有详细解释。我们再执行命令 docker images ，它会列出当前 Docker 所存储的所有镜像：</p>
<p>可以看到，命令会显示有一个叫 busybox 的镜像，镜像的 ID 号是一串 16 进制数字，大小是 1.41MB。现在，我们就要从这个镜像启动容器了，命令是 docker run ，执行 echo 输出字符串，这也正是 Solomon Hykes 在大会上所展示的最精彩的那部分：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run busybox echo hello world
</code></pre></td></tr></table>
</div>
</div><p>然后我们再用 docker ps 命令，加上一个参数 -a ，就可以看到这个已经运行完毕的容器</p>
<h3 id="docker-的架构">Docker 的架构</h3>
<p>这里我再稍微讲一下 Docker Engine 的架构，让你有个初步的印象，也为之后的学习做一个铺垫。下面的这张图来自 Docker 官网（https://docs.docker.com/get-started/overview/），精准地描述了 Docker Engine 的内部角色和工作流程，对我们的学习研究非常有指导意义。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        data-srcset="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 1.5x, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        title="img" /></p>
<p>刚才我们敲的命令行 docker 实际上是一个客户端 client ，它会与 Docker Engine 里的后台服务 Docker daemon 通信，而镜像则存储在远端的仓库 Registry 里，客户端并不能直接访问镜像仓库。Docker client 可以通过 build、pull、run等命令向 Docker daemon 发送请求，而 Docker daemon 则是容器和镜像的“大管家”，负责从远端拉取镜像、在本地存储镜像，还有从镜像生成容器、管理容器等所有功能。所以，在 Docker Engine 里，真正干活的其实是默默运行在后台的 Docker daemon，而我们实际操作的命令行工具“docker”只是个“传声筒”的角色。Docker 官方还提供一个“hello-world”示例，可以为你展示 Docker client 到 Docker daemon 再到 Registry 的详细工作流程，你只需要执行这样一个命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run hello-world
</code></pre></td></tr></table>
</div>
</div><p>它会先检查本地镜像，如果没有就从远程仓库拉取，再运行容器，最后输出运行信息：</p>
<h3 id="小结-1">小结</h3>
<p>好了，今天我们初步了解了容器技术，再简单小结一下主要的内容：容器技术起源于 Docker，它目前有两个产品：Docker Desktop 和 Docker Engine，我们的课程里推荐使用免费的 Docker Engine，它可以在 Ubuntu 系统里直接用 apt 命令安装。Docker Engine 需要使用命令行操作，主命令是 docker，后面再接各种子命令。查看 Docker 的基本信息的命令是 docker version 和 docker info ，其他常用的命令有 docker ps、docker pull、docker images、docker run。Docker Engine 是典型的客户端 / 服务器（C/S）架构，命令行工具 Docker 直接面对用户，后面的 Docker daemon 和 Registry 协作完成各种功能。</p>
<h2 id="02被隔离的进程一起来看看容器的本质">02｜被隔离的进程：一起来看看容器的本质</h2>
<p>广义上来说，容器技术是动态的容器、静态的镜像和远端的仓库这三者的组合。不过，“容器”这个术语作为容器技术里的核心概念，不仅是大多数初次接触这个领域的人，即使是一些已经有使用经验的人，想要准确地把握它们的内涵、本质都是比较困难的。那么今天，我们就一起来看看究竟什么是容器（即狭义的、动态的容器）。</p>
<h3 id="容器到底是什么">容器到底是什么</h3>
<p>从字面上来看，容器就是 Container，一般把它形象地比喻成现实世界里的集装箱，它也正好和 Docker 的现实含义相对应，因为码头工人（那只可爱的小鲸鱼）就是不停地在搬运集装箱。</p>
<p>集装箱的作用是标准化封装各种货物，一旦打包完成之后，就可以从一个地方迁移到任意的其他地方。相比散装形式而言，集装箱隔离了箱内箱外两个世界，保持了货物的原始形态，避免了内外部相互干扰，极大地简化了商品的存储、运输、管理等工作。再回到我们的计算机世界，容器也发挥着同样的作用，不过它封装的货物是运行中的应用程序，也就是进程，同样它也会把进程与外界隔离开，让进程与外部系统互不影响。我们还是来实际操作一下吧，来看看在容器里运行的进程是个什么样子。首先，我们使用 docker pull 命令，拉取一个新的镜像——操作系统 Alpine：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker pull alpine
</code></pre></td></tr></table>
</div>
</div><p>然后我们使用 docker run 命令运行它的 Shell 程序：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -it alpine sh
</code></pre></td></tr></table>
</div>
</div><p>注意我们在这里多加了一个 -it 参数，这样我们就会暂时离开当前的 Ubuntu 操作系统，进入容器内部。现在，让我们执行 cat /etc/os-release ，还有 ps 这两个命令，最后再使用 exit 退出，看看容器里与容器外有什么不同：</p>
<p>就像这张截图里所显示的，在容器里查看系统信息，会发现已经不再是外面的 Ubuntu 系统了，而是变成了 Alpine Linux 3.15，使用 ps 命令也只会看到一个完全“干净”的运行环境，除了 Shell（即 sh）没有其他的进程存在。也就是说，在容器内部是一个全新的 Alpine 操作系统，在这里运行的应用程序完全看不到外面的 Ubuntu 系统，两个系统被互相“隔离”了，就像是一个“世外桃源”。我们还可以再拉取一个 Ubuntu 18.04 的镜像，用同样的方式进入容器内部，然后执行 apt update、apt install 等命令来看看：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker pull ubuntu:18.04
docker run -it ubuntu:18.04 sh

# 下面的命令都是在容器内执行
cat /etc/os-release
apt update
apt install -y wget redis
redis-server &amp;
</code></pre></td></tr></table>
</div>
</div><p>这里我就不截图了，具体的结果留给你课下去实际操作体会。可以看到的是，容器里是另一个完整的 Ubuntu 18.04 系统，我们可以在这个“世外桃源”做任意的事情，比如安装应用、运行 Redis 服务等。但无论我们在容器里做什么，都不会影响外面的 Ubuntu 系统（当然不是绝对的）。到这里，我们就可以得到一个初步的结论：<strong>容器，就是一个特殊的隔离环境，它能够让进程只看到这个环境里的有限信息，不能对外界环境施加影响</strong>。那么，很自然地，我们会产生另外一个问题：为什么需要创建这样的一个隔离环境，直接让进程在系统里运行不好吗？</p>
<h3 id="为什么要隔离">为什么要隔离</h3>
<p>相信因为这两年疫情，你对“隔离”这个词不会感觉到太陌生。为了防止疫情蔓延，我们需要建立方舱、定点医院，把患病人群控制在特定的区域内，更进一步还会实施封闭小区、关停商场等行动。虽然这些措施带来了一些不便，但都是为了整个社会更大范围的正常运转。同样的，在计算机世界里的隔离也是出于同样的考虑，也就是<strong>系统安全</strong>。对于 Linux 操作系统来说，一个不受任何限制的应用程序是十分危险的。这个进程能够看到系统里所有的文件、所有的进程、所有的网络流量，访问内存里的任何数据，那么恶意程序很容易就会把系统搞瘫痪，正常程序也可能会因为无意的 Bug 导致信息泄漏或者其他安全事故。虽然 Linux 提供了用户权限控制，能够限制进程只访问某些资源，但这个机制还是比较薄弱的，和真正的“隔离”需求相差得很远。而现在，<strong>使用容器技术，我们就可以让应用程序运行在一个有严密防护的“沙盒”（Sandbox）环境之内</strong>，就好像是把进程请进了“隔离酒店”，它可以在这个环境里自由活动，但绝不允许“越界”，从而保证了容器外系统的安全。</p>
<p>另外，在计算机里有各种各样的资源，CPU、内存、硬盘、网卡，虽然目前的高性能服务器都是几十核 CPU、上百 GB 的内存、数 TB 的硬盘、万兆网卡，但这些资源终究是有限的，而且考虑到成本，也不允许某个应用程序无限制地占用。容器技术的另一个本领就是为应用程序加上资源隔离，在系统里切分出一部分资源，让它只能使用指定的配额，比如只能使用一个 CPU，只能使用 1GB 内存等等，就好像在隔离酒店里保证一日三餐，但想要吃山珍海味那是不行的。这样就可以避免容器内进程的过度系统消耗，充分利用计算机硬件，让有限的资源能够提供稳定可靠的服务。所以，虽然进程被“关”在了容器里，损失了一些自由，但却保证了整个系统的安全。而且只要进程遵守隔离规定，不做什么出格的事情，也完全是可以正常运行的。</p>
<h3 id="与虚拟机的区别是什么">与虚拟机的区别是什么</h3>
<p>你也许会说，这么看来，容器不过就是常见的“沙盒”技术中的一种，和虚拟机差不了多少，那么它与虚拟机的区别在哪里呢？又有什么样的优势呢？在我看来，其实容器和虚拟机面对的都是相同的问题，使用的也都是虚拟化技术，只是所在的层次不同，我们可以参考 Docker 官网上的两张图，把这两者对比起来会更利于学习理解。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b7/02/b734f7d91bda055236b3467bc16f6302.jpg?wh=1920x911"
        data-srcset="https://static001.geekbang.org/resource/image/b7/02/b734f7d91bda055236b3467bc16f6302.jpg?wh=1920x911, https://static001.geekbang.org/resource/image/b7/02/b734f7d91bda055236b3467bc16f6302.jpg?wh=1920x911 1.5x, https://static001.geekbang.org/resource/image/b7/02/b734f7d91bda055236b3467bc16f6302.jpg?wh=1920x911 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b7/02/b734f7d91bda055236b3467bc16f6302.jpg?wh=1920x911"
        title="img" /></p>
<p>（Docker 官网的图示其实并不太准确，容器并不直接运行在 Docker 上，Docker 只是辅助建立隔离环境，让容器基于 Linux 操作系统运行）首先，容器和虚拟机的目的都是隔离资源，保证系统安全，然后是尽量提高资源的利用率。之前在使用 VirtualBox/VMware 创建虚拟机的时候，你也应该看到了，它们能够在宿主机系统里完整虚拟化出一套计算机硬件，在里面还能够安装任意的操作系统，这内外两个系统也同样是完全隔离，互不干扰。而在数据中心的服务器上，虚拟机软件（即图中的 Hypervisor）同样可以把一台物理服务器虚拟成多台逻辑服务器，这些逻辑服务器彼此独立，可以按需分隔物理服务器的资源，为不同的用户所使用。从实现的角度来看，虚拟机虚拟化出来的是硬件，需要在上面再安装一个操作系统后才能够运行应用程序，而硬件虚拟化和操作系统都比较“重”，会消耗大量的 CPU、内存、硬盘等系统资源，但这些消耗其实并没有带来什么价值，属于“重复劳动”和“无用功”，不过好处就是隔离程度非常高，每个虚拟机之间可以做到完全无干扰。</p>
<p>我们再来看容器（即图中的 Docker），它直接利用了下层的计算机硬件和操作系统，因为比虚拟机少了一层，所以自然就会节约 CPU 和内存，显得非常轻量级，能够更高效地利用硬件资源。不过，因为多个容器共用操作系统内核，应用程序的隔离程度就没有虚拟机那么高了。运行效率，可以说是容器相比于虚拟机最大的优势，在这个对比图中就可以看到，同样的系统资源，虚拟机只能跑 3 个应用，其他的资源都用来支持虚拟机运行了，而容器则能够把这部分资源释放出来，同时运行 6 个应用。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/26/6d/26cb446ac5ec53abde2744c431200c6d.jpg?wh=1920x869"
        data-srcset="https://static001.geekbang.org/resource/image/26/6d/26cb446ac5ec53abde2744c431200c6d.jpg?wh=1920x869, https://static001.geekbang.org/resource/image/26/6d/26cb446ac5ec53abde2744c431200c6d.jpg?wh=1920x869 1.5x, https://static001.geekbang.org/resource/image/26/6d/26cb446ac5ec53abde2744c431200c6d.jpg?wh=1920x869 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/26/6d/26cb446ac5ec53abde2744c431200c6d.jpg?wh=1920x869"
        title="img" /></p>
<p>当然，这个对比图只是一个形象的展示，不是严谨的数值比较，不过我们还可以用手里现有的 VirtualBox/VMware 虚拟机与 Docker 容器做个简单对比。一个普通的 Ubuntu 虚拟机安装完成之后，体积都是 GB 级别的，再安装一些应用很容易就会上到 10GB，启动的时间通常需要几分钟，我们的电脑上同时运行十来个虚拟机可能就是极限了。而一个 Ubuntu 镜像大小则只有几十 MB，启动起来更是非常快，基本上不超过一秒钟，同时跑上百个容器也毫无问题。不过，虚拟机和容器这两种技术也不是互相排斥的，它们完全可以结合起来使用，就像我们的课程里一样，用虚拟机实现与宿主机的强隔离，然后在虚拟机里使用 Docker 容器来快速运行应用程序。</p>
<h3 id="隔离是怎么实现的">隔离是怎么实现的</h3>
<p>我们知道虚拟机使用的是 Hypervisor（KVM、Xen 等），那么，容器是怎么实现和下层计算机硬件和操作系统交互的呢？为什么它会具有高效轻便的隔离特性呢？其实奥秘就在于 Linux 操作系统内核之中，为资源隔离提供了三种技术：namespace、cgroup、chroot，虽然这三种技术的初衷并不是为了实现容器，但它们三个结合在一起就会发生奇妙的“化学反应”。namespace 是 2002 年从 Linux 2.4.19 开始出现的，和编程语言里的 namespace 有点类似，它可以创建出独立的文件系统、主机名、进程号、网络等资源空间，相当于给进程盖了一间小板房，这样就实现了系统全局资源和进程局部资源的隔离。cgroup 是 2008 年从 Linux 2.6.24 开始出现的，它的全称是 Linux Control Group，用来实现对进程的 CPU、内存等资源的优先级和配额限制，相当于给进程的小板房加了一个天花板。</p>
<p>chroot 的历史则要比前面的 namespace、cgroup 要古老得多，早在 1979 年的 UNIX V7 就已经出现了，它可以更改进程的根目录，也就是限制访问文件系统，相当于给进程的小板房铺上了地砖。你看，综合运用这三种技术，一个四四方方、具有完善的隔离特性的容器就此出现了，进程就可以搬进这个小房间，过它的“快乐生活”了。我觉得用鲁迅先生的一句诗来描述这个情景最为恰当：躲进小楼成一统，管他冬夏与春秋。</p>
<h3 id="小结-2">小结</h3>
<p>好了，今天我们一起学习了容器技术中最关键的概念：动态的容器，再简单小结一下课程的要点：容器就是操作系统里一个特殊的“沙盒”环境，里面运行的进程只能看到受限的信息，与外部系统实现了隔离。容器隔离的目的是为了系统安全，限制了进程能够访问的各种资源。相比虚拟机技术，容器更加轻巧、更加高效，消耗的系统资源非常少，在云计算时代极具优势。容器的基本实现技术是 Linux 系统里的 namespace、cgroup、chroot。</p>
<h2 id="03容器化的应用会了这些你就是docker高手">03｜容器化的应用：会了这些你就是Docker高手</h2>
<p>在上一次课里，我们了解了容器技术中最核心的概念：容器，知道它就是一个系统中被隔离的特殊环境，进程可以在其中不受干扰地运行。我们也可以把这段描述再简化一点：容器就是被隔离的进程。相比笨重的虚拟机，容器有许多优点，那我们应该如何创建并运行容器呢？是要用 Linux 内核里的 namespace、cgroup、chroot 三件套吗？当然不会，那样的方式实在是太原始了，所以今天，我们就以 Docker 为例，来看看什么是容器化的应用，怎么来操纵容器化的应用。</p>
<h3 id="什么是容器化的应用">什么是容器化的应用</h3>
<p>之前我们运行容器的时候，显然不是从零开始的，而是要先拉取一个“镜像”（image），再从这个“镜像”来启动容器，像第一节课这样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker pull busybox      
docker run busybox echo hello world
</code></pre></td></tr></table>
</div>
</div><p>那么，这个“镜像”到底是什么东西呢？它又和“容器”有什么关系呢？其实我们在其他场合中也曾经见到过“镜像”这个词，比如最常见的光盘镜像，重装电脑时使用的硬盘镜像，还有虚拟机系统镜像。这些“镜像”都有一些相同点：只读，不允许修改，以标准格式存储了一系列的文件，然后在需要的时候再从中提取出数据运行起来。容器技术里的镜像也是同样的道理。因为容器是由操作系统动态创建的，那么必然就可以用一种办法把它的初始环境给固化下来，保存成一个静态的文件，相当于是把容器给“拍扁”了，这样就可以非常方便地存放、传输、版本化管理了。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/59/33/59a1cd035e21fe297072b20475d3c833.jpg?wh=1418x759"
        data-srcset="https://static001.geekbang.org/resource/image/59/33/59a1cd035e21fe297072b20475d3c833.jpg?wh=1418x759, https://static001.geekbang.org/resource/image/59/33/59a1cd035e21fe297072b20475d3c833.jpg?wh=1418x759 1.5x, https://static001.geekbang.org/resource/image/59/33/59a1cd035e21fe297072b20475d3c833.jpg?wh=1418x759 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/59/33/59a1cd035e21fe297072b20475d3c833.jpg?wh=1418x759"
        title="img" /></p>
<p>如果还拿之前的“小板房”来做比喻的话，那么镜像就可以说是一个“样板间”，把运行进程所需要的文件系统、依赖库、环境变量、启动参数等所有信息打包整合到了一起。之后镜像文件无论放在哪里，操作系统都能根据这个“样板间”快速重建容器，应用程序看到的就会是一致的运行环境了。从功能上来看，镜像和常见的 tar、rpm、deb 等安装包一样，都打包了应用程序，<strong>但最大的不同点在于它里面不仅有基本的可执行文件，还有应用运行时的整个系统环境。这就让镜像具有了非常好的跨平台便携性和兼容性</strong>，能够让开发者在一个系统上开发（例如 Ubuntu），然后打包成镜像，再去另一个系统上运行（例如 CentOS），完全不需要考虑环境依赖的问题，是一种更高级的应用打包方式。</p>
<p>理解了这一点，我们再回过头来看看第一节课里运行的 Docker 命令。docker pull busybox ，就是获取了一个打包了 busybox 应用的镜像，里面固化了 busybox 程序和它所需的完整运行环境。docker run busybox echo hello world ，就是提取镜像里的各种信息，运用 namespace、cgroup、chroot 技术创建出隔离环境，然后再运行 busybox 的 echo 命令，输出 hello world 的字符串。这两个步骤，由于是基于标准的 Linux 系统调用和只读的镜像文件，所以，无论是在哪种操作系统上，或者是使用哪种容器实现技术，都会得到完全一致的结果。</p>
<p>推而广之，任何应用都能够用这种形式打包再分发后运行，这也是无数开发者梦寐以求的“一次编写，到处运行（Build once, Run anywhere）”的至高境界。所以，所谓的“容器化的应用”，或者“应用的容器化”，就是指应用程序不再直接和操作系统打交道，而是封装成镜像，再交给容器环境去运行。现在你就应该知道了，镜像就是静态的应用容器，容器就是动态的应用镜像，两者互相依存，互相转化，密不可分。之前的那张 Docker 官方架构图你还有印象吧，我们在第一节课曾经简单地介绍过。可以看到，在 Docker 里的核心处理对象就是镜像（image）和容器（container）：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        data-srcset="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 1.5x, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        title="img" /></p>
<p>好，理解了什么是容器化的应用，接下来我们再来学习怎么操纵容器化的应用。因为镜像是容器运行的根本，先有镜像才有容器，所以先来看看关于镜像的一些常用命令。</p>
<h3 id="常用的镜像操作有哪些">常用的镜像操作有哪些</h3>
<p>在前面的课程里你应该已经了解了两个基本命令，docker pull 从远端仓库拉取镜像，docker images 列出当前本地已有的镜像。docker pull 的用法还是比较简单的，和普通的下载非常像，不过我们需要知道镜像的命名规则，这样才能准确地获取到我们想要的容器镜像。</p>
<p>镜像的完整名字由两个部分组成，名字和标签，中间用 : 连接起来。</p>
<p>名字表明了应用的身份，比如 busybox、Alpine、Nginx、Redis 等等。标签（tag）则可以理解成是为了区分不同版本的应用而做的额外标记，任何字符串都可以，比如 3.15 是纯数字的版本号、jammy 是项目代号、1.21-alpine 是版本号加操作系统名等等。其中有一个比较特殊的标签叫“latest”，它是默认的标签，如果只提供名字没有附带标签，那么就会使用这个默认的“latest”标签。那么现在，你就可以把名字和标签组合起来，使用 docker pull 来拉取一些镜像了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker pull alpine:3.15
docker pull ubuntu:jammy
docker pull nginx:1.21-alpine
docker pull nginx:alpine
docker pull redis
</code></pre></td></tr></table>
</div>
</div><p>有了这些镜像之后，我们再用 docker images 命令来看看它们的具体信息</p>
<p>在这个列表里，你可以看到，REPOSITORY 列就是镜像的名字，TAG 就是这个镜像的标签，那么第三列“IMAGE ID”又是什么意思呢？它可以说是镜像唯一的标识，就好像是身份证号一样。比如这里我们可以用“ubuntu:jammy”来表示 Ubuntu 22.04 镜像，同样也可以用它的 ID“d4c2c……”来表示。另外，你可能还会注意到，截图里的两个镜像“nginx:1.21-alpine”和“nginx:alpine”的 IMAGE ID 是一样的，都是“a63aa……”。这其实也很好理解，这就像是人的身份证号码是唯一的，但可以有大名、小名、昵称、绰号，同一个镜像也可以打上不同的标签，这样应用在不同的场合就更容易理解。IMAGE ID 还有一个好处，因为它是十六进制形式且唯一，Docker 特意为它提供了“短路”操作，在本地使用镜像的时候，我们不用像名字那样要完全写出来这一长串数字，通常只需要写出前三位就能够快速定位，在镜像数量比较少的时候用两位甚至一位数字也许就可以了。</p>
<p>来看另一个镜像操作命令 docker rmi ，它用来删除不再使用的镜像，可以节约磁盘空间，注意命令 rmi ，实际上是“remove image”的简写。下面我们就来试验一下，使用名字和 IMAGE ID 来删除镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker rmi redis    
docker rmi d4c
</code></pre></td></tr></table>
</div>
</div><p>这里的第一个 rmi 删除了 Redis 镜像，因为没有显式写出标签，默认使用的就是“latest”。第二个 rmi 没有给出名字，而是直接使用了 IMAGE ID 的前三位，也就是“d4c”，Docker 就会直接找到这个 ID 前缀的镜像然后删除。Docker 里与镜像相关的命令还有很多，不过以上的 docker pull、docker images、docker rmi 就是最常用的三个了，其他的命令我们后续课程会陆续介绍。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/27/19/27364161a8d3c1f960a91e07b5094419.jpg?wh=1920x963"
        data-srcset="https://static001.geekbang.org/resource/image/27/19/27364161a8d3c1f960a91e07b5094419.jpg?wh=1920x963, https://static001.geekbang.org/resource/image/27/19/27364161a8d3c1f960a91e07b5094419.jpg?wh=1920x963 1.5x, https://static001.geekbang.org/resource/image/27/19/27364161a8d3c1f960a91e07b5094419.jpg?wh=1920x963 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/27/19/27364161a8d3c1f960a91e07b5094419.jpg?wh=1920x963"
        title="img" /></p>
<h3 id="常用的容器操作">常用的容器操作</h3>
<p>有哪些现在我们已经在本地存放了镜像，就可以使用 docker run 命令把这些静态的应用运行起来，变成动态的容器了。基本的格式是“docker run 设置参数”，再跟上“镜像名或 ID”，后面可能还会有附加的“运行命令”。比如这个命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -h srv alpine hostname
</code></pre></td></tr></table>
</div>
</div><p>这里的 -h srv 就是容器的运行参数，alpine 是镜像名，它后面的 hostname 表示要在容器里运行的“hostname”这个程序，输出主机名。docker run 是最复杂的一个容器操作命令，有非常多的额外参数用来调整容器的运行状态，你可以加上 &ndash;help 来看它的帮助信息，今天我只说几个最常用的参数。-it 表示开启一个交互式操作的 Shell，这样可以直接进入容器内部，就好像是登录虚拟机一样。（它实际上是“-i”和“-t”两个参数的组合形式）-d 表示让容器在后台运行，这在我们启动 Nginx、Redis 等服务器程序的时候非常有用。&ndash;name 可以为容器起一个名字，方便我们查看，不过它不是必须的，如果不用这个参数，Docker 会分配一个随机的名字。</p>
<p>下面我们就来练习一下这三个参数，分别运行 Nginx、Redis 和 Ubuntu：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d nginx:alpine            # 后台运行Nginx
docker run -d --name red_srv redis    # 后台运行Redis
docker run -it --name ubuntu 2e6 sh   # 使用IMAGE ID，登录Ubuntu18.04
</code></pre></td></tr></table>
</div>
</div><p>因为第三个命令使用的是 -it 而不是 -d ，所以它会进入容器里的 Ubuntu 系统，我们需要另外开一个终端窗口，使用 docker ps 命令来查看容器的运行状态：</p>
<p>可以看到，每一个容器也会有一个“CONTAINER ID”，它的作用和镜像的“IMAGE ID”是一样的，唯一标识了容器。对于正在运行中的容器，我们可以使用 docker exec 命令在里面执行另一个程序，效果和 docker run 很类似，但因为容器已经存在，所以不会创建新的容器。它最常见的用法是使用 -it 参数打开一个 Shell，从而进入容器内部，例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker exec -it red_srv sh
</code></pre></td></tr></table>
</div>
</div><p>这样我们就“登录”进了 Redis 容器，可以很方便地查看服务的运行状态或者日志。运行中的容器还可以使用 docker stop 命令来强制停止，这里我们仍然可以使用容器名字，不过或许用“CONTAINER ID”的前三位数字会更加方便。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker stop ed4 d60 45c
</code></pre></td></tr></table>
</div>
</div><p>容器被停止后使用 docker ps 命令就看不到了，不过容器并没有被彻底销毁，我们可以使用 docker ps -a 命令查看系统里所有的容器，当然也包括已经停止运行的容器：</p>
<p>这些停止运行的容器可以用 docker start 再次启动运行，如果你确定不再需要它们，可以使用 docker rm 命令来彻底删除。</p>
<p>注意，这个命令与 docker rmi 非常像，区别在于它没有后面的字母“i”，所以只会删除容器，不删除镜像。下面我们就来运行 docker rm 命令，使用“CONTAINER ID”的前两位数字来删除这些容器：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker rm ed d6 45
</code></pre></td></tr></table>
</div>
</div><p>执行删除命令之后，再用 docker ps -a 查看列表就会发现这些容器已经彻底消失了。你可能会感觉这样的容器管理方式很麻烦，启动后要 ps 看 ID 再删除，如果稍微不注意，系统就会遗留非常多的“死”容器，占用系统资源，有没有什么办法能够让 Docker 自动删除不需要的容器呢？办法当然有，就是在执行 docker run 命令的时候加上一个 &ndash;rm 参数，这就会告诉 Docker 不保存容器，只要运行完毕就自动清除，省去了我们手工管理容器的麻烦。我们还是用刚才的 Nginx、Redis 和 Ubuntu 这三个容器来试验一下，加上 &ndash;rm 参数（省略了 name 参数）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d --rm nginx:alpine 
docker run -d --rm redis
docker run -it --rm 2e6 sh 
</code></pre></td></tr></table>
</div>
</div><p>然后我们用 docker stop 停止容器，再用 docker ps -a ，就会发现不需要我们再手动执行 docker rm ，Docker 已经自动删除了这三个容器。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c8/85/c8cd008e91aaff2cd91e0392b0079085.jpg?wh=1920x1747"
        data-srcset="https://static001.geekbang.org/resource/image/c8/85/c8cd008e91aaff2cd91e0392b0079085.jpg?wh=1920x1747, https://static001.geekbang.org/resource/image/c8/85/c8cd008e91aaff2cd91e0392b0079085.jpg?wh=1920x1747 1.5x, https://static001.geekbang.org/resource/image/c8/85/c8cd008e91aaff2cd91e0392b0079085.jpg?wh=1920x1747 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c8/85/c8cd008e91aaff2cd91e0392b0079085.jpg?wh=1920x1747"
        title="img" /></p>
<h3 id="小结-3">小结</h3>
<p>好了，今天我们一起学习了容器化的应用，然后使用 Docker 实际操作了镜像和容器，运行了被容器化的 Alpine、Nginx、Redis 等应用。镜像是容器的静态形式，它打包了应用程序的所有运行依赖项，方便保存和传输。使用容器技术运行镜像，就形成了动态的容器，由于镜像只读不可修改，所以应用程序的运行环境总是一致的。而容器化的应用就是指以镜像的形式打包应用程序，然后在容器环境里从镜像启动容器。由于 Docker 的命令比较多，而且每个命令还有许多参数，一节课里很难把它们都详细说清楚，希望你课下参考 Docker 自带的帮助或者官网文档（https://docs.docker.com/reference/），再多加实操练习，相信你一定能够成为 Docker 高手。</p>
<p>我这里就对今天的镜像操作和容器操作做个小结：常用的镜像操作有 docker pull、docker images、docker rmi，分别是拉取镜像、查看镜像和删除镜像。用来启动容器的 docker run 是最常用的命令，它有很多参数用来调整容器的运行状态，对于后台服务来说应该使用 -d。docker exec 命令可以在容器内部执行任意程序，对于调试排错特别有用。其他常用的容器操作还有 docker ps、docker stop、docker rm，用来查看容器、停止容器和删除容器。</p>
<h2 id="04创建容器镜像如何编写正确高效的dockerfile">04｜创建容器镜像：如何编写正确、高效的Dockerfile</h2>
<p>上一次的课程里我们一起学习了容器化的应用，也就是被打包成镜像的应用程序，然后再用各种 Docker 命令来运行、管理它们。那么这又会带来一个疑问：这些镜像是怎么创建出来的？我们能不能够制作属于自己的镜像呢？所以今天，我就来讲解镜像的内部机制，还有高效、正确地编写 Dockerfile 制作容器镜像的方法。</p>
<h3 id="镜像的内部机制是什么">镜像的内部机制是什么</h3>
<p>现在你应该知道，镜像就是一个打包文件，里面包含了应用程序还有它运行所依赖的环境，例如文件系统、环境变量、配置参数等等。环境变量、配置参数这些东西还是比较简单的，随便用一个 manifest 清单就可以管理，真正麻烦的是文件系统。为了保证容器运行环境的一致性，镜像必须把应用程序所在操作系统的根目录，也就是 rootfs，都包含进来。虽然这些文件里不包含系统内核（因为容器共享了宿主机的内核），但如果每个镜像都重复做这样的打包操作，仍然会导致大量的冗余。可以想象，如果有一千个镜像，都基于 Ubuntu 系统打包，那么这些镜像里就会重复一千次 Ubuntu 根目录，对磁盘存储、网络传输都是很大的浪费。</p>
<p>很自然的，我们就会想到，应该把重复的部分抽取出来，只存放一份 Ubuntu 根目录文件，然后让这一千个镜像以某种方式共享这部分数据。这个思路，也正是容器镜像的一个重大创新点：分层，术语叫“Layer”。容器镜像内部并不是一个平坦的结构，而是由许多的镜像层组成的，每层都是只读不可修改的一组文件，相同的层可以在镜像之间共享，然后多个层像搭积木一样堆叠起来，再使用一种叫“Union FS 联合文件系统”的技术把它们合并在一起，就形成了容器最终看到的文件系统（图片来源）。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c7/3f/c750a7795ff4787c6639dd42bf0a473f.png?wh=800x600"
        data-srcset="https://static001.geekbang.org/resource/image/c7/3f/c750a7795ff4787c6639dd42bf0a473f.png?wh=800x600, https://static001.geekbang.org/resource/image/c7/3f/c750a7795ff4787c6639dd42bf0a473f.png?wh=800x600 1.5x, https://static001.geekbang.org/resource/image/c7/3f/c750a7795ff4787c6639dd42bf0a473f.png?wh=800x600 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c7/3f/c750a7795ff4787c6639dd42bf0a473f.png?wh=800x600"
        title="img" /></p>
<p>我来拿大家都熟悉的千层糕做一个形象的比喻吧。千层糕也是由很多层叠加在一起的，从最上面可以看到每层里面镶嵌的葡萄干、核桃、杏仁、青丝等，每一层糕就相当于一个 Layer，干果就好比是 Layer 里的各个文件。但如果某两层的同一个位置都有干果，也就是有文件同名，那么我们就只能看到上层的文件，而下层的就被屏蔽了。你可以用命令 docker inspect 来查看镜像的分层信息，比如 nginx:alpine 镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker inspect nginx:alpine
</code></pre></td></tr></table>
</div>
</div><p>它的分层信息在“RootFS”部分：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/5y/b7/5yybd821a12ec1323f6ea8bb5a5c4ab7.png?wh=1920x592"
        data-srcset="https://static001.geekbang.org/resource/image/5y/b7/5yybd821a12ec1323f6ea8bb5a5c4ab7.png?wh=1920x592, https://static001.geekbang.org/resource/image/5y/b7/5yybd821a12ec1323f6ea8bb5a5c4ab7.png?wh=1920x592 1.5x, https://static001.geekbang.org/resource/image/5y/b7/5yybd821a12ec1323f6ea8bb5a5c4ab7.png?wh=1920x592 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/5y/b7/5yybd821a12ec1323f6ea8bb5a5c4ab7.png?wh=1920x592"
        title="img" /></p>
<p>通过这张截图就可以看到，nginx:alpine 镜像里一共有 6 个 Layer。相信你现在也就明白，之前在使用 docker pull、docker rmi 等命令操作镜像的时候，那些“奇怪”的输出信息是什么了，其实就是镜像里的各个 Layer。Docker 会检查是否有重复的层，如果本地已经存在就不会重复下载，如果层被其他镜像共享就不会删除，这样就可以节约磁盘和网络成本。</p>
<h3 id="dockerfile-是什么">Dockerfile 是什么</h3>
<p>知道了容器镜像的内部结构和基本原理，我们就可以来学习如何自己动手制作容器镜像了，也就是自己打包应用。在之前我们讲容器的时候，曾经说过容器就是“小板房”，镜像就是“样板间”。那么，要造出这个“样板间”，就必然要有一个“施工图纸”，由它来规定如何建造地基、铺设水电、开窗搭门等动作。这个“施工图纸”就是“Dockerfile”。比起容器、镜像来说，Dockerfile 非常普通，它就是一个纯文本，里面记录了一系列的构建指令，比如选择基础镜像、拷贝文件、运行脚本等等，每个指令都会生成一个 Layer，而 Docker 顺序执行这个文件里的所有步骤，最后就会创建出一个新的镜像出来。我们来看一个最简单的 Dockerfile 实例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># Dockerfile.busybox
FROM busybox                  # 选择基础镜像
CMD echo &#34;hello world&#34;        # 启动容器时默认运行的命令
</code></pre></td></tr></table>
</div>
</div><p>这个文件里只有两条指令。第一条指令是 FROM，所有的 Dockerfile 都要从它开始，表示选择构建使用的基础镜像，相当于“打地基”，这里我们使用的是 busybox。第二条指令是 CMD，它指定 docker run 启动容器时默认运行的命令，这里我们使用了 echo 命令，输出“hello world”字符串。现在有了 Dockerfile 这张“施工图纸”，我们就可以请出“施工队”了，用 docker build 命令来创建出镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker build -f Dockerfile.busybox .

Sending build context to Docker daemon   7.68kB
Step 1/2 : FROM busybox
 ---&gt; d38589532d97
Step 2/2 : CMD echo &#34;hello world&#34;
 ---&gt; Running in c5a762edd1c8
Removing intermediate container c5a762edd1c8
 ---&gt; b61882f42db7
Successfully built b61882f42db7
</code></pre></td></tr></table>
</div>
</div><p>你需要特别注意命令的格式，用 -f 参数指定 Dockerfile 文件名，后面必须跟一个文件路径，叫做“构建上下文”（build’s context），这里只是一个简单的点号，表示当前路径的意思。接下来，你就会看到 Docker 会逐行地读取并执行 Dockerfile 里的指令，依次创建镜像层，再生成完整的镜像。新的镜像暂时还没有名字（用 docker images 会看到是 &lt; none&gt;），但我们可以直接使用“IMAGE ID”来查看或者运行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker inspect b61
docker run b61
</code></pre></td></tr></table>
</div>
</div><h3 id="怎样编写正确高效的-dockerfile">怎样编写正确、高效的 Dockerfile</h3>
<p>大概了解了 Dockerfile 之后，我再来讲讲编写 Dockerfile 的一些常用指令和最佳实践，帮你在今后的工作中把它写好、用好。首先因为构建镜像的第一条指令必须是 FROM，所以基础镜像的选择非常关键。如果关注的是镜像的安全和大小，那么一般会选择 Alpine；如果关注的是应用的运行稳定性，那么可能会选择 Ubuntu、Debian、CentOS。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">FROM alpine:3.15                # 选择Alpine镜像
FROM ubuntu:bionic              # 选择Ubuntu镜像
</code></pre></td></tr></table>
</div>
</div><p>我们在本机上开发测试时会产生一些源码、配置等文件，需要打包进镜像里，这时可以使用 COPY 命令，它的用法和 Linux 的 cp 差不多，不过拷贝的源文件必须是“构建上下文”路径里的，不能随意指定文件。也就是说，如果要从本机向镜像拷贝文件，就必须把这些文件放到一个专门的目录，然后在 docker build 里指定“构建上下文”到这个目录才行。这里有两个 COPY 命令示例，你可以看一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">COPY ./a.txt  /tmp/a.txt    # 把构建上下文里的a.txt拷贝到镜像的/tmp目录
COPY /etc/hosts  /tmp       # 错误！不能使用构建上下文之外的文件
</code></pre></td></tr></table>
</div>
</div><p>接下来要说的就是 Dockerfile 里最重要的一个指令 RUN ，它可以执行任意的 Shell 命令，比如更新系统、安装应用、下载文件、创建目录、编译程序等等，实现任意的镜像构建步骤，非常灵活。RUN 通常会是 Dockerfile 里最复杂的指令，会包含很多的 Shell 命令，但 Dockerfile 里一条指令只能是一行，所以有的 RUN 指令会在每行的末尾使用续行符 \，命令之间也会用 &amp;&amp; 来连接，这样保证在逻辑上是一行，就像下面这样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">RUN apt-get update \
    &amp;&amp; apt-get install -y \
        build-essential \
        curl \
        make \
        unzip \
    &amp;&amp; cd /tmp \
    &amp;&amp; curl -fSL xxx.tar.gz -o xxx.tar.gz\
    &amp;&amp; tar xzf xxx.tar.gz \
    &amp;&amp; cd xxx \
    &amp;&amp; ./config \
    &amp;&amp; make \
    &amp;&amp; make clean
</code></pre></td></tr></table>
</div>
</div><p>有的时候在 Dockerfile 里写这种超长的 RUN 指令很不美观，而且一旦写错了，每次调试都要重新构建也很麻烦，所以你可以采用一种变通的技巧：把这些 Shell 命令集中到一个脚本文件里，用 COPY 命令拷贝进去再用 RUN 来执行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">COPY setup.sh  /tmp/                # 拷贝脚本到/tmp目录

RUN cd /tmp &amp;&amp; chmod +x setup.sh \  # 添加执行权限
    &amp;&amp; ./setup.sh &amp;&amp; rm setup.sh    # 运行脚本然后再删除
</code></pre></td></tr></table>
</div>
</div><p>RUN 指令实际上就是 Shell 编程，如果你对它有所了解，就应该知道它有变量的概念，可以实现参数化运行，这在 Dockerfile 里也可以做到，需要使用两个指令 ARG 和 ENV。它们区别在于 ARG 创建的变量只在镜像构建过程中可见，容器运行时不可见，而 ENV 创建的变量不仅能够在构建镜像的过程中使用，在容器运行时也能够以环境变量的形式被应用程序使用。下面是一个简单的例子，使用 ARG 定义了基础镜像的名字（可以用在“FROM”指令里），使用 ENV 定义了两个环境变量：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ARG IMAGE_BASE=&#34;node&#34;
ARG IMAGE_TAG=&#34;alpine&#34;

ENV PATH=$PATH:/tmp
ENV DEBUG=OFF

</code></pre></td></tr></table>
</div>
</div><p>还有一个重要的指令是 EXPOSE，它用来声明容器对外服务的端口号，对现在基于 Node.js、Tomcat、Nginx、Go 等开发的微服务系统来说非常有用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">EXPOSE 443           # 默认是tcp协议
EXPOSE 53/udp        # 可以指定udp协议
</code></pre></td></tr></table>
</div>
</div><p>讲了这些 Dockerfile 指令之后，我还要特别强调一下，因为每个指令都会生成一个镜像层，所以 Dockerfile 里最好不要滥用指令，尽量精简合并，否则太多的层会导致镜像臃肿不堪。</p>
<h3 id="docker-build-是怎么工作的">docker build 是怎么工作的</h3>
<p>Dockerfile 必须要经过 docker build 才能生效，所以我们再来看看 docker build 的详细用法。刚才在构建镜像的时候，你是否对“构建上下文”这个词感到有些困惑呢？它到底是什么含义呢？我觉得用 Docker 的官方架构图来理解会比较清楚（注意图中与“docker build”关联的虚线）。因为命令行“docker”是一个简单的客户端，真正的镜像构建工作是由服务器端的“Docker daemon”来完成的，所以“docker”客户端就只能把“构建上下文”目录打包上传（显示信息 Sending build context to Docker daemon ），这样服务器才能够获取本地的这些文件。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        data-srcset="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 1.5x, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        title="img" /></p>
<p>明白了这一点，你就会知道，“构建上下文”其实与 Dockerfile 并没有直接的关系，它其实指定了要打包进镜像的一些依赖文件。而 COPY 命令也只能使用基于“构建上下文”的相对路径，因为“Docker daemon”看不到本地环境，只能看到打包上传的那些文件。但这个机制也会导致一些麻烦，如果目录里有的文件（例如 readme/.git/.svn 等）不需要拷贝进镜像，docker 也会一股脑地打包上传，效率很低。为了避免这种问题，你可以在“构建上下文”目录里再建立一个 .dockerignore 文件，语法与 .gitignore 类似，排除那些不需要的文件。下面是一个简单的示例，表示不打包上传后缀是“swp”“sh”的文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># docker ignore
*.swp
*.sh
</code></pre></td></tr></table>
</div>
</div><p>另外关于 Dockerfile，一般应该在命令行里使用 -f 来显式指定。但如果省略这个参数，docker build 就会在当前目录下找名字是 Dockerfile 的文件。所以，如果只有一个构建目标的话，文件直接叫“Dockerfile”是最省事的。现在我们使用 docker build 应该就没什么难点了，不过构建出来的镜像只有“IMAGE ID”没有名字，不是很方便。为此你可以加上一个 -t 参数，也就是指定镜像的标签（tag），这样 Docker 就会在构建完成后自动给镜像添加名字。当然，名字必须要符合上节课里的命名规范，用 : 分隔名字和标签，如果不提供标签默认就是“latest”。</p>
<h3 id="小结-4">小结</h3>
<p>了，今天我们一起学习了容器镜像的内部结构，重点理解容器镜像是由多个只读的 Layer 构成的，同一个 Layer 可以被不同的镜像共享，减少了存储和传输的成本。如何编写 Dockerfile 内容稍微多一点，我再简单做个小结：</p>
<p>创建镜像需要编写 Dockerfile，写清楚创建镜像的步骤，每个指令都会生成一个 Layer。Dockerfile 里，第一个指令必须是 FROM，用来选择基础镜像，常用的有 Alpine、Ubuntu 等。其他常用的指令有：COPY、RUN、EXPOSE，分别是拷贝文件，运行 Shell 命令，声明服务端口号。docker build 需要用 -f 来指定 Dockerfile，如果不指定就使用当前目录下名字是“Dockerfile”的文件。docker build 需要指定“构建上下文”，其中的文件会打包上传到 Docker daemon，所以尽量不要在“构建上下文”中存放多余的文件。创建镜像的时候应当尽量使用 -t 参数，为镜像起一个有意义的名字，方便管理。</p>
<p>今天讲了不少，但关于创建镜像还有很多高级技巧等待你去探索，比如使用缓存、多阶段构建等等，你可以再参考 Docker 官方文档（https://docs.docker.com/engine/reference/builder/），或者一些知名应用的镜像（如 Nginx、Redis、Node.js 等）进一步学习。</p>
<h2 id="05镜像仓库该怎样用好docker-hub这个宝藏">05｜镜像仓库：该怎样用好Docker Hub这个宝藏</h2>
<p>上一次课里我们学习了“Dockerfile”和“docker build”的用法，知道了如何创建自己的镜像。那么镜像文件应该如何管理呢，具体来说，应该如何存储、检索、分发、共享镜像呢？不解决这些问题，我们的容器化应用还是无法顺利地实施。今天，我就来谈一下这个话题，聊聊什么是镜像仓库，还有该怎么用好镜像仓库。</p>
<h3 id="什么是镜像仓库registry">什么是镜像仓库（Registry）</h3>
<p>之前我们已经用过 docker pull 命令拉取镜像，也说过有一个“镜像仓库”（Registry）的概念，那到底什么是镜像仓库呢？还是来看 Docker 的官方架构图（它真的非常重要）：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        data-srcset="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 1.5x, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        title="img" /></p>
<p>图里右边的区域就是镜像仓库，术语叫 Registry，直译就是“注册中心”，意思是所有镜像的 Repository 都在这里登记保管，就像是一个巨大的档案馆。然后我们再来看左边的“docker pull”，虚线显示了它的工作流程，先到“Docker daemon”，再到 Registry，只有当 Registry 里存有镜像才能真正把它下载到本地。当然了，拉取镜像只是镜像仓库最基本的一个功能，它还会提供更多的功能，比如上传、查询、删除等等，是一个全面的镜像管理服务站点。你也可以把镜像仓库类比成手机上的应用商店，里面分门别类存放了许多容器化的应用，需要什么去找一下就行了。有了它，我们使用镜像才能够免除后顾之忧。</p>
<h3 id="什么是-docker-hub">什么是 Docker Hub</h3>
<p>不过，你有没有注意到，在使用 docker pull 获取镜像的时候，我们并没有明确地指定镜像仓库。在这种情况下，Docker 就会使用一个默认的镜像仓库，也就是大名鼎鼎的“Docker Hub”（https://hub.docker.com/）。Docker Hub 是 Docker 公司搭建的官方 Registry 服务，创立于 2014 年 6 月，和 Docker 1.0 同时发布。它号称是世界上最大的镜像仓库，和 GitHub 一样，几乎成为了容器世界的基础设施。Docker Hub 里面不仅有 Docker 自己打包的镜像，而且还对公众免费开放，任何人都可以上传自己的作品。经过这 8 年的发展，Docker Hub 已经不再是一个单纯的镜像仓库了，更应该说是一个丰富而繁荣的容器社区。你可以看看下面的这张截图，里面列出的都是下载量超过 10 亿次（1 Billion）的最受欢迎的应用程序，比如 Nginx、MongoDB、Node.js、Redis、OpenJDK 等等。显然，把这些容器化的应用引入到我们自己的系统里，就像是站在了巨人的肩膀上，一开始就会有一个高水平的起点。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d4/e3/d47cc4d3f867069b055a47628acac2e3.png?wh=1920x890"
        data-srcset="https://static001.geekbang.org/resource/image/d4/e3/d47cc4d3f867069b055a47628acac2e3.png?wh=1920x890, https://static001.geekbang.org/resource/image/d4/e3/d47cc4d3f867069b055a47628acac2e3.png?wh=1920x890 1.5x, https://static001.geekbang.org/resource/image/d4/e3/d47cc4d3f867069b055a47628acac2e3.png?wh=1920x890 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d4/e3/d47cc4d3f867069b055a47628acac2e3.png?wh=1920x890"
        title="img" /></p>
<p>但和 GitHub、App Store 一样，面向所有人公开的 Docker Hub 也有一个不可避免的缺点，就是“良莠不齐”。在 Docker Hub 搜索框里输入关键字，比如 Nginx、MySQL，它立即就会给出几百几千个搜索结果，有点“乱花迷人眼”的感觉，这么多镜像，应该如何挑选出最适合自己的呢？下面我就来说说自己在这方面的一些经验。</p>
<h3 id="如何在-docker-hub-上挑选镜像">如何在 Docker Hub 上挑选镜像</h3>
<p>首先，你应该知道，在 Docker Hub 上有官方镜像、认证镜像和非官方镜像的区别。官方镜像是指 Docker 公司官方提供的高质量镜像（https://github.com/docker-library/official-images），都经过了严格的漏洞扫描和安全检测，支持 x86_64、arm64 等多种硬件架构，还具有清晰易读的文档，一般来说是我们构建镜像的首选，也是我们编写 Dockerfile 的最佳范例。官方镜像目前有大约 100 多个，基本上囊括了现在的各种流行技术，下面就是官方的 Nginx 镜像网页截图：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/10/f3/109fc664da4f5124c4758b0e8f9c95f3.png?wh=1376x754"
        data-srcset="https://static001.geekbang.org/resource/image/10/f3/109fc664da4f5124c4758b0e8f9c95f3.png?wh=1376x754, https://static001.geekbang.org/resource/image/10/f3/109fc664da4f5124c4758b0e8f9c95f3.png?wh=1376x754 1.5x, https://static001.geekbang.org/resource/image/10/f3/109fc664da4f5124c4758b0e8f9c95f3.png?wh=1376x754 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/10/f3/109fc664da4f5124c4758b0e8f9c95f3.png?wh=1376x754"
        title="img" /></p>
<p>你会看到，官方镜像会有一个特殊的“Official image”的标记，这就表示这个镜像经过了 Docker 公司的认证，有专门的团队负责审核、发布和更新，质量上绝对可以放心。第二类是认证镜像，标记是“Verified publisher”，也就是认证发行商，比如 Bitnami、Rancher、Ubuntu 等。它们都是颇具规模的大公司，具有不逊于 Docker 公司的实力，所以就在 Docker Hub 上开了个认证账号，发布自己打包的镜像，有点类似我们微博上的“大 V”。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/57/f2/576d07439fc85d2bc461953f31a084f2.png?wh=1058x542"
        data-srcset="https://static001.geekbang.org/resource/image/57/f2/576d07439fc85d2bc461953f31a084f2.png?wh=1058x542, https://static001.geekbang.org/resource/image/57/f2/576d07439fc85d2bc461953f31a084f2.png?wh=1058x542 1.5x, https://static001.geekbang.org/resource/image/57/f2/576d07439fc85d2bc461953f31a084f2.png?wh=1058x542 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/57/f2/576d07439fc85d2bc461953f31a084f2.png?wh=1058x542"
        title="img" /></p>
<p>这些镜像有公司背书，当然也很值得信赖，不过它们难免会带上一些各自公司的“烙印”，比如 Bitnami 的镜像就统一以“minideb”为基础，灵活性上比 Docker 官方镜像略差，有的时候也许会不符合我们的需求。除了官方镜像和认证镜像，剩下的就都属于非官方镜像了，不过这里面也可以分出两类。第一类是“半官方”镜像。因为成为“Verified publisher”是要给 Docker 公司交钱的，而很多公司不想花这笔“冤枉钱”，所以只在 Docker Hub 上开了公司账号，但并不加入认证。这里我以 OpenResty 为例，看一下它的 Docker Hub 页面，可以看到显示的是 OpenResty 官方发布，但并没有经过 Docker 正式认证，所以难免就会存在一些风险，有被“冒名顶替”的可能，需要我们在使用的时候留心鉴别一下。不过一般来说，这种“半官方”镜像也是比较可靠的。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6d/3c/6d94d351137fb72cab36d73e8eea1f3c.png?wh=1496x566"
        data-srcset="https://static001.geekbang.org/resource/image/6d/3c/6d94d351137fb72cab36d73e8eea1f3c.png?wh=1496x566, https://static001.geekbang.org/resource/image/6d/3c/6d94d351137fb72cab36d73e8eea1f3c.png?wh=1496x566 1.5x, https://static001.geekbang.org/resource/image/6d/3c/6d94d351137fb72cab36d73e8eea1f3c.png?wh=1496x566 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6d/3c/6d94d351137fb72cab36d73e8eea1f3c.png?wh=1496x566"
        title="img" /></p>
<p>第二类就是纯粹的“民间”镜像了，通常是个人上传到 Docker Hub 的，因为条件所限，测试不完全甚至没有测试，质量上难以得到保证，下载的时候需要小心谨慎。除了查看镜像是否为官方认证，我们还应该再结合其他的条件来判断镜像质量是否足够好。做法和 GitHub 差不多，就是看它的下载量、星数、还有更新历史，简单来说就是“好评”数量。一般来说下载量是最重要的参考依据，好的镜像下载量通常都在百万级别（超过 1M），而有的镜像虽然也是官方认证，但缺乏维护，更新不及时，用的人很少，星数、下载数都寥寥无几，那么还是应该选择下载量最多的镜像，通俗来说就是“随大流”。下面的这张截图就是 OpenResty 在 Docker Hub 上的搜索结果。可以看到，有两个认证发行商的镜像（Bitnami、IBM），但下载量都很少，还有一个“民间”镜像下载量虽然超过了 1M，但更新时间是 3 年前，所以毫无疑问，我们应该选择排在第三位，但下载量超过 10M、有 360 多个星的“半官方”镜像。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/5c/93/5c0b39da3bba66955e2byydcbe0d8593.png?wh=1878x1530"
        data-srcset="https://static001.geekbang.org/resource/image/5c/93/5c0b39da3bba66955e2byydcbe0d8593.png?wh=1878x1530, https://static001.geekbang.org/resource/image/5c/93/5c0b39da3bba66955e2byydcbe0d8593.png?wh=1878x1530 1.5x, https://static001.geekbang.org/resource/image/5c/93/5c0b39da3bba66955e2byydcbe0d8593.png?wh=1878x1530 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/5c/93/5c0b39da3bba66955e2byydcbe0d8593.png?wh=1878x1530"
        title="img" /></p>
<p>看了这么多 Docker Hub 上的镜像，你一定注意到了，应用都是一样的名字，比如都是 Nginx、Redis、OpenResty，该怎么区分不同作者打包出的镜像呢？如果你熟悉 GitHub，就会发现 Docker Hub 也使用了同样的规则，就是“用户名 / 应用名”的形式，比如 bitnami/nginx、ubuntu/nginx、rancher/nginx 等等。所以，我们在使用 docker pull 下载这些非官方镜像的时候，就必须把用户名也带上，否则默认就会使用官方镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker pull bitnami/nginx
docker pull ubuntu/nginx
</code></pre></td></tr></table>
</div>
</div><h3 id="docker-hub-上镜像命名的规则是什么">Docker Hub 上镜像命名的规则是什么</h3>
<p>确定了要使用的镜像还不够，因为镜像还会有许多不同的版本，也就是“标签”（tag）。直接使用默认的“latest”虽然简单方便，但在生产环境里是一种非常不负责任的做法，会导致版本不可控。所以我们还需要理解 Docker Hub 上标签命名的含义，才能够挑选出最适合我们自己的镜像版本。下面我就拿官方的 Redis 镜像作为例子，解释一下这些标签都是什么意思。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/1d/d5/1dd392b8f286507b83cd31400d5dccd5.png?wh=1796x846"
        data-srcset="https://static001.geekbang.org/resource/image/1d/d5/1dd392b8f286507b83cd31400d5dccd5.png?wh=1796x846, https://static001.geekbang.org/resource/image/1d/d5/1dd392b8f286507b83cd31400d5dccd5.png?wh=1796x846 1.5x, https://static001.geekbang.org/resource/image/1d/d5/1dd392b8f286507b83cd31400d5dccd5.png?wh=1796x846 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/1d/d5/1dd392b8f286507b83cd31400d5dccd5.png?wh=1796x846"
        title="img" /></p>
<p>通常来说，镜像标签的格式是应用的版本号加上操作系统。版本号你应该比较了解吧，基本上都是主版本号 + 次版本号 + 补丁号的形式，有的还会在正式发布前出 rc 版（候选版本，release candidate）。而操作系统的情况略微复杂一些，因为各个 Linux 发行版的命名方式“花样”太多了。Alpine、CentOS 的命名比较简单明了，就是数字的版本号，像这里的 alpine3.15 ，而 Ubuntu、Debian 则采用了代号的形式。比如 Ubuntu 18.04 是 bionic，Ubuntu 20.04 是 focal，Debian 9 是 stretch，Debian 10 是 buster，Debian 11 是 bullseye。另外，有的标签还会加上 slim、fat，来进一步表示这个镜像的内容是经过精简的，还是包含了较多的辅助工具。通常 slim 镜像会比较小，运行效率高，而 fat 镜像会比较大，适合用来开发调试。</p>
<p>下面我就列出几个标签的例子来说明一下。nginx:1.21.6-alpine，表示版本号是 1.21.6，基础镜像是最新的 Alpine。redis:7.0-rc-bullseye，表示版本号是 7.0 候选版，基础镜像是 Debian 11。node:17-buster-slim，表示版本号是 17，基础镜像是精简的 Debian 10。</p>
<h3 id="该怎么上传自己的镜像">该怎么上传自己的镜像</h3>
<p>现在，我想你应该对如何在 Docker Hub 上选择镜像有了比较全面的了解，那么接下来的问题就是，我们自己用 Dockerfile 创建的镜像该如何上传到 Docker Hub 上呢？这件事其实一点也不难，只需要 4 个步骤就能完成。第一步，你需要在 Docker Hub 上注册一个用户，这个就不必再多说了。第二步，你需要在本机上使用 docker login 命令，用刚才注册的用户名和密码认证身份登录，像这里就用了我的用户名“chronolaw”：</p>
<p>第三步很关键，需要使用 docker tag 命令，给镜像改成带用户名的完整名字，表示镜像是属于这个用户的。或者简单一点，直接用 docker build -t 在创建镜像的时候就起好名字。这里我就用上次课里的镜像“ngx-app”作为例子，给它改名成 chronolaw/ngx-app:1.0：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker tag ngx-app chronolaw/ngx-app:1.0
</code></pre></td></tr></table>
</div>
</div><p>第四步，用 docker push 把这个镜像推上去，我们的镜像发布工作就大功告成了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker push chronolaw/ngx-app:1.0
</code></pre></td></tr></table>
</div>
</div><p>你还可以登录 Docker Hub 网站验证一下镜像发布的效果，可以看到它会自动为我们生成一个页面模板，里面还可以进一步丰富完善，比如添加描述信息、使用说明等等：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/76/d1/76f1f566c029d2a3743d79d80cdeddd1.png?wh=1920x715"
        data-srcset="https://static001.geekbang.org/resource/image/76/d1/76f1f566c029d2a3743d79d80cdeddd1.png?wh=1920x715, https://static001.geekbang.org/resource/image/76/d1/76f1f566c029d2a3743d79d80cdeddd1.png?wh=1920x715 1.5x, https://static001.geekbang.org/resource/image/76/d1/76f1f566c029d2a3743d79d80cdeddd1.png?wh=1920x715 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/76/d1/76f1f566c029d2a3743d79d80cdeddd1.png?wh=1920x715"
        title="img" /></p>
<p>现在你就可以把这个镜像的名字（用户名 / 应用名: 标签）告诉你的同事，让他去用 docker pull 下载部署了。</p>
<h3 id="离线环境该怎么办">离线环境该怎么办</h3>
<p>使用 Docker Hub 来管理镜像的确是非常方便，不过有一种场景下它却是无法发挥作用，那就是企业内网的离线环境，连不上外网，自然也就不能使用 docker push、docker pull  来推送拉取镜像了。那这种情况有没有解决办法呢？方法当然有，而且有很多。最佳的方法就是在内网环境里仿造 Docker Hub，创建一个自己的私有 Registry 服务，由它来管理我们的镜像，就像我们自己搭建 GitLab 做版本管理一样。自建 Registry 已经有很多成熟的解决方案，比如 Docker Registry，还有 CNCF Harbor，不过使用它们还需要一些目前没有讲到的知识，步骤也有点繁琐，所以我会在后续的课程里再介绍。下面我讲讲存储、分发镜像的一种“笨”办法，虽然比较“原始”，但简单易行，可以作为临时的应急手段。Docker 提供了 save 和 load 这两个镜像归档命令，可以把镜像导出成压缩包，或者从压缩包导入 Docker，而压缩包是非常容易保管和传输的，可以联机拷贝，FTP 共享，甚至存在 U 盘上随身携带。</p>
<p>需要注意的是，这两个命令默认使用标准流作为输入输出（为了方便 Linux 管道操作），所以一般会用 -o、-i 参数来使用文件的形式，例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker save ngx-app:latest -o ngx.tar
docker load -i ngx.tar
</code></pre></td></tr></table>
</div>
</div><h3 id="小结-5">小结</h3>
<p>好了，今天我们一起学习了镜像仓库，了解了 Docker Hub 的使用方法，整理一下要点方便你加深理解：镜像仓库（Registry）是一个提供综合镜像服务的网站，最基本的功能是上传和下载。Docker Hub 是目前最大的镜像仓库，拥有许多高质量的镜像。上面的镜像非常多，选择的标准有官方认证、下载量、星数等，需要综合评估。镜像也有很多版本，应该根据版本号和操作系统仔细确认合适的标签。在 Docker Hub 注册之后就可以上传自己的镜像，用 docker tag 打上标签再用 docker push 推送。离线环境可以自己搭建私有镜像仓库，或者使用 docker save 把镜像存成压缩包，再用 docker load 从压缩包恢复成镜像。</p>
<h2 id="06打破次元壁容器该如何与外界互联互通">06｜打破次元壁：容器该如何与外界互联互通</h2>
<p>在前面的几节课里，我们已经学习了容器、镜像、镜像仓库的概念和用法，也知道了应该如何创建镜像，再以容器的形式启动应用。不过，用容器来运行“busybox”“hello world”这样比较简单的应用还好，如果是 Nginx、Redis、MySQL 这样的后台服务应用，因为它们运行在容器的“沙盒”里，完全与外界隔离，无法对外提供服务，也就失去了价值。这个时候，容器的隔离环境反而成为了一种负面特性。所以，容器的这个“小板房”不应该是一个完全密闭的铁屋子，而是应该给它开几扇门窗，让应用在“足不出户”的情况下，也能够与外界交换数据、互通有无，这样“有限的隔离”才是我们真正所需要的运行环境。那么今天，我就以 Docker 为例，来讲讲有哪些手段能够在容器与外部系统之间沟通交流。</p>
<h3 id="如何拷贝容器内的数据">如何拷贝容器内的数据</h3>
<p>我们首先来看看 Docker 提供的 cp 命令，它可以在宿主机和容器之间拷贝文件，是最基本的一种数据交换功能。试验这个命令需要先用 docker run 启动一个容器，就用 Redis 吧：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d --rm redis
</code></pre></td></tr></table>
</div>
</div><p>注意这里使用了 -d、&ndash;rm 两个参数，表示运行在后台，容器结束后自动删除，然后使用 docker ps 命令可以看到 Redis 容器正在运行，容器 ID 是“062”。docker cp 的用法很简单，很类似 Linux 的“cp”“scp”，指定源路径（src path）和目标路径（dest path）就可以了。如果源路径是宿主机那么就是把文件拷贝进容器，如果源路径是容器那么就是把文件拷贝出容器，注意需要用容器名或者容器 ID 来指明是哪个容器的路径。假设当前目录下有一个“a.txt”的文件，现在我们要把它拷贝进 Redis 容器的“/tmp”目录，如果使用容器 ID，命令就会是这样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker cp a.txt 062:/tmp
</code></pre></td></tr></table>
</div>
</div><p>接下来我们可以使用 docker exec 命令，进入容器看看文件是否已经正确拷贝了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker exec -it 062 sh
</code></pre></td></tr></table>
</div>
</div><p>可以看到，在“/tmp”目录下，确实已经有了一个“a.txt”。现在让我们再来试验一下从容器拷贝出文件，只需要把 docker cp 后面的两个路径调换一下位置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker cp 062:/tmp/a.txt ./b.txt
</code></pre></td></tr></table>
</div>
</div><p>这样，在宿主机的当前目录里，就会多出一个新的“b.txt”，也就是从容器里拿到的文件。</p>
<h3 id="如何共享主机上的文件">如何共享主机上的文件</h3>
<p>docker cp 的用法模仿了操作系统的拷贝命令，偶尔一两次的文件共享还可以应付，如果容器运行时经常有文件来往互通，这样反复地拷来拷去就显得很麻烦，也很容易出错。你也许会联想到虚拟机有一种“共享目录”的功能。它可以在宿主机上开一个目录，然后把这个目录“挂载”进虚拟机，这样就实现了两者共享同一个目录，一边对目录里文件的操作另一边立刻就能看到，没有了数据拷贝，效率自然也会高很多。沿用这个思路，容器也提供了这样的共享宿主机目录的功能，效果也和虚拟机几乎一样，用起来很方便，只需要在 docker run 命令启动容器的时候使用 -v 参数就行，具体的格式是“宿主机路径: 容器内路径”。</p>
<p>我还是以 Redis 为例，启动容器，使用 -v 参数把本机的“/tmp”目录挂载到容器里的“/tmp”目录，也就是说让容器共享宿主机的“/tmp”目录：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d --rm -v /tmp:/tmp redis
</code></pre></td></tr></table>
</div>
</div><p>然后我们再用 docker exec 进入容器，查看一下容器内的“/tmp”目录，应该就可以看到文件与宿主机是完全一致的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker exec -it b5a sh    # b5a是容器ID
</code></pre></td></tr></table>
</div>
</div><p>你也可以在容器里的“/tmp”目录下随便做一些操作，比如删除文件、建立新目录等等，再回头观察一下宿主机，会发现修改会即时同步，这就表明容器和宿主机确实已经共享了这个目录。-v 参数挂载宿主机目录的这个功能，对于我们日常开发测试工作来说非常有用，我们可以在不变动本机环境的前提下，使用镜像安装任意的应用，然后直接以容器来运行我们本地的源码、脚本，非常方便。这里我举一个简单的例子。比如我本机上只有 Python 2.7，但我想用 Python 3 开发，如果同时安装 Python 2 和 Python 3 很容易就会把系统搞乱，所以我就可以这么做：</p>
<p>先使用 docker pull 拉取一个 Python 3 的镜像，因为它打包了完整的运行环境，运行时有隔离，所以不会对现有系统的 Python 2.7 产生任何影响。在本地的某个目录编写 Python 代码，然后用 -v 参数让容器共享这个目录。现在就可以在容器里以 Python 3 来安装各种包，再运行脚本做开发了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mysql" data-lang="mysql"><span class="n">docker</span><span class="w"> </span><span class="n">pull</span><span class="w"> </span><span class="n">python</span><span class="p">:</span><span class="n">alpine</span><span class="w">
</span><span class="w"></span><span class="n">docker</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="o">-</span><span class="n">it</span><span class="w"> </span><span class="o">--</span><span class="n">rm</span><span class="w"> </span><span class="o">-</span><span class="n">v</span><span class="w"> </span><span class="o">`</span><span class="n">pwd</span><span class="o">`</span><span class="p">:</span><span class="o">/</span><span class="n">tmp</span><span class="w"> </span><span class="n">python</span><span class="p">:</span><span class="n">alpine</span><span class="w"> </span><span class="n">sh</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>显然，这种方式比把文件打包到镜像或者 docker cp 会更加灵活，非常适合有频繁修改的开发测试工作。</p>
<h3 id="如何实现网络互通">如何实现网络互通</h3>
<p>现在我们使用 docker cp 和 docker run -v 可以解决容器与外界的文件互通问题，但对于 Nginx、Redis 这些服务器来说，网络互通才是更要紧的问题。网络互通的关键在于“打通”容器内外的网络，而处理网络通信无疑是计算机系统里最棘手的工作之一，有许许多多的名词、协议、工具，在这里我也没有办法一下子就把它都完全说清楚，所以只能从“宏观”层面讲个大概，帮助你快速理解。</p>
<p>Docker 提供了三种网络模式，分别是 null、host 和 bridge。null 是最简单的模式，也就是没有网络，但允许其他的网络插件来自定义网络连接，这里就不多做介绍了。host 的意思是直接使用宿主机网络，相当于去掉了容器的网络隔离（其他隔离依然保留），所有的容器会共享宿主机的 IP 地址和网卡。这种模式没有中间层，自然通信效率高，但缺少了隔离，运行太多的容器也容易导致端口冲突。host 模式需要在 docker run 时使用 &ndash;net=host 参数，下面我就用这个参数启动 Nginx：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d --rm --net=host nginx:alpine
</code></pre></td></tr></table>
</div>
</div><p>为了验证效果，我们可以在本机和容器里分别执行 ip addr 命令，查看网卡信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ip addr                    # 本机查看网卡
docker exec xxx ip addr    # 容器查看网卡
</code></pre></td></tr></table>
</div>
</div><p>可以看到这两个 ip addr 命令的输出信息是完全一样的，比如都是一个网卡 ens160，IP 地址是“192.168.10.208”，这就证明 Nginx 容器确实与本机共享了网络栈。第三种 bridge，也就是桥接模式，它有点类似现实世界里的交换机、路由器，只不过是由软件虚拟出来的，容器和宿主机再通过虚拟网卡接入这个网桥（图中的 docker0），那么它们之间也就可以正常的收发网络数据包了。不过和 host 模式相比，bridge 模式多了虚拟网桥和网卡，通信效率会低一些。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6e/60/6e0d05cf19720f44ca68f88238627460.jpg?wh=1920x1407"
        data-srcset="https://static001.geekbang.org/resource/image/6e/60/6e0d05cf19720f44ca68f88238627460.jpg?wh=1920x1407, https://static001.geekbang.org/resource/image/6e/60/6e0d05cf19720f44ca68f88238627460.jpg?wh=1920x1407 1.5x, https://static001.geekbang.org/resource/image/6e/60/6e0d05cf19720f44ca68f88238627460.jpg?wh=1920x1407 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6e/60/6e0d05cf19720f44ca68f88238627460.jpg?wh=1920x1407"
        title="img" /></p>
<p>和 host 模式一样，我们也可以用 &ndash;net=bridge 来启用桥接模式，但其实并没有这个必要，因为 Docker 默认的网络模式就是 bridge，所以一般不需要显式指定。下面我们启动两个容器 Nginx 和 Redis，就像刚才说的，没有特殊指定就会使用 bridge 模式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d --rm nginx:alpine    # 默认使用桥接模式
docker run -d --rm redis           # 默认使用桥接模式
</code></pre></td></tr></table>
</div>
</div><p>然后我们还是在本机和容器里执行 ip addr 命令（Redis 容器里没有 ip 命令，所以只能在 Nginx 容器里执行）</p>
<p>对比一下刚才 host 模式的输出，就可以发现容器里的网卡设置与宿主机完全不同，eth0 是一个虚拟网卡，IP 地址是 B 类私有地址“172.17.0.2”。我们还可以用 docker inspect 直接查看容器的 ip 地址：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker inspect xxx |grep IPAddress
</code></pre></td></tr></table>
</div>
</div><p>这显示出两个容器的 IP 地址分别是“172.17.0.2”和“172.17.0.3”，而宿主机的 IP 地址则是“172.17.0.1”，所以它们都在“172.17.0.0/16”这个 Docker 的默认网段，彼此之间就能够使用 IP 地址来实现网络通信了。</p>
<h3 id="如何分配服务端口号">如何分配服务端口号</h3>
<p>使用 host 模式或者 bridge 模式，我们的容器就有了 IP 地址，建立了与外部世界的网络连接，接下来要解决的就是网络服务的端口号问题。你一定知道，服务器应用都必须要有端口号才能对外提供服务，比如 HTTP 协议用 80、HTTPS 用 443、Redis 是 6379、MySQL 是 3306。第 4 讲我们在学习编写 Dockerfile 的时候也看到过，可以用 EXPOSE 指令声明容器对外的端口号。一台主机上的端口号数量是有限的，而且多个服务之间还不能够冲突，但我们打包镜像应用的时候通常都使用的是默认端口，容器实际运行起来就很容易因为端口号被占用而无法启动。</p>
<p>解决这个问题的方法就是加入一个“中间层”，由容器环境例如 Docker 来统一管理分配端口号，在本机端口和容器端口之间做一个“映射”操作，容器内部还是用自己的端口号，但外界看到的却是另外一个端口号，这样就很好地避免了冲突。端口号映射需要使用 bridge 模式，并且在 docker run 启动容器时使用 -p 参数，形式和共享目录的 -v 参数很类似，用 : 分隔本机端口和容器端口。比如，如果要启动两个 Nginx 容器，分别跑在 80 和 8080 端口上：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d -p 80:80 --rm nginx:alpine
docker run -d -p 8080:80 --rm nginx:alpine
</code></pre></td></tr></table>
</div>
</div><p>这样就把本机的 80 和 8080 端口分别“映射”到了两个容器里的 80 端口，不会发生冲突，我们可以用 curl 再验证一下：</p>
<p>使用 docker ps 命令能够在“PORTS”栏里更直观地看到端口的映射情况：</p>
<h3 id="小结-6">小结</h3>
<p>好了，今天我们一起学习了容器与外部系统之间沟通交流的几种方法。你会发现，这些方法几乎消除了容器化的应用和本地应用因为隔离特性而产生的差异，而因为镜像独特的打包机制，容器技术显然能够比 apt/yum 更方便地安装各种应用，绝不会“污染”已有的系统。今天的课里我列举了 Python、Nginx 等例子，你还可以举一反三，借鉴它们把本地配置文件加载到容器里适当的位置，再映射端口号，把 Redis、MySQL、Node.js 都运行起来，让容器成为我们工作中的得力助手。照例简单小结一下这次的要点：</p>
<p>docker cp 命令可以在容器和主机之间互相拷贝文件，适合简单的数据交换。docker run -v 命令可以让容器和主机共享本地目录，免去了拷贝操作，提升工作效率。host 网络模式让容器与主机共享网络栈，效率高但容易导致端口冲突。bridge 网络模式实现了一个虚拟网桥，容器和主机都在一个私有网段内互联互通。docker run -p 命令可以把主机的端口号映射到容器的内部端口号，解决了潜在的端口冲突问题。</p>
<h2 id="07实战演练玩转docker">07｜实战演练：玩转Docker</h2>
<p>要提醒你的是，Docker 相关的内容很多很广，在入门篇中，我只从中挑选出了一些最基本最有用的介绍给你。而且在我看来，我们不需要完全了解 Docker 的所有功能，我也不建议你对 Docker 的内部架构细节和具体的命令行参数做过多的了解，太浪费精力，只要会用够用，需要的时候能够查找官方手册就行。毕竟我们这门课程的目标是 Kubernetes，而 Docker 只不过是众多容器运行时（Container Runtime）中最出名的一款而已。当然，如果你当前的工作是与 Docker 深度绑定，那就另当别论了。好下面我先把容器技术做一个简要的总结，然后演示两个实战项目：使用 Docker 部署 Registry 和 WordPress。</p>
<h3 id="容器技术要点回顾">容器技术要点回顾</h3>
<p>容器技术是后端应用领域的一项重大创新，它彻底变革了应用的开发、交付与部署方式，是“云原生”的根本（01 讲）。容器基于 Linux 底层的 namespace、cgroup、chroot 等功能，虽然它们很早就出现了，但直到 Docker“横空出世”，把它们整合在一起，容器才真正走近了大众的视野，逐渐为广大开发者所熟知（02 讲）。容器技术中有三个核心概念：容器（Container）、镜像（Image），以及镜像仓库（Registry）（03 讲）。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        data-srcset="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 1.5x, https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c8/fe/c8116066bdbf295a7c9fc25b87755dfe.jpg?wh=1920x1048"
        title="img" /></p>
<p>从本质上来说，容器属于虚拟化技术的一种，和虚拟机（Virtual Machine）很类似，都能够分拆系统资源，隔离应用进程，但容器更加轻量级，运行效率更高，比虚拟机更适合云计算的需求。镜像是容器的静态形式，它把应用程序连同依赖的操作系统、配置文件、环境变量等等都打包到了一起，因而能够在任何系统上运行，免除了很多部署运维和平台迁移的麻烦。镜像内部由多个层（Layer）组成，每一层都是一组文件，多个层会使用 Union FS 技术合并成一个文件系统供容器使用。这种细粒度结构的好处是相同的层可以共享、复用，节约磁盘存储和网络传输的成本，也让构建镜像的工作变得更加容易（04 讲）。为了方便管理镜像，就出现了镜像仓库，它集中存放各种容器化的应用，用户可以任意上传下载，是分发镜像的最佳方式（05 讲）。目前最知名的公开镜像仓库是 Docker Hub，其他的还有 quay.io、gcr.io，我们可以在这些网站上找到许多高质量镜像，集成到我们自己的应用系统中。容器技术有很多具体的实现，Docker 是最初也是最流行的容器技术，它的主要形态是运行在 Linux 上的“Docker Engine”。我们日常使用的 docker 命令其实只是一个前端工具，它必须与后台服务“Docker daemon”通信才能实现各种功能。操作容器的常用命令有 docker ps、docker run、docker exec、docker stop 等；操作镜像的常用命令有 docker images、docker rmi、docker build、docker tag 等；操作镜像仓库的常用命令有 docker pull、docker push 等。好简单地回顾了容器技术，下面我们就来综合运用在“入门篇”所学到的各个知识点，开始实战演练，玩转 Docker。</p>
<h3 id="搭建私有镜像仓库">搭建私有镜像仓库</h3>
<p>在第 5 节课讲 Docker Hub 的时候曾经说过，在离线环境里，我们可以自己搭建私有仓库。但因为镜像仓库是网络服务的形式，当时还没有学到容器网络相关的知识，所以只有到了现在，我们具备了比较完整的 Docker 知识体系，才能够搭建私有仓库。私有镜像仓库有很多现成的解决方案，今天我只选择最简单的 Docker Registry，而功能更完善的 CNCF Harbor 留到后续学习 Kubernetes 时再介绍。你可以在 Docker Hub 网站上搜索“registry”，找到它的官方页面（https://registry.hub.docker.com/_/registry/）：</p>
<p>Docker Registry 的网页上有很详细的说明，包括下载命令、用法等，我们可以完全照着它来操作。首先，你需要使用 docker pull 命令拉取镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker pull registry
</code></pre></td></tr></table>
</div>
</div><p>然后，我们需要做一个端口映射，对外暴露端口，这样 Docker Registry 才能提供服务。它的容器内端口是 5000，简单起见，我们在外面也使用同样的 5000 端口，所以运行命令就是 docker run -d -p 5000:5000 registry ：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d -p 5000:5000 registry

</code></pre></td></tr></table>
</div>
</div><p>启动 Docker Registry 之后，你可以使用 docker ps 查看它的运行状态，可以看到它确实把本机的 5000 端口映射到了容器内的 5000 端口。</p>
<p>接下来，我们就要使用 docker tag 命令给镜像打标签再上传了。因为上传的目标不是默认的 Docker Hub，而是本地的私有仓库，所以镜像的名字前面还必须再加上仓库的地址（域名或者 IP 地址都行），形式上和 HTTP 的 URL 非常像。比如在这里，我就把“nginx:alpine”改成了“127.0.0.1:5000/nginx:alpine”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker tag nginx:alpine 127.0.0.1:5000/nginx:alpine

</code></pre></td></tr></table>
</div>
</div><p>现在，这个镜像有了一个附加仓库地址的完整名字，就可以用 docker push 推上去了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker push 127.0.0.1:5000/nginx:alpine
</code></pre></td></tr></table>
</div>
</div><p>为了验证是否已经成功推送，我们可以把刚才打标签的镜像删掉，再重新下载：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker rmi  127.0.0.1:5000/nginx:alpine
docker pull 127.0.0.1:5000/nginx:alpine
</code></pre></td></tr></table>
</div>
</div><p>这里 docker pull 确实完成了镜像下载任务，不过因为原来的层原本就已经存在，所以不会有实际的下载动作，只会创建一个新的镜像标签。Docker Registry 虽然没有图形界面，但提供了 RESTful API，也可以发送 HTTP 请求来查看仓库里的镜像，具体的端点信息可以参考官方文档（https://docs.docker.com/registry/spec/api/），下面的这两条 curl 命令就分别获取了镜像列表和 Nginx 镜像的标签列表：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">curl 127.1:5000/v2/_catalog
curl 127.1:5000/v2/nginx/tags/list
</code></pre></td></tr></table>
</div>
</div><p>可以看到，因为应用被封装到了镜像里，所以我们只用简单的一两条命令就完成了私有仓库的搭建工作，完全不需要复杂的软件安装、环境设置、调试测试等繁琐的操作，这在容器技术出现之前简直是不可想象的。</p>
<h3 id="搭建-wordpress-网站">搭建 WordPress 网站</h3>
<p>Docker Registry 应用比较简单，只用单个容器就运行了一个完整的服务，下面我们再来搭建一个有点复杂的 WordPress 网站。网站需要用到三个容器：WordPress、MariaDB、Nginx，它们都是非常流行的开源项目，在 Docker Hub 网站上有官方镜像，网页上的说明也很详细，所以具体的搜索过程我就略过了，直接使用 docker pull 拉取它们的镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker pull wordpress:5
docker pull mariadb:10
docker pull nginx:alpine
</code></pre></td></tr></table>
</div>
</div><p>我画了一个简单的网络架构图，你可以直观感受一下它们之间的关系：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643"
        data-srcset="https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643, https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643 1.5x, https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643"
        title="img" /></p>
<p>这个系统可以说是比较典型的网站了。MariaDB 作为后面的关系型数据库，端口号是 3306；WordPress 是中间的应用服务器，使用 MariaDB 来存储数据，它的端口是 80；Nginx 是前面的反向代理，它对外暴露 80 端口，然后把请求转发给 WordPress。我们先来运行 MariaDB。根据说明文档，需要配置“MARIADB_DATABASE”等几个环境变量，用 &ndash;env 参数来指定启动时的数据库、用户名和密码，这里我指定数据库是“db”，用户名是“wp”，密码是“123”，管理员密码（root password）也是“123”。下面就是启动 MariaDB 的 docker run 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d --rm \
    --env MARIADB_DATABASE=db \
    --env MARIADB_USER=wp \
    --env MARIADB_PASSWORD=123 \
    --env MARIADB_ROOT_PASSWORD=123 \
    mariadb:10
</code></pre></td></tr></table>
</div>
</div><p>启动之后，我们还可以使用 docker exec 命令，执行数据库的客户端工具“mysql”，验证数据库是否正常运行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker exec -it 9ac mysql -u wp -p
</code></pre></td></tr></table>
</div>
</div><p>输入刚才设定的用户名“wp”和密码“123”之后，我们就连接上了 MariaDB，可以使用 show databases; 和 show tables; 等命令来查看数据库里的内容。当然，现在肯定是空的。</p>
<p>因为 Docker 的 bridge 网络模式的默认网段是“172.17.0.0/16”，宿主机固定是“172.17.0.1”，而且 IP 地址是顺序分配的，所以如果之前没有其他容器在运行的话，MariaDB 容器的 IP 地址应该就是“172.17.0.2”，这可以通过 docker inspect 命令来验证：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker inspect 9ac |grep IPAddress

</code></pre></td></tr></table>
</div>
</div><p>现在数据库服务已经正常，该运行应用服务器 WordPress 了，它也要用 &ndash;env 参数来指定一些环境变量才能连接到 MariaDB，注意“WORDPRESS_DB_HOST”必须是 MariaDB 的 IP 地址，否则会无法连接数据库：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d --rm \
    --env WORDPRESS_DB_HOST=172.17.0.2 \
    --env WORDPRESS_DB_USER=wp \
    --env WORDPRESS_DB_PASSWORD=123 \
    --env WORDPRESS_DB_NAME=db \
    wordpress:5
</code></pre></td></tr></table>
</div>
</div><p>WordPress 容器在启动的时候并没有使用 -p 参数映射端口号，所以外界是不能直接访问的，我们需要在前面配一个 Nginx 反向代理，把请求转发给 WordPress 的 80 端口。配置 Nginx 反向代理必须要知道 WordPress 的 IP 地址，同样可以用 docker inspect 命令查看，如果没有什么意外的话它应该是“172.17.0.3”，所以我们就能够写出如下的配置文件（Nginx 的用法可参考其他资料，这里就不展开讲了）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">server {
  listen 80;
  default_type text/html;

  location / {
      proxy_http_version 1.1;
      proxy_set_header Host $host;
      proxy_pass http://172.17.0.3;
  }
}
</code></pre></td></tr></table>
</div>
</div><p>有了这个配置文件，最关键的一步就来了，我们需要用 -p 参数把本机的端口映射到 Nginx 容器内部的 80 端口，再用 -v 参数把配置文件挂载到 Nginx 的“conf.d”目录下。这样，Nginx 就会使用刚才编写好的配置文件，在 80 端口上监听 HTTP 请求，再转发到 WordPress 应用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mysql" data-lang="mysql"><span class="n">docker</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="o">-</span><span class="n">d</span><span class="w"> </span><span class="o">--</span><span class="n">rm</span><span class="w"> </span><span class="err">\</span><span class="w">
</span><span class="w">    </span><span class="o">-</span><span class="n">p</span><span class="w"> </span><span class="mi">80</span><span class="p">:</span><span class="mi">80</span><span class="w"> </span><span class="err">\</span><span class="w">
</span><span class="w">    </span><span class="o">-</span><span class="n">v</span><span class="w"> </span><span class="o">`</span><span class="n">pwd</span><span class="o">`/</span><span class="n">wp</span><span class="p">.</span><span class="n">conf</span><span class="p">:</span><span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">nginx</span><span class="o">/</span><span class="n">conf</span><span class="p">.</span><span class="n">d</span><span class="o">/</span><span class="k">default</span><span class="p">.</span><span class="n">conf</span><span class="w"> </span><span class="err">\</span><span class="w">
</span><span class="w">    </span><span class="n">nginx</span><span class="p">:</span><span class="n">alpine</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>三个容器都启动之后，我们再用 docker ps 来看看它们的状态：
可以看到，WordPress 和 MariaDB 虽然使用了 80 和 3306 端口，但被容器隔离，外界不可见，只有 Nginx 有端口映射，能够从外界的 80 端口收发数据，网络状态和我们的架构图是一致的。现在整个系统就已经在容器环境里运行好了，我们来打开浏览器，输入本机的“127.0.0.1”或者是虚拟机的 IP 地址（我这里是“http://192.168.10.208”），就可以看到 WordPress 的界面：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a6/31/a63084a8dd95d0034ba72dcb60613531.png?wh=1224x1156"
        data-srcset="https://static001.geekbang.org/resource/image/a6/31/a63084a8dd95d0034ba72dcb60613531.png?wh=1224x1156, https://static001.geekbang.org/resource/image/a6/31/a63084a8dd95d0034ba72dcb60613531.png?wh=1224x1156 1.5x, https://static001.geekbang.org/resource/image/a6/31/a63084a8dd95d0034ba72dcb60613531.png?wh=1224x1156 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a6/31/a63084a8dd95d0034ba72dcb60613531.png?wh=1224x1156"
        title="img" /></p>
<p>在创建基本的用户、初始化网站之后，我们可以再登录 MariaDB，看看是否已经有了一些数据：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a2/1e/a22dcabe805471e304a74c715e7fb51e.png?wh=1920x1749"
        data-srcset="https://static001.geekbang.org/resource/image/a2/1e/a22dcabe805471e304a74c715e7fb51e.png?wh=1920x1749, https://static001.geekbang.org/resource/image/a2/1e/a22dcabe805471e304a74c715e7fb51e.png?wh=1920x1749 1.5x, https://static001.geekbang.org/resource/image/a2/1e/a22dcabe805471e304a74c715e7fb51e.png?wh=1920x1749 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a2/1e/a22dcabe805471e304a74c715e7fb51e.png?wh=1920x1749"
        title="img" /></p>
<p>可以看到，WordPress 已经在数据库里新建了很多的表，这就证明我们的容器化的 WordPress 网站搭建成功。</p>
<h3 id="小结-7">小结</h3>
<p>好了，今天我们简单地回顾了一下容器技术，这里有一份思维导图，是对前面所有容器知识要点的总结，你可以对照着用来复习。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/79/16/79f8c75e018e0a82eff432786110ef16.jpg?wh=1920x2142"
        data-srcset="https://static001.geekbang.org/resource/image/79/16/79f8c75e018e0a82eff432786110ef16.jpg?wh=1920x2142, https://static001.geekbang.org/resource/image/79/16/79f8c75e018e0a82eff432786110ef16.jpg?wh=1920x2142 1.5x, https://static001.geekbang.org/resource/image/79/16/79f8c75e018e0a82eff432786110ef16.jpg?wh=1920x2142 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/79/16/79f8c75e018e0a82eff432786110ef16.jpg?wh=1920x2142"
        title="img" /></p>
<p>我们还使用 Docker 实际搭建了两个服务：Registry 镜像仓库和 WordPress 网站。通过这两个项目的实战演练，你应该能够感受到容器化对后端开发带来的巨大改变，它简化了应用的打包、分发和部署，简单的几条命令就可以完成之前需要编写大量脚本才能完成的任务，对于开发、运维来绝对是一个“福音”。不过，在感受容器便利的同时，你有没有注意到它还是存在一些遗憾呢？比如说：</p>
<p>我们还是要手动运行一些命令来启动应用，然后再人工确认运行状态。运行多个容器组成的应用比较麻烦，需要人工干预（如检查 IP 地址）才能维护网络通信。现有的网络模式功能只适合单机，多台服务器上运行应用、负载均衡该怎么做？如果要增加应用数量该怎么办？这时容器技术完全帮不上忙。</p>
<p>其实，如果我们仔细整理这些运行容器的 docker run 命令，写成脚本，再加上一些 Shell、Python 编程来实现自动化，也许就能够得到一个勉强可用的解决方案。这个方案已经超越了容器技术本身，是在更高的层次上规划容器的运行次序、网络连接、数据持久化等应用要素，也就是现在我们常说的“容器编排”（Container Orchestration）的雏形，也正是后面要学习的 Kubernetes 的主要出发点。</p>
<h2 id="初级">初级</h2>
<h2 id="09走近云原生如何在本机搭建小巧完备的kubernetes环境">09｜走近云原生：如何在本机搭建小巧完备的Kubernetes环境</h2>
<p>在前面的“入门篇”里，我们学习了以 Docker 为代表的容器技术，做好了充分的准备，那么今天我们就来看看什么是容器编排、什么是 Kubernetes，还有应该怎么在自己的电脑上搭建出一个小巧完善的 Kubernetes 环境，一起走近云原生。</p>
<h3 id="什么是容器编排">什么是容器编排</h3>
<p>容器技术的核心概念是容器、镜像、仓库，使用这三大基本要素我们就可以轻松地完成应用的打包、分发工作，实现“一次开发，到处运行”的梦想。不过，当我们熟练地掌握了容器技术，信心满满地要在服务器集群里大规模实施的时候，却会发现容器技术的创新只是解决了运维部署工作中一个很小的问题。现实生产环境的复杂程度实在是太高了，除了最基本的安装，还会有各式各样的需求，比如服务发现、负载均衡、状态监控、健康检查、扩容缩容、应用迁移、高可用等等。</p>
<p>虽然容器技术开启了云原生时代，但它也只走出了一小步，再继续前进就无能为力了，因为这已经不再是隔离一两个进程的普通问题，而是要隔离数不清的进程，还有它们之间互相通信、互相协作的超级问题，困难程度可以说是指数级别的上升。这些容器之上的管理、调度工作，就是这些年最流行的词汇：“容器编排”（Container Orchestration）。</p>
<p>容器编排这个词听起来好像挺高大上，但如果你理解了之后就会发现其实也并不神秘。像我们在上次课里使用 Docker 部署 WordPress 网站的时候，把 Nginx、WordPress、MariaDB 这三个容器理清次序、配好 IP 地址去运行，就是最初级的一种“容器编排”，只不过这是纯手工操作，比较原始、粗糙。面对单机上的几个容器，“人肉”编排调度还可以应付，但如果规模上到几百台服务器、成千上万的容器，处理它们之间的复杂联系就必须要依靠计算机了，而目前计算机用来调度管理的“事实标准”，就是我们专栏的主角：Kubernetes。</p>
<h3 id="什么是-kubernetes">什么是 Kubernetes</h3>
<p>现在大家谈到容器都会说是 Docker，但其实早在 Docker 之前，Google 在公司内部就使用了类似的技术（cgroup 就是 Google 开发再提交给 Linux 内核的），只不过不叫容器。作为世界上最大的搜索引擎，Google 拥有数量庞大的服务器集群，为了提高资源利用率和部署运维效率，它专门开发了一个集群应用管理系统，代号 Borg，在底层支持整个公司的运转。</p>
<p>2014 年，Google 内部系统要“升级换代”，从原来的 Borg 切换到 Omega，于是按照惯例，Google 会发表公开论文。因为之前在发表 MapReduce、BigTable、GFS 时吃过亏（被 Yahoo 开发的 Hadoop 占领了市场），所以 Google 决定借着 Docker 的“东风”，在发论文的同时，把 C++ 开发的 Borg 系统用 Go 语言重写并开源，于是 Kubernetes 就这样诞生了。</p>
<p>由于 Kubernetes 背后有 Borg 系统十多年生产环境经验的支持，技术底蕴深厚，理论水平也非常高，一经推出就引起了轰动。然后在 2015 年，Google 又联合 Linux 基金会成立了 CNCF（Cloud Native Computing Foundation，云原生基金会），并把 Kubernetes 捐献出来作为种子项目。有了 Google 和 Linux 这两大家族的保驾护航，再加上宽容开放的社区，作为 CNCF 的“头把交椅”，Kubernetes 旗下很快就汇集了众多行业精英，仅用了两年的时间就打败了同期的竞争对手 Apache Mesos 和 Docker Swarm，成为了这个领域的唯一霸主。</p>
<p>那么，Kubernetes 到底能够为我们做什么呢？简单来说，Kubernetes 就是一个生产级别的容器编排平台和集群管理系统，不仅能够创建、调度容器，还能够监控、管理服务器，它凝聚了 Google 等大公司和开源社区的集体智慧，从而让中小型公司也可以具备轻松运维海量计算节点——也就是“云计算”的能力。</p>
<h4 id="什么是-minikube">什么是 minikube</h4>
<p>Kubernetes 一般都运行在大规模的计算集群上，管理很严格，这就对我们个人来说造成了一定的障碍，没有实际操作环境怎么能够学好用好呢？好在 Kubernetes 充分考虑到了这方面的需求，提供了一些快速搭建 Kubernetes 环境的工具，在官网（https://kubernetes.io/zh/docs/tasks/tools/）上推荐的有两个：kind 和 minikube，它们都可以在本机上运行完整的 Kubernetes 环境。</p>
<p>我说一下对这两个工具的个人看法，供你参考。kind 基于 Docker，意思是“Kubernetes in Docker”。它功能少，用法简单，也因此运行速度快，容易上手。不过它缺少很多 Kubernetes 的标准功能，例如仪表盘、网络插件，也很难定制化，所以我认为它比较适合有经验的 Kubernetes 用户做快速开发测试，不太适合学习研究。不选 kind 还有一个原因，它的名字与 Kubernetes YAML 配置里的字段 kind 重名，会对初学者造成误解，干扰学习。再来看 minikube，从名字就能够看出来，它是一个“迷你”版本的 Kubernetes，自从 2016 年发布以来一直在积极地开发维护，紧跟 Kubernetes 的版本更新，同时也兼容较旧的版本（最多只到之前的 6 个小版本）。minikube 最大特点就是“小而美”，可执行文件仅有不到 100MB，运行镜像也不过 1GB，但就在这么小的空间里却集成了 Kubernetes 的绝大多数功能特性，不仅有核心的容器编排功能，还有丰富的插件，例如 Dashboard、GPU、Ingress、Istio、Kong、Registry 等等，综合来看非常完善。所以，我建议你在这个专栏里选择 minikube 来学习 Kubernetes。</p>
<h3 id="如何搭建-minikube-环境">如何搭建 minikube 环境</h3>
<p>minikube 支持 Mac、Windows、Linux 这三种主流平台，你可以在它的官网（https://minikube.sigs.k8s.io）找到详细的安装说明，当然在我们这里就只用虚拟机里的 Linux 了。minikube 的最新版本是 1.25.2，支持的 Kubernetes 版本是 1.23.3，所以我们就选定它作为我们初级篇的学习工具。</p>
<p>minikube 不包含在系统自带的 apt/yum 软件仓库里，我们只能自己去网上找安装包。不过因为它是用 Go 语言开发的，整体就是一个二进制文件，没有多余的依赖，所以安装过程也非常简单，只需要用 curl 或者 wget 下载就行。minikube 的官网提供了各种系统的安装命令，通常就是下载、拷贝这两步，不过你需要注意一下本机电脑的硬件架构，Intel 芯片要选择带“amd64”后缀，Apple M1 芯片要选择“arm64”后缀，选错了就会因为 CPU 指令集不同而无法运行：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d5/84/d526aa920fba9bee9856177495a1c884.png?wh=1920x1004"
        data-srcset="https://static001.geekbang.org/resource/image/d5/84/d526aa920fba9bee9856177495a1c884.png?wh=1920x1004, https://static001.geekbang.org/resource/image/d5/84/d526aa920fba9bee9856177495a1c884.png?wh=1920x1004 1.5x, https://static001.geekbang.org/resource/image/d5/84/d526aa920fba9bee9856177495a1c884.png?wh=1920x1004 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d5/84/d526aa920fba9bee9856177495a1c884.png?wh=1920x1004"
        title="img" /></p>
<p>我也把官网上 Linux 系统安装的命令抄在了这里，你可以直接拷贝后安装：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># Intel x86_64
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

# Apple arm64
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-arm64

sudo install minikube /usr/local/bin/
</code></pre></td></tr></table>
</div>
</div><p>安装完成之后，你可以执行命令 minikube version，看看它的版本号，验证是否安装成功：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">minikube version
</code></pre></td></tr></table>
</div>
</div><p>不过 minikube 只能够搭建 Kubernetes 环境，要操作 Kubernetes，还需要另一个专门的客户端工具“kubectl”。kubectl 的作用有点类似之前我们学习容器技术时候的工具“docker”，它也是一个命令行工具，作用也比较类似，同样是与 Kubernetes 后台服务通信，把我们的命令转发给 Kubernetes，实现容器和集群的管理功能。kubectl 是一个与 Kubernetes、minikube 彼此独立的项目，所以不包含在 minikube 里，但 minikube 提供了安装它的简化方式，你只需执行下面的这条命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">minikube kubectl
</code></pre></td></tr></table>
</div>
</div><p>它就会把与当前 Kubernetes 版本匹配的 kubectl 下载下来，存放在内部目录（例如 .minikube/cache/linux/arm64/v1.23.3），然后我们就可以使用它来对 Kubernetes“发号施令”了。所以，在 minikube 环境里，我们会用到两个客户端：minikube 管理 Kubernetes 集群环境，kubectl 操作实际的 Kubernetes 功能，和 Docker 比起来有点复杂。我画了一个简单的 minikube 环境示意图，方便你理解它们的关系。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/22/e3/22c4d6ef48a0cf009946ebbbc31b91e3.jpg?wh=1920x1406"
        data-srcset="https://static001.geekbang.org/resource/image/22/e3/22c4d6ef48a0cf009946ebbbc31b91e3.jpg?wh=1920x1406, https://static001.geekbang.org/resource/image/22/e3/22c4d6ef48a0cf009946ebbbc31b91e3.jpg?wh=1920x1406 1.5x, https://static001.geekbang.org/resource/image/22/e3/22c4d6ef48a0cf009946ebbbc31b91e3.jpg?wh=1920x1406 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/22/e3/22c4d6ef48a0cf009946ebbbc31b91e3.jpg?wh=1920x1406"
        title="img" /></p>
<h3 id="实际验证-minikube-环境">实际验证 minikube 环境</h3>
<p>前面的工作都做完之后，我们就可以在本机上运行 minikube，创建 Kubernetes 实验环境了。使用命令 minikube start 会从 Docker Hub 上拉取镜像，以当前最新版本的 Kubernetes 启动集群。不过为了保证实验环境的一致性，我们可以在后面再加上一个参数 &ndash;kubernetes-version，明确指定要使用 Kubernetes 版本。这里我使用“1.23.3”，启动命令就是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">minikube start --kubernetes-version=v1.23.3
</code></pre></td></tr></table>
</div>
</div><p>（它的启动过程使用了比较活泼的表情符号，可能是想表现得平易近人吧，如果不喜欢也可以调整设置关闭它。）现在 Kubernetes 集群就已经在我们本地运行了，你可以使用 minikube status、minikube node list这两个命令来查看集群的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">minikube status
minikube node list
</code></pre></td></tr></table>
</div>
</div><p>从截图里可以看到，Kubernetes 集群里现在只有一个节点，名字就叫“minikube”，类型是“Control Plane”，里面有 host、kubelet、apiserver 三个服务，IP 地址是 192.168.49.2。你还可以用命令 minikube ssh 登录到这个节点上，虽然它是虚拟的，但用起来和实机也没什么区别</p>
<p>有了集群，接下来我们就可以使用 kubectl 来操作一下，初步体会 Kubernetes 这个容器编排系统，最简单的命令当然就是查看版本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl version
</code></pre></td></tr></table>
</div>
</div><p>不过这条命令还不能直接用，因为使用 minikube 自带的 kubectl 有一点形式上的限制，要在前面加上 minikube 的前缀，后面再有个 &ndash;，像这样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">minikube kubectl -- version 
</code></pre></td></tr></table>
</div>
</div><p>为了避免这个不大不小的麻烦，我建议你使用 Linux 的“alias”功能，为它创建一个别名，写到当前用户目录下的 .bashrc 里，也就是这样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">alias kubectl=&#34;minikube kubectl --&#34;
</code></pre></td></tr></table>
</div>
</div><p>另外，kubectl 还提供了命令自动补全的功能，你还应该再加上“kubectl completion”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">source &lt;(kubectl completion bash)
</code></pre></td></tr></table>
</div>
</div><p>现在，我们就可以愉快地使用 kubectl 了</p>
<p>下面我们在 Kubernetes 里运行一个 Nginx 应用，命令与 Docker 一样，也是 run，不过形式上有点区别，需要用 &ndash;image 指定镜像，然后 Kubernetes 会自动拉取并运行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl run ngx --image=nginx:alpine
</code></pre></td></tr></table>
</div>
</div><p>这里涉及 Kubernetes 里的一个非常重要的概念：Pod，你可以暂时把它理解成是“穿了马甲”的容器，查看 Pod 列表需要使用命令 kubectl get pod，它的效果类似 docker ps</p>
<p>命令执行之后可以看到，在 Kubernetes 集群里就有了一个名字叫 ngx 的 Pod 正在运行，表示我们的这个单节点 minikube 环境已经搭建成功。</p>
<h3 id="小结-8">小结</h3>
<p>好了，今天我们先了解了容器编排概念和 Kubernetes 的历史，然后在 Linux 虚拟机上安装了 minikube 和 kubectl，运行了一个简单但完整的 Kubernetes 集群，实现了与云原生的“第一次亲密接触”。那什么是云原生呢？这在 CNCF 上有明确的定义，不过我觉得太学术化了，我也不想机械重复，就讲讲我自己的通俗理解吧。所谓的“云”，现在就指的是 Kubernetes，那么“云原生”的意思就是应用的开发、部署、运维等一系列工作都要向 Kubernetes 看齐，使用容器、微服务、声明式 API 等技术，保证应用的整个生命周期都能够在 Kubernetes 环境里顺利实施，不需要附加额外的条件。换句话说，“云原生”就是 Kubernetes 里的“原住民”，而不是从其他环境迁过来的“移民”。最后照例小结一下今天的内容：</p>
<p>容器技术只解决了应用的打包、安装问题，面对复杂的生产环境就束手无策了，解决之道就是容器编排，它能够组织管理各个应用容器之间的关系，让它们顺利地协同运行。Kubernetes 源自 Google 内部的 Borg 系统，也是当前容器编排领域的事实标准。minikube 可以在本机搭建 Kubernetes 环境，功能很完善，适合学习研究。操作 Kubernetes 需要使用命令行工具 kubectl，只有通过它才能与 Kubernetes 集群交互。kubectl 的用法与 docker 类似，也可以拉取镜像运行，但操作的不是简单的容器，而是 Pod。</p>
<p>另外还要说一下 Kubernetes 的官网（https://kubernetes.io/zh/），里面有非常详细的文档，包括概念解释、入门教程、参考手册等等，最难得的是它有全中文版本，我们阅读起来完全不会有语言障碍，希望你有时间多上去看看，及时获取官方第一手知识。</p>
<h2 id="10自动化的运维管理探究kubernetes工作机制的奥秘">10｜自动化的运维管理：探究Kubernetes工作机制的奥秘</h2>
<p>在上一次课里，我们看到容器技术只实现了应用的打包分发，到运维真正落地实施的时候仍然会遇到很多困难，所以就需要用容器编排技术来解决这些问题，而 Kubernetes 是这个领域的唯一霸主，已经成为了“事实标准”。那么，Kubernetes 凭什么能担当这样的领军重任呢？难道仅仅因为它是由 Google 主导开发的吗？今天我就带你一起来看看 Kubernetes 的内部架构和工作机制，了解它能够傲视群雄的秘密所在。</p>
<h3 id="云计算时代的操作系统">云计算时代的操作系统</h3>
<p>前面我曾经说过，Kubernetes 是一个生产级别的容器编排平台和集群管理系统，能够创建、调度容器，监控、管理服务器。容器是什么？容器是软件，是应用，是进程。服务器是什么？服务器是硬件，是 CPU、内存、硬盘、网卡。那么，既可以管理软件，也可以管理硬件，这样的东西应该是什么？你也许会脱口而出：这就是一个操作系统（Operating System）！没错，从某种角度来看，Kubernetes 可以说是一个集群级别的操作系统，主要功能就是资源管理和作业调度。但 Kubernetes 不是运行在单机上管理单台计算资源和进程，而是运行在多台服务器上管理几百几千台的计算资源，以及在这些资源上运行的上万上百万的进程，规模要大得多。</p>
<p>所以，你可以把 Kubernetes 与 Linux 对比起来学习，而这个新的操作系统里自然会有一系列新名词、新术语，你也需要使用新的思维方式来考虑问题，必要的时候还得和过去的习惯“说再见”。Kubernetes 这个操作系统与 Linux 还有一点区别你值得注意。Linux 的用户通常是两类人：Dev 和 Ops，而在 Kubernetes 里则只有一类人：DevOps。在以前的应用实施流程中，开发人员和运维人员分工明确，开发完成后需要编写详细的说明文档，然后交给运维去部署管理，两者之间不能随便“越线”。而在 Kubernetes 这里，开发和运维的界限变得不那么清晰了。由于云原生的兴起，开发人员从一开始就必须考虑后续的部署运维工作，而运维人员也需要在早期介入开发，才能做好应用的运维监控工作。这就会导致很多 Kubernetes 的新用户会面临身份的转变，一开始可能会有点困难。不过不用担心，这也非常正常，任何的学习过程都有个适应期，只要过了最初的概念理解阶段就好了。</p>
<h3 id="kubernetes-的基本架构">Kubernetes 的基本架构</h3>
<p>操作系统的一个重要功能就是抽象，从繁琐的底层事务中抽象出一些简洁的概念，然后基于这些概念去管理系统资源。Kubernetes 也是这样，它的管理目标是大规模的集群和应用，必须要能够把系统抽象到足够高的层次，分解出一些松耦合的对象，才能简化系统模型，减轻用户的心智负担。所以，Kubernetes 扮演的角色就如同一个“大师级别”的系统管理员，具有丰富的集群运维经验，独创了自己的一套工作方式，不需要太多的外部干预，就能够自主实现原先许多复杂的管理工作。下面我们就来看看这位资深管理员的“内功心法”。Kubernetes 官网上有一张架构图，但我觉得不是太清晰、重点不突出，所以另外找了一份（图片来源）。虽然这张图有点“老”，但对于我们初学 Kubernetes 还是比较合适的。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704"
        data-srcset="https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704, https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704 1.5x, https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704"
        title="img" /></p>
<p>Kubernetes 采用了现今流行的“控制面 / 数据面”（Control Plane / Data Plane）架构，集群里的计算机被称为“节点”（Node），可以是实机也可以是虚机，少量的节点用作控制面来执行集群的管理维护工作，其他的大部分节点都被划归数据面，用来跑业务应用。控制面的节点在 Kubernetes 里叫做 Master Node，一般简称为 Master，它是整个集群里最重要的部分，可以说是 Kubernetes 的大脑和心脏。数据面的节点叫做 Worker Node，一般就简称为 Worker 或者 Node，相当于 Kubernetes 的手和脚，在 Master 的指挥下干活。Node 的数量非常多，构成了一个资源池，Kubernetes 就在这个池里分配资源，调度应用。因为资源被“池化”了，所以管理也就变得比较简单，可以在集群中任意添加或者删除节点。在这张架构图里，我们还可以看到有一个 kubectl，它就是 Kubernetes 的客户端工具，用来操作 Kubernetes，但它位于集群之外，理论上不属于集群。你可以使用命令 kubectl get node 来查看 Kubernetes 的节点状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get node
</code></pre></td></tr></table>
</div>
</div><p>可以看到当前的 minikube 集群里只有一个 Master，那 Node 怎么不见了？这是因为 Master 和 Node 的划分不是绝对的。当集群的规模较小，工作负载较少的时候，Master 也可以承担 Node 的工作，就像我们搭建的 minikube 环境，它就只有一个节点，这个节点既是 Master 又是 Node。</p>
<h3 id="节点内部的结构">节点内部的结构</h3>
<p>Kubernetes 的节点内部也具有复杂的结构，是由很多的模块构成的，这些模块又可以分成组件（Component）和插件（Addon）两类。组件实现了 Kubernetes 的核心功能特性，没有这些组件 Kubernetes 就无法启动，而插件则是 Kubernetes 的一些附加功能，属于“锦上添花”，不安装也不会影响 Kubernetes 的正常运行。接下来我先来讲讲 Master 和 Node 里的组件，然后再捎带提一下插件，理解了它们的工作流程，你就会明白为什么 Kubernetes 有如此强大的自动化运维能力。</p>
<h3 id="master-里的组件有哪些">Master 里的组件有哪些</h3>
<p>Master 里有 4 个组件，分别是 apiserver、etcd、scheduler、controller-manager。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/33/c6/330e03a66f636657c0d8695397c508c6.jpg?wh=1278x704"
        data-srcset="https://static001.geekbang.org/resource/image/33/c6/330e03a66f636657c0d8695397c508c6.jpg?wh=1278x704, https://static001.geekbang.org/resource/image/33/c6/330e03a66f636657c0d8695397c508c6.jpg?wh=1278x704 1.5x, https://static001.geekbang.org/resource/image/33/c6/330e03a66f636657c0d8695397c508c6.jpg?wh=1278x704 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/33/c6/330e03a66f636657c0d8695397c508c6.jpg?wh=1278x704"
        title="img" /></p>
<p>apiserver 是 Master 节点——同时也是整个 Kubernetes 系统的唯一入口，它对外公开了一系列的 RESTful API，并且加上了验证、授权等功能，所有其他组件都只能和它直接通信，可以说是 Kubernetes 里的联络员。etcd 是一个高可用的分布式 Key-Value 数据库，用来持久化存储系统里的各种资源对象和状态，相当于 Kubernetes 里的配置管理员。注意它只与 apiserver 有直接联系，也就是说任何其他组件想要读写 etcd 里的数据都必须经过 apiserver。scheduler 负责容器的编排工作，检查节点的资源状态，把 Pod 调度到最适合的节点上运行，相当于部署人员。因为节点状态和 Pod 信息都存储在 etcd 里，所以 scheduler 必须通过 apiserver 才能获得。controller-manager 负责维护容器和节点等资源的状态，实现故障检测、服务迁移、应用伸缩等功能，相当于监控运维人员。同样地，它也必须通过 apiserver 获得存储在 etcd 里的信息，才能够实现对资源的各种操作。这 4 个组件也都被容器化了，运行在集群的 Pod 里，我们可以用 kubectl 来查看它们的状态，使用命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pod -n kube-system
</code></pre></td></tr></table>
</div>
</div><p>注意命令行里要用 -n kube-system 参数，表示检查“kube-system”名字空间里的 Pod，至于名字空间是什么，我们后面会讲到。</p>
<h3 id="node-里的组件有哪些">Node 里的组件有哪些</h3>
<p>Master 里的 apiserver、scheduler 等组件需要获取节点的各种信息才能够作出管理决策，那这些信息该怎么来呢？这就需要 Node 里的 3 个组件了，分别是 kubelet、kube-proxy、container-runtime。kubelet 是 Node 的代理，负责管理 Node 相关的绝大部分操作，Node 上只有它能够与 apiserver 通信，实现状态报告、命令下发、启停容器等功能，相当于是 Node 上的一个“小管家”。kube-proxy 的作用有点特别，它是 Node 的网络代理，只负责管理容器的网络通信，简单来说就是为 Pod 转发 TCP/UDP 数据包，相当于是专职的“小邮差”。第三个组件 container-runtime 我们就比较熟悉了，它是容器和镜像的实际使用者，在 kubelet 的指挥下创建容器，管理 Pod 的生命周期，是真正干活的“苦力”。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/87/35/87bab507ce8381325e85570f3bc1d935.jpg?wh=1278x704"
        data-srcset="https://static001.geekbang.org/resource/image/87/35/87bab507ce8381325e85570f3bc1d935.jpg?wh=1278x704, https://static001.geekbang.org/resource/image/87/35/87bab507ce8381325e85570f3bc1d935.jpg?wh=1278x704 1.5x, https://static001.geekbang.org/resource/image/87/35/87bab507ce8381325e85570f3bc1d935.jpg?wh=1278x704 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/87/35/87bab507ce8381325e85570f3bc1d935.jpg?wh=1278x704"
        title="img" /></p>
<p>我们一定要注意，因为 Kubernetes 的定位是容器编排平台，所以它没有限定 container-runtime 必须是 Docker，完全可以替换成任何符合标准的其他容器运行时，例如 containerd、CRI-O 等等，只不过在这里我们使用的是 Docker。这 3 个组件中只有 kube-proxy 被容器化了，而 kubelet 因为必须要管理整个节点，容器化会限制它的能力，所以它必须在 container-runtime 之外运行。使用 minikube ssh 命令登录到节点后，可以用 docker ps 看到 kube-proxy：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">minikube ssh
docker ps |grep kube-proxy
</code></pre></td></tr></table>
</div>
</div><p>而 kubelet 用 docker ps 是找不到的，需要用操作系统的 ps 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ps -ef|grep kubelet
</code></pre></td></tr></table>
</div>
</div><p>现在，我们再把 Node 里的组件和 Master 里的组件放在一起来看，就能够明白 Kubernetes 的大致工作流程了：每个 Node 上的 kubelet 会定期向 apiserver 上报节点状态，apiserver 再存到 etcd 里。每个 Node 上的 kube-proxy 实现了 TCP/UDP 反向代理，让容器对外提供稳定的服务。scheduler 通过 apiserver 得到当前的节点状态，调度 Pod，然后 apiserver 下发命令给某个 Node 的 kubelet，kubelet 调用 container-runtime 启动容器。controller-manager 也通过 apiserver 得到实时的节点状态，监控可能的异常情况，再使用相应的手段去调节恢复。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704"
        data-srcset="https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704, https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704 1.5x, https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/34/b7/344e0c6dc2141b12f99e61252110f6b7.png?wh=1278x704"
        title="img" /></p>
<p>其实，这和我们在 Kubernetes 出现之前的操作流程也差不了多少，但 Kubernetes 的高明之处就在于把这些都抽象化规范化了。于是，这些组件就好像是无数个不知疲倦的运维工程师，把原先繁琐低效的人力工作搬进了高效的计算机里，就能够随时发现集群里的变化和异常，再互相协作，维护集群的健康状态。</p>
<h3 id="插件addons有哪些">插件（Addons）有哪些</h3>
<p>只要服务器节点上运行了 apiserver、scheduler、kubelet、kube-proxy、container-runtime 等组件，就可以说是一个功能齐全的 Kubernetes 集群了。不过就像 Linux 一样，操作系统提供的基础功能虽然“可用”，但想达到“好用”的程度，还是要再安装一些附加功能，这在 Kubernetes 里就是插件（Addon）。由于 Kubernetes 本身的设计非常灵活，所以就有大量的插件用来扩展、增强它对应用和集群的管理能力。minikube 也支持很多的插件，使用命令 minikube addons list 就可以查看插件列表：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">minikube addons list
</code></pre></td></tr></table>
</div>
</div><p>插件中我个人认为比较重要的有两个：DNS 和 Dashboard。DNS 你应该比较熟悉吧，它在 Kubernetes 集群里实现了域名解析服务，能够让我们以域名而不是 IP 地址的方式来互相通信，是服务发现和负载均衡的基础。由于它对微服务、服务网格等架构至关重要，所以基本上是 Kubernetes 的必备插件。Dashboard 就是仪表盘，为 Kubernetes 提供了一个图形化的操作界面，非常直观友好，虽然大多数 Kubernetes 工作都是使用命令行 kubectl，但有的时候在 Dashboard 上查看信息也是挺方便的。你只要在 minikube 环境里执行一条简单的命令，就可以自动用浏览器打开 Dashboard 页面，而且还支持中文：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">minikube dashboard
</code></pre></td></tr></table>
</div>
</div><h3 id="小结-9">小结</h3>
<p>好了，今天我们一起来研究了 Kubernetes 的内部架构和工作机制，可以看到它的功能非常完善，实现了大部分常见的运维管理工作，而且是全自动化的，能够节约大量的人力成本。由于 Kubernetes 的抽象程度比较高，有很多陌生的新术语，不太好理解，所以我画了一张思维导图，你可以对照着再加深理解。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/65/e1/65d38ac50b4f2f1fd4b6700d5b8e7be1.jpg?wh=1920x1096"
        data-srcset="https://static001.geekbang.org/resource/image/65/e1/65d38ac50b4f2f1fd4b6700d5b8e7be1.jpg?wh=1920x1096, https://static001.geekbang.org/resource/image/65/e1/65d38ac50b4f2f1fd4b6700d5b8e7be1.jpg?wh=1920x1096 1.5x, https://static001.geekbang.org/resource/image/65/e1/65d38ac50b4f2f1fd4b6700d5b8e7be1.jpg?wh=1920x1096 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/65/e1/65d38ac50b4f2f1fd4b6700d5b8e7be1.jpg?wh=1920x1096"
        title="img" /></p>
<p>最后小结一下今天的要点：Kubernetes 能够在集群级别管理应用和服务器，可以认为是一种集群操作系统。它使用“控制面 / 数据面”的基本架构，Master 节点实现管理控制功能，Worker 节点运行具体业务。Kubernetes 由很多模块组成，可分为核心的组件和选配的插件两类。Master 里有 4 个组件，分别是 apiserver、etcd、scheduler、controller-manager。Node 里有 3 个组件，分别是 kubelet、kube-proxy、container-runtime。通常必备的插件有 DNS 和 Dashboard。</p>
<h2 id="加餐kubernetes弃用docker是怎么回事">加餐｜Kubernetes“弃用Docker”是怎么回事？</h2>
<p>在“入门篇”学习容器技术的过程中，我看到有不少同学留言问 Kubernetes“弃用 Docker”的事情，担心现在学 Docker 是否还有价值，是否现在就应该切换到 containerd 或者是其他 runtime。这些疑虑的确是有些道理。两年前，Kubernetes 放出消息要“弃用 Docker”的时候，确确实实在 Kubernetes 社区里掀起了一场“轩然大波”，影响甚至波及到社区之外，也导致 Kubernetes 不得不写了好几篇博客来反复解释这么做的原因。两年过去了，虽然最新的 Kubernetes 1.24 已经达成了“弃用”的目标，但很多人对这件事似乎还是没有非常清晰的认识。所以今天，我们就来聊聊这个话题，我也讲讲我的一些看法。</p>
<h3 id="什么是-cri">什么是 CRI</h3>
<p>要了解 Kubernetes 为什么要“弃用 Docker”，还得追根溯源，回头去看 Kubernetes 的发展历史。2014 年，Docker 正如日中天，在容器领域没有任何对手，而这时 Kubernetes 才刚刚诞生，虽然背后有 Google 和 Borg 的支持，但还是比较弱小的。所以，Kubernetes 很自然就选择了在 Docker 上运行，毕竟“背靠大树好乘凉”，同时也能趁机“养精蓄锐”逐步发展壮大自己。时间一转眼到了 2016 年，CNCF 已经成立一年了，而 Kubernetes 也已经发布了 1.0 版，可以正式用于生产环境，这些都标志着 Kubernetes 已经成长起来了，不再需要“看脸色吃饭”。于是它就宣布加入了 CNCF，成为了第一个 CNCF 托管项目，想要借助基金会的力量联合其他厂商，一起来“扳倒”Docker。那它是怎么做的呢？</p>
<p>在 2016 年底的 1.5 版里，Kubernetes 引入了一个新的接口标准：CRI ，Container Runtime Interface。CRI 采用了 ProtoBuffer 和 gPRC，规定 kubelet 该如何调用容器运行时去管理容器和镜像，但这是一套全新的接口，和之前的 Docker 调用完全不兼容。Kubernetes 意思很明显，就是不想再绑定在 Docker 上了，允许在底层接入其他容器技术（比如 rkt、kata 等），随时可以把 Docker“踢开”。但是这个时候 Docker 已经非常成熟，而且市场的惯性也非常强大，各大云厂商不可能一下子就把 Docker 全部替换掉。所以 Kubernetes 也只能同时提供一个“折中”方案，在 kubelet 和 Docker 中间加入一个“适配器”，把 Docker 的接口转换成符合 CRI 标准的接口（图片来源）：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/11/ef/11e3de04b296248711455f22ce5578ef.png?wh=572x136"
        data-srcset="https://static001.geekbang.org/resource/image/11/ef/11e3de04b296248711455f22ce5578ef.png?wh=572x136, https://static001.geekbang.org/resource/image/11/ef/11e3de04b296248711455f22ce5578ef.png?wh=572x136 1.5x, https://static001.geekbang.org/resource/image/11/ef/11e3de04b296248711455f22ce5578ef.png?wh=572x136 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/11/ef/11e3de04b296248711455f22ce5578ef.png?wh=572x136"
        title="img" /></p>
<p>因为这个“适配器”夹在 kubelet 和 Docker 之间，所以就被形象地称为是“shim”，也就是“垫片”的意思。有了 CRI 和 shim，虽然 Kubernetes 还使用 Docker 作为底层运行时，但也具备了和 Docker 解耦的条件，从此就拉开了“弃用 Docker”这场大戏的帷幕。</p>
<h3 id="什么是-containerd">什么是 containerd</h3>
<p>面对 Kubernetes“咄咄逼人”的架势，Docker 是看在眼里痛在心里，虽然有苦心经营了多年的社区和用户群，但公司的体量太小，实在是没有足够的实力与大公司相抗衡。不过 Docker 也没有“坐以待毙”，而是采取了“断臂求生”的策略，推动自身的重构，把原本单体架构的 Docker Engine 拆分成了多个模块，其中的 Docker daemon 部分就捐献给了 CNCF，形成了 containerd。containerd 作为 CNCF 的托管项目，自然是要符合 CRI 标准的。但 Docker 出于自己诸多原因的考虑，它只是在 Docker Engine 里调用了 containerd，外部的接口仍然保持不变，也就是说还不与 CRI 兼容。</p>
<p>由于 Docker 的“固执己见”，这时 Kubernetes 里就出现了两种调用链：第一种是用 CRI 接口调用 dockershim，然后 dockershim 调用 Docker，Docker 再走 containerd 去操作容器。第二种是用 CRI 接口直接调用 containerd 去操作容器。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a8/9b/a8abfe5a55d0fa8b383867cc6062089b.png?wh=1920x627"
        data-srcset="https://static001.geekbang.org/resource/image/a8/9b/a8abfe5a55d0fa8b383867cc6062089b.png?wh=1920x627, https://static001.geekbang.org/resource/image/a8/9b/a8abfe5a55d0fa8b383867cc6062089b.png?wh=1920x627 1.5x, https://static001.geekbang.org/resource/image/a8/9b/a8abfe5a55d0fa8b383867cc6062089b.png?wh=1920x627 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a8/9b/a8abfe5a55d0fa8b383867cc6062089b.png?wh=1920x627"
        title="img" /></p>
<p>显然，由于都是用 containerd 来管理容器，所以这两种调用链的最终效果是完全一样的，但是第二种方式省去了 dockershim 和 Docker Engine 两个环节，更加简洁明了，损耗更少，性能也会提升一些。在 2018 年 Kubernetes 1.10 发布的时候，containerd 也更新到了 1.1 版，正式与 Kubernetes 集成，同时还发表了一篇博客文章（https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/），展示了一些性能测试数据：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6f/e9/6fd065d916e5815e044c10738746ace9.jpg?wh=1784x591"
        data-srcset="https://static001.geekbang.org/resource/image/6f/e9/6fd065d916e5815e044c10738746ace9.jpg?wh=1784x591, https://static001.geekbang.org/resource/image/6f/e9/6fd065d916e5815e044c10738746ace9.jpg?wh=1784x591 1.5x, https://static001.geekbang.org/resource/image/6f/e9/6fd065d916e5815e044c10738746ace9.jpg?wh=1784x591 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6f/e9/6fd065d916e5815e044c10738746ace9.jpg?wh=1784x591"
        title="img" /></p>
<p>从这些数据可以看到，containerd1.1 相比当时的 Docker 18.03，Pod 的启动延迟降低了大约 20%，CPU 使用率降低了 68%，内存使用率降低了 12%，这是一个相当大的性能改善，对于云厂商非常有诱惑力。</p>
<h3 id="正式弃用-docker">正式“弃用 Docker”</h3>
<p>有了 CRI 和 containerd 这两件强大的武器，胜利的天平已经明显向 Kubernetes 倾斜了。又是两年之后，到了 2020 年，Kubernetes 1.20 终于正式向 Docker“宣战”：kubelet 将弃用 Docker 支持，并会在未来的版本中彻底删除。但由于 Docker 几乎成为了容器技术的代名词，而且 Kubernetes 也已经使用 Docker 很多年，这个声明在不断传播的过程中很快就“变味”了，“kubelet 将弃用 Docker 支持”被简化成了更吸引眼球的“Kubernetes 将弃用 Docker”。</p>
<p>这自然就在 IT 界引起了恐慌，“不明真相的广大群众”纷纷表示震惊：用了这么久的 Docker 突然就不能用了，Kubernetes 为什么要如此对待 Docker？之前在 Docker 上的投入会不会就全归零了？现有的大量镜像该怎么办？其实，如果你理解了前面讲的 CRI 和 containerd 这两个项目，就会知道 Kubernetes 的这个举动也没有什么值得大惊小怪的，一切都是“水到渠成”的：<strong>它实际上只是“弃用了 dockershim”这个小组件，也就是说把 dockershim 移出了 kubelet，并不是“弃用了 Docker”这个软件产品</strong>。所以，“弃用 Docker”对 Kubernetes 和 Docker 来说都不会有什么太大的影响，因为他们两个都早已经把下层都改成了开源的 containerd，原来的 Docker 镜像和容器仍然会正常运行，唯一的变化就是 Kubernetes 绕过了 Docker，直接调用 Docker 内部的 containerd 而已。这个关系你可以参考下面的这张图来理解：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/97/e8/970a234bd610b55340505dac74b026e8.png?wh=740x680"
        data-srcset="https://static001.geekbang.org/resource/image/97/e8/970a234bd610b55340505dac74b026e8.png?wh=740x680, https://static001.geekbang.org/resource/image/97/e8/970a234bd610b55340505dac74b026e8.png?wh=740x680 1.5x, https://static001.geekbang.org/resource/image/97/e8/970a234bd610b55340505dac74b026e8.png?wh=740x680 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/97/e8/970a234bd610b55340505dac74b026e8.png?wh=740x680"
        title="img" /></p>
<p>当然，影响也不是完全没有。如果 Kubernetes 直接使用 containerd 来操纵容器，那么它就是一个与 Docker 独立的工作环境，彼此都不能访问对方管理的容器和镜像。换句话说，使用命令 docker ps 就看不到在 Kubernetes 里运行的容器了。这对有的人来说可能需要稍微习惯一下，改用新的工具 crictl，不过用来查看容器、镜像的子命令还是一样的，比如 ps、images 等等，适应起来难度不大（但如果我们一直用 kubectl 来管理 Kubernetes 的话，这就是没有任何影响了）。“宣战”之后，Kubernetes 原本打算用一年的时间完成“弃用 Docker”的工作，但它也确实低估了 Docker 的根基，到了 1.23 版还是没能移除 dockershim，不得已又往后推迟了半年，终于在今年 5 月份发布的 1.24 版把 dockershim 的代码从 kubelet 里删掉了。自此，Kubernetes 彻底和 Docker“分道扬镳”，今后就是“大路朝天，各走一边”。</p>
<h3 id="docker-的未来">Docker 的未来</h3>
<p>那么，Docker 的未来会是怎么样的呢？难道云原生时代就没有它的立足之地了吗？这个问题的答案很显然是否定的。作为容器技术的初创者，Docker 的历史地位无人能够质疑，虽然现在 Kubernetes 不再默认绑定 Docker，但 Docker 还是能够以其他的形式与 Kubernetes 共存的。首先，因为容器镜像格式已经被标准化了（OCI 规范，Open Container Initiative），Docker 镜像仍然可以在 Kubernetes 里正常使用，原来的开发测试、CI/CD 流程都不需要改动，我们仍然可以拉取 Docker Hub 上的镜像，或者编写 Dockerfile 来打包应用。其次，Docker 是一个完整的软件产品线，不止是 containerd，它还包括了镜像构建、分发、测试等许多服务，甚至在 Docker Desktop 里还内置了 Kubernetes。单就容器开发的便利性来讲，Docker 还是暂时难以被替代的，广大云原生开发者可以在这个熟悉的环境里继续工作，利用 Docker 来开发运行在 Kubernetes 里的应用。</p>
<p>再次，虽然 Kubernetes 已经不再包含 dockershim，但 Docker 公司却把这部分代码接管了过来，另建了一个叫 cri-dockerd（https://github.com/mirantis/cri-dockerd）的项目，作用也是一样的，把 Docker Engine 适配成 CRI 接口，这样 kubelet 就又可以通过它来操作 Docker 了，就仿佛是一切从未发生过。综合来看，Docker 虽然在容器编排战争里落败，被 Kubernetes 排挤到了角落，但它仍然具有强韧的生命力，多年来积累的众多忠实用户和数量庞大的应用镜像是它的最大资本和后盾，足以支持它在另一条不与 Kubernetes 正面交锋的道路上走下去。而对于我们这些初学者来说，Docker 方便易用，具有完善的工具链和友好的交互界面，市面上很难找到能够与它媲美的软件了，应该说是入门学习容器技术和云原生的“不二之选”。至于 Kubernetes 底层用的什么，我们又何必太过于执着和关心呢？</p>
<h2 id="11yamlkubernetes世界里的通用语">11｜YAML：Kubernetes世界里的通用语</h2>
<p>在上次课里，我们一起研究了 Kubernetes 的内部架构和组成，知道它分为控制面和数据面。控制面管理集群，数据面跑业务应用，节点内部又有 apiserver、etcd、scheduler、kubelet、kube-proxy 等组件，它们互相协作来维护整个集群的稳定运行。这套独特的 Master/Node 架构是 Kubernetes 得以安身立命的根本，但仅依靠这套“内功心法”是不是就能够随意仗剑走天涯了呢？显然不行。就像许多武侠、玄幻作品里的人物一样，Kubernetes 也需要一份“招式秘籍”才能把自己的“内功”完全发挥出来，只有内外兼修才能够达到笑傲江湖的境界。而这份“招式秘籍”，就是 Kubernetes 世界里的标准工作语言 YAML，所以今天，我就来讲讲为什么要有 YAML、它是个什么样子、该怎么使用。</p>
<h3 id="声明式与命令式是怎么回事">声明式与命令式是怎么回事</h3>
<p>Kubernetes 使用的 YAML 语言有一个非常关键的特性，叫“声明式”（Declarative），对应的有另外一个词：“命令式”（Imperative）。所以在详细了解 YAML 之前，我们得先来看看“声明式”与“命令式”这两种工作方式，它们在计算机世界里的关系有点像小说里的“剑宗”与“气宗”。我们在入门篇里学习的 Docker 命令和 Dockerfile 就属于“命令式”，大多数编程语言也属于命令式，它的特点是交互性强，注重顺序和过程，你必须“告诉”计算机每步该做什么，所有的步骤都列清楚，这样程序才能够一步步走下去，最后完成任务，显得计算机有点“笨”。“声明式”，在 Kubernetes 出现之前比较少见，它与“命令式”完全相反，不关心具体的过程，更注重结果。我们不需要“教”计算机该怎么做，只要告诉它一个目标状态，它自己就会想办法去完成任务，相比起来自动化、智能化程度更高。这两个概念比较抽象，不太好理解，也是 Kubernetes 初学者经常遇到的障碍之一。Kubernetes 官网上特意以空调为例，解说“声明式”的原理，但我感觉还是没有说得太清楚，所以这里我就再以“打车”来形象地解释一下“命令式”和“声明式”的区别。</p>
<p>假设你要打车去高铁站，但司机不熟悉路况，你就只好不厌其烦地告诉他该走哪条路、在哪个路口转向、在哪里进出主路、停哪个站口。虽然最后到达了目的地，但这一路上也费了很多口舌，发出了无数的“命令”。很显然，这段路程就属于“命令式”。现在我们来换一种方式，同样是去高铁站，但司机经验丰富，他知道哪里有拥堵、哪条路的红绿灯多、哪段路有临时管控、哪里可以抄小道，此时你再多嘴无疑会干扰他的正常驾驶，所以，你只要给他一个“声明”：我要去高铁站，接下来就可以舒舒服服地躺在后座上休息，顺利到达目的地了。在这个“打车”的例子里，Kubernetes 就是这样的一位熟练的司机，Master/Node 架构让它对整个集群的状态了如指掌，内部的众多组件和插件也能够自动监控管理应用。这个时候我们再用“命令式”跟它打交道就不太合适了，因为它知道的信息比我们更多更全面，不需要我们这个外行去指导它这个内行，所以我们最好是做一个“甩手掌柜”，用“声明式”把任务的目标告诉它，比如使用哪个镜像、什么时候运行，让它自己去处理执行过程中的细节。那么，该用什么方式去给 Kubernetes 发出一个“声明”呢？容器技术里的 Shell 脚本和 Dockerfile 可以很好地描述“命令式”，但对于“声明式”就不太合适了，这个时候，我们需要使用专门的 YAML 语言。</p>
<h3 id="什么是-yaml">什么是 YAML</h3>
<p>YAML 语言创建于 2001 年，比 XML 晚了三年。XML 你应该知道吧，它是一种类似 HTML 的标签式语言，有很多繁文缛节。而 YAML 虽然在名字上模仿了 XML，但实质上与 XML 完全不同，更适合人类阅读，计算机解析起来也很容易。YAML 的官网（https://yaml.org/）有对语言规范的完整介绍，所以我就不在这里列举语言的细节了，只讲一些与 Kubernetes 相关的要点，帮助你快速掌握。你需要知道，YAML 是 JSON 的超集，支持整数、浮点数、布尔、字符串、数组和对象等数据类型。也就是说，任何合法的 JSON 文档也都是 YAML 文档，如果你了解 JSON，那么学习 YAML 会容易很多。但和 JSON 比起来，YAML 的语法更简单，形式也更清晰紧凑，比如：</p>
<p>使用空白与缩进表示层次（有点类似 Python），可以不使用花括号和方括号。可以使用 # 书写注释，比起 JSON 是很大的改进。对象（字典）的格式与 JSON 基本相同，但 Key 不需要使用双引号。数组（列表）是使用 - 开头的清单形式（有点类似 MarkDown）。表示对象的 : 和表示数组的 - 后面都必须要有空格。可以使用 &mdash; 在一个文件里分隔多个 YAML 对象。</p>
<p>下面我们来看几个 YAML 的简单示例。首先是数组，它使用 - 列出了三种操作系统：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># YAML数组(列表)
OS:
  - linux
  - macOS
  - Windows
</code></pre></td></tr></table>
</div>
</div><p>这段 YAML 对应的 JSON 如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{
  &#34;OS&#34;: [&#34;linux&#34;, &#34;macOS&#34;, &#34;Windows&#34;]
}
</code></pre></td></tr></table>
</div>
</div><p>对比可以看到 YAML 形式上很简单，没有闭合花括号、方括号的麻烦，每个元素后面也不需要逗号。再来看一个 YAML 对象，声明了 1 个 Master 节点，3 个 Worker 节点：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># YAML对象(字典)
Kubernetes:
  master: 1
  worker: 3
</code></pre></td></tr></table>
</div>
</div><p>它等价的 JSON 如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{
  &#34;Kubernetes&#34;: {
    &#34;master&#34;: 1,
    &#34;worker&#34;: 3
  }
}
</code></pre></td></tr></table>
</div>
</div><p>注意到了吗 YAML 里的 Key 都不需要使用双引号，看起来更舒服。把 YAML 的数组、对象组合起来，我们就可以描述出任意的 Kubernetes 资源对象，第三个例子略微复杂点，你可以自己尝试着解释一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># 复杂的例子，组合数组和对象
Kubernetes:
  master:
    - apiserver: running
    - etcd: running
  node:
    - kubelet: running
    - kube-proxy: down
    - container-runtime: [docker, containerd, cri-o]

</code></pre></td></tr></table>
</div>
</div><p>关于 YAML 语言的其他知识点我就不再一一细说了，都整理在了这张图里，你可以参考YAML 官网，在今后的课程中慢慢体会。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d4/04/d4f3d4cc27a8a4a70d4898b41efebf04.jpg?wh=1920x2030"
        data-srcset="https://static001.geekbang.org/resource/image/d4/04/d4f3d4cc27a8a4a70d4898b41efebf04.jpg?wh=1920x2030, https://static001.geekbang.org/resource/image/d4/04/d4f3d4cc27a8a4a70d4898b41efebf04.jpg?wh=1920x2030 1.5x, https://static001.geekbang.org/resource/image/d4/04/d4f3d4cc27a8a4a70d4898b41efebf04.jpg?wh=1920x2030 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d4/04/d4f3d4cc27a8a4a70d4898b41efebf04.jpg?wh=1920x2030"
        title="img" /></p>
<h3 id="什么是-api-对象">什么是 API 对象</h3>
<p>学到这里还不够，因为 YAML 语言只相当于“语法”，要与 Kubernetes 对话，我们还必须有足够的“词汇”来表示“语义”。那么应该声明 Kubernetes 里的哪些东西，才能够让 Kubernetes 明白我们的意思呢？作为一个集群操作系统，Kubernetes 归纳总结了 Google 多年的经验，在理论层面抽象出了很多个概念，用来描述系统的管理运维工作，这些概念就叫做“API 对象”。说到这个名字，你也许会联想到上次课里讲到的 Kubernetes 组件 apiserver。没错，它正是来源于此。因为 apiserver 是 Kubernetes 系统的唯一入口，外部用户和内部组件都必须和它通信，而它采用了 HTTP 协议的 URL 资源理念，API 风格也用 RESTful 的 GET/POST/DELETE 等等，所以，这些概念很自然地就被称为是“API 对象”了。那都有哪些 API 对象呢？你可以使用 kubectl api-resources 来查看当前 Kubernetes 版本支持的所有对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl api-resources
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b2/91/b259e8bfbd0d15b796228d92ede42a91.png?wh=1920x701"
        data-srcset="https://static001.geekbang.org/resource/image/b2/91/b259e8bfbd0d15b796228d92ede42a91.png?wh=1920x701, https://static001.geekbang.org/resource/image/b2/91/b259e8bfbd0d15b796228d92ede42a91.png?wh=1920x701 1.5x, https://static001.geekbang.org/resource/image/b2/91/b259e8bfbd0d15b796228d92ede42a91.png?wh=1920x701 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b2/91/b259e8bfbd0d15b796228d92ede42a91.png?wh=1920x701"
        title="img" /></p>
<p>在输出的“NAME”一栏，就是对象的名字，比如 ConfigMap、Pod、Service 等等，第二栏“SHORTNAMES”则是这种资源的简写，在我们使用 kubectl 命令的时候很有用，可以少敲几次键盘，比如 Pod 可以简写成 po，Service 可以简写成 svc。在使用 kubectl 命令的时候，你还可以加上一个参数 &ndash;v=9，它会显示出详细的命令执行过程，清楚地看到发出的 HTTP 请求，比如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pod --v=9
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3f/31/3fe4823f6ba10600e63c197487e84931.png?wh=1920x571"
        data-srcset="https://static001.geekbang.org/resource/image/3f/31/3fe4823f6ba10600e63c197487e84931.png?wh=1920x571, https://static001.geekbang.org/resource/image/3f/31/3fe4823f6ba10600e63c197487e84931.png?wh=1920x571 1.5x, https://static001.geekbang.org/resource/image/3f/31/3fe4823f6ba10600e63c197487e84931.png?wh=1920x571 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3f/31/3fe4823f6ba10600e63c197487e84931.png?wh=1920x571"
        title="img" /></p>
<p>从截图里可以看到，kubectl 客户端等价于调用了 curl，向 8443 端口发送了 HTTP GET 请求，URL 是 /api/v1/namespaces/default/pods。目前的 Kubernetes 1.23 版本有 50 多种 API 对象，全面地描述了集群的节点、应用、配置、服务、账号等等信息，apiserver 会把它们都存储在数据库 etcd 里，然后 kubelet、scheduler、controller-manager 等组件通过 apiserver 来操作它们，就在 API 对象这个抽象层次实现了对整个集群的管理。</p>
<h3 id="如何描述-api-对象">如何描述 API 对象</h3>
<p>现在我们就来看看如何以 YAML 语言，使用“声明式”在 Kubernetes 里描述并创建 API 对象。之前我们运行 Nginx 的命令你还记得吗？使用的是 kubectl run，和 Docker 一样是“命令式”的：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl run ngx --image=nginx:alpine
</code></pre></td></tr></table>
</div>
</div><p>我们来把它改写成“声明式”的 YAML，说清楚我们想要的 Nginx 应用是个什么样子，也就是“目标状态”，让 Kubernetes 自己去决定如何拉取镜像运行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: ngx-pod
  labels:
    env: demo
    owner: chrono

spec:
  containers:
  - image: nginx:alpine
    name: ngx
    ports:
    - containerPort: 80

</code></pre></td></tr></table>
</div>
</div><p>有了刚才 YAML 语言知识“打底”，相信你基本上能够把它看明白，知道它是一个 Pod，要使用 nginx:alpine 镜像创建一个容器，开放端口 80，而其他的部分，就是 Kubernetes 对 API 对象强制的格式要求了。因为 API 对象采用标准的 HTTP 协议，为了方便理解，我们可以借鉴一下 HTTP 的报文格式，把 API 对象的描述分成“header”和“body”两部分。“header”包含的是 API 对象的基本信息，有三个字段：apiVersion、kind、metadata。</p>
<p>apiVersion 表示操作这种资源的 API 版本号，由于 Kubernetes 的迭代速度很快，不同的版本创建的对象会有差异，为了区分这些版本就需要使用 apiVersion 这个字段，比如 v1、v1alpha1、v1beta1 等等。kind 表示资源对象的类型，这个应该很好理解，比如 Pod、Node、Job、Service 等等。metadata 这个字段顾名思义，表示的是资源的一些“元信息”，也就是用来标记对象，方便 Kubernetes 管理的一些信息。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: ngx-pod
  labels:
    env: demo
    owner: chrono
</code></pre></td></tr></table>
</div>
</div><p>比如在这个 YAML 示例里就有两个“元信息”，一个是 name，给 Pod 起了个名字叫 ngx-pod，另一个是 labels，给 Pod“贴”上了一些便于查找的标签，分别是 env 和 owner。apiVersion、kind、metadata 都被 kubectl 用于生成 HTTP 请求发给 apiserver，你可以用 &ndash;v=9 参数在请求的 URL 里看到它们，比如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">https://192.168.49.2:8443/api/v1/namespaces/default/pods/ngx-pod
</code></pre></td></tr></table>
</div>
</div><p>和 HTTP 协议一样，“header”里的 apiVersion、kind、metadata 这三个字段是任何对象都必须有的，而“body”部分则会与对象特定相关，每种对象会有不同的规格定义，在 YAML 里就表现为 spec 字段（即 specification），表示我们对对象的“期望状态”（desired status）。还是来看这个 Pod，它的 spec 里就是一个 containers 数组，里面的每个元素又是一个对象，指定了名字、镜像、端口等信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">spec:
  containers:
  - image: nginx:alpine
    name: ngx
    ports:
    - containerPort: 80
</code></pre></td></tr></table>
</div>
</div><p>现在把这些字段综合起来，我们就能够看出，这份 YAML 文档完整地描述了一个类型是 Pod 的 API 对象，要求使用 v1 版本的 API 接口去管理，其他更具体的名称、标签、状态等细节都记录在了 metadata 和 spec 字段等里。使用 kubectl apply、kubectl delete，再加上参数 -f，你就可以使用这个 YAML 文件，创建或者删除对象了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f ngx-pod.yml
kubectl delete -f ngx-pod.yml
</code></pre></td></tr></table>
</div>
</div><p>Kubernetes 收到这份“声明式”的数据，再根据 HTTP 请求里的 POST/DELETE 等方法，就会自动操作这个资源对象，至于对象在哪个节点上、怎么创建、怎么删除完全不用我们操心。</p>
<h3 id="如何编写-yaml">如何编写 YAML</h3>
<p>讲到这里，相信你对如何使用 YAML 与 Kubernetes 沟通应该大概了解了，不过疑问也会随之而来：这么多 API 对象，我们怎么知道该用什么 apiVersion、什么 kind？metadata、spec 里又该写哪些字段呢？还有，YAML 看起来简单，写起来却比较麻烦，缩进对齐很容易搞错，有没有什么简单的方法呢？这些问题最权威的答案无疑是 Kubernetes 的官方参考文档（https://kubernetes.io/docs/reference/kubernetes-api/），API 对象的所有字段都可以在里面找到。不过官方文档内容太多太细，查阅起来有些费劲，所以下面我就介绍几个简单实用的小技巧。</p>
<p>第一个技巧其实前面已经说过了，就是 kubectl api-resources 命令，它会显示出资源对象相应的 API 版本和类型，比如 Pod 的版本是“v1”，Ingress 的版本是“networking.k8s.io/v1”，照着它写绝对不会错。第二个技巧，是命令 kubectl explain，它相当于是 Kubernetes 自带的 API 文档，会给出对象字段的详细说明，这样我们就不必去网上查找了。比如想要看 Pod 里的字段该怎么写，就可以这样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl explain pod
kubectl explain pod.metadata
kubectl explain pod.spec
kubectl explain pod.spec.containers
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/53/85/53cf783195be896e7632c1fc6bd24185.png?wh=1920x1068"
        data-srcset="https://static001.geekbang.org/resource/image/53/85/53cf783195be896e7632c1fc6bd24185.png?wh=1920x1068, https://static001.geekbang.org/resource/image/53/85/53cf783195be896e7632c1fc6bd24185.png?wh=1920x1068 1.5x, https://static001.geekbang.org/resource/image/53/85/53cf783195be896e7632c1fc6bd24185.png?wh=1920x1068 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/53/85/53cf783195be896e7632c1fc6bd24185.png?wh=1920x1068"
        title="img" /></p>
<p>使用前两个技巧编写 YAML 就基本上没有难度了。不过我们还可以让 kubectl 为我们“代劳”，生成一份“文档样板”，免去我们打字和对齐格式的工作。这第三个技巧就是 kubectl 的两个特殊参数 &ndash;dry-run=client 和 -o yaml，前者是空运行，后者是生成 YAML 格式，结合起来使用就会让 kubectl 不会有实际的创建动作，而只生成 YAML 文件。例如，想要生成一个 Pod 的 YAML 样板示例，可以在 kubectl run 后面加上这两个参数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl run ngx --image=nginx:alpine --dry-run=client -o yaml
</code></pre></td></tr></table>
</div>
</div><p>就会生成一个绝对正确的 YAML 文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: ngx
  name: ngx
spec:
  containers:
  - image: nginx:alpine
    name: ngx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
</code></pre></td></tr></table>
</div>
</div><p>接下来你要做的，就是查阅对象的说明文档，添加或者删除字段来定制这个 YAML 了。这个小技巧还可以再进化一下，把这段参数定义成 Shell 变量（名字任意，比如$do/$go，这里用的是$out），用起来会更省事，比如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;
kubectl run ngx --image=nginx:alpine $out
</code></pre></td></tr></table>
</div>
</div><p>今后除了一些特殊情况，我们都不会再使用 kubectl run 这样的命令去直接创建 Pod，而是会编写 YAML，用“声明式”来描述对象，再用 kubectl apply 去发布 YAML 来创建对象。</p>
<h3 id="小结-10">小结</h3>
<p>好了，今天就到这里，我们一起学习了“声明式”和“命令式”的区别、YAML 语言的语法、如何用 YAML 来描述 API 对象，还有一些编写 YAML 文件的技巧。Kubernetes 采用 YAML 作为工作语言是它有别与其他系统的一大特色，声明式的语言能够更准确更清晰地描述系统状态，避免引入繁琐的操作步骤扰乱系统，与 Kubernetes 高度自动化的内部结构相得益彰，而且纯文本形式的 YAML 也很容易版本化，适合 CI/CD。再小结一下今天的内容要点：YAML 是 JSON 的超集，支持数组和对象，能够描述复杂的状态，可读性也很好。Kubernetes 把集群里的一切资源都定义为 API 对象，通过 RESTful 接口来管理。描述 API 对象需要使用 YAML 语言，必须的字段是 apiVersion、kind、metadata。命令 kubectl api-resources 可以查看对象的 apiVersion 和 kind，命令 kubectl explain 可以查看对象字段的说明文档。命令 kubectl apply、kubectl delete 发送 HTTP 请求，管理 API 对象。使用参数 &ndash;dry-run=client -o yaml 可以生成对象的 YAML 模板，简化编写工作。</p>
<h2 id="12pod如何理解这个kubernetes里最核心的概念">12｜Pod：如何理解这个Kubernetes里最核心的概念？</h2>
<p>前两天我们学习了 Kubernetes 世界里的工作语言 YAML，还编写了一个简短的 YAML 文件，描述了一个 API 对象：Pod，它在 spec 字段里包含了容器的定义。那么为什么 Kubernetes 不直接使用已经非常成熟稳定的容器？为什么要再单独抽象出一个 Pod 对象？为什么几乎所有人都说 Pod 是 Kubernetes 里最核心最基本的概念呢？今天我就来逐一解答这些问题，希望你学完今天的这次课，心里面能够有明确的答案。</p>
<h3 id="为什么要有-pod">为什么要有 Pod</h3>
<p>Pod 这个词原意是“豌豆荚”，后来又延伸出“舱室”“太空舱”等含义，你可以看一下这张图片，形象地来说 Pod 就是包含了很多组件、成员的一种结构。</p>
<p>容器技术我想你现在已经比较熟悉了，它让进程在一个“沙盒”环境里运行，具有良好的隔离性，对应用是一个非常好的封装。不过，当容器技术进入到现实的生产环境中时，这种隔离性就带来了一些麻烦。因为很少有应用是完全独立运行的，经常需要几个进程互相协作才能完成任务，比如在“入门篇”里我们搭建 WordPress 网站的时候，就需要 Nginx、WordPress、MariaDB 三个容器一起工作。WordPress 例子里的这三个应用之间的关系还是比较松散的，它们可以分别调度，运行在不同的机器上也能够以 IP 地址通信。但还有一些特殊情况，多个应用结合得非常紧密以至于无法把它们拆开。比如，有的应用运行前需要其他应用帮它初始化一些配置，还有就是日志代理，它必须读取另一个应用存储在本地磁盘的文件再转发出去。这些应用如果被强制分离成两个容器，切断联系，就无法正常工作了。那么把这些应用都放在一个容器里运行可不可以呢？当然可以，但这并不是一种好的做法。因为容器的理念是对应用的独立封装，它里面就应该是一个进程、一个应用，如果里面有多个应用，不仅违背了容器的初衷，也会让容器更难以管理。为了解决这样多应用联合运行的问题，同时还要不破坏容器的隔离，就需要在容器外面再建立一个“收纳舱”，让多个容器既保持相对独立，又能够小范围共享网络、存储等资源，而且永远是“绑在一起”的状态。所以，Pod 的概念也就呼之欲出了，容器正是“豆荚”里那些小小的“豌豆”，你可以在 Pod 的 YAML 里看到，“spec.containers”字段其实是一个数组，里面允许定义多个容器。如果再拿之前讲过的“小板房”来比喻的话，Pod 就是由客厅、卧室、厨房等预制房间拼装成的一个齐全的生活环境，不仅同样具备易于拆装易于搬迁的优点，而且要比单独的“一居室”功能强大得多，能够让进程“住”得更舒服。</p>
<h3 id="为什么-pod-是-kubernetes-的核心对象">为什么 Pod 是 Kubernetes 的核心对象</h3>
<p>因为 Pod 是对容器的“打包”，里面的容器是一个整体，总是能够一起调度、一起运行，绝不会出现分离的情况，而且 Pod 属于 Kubernetes，可以在不触碰下层容器的情况下任意定制修改。所以有了 Pod 这个抽象概念，Kubernetes 在集群级别上管理应用就会“得心应手”了。Kubernetes 让 Pod 去编排处理容器，然后把 Pod 作为应用调度部署的最小单位，Pod 也因此成为了 Kubernetes 世界里的“原子”（当然这个“原子”内部是有结构的，不是铁板一块），基于 Pod 就可以构建出更多更复杂的业务形态了。下面的这张图你也许在其他资料里见过，它从 Pod 开始，扩展出了 Kubernetes 里的一些重要 API 对象，比如配置信息 ConfigMap、离线作业 Job、多实例部署 Deployment 等等，它们都分别对应到现实中的各种实际运维需求。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/9e/75/9ebab7d513a211a926dd69f7535ac175.png?wh=1478x812"
        data-srcset="https://static001.geekbang.org/resource/image/9e/75/9ebab7d513a211a926dd69f7535ac175.png?wh=1478x812, https://static001.geekbang.org/resource/image/9e/75/9ebab7d513a211a926dd69f7535ac175.png?wh=1478x812 1.5x, https://static001.geekbang.org/resource/image/9e/75/9ebab7d513a211a926dd69f7535ac175.png?wh=1478x812 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/9e/75/9ebab7d513a211a926dd69f7535ac175.png?wh=1478x812"
        title="img" /></p>
<p>不过这张图虽然很经典，参考价值很高，但毕竟有些年头了，随着 Kubernetes 的发展，它已经不能够全面地描述 Kubernetes 的资源对象了。受这张图的启发，我自己重新画了一份以 Pod 为中心的 Kubernetes 资源对象关系图，添加了一些新增的 Kubernetes 概念，今后我们就依据这张图来探索 Kubernetes 的各项功能。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b5/cf/b5a7003788cb6f2b1c5c4f6873a8b5cf.jpg?wh=1920x1298"
        data-srcset="https://static001.geekbang.org/resource/image/b5/cf/b5a7003788cb6f2b1c5c4f6873a8b5cf.jpg?wh=1920x1298, https://static001.geekbang.org/resource/image/b5/cf/b5a7003788cb6f2b1c5c4f6873a8b5cf.jpg?wh=1920x1298 1.5x, https://static001.geekbang.org/resource/image/b5/cf/b5a7003788cb6f2b1c5c4f6873a8b5cf.jpg?wh=1920x1298 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b5/cf/b5a7003788cb6f2b1c5c4f6873a8b5cf.jpg?wh=1920x1298"
        title="img" /></p>
<p>从这两张图中你也应该能够看出来，所有的 Kubernetes 资源都直接或者间接地依附在 Pod 之上，所有的 Kubernetes 功能都必须通过 Pod 来实现，所以 Pod 理所当然地成为了 Kubernetes 的核心对象。</p>
<h3 id="如何使用-yaml-描述-pod">如何使用 YAML 描述 Pod</h3>
<p>既然 Pod 这么重要，那么我们就很有必要来详细了解一下 Pod，理解了 Pod 概念，我们的 Kubernetes 学习之旅就成功了一半。还记得吧，我们始终可以用命令 kubectl explain 来查看任意字段的详细说明，所以接下来我就只简要说说写 YAML 时 Pod 里的一些常用字段。因为 Pod 也是 API 对象，所以它也必然具有 apiVersion、kind、metadata、spec 这四个基本组成部分。“apiVersion”和“kind”这两个字段很简单，对于 Pod 来说分别是固定的值 v1 和 Pod，而一般来说，“metadata”里应该有 name 和 labels 这两个字段。我们在使用 Docker 创建容器的时候，可以不给容器起名字，但在 Kubernetes 里，Pod 必须要有一个名字，这也是 Kubernetes 里所有资源对象的一个约定。在课程里，我通常会为 Pod 名字统一加上 pod 后缀，这样可以和其他类型的资源区分开。</p>
<p>name 只是一个基本的标识，信息有限，所以 labels 字段就派上了用处。它可以添加任意数量的 Key-Value，给 Pod“贴”上归类的标签，结合 name 就更方便识别和管理了。比如说，我们可以根据运行环境，使用标签 env=dev/test/prod，或者根据所在的数据中心，使用标签 region: north/south，还可以根据应用在系统中的层次，使用 tier=front/middle/back ……如此种种，只需要发挥你的想象力。下面这段 YAML 代码就描述了一个简单的 Pod，名字是“busy-pod”，再附加上一些标签：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: busy-pod
  labels:
    owner: chrono
    env: demo
    region: north
    tier: back
</code></pre></td></tr></table>
</div>
</div><p>“metadata”一般写上 name 和 labels 就足够了，而“spec”字段由于需要管理、维护 Pod 这个 Kubernetes 的基本调度单元，里面有非常多的关键信息，今天我介绍最重要的“containers”，其他的 hostname、restartPolicy 等字段你可以课后自己查阅文档学习。“containers”是一个数组，里面的每一个元素又是一个 container 对象，也就是容器。和 Pod 一样，container 对象也必须要有一个 name 表示名字，然后当然还要有一个 image 字段来说明它使用的镜像，这两个字段是必须要有的，否则 Kubernetes 会报告数据验证错误。container 对象的其他字段基本上都可以和“入门篇”学过的 Docker、容器技术对应，理解起来难度不大，我就随便列举几个：</p>
<p>ports：列出容器对外暴露的端口，和 Docker 的 -p 参数有点像。imagePullPolicy：指定镜像的拉取策略，可以是 Always/Never/IfNotPresent，一般默认是 IfNotPresent，也就是说只有本地不存在才会远程拉取镜像，可以减少网络消耗。env：定义 Pod 的环境变量，和 Dockerfile 里的 ENV 指令有点类似，但它是运行时指定的，更加灵活可配置。command：定义容器启动时要执行的命令，相当于 Dockerfile 里的 ENTRYPOINT 指令。args：它是 command 运行时的参数，相当于 Dockerfile 里的 CMD 指令，这两个命令和 Docker 的含义不同，要特别注意。</p>
<p>现在我们就来编写“busy-pod”的 spec 部分，添加 env、command、args 等字段：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">spec:
  containers:
  - image: busybox:latest
    name: busy
    imagePullPolicy: IfNotPresent
    env:
      - name: os
        value: &#34;ubuntu&#34;
      - name: debug
        value: &#34;on&#34;
    command:
      - /bin/echo
    args:
      - &#34;$(os), $(debug)&#34;
</code></pre></td></tr></table>
</div>
</div><p>这里我为 Pod 指定使用镜像 busybox:latest，拉取策略是 IfNotPresent ，然后定义了 os 和 debug 两个环境变量，启动命令是 /bin/echo，参数里输出刚才定义的环境变量。把这份 YAML 文件和 Docker 命令对比一下，你就可以看出，YAML 在 spec.containers 字段里用“声明式”把容器的运行状态描述得非常清晰准确，要比 docker run 那长长的命令行要整洁的多，对人、对机器都非常友好。</p>
<h3 id="如何使用-kubectl-操作-pod">如何使用 kubectl 操作 Pod</h3>
<p>有了描述 Pod 的 YAML 文件，现在我就介绍一下用来操作 Pod 的 kubectl 命令。kubectl apply、kubectl delete 这两个命令在上次课里已经说过了，它们可以使用 -f 参数指定 YAML 文件创建或者删除 Pod，例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f busy-pod.yml
kubectl delete -f busy-pod.yaml
</code></pre></td></tr></table>
</div>
</div><p>不过，因为我们在 YAML 里定义了“name”字段，所以也可以在删除的时候直接指定名字来删除：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete pod busy-pod
</code></pre></td></tr></table>
</div>
</div><p>和 Docker 不一样，Kubernetes 的 Pod 不会在前台运行，只能在后台（相当于默认使用了参数 -d），所以输出信息不能直接看到。我们可以用命令 kubectl logs，它会把 Pod 的标准输出流信息展示给我们看，在这里就会显示出预设的两个环境变量的值：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl logs busy-pod
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/76/f2/76452a603cddaf3cce6706697369d1f2.png?wh=948x124"
        data-srcset="https://static001.geekbang.org/resource/image/76/f2/76452a603cddaf3cce6706697369d1f2.png?wh=948x124, https://static001.geekbang.org/resource/image/76/f2/76452a603cddaf3cce6706697369d1f2.png?wh=948x124 1.5x, https://static001.geekbang.org/resource/image/76/f2/76452a603cddaf3cce6706697369d1f2.png?wh=948x124 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/76/f2/76452a603cddaf3cce6706697369d1f2.png?wh=948x124"
        title="img" /></p>
<p>使用命令 kubectl get pod 可以查看 Pod 列表和运行状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pod
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/54/9c/544d4d4521yy1e2cyy3b79615cbcc69c.png?wh=1464x184"
        data-srcset="https://static001.geekbang.org/resource/image/54/9c/544d4d4521yy1e2cyy3b79615cbcc69c.png?wh=1464x184, https://static001.geekbang.org/resource/image/54/9c/544d4d4521yy1e2cyy3b79615cbcc69c.png?wh=1464x184 1.5x, https://static001.geekbang.org/resource/image/54/9c/544d4d4521yy1e2cyy3b79615cbcc69c.png?wh=1464x184 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/54/9c/544d4d4521yy1e2cyy3b79615cbcc69c.png?wh=1464x184"
        title="img" /></p>
<p>你会发现这个 Pod 运行有点不正常，状态是“CrashLoopBackOff”，那么我们可以使用命令 kubectl describe 来检查它的详细状态，它在调试排错时很有用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl describe pod busy-pod
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/78/68/786bb31f3d6d69edd16ddfb540d9ef68.png?wh=1920x294"
        data-srcset="https://static001.geekbang.org/resource/image/78/68/786bb31f3d6d69edd16ddfb540d9ef68.png?wh=1920x294, https://static001.geekbang.org/resource/image/78/68/786bb31f3d6d69edd16ddfb540d9ef68.png?wh=1920x294 1.5x, https://static001.geekbang.org/resource/image/78/68/786bb31f3d6d69edd16ddfb540d9ef68.png?wh=1920x294 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/78/68/786bb31f3d6d69edd16ddfb540d9ef68.png?wh=1920x294"
        title="img" /></p>
<p>通常需要关注的是末尾的“Events”部分，它显示的是 Pod 运行过程中的一些关键节点事件。对于这个 busy-pod，因为它只执行了一条 echo 命令就退出了，而 Kubernetes 默认会重启 Pod，所以就会进入一个反复停止 - 启动的循环错误状态。因为 Kubernetes 里运行的应用大部分都是不会主动退出的服务，所以我们可以把这个 busy-pod 删掉，用上次课里创建的 ngx-pod.yml，启动一个 Nginx 服务，这才是大多数 Pod 的工作方式。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f ngx-pod.yml
</code></pre></td></tr></table>
</div>
</div><p>启动之后，我们再用 kubectl get pod 来查看状态，就会发现它已经是“Running”状态了：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6c/b1/6c1ce29c29602f111ba39dea6aab95b1.png?wh=1920x415"
        data-srcset="https://static001.geekbang.org/resource/image/6c/b1/6c1ce29c29602f111ba39dea6aab95b1.png?wh=1920x415, https://static001.geekbang.org/resource/image/6c/b1/6c1ce29c29602f111ba39dea6aab95b1.png?wh=1920x415 1.5x, https://static001.geekbang.org/resource/image/6c/b1/6c1ce29c29602f111ba39dea6aab95b1.png?wh=1920x415 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6c/b1/6c1ce29c29602f111ba39dea6aab95b1.png?wh=1920x415"
        title="img" /></p>
<p>另外，kubectl 也提供与 docker 类似的 cp 和 exec 命令，kubectl cp 可以把本地文件拷贝进 Pod，kubectl exec 是进入 Pod 内部执行 Shell 命令，用法也差不多。比如我有一个“a.txt”文件，那么就可以使用 kubectl cp 拷贝进 Pod 的“/tmp”目录里：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">echo &#39;aaa&#39; &gt; a.txt
kubectl cp a.txt ngx-pod:/tmp
</code></pre></td></tr></table>
</div>
</div><p>不过 kubectl exec 的命令格式与 Docker 有一点小差异，需要在 Pod 后面加上 &ndash;，把 kubectl 的命令与 Shell 命令分隔开，你在用的时候需要小心一些：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl exec -it ngx-pod -- sh
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/34/6b/343756ee45533a056fdca97f9fe2dd6b.png?wh=1920x402"
        data-srcset="https://static001.geekbang.org/resource/image/34/6b/343756ee45533a056fdca97f9fe2dd6b.png?wh=1920x402, https://static001.geekbang.org/resource/image/34/6b/343756ee45533a056fdca97f9fe2dd6b.png?wh=1920x402 1.5x, https://static001.geekbang.org/resource/image/34/6b/343756ee45533a056fdca97f9fe2dd6b.png?wh=1920x402 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/34/6b/343756ee45533a056fdca97f9fe2dd6b.png?wh=1920x402"
        title="img" /></p>
<h3 id="小结-11">小结</h3>
<p>好了，今天我们一起学习了 Kubernetes 里最核心最基本的概念 Pod，知道了应该如何使用 YAML 来定制 Pod，还有如何使用 kubectl 命令来创建、删除、查看、调试 Pod。Pod 屏蔽了容器的一些底层细节，同时又具有足够的控制管理能力，比起容器的“细粒度”、虚拟机的“粗粒度”，Pod 可以说是“中粒度”，灵活又轻便，非常适合在云计算领域作为应用调度的基本单元，因而成为了 Kubernetes 世界里构建一切业务的“原子”。今天的知识要点我简单列在了下面：</p>
<p>现实中经常会有多个进程密切协作才能完成任务的应用，而仅使用容器很难描述这种关系，所以就出现了 Pod，它“打包”一个或多个容器，保证里面的进程能够被整体调度。Pod 是 Kubernetes 管理应用的最小单位，其他的所有概念都是从 Pod 衍生出来的。Pod 也应该使用 YAML“声明式”描述，关键字段是“spec.containers”，列出名字、镜像、端口等要素，定义内部的容器运行状态。操作 Pod 的命令很多与 Docker 类似，如 kubectl run、kubectl cp、kubectl exec 等，但有的命令有些小差异，使用的时候需要注意。</p>
<p>虽然 Pod 是 Kubernetes 的核心概念，非常重要，但事实上在 Kubernetes 里通常并不会直接创建 Pod，因为它只是对容器做了简单的包装，比较脆弱，离复杂的业务需求还有些距离，需要 Job、CronJob、Deployment 等其他对象增添更多的功能才能投入生产使用。</p>
<h2 id="13jobcronjob为什么不直接用pod来处理业务">13｜Job/CronJob：为什么不直接用Pod来处理业务？</h2>
<p>在上次的课里我们学习了 Kubernetes 的核心对象 Pod，用来编排一个或多个容器，让这些容器共享网络、存储等资源，总是共同调度，从而紧密协同工作。因为 Pod 比容器更能够表示实际的应用，所以 Kubernetes 不会在容器层面来编排业务，而是把 Pod 作为在集群里调度运维的最小单位。前面我们也看到了一张 Kubernetes 的资源对象关系图，以 Pod 为中心，延伸出了很多表示各种业务的其他资源对象。那么你会不会有这样的疑问：Pod 的功能已经足够完善了，为什么还要定义这些额外的对象呢？为什么不直接在 Pod 里添加功能，来处理业务需求呢？这个问题体现了 Google 对大规模计算集群管理的深度思考，今天我就说说 Kubernetes 基于 Pod 的设计理念，先从最简单的两种对象——Job 和 CronJob 讲起。</p>
<h3 id="为什么不直接使用-pod">为什么不直接使用 Pod</h3>
<p>现在你应该知道，Kubernetes 使用的是 RESTful API，把集群中的各种业务都抽象为 HTTP 资源对象，那么在这个层次之上，我们就可以使用面向对象的方式来考虑问题。如果你有一些编程方面的经验，就会知道面向对象编程（OOP），它把一切都视为高内聚的对象，强调对象之间互相通信来完成任务。虽然面向对象的设计思想多用于软件开发，但它放到 Kubernetes 里却意外地合适。因为 Kubernetes 使用 YAML 来描述资源，把业务简化成了一个个的对象，内部有属性，外部有联系，也需要互相协作，只不过我们不需要编程，完全由 Kubernetes 自动处理（其实 Kubernetes 的 Go 语言内部实现就大量应用了面向对象）。面向对象的设计有许多基本原则，其中有两条我认为比较恰当地描述了 Kubernetes 对象设计思路，一个是“单一职责”，另一个是“组合优于继承”。</p>
<p>“单一职责”的意思是对象应该只专注于做好一件事情，不要贪大求全，保持足够小的粒度才更方便复用和管理。“组合优于继承”的意思是应该尽量让对象在运行时产生联系，保持松耦合，而不要用硬编码的方式固定对象的关系。应用这两条原则，我们再来看 Kubernetes 的资源对象就会很清晰了。因为 Pod 已经是一个相对完善的对象，专门负责管理容器，那么我们就不应该再“画蛇添足”地盲目为它扩充功能，而是要保持它的独立性，容器之外的功能就需要定义其他的对象，把 Pod 作为它的一个成员“组合”进去。这样每种 Kubernetes 对象就可以只关注自己的业务领域，只做自己最擅长的事情，其他的工作交给其他对象来处理，既不“缺位”也不“越位”，既有分工又有协作，从而以最小成本实现最大收益。</p>
<h3 id="为什么要有-jobcronjob">为什么要有 Job/CronJob</h3>
<p>现在我们来看看 Kubernetes 里的两种新对象：Job 和 CronJob，它们就组合了 Pod，实现了对离线业务的处理。上次课讲 Pod 的时候我们运行了两个 Pod：Nginx 和 busybox，它们分别代表了 Kubernetes 里的两大类业务。一类是像 Nginx 这样长时间运行的“在线业务”，另一类是像 busybox 这样短时间运行的“离线业务”。“在线业务”类型的应用有很多，比如 Nginx、Node.js、MySQL、Redis 等等，一旦运行起来基本上不会停，也就是永远在线。而“离线业务”类型的应用也并不少见，它们一般不直接服务于外部用户，只对内部用户有意义，比如日志分析、数据建模、视频转码等等，虽然计算量很大，但只会运行一段时间。“离线业务”的特点是必定会退出，不会无期限地运行下去，所以它的调度策略也就与“在线业务”存在很大的不同，需要考虑运行超时、状态检查、失败重试、获取计算结果等管理事项。而这些业务特性与容器管理没有必然的联系，如果由 Pod 来实现就会承担不必要的义务，违反了“单一职责”，所以我们应该把这部分功能分离到另外一个对象上实现，让这个对象去控制 Pod 的运行，完成附加的工作。</p>
<p>“离线业务”也可以分为两种。一种是“临时任务”，跑完就完事了，下次有需求了说一声再重新安排；另一种是“定时任务”，可以按时按点周期运行，不需要过多干预。对应到 Kubernetes 里，“临时任务”就是 API 对象 Job，“定时任务”就是 API 对象 CronJob，使用这两个对象你就能够在 Kubernetes 里调度管理任意的离线业务了。由于 Job 和 CronJob 都属于离线业务，所以它们也比较相似。我们先学习通常只会运行一次的 Job 对象以及如何操作。</p>
<h3 id="如何使用-yaml-描述-job">如何使用 YAML 描述 Job</h3>
<p>Job 的 YAML“文件头”部分还是那几个必备字段，我就不再重复解释了，简单说一下：apiVersion 不是 v1，而是 batch/v1。kind 是 Job，这个和对象的名字是一致的。metadata 里仍然要有 name 标记名字，也可以用 labels 添加任意的标签。</p>
<p>如果记不住这些也不要紧，你还可以使用命令 kubectl explain job 来看它的字段说明。不过想要生成 YAML 样板文件的话不能使用 kubectl run，因为 kubectl run 只能创建 Pod，要创建 Pod 以外的其他 API 对象，需要使用命令 kubectl create，再加上对象的类型名。比如用 busybox 创建一个“echo-job”，命令就是这样的：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;              # 定义Shell变量
kubectl create job echo-job --image=busybox $out
</code></pre></td></tr></table>
</div>
</div><p>会生成一个基本的 YAML 文件，保存之后做点修改，就有了一个 Job 对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: batch/v1
kind: Job
metadata:
  name: echo-job

spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - image: busybox
        name: echo-job
        imagePullPolicy: IfNotPresent
        command: [&#34;/bin/echo&#34;]
        args: [&#34;hello&#34;, &#34;world&#34;]
</code></pre></td></tr></table>
</div>
</div><p>你会注意到 Job 的描述与 Pod 很像，但又有些不一样，主要的区别就在“spec”字段里，多了一个 template 字段，然后又是一个“spec”，显得有点怪。如果你理解了刚才说的面向对象设计思想，就会明白这种做法的道理。它其实就是在 Job 对象里应用了组合模式，template 字段定义了一个“应用模板”，里面嵌入了一个 Pod，这样 Job 就可以从这个模板来创建出 Pod。而这个 Pod 因为受 Job 的管理控制，不直接和 apiserver 打交道，也就没必要重复 apiVersion 等“头字段”，只需要定义好关键的 spec，描述清楚容器相关的信息就可以了，可以说是一个“无头”的 Pod 对象。为了辅助你理解，我把 Job 对象重新组织了一下，用不同的颜色来区分字段，这样你就能够很容易看出来，其实这个“echo-job”里并没有太多额外的功能，只是把 Pod 做了个简单的包装：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/9b/28/9b780905a824d2103d4ayyc79267ae28.jpg?wh=1920x2141"
        data-srcset="https://static001.geekbang.org/resource/image/9b/28/9b780905a824d2103d4ayyc79267ae28.jpg?wh=1920x2141, https://static001.geekbang.org/resource/image/9b/28/9b780905a824d2103d4ayyc79267ae28.jpg?wh=1920x2141 1.5x, https://static001.geekbang.org/resource/image/9b/28/9b780905a824d2103d4ayyc79267ae28.jpg?wh=1920x2141 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/9b/28/9b780905a824d2103d4ayyc79267ae28.jpg?wh=1920x2141"
        title="img" /></p>
<p>总的来说，这里的 Pod 工作非常简单，在 containers 里写好名字和镜像，command 执行 /bin/echo，输出“hello world”。不过，因为 Job 业务的特殊性，所以我们还要在 spec 里多加一个字段 restartPolicy，确定 Pod 运行失败时的策略，OnFailure 是失败原地重启容器，而 Never 则是不重启容器，让 Job 去重新调度生成一个新的 Pod。</p>
<h3 id="如何在-kubernetes-里操作-job">如何在 Kubernetes 里操作 Job</h3>
<p>现在让我们来创建 Job 对象，运行这个简单的离线作业，用的命令还是 kubectl apply：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f job.yml
</code></pre></td></tr></table>
</div>
</div><p>创建之后 Kubernetes 就会从 YAML 的模板定义中提取 Pod，在 Job 的控制下运行 Pod，你可以用 kubectl get job、kubectl get pod 来分别查看 Job 和 Pod 的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get job
kubectl get pod
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/33/55/33ac80cb9f5dd91d1affc133e56efc55.png?wh=1382x368"
        data-srcset="https://static001.geekbang.org/resource/image/33/55/33ac80cb9f5dd91d1affc133e56efc55.png?wh=1382x368, https://static001.geekbang.org/resource/image/33/55/33ac80cb9f5dd91d1affc133e56efc55.png?wh=1382x368 1.5x, https://static001.geekbang.org/resource/image/33/55/33ac80cb9f5dd91d1affc133e56efc55.png?wh=1382x368 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/33/55/33ac80cb9f5dd91d1affc133e56efc55.png?wh=1382x368"
        title="img" /></p>
<p>可以看到，因为 Pod 被 Job 管理，它就不会反复重启报错了，而是会显示为 Completed 表示任务完成，而 Job 里也会列出运行成功的作业数量，这里只有一个作业，所以就是 1/1。你还可以看到，Pod 被自动关联了一个名字，用的是 Job 的名字（echo-job）再加上一个随机字符串（pb5gh），这当然也是 Job 管理的“功劳”，免去了我们手工定义的麻烦，这样我们就可以使用命令 kubectl logs 来获取 Pod 的运行结果：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/81/b5/81224cedf0acf209b746a1162d09b3b5.png?wh=1114x118"
        data-srcset="https://static001.geekbang.org/resource/image/81/b5/81224cedf0acf209b746a1162d09b3b5.png?wh=1114x118, https://static001.geekbang.org/resource/image/81/b5/81224cedf0acf209b746a1162d09b3b5.png?wh=1114x118 1.5x, https://static001.geekbang.org/resource/image/81/b5/81224cedf0acf209b746a1162d09b3b5.png?wh=1114x118 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/81/b5/81224cedf0acf209b746a1162d09b3b5.png?wh=1114x118"
        title="img" /></p>
<p>到这里，你可能会觉得，经过了 Job、Pod 对容器的两次封装，虽然从概念上很清晰，但好像并没有带来什么实际的好处，和直接跑容器也差不了多少。其实 Kubernetes 的这套 YAML 描述对象的框架提供了非常多的灵活性，可以在 Job 级别、Pod 级别添加任意的字段来定制业务，这种优势是简单的容器技术无法相比的。这里我列出几个控制离线作业的重要字段，其他更详细的信息可以参考 Job 文档：</p>
<p>activeDeadlineSeconds，设置 Pod 运行的超时时间。backoffLimit，设置 Pod 的失败重试次数。completions，Job 完成需要运行多少个 Pod，默认是 1 个。parallelism，它与 completions 相关，表示允许并发运行的 Pod 数量，避免过多占用资源。</p>
<p>要注意这 4 个字段并不在 template 字段下，而是在 spec 字段下，所以它们是属于 Job 级别的，用来控制模板里的 Pod 对象。下面我再创建一个 Job 对象，名字叫“sleep-job”，它随机睡眠一段时间再退出，模拟运行时间较长的作业（比如 MapReduce）。Job 的参数设置成 15 秒超时，最多重试 2 次，总共需要运行完 4 个 Pod，但同一时刻最多并发 2 个 Pod：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: batch/v1
kind: Job
metadata:
  name: sleep-job

spec:
  activeDeadlineSeconds: 15
  backoffLimit: 2
  completions: 4
  parallelism: 2

  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - image: busybox
        name: echo-job
        imagePullPolicy: IfNotPresent
        command:
          - sh
          - -c
          - sleep $(($RANDOM % 10 + 1)) &amp;&amp; echo done
</code></pre></td></tr></table>
</div>
</div><p>使用 kubectl apply 创建 Job 之后，我们可以用 kubectl get pod -w 来实时观察 Pod 的状态，看到 Pod 不断被排队、创建、运行的过程：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f sleep-job.yml
kubectl get pod -w
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/7d/b7/7d413a0c38065de2063a99e7df2b7eb7.png?wh=1591x1328"
        data-srcset="https://static001.geekbang.org/resource/image/7d/b7/7d413a0c38065de2063a99e7df2b7eb7.png?wh=1591x1328, https://static001.geekbang.org/resource/image/7d/b7/7d413a0c38065de2063a99e7df2b7eb7.png?wh=1591x1328 1.5x, https://static001.geekbang.org/resource/image/7d/b7/7d413a0c38065de2063a99e7df2b7eb7.png?wh=1591x1328 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/7d/b7/7d413a0c38065de2063a99e7df2b7eb7.png?wh=1591x1328"
        title="img" /></p>
<p>等到 4 个 Pod 都运行完毕，我们再用 kubectl get 来看看 Job 和 Pod 的状态：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/58/46/58b99356c811bd377acfa4cb921d2446.png?wh=1426x542"
        data-srcset="https://static001.geekbang.org/resource/image/58/46/58b99356c811bd377acfa4cb921d2446.png?wh=1426x542, https://static001.geekbang.org/resource/image/58/46/58b99356c811bd377acfa4cb921d2446.png?wh=1426x542 1.5x, https://static001.geekbang.org/resource/image/58/46/58b99356c811bd377acfa4cb921d2446.png?wh=1426x542 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/58/46/58b99356c811bd377acfa4cb921d2446.png?wh=1426x542"
        title="img" /></p>
<p>就会看到 Job 的完成数量如同我们预期的是 4，而 4 个 Pod 也都是完成状态。显然，“声明式”的 Job 对象让离线业务的描述变得非常直观，简单的几个字段就可以很好地控制作业的并行度和完成数量，不需要我们去人工监控干预，Kubernetes 把这些都自动化实现了。</p>
<h3 id="如何使用-yaml-描述-cronjob">如何使用 YAML 描述 CronJob</h3>
<p>学习了“临时任务”的 Job 对象之后，再学习“定时任务”的 CronJob 对象也就比较容易了，我就直接使用命令 kubectl create 来创建 CronJob 的样板。要注意两点。第一，因为 CronJob 的名字有点长，所以 Kubernetes 提供了简写 cj，这个简写也可以使用命令 kubectl api-resources 看到；第二，CronJob 需要定时运行，所以我们在命令行里还需要指定参数 &ndash;schedule。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;              # 定义Shell变量
kubectl create cj echo-cj --image=busybox --schedule=&#34;&#34; $out
</code></pre></td></tr></table>
</div>
</div><p>然后我们编辑这个 YAML 样板，生成 CronJob 对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: batch/v1
kind: CronJob
metadata:
  name: echo-cj

spec:
  schedule: &#39;*/1 * * * *&#39;
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - image: busybox
            name: echo-cj
            imagePullPolicy: IfNotPresent
            command: [&#34;/bin/echo&#34;]
            args: [&#34;hello&#34;, &#34;world&#34;]

</code></pre></td></tr></table>
</div>
</div><p>我们还是重点关注它的 spec 字段，你会发现它居然连续有三个 spec 嵌套层次：第一个 spec 是 CronJob 自己的对象规格声明第二个 spec 从属于“jobTemplate”，它定义了一个 Job 对象。第三个 spec 从属于“template”，它定义了 Job 里运行的 Pod。</p>
<p>所以，CronJob 其实是又组合了 Job 而生成的新对象，我还是画了一张图，方便你理解它的“套娃”结构：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/yy/3c/yy352c661ae37dd116dd12c61932b43c.jpg?wh=1920x2206"
        data-srcset="https://static001.geekbang.org/resource/image/yy/3c/yy352c661ae37dd116dd12c61932b43c.jpg?wh=1920x2206, https://static001.geekbang.org/resource/image/yy/3c/yy352c661ae37dd116dd12c61932b43c.jpg?wh=1920x2206 1.5x, https://static001.geekbang.org/resource/image/yy/3c/yy352c661ae37dd116dd12c61932b43c.jpg?wh=1920x2206 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/yy/3c/yy352c661ae37dd116dd12c61932b43c.jpg?wh=1920x2206"
        title="img" /></p>
<p>除了定义 Job 对象的“jobTemplate”字段之外，CronJob 还有一个新字段就是“schedule”，用来定义任务周期运行的规则。它使用的是标准的 Cron 语法，指定分钟、小时、天、月、周，和 Linux 上的 crontab 是一样的。像在这里我就指定每分钟运行一次，格式具体的含义你可以课后参考 Kubernetes 官网文档。除了名字不同，CronJob 和 Job 的用法几乎是一样的，使用 kubectl apply 创建 CronJob，使用 kubectl get cj、kubectl get pod 来查看状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f cronjob.yml
kubectl get cj
kubectl get pod
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b0/2c/b00fdd8541372fb7a4de00de5ac6342c.png?wh=1644x484"
        data-srcset="https://static001.geekbang.org/resource/image/b0/2c/b00fdd8541372fb7a4de00de5ac6342c.png?wh=1644x484, https://static001.geekbang.org/resource/image/b0/2c/b00fdd8541372fb7a4de00de5ac6342c.png?wh=1644x484 1.5x, https://static001.geekbang.org/resource/image/b0/2c/b00fdd8541372fb7a4de00de5ac6342c.png?wh=1644x484 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b0/2c/b00fdd8541372fb7a4de00de5ac6342c.png?wh=1644x484"
        title="img" /></p>
<h3 id="小结-12">小结</h3>
<p>好了，今天我们以面向对象思想分析了一下 Kubernetes 里的资源对象设计，它强调“职责单一”和“对象组合”，简单来说就是“对象套对象”。通过这种嵌套方式，Kubernetes 里的这些 API 对象就形成了一个“控制链”：CronJob 使用定时规则控制 Job，Job 使用并发数量控制 Pod，Pod 再定义参数控制容器，容器再隔离控制进程，进程最终实现业务功能，层层递进的形式有点像设计模式里的 Decorator（装饰模式），链条里的每个环节都各司其职，在 Kubernetes 的统一指挥下完成任务。小结一下今天的内容：</p>
<p>Pod 是 Kubernetes 的最小调度单元，但为了保持它的独立性，不应该向它添加多余的功能。Kubernetes 为离线业务提供了 Job 和 CronJob 两种 API 对象，分别处理“临时任务”和“定时任务”。Job 的关键字段是 spec.template，里面定义了用来运行业务的 Pod 模板，其他的重要字段有 completions、parallelism 等CronJob 的关键字段是 spec.jobTemplate 和 spec.schedule，分别定义了 Job 模板和定时运行的规则。</p>
<h2 id="14configmapsecret怎样配置定制我的应用">14｜ConfigMap/Secret：怎样配置、定制我的应用</h2>
<p>Chrono 2022-07-22
前两节课里我们学习了 Kubernetes 里的三种 API 对象：Pod、Job 和 CronJob，虽然还没有讲到更高级的其他对象，但使用它们也可以在集群里编排运行一些实际的业务了。
不过想让业务更顺利地运行，有一个问题不容忽视，那就是应用的配置管理。
配置文件，你应该有所了解吧，通常来说应用程序都会有一个，它把运行时需要的一些参数从代码中分离出来，让我们在实际运行的时候能更方便地调整优化，比如说 Nginx 有 nginx.conf、Redis 有 redis.conf、MySQL 有 my.cnf 等等。
我们在“入门篇”里学习容器技术的时候讲过，可以选择两种管理配置文件的方式。第一种是编写 Dockerfile，用 COPY 指令把配置文件打包到镜像里；第二种是在运行时使用 docker cp 或者 docker run -v，把本机的文件拷贝进容器。
但这两种方式都存在缺陷。第一种方法相当于是在镜像里固定了配置文件，不好修改，不灵活，第二种方法则显得有点“笨拙”，不适合在集群中自动化运维管理。
对于这个问题 Kubernetes 有它自己的解决方案，你也应该能够猜得到，当然还是使用 YAML 语言来定义 API 对象，再组合起来实现动态配置。
今天我就来讲解 Kubernetes 里专门用来管理配置信息的两种对象：ConfigMap 和 Secret，使用它们来灵活地配置、定制我们的应用。</p>
<h3 id="configmapsecret">ConfigMap/Secret</h3>
<p>首先你要知道，应用程序有很多类别的配置信息，但从数据安全的角度来看可以分成两类：
一类是明文配置，也就是不保密，可以任意查询修改，比如服务端口、运行参数、文件路径等等。
另一类则是机密配置，由于涉及敏感信息需要保密，不能随便查看，比如密码、密钥、证书等等。
这两类配置信息本质上都是字符串，只是由于安全性的原因，在存放和使用方面有些差异，所以 Kubernetes 也就定义了两个 API 对象，ConfigMap 用来保存明文配置，Secret 用来保存秘密配置。</p>
<h3 id="什么是-configmap">什么是 ConfigMap</h3>
<p>先来看 ConfigMap，我们仍然可以用命令 kubectl create 来创建一个它的 YAML 样板。注意，它有简写名字“cm”，所以命令行里没必要写出它的全称：
export out=&quot;&ndash;dry-run=client -o yaml&quot;        # 定义Shell变量
kubectl create cm info $out
得到的样板文件大概是这个样子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ConfigMap
metadata:
  name: info
</code></pre></td></tr></table>
</div>
</div><p>你可能会有点惊讶，ConfigMap 的 YAML 和之前我们学过的 Pod、Job 不一样，除了熟悉的“apiVersion”“kind”“metadata”，居然就没有其他的了，最重要的字段“spec”哪里去了？这是因为 ConfigMap 存储的是配置数据，是静态的字符串，并不是容器，所以它们就不需要用“spec”字段来说明运行时的“规格”。
既然 ConfigMap 要存储数据，我们就需要用另一个含义更明确的字段“data”。
要生成带有“data”字段的 YAML 样板，你需要在 kubectl create 后面多加一个参数 &ndash;from-literal ，表示从字面值生成一些数据：
kubectl create cm info &ndash;from-literal=k=v $out
注意，因为在 ConfigMap 里的数据都是 Key-Value 结构，所以 &ndash;from-literal 参数需要使用 k=v 的形式。
把 YAML 样板文件修改一下，再多增添一些 Key-Value，就得到了一个比较完整的 ConfigMap 对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ConfigMap
metadata:
  name: info
data:
  count: &#39;10&#39;
  debug: &#39;on&#39;
  path: &#39;/etc/systemd&#39;
  greeting: |
    say hello to kubernetes.
</code></pre></td></tr></table>
</div>
</div><p>现在就可以使用 kubectl apply 把这个 YAML 交给 Kubernetes，让它创建 ConfigMap 对象了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f cm.yml
</code></pre></td></tr></table>
</div>
</div><p>创建成功后，我们还是可以用 kubectl get、kubectl describe 来查看 ConfigMap 的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get cm
kubectl describe cm info
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a6/78/a61239d55a93a5cd9da7148297d22878.png?wh=782x184"
        data-srcset="https://static001.geekbang.org/resource/image/a6/78/a61239d55a93a5cd9da7148297d22878.png?wh=782x184, https://static001.geekbang.org/resource/image/a6/78/a61239d55a93a5cd9da7148297d22878.png?wh=782x184 1.5x, https://static001.geekbang.org/resource/image/a6/78/a61239d55a93a5cd9da7148297d22878.png?wh=782x184 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a6/78/a61239d55a93a5cd9da7148297d22878.png?wh=782x184"
        title="https://static001.geekbang.org/resource/image/a6/78/a61239d55a93a5cd9da7148297d22878.png?wh=782x184" />
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/34/48/343c94dacb9f872721597e99b346b148.png?wh=1042x1272"
        data-srcset="https://static001.geekbang.org/resource/image/34/48/343c94dacb9f872721597e99b346b148.png?wh=1042x1272, https://static001.geekbang.org/resource/image/34/48/343c94dacb9f872721597e99b346b148.png?wh=1042x1272 1.5x, https://static001.geekbang.org/resource/image/34/48/343c94dacb9f872721597e99b346b148.png?wh=1042x1272 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/34/48/343c94dacb9f872721597e99b346b148.png?wh=1042x1272"
        title="https://static001.geekbang.org/resource/image/34/48/343c94dacb9f872721597e99b346b148.png?wh=1042x1272" />
你可以看到，现在 ConfigMap 的 Key-Value 信息就已经存入了 etcd 数据库，后续就可以被其他 API 对象使用。</p>
<h3 id="什么是-secret">什么是 Secret</h3>
<p>了解了 ConfigMap 对象，我们再来看 Secret 对象就会容易很多，它和 ConfigMap 的结构和用法很类似，不过在 Kubernetes 里 Secret 对象又细分出很多类，比如：
访问私有镜像仓库的认证信息
身份识别的凭证信息
HTTPS 通信的证书和私钥
一般的机密信息（格式由用户自行解释）
前几种我们现在暂时用不到，所以就只使用最后一种，创建 YAML 样板的命令是 kubectl create secret generic ，同样，也要使用参数 &ndash;from-literal 给出 Key-Value 值：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create secret generic user --from-literal=name=root $out

</code></pre></td></tr></table>
</div>
</div><p>得到的 Secret 对象大概是这个样子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Secret
metadata:
  name: user
data:
  name: cm9vdA==
</code></pre></td></tr></table>
</div>
</div><p>Secret 对象第一眼的感觉和 ConfigMap 非常相似，只是“kind”字段由“ConfigMap”变成了“Secret”，后面同样也是“data”字段，里面也是 Key-Value 的数据。
不过，既然它的名字是 Secret，我们就不能像 ConfigMap 那样直接保存明文了，需要对数据“做点手脚”。你会发现，这里的“name”值是一串“乱码”，而不是刚才在命令行里写的明文“root”。
这串“乱码”就是 Secret 与 ConfigMap 的不同之处，不让用户直接看到原始数据，起到一定的保密作用。不过它的手法非常简单，只是做了 Base64 编码，根本算不上真正的加密，所以我们完全可以绕开 kubectl，自己用 Linux 小工具“base64”来对数据编码，然后写入 YAML 文件，比如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">echo -n &#34;123456&#34; | base64
MTIzNDU2
</code></pre></td></tr></table>
</div>
</div><p>要注意这条命令里的 echo ，必须要加参数 -n 去掉字符串里隐含的换行符，否则 Base64 编码出来的字符串就是错误的。
我们再来重新编辑 Secret 的 YAML，为它添加两个新的数据，方式可以是参数 &ndash;from-literal 自动编码，也可以是自己手动编码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Secret
metadata:
  name: user
data:
  name: cm9vdA==  # root
  pwd: MTIzNDU2   # 123456
  db: bXlzcWw=    # mysql
</code></pre></td></tr></table>
</div>
</div><p>接下来的创建和查看对象操作和 ConfigMap 是一样的，使用 kubectl apply、kubectl get、kubectl describe：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply  -f secret.yml
kubectl get secret
kubectl describe secret user
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/0f/10/0f769ba725d1006c1cb98ed9003d7210.png?wh=1838x250"
        data-srcset="https://static001.geekbang.org/resource/image/0f/10/0f769ba725d1006c1cb98ed9003d7210.png?wh=1838x250, https://static001.geekbang.org/resource/image/0f/10/0f769ba725d1006c1cb98ed9003d7210.png?wh=1838x250 1.5x, https://static001.geekbang.org/resource/image/0f/10/0f769ba725d1006c1cb98ed9003d7210.png?wh=1838x250 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/0f/10/0f769ba725d1006c1cb98ed9003d7210.png?wh=1838x250"
        title="https://static001.geekbang.org/resource/image/0f/10/0f769ba725d1006c1cb98ed9003d7210.png?wh=1838x250" />
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/59/6c/59ac74796771897e0246a4532789076c.png?wh=1138x782"
        data-srcset="https://static001.geekbang.org/resource/image/59/6c/59ac74796771897e0246a4532789076c.png?wh=1138x782, https://static001.geekbang.org/resource/image/59/6c/59ac74796771897e0246a4532789076c.png?wh=1138x782 1.5x, https://static001.geekbang.org/resource/image/59/6c/59ac74796771897e0246a4532789076c.png?wh=1138x782 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/59/6c/59ac74796771897e0246a4532789076c.png?wh=1138x782"
        title="https://static001.geekbang.org/resource/image/59/6c/59ac74796771897e0246a4532789076c.png?wh=1138x782" />
这样一个存储敏感信息的 Secret 对象也就创建好了，而且因为它是保密的，使用 kubectl describe 不能直接看到内容，只能看到数据的大小，你可以和 ConfigMap 对比一下。</p>
<h3 id="如何使用">如何使用</h3>
<p>现在通过编写 YAML 文件，我们创建了 ConfigMap 和 Secret 对象，该怎么在 Kubernetes 里应用它们呢？
因为 ConfigMap 和 Secret 只是一些存储在 etcd 里的字符串，所以如果想要在运行时产生效果，就必须要以某种方式“注入”到 Pod 里，让应用去读取。在这方面的处理上 Kubernetes 和 Docker 是一样的，也是两种途径：环境变量和加载文件。
先看比较简单的环境变量。</p>
<h3 id="如何以环境变量的方式使用-configmapsecret">如何以环境变量的方式使用 ConfigMap/Secret</h3>
<p>在前面讲 Pod 的时候，说过描述容器的字段“containers”里有一个“env”，它定义了 Pod 里容器能够看到的环境变量。
当时我们只使用了简单的“value”，把环境变量的值写“死”在了 YAML 里，实际上它还可以使用另一个“valueFrom”字段，从 ConfigMap 或者 Secret 对象里获取值，这样就实现了把配置信息以环境变量的形式注入进 Pod，也就是配置与应用的解耦。
由于“valueFrom”字段在 YAML 里的嵌套层次比较深，初次使用最好看一下 kubectl explain 对它的说明：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl explain pod.spec.containers.env.valueFrom

</code></pre></td></tr></table>
</div>
</div><p>“valueFrom”字段指定了环境变量值的来源，可以是“configMapKeyRef”或者“secretKeyRef”，然后你要再进一步指定应用的 ConfigMap/Secret 的“name”和它里面的“key”，要当心的是这个“name”字段是 API 对象的名字，而不是 Key-Value 的名字。
下面我就把引用了 ConfigMap 和 Secret 对象的 Pod 列出来，给你做个示范，为了提醒你注意，我把“env”字段提到了前面：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: env-pod
spec:
  containers:

  - env:
    - name: COUNT
      valueFrom:
        configMapKeyRef:
          name: info
          key: count
    - name: GREETING
      valueFrom:
        configMapKeyRef:
          name: info
          key: greeting
    - name: USERNAME
      valueFrom:
        secretKeyRef:
          name: user
          key: name
    - name: PASSWORD
      valueFrom:
        secretKeyRef:
          name: user
          key: pwd
      image: busybox
      name: busy
      imagePullPolicy: IfNotPresent
      command: [&#34;/bin/sleep&#34;, &#34;300&#34;]
</code></pre></td></tr></table>
</div>
</div><p>这个 Pod 的名字是“env-pod”，镜像是“busybox”，执行命令 sleep 睡眠 300 秒，我们可以在这段时间里使用命令 kubectl exec 进入 Pod 观察环境变量。
你需要重点关注的是它的“env”字段，里面定义了 4 个环境变量，COUNT、GREETING、USERNAME、PASSWORD。
对于明文配置数据， COUNT、GREETING 引用的是 ConfigMap 对象，所以使用字段“configMapKeyRef”，里面的“name”是 ConfigMap 对象的名字，也就是之前我们创建的“info”，而“key”字段分别是“info”对象里的 count 和 greeting。
同样的对于机密配置数据， USERNAME、PASSWORD 引用的是 Secret 对象，要使用字段“secretKeyRef”，再用“name”指定 Secret 对象的名字 user，用“key”字段应用它里面的 name 和 pwd 。
这段解释确实是有点绕口令的感觉，因为 ConfigMap 和 Secret 在 Pod 里的组合关系不像 Job/CronJob 那么简单直接，所以我还是用画图来表示它们的引用关系：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661"
        data-srcset="https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661, https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661 1.5x, https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661"
        title="https://static001.geekbang.org/resource/image/06/9d/0663d692b33c1dee5b08e486d271b69d.jpg?wh=1920x1661" />
从这张图你就应该能够比较清楚地看出 Pod 与 ConfigMap、Secret 的“松耦合”关系，它们不是直接嵌套包含，而是使用“KeyRef”字段间接引用对象，这样，同一段配置信息就可以在不同的对象之间共享。
弄清楚了环境变量的注入方式之后，让我们用 kubectl apply 创建 Pod，再用 kubectl exec 进入 Pod，验证环境变量是否生效：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f env-pod.yml
kubectl exec -it env-pod -- sh
echo $COUNT
echo $GREETING
echo $USERNAME $PASSWORD
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6f/bb/6f0f711de995010498b6807709a811bb.png?wh=1202x660"
        data-srcset="https://static001.geekbang.org/resource/image/6f/bb/6f0f711de995010498b6807709a811bb.png?wh=1202x660, https://static001.geekbang.org/resource/image/6f/bb/6f0f711de995010498b6807709a811bb.png?wh=1202x660 1.5x, https://static001.geekbang.org/resource/image/6f/bb/6f0f711de995010498b6807709a811bb.png?wh=1202x660 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6f/bb/6f0f711de995010498b6807709a811bb.png?wh=1202x660"
        title="https://static001.geekbang.org/resource/image/6f/bb/6f0f711de995010498b6807709a811bb.png?wh=1202x660" />
这张截图就显示了 Pod 的运行结果，可以看到在 Pod 里使用 echo 命令确实输出了我们在两个 YAML 里定义的配置信息，也就证明 Pod 对象成功组合了 ConfigMap 和 Secret 对象。
以环境变量的方式使用 ConfigMap/Secret 还是比较简单的，下面来看第二种加载文件的方式。</p>
<h3 id="如何以-volume-的方式使用-configmapsecret">如何以 Volume 的方式使用 ConfigMap/Secret</h3>
<p>Kubernetes 为 Pod 定义了一个“Volume”的概念，可以翻译成是“存储卷”。如果把 Pod 理解成是一个虚拟机，那么 Volume 就相当于是虚拟机里的磁盘。
我们可以为 Pod“挂载（mount）”多个 Volume，里面存放供 Pod 访问的数据，这种方式有点类似 docker run -v，虽然用法复杂了一些，但功能也相应强大一些。
在 Pod 里挂载 Volume 很容易，只需要在“spec”里增加一个“volumes”字段，然后再定义卷的名字和引用的 ConfigMap/Secret 就可以了。要注意的是 Volume 属于 Pod，不属于容器，所以它和字段“containers”是同级的，都属于“spec”。
下面让我们来定义两个 Volume，分别引用 ConfigMap 和 Secret，名字是 cm-vol 和 sec-vol：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">spec:
  volumes:

  - name: cm-vol
    configMap:
      name: info
  - name: sec-vol
    secret:
      secretName: user
</code></pre></td></tr></table>
</div>
</div><p>有了 Volume 的定义之后，就可以在容器里挂载了，这要用到“volumeMounts”字段，正如它的字面含义，可以把定义好的 Volume 挂载到容器里的某个路径下，所以需要在里面用“mountPath”“name”明确地指定挂载路径和 Volume 的名字。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">containers:

  - volumeMounts:
    - mountPath: /tmp/cm-items
      name: cm-vol
    - mountPath: /tmp/sec-items
      name: sec-vol
</code></pre></td></tr></table>
</div>
</div><p>把“volumes”和“volumeMounts”字段都写好之后，配置信息就可以加载成文件了。这里我还是画了图来表示它们的引用关系：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/9d/yy/9d3258da1f40554ae88212db2b4yybyy.jpg?wh=1920x1630"
        data-srcset="https://static001.geekbang.org/resource/image/9d/yy/9d3258da1f40554ae88212db2b4yybyy.jpg?wh=1920x1630, https://static001.geekbang.org/resource/image/9d/yy/9d3258da1f40554ae88212db2b4yybyy.jpg?wh=1920x1630 1.5x, https://static001.geekbang.org/resource/image/9d/yy/9d3258da1f40554ae88212db2b4yybyy.jpg?wh=1920x1630 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/9d/yy/9d3258da1f40554ae88212db2b4yybyy.jpg?wh=1920x1630"
        title="https://static001.geekbang.org/resource/image/9d/yy/9d3258da1f40554ae88212db2b4yybyy.jpg?wh=1920x1630" />
你可以看到，挂载 Volume 的方式和环境变量又不太相同。环境变量是直接引用了 ConfigMap/Secret，而 Volume 又多加了一个环节，需要先用 Volume 引用 ConfigMap/Secret，然后在容器里挂载 Volume，有点“兜圈子”“弯弯绕”。
这种方式的好处在于：以 Volume 的概念统一抽象了所有的存储，不仅现在支持 ConfigMap/Secret，以后还能够支持临时卷、持久卷、动态卷、快照卷等许多形式的存储，扩展性非常好。
现在我把 Pod 的完整 YAML 描述列出来，然后使用 kubectl apply 创建它：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
name: vol-pod
spec:
volumes:

  - name: cm-vol
    configMap:
      name: info
  - name: sec-vol
    secret:
      secretName: user
      containers:
  - volumeMounts:
    - mountPath: /tmp/cm-items
      name: cm-vol
    - mountPath: /tmp/sec-items
      name: sec-vol
      image: busybox
      name: busy
      imagePullPolicy: IfNotPresent
      command: [&#34;/bin/sleep&#34;, &#34;300&#34;]
</code></pre></td></tr></table>
</div>
</div><p>创建之后，我们还是用 kubectl exec 进入 Pod，看看配置信息被加载成了什么形式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f vol-pod.yml
kubectl get pod
kubectl exec -it vol-pod -- sh
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/9f/67/9fdc3a7bafcfa0fa277b7c7bed891967.png?wh=1192x728"
        data-srcset="https://static001.geekbang.org/resource/image/9f/67/9fdc3a7bafcfa0fa277b7c7bed891967.png?wh=1192x728, https://static001.geekbang.org/resource/image/9f/67/9fdc3a7bafcfa0fa277b7c7bed891967.png?wh=1192x728 1.5x, https://static001.geekbang.org/resource/image/9f/67/9fdc3a7bafcfa0fa277b7c7bed891967.png?wh=1192x728 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/9f/67/9fdc3a7bafcfa0fa277b7c7bed891967.png?wh=1192x728"
        title="https://static001.geekbang.org/resource/image/9f/67/9fdc3a7bafcfa0fa277b7c7bed891967.png?wh=1192x728" />
你会看到，ConfigMap 和 Secret 都变成了目录的形式，而它们里面的 Key-Value 变成了一个个的文件，而文件名就是 Key。
因为这种形式上的差异，以 Volume 的方式来使用 ConfigMap/Secret，就和环境变量不太一样。环境变量用法简单，更适合存放简短的字符串，而 Volume 更适合存放大数据量的配置文件，在 Pod 里加载成文件后让应用直接读取使用。</p>
<h3 id="小结-13">小结</h3>
<p>好了，今天我们学习了两种在 Kubernetes 里管理配置信息的 API 对象 ConfigMap 和 Secret，它们分别代表了明文信息和机密敏感信息，存储在 etcd 里，在需要的时候可以注入 Pod 供 Pod 使用。
简单小结一下今天的要点：
ConfigMap 记录了一些 Key-Value 格式的字符串数据，描述字段是“data”，不是“spec”。
Secret 与 ConfigMap 很类似，也使用“data”保存字符串数据，但它要求数据必须是 Base64 编码，起到一定的保密效果。
在 Pod 的“env.valueFrom”字段中可以引用 ConfigMap 和 Secret，把它们变成应用可以访问的环境变量。
在 Pod 的“spec.volumes”字段中可以引用 ConfigMap 和 Secret，把它们变成存储卷，然后在“spec.containers.volumeMounts”字段中加载成文件的形式。
ConfigMap 和 Secret 对存储数据的大小没有限制，但小数据用环境变量比较适合，大数据应该用存储卷，可根据具体场景灵活应用。</p>
<h2 id="15实战演练玩转kubernetes1">15｜实战演练：玩转Kubernetes（1）</h2>
<p>经过两个星期的学习，到今天我们的“初级篇”也快要结束了。
和之前的“入门篇”一样，在这次课里，我也会对前面学过的知识做一个比较全面的回顾，毕竟 Kubernetes 领域里有很多新名词、新术语、新架构，知识点多且杂，这样的总结复习就更有必要。
接下来我还是先简要列举一下“初级篇”里讲到的 Kubernetes 要点，然后再综合运用这些知识，演示一个实战项目——还是搭建 WordPress 网站，不过这次不是在 Docker 里，而是在 Kubernetes 集群里。</p>
<h3 id="kubernetes-技术要点回顾">Kubernetes 技术要点回顾</h3>
<p>容器技术开启了云原生的大潮，但成熟的容器技术，到生产环境的应用部署的时候，却显得“步履维艰”。因为容器只是针对单个进程的隔离和封装，而实际的应用场景却是要求许多的应用进程互相协同工作，其中的各种关系和需求非常复杂，在容器这个技术层次很难掌控。
为了解决这个问题，容器编排（Container Orchestration）就出现了，它可以说是以前的运维工作在云原生世界的落地实践，本质上还是在集群里调度管理应用程序，只不过管理的主体由人变成了计算机，管理的目标由原生进程变成了容器和镜像。
而现在，容器编排领域的王者就是——Kubernetes。
Kubernetes 源自 Borg 系统，它凝聚了 Google 的内部经验和 CNCF 的社区智慧，所以战胜了竞争对手 Apache Mesos 和 Docker Swarm，成为了容器编排领域的事实标准，也成为了云原生时代的基础操作系统，学习云原生就必须要掌握 Kubernetes。
（10 讲）Kubernetes 的 Master/Node 架构是它具有自动化运维能力的关键，也对我们的学习至关重要，这里我再用另一张参考架构图来简略说明一下它的运行机制（图片来源）：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/f4/05/f429ca7114eebf140632409f3fbcbb05.png?wh=1475x852"
        data-srcset="https://static001.geekbang.org/resource/image/f4/05/f429ca7114eebf140632409f3fbcbb05.png?wh=1475x852, https://static001.geekbang.org/resource/image/f4/05/f429ca7114eebf140632409f3fbcbb05.png?wh=1475x852 1.5x, https://static001.geekbang.org/resource/image/f4/05/f429ca7114eebf140632409f3fbcbb05.png?wh=1475x852 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/f4/05/f429ca7114eebf140632409f3fbcbb05.png?wh=1475x852"
        title="https://static001.geekbang.org/resource/image/f4/05/f429ca7114eebf140632409f3fbcbb05.png?wh=1475x852" />
Kubernetes 把集群里的计算资源定义为节点（Node），其中又划分成控制面和数据面两类。
控制面是 Master 节点，负责管理集群和运维监控应用，里面的核心组件是 apiserver、etcd、scheduler、controller-manager。
数据面是 Worker 节点，受 Master 节点的管控，里面的核心组件是 kubelet、kube-proxy、container-runtime。
此外，Kubernetes 还支持插件机制，能够灵活扩展各项功能，常用的插件有 DNS 和 Dashboard。
为了更好地管理集群和业务应用，Kubernetes 从现实世界中抽象出了许多概念，称为“API 对象”，描述这些对象就需要使用 YAML 语言。
YAML 是 JSON 的超集，但语法更简洁，表现能力更强，更重要的是它以“声明式”来表述对象的状态，不涉及具体的操作细节，这样 Kubernetes 就能够依靠存储在 etcd 里集群的状态信息，不断地“调控”对象，直至实际状态与期望状态相同，这个过程就是 Kubernetes 的自动化运维管理（11 讲）。
Kubernetes 里有很多的 API 对象，其中最核心的对象是“Pod”，它捆绑了一组存在密切协作关系的容器，容器之间共享网络和存储，在集群里必须一起调度一起运行。通过 Pod 这个概念，Kubernetes 就简化了对容器的管理工作，其他的所有任务都是通过对 Pod 这个最小单位的再包装来实现的（12 讲）。
除了核心的 Pod 对象，基于“单一职责”和“对象组合”这两个基本原则，我们又学习了 4 个比较简单的 API 对象，分别是 Job/CronJob 和 ConfigMap/Secret。
Job/CronJob 对应的是离线作业，它们逐层包装了 Pod，添加了作业控制和定时规则（13 讲）。
ConfigMap/Secret 对应的是配置信息，需要以环境变量或者存储卷的形式注入进 Pod，然后进程才能在运行时使用（14 讲）。
和 Docker 类似，Kubernetes 也提供一个客户端工具，名字叫“kubectl”，它直接与 Master 节点的 apiserver 通信，把 YAML 文件发送给 RESTful 接口，从而触发 Kubernetes 的对象管理工作流程。
kubectl 的命令很多，查看自带文档可以用 api-resources、explain ，查看对象状态可以用 get、describe、logs ，操作对象可以用 run、apply、exec、delete 等等（09 讲)。
使用 YAML 描述 API 对象也有固定的格式，必须写的“头字段”是“apiVersion”“kind”“metadata”，它们表示对象的版本、种类和名字等元信息。实体对象如 Pod、Job、CronJob 会再有“spec”字段描述对象的期望状态，最基本的就是容器信息，非实体对象如 ConfigMap、Secret 使用的是“data”字段，记录一些静态的字符串信息。
好了，“初级篇”里的 Kubernetes 知识要点我们就基本总结完了，如果你发现哪部分不太清楚，可以课后再多复习一下前面的课程加以巩固。</p>
<h3 id="wordpress-网站基本架构">WordPress 网站基本架构</h3>
<p>下面我们就在 Kubernetes 集群里再搭建出一个 WordPress 网站，用的镜像还是“入门篇”里的那三个应用：WordPress、MariaDB、Nginx，不过当时我们是直接以容器的形式来使用它们，现在要改成 Pod 的形式，让它们运行在 Kubernetes 里。
我还是画了一张简单的架构图，来说明这个系统的内部逻辑关系：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3d/cc/3d9d09078f1200a84c63a7cea2f40bcc.jpg?wh=1920x865"
        data-srcset="https://static001.geekbang.org/resource/image/3d/cc/3d9d09078f1200a84c63a7cea2f40bcc.jpg?wh=1920x865, https://static001.geekbang.org/resource/image/3d/cc/3d9d09078f1200a84c63a7cea2f40bcc.jpg?wh=1920x865 1.5x, https://static001.geekbang.org/resource/image/3d/cc/3d9d09078f1200a84c63a7cea2f40bcc.jpg?wh=1920x865 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3d/cc/3d9d09078f1200a84c63a7cea2f40bcc.jpg?wh=1920x865"
        title="img" /></p>
<p>从这张图中你可以看到，网站的大体架构是没有变化的，毕竟应用还是那三个，它们的调用依赖关系也必然没有变化。
那么 Kubernetes 系统和 Docker 系统的区别又在哪里呢？
关键就在对应用的封装和网络环境这两点上。
现在 WordPress、MariaDB 这两个应用被封装成了 Pod（由于它们都是在线业务，所以 Job/CronJob 在这里派不上用场），运行所需的环境变量也都被改写成 ConfigMap，统一用“声明式”来管理，比起 Shell 脚本更容易阅读和版本化管理。
另外，Kubernetes 集群在内部维护了一个自己的专用网络，这个网络和外界隔离，要用特殊的“端口转发”方式来传递数据，还需要在集群之外用 Nginx 反向代理这个地址，这样才能实现内外沟通，对比 Docker 的直接端口映射，这里略微麻烦了一些。</p>
<h3 id="wordpress-网站搭建步骤">WordPress 网站搭建步骤</h3>
<p>了解基本架构之后，接下来我们就逐步搭建这个网站系统，总共需要 4 步。
第一步当然是要编排 MariaDB 对象，它的具体运行需求可以参考“入门篇”的实战演练课，这里我就不再重复了。
MariaDB 需要 4 个环境变量，比如数据库名、用户名、密码等，在 Docker 里我们是在命令行里使用参数 &ndash;env，而在 Kubernetes 里我们就应该使用 ConfigMap，为此需要定义一个 maria-cm 对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ConfigMap
metadata:
  name: maria-cm
data:
  DATABASE: &#39;db&#39;
  USER: &#39;wp&#39;
  PASSWORD: &#39;123&#39;
  ROOT_PASSWORD: &#39;123&#39;
</code></pre></td></tr></table>
</div>
</div><p>然后我们定义 Pod 对象 maria-pod，把配置信息注入 Pod，让 MariaDB 运行时从环境变量读取这些信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: maria-pod
  labels:
    app: wordpress
    role: database
spec:
  containers:

  - image: mariadb:10
    name: maria
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 3306
      envFrom:
    - prefix: &#39;MARIADB_&#39;
      configMapRef:
        name: maria-cm
</code></pre></td></tr></table>
</div>
</div><p>注意这里我们使用了一个新的字段“envFrom”，这是因为 ConfigMap 里的信息比较多，如果用 env.valueFrom 一个个地写会非常麻烦，容易出错，而 envFrom 可以一次性地把 ConfigMap 里的字段全导入进 Pod，并且能够指定变量名的前缀（即这里的 MARIADB_），非常方便。
使用 kubectl apply 创建这个对象之后，可以用 kubectl get pod 查看它的状态，如果想要获取 IP 地址需要加上参数 -o wide ：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f mariadb-pod.yml
kubectl get pod -o wide
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3f/98/3fb0242f97c782f79ecf8ba845c81798.png?wh=1788x362"
        data-srcset="https://static001.geekbang.org/resource/image/3f/98/3fb0242f97c782f79ecf8ba845c81798.png?wh=1788x362, https://static001.geekbang.org/resource/image/3f/98/3fb0242f97c782f79ecf8ba845c81798.png?wh=1788x362 1.5x, https://static001.geekbang.org/resource/image/3f/98/3fb0242f97c782f79ecf8ba845c81798.png?wh=1788x362 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3f/98/3fb0242f97c782f79ecf8ba845c81798.png?wh=1788x362"
        title="img" /></p>
<p>现在数据库就成功地在 Kubernetes 集群里跑起来了，IP 地址是“172.17.0.2”，注意这个地址和 Docker 的不同，是 Kubernetes 里的私有网段。
接着是第二步，编排 WordPress 对象，还是先用 ConfigMap 定义它的环境变量：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ConfigMap
metadata:
  name: wp-cm
data:
  HOST: &#39;172.17.0.2&#39;
  USER: &#39;wp&#39;
  PASSWORD: &#39;123&#39;
  NAME: &#39;db&#39;
</code></pre></td></tr></table>
</div>
</div><p>在这个 ConfigMap 里要注意的是“HOST”字段，它必须是 MariaDB Pod 的 IP 地址，如果不写正确 WordPress 会无法正常连接数据库。
然后我们再编写 WordPress 的 YAML 文件，为了简化环境变量的设置同样使用了 envFrom：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: wp-pod
  labels:
    app: wordpress
    role: website
spec:
  containers:

  - image: wordpress:5
    name: wp-pod
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 80
      envFrom:
    - prefix: &#39;WORDPRESS_DB_&#39;
      configMapRef:
        name: wp-cm
</code></pre></td></tr></table>
</div>
</div><p>接着还是用 kubectl apply 创建对象，kubectl get pod 查看它的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f wp-pod.yml
kubectl get pod -o wide
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d5/de/d5e8c09e70e90179d651bf3c28abc0de.png?wh=1562x426"
        data-srcset="https://static001.geekbang.org/resource/image/d5/de/d5e8c09e70e90179d651bf3c28abc0de.png?wh=1562x426, https://static001.geekbang.org/resource/image/d5/de/d5e8c09e70e90179d651bf3c28abc0de.png?wh=1562x426 1.5x, https://static001.geekbang.org/resource/image/d5/de/d5e8c09e70e90179d651bf3c28abc0de.png?wh=1562x426 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d5/de/d5e8c09e70e90179d651bf3c28abc0de.png?wh=1562x426"
        title="https://static001.geekbang.org/resource/image/d5/de/d5e8c09e70e90179d651bf3c28abc0de.png?wh=1562x426" /></p>
<p>第三步是为 WordPress Pod 映射端口号，让它在集群外可见。
因为 Pod 都是运行在 Kubernetes 内部的私有网段里的，外界无法直接访问，想要对外暴露服务，需要使用一个专门的 kubectl port-forward 命令，它专门负责把本机的端口映射到在目标对象的端口号，有点类似 Docker 的参数 -p，经常用于 Kubernetes 的临时调试和测试。
下面我就把本地的“8080”映射到 WordPress Pod 的“80”，kubectl 会把这个端口的所有数据都转发给集群内部的 Pod：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl port-forward wp-pod 8080:80 &amp;
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d4/be/d445d205ae6f8c966200ffa9ba7f29be.png?wh=1366x240"
        data-srcset="https://static001.geekbang.org/resource/image/d4/be/d445d205ae6f8c966200ffa9ba7f29be.png?wh=1366x240, https://static001.geekbang.org/resource/image/d4/be/d445d205ae6f8c966200ffa9ba7f29be.png?wh=1366x240 1.5x, https://static001.geekbang.org/resource/image/d4/be/d445d205ae6f8c966200ffa9ba7f29be.png?wh=1366x240 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d4/be/d445d205ae6f8c966200ffa9ba7f29be.png?wh=1366x240"
        title="img" /></p>
<p>注意在命令的末尾我使用了一个 &amp; 符号，让端口转发工作在后台进行，这样就不会阻碍我们后续的操作。
如果想关闭端口转发，需要敲命令 fg ，它会把后台的任务带回到前台，然后就可以简单地用“Ctrl + C”来停止转发了。
第四步是创建反向代理的 Nginx，让我们的网站对外提供服务。
这是因为 WordPress 网站使用了 URL 重定向，直接使用“8080”会导致跳转故障，所以为了让网站正常工作，我们还应该在 Kubernetes 之外启动 Nginx 反向代理，保证外界看到的仍然是“80”端口号。（这里的细节和我们的课程关系不大，感兴趣的同学可以留言提问讨论)
Nginx 的配置文件和第 7 讲基本一样，只是目标地址变成了“127.0.0.1:8080”，它就是我们在第三步里用 kubectl port-forward 命令创建的本地地址：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">server {
  listen 80;
  default_type text/html;
  location / {
      proxy_http_version 1.1;
      proxy_set_header Host $host;
      proxy_pass http://127.0.0.1:8080;
  }
}
</code></pre></td></tr></table>
</div>
</div><p>然后我们用 docker run -v 命令加载这个配置文件，以容器的方式启动这个 Nginx 代理：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d --rm \
    --net=host \
    -v /tmp/proxy.conf:/etc/nginx/conf.d/default.conf \
    nginx:alpine
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/9f/51/9f2b16fb58dbe0a358e26042565f9851.png?wh=1920x238"
        data-srcset="https://static001.geekbang.org/resource/image/9f/51/9f2b16fb58dbe0a358e26042565f9851.png?wh=1920x238, https://static001.geekbang.org/resource/image/9f/51/9f2b16fb58dbe0a358e26042565f9851.png?wh=1920x238 1.5x, https://static001.geekbang.org/resource/image/9f/51/9f2b16fb58dbe0a358e26042565f9851.png?wh=1920x238 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/9f/51/9f2b16fb58dbe0a358e26042565f9851.png?wh=1920x238"
        title="img" /></p>
<p>有了 Nginx 的反向代理之后，我们就可以打开浏览器，输入本机的“127.0.0.1”或者是虚拟机的 IP 地址（我这里仍然是“http://192.168.10.208”)，看到 WordPress 的界面：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/73/f4/735552be9cf6d45ac41a001252ayyef4.png?wh=1524x1858"
        data-srcset="https://static001.geekbang.org/resource/image/73/f4/735552be9cf6d45ac41a001252ayyef4.png?wh=1524x1858, https://static001.geekbang.org/resource/image/73/f4/735552be9cf6d45ac41a001252ayyef4.png?wh=1524x1858 1.5x, https://static001.geekbang.org/resource/image/73/f4/735552be9cf6d45ac41a001252ayyef4.png?wh=1524x1858 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/73/f4/735552be9cf6d45ac41a001252ayyef4.png?wh=1524x1858"
        title="img" /></p>
<p>你也可以在 Kubernetes 里使用命令 kubectl logs 查看 WordPress、MariaDB 等 Pod 的运行日志，来验证它们是否已经正确地响应了请求：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/84/62/8498c598e6f3142d490218601acdbc62.png?wh=1920x809"
        data-srcset="https://static001.geekbang.org/resource/image/84/62/8498c598e6f3142d490218601acdbc62.png?wh=1920x809, https://static001.geekbang.org/resource/image/84/62/8498c598e6f3142d490218601acdbc62.png?wh=1920x809 1.5x, https://static001.geekbang.org/resource/image/84/62/8498c598e6f3142d490218601acdbc62.png?wh=1920x809 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/84/62/8498c598e6f3142d490218601acdbc62.png?wh=1920x809"
        title="img" /></p>
<h3 id="使用-dashboard-管理-kubernetes">使用 Dashboard 管理 Kubernetes</h3>
<p>到这里 WordPress 网站就搭建成功了，我们的主要任务也算是完成了，不过我还想再带你看看 Kubernetes 的图形管理界面，也就是 Dashboard，看看不用命令行该怎么管理 Kubernetes。
启动 Dashboard 的命令你还记得吗，在第 10 节课里讲插件的时候曾经说过，需要用 minikube，命令是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">minikube dashboard

</code></pre></td></tr></table>
</div>
</div><p>它会自动打开浏览器界面，显示出当前 Kubernetes 集群里的工作负载：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/53/59/536eeb176a7737c9ed815c10af0fcf59.png?wh=1920x1022"
        data-srcset="https://static001.geekbang.org/resource/image/53/59/536eeb176a7737c9ed815c10af0fcf59.png?wh=1920x1022, https://static001.geekbang.org/resource/image/53/59/536eeb176a7737c9ed815c10af0fcf59.png?wh=1920x1022 1.5x, https://static001.geekbang.org/resource/image/53/59/536eeb176a7737c9ed815c10af0fcf59.png?wh=1920x1022 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/53/59/536eeb176a7737c9ed815c10af0fcf59.png?wh=1920x1022"
        title="img" /></p>
<p>点击任意一个 Pod 的名字，就会进入管理界面，可以看到 Pod 的详细信息，而右上角有 4 个很重要的功能，分别可以查看日志、进入 Pod 内部、编辑 Pod 和删除 Pod，相当于执行 logs、exec、edit、delete 命令，但要比命令行要直观友好的多：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d5/28/d5e5131bfb1d6aae2f026177bf283628.png?wh=1920x781"
        data-srcset="https://static001.geekbang.org/resource/image/d5/28/d5e5131bfb1d6aae2f026177bf283628.png?wh=1920x781, https://static001.geekbang.org/resource/image/d5/28/d5e5131bfb1d6aae2f026177bf283628.png?wh=1920x781 1.5x, https://static001.geekbang.org/resource/image/d5/28/d5e5131bfb1d6aae2f026177bf283628.png?wh=1920x781 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d5/28/d5e5131bfb1d6aae2f026177bf283628.png?wh=1920x781"
        title="img" /></p>
<p>比如说，我点击了第二个按钮，就会在浏览器里开启一个 Shell 窗口，直接就是 Pod 的内部 Linux 环境，在里面可以输入任意的命令，无论是查看状态还是调试都很方便：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/46/4c/466c67a48616c946505242d0796ed74c.png?wh=1820x1240"
        data-srcset="https://static001.geekbang.org/resource/image/46/4c/466c67a48616c946505242d0796ed74c.png?wh=1820x1240, https://static001.geekbang.org/resource/image/46/4c/466c67a48616c946505242d0796ed74c.png?wh=1820x1240 1.5x, https://static001.geekbang.org/resource/image/46/4c/466c67a48616c946505242d0796ed74c.png?wh=1820x1240 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/46/4c/466c67a48616c946505242d0796ed74c.png?wh=1820x1240"
        title="img" /></p>
<p>ConfigMap/Secret 等对象也可以在这里任意查看或编辑：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/de/22/defyybc05ed793b7966e1f6b68018022.png?wh=1312x976"
        data-srcset="https://static001.geekbang.org/resource/image/de/22/defyybc05ed793b7966e1f6b68018022.png?wh=1312x976, https://static001.geekbang.org/resource/image/de/22/defyybc05ed793b7966e1f6b68018022.png?wh=1312x976 1.5x, https://static001.geekbang.org/resource/image/de/22/defyybc05ed793b7966e1f6b68018022.png?wh=1312x976 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/de/22/defyybc05ed793b7966e1f6b68018022.png?wh=1312x976"
        title="img" /></p>
<p>Dashboard 里的可操作的地方还有很多，这里我只是一个非常简单的介绍。虽然你也许已经习惯了使用键盘和命令行，但偶尔换一换口味，改用鼠标和图形界面来管理 Kubernetes 也是件挺有意思的事情，有机会不妨尝试一下。</p>
<h3 id="小结-14">小结</h3>
<p>好了，作为“初级篇”的最后一节课，今天我们回顾了一下 Kubernetes 的知识要点，我还是画一份详细的思维导图，帮助你课后随时复习总结。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/87/1f/87a1d338340c8ca771a97d0fyy4b611f.jpg?wh=1920x1877"
        data-srcset="https://static001.geekbang.org/resource/image/87/1f/87a1d338340c8ca771a97d0fyy4b611f.jpg?wh=1920x1877, https://static001.geekbang.org/resource/image/87/1f/87a1d338340c8ca771a97d0fyy4b611f.jpg?wh=1920x1877 1.5x, https://static001.geekbang.org/resource/image/87/1f/87a1d338340c8ca771a97d0fyy4b611f.jpg?wh=1920x1877 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/87/1f/87a1d338340c8ca771a97d0fyy4b611f.jpg?wh=1920x1877"
        title="img" /></p>
<p>这节课里我们使用 Kubernetes 搭建了 WordPress 网站，和第 7 讲里的 Docker 比较起来，我们应用了容器编排技术，以“声明式”的 YAML 来描述应用的状态和它们之间的关系，而不会列出详细的操作步骤，这就降低了我们的心智负担——调度、创建、监控等杂事都交给 Kubernetes 处理，我们只需“坐享其成”。
虽然我们朝着云原生的方向迈出了一大步，不过现在我们的容器编排还不够完善，Pod 的 IP 地址还必须手工查找填写，缺少自动的服务发现机制，另外对外暴露服务的方式还很原始，必须要依赖集群外部力量的帮助。
所以，我们的学习之旅还将继续，在接下来的“中级篇”里，会开始研究更多的 API 对象，来解决这里遇到的问题。</p>
<h2 id="16视频初级篇实操总结">16｜视频：初级篇实操总结</h2>
<p><a href="https://time.geekbang.org/column/article/534688" target="_blank" rel="noopener noreffer">https://time.geekbang.org/column/article/534688</a></p>
<h2 id="17更真实的云原生实际搭建多节点的kubernetes集群">17｜更真实的云原生：实际搭建多节点的Kubernetes集群</h2>
<p>到今天，你学习这个专栏的进度就已经过半了，在前面的“入门篇”我们了解了 Docker 和容器技术，在“初级篇”我们掌握了 Kubernetes 的基本对象、原理和操作方法，一路走下来收获很多。
现在你应该对 Kubernetes 和容器编排有了一些初步的认识，那么接下来，让我们继续深入研究 Kubernetes 的其他 API 对象，也就是那些在 Docker 中不存在的但对云计算、集群管理至关重要的概念。
不过在那之前，我们还需要有一个比 minikube 更真实的 Kubernetes 环境，它应该是一个多节点的 Kubernetes 集群，这样更贴近现实中的生产系统，能够让我们尽快地拥有实际的集群使用经验。
所以在今天的这节课里，我们就来暂时忘掉 minikube，改用 kubeadm（https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/）搭建出一个新的 Kubernetes 集群，一起来看看更真实的云原生环境。</p>
<h3 id="什么是-kubeadm">什么是 kubeadm</h3>
<p>前面的几节课里我们使用的都是 minikube，它非常简单易用，不需要什么配置工作，就能够在单机环境里创建出一个功能完善的 Kubernetes 集群，给学习、开发、测试都带来了极大的便利。
不过 minikube 还是太“迷你”了，方便的同时也隐藏了很多细节，离真正生产环境里的计算集群有一些差距，毕竟许多需求、任务只有在多节点的大集群里才能够遇到，相比起来，minikube 真的只能算是一个“玩具”。
那么，多节点的 Kubernetes 集群是怎么从无到有地创建出来的呢？
第 10 讲说过 Kubernetes 是很多模块构成的，而实现核心功能的组件像 apiserver、etcd、scheduler 等本质上都是可执行文件，所以也可以采用和其他系统差不多的方式，使用 Shell 脚本或者 Ansible 等工具打包发布到服务器上。
不过 Kubernetes 里的这些组件的配置和相互关系实在是太复杂了，用 Shell、Ansible 来部署的难度很高，需要具有相当专业的运维管理知识才能配置、搭建好集群，而且即使这样，搭建的过程也非常麻烦。
为了简化 Kubernetes 的部署工作，让它能够更“接地气”，社区里就出现了一个专门用来在集群中安装 Kubernetes 的工具，名字就叫“kubeadm”，意思就是“Kubernetes 管理员”。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/f2/88/f27c7938cba21215621ac33635d63288.jpg?wh=1044x640"
        data-srcset="https://static001.geekbang.org/resource/image/f2/88/f27c7938cba21215621ac33635d63288.jpg?wh=1044x640, https://static001.geekbang.org/resource/image/f2/88/f27c7938cba21215621ac33635d63288.jpg?wh=1044x640 1.5x, https://static001.geekbang.org/resource/image/f2/88/f27c7938cba21215621ac33635d63288.jpg?wh=1044x640 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/f2/88/f27c7938cba21215621ac33635d63288.jpg?wh=1044x640"
        title="img" /></p>
<p>kubeadm，原理和 minikube 类似，也是用容器和镜像来封装 Kubernetes 的各种组件，但它的目标不是单机部署，而是要能够轻松地在集群环境里部署 Kubernetes，并且让这个集群接近甚至达到生产级质量。
而在保持这个高水准的同时，kubeadm 还具有了和 minikube 一样的易用性，只要很少的几条命令，如 init、join、upgrade、reset 就能够完成 Kubernetes 集群的管理维护工作，这让它不仅适用于集群管理员，也适用于开发、测试人员。</p>
<h3 id="实验环境的架构是什么样的">实验环境的架构是什么样的</h3>
<p>在使用 kubeadm 搭建实验环境之前，我们先来看看集群的架构设计，也就是说要准备好集群所需的硬件设施。
这里我画了一张系统架构图，图里一共有 3 台主机，当然它们都是使用虚拟机软件 VirtualBox/VMWare 虚拟出来的，下面我来详细说明一下：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/yy/3e/yyf5db64d398b4d5dyyd5e8e23ece53e.jpg?wh=1920x1294"
        data-srcset="https://static001.geekbang.org/resource/image/yy/3e/yyf5db64d398b4d5dyyd5e8e23ece53e.jpg?wh=1920x1294, https://static001.geekbang.org/resource/image/yy/3e/yyf5db64d398b4d5dyyd5e8e23ece53e.jpg?wh=1920x1294 1.5x, https://static001.geekbang.org/resource/image/yy/3e/yyf5db64d398b4d5dyyd5e8e23ece53e.jpg?wh=1920x1294 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/yy/3e/yyf5db64d398b4d5dyyd5e8e23ece53e.jpg?wh=1920x1294"
        title="img" /></p>
<p>所谓的多节点集群，要求服务器应该有两台或者更多，为了简化我们只取最小值，所以这个 Kubernetes 集群就只有两台主机，一台是 Master 节点，另一台是 Worker 节点。当然，在完全掌握了 kubeadm 的用法之后，你可以在这个集群里添加更多的节点。
Master 节点需要运行 apiserver、etcd、scheduler、controller-manager 等组件，管理整个集群，所以对配置要求比较高，至少是 2 核 CPU、4GB 的内存。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d1/3c/d19a8ceafd4db10a5yy35c623384ba3c.png?wh=1504x920"
        data-srcset="https://static001.geekbang.org/resource/image/d1/3c/d19a8ceafd4db10a5yy35c623384ba3c.png?wh=1504x920, https://static001.geekbang.org/resource/image/d1/3c/d19a8ceafd4db10a5yy35c623384ba3c.png?wh=1504x920 1.5x, https://static001.geekbang.org/resource/image/d1/3c/d19a8ceafd4db10a5yy35c623384ba3c.png?wh=1504x920 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d1/3c/d19a8ceafd4db10a5yy35c623384ba3c.png?wh=1504x920"
        title="img" /></p>
<p>而 Worker 节点没有管理工作，只运行业务应用，所以配置可以低一些，为了节省资源我给它分配了 1 核 CPU 和 1GB 的内存，可以说是低到不能再低了。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/ee/f3/eeee60b6e29d7b6c4c74f913ac663ef3.png?wh=1504x1024"
        data-srcset="https://static001.geekbang.org/resource/image/ee/f3/eeee60b6e29d7b6c4c74f913ac663ef3.png?wh=1504x1024, https://static001.geekbang.org/resource/image/ee/f3/eeee60b6e29d7b6c4c74f913ac663ef3.png?wh=1504x1024 1.5x, https://static001.geekbang.org/resource/image/ee/f3/eeee60b6e29d7b6c4c74f913ac663ef3.png?wh=1504x1024 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/ee/f3/eeee60b6e29d7b6c4c74f913ac663ef3.png?wh=1504x1024"
        title="img" /></p>
<p>基于模拟生产环境的考虑，在 Kubernetes 集群之外还需要有一台起辅助作用的服务器。
它的名字叫 Console，意思是控制台，我们要在上面安装命令行工具 kubectl，所有对 Kubernetes 集群的管理命令都是从这台主机发出去的。这也比较符合实际情况，因为安全的原因，集群里的主机部署好之后应该尽量少直接登录上去操作。
要提醒你的是，Console 这台主机只是逻辑上的概念，不一定要是独立，你在实际安装部署的时候完全可以复用之前 minikube 的虚拟机，或者直接使用 Master/Worker 节点作为控制台。
这 3 台主机共同组成了我们的实验环境，所以在配置的时候要注意它们的网络选项，必须是在同一个网段，你可以再回顾一下课前准备，保证它们使用的是同一个“Host-Only”（VirtualBox）或者“自定”（VMWare Fusion）网络。</p>
<h3 id="安装前的准备工作">安装前的准备工作</h3>
<p>不过有了架构图里的这些主机之后，我们还不能立即开始使用 kubeadm 安装 Kubernetes，因为 Kubernetes 对系统有一些特殊要求，我们必须还要在 Master 和 Worker 节点上做一些准备。
这些工作的详细信息你都可以在 Kubernetes 的官网上找到，但它们分散在不同的文档里，比较凌乱，所以我把它们整合到了这里，包括改主机名、改 Docker 配置、改网络设置、改交换分区这四步。
第一，由于 Kubernetes 使用主机名来区分集群里的节点，所以每个节点的 hostname 必须不能重名。你需要修改“/etc/hostname”这个文件，把它改成容易辨识的名字，比如 Master 节点就叫 master，Worker 节点就叫 worker：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo vi /etc/hostname

</code></pre></td></tr></table>
</div>
</div><p>第二，虽然 Kubernetes 目前支持多种容器运行时，但 Docker 还是最方便最易用的一种，所以我们仍然继续使用 Docker 作为 Kubernetes 的底层支持，使用 apt 安装 Docker Engine（可参考第 1 讲）。
安装完成后需要你再对 Docker 的配置做一点修改，在“/etc/docker/daemon.json”里把 cgroup 的驱动程序改成 systemd ，然后重启 Docker 的守护进程，具体的操作我列在了下面：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json
{
  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
  &#34;log-driver&#34;: &#34;json-file&#34;,
  &#34;log-opts&#34;: {
    &#34;max-size&#34;: &#34;100m&#34;
  },
  &#34;storage-driver&#34;: &#34;overlay2&#34;
}
EOF
sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre></td></tr></table>
</div>
</div><p>第三，为了让 Kubernetes 能够检查、转发网络流量，你需要修改 iptables 的配置，启用“br_netfilter”模块：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward=1 # better than modify /etc/sysctl.conf
EOF
sudo sysctl --system
</code></pre></td></tr></table>
</div>
</div><p>第四，你需要修改“/etc/fstab”，关闭 Linux 的 swap 分区，提升 Kubernetes 的性能：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo swapoff -a
sudo sed -ri &#39;/\sswap\s/s/^#?/#/&#39; /etc/fstab
</code></pre></td></tr></table>
</div>
</div><p>完成之后，最好记得重启一下系统，然后给虚拟机拍个快照做备份，避免后续的操作失误导致重复劳动。</p>
<h3 id="安装-kubeadm">安装 kubeadm</h3>
<p>好，现在我们就要安装 kubeadm 了，在 Master 节点和 Worker 节点上都要做这一步。
kubeadm 可以直接从 Google 自己的软件仓库下载安装，但国内的网络不稳定，很难下载成功，需要改用其他的软件源，这里我选择了国内的某云厂商：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo apt install -y apt-transport-https ca-certificates curl
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -
cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
sudo apt update
</code></pre></td></tr></table>
</div>
</div><p>更新了软件仓库，我们就可以用 apt install 获取 kubeadm、kubelet 和 kubectl 这三个安装必备工具了。apt 默认会下载最新版本，但我们也可以指定版本号，比如使用和 minikube 相同的“1.23.3”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo apt install -y kubeadm=1.23.3-00 kubelet=1.23.3-00 kubectl=1.23.3-00

</code></pre></td></tr></table>
</div>
</div><p>安装完成之后，你可以用 kubeadm version、kubectl version 来验证版本是否正确：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubeadm version
kubectl version --client
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/72/c9/72d79f46d9132af0dca110d982eff1c9.png?wh=1408x306"
        data-srcset="https://static001.geekbang.org/resource/image/72/c9/72d79f46d9132af0dca110d982eff1c9.png?wh=1408x306, https://static001.geekbang.org/resource/image/72/c9/72d79f46d9132af0dca110d982eff1c9.png?wh=1408x306 1.5x, https://static001.geekbang.org/resource/image/72/c9/72d79f46d9132af0dca110d982eff1c9.png?wh=1408x306 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/72/c9/72d79f46d9132af0dca110d982eff1c9.png?wh=1408x306"
        title="img" />另外按照 Kubernetes 官网的要求，我们最好再使用命令 apt-mark hold ，锁定这三个软件的版本，避免意外升级导致版本错误：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo apt-mark hold kubeadm kubelet kubectl

</code></pre></td></tr></table>
</div>
</div><h3 id="下载-kubernetes-组件镜像">下载 Kubernetes 组件镜像</h3>
<p>前面我说过，kubeadm 把 apiserver、etcd、scheduler 等组件都打包成了镜像，以容器的方式启动 Kubernetes，但这些镜像不是放在 Docker Hub 上，而是放在 Google 自己的镜像仓库网站 gcr.io，而它在国内的访问很困难，直接拉取镜像几乎是不可能的。
所以我们需要采取一些变通措施，提前把镜像下载到本地。
使用命令 kubeadm config images list 可以查看安装 Kubernetes 所需的镜像列表，参数 &ndash;kubernetes-version 可以指定版本号：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubeadm config images list --kubernetes-version v1.23.3
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/pause:3.6
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
</code></pre></td></tr></table>
</div>
</div><p>知道了镜像的名字和标签就好办了，我们有两种方法可以比较容易地获取这些镜像。
第一种方法是利用 minikube。因为 minikube 本身也打包了 Kubernetes 的组件镜像，所以完全可以从它的节点里把这些镜像导出之后再拷贝过来。
具体做法也很简单，先启动 minikube，然后 minikube ssh 登录进虚拟节点，用 docker save -o 命令把相应版本的镜像都保存下来，再用 minikube cp 拷贝到本地，剩下的事情就不用我多说了：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/66/4f/6609a62525bbf5d77eb7331f9835244f.png?wh=1848x484"
        data-srcset="https://static001.geekbang.org/resource/image/66/4f/6609a62525bbf5d77eb7331f9835244f.png?wh=1848x484, https://static001.geekbang.org/resource/image/66/4f/6609a62525bbf5d77eb7331f9835244f.png?wh=1848x484 1.5x, https://static001.geekbang.org/resource/image/66/4f/6609a62525bbf5d77eb7331f9835244f.png?wh=1848x484 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/66/4f/6609a62525bbf5d77eb7331f9835244f.png?wh=1848x484"
        title="img" /></p>
<p>这种方法安全可靠，不过操作上麻烦了些，所以就有了第二种方法，从国内的镜像网站下载然后再用 docker tag 改名，能够使用 Shell 编程实现自动化：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">repo=registry.aliyuncs.com/google_containers
for name in `kubeadm config images list --kubernetes-version v1.23.3`; do
    src_name=${name#k8s.gcr.io/}
    src_name=${src_name#coredns/}
    docker pull $repo/$src_name
    docker tag $repo/$src_name $name
    docker rmi $repo/$src_name
done
</code></pre></td></tr></table>
</div>
</div><p>第二种方法速度快，但也有隐患，万一网站不提供服务，或者改动了镜像就比较危险了。
所以你可以把这两种方法结合起来，先用脚本从国内镜像仓库下载，然后再用 minikube 里的镜像做对比，只要 IMAGE ID 是一样就说明镜像是正确的。
这张截图就是 Kubernetes 1.23.3 的镜像列表（amd64/arm64），你在安装时可以参考：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/11/5c/11d9d4c91b08d95e82e75406a4d3aa5c.png?wh=1920x353"
        data-srcset="https://static001.geekbang.org/resource/image/11/5c/11d9d4c91b08d95e82e75406a4d3aa5c.png?wh=1920x353, https://static001.geekbang.org/resource/image/11/5c/11d9d4c91b08d95e82e75406a4d3aa5c.png?wh=1920x353 1.5x, https://static001.geekbang.org/resource/image/11/5c/11d9d4c91b08d95e82e75406a4d3aa5c.png?wh=1920x353 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/11/5c/11d9d4c91b08d95e82e75406a4d3aa5c.png?wh=1920x353"
        title="img" /></p>
<p>amd64<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/52/6c/528d9913620015f594988e648eeac66c.png?wh=1920x420"
        data-srcset="https://static001.geekbang.org/resource/image/52/6c/528d9913620015f594988e648eeac66c.png?wh=1920x420, https://static001.geekbang.org/resource/image/52/6c/528d9913620015f594988e648eeac66c.png?wh=1920x420 1.5x, https://static001.geekbang.org/resource/image/52/6c/528d9913620015f594988e648eeac66c.png?wh=1920x420 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/52/6c/528d9913620015f594988e648eeac66c.png?wh=1920x420"
        title="img" /></p>
<p>arm64</p>
<h3 id="安装-master-节点">安装 Master 节点</h3>
<p>准备工作都做好了，现在就可以开始正式安装 Kubernetes 了，我们先从 Master 节点开始。
kubeadm 的用法非常简单，只需要一个命令 kubeadm init 就可以把组件在 Master 节点上运行起来，不过它还有很多参数用来调整集群的配置，你可以用 -h 查看。这里我只说一下我们实验环境用到的 3 个参数：
&ndash;pod-network-cidr，设置集群里 Pod 的 IP 地址段。
&ndash;apiserver-advertise-address，设置 apiserver 的 IP 地址，对于多网卡服务器来说很重要（比如 VirtualBox 虚拟机就用了两块网卡），可以指定 apiserver 在哪个网卡上对外提供服务。
&ndash;kubernetes-version，指定 Kubernetes 的版本号。
下面的这个安装命令里，我指定了 Pod 的地址段是“10.10.0.0/16”，apiserver 的服务地址是“192.168.10.210”，Kubernetes 的版本号是“1.23.3”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo kubeadm init \
    --pod-network-cidr=10.10.0.0/16 \
    --apiserver-advertise-address=192.168.10.210 \
    --kubernetes-version=v1.23.3
</code></pre></td></tr></table>
</div>
</div><p>因为我们已经提前把镜像下载到了本地，所以 kubeadm 的安装过程很快就完成了，它还会提示出接下来要做的工作：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">To start using your cluster, you need to run the following as a regular user:
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre></td></tr></table>
</div>
</div><p>意思是要在本地建立一个“.kube”目录，然后拷贝 kubectl 的配置文件，你只要原样拷贝粘贴就行。
另外还有一个很重要的“kubeadm join”提示，其他节点要加入集群必须要用指令里的 token 和 ca 证书，所以这条命令务必拷贝后保存好：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 192.168.10.210:6443 --token tv9mkx.tw7it9vphe158e74 \
  --discovery-token-ca-cert-hash sha256:e8721b8630d5b562e23c010c70559a6d3084f629abad6a2920e87855f8fb96f3
</code></pre></td></tr></table>
</div>
</div><p>安装完成后，你就可以使用 kubectl version、kubectl get node 来检查 Kubernetes 的版本和集群的节点状态了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl version
kubectl get node
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c6/09/c63ce96bfyy0e1bc2927d575a66ee209.png?wh=1482x366"
        data-srcset="https://static001.geekbang.org/resource/image/c6/09/c63ce96bfyy0e1bc2927d575a66ee209.png?wh=1482x366, https://static001.geekbang.org/resource/image/c6/09/c63ce96bfyy0e1bc2927d575a66ee209.png?wh=1482x366 1.5x, https://static001.geekbang.org/resource/image/c6/09/c63ce96bfyy0e1bc2927d575a66ee209.png?wh=1482x366 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c6/09/c63ce96bfyy0e1bc2927d575a66ee209.png?wh=1482x366"
        title="img" />你会注意到 Master 节点的状态是“NotReady”，这是由于还缺少网络插件，集群的内部网络还没有正常运作。</p>
<h3 id="安装-flannel-网络插件">安装 Flannel 网络插件</h3>
<p>Kubernetes 定义了 CNI 标准，有很多网络插件，这里我选择最常用的 Flannel，可以在它的 GitHub 仓库里（https://github.com/flannel-io/flannel/)找到相关文档。
它安装也很简单，只需要使用项目的“kube-flannel.yml”在 Kubernetes 里部署一下就好了。不过因为它应用了 Kubernetes 的网段地址，你需要修改文件里的“net-conf.json”字段，把 Network 改成刚才 kubeadm 的参数 &ndash;pod-network-cidr 设置的地址段。
比如在这里，就要修改成“10.10.0.0/16”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">net-conf.json: |
    {
      &#34;Network&#34;: &#34;10.10.0.0/16&#34;,
      &#34;Backend&#34;: {
        &#34;Type&#34;: &#34;vxlan&#34;
      }
    }
</code></pre></td></tr></table>
</div>
</div><p>改好后，你就可以用 kubectl apply 来安装 Flannel 网络了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f kube-flannel.yml

</code></pre></td></tr></table>
</div>
</div><p>稍等一小会，等镜像拉取下来并运行之后，你就可以执行 kubectl get node 来看节点状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get node
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6a/7a/6a3c852abe5b193a6997b154163ed67a.png?wh=1434x184"
        data-srcset="https://static001.geekbang.org/resource/image/6a/7a/6a3c852abe5b193a6997b154163ed67a.png?wh=1434x184, https://static001.geekbang.org/resource/image/6a/7a/6a3c852abe5b193a6997b154163ed67a.png?wh=1434x184 1.5x, https://static001.geekbang.org/resource/image/6a/7a/6a3c852abe5b193a6997b154163ed67a.png?wh=1434x184 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6a/7a/6a3c852abe5b193a6997b154163ed67a.png?wh=1434x184"
        title="img" />这时你应该能够看到 Master 节点的状态是“Ready”，表明节点网络也工作正常了。</p>
<h3 id="安装-worker-节点">安装 Worker 节点</h3>
<p>如果你成功安装了 Master 节点，那么 Worker 节点的安装就简单多了，只需要用之前拷贝的那条 kubeadm join 命令就可以了，记得要用 sudo 来执行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo \
kubeadm join 192.168.10.210:6443 --token tv9mkx.tw7it9vphe158e74 \
  --discovery-token-ca-cert-hash sha256:e8721b8630d5b562e23c010c70559a6d3084f629abad6a2920e87855f8fb96f3
</code></pre></td></tr></table>
</div>
</div><p>它会连接 Master 节点，然后拉取镜像，安装网络插件，最后把节点加入集群。
当然，这个过程中同样也会遇到拉取镜像的问题，你可以如法炮制，提前把镜像下载到 Worker 节点本地，这样安装过程中就不会再有障碍了。
Worker 节点安装完毕后，执行 kubectl get node ，就会看到两个节点都是“Ready”状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/f7/26/f756ece9e81af80a7204243f15777026.png?wh=1430x246"
        data-srcset="https://static001.geekbang.org/resource/image/f7/26/f756ece9e81af80a7204243f15777026.png?wh=1430x246, https://static001.geekbang.org/resource/image/f7/26/f756ece9e81af80a7204243f15777026.png?wh=1430x246 1.5x, https://static001.geekbang.org/resource/image/f7/26/f756ece9e81af80a7204243f15777026.png?wh=1430x246 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/f7/26/f756ece9e81af80a7204243f15777026.png?wh=1430x246"
        title="img" /></p>
<p>现在让我们用 kubectl run ，运行 Nginx 来测试一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl run ngx --image=nginx:alpine
kubectl get pod -o wide
</code></pre></td></tr></table>
</div>
</div><p>会看到 Pod 运行在 Worker 节点上，IP 地址是“10.10.1.2”，表明我们的 Kubernetes 集群部署成功。</p>
<h3 id="小结-15">小结</h3>
<p>好了，把 Master 节点和 Worker 节点都安装好，我们今天的任务就算是基本完成了。
后面 Console 节点的部署工作更加简单，它只需要安装一个 kubectl，然后复制“config”文件就行，你可以直接在 Master 节点上用“scp”远程拷贝，例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">scp `which kubectl` chrono@192.168.10.208:~/
scp ~/.kube/config chrono@192.168.10.208:~/.kube
</code></pre></td></tr></table>
</div>
</div><p>今天的过程多一些，要点我列在了下面：
kubeadm 是一个方便易用的 Kubernetes 工具，能够部署生产级别的 Kubernetes 集群。
安装 Kubernetes 之前需要修改主机的配置，包括主机名、Docker 配置、网络设置、交换分区等。
Kubernetes 的组件镜像存放在 gcr.io，国内下载比较麻烦，可以考虑从 minikube 或者国内镜像网站获取。
安装 Master 节点需要使用命令 kubeadm init，安装 Worker 节点需要使用命令 kubeadm join，还要部署 Flannel 等网络插件才能让集群正常工作。
因为这些操作都是各种 Linux 命令，全手动敲下来确实很繁琐，所以我把这些步骤都做成了 Shell 脚本放在了 GitHub 上（https://github.com/chronolaw/k8s_study/tree/master/admin），你可以下载后直接运行。</p>
<h2 id="18deployment让应用永不宕机">18｜Deployment：让应用永不宕机</h2>
<p>在上一节课里，我们使用 kubeadm 搭建了一个由两个节点组成的小型 Kubernetes 集群，比起单机的 minikube，它更接近真实环境，在这里面做实验我们今后也更容易过渡到生产系统。
有了这个 Kubernetes 环境，接下来我们就在“初级篇”里学习的 Pod 知识基础上，深入研究一些由 Pod 衍生出来的其他 API 对象。
今天要看的 API 对象名字叫“Deployment”，顾名思义，它是专门用来部署应用程序的，能够让应用永不宕机，多用来发布无状态的应用，是 Kubernetes 里最常用也是最有用的一个对象。</p>
<h3 id="为什么要有-deployment">为什么要有 Deployment</h3>
<p>在第 13 讲里，我们学习了 API 对象 Job 和 CronJob，它们代表了生产环境中的离线业务，通过对 Pod 的包装，向 Pod 添加控制字段，实现了基于 Pod 运行临时任务和定时任务的功能。
那么，除了“离线业务”，另一大类业务——也就是“在线业务”，在 Kubernetes 里应该如何处理呢？
我们先看看用 Pod 是否就足够了。因为它在 YAML 里使用“containers”就可以任意编排容器，而且还有一个“restartPolicy”字段，默认值就是 Always，可以监控 Pod 里容器的状态，一旦发生异常，就会自动重启容器。
不过，“restartPolicy”只能保证容器正常工作。不知你有没有想到，如果容器之外的 Pod 出错了该怎么办呢？比如说，有人不小心用 kubectl delete 误删了 Pod，或者 Pod 运行的节点发生了断电故障，那么 Pod 就会在集群里彻底消失，对容器的控制也就无从谈起了。
还有我们也都知道，在线业务远不是单纯启动一个 Pod 这么简单，还有多实例、高可用、版本更新等许多复杂的操作。比如最简单的多实例需求，为了提高系统的服务能力，应对突发的流量和压力，我们需要创建多个应用的副本，还要即时监控它们的状态。如果还是只使用 Pod，那就会又走回手工管理的老路，没有利用好 Kubernetes 自动化运维的优势。
其实，解决的办法也很简单，因为 Kubernetes 已经给我们提供了处理这种问题的思路，就是“单一职责”和“对象组合”。既然 Pod 管理不了自己，那么我们就再创建一个新的对象，由它来管理 Pod，采用和 Job/CronJob 一样的形式——“对象套对象”。
这个用来管理 Pod，实现在线业务应用的新 API 对象，就是 Deployment。</p>
<h3 id="如何使用-yaml-描述-deployment">如何使用 YAML 描述 Deployment</h3>
<p>我们先用命令 kubectl api-resources 来看看 Deployment 的基本信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl api-resources
NAME         SHORTNAMES   APIVERSION   NAMESPACED   KIND
deployments  deploy       apps/v1      true        Deployment
</code></pre></td></tr></table>
</div>
</div><p>从它的输出信息里可以知道，Deployment 的简称是“deploy”，它的 apiVersion 是“apps/v1”，kind 是“Deployment”。
所以，依据前面学习 Pod、Job 的经验，你就应该知道 Deployment 的 YAML 文件头该怎么写了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  name: xxx-dep
</code></pre></td></tr></table>
</div>
</div><p>当然了，我们还是可以使用命令 kubectl create 来创建 Deployment 的 YAML 样板，免去反复手工输入的麻烦。
创建 Deployment 样板的方式和 Job 也差不多，先指定类型是 Deployment（简写 deploy），然后是它的名字，再用 &ndash;image 参数指定镜像名字。
比如下面的这条命令，我就创建了一个名字叫 ngx-dep 的对象，使用的镜像是 nginx:alpine：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;
kubectl create deploy ngx-dep --image=nginx:alpine $out
</code></pre></td></tr></table>
</div>
</div><p>得到的 Deployment 样板大概是下面的这个样子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ngx-dep
  name: ngx-dep

spec:
  replicas: 2
  selector:
    matchLabels:
      app: ngx-dep
      
  template:
    metadata:
      labels:
        app: ngx-dep
    spec:
      containers:
      - image: nginx:alpine
        name: nginx
</code></pre></td></tr></table>
</div>
</div><p>把它和 Job/CronJob 对比一下，你会发现有相似也有不同。相似的地方是都有“spec”“template”字段，“template”字段里也是一个 Pod；不同的地方在于它的“spec”部分多了 replicas、selector 这两个新字段，聪明的你应该会猜到，这或许就会是 Deployment 特殊能力的根本。
没错，这两个新字段就是 Deployment 实现多实例、高可用等功能的关键所在。</p>
<h3 id="deployment-的关键字段">Deployment 的关键字段</h3>
<p>先看 replicas 字段。它的含义比较简单明了，就是“副本数量”的意思，也就是说，指定要在 Kubernetes 集群里运行多少个 Pod 实例。
有了这个字段，就相当于为 Kubernetes 明确了应用部署的“期望状态”，Deployment 对象就可以扮演运维监控人员的角色，自动地在集群里调整 Pod 的数量。
比如，Deployment 对象刚创建出来的时候，Pod 数量肯定是 0，那么它就会根据 YAML 文件里的 Pod 模板，逐个创建出要求数量的 Pod。
接下来 Kubernetes 还会持续地监控 Pod 的运行状态，万一有 Pod 发生意外消失了，数量不满足“期望状态”，它就会通过 apiserver、scheduler 等核心组件去选择新的节点，创建出新的 Pod，直至数量与“期望状态”一致。
这里面的工作流程很复杂，但对于我们这些外部用户来说，设置起来却是非常简单，只需要一个 replicas 字段就搞定了，不需要再用人工监控管理，整个过程完全自动化。
下面我们再来看另一个关键字段 selector，它的作用是“筛选”出要被 Deployment 管理的 Pod 对象，下属字段“matchLabels”定义了 Pod 对象应该携带的 label，它必须和“template”里 Pod 定义的“labels”完全相同，否则 Deployment 就会找不到要控制的 Pod 对象，apiserver 也会告诉你 YAML 格式校验错误无法创建。
这个 selector 字段的用法初看起来好像是有点多余，为了保证 Deployment 成功创建，我们必须在 YAML 里把 label 重复写两次：一次是在“selector.matchLabels”，另一次是在“template.matadata”。像在这里，你就要在这两个地方连续写 app: ngx-dep ：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">...
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ngx-dep
      
  template:
    metadata:
      labels:
        app: ngx-dep
    ...
</code></pre></td></tr></table>
</div>
</div><p>你也许会产生疑问：为什么要这么麻烦？为什么不能像 Job 对象一样，直接用“template”里定义好的 Pod 就行了呢？
这是因为在线业务和离线业务的应用场景差异很大。离线业务中的 Pod 基本上是一次性的，只与这个业务有关，紧紧地绑定在 Job 对象里，一般不会被其他对象所使用。
而在线业务就要复杂得多了，因为 Pod 永远在线，除了要在 Deployment 里部署运行，还可能会被其他的 API 对象引用来管理，比如负责负载均衡的 Service 对象。
所以 Deployment 和 Pod 实际上是一种松散的组合关系，Deployment 实际上并不“持有”Pod 对象，它只是帮助 Pod 对象能够有足够的副本数量运行，仅此而已。如果像 Job 那样，把 Pod 在模板里“写死”，那么其他的对象再想要去管理这些 Pod 就无能为力了。
好明白了这一点，那我们该用什么方式来描述 Deployment 和 Pod 的组合关系呢？
Kubernetes 采用的是这种“贴标签”的方式，通过在 API 对象的“metadata”元信息里加各种标签（labels），我们就可以使用类似关系数据库里查询语句的方式，筛选出具有特定标识的那些对象。通过标签这种设计，Kubernetes 就解除了 Deployment 和模板里 Pod 的强绑定，把组合关系变成了“弱引用”。
虽然话是这么说，但对于很多 Kubernetes 的初学者来说，理解 Deployment 里的 spec 定义还是一个难点。
所以我还是画了一张图，用不同的颜色来区分 Deployment YAML 里的字段，并且用虚线特别标记了 matchLabels 和 labels 之间的联系，希望能够帮助你理解 Deployment 与被它管理的 Pod 的组合关系。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/1f/b0/1f1fdcd112a07cce85757e27fbcc1bb0.jpg?wh=1920x2316"
        data-srcset="https://static001.geekbang.org/resource/image/1f/b0/1f1fdcd112a07cce85757e27fbcc1bb0.jpg?wh=1920x2316, https://static001.geekbang.org/resource/image/1f/b0/1f1fdcd112a07cce85757e27fbcc1bb0.jpg?wh=1920x2316 1.5x, https://static001.geekbang.org/resource/image/1f/b0/1f1fdcd112a07cce85757e27fbcc1bb0.jpg?wh=1920x2316 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/1f/b0/1f1fdcd112a07cce85757e27fbcc1bb0.jpg?wh=1920x2316"
        title="img" /></p>
<h3 id="如何使用-kubectl-操作-deployment">如何使用 kubectl 操作 Deployment</h3>
<p>把 Deployment 的 YAML 写好之后，我们就可以用 kubectl apply 来创建对象了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f deploy.yml

</code></pre></td></tr></table>
</div>
</div><p>要查看 Deployment 的状态，仍然是用 kubectl get 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get deploy
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a5/72/a5b3f8a4c6ac5560dc9dfyybfb257872.png?wh=1222x184"
        data-srcset="https://static001.geekbang.org/resource/image/a5/72/a5b3f8a4c6ac5560dc9dfyybfb257872.png?wh=1222x184, https://static001.geekbang.org/resource/image/a5/72/a5b3f8a4c6ac5560dc9dfyybfb257872.png?wh=1222x184 1.5x, https://static001.geekbang.org/resource/image/a5/72/a5b3f8a4c6ac5560dc9dfyybfb257872.png?wh=1222x184 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a5/72/a5b3f8a4c6ac5560dc9dfyybfb257872.png?wh=1222x184"
        title="img" />它显示的信息都很重要：
READY 表示运行的 Pod 数量，前面的数字是当前数量，后面的数字是期望数量，所以“2/2”的意思就是要求有两个 Pod 运行，现在已经启动了两个 Pod。
UP-TO-DATE 指的是当前已经更新到最新状态的 Pod 数量。因为如果要部署的 Pod 数量很多或者 Pod 启动比较慢，Deployment 完全生效需要一个过程，UP-TO-DATE 就表示现在有多少个 Pod 已经完成了部署，达成了模板里的“期望状态”。
AVAILABLE 要比 READY、UP-TO-DATE 更进一步，不仅要求已经运行，还必须是健康状态，能够正常对外提供服务，它才是我们最关心的 Deployment 指标。
最后一个 AGE 就简单了，表示 Deployment 从创建到现在所经过的时间，也就是运行的时间。
因为 Deployment 管理的是 Pod，我们最终用的也是 Pod，所以还需要用 kubectl get pod 命令来看看 Pod 的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pod
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/4e/cb/4e47298ab0fa443e2c8936ac8ed9e5cb.png?wh=1554x244"
        data-srcset="https://static001.geekbang.org/resource/image/4e/cb/4e47298ab0fa443e2c8936ac8ed9e5cb.png?wh=1554x244, https://static001.geekbang.org/resource/image/4e/cb/4e47298ab0fa443e2c8936ac8ed9e5cb.png?wh=1554x244 1.5x, https://static001.geekbang.org/resource/image/4e/cb/4e47298ab0fa443e2c8936ac8ed9e5cb.png?wh=1554x244 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/4e/cb/4e47298ab0fa443e2c8936ac8ed9e5cb.png?wh=1554x244"
        title="img" />从截图里你可以看到，被 Deployment 管理的 Pod 自动带上了名字，命名的规则是 Deployment 的名字加上两串随机数（其实是 Pod 模板的 Hash 值)。
好，到现在对象创建成功，Deployment 和 Pod 的状态也都没问题，可以正常服务，我们是时候检验一下 Deployment 部署的效果了，看看是否如前面所说的，Deployment 部署的应用真的可以做到“永不宕机”？
来尝试一下吧，让我们用 kubectl delete 删除一个 Pod，模拟一下 Pod 发生故障的情景：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete pod ngx-dep-6796688696-jm6tt

</code></pre></td></tr></table>
</div>
</div><p>然后再查看 Pod 的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pod
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/44/80/4467538713d83434bf6ff983acde1c80.png?wh=1562x248"
        data-srcset="https://static001.geekbang.org/resource/image/44/80/4467538713d83434bf6ff983acde1c80.png?wh=1562x248, https://static001.geekbang.org/resource/image/44/80/4467538713d83434bf6ff983acde1c80.png?wh=1562x248 1.5x, https://static001.geekbang.org/resource/image/44/80/4467538713d83434bf6ff983acde1c80.png?wh=1562x248 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/44/80/4467538713d83434bf6ff983acde1c80.png?wh=1562x248"
        title="img" />你就会“惊喜”地发现，被删除的 Pod 确实是消失了，但 Kubernetes 在 Deployment 的管理之下，很快又创建出了一个新的 Pod，保证了应用实例的数量始终是我们在 YAML 里定义的数量。
这就证明，Deployment 确实实现了它预定的目标，能够让应用“永远在线”“永不宕机”。
在 Deployment 部署成功之后，你还可以随时调整 Pod 的数量，实现所谓的“应用伸缩”。这项工作在 Kubernetes 出现之前对于运维来说是一件很困难的事情，而现在由于有了 Deployment 就变得轻而易举了。
kubectl scale 是专门用于实现“扩容”和“缩容”的命令，你只要用参数 &ndash;replicas 指定需要的副本数量，Kubernetes 就会自动增加或者删除 Pod，让最终的 Pod 数量达到“期望状态”。
比如下面的这条命令，就把 Nginx 应用扩容到了 5 个：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl scale --replicas=5 deploy ngx-dep
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/84/c4/843cc2d702b4e4034bb3a2f2f988fdc4.png?wh=1486x302"
        data-srcset="https://static001.geekbang.org/resource/image/84/c4/843cc2d702b4e4034bb3a2f2f988fdc4.png?wh=1486x302, https://static001.geekbang.org/resource/image/84/c4/843cc2d702b4e4034bb3a2f2f988fdc4.png?wh=1486x302 1.5x, https://static001.geekbang.org/resource/image/84/c4/843cc2d702b4e4034bb3a2f2f988fdc4.png?wh=1486x302 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/84/c4/843cc2d702b4e4034bb3a2f2f988fdc4.png?wh=1486x302"
        title="img" />但要注意， kubectl scale 是命令式操作，扩容和缩容只是临时的措施，如果应用需要长时间保持一个确定的 Pod 数量，最好还是编辑 Deployment 的 YAML 文件，改动“replicas”，再以声明式的 kubectl apply 修改对象的状态。
因为 Deployment 使用了 selector 字段，这里我就顺便提一下 Kubernetes 里 labels 字段的使用方法吧。
之前我们通过 labels 为对象“贴”了各种“标签”，在使用 kubectl get 命令的时候，加上参数 -l，使用 ==、!=、in、notin 的表达式，就能够很容易地用“标签”筛选、过滤出所要查找的对象（有点类似社交媒体的 #tag 功能），效果和 Deployment 里的 selector 字段是一样的。
看两个例子，第一条命令找出“app”标签是 nginx 的所有 Pod，第二条命令找出“app”标签是 ngx、nginx、ngx-dep 的所有 Pod：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pod -l app=nginx
kubectl get pod -l &#39;app in (ngx, nginx, ngx-dep)&#39;
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b0/26/b07ba6a3a9207a5a998c237a6ef49d26.png?wh=1692x546"
        data-srcset="https://static001.geekbang.org/resource/image/b0/26/b07ba6a3a9207a5a998c237a6ef49d26.png?wh=1692x546, https://static001.geekbang.org/resource/image/b0/26/b07ba6a3a9207a5a998c237a6ef49d26.png?wh=1692x546 1.5x, https://static001.geekbang.org/resource/image/b0/26/b07ba6a3a9207a5a998c237a6ef49d26.png?wh=1692x546 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b0/26/b07ba6a3a9207a5a998c237a6ef49d26.png?wh=1692x546"
        title="img" /></p>
<h3 id="小结-16">小结</h3>
<p>好了，今天我们学习了 Kubernetes 里的一个重要的对象：Deployment，它表示的是在线业务，和 Job/CronJob 的结构类似，也包装了 Pod 对象，通过添加额外的控制功能实现了应用永不宕机，你也可以再对比一下第 13 讲来加深对它的理解。
我再简单小结一下今天的内容：
Pod 只能管理容器，不能管理自身，所以就出现了 Deployment，由它来管理 Pod。
Deployment 里有三个关键字段，其中的 template 和 Job 一样，定义了要运行的 Pod 模板。
replicas 字段定义了 Pod 的“期望数量”，Kubernetes 会自动维护 Pod 数量到正常水平。
selector 字段定义了基于 labels 筛选 Pod 的规则，它必须与 template 里 Pod 的 labels 一致。
创建 Deployment 使用命令 kubectl apply，应用的扩容、缩容使用命令 kubectl scale。
学了 Deployment 这个 API 对象，我们今后就不应该再使用“裸 Pod”了。即使我们只运行一个 Pod，也要以 Deployment 的方式来创建它，虽然它的 replicas 字段值是 1，但 Deployment 会保证应用永远在线。
另外，作为 Kubernetes 里最常用的对象，Deployment 的本事还不止这些，它还支持滚动更新、版本回退，自动伸缩等高级功能，这些在“高级篇”里我们再详细学习。</p>
<h2 id="19daemonset忠实可靠的看门狗">19｜Daemonset：忠实可靠的看门狗</h2>
<p>上一次课里我们学习了 Kubernetes 里的一个新 API 对象 Deployment，它代表了在线业务，能够管理多个 Pod 副本，让应用永远在线，还能够任意扩容缩容。
虽然 Deployment 非常有用，但是，它并没有完全解决运维部署应用程序的所有难题。因为和简单的离线业务比起来，在线业务的应用场景太多太复杂，Deployment 的功能特性只覆盖了其中的一部分，无法满足其他场景的需求。
今天我们就来看看另一类代表在线业务 API 对象：DaemonSet，它会在 Kubernetes 集群的每个节点上都运行一个 Pod，就好像是 Linux 系统里的“守护进程”（Daemon）。</p>
<h3 id="为什么要有-daemonset">为什么要有 DaemonSet</h3>
<p>想知道为什么 Kubernetes 会引入 DaemonSet 对象，那就得知道 Deployment 有哪些不足。
我们先简单复习一下 Deployment，它能够创建任意多个的 Pod 实例，并且维护这些 Pod 的正常运行，保证应用始终处于可用状态。
但是，Deployment 并不关心这些 Pod 会在集群的哪些节点上运行，在它看来，Pod 的运行环境与功能是无关的，只要 Pod 的数量足够，应用程序应该会正常工作。
这个假设对于大多数业务来说是没问题的，比如 Nginx、WordPress、MySQL，它们不需要知道集群、节点的细节信息，只要配置好环境变量和存储卷，在哪里“跑”都是一样的。
但是有一些业务比较特殊，它们不是完全独立于系统运行的，而是与主机存在“绑定”关系，必须要依附于节点才能产生价值，比如说：
网络应用（如 kube-proxy），必须每个节点都运行一个 Pod，否则节点就无法加入 Kubernetes 网络。
监控应用（如 Prometheus），必须每个节点都有一个 Pod 用来监控节点的状态，实时上报信息。
日志应用（如 Fluentd），必须在每个节点上运行一个 Pod，才能够搜集容器运行时产生的日志数据。
安全应用，同样的，每个节点都要有一个 Pod 来执行安全审计、入侵检查、漏洞扫描等工作。
这些业务如果用 Deployment 来部署就不太合适了，因为 Deployment 所管理的 Pod 数量是固定的，而且可能会在集群里“漂移”，但，实际的需求却是要在集群里的每个节点上都运行 Pod，也就是说 Pod 的数量与节点数量保持同步。
所以，Kubernetes 就定义了新的 API 对象 DaemonSet，它在形式上和 Deployment 类似，都是管理控制 Pod，但管理调度策略却不同。DaemonSet 的目标是在集群的每个节点上运行且仅运行一个 Pod，就好像是为节点配上一只“看门狗”，忠实地“守护”着节点，这就是 DaemonSet 名字的由来。</p>
<h3 id="如何使用-yaml-描述-daemonset">如何使用 YAML 描述 DaemonSet</h3>
<p>DaemonSet 和 Deployment 都属于在线业务，所以它们也都是“apps”组，使用命令  kubectl api-resources  可以知道它的简称是 ds ，YAML 文件头信息应该是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: xxx-ds
</code></pre></td></tr></table>
</div>
</div><p>不过非常奇怪，kubectl 不提供自动创建 DaemonSet YAML 样板的功能，也就是说，我们不能用命令 kubectl create 直接创建出一个 DaemonSet 对象。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/99/7f/99b434fc4089ce23a7e54ed8b857a27f.png?wh=1190x304"
        data-srcset="https://static001.geekbang.org/resource/image/99/7f/99b434fc4089ce23a7e54ed8b857a27f.png?wh=1190x304, https://static001.geekbang.org/resource/image/99/7f/99b434fc4089ce23a7e54ed8b857a27f.png?wh=1190x304 1.5x, https://static001.geekbang.org/resource/image/99/7f/99b434fc4089ce23a7e54ed8b857a27f.png?wh=1190x304 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/99/7f/99b434fc4089ce23a7e54ed8b857a27f.png?wh=1190x304"
        title="img" /></p>
<p>这个缺点对于我们使用 DaemonSet 的确造成了不小的麻烦，毕竟如果用 kubectl explain 一个个地去查字段再去写 YAML 实在是太辛苦了。
不过，Kubernetes 不给我们生成样板文件的机会，我们也可以自己去“抄”。你可以在 Kubernetes 的官网（https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/）上找到一份 DaemonSet 的 YAML 示例，把它拷贝下来，再去掉多余的部分，就可以做成自己的一份样板文件，大概是下面的这个样子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: redis-ds
  labels:
    app: redis-ds
spec:
  selector:
    matchLabels:
      name: redis-ds
  template:
    metadata:
      labels:
        name: redis-ds
    spec:
      containers:
      - image: redis:5-alpine
        name: redis
        ports:
        - containerPort: 6379
</code></pre></td></tr></table>
</div>
</div><p>这个 DaemonSet 对象的名字是 redis-ds，镜像是 redis:5-alpine，使用了流行的 NoSQL 数据库 Redis（你也许对它很熟悉）。
把这份 YAML 和上节课里的 Deployment 对象简单对比一下，你会发现：
前面的 kind、metadata 是对象独有的信息，自然是不同的，但下面的 spec 部分，DaemonSet 也有 selector 字段，匹配 template 里 Pod 的 labels 标签，和 Deployment 对象几乎一模一样。
再仔细观察，我们就会看到，DaemonSet 在 spec 里没有 replicas 字段，这是它与 Deployment 的一个关键不同点，意味着它不会在集群里创建多个 Pod 副本，而是要在每个节点上只创建出一个 Pod 实例。
也就是说，DaemonSet 仅仅是在 Pod 的部署调度策略上和 Deployment 不同，其他的都是相同的，某种程度上我们也可以把 DaemonSet 看做是 Deployment 的一个特例。
我还是把 YAML 描述文件画了一张图，好让你看清楚与 Deployment 的差异：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c1/1c/c1dee411aa02f4ff2b8caaf0bd627a1c.jpg?wh=1920x1173"
        data-srcset="https://static001.geekbang.org/resource/image/c1/1c/c1dee411aa02f4ff2b8caaf0bd627a1c.jpg?wh=1920x1173, https://static001.geekbang.org/resource/image/c1/1c/c1dee411aa02f4ff2b8caaf0bd627a1c.jpg?wh=1920x1173 1.5x, https://static001.geekbang.org/resource/image/c1/1c/c1dee411aa02f4ff2b8caaf0bd627a1c.jpg?wh=1920x1173 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c1/1c/c1dee411aa02f4ff2b8caaf0bd627a1c.jpg?wh=1920x1173"
        title="img" /></p>
<p>了解到这些区别，现在，我们就可以用变通的方法来创建 DaemonSet 的 YAML 样板了，你只需要用 kubectl create 先创建出一个 Deployment 对象，然后把 kind 改成 DaemonSet，再删除 spec.replicas 就行了，比如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;

# change &#34;kind&#34; to DaemonSet

kubectl create deploy redis-ds --image=redis:5-alpine $out
</code></pre></td></tr></table>
</div>
</div><h3 id="如何在-kubernetes-里使用-daemonset">如何在 Kubernetes 里使用 DaemonSet</h3>
<p>现在，让我们执行命令 kubectl apply，把 YAML 发送给 Kubernetes，让它创建 DaemonSet 对象，再用 kubectl get 查看对象的状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/43/f3/4349f1f2aed7f4ffac017ee6064059f3.png?wh=1884x486"
        data-srcset="https://static001.geekbang.org/resource/image/43/f3/4349f1f2aed7f4ffac017ee6064059f3.png?wh=1884x486, https://static001.geekbang.org/resource/image/43/f3/4349f1f2aed7f4ffac017ee6064059f3.png?wh=1884x486 1.5x, https://static001.geekbang.org/resource/image/43/f3/4349f1f2aed7f4ffac017ee6064059f3.png?wh=1884x486 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/43/f3/4349f1f2aed7f4ffac017ee6064059f3.png?wh=1884x486"
        title="img" /></p>
<p>看这张截图，虽然我们没有指定 DaemonSet 里 Pod 要运行的数量，但它自己就会去查找集群里的节点，在节点里创建 Pod。因为我们的实验环境里有一个 Master 一个 Worker，而 Master 默认是不跑应用的，所以 DaemonSet 就只生成了一个 Pod，运行在了“worker”节点上。
暂停一下，你发现这里有什么不对劲了吗？
按照 DaemonSet 的本意，应该在每个节点上都运行一个 Pod 实例才对，但 Master 节点却被排除在外了，这就不符合我们当初的设想了。
显然，DaemonSet 没有尽到“看门”的职责，它的设计与 Kubernetes 集群的工作机制发生了冲突，有没有办法解决呢？
当然，Kubernetes 早就想到了这点，为了应对 Pod 在某些节点的“调度”和“驱逐”问题，它定义了两个新的概念：污点（taint）和容忍度（toleration）。</p>
<h3 id="什么是污点taint和容忍度toleration">什么是污点（taint）和容忍度（toleration）</h3>
<p>“污点”是 Kubernetes 节点的一个属性，它的作用也是给节点“贴标签”，但为了不和已有的 labels 字段混淆，就改成了 taint。
和“污点”相对的，就是 Pod 的“容忍度”，顾名思义，就是 Pod 能否“容忍”污点。
我们把它俩放在一起就比较好理解了。集群里的节点各式各样，有的节点“纯洁无瑕”，没有“污点”；而有的节点因为某种原因粘上了“泥巴”，也就有了“污点”。Pod 也脾气各异，有的“洁癖”很严重，不能容忍“污点”，只能挑选“干净”的节点；而有的 Pod 则比较“大大咧咧”，要求不那么高，可以适当地容忍一些小“污点”。
这么看来，“污点”和“容忍度”倒是有点像是一个“相亲”的过程。Pod 就是一个挑剔的“甲方”，而“乙方”就是集群里的各个节点，Pod 会根据自己对“污点”的“容忍程度”来选择合适的目标，比如要求“不抽烟不喝酒”，但可以“无车无房”，最终决定在哪个节点上“落户”。
Kubernetes 在创建集群的时候会自动给节点 Node 加上一些“污点”，方便 Pod 的调度和部署。你可以用 kubectl describe node 来查看 Master 和 Worker 的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl describe node master
Name:     master
Roles:    control-plane,master
...
Taints:   node-role.kubernetes.io/master:NoSchedule
...
kubectl describe node worker
Name:     worker
Roles:    &lt;none&gt;
...
Taints:   &lt;none&gt;
...
</code></pre></td></tr></table>
</div>
</div><p>可以看到，Master 节点默认有一个 taint，名字是 node-role.kubernetes.io/master，它的效果是 NoSchedule，也就是说这个污点会拒绝 Pod 调度到本节点上运行，而 Worker 节点的 taint 字段则是空的。
这正是 Master 和 Worker 在 Pod 调度策略上的区别所在，通常来说 Pod 都不能容忍任何“污点”，所以加上了 taint 属性的 Master 节点也就会无缘 Pod 了。
明白了“污点”和“容忍度”的概念，你就知道该怎么让 DaemonSet 在 Master 节点（或者任意其他节点）上运行了，方法有两种。
第一种方法是去掉 Master 节点上的 taint，让 Master 变得和 Worker 一样“纯洁无瑕”，DaemonSet 自然就不需要再区分 Master/Worker。
操作 Node 上的“污点”属性需要使用命令 kubectl taint，然后指定节点名、污点名和污点的效果，去掉污点要额外加上一个 -。
比如要去掉 Master 节点的“NoSchedule”效果，就要用这条命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl taint node master node-role.kubernetes.io/master:NoSchedule-
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/e8/0e/e8e877c960e43a407ab0d95963de400e.png?wh=1920x103"
        data-srcset="https://static001.geekbang.org/resource/image/e8/0e/e8e877c960e43a407ab0d95963de400e.png?wh=1920x103, https://static001.geekbang.org/resource/image/e8/0e/e8e877c960e43a407ab0d95963de400e.png?wh=1920x103 1.5x, https://static001.geekbang.org/resource/image/e8/0e/e8e877c960e43a407ab0d95963de400e.png?wh=1920x103 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/e8/0e/e8e877c960e43a407ab0d95963de400e.png?wh=1920x103"
        title="img" />因为 DaemonSet 一直在监控集群节点的状态，命令执行后 Master 节点已经没有了“污点”，所以它立刻就会发现变化，然后就会在 Master 节点上创建一个“守护”Pod。你可以用 kubectl get 来查看这个变动情况：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/44/37/4440c4f05dd7718c52152ef20fc77237.png?wh=1882x422"
        data-srcset="https://static001.geekbang.org/resource/image/44/37/4440c4f05dd7718c52152ef20fc77237.png?wh=1882x422, https://static001.geekbang.org/resource/image/44/37/4440c4f05dd7718c52152ef20fc77237.png?wh=1882x422 1.5x, https://static001.geekbang.org/resource/image/44/37/4440c4f05dd7718c52152ef20fc77237.png?wh=1882x422 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/44/37/4440c4f05dd7718c52152ef20fc77237.png?wh=1882x422"
        title="img" /></p>
<p>但是，这种方法修改的是 Node 的状态，影响面会比较大，可能会导致很多 Pod 都跑到这个节点上运行，所以我们可以保留 Node 的“污点”，为需要的 Pod 添加“容忍度”，只让某些 Pod 运行在个别节点上，实现“精细化”调度。
这就是第二种方法，为 Pod 添加字段 tolerations，让它能够“容忍”某些“污点”，就可以在任意的节点上运行了。
tolerations 是一个数组，里面可以列出多个被“容忍”的“污点”，需要写清楚“污点”的名字、效果。比较特别是要用 operator 字段指定如何匹配“污点”，一般我们都使用 Exists，也就是说存在这个名字和效果的“污点”。
如果我们想让 DaemonSet 里的 Pod 能够在 Master 节点上运行，就要写出这样的一个 tolerations，容忍节点的 node-role.kubernetes.io/master:NoSchedule 这个污点：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">tolerations:

- key: node-role.kubernetes.io/master
  effect: NoSchedule
  operator: Exists
</code></pre></td></tr></table>
</div>
</div><p>现在我们先用 kubectl taint 命令把 Master 的“污点”加上：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl taint node master node-role.kubernetes.io/master:NoSchedule
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3e/c4/3eb49484fd460e53a40fb239298077c4.png?wh=1920x103"
        data-srcset="https://static001.geekbang.org/resource/image/3e/c4/3eb49484fd460e53a40fb239298077c4.png?wh=1920x103, https://static001.geekbang.org/resource/image/3e/c4/3eb49484fd460e53a40fb239298077c4.png?wh=1920x103 1.5x, https://static001.geekbang.org/resource/image/3e/c4/3eb49484fd460e53a40fb239298077c4.png?wh=1920x103 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3e/c4/3eb49484fd460e53a40fb239298077c4.png?wh=1920x103"
        title="img" />然后我们再重新部署加上了“容忍度”的 DaemonSet：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f ds.yml
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/20/e8/2060a08c2b5572b71780c5f5dyyedae8.png?wh=1888x542"
        data-srcset="https://static001.geekbang.org/resource/image/20/e8/2060a08c2b5572b71780c5f5dyyedae8.png?wh=1888x542, https://static001.geekbang.org/resource/image/20/e8/2060a08c2b5572b71780c5f5dyyedae8.png?wh=1888x542 1.5x, https://static001.geekbang.org/resource/image/20/e8/2060a08c2b5572b71780c5f5dyyedae8.png?wh=1888x542 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/20/e8/2060a08c2b5572b71780c5f5dyyedae8.png?wh=1888x542"
        title="img" />你就会看到 DaemonSet 仍然有两个 Pod，分别运行在 Master 和 Worker 节点上，与第一种方法的效果相同。
需要特别说明一下，“容忍度”并不是 DaemonSet 独有的概念，而是从属于 Pod，所以理解了“污点”和“容忍度”之后，你可以在 Job/CronJob、Deployment 里为它们管理的 Pod 也加上 tolerations，从而能够更灵活地调度应用。
至于都有哪些污点、污点有哪些效果我就不细说了，Kubernetes 官网文档（https://kubernetes.io/zh/docs/concepts/scheduling-eviction/taint-and-toleration/)上都列的非常清楚，在理解了工作原理之后，相信你自己学起来也不会太难。</p>
<h3 id="什么是静态-pod">什么是静态 Pod</h3>
<p>DaemonSet 是在 Kubernetes 里运行节点专属 Pod 最常用的方式，但它不是唯一的方式，Kubernetes 还支持另外一种叫“静态 Pod”的应用部署手段。
“静态 Pod”非常特殊，它不受 Kubernetes 系统的管控，不与 apiserver、scheduler 发生关系，所以是“静态”的。
但既然它是 Pod，也必然会“跑”在容器运行时上，也会有 YAML 文件来描述它，而唯一能够管理它的 Kubernetes 组件也就只有在每个节点上运行的 kubelet 了。
“静态 Pod”的 YAML 文件默认都存放在节点的 /etc/kubernetes/manifests 目录下，它是 Kubernetes 的专用目录。
下面的这张截图就是 Master 节点里目录的情况：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/f5/c2/f5477bf666beffcaf3b8663d5a5692c2.png?wh=1842x486"
        data-srcset="https://static001.geekbang.org/resource/image/f5/c2/f5477bf666beffcaf3b8663d5a5692c2.png?wh=1842x486, https://static001.geekbang.org/resource/image/f5/c2/f5477bf666beffcaf3b8663d5a5692c2.png?wh=1842x486 1.5x, https://static001.geekbang.org/resource/image/f5/c2/f5477bf666beffcaf3b8663d5a5692c2.png?wh=1842x486 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/f5/c2/f5477bf666beffcaf3b8663d5a5692c2.png?wh=1842x486"
        title="img" /></p>
<p>你可以看到，Kubernetes 的 4 个核心组件 apiserver、etcd、scheduler、controller-manager 原来都以静态 Pod 的形式存在的，这也是为什么它们能够先于 Kubernetes 集群启动的原因。
如果你有一些 DaemonSet 无法满足的特殊的需求，可以考虑使用静态 Pod，编写一个 YAML 文件放到这个目录里，节点的 kubelet 会定期检查目录里的文件，发现变化就会调用容器运行时创建或者删除静态 Pod。</p>
<h3 id="小结-17">小结</h3>
<p>好了，今天我们学习了 Kubernetes 里部署应用程序的另一种方式：DaemonSet，它与 Deployment 很类似，差别只在于 Pod 的调度策略，适用于在系统里运行节点的“守护进程”。
简单小结一下今天的内容：
DaemonSet 的目标是为集群里的每个节点部署唯一的 Pod，常用于监控、日志等业务。
DaemonSet 的 YAML 描述与 Deployment 非常接近，只是没有 replicas 字段。
“污点”和“容忍度”是与 DaemonSet 相关的两个重要概念，分别从属于 Node 和 Pod，共同决定了 Pod 的调度策略。
静态 Pod 也可以实现和 DaemonSet 同样的效果，但它不受 Kubernetes 控制，必须在节点上纯手动部署，应当慎用。</p>
<h2 id="20service微服务架构的应对之道">20｜Service：微服务架构的应对之道</h2>
<p>在前面的课里我们学习了 Deployment 和 DaemonSet 这两个 API 对象，它们都是在线业务，只是以不同的策略部署应用，Deployment 创建任意多个实例，Daemon 为每个节点创建一个实例。
这两个 API 对象可以部署多种形式的应用，而在云原生时代，微服务无疑是应用的主流形态。为了更好地支持微服务以及服务网格这样的应用架构，Kubernetes 又专门定义了一个新的对象：Service，它是集群内部的负载均衡机制，用来解决服务发现的关键问题。
今天我们就来看看什么是 Service、如何使用 YAML 来定义 Service，以及如何在 Kubernetes 里用好 Service。</p>
<h3 id="为什么要有-service">为什么要有 Service</h3>
<p>有了 Deployment 和 DaemonSet，我们在集群里发布应用程序的工作轻松了很多。借助 Kubernetes 强大的自动化运维能力，我们可以把应用的更新上线频率由以前的月、周级别提升到天、小时级别，让服务质量更上一层楼。
不过，在应用程序快速版本迭代的同时，另一个问题也逐渐显现出来了，就是“服务发现”。
在 Kubernetes 集群里 Pod 的生命周期是比较“短暂”的，虽然 Deployment 和 DaemonSet 可以维持 Pod 总体数量的稳定，但在运行过程中，难免会有 Pod 销毁又重建，这就会导致 Pod 集合处于动态的变化之中。
这种“动态稳定”对于现在流行的微服务架构来说是非常致命的，试想一下，后台 Pod 的 IP 地址老是变来变去，客户端该怎么访问呢？如果不处理好这个问题，Deployment 和 DaemonSet 把 Pod 管理得再完善也是没有价值的。
其实，这个问题也并不是什么难事，业内早就有解决方案来针对这样“不稳定”的后端服务，那就是“负载均衡”，典型的应用有 LVS、Nginx 等等。它们在前端与后端之间加入了一个“中间层”，屏蔽后端的变化，为前端提供一个稳定的服务。
但 LVS、Nginx 毕竟不是云原生技术，所以 Kubernetes 就按照这个思路，定义了新的 API 对象：Service。
所以估计你也能想到，Service 的工作原理和 LVS、Nginx 差不多，Kubernetes 会给它分配一个静态 IP 地址，然后它再去自动管理、维护后面动态变化的 Pod 集合，当客户端访问 Service，它就根据某种策略，把流量转发给后面的某个 Pod。
下面的这张图来自 Kubernetes官网文档，比较清楚地展示了 Service 的工作原理：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214"
        data-srcset="https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214, https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214 1.5x, https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214"
        title="img" /></p>
<p>你可以看到，这里 Service 使用了 iptables 技术，每个节点上的 kube-proxy 组件自动维护 iptables 规则，客户不再关心 Pod 的具体地址，只要访问 Service 的固定 IP 地址，Service 就会根据 iptables 规则转发请求给它管理的多个 Pod，是典型的负载均衡架构。
不过 Service 并不是只能使用 iptables 来实现负载均衡，它还有另外两种实现技术：性能更差的 userspace 和性能更好的 ipvs，但这些都属于底层细节，我们不需要刻意关注。</p>
<h3 id="如何使用-yaml-描述-service">如何使用 YAML 描述 Service</h3>
<p>知道了 Service 的基本工作原理，我们来看看怎么为 Service 编写 YAML 描述文件。
照例我们还是可以用命令 kubectl api-resources 查看它的基本信息，可以知道它的简称是svc，apiVersion 是 v1。注意，这说明它与 Pod 一样，属于 Kubernetes 的核心对象，不关联业务应用，与 Job、Deployment 是不同的。
现在，相信你很容易写出 Service 的 YAML 文件头了吧：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Service
metadata:
  name: xxx-svc
</code></pre></td></tr></table>
</div>
</div><p>同样的，能否让 Kubernetes 为我们自动创建 Service 的 YAML 样板呢？还是使用命令 kubectl create 吗？
这里 Kubernetes 又表现出了行为上的不一致。虽然它可以自动创建 YAML 样板，但不是用命令 kubectl create，而是另外一个命令 kubectl expose，也许 Kubernetes 认为“expose”能够更好地表达 Service“暴露”服务地址的意思吧。
因为在 Kubernetes 里提供服务的是 Pod，而 Pod 又可以用 Deployment/DaemonSet 对象来部署，所以 kubectl expose  支持从多种对象创建服务，Pod、Deployment、DaemonSet 都可以。
使用 kubectl expose 指令时还需要用参数 &ndash;port 和 &ndash;target-port 分别指定映射端口和容器端口，而 Service 自己的 IP 地址和后端 Pod 的 IP 地址可以自动生成，用法上和 Docker 的命令行参数 -p 很类似，只是略微麻烦一点。
比如，如果我们要为第 18 讲里的 ngx-dep 对象生成 Service，命令就要这么写：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;
kubectl expose deploy ngx-dep --port=80 --target-port=80 $out
</code></pre></td></tr></table>
</div>
</div><p>生成的 Service YAML 大概是这样的：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Service
metadata:
  name: ngx-svc

spec:
  selector:
    app: ngx-dep
    
  ports:

  - port: 80
    targetPort: 80
    protocol: TCP
</code></pre></td></tr></table>
</div>
</div><p>你会发现，Service 的定义非常简单，在“spec”里只有两个关键字段，selector 和 ports。
selector 和 Deployment/DaemonSet 里的作用是一样的，用来过滤出要代理的那些 Pod。因为我们指定要代理 Deployment，所以 Kubernetes 就为我们自动填上了 ngx-dep 的标签，会选择这个 Deployment 对象部署的所有 Pod。
从这里你也可以看到，Kubernetes 的这个标签机制虽然很简单，却非常强大有效，很轻松就关联上了 Deployment 的 Pod。
ports 就很好理解了，里面的三个字段分别表示外部端口、内部端口和使用的协议，在这里就是内外部都使用 80 端口，协议是 TCP。
当然，你在这里也可以把 ports 改成“8080”等其他的端口，这样外部服务看到的就是 Service 给出的端口，而不会知道 Pod 的真正服务端口。
为了让你看清楚 Service 与它引用的 Pod 的关系，我把这两个 YAML 对象画在了下面的这张图里，需要重点关注的是 selector、targetPort 与 Pod 的关联：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/0f/64/0f74ae3a71a6a661376698e481903d64.jpg?wh=1920x1322"
        data-srcset="https://static001.geekbang.org/resource/image/0f/64/0f74ae3a71a6a661376698e481903d64.jpg?wh=1920x1322, https://static001.geekbang.org/resource/image/0f/64/0f74ae3a71a6a661376698e481903d64.jpg?wh=1920x1322 1.5x, https://static001.geekbang.org/resource/image/0f/64/0f74ae3a71a6a661376698e481903d64.jpg?wh=1920x1322 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/0f/64/0f74ae3a71a6a661376698e481903d64.jpg?wh=1920x1322"
        title="img" /></p>
<h3 id="如何在-kubernetes-里使用-service">如何在 Kubernetes 里使用 Service</h3>
<p>在使用 YAML 创建 Service 对象之前，让我们先对第 18 讲里的 Deployment 做一点改造，方便观察 Service 的效果。
首先，我们创建一个 ConfigMap，定义一个 Nginx 的配置片段，它会输出服务器的地址、主机名、请求的 URI 等基本信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ConfigMap
metadata:
  name: ngx-conf
data:
  default.conf: |
    server {
      listen 80;
      location / {
        default_type text/plain;
        return 200
          &#39;srv : $server_addr:$server_port\nhost: $hostname\nuri : $request_method $host $request_uri\ndate: $time_iso8601\n&#39;;
      }
    }
</code></pre></td></tr></table>
</div>
</div><p>然后我们在 Deployment 的“template.volumes”里定义存储卷，再用“volumeMounts”把配置文件加载进 Nginx 容器里：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-dep
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ngx-dep
  template:
    metadata:
      labels:
        app: ngx-dep
    spec:
      volumes:
      - name: ngx-conf-vol
        configMap:
          name: ngx-conf
      containers:
      - image: nginx:alpine
        name: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /etc/nginx/conf.d
          name: ngx-conf-vol
</code></pre></td></tr></table>
</div>
</div><p>这两处修改用到了第 14 讲里的知识，如果你还没有熟练掌握，可以回去复习一下。
部署这个 Deployment 之后，我们就可以创建 Service 对象了，用的还是 kubectl apply：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f svc.yml

</code></pre></td></tr></table>
</div>
</div><p>创建之后，用命令 kubectl get 就可以看到它的状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c3/e1/c3502c6c00d870eyy506351e2ba828e1.png?wh=1844x362"
        data-srcset="https://static001.geekbang.org/resource/image/c3/e1/c3502c6c00d870eyy506351e2ba828e1.png?wh=1844x362, https://static001.geekbang.org/resource/image/c3/e1/c3502c6c00d870eyy506351e2ba828e1.png?wh=1844x362 1.5x, https://static001.geekbang.org/resource/image/c3/e1/c3502c6c00d870eyy506351e2ba828e1.png?wh=1844x362 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c3/e1/c3502c6c00d870eyy506351e2ba828e1.png?wh=1844x362"
        title="img" /></p>
<p>你可以看到，Kubernetes 为 Service 对象自动分配了一个 IP 地址“10.96.240.115”，这个地址段是独立于 Pod 地址段的（比如第 17 讲里的 10.10.xx.xx）。而且 Service 对象的 IP 地址还有一个特点，它是一个“虚地址”，不存在实体，只能用来转发流量。
想要看 Service 代理了哪些后端的 Pod，你可以用 kubectl describe 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl describe svc ngx-svc
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/80/16/80b6e738bc13e1f1d56fa99080f65716.png?wh=1244x846"
        data-srcset="https://static001.geekbang.org/resource/image/80/16/80b6e738bc13e1f1d56fa99080f65716.png?wh=1244x846, https://static001.geekbang.org/resource/image/80/16/80b6e738bc13e1f1d56fa99080f65716.png?wh=1244x846 1.5x, https://static001.geekbang.org/resource/image/80/16/80b6e738bc13e1f1d56fa99080f65716.png?wh=1244x846 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/80/16/80b6e738bc13e1f1d56fa99080f65716.png?wh=1244x846"
        title="img" />截图里显示 Service 对象管理了两个 endpoint，分别是“10.10.0.232:80”和“10.10.1.86:80”，初步判断与 Service、Deployment 的定义相符，那么这两个 IP 地址是不是 Nginx Pod 的实际地址呢？
我们还是用 kubectl get pod 来看一下，加上参数 -o wide：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pod -o wide
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/35/34/355129b4eb2290b3df50f7c184c06634.png?wh=1920x241"
        data-srcset="https://static001.geekbang.org/resource/image/35/34/355129b4eb2290b3df50f7c184c06634.png?wh=1920x241, https://static001.geekbang.org/resource/image/35/34/355129b4eb2290b3df50f7c184c06634.png?wh=1920x241 1.5x, https://static001.geekbang.org/resource/image/35/34/355129b4eb2290b3df50f7c184c06634.png?wh=1920x241 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/35/34/355129b4eb2290b3df50f7c184c06634.png?wh=1920x241"
        title="img" />把 Pod 的地址与 Service 的信息做个对比，我们就能够验证 Service 确实用一个静态 IP 地址代理了两个 Pod 的动态 IP 地址。
那怎么测试 Service 的负载均衡效果呢？
因为 Service、 Pod 的 IP 地址都是 Kubernetes 集群的内部网段，所以我们需要用 kubectl exec 进入到 Pod 内部（或者 ssh 登录集群节点)，再用 curl 等工具来访问 Service：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl exec -it ngx-dep-6796688696-r2j6t -- sh
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/72/28/72eab1f20e7d91ddfe07b5e521712b28.png?wh=1638x838"
        data-srcset="https://static001.geekbang.org/resource/image/72/28/72eab1f20e7d91ddfe07b5e521712b28.png?wh=1638x838, https://static001.geekbang.org/resource/image/72/28/72eab1f20e7d91ddfe07b5e521712b28.png?wh=1638x838 1.5x, https://static001.geekbang.org/resource/image/72/28/72eab1f20e7d91ddfe07b5e521712b28.png?wh=1638x838 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/72/28/72eab1f20e7d91ddfe07b5e521712b28.png?wh=1638x838"
        title="img" />在 Pod 里，用 curl 访问 Service 的 IP 地址，就会看到它把数据转发给后端的 Pod，输出信息会显示具体是哪个 Pod 响应了请求，就表明 Service 确实完成了对 Pod 的负载均衡任务。
我们再试着删除一个 Pod，看看 Service 是否会更新后端 Pod 的信息，实现自动化的服务发现：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete pod ngx-dep-6796688696-r2j6t
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/68/65/688362b0d462ba94fed6f9c2fcbed565.png?wh=1920x1200"
        data-srcset="https://static001.geekbang.org/resource/image/68/65/688362b0d462ba94fed6f9c2fcbed565.png?wh=1920x1200, https://static001.geekbang.org/resource/image/68/65/688362b0d462ba94fed6f9c2fcbed565.png?wh=1920x1200 1.5x, https://static001.geekbang.org/resource/image/68/65/688362b0d462ba94fed6f9c2fcbed565.png?wh=1920x1200 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/68/65/688362b0d462ba94fed6f9c2fcbed565.png?wh=1920x1200"
        title="img" />由于 Pod 被 Deployment 对象管理，删除后会自动重建，而 Service 又会通过 controller-manager 实时监控 Pod 的变化情况，所以就会立即更新它代理的 IP 地址。通过截图你就可以看到有一个 IP 地址“10.10.1.86”消失了，换成了新的“10.10.1.87”，它就是新创建的 Pod。
你也可以再尝试一下使用“ping”来测试 Service 的 IP 地址：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/71/1d/7182131d675c5d03ab9c91be4869a51d.png?wh=1638x428"
        data-srcset="https://static001.geekbang.org/resource/image/71/1d/7182131d675c5d03ab9c91be4869a51d.png?wh=1638x428, https://static001.geekbang.org/resource/image/71/1d/7182131d675c5d03ab9c91be4869a51d.png?wh=1638x428 1.5x, https://static001.geekbang.org/resource/image/71/1d/7182131d675c5d03ab9c91be4869a51d.png?wh=1638x428 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/71/1d/7182131d675c5d03ab9c91be4869a51d.png?wh=1638x428"
        title="img" /></p>
<p>会发现根本 ping 不通，因为 Service 的 IP 地址是“虚”的，只用于转发流量，所以 ping 无法得到回应数据包，也就失败了。</p>
<h3 id="如何以域名的方式使用-service">如何以域名的方式使用 Service</h3>
<p>到这里 Service 的基本用法就讲得差不多了，不过它还有一些高级特性值得了解。
我们先来看看 DNS 域名。
Service 对象的 IP 地址是静态的，保持稳定，这在微服务里确实很重要，不过数字形式的 IP 地址用起来还是不太方便。这个时候 Kubernetes 的 DNS 插件就派上了用处，它可以为 Service 创建易写易记的域名，让 Service 更容易使用。
使用 DNS 域名之前，我们要先了解一个新的概念：名字空间（namespace）。
注意它与我们在第 2 讲里说的用于资源隔离的 Linux namespace 技术完全不同，千万不要弄混了。Kubernetes 只是借用了这个术语，但目标是类似的，用来在集群里实现对 API 对象的隔离和分组。
namespace 的简写是“ns”，你可以使用命令 kubectl get ns 来查看当前集群里都有哪些名字空间，也就是说 API 对象有哪些分组：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get ns
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/16/09/169398a24700368f1550950f0e34b409.png?wh=854x368"
        data-srcset="https://static001.geekbang.org/resource/image/16/09/169398a24700368f1550950f0e34b409.png?wh=854x368, https://static001.geekbang.org/resource/image/16/09/169398a24700368f1550950f0e34b409.png?wh=854x368 1.5x, https://static001.geekbang.org/resource/image/16/09/169398a24700368f1550950f0e34b409.png?wh=854x368 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/16/09/169398a24700368f1550950f0e34b409.png?wh=854x368"
        title="img" />Kubernetes 有一个默认的名字空间，叫“default”，如果不显式指定，API 对象都会在这个“default”名字空间里。而其他的名字空间都有各自的用途，比如“kube-system”就包含了 apiserver、etcd 等核心组件的 Pod。
因为 DNS 是一种层次结构，为了避免太多的域名导致冲突，Kubernetes 就把名字空间作为域名的一部分，减少了重名的可能性。
Service 对象的域名完全形式是“对象. 名字空间.svc.cluster.local”，但很多时候也可以省略后面的部分，直接写“对象. 名字空间”甚至“对象名”就足够了，默认会使用对象所在的名字空间（比如这里就是 default)。
现在我们来试验一下 DNS 域名的用法，还是先 kubectl exec  进入 Pod，然后用 curl 访问 ngx-svc、ngx-svc.default 等域名：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/9b/8b/9b8f58e19f7551f9e3a152d79d9d1e8b.png?wh=1638x1204"
        data-srcset="https://static001.geekbang.org/resource/image/9b/8b/9b8f58e19f7551f9e3a152d79d9d1e8b.png?wh=1638x1204, https://static001.geekbang.org/resource/image/9b/8b/9b8f58e19f7551f9e3a152d79d9d1e8b.png?wh=1638x1204 1.5x, https://static001.geekbang.org/resource/image/9b/8b/9b8f58e19f7551f9e3a152d79d9d1e8b.png?wh=1638x1204 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/9b/8b/9b8f58e19f7551f9e3a152d79d9d1e8b.png?wh=1638x1204"
        title="img" /></p>
<p>可以看到，现在我们就不再关心 Service 对象的 IP 地址，只需要知道它的名字，就可以用 DNS 的方式去访问后端服务。
比起 Docker，这无疑是一个巨大的进步，而且对比其他微服务框架（如 Dubbo、Spring Cloud），由于服务发现机制被集成在了基础设施里，也会让应用的开发更加便捷。
（顺便说一下，Kubernetes 也为每个 Pod 分配了域名，形式是“IP 地址. 名字空间.pod.cluster.local”，但需要把 IP 地址里的 . 改成 - 。比如地址 10.10.1.87，它对应的域名就是 10-10-1-87.default.pod。）</p>
<h3 id="如何让-service-对外暴露服务">如何让 Service 对外暴露服务</h3>
<p>由于 Service 是一种负载均衡技术，所以它不仅能够管理 Kubernetes 集群内部的服务，还能够担当向集群外部暴露服务的重任。
Service 对象有一个关键字段“type”，表示 Service 是哪种类型的负载均衡。前面我们看到的用法都是对集群内部 Pod 的负载均衡，所以这个字段的值就是默认的“ClusterIP”，Service 的静态 IP 地址只能在集群内访问。
除了“ClusterIP”，Service 还支持其他三种类型，分别是“ExternalName”“LoadBalancer”“NodePort”。不过前两种类型一般由云服务商提供，我们的实验环境用不到，所以接下来就重点看“NodePort”这个类型。
如果我们在使用命令 kubectl expose 的时候加上参数 &ndash;type=NodePort，或者在 YAML 里添加字段 type:NodePort，那么 Service 除了会对后端的 Pod 做负载均衡之外，还会在集群里的每个节点上创建一个独立的端口，用这个端口对外提供服务，这也正是“NodePort”这个名字的由来。
让我们修改一下 Service 的 YAML 文件，加上字段“type”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
...
spec:
  ...
  type: NodePort
</code></pre></td></tr></table>
</div>
</div><p>然后创建对象，再查看它的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get svc
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/64/f9/643cf4690a42f723732f9f150021fff9.png?wh=1756x248"
        data-srcset="https://static001.geekbang.org/resource/image/64/f9/643cf4690a42f723732f9f150021fff9.png?wh=1756x248, https://static001.geekbang.org/resource/image/64/f9/643cf4690a42f723732f9f150021fff9.png?wh=1756x248 1.5x, https://static001.geekbang.org/resource/image/64/f9/643cf4690a42f723732f9f150021fff9.png?wh=1756x248 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/64/f9/643cf4690a42f723732f9f150021fff9.png?wh=1756x248"
        title="img" />就会看到“TYPE”变成了“NodePort”，而在“PORT”列里的端口信息也不一样，除了集群内部使用的“80”端口，还多出了一个“30651”端口，这就是 Kubernetes 在节点上为 Service 创建的专用映射端口。
因为这个端口号属于节点，外部能够直接访问，所以现在我们就可以不用登录集群节点或者进入 Pod 内部，直接在集群外使用任意一个节点的 IP 地址，就能够访问 Service 和它代理的后端服务了。
比如我现在所在的服务器是“192.168.10.208”，在这台主机上用 curl 访问 Kubernetes 集群的两个节点“192.168.10.210”“192.168.10.220”，就可以得到 Nginx Pod 的响应数据：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/eb/75/eb917ecdf52cc3f266e6555bd7a1b075.png?wh=1076x666"
        data-srcset="https://static001.geekbang.org/resource/image/eb/75/eb917ecdf52cc3f266e6555bd7a1b075.png?wh=1076x666, https://static001.geekbang.org/resource/image/eb/75/eb917ecdf52cc3f266e6555bd7a1b075.png?wh=1076x666 1.5x, https://static001.geekbang.org/resource/image/eb/75/eb917ecdf52cc3f266e6555bd7a1b075.png?wh=1076x666 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/eb/75/eb917ecdf52cc3f266e6555bd7a1b075.png?wh=1076x666"
        title="img" /></p>
<p>我把 NodePort 与 Service、Deployment 的对应关系画成了图，你看了应该就能更好地明白它的工作原理：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/fy/4a/fyyebea67e4471aa53cb3a0e8ebe624a.jpg?wh=1920x940"
        data-srcset="https://static001.geekbang.org/resource/image/fy/4a/fyyebea67e4471aa53cb3a0e8ebe624a.jpg?wh=1920x940, https://static001.geekbang.org/resource/image/fy/4a/fyyebea67e4471aa53cb3a0e8ebe624a.jpg?wh=1920x940 1.5x, https://static001.geekbang.org/resource/image/fy/4a/fyyebea67e4471aa53cb3a0e8ebe624a.jpg?wh=1920x940 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/fy/4a/fyyebea67e4471aa53cb3a0e8ebe624a.jpg?wh=1920x940"
        title="img" /></p>
<p>学到这里，你是不是觉得 NodePort 类型的 Service 很方便呢。
不过它也有一些缺点。
第一个缺点是它的端口数量很有限。Kubernetes 为了避免端口冲突，默认只在“30000~32767”这个范围内随机分配，只有 2000 多个，而且都不是标准端口号，这对于具有大量业务应用的系统来说根本不够用。
第二个缺点是它会在每个节点上都开端口，然后使用 kube-proxy 路由到真正的后端 Service，这对于有很多计算节点的大集群来说就带来了一些网络通信成本，不是特别经济。
第三个缺点，它要求向外界暴露节点的 IP 地址，这在很多时候是不可行的，为了安全还需要在集群外再搭一个反向代理，增加了方案的复杂度。
虽然有这些缺点，但 NodePort 仍然是 Kubernetes 对外提供服务的一种简单易行的方式，在其他更好的方式出现之前，我们也只能使用它。</p>
<h3 id="小结-18">小结</h3>
<p>好了，今天我们学习了 Service 对象，它实现了负载均衡和服务发现技术，是 Kubernetes 应对微服务、服务网格等现代流行应用架构的解决方案。
我再小结一下今天的要点：
Pod 的生命周期很短暂，会不停地创建销毁，所以就需要用 Service 来实现负载均衡，它由 Kubernetes 分配固定的 IP 地址，能够屏蔽后端的 Pod 变化。
Service 对象使用与 Deployment、DaemonSet 相同的“selector”字段，选择要代理的后端 Pod，是松耦合关系。
基于 DNS 插件，我们能够以域名的方式访问 Service，比静态 IP 地址更方便。
名字空间是 Kubernetes 用来隔离对象的一种方式，实现了逻辑上的对象分组，Service 的域名里就包含了名字空间限定。
Service 的默认类型是“ClusterIP”，只能在集群内部访问，如果改成“NodePort”，就会在节点上开启一个随机端口号，让外界也能够访问内部的服务。</p>
<h2 id="21ingress集群进出流量的总管">21｜Ingress：集群进出流量的总管</h2>
<p>上次课里我们学习了 Service 对象，它是 Kubernetes 内置的负载均衡机制，使用静态 IP 地址代理动态变化的 Pod，支持域名访问和服务发现，是微服务架构必需的基础设施。
Service 很有用，但也只能说是“基础设施”，它对网络流量的管理方案还是太简单，离复杂的现代应用架构需求还有很大的差距，所以 Kubernetes 就在 Service 之上又提出了一个新的概念：Ingress。
比起 Service，Ingress 更接近实际业务，对它的开发、应用和讨论也是社区里最火爆的，今天我们就来看看 Ingress，还有与它关联的 Ingress Controller、Ingress Class 等对象。</p>
<h3 id="为什么要有-ingress">为什么要有 Ingress</h3>
<p>通过上次课程的讲解，我们知道了 Service 的功能和运行机制，它本质上就是一个由 kube-proxy 控制的四层负载均衡，在 TCP/IP 协议栈上转发流量（Service 工作原理示意图）：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214"
        data-srcset="https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214, https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214 1.5x, https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/03/74/0347a0b3bae55fb9ef6c07469e964b74.png?wh=1622x1214"
        title="img" /></p>
<p>但在四层上的负载均衡功能还是太有限了，只能够依据 IP 地址和端口号做一些简单的判断和组合，而我们现在的绝大多数应用都是跑在七层的 HTTP/HTTPS 协议上的，有更多的高级路由条件，比如主机名、URI、请求头、证书等等，而这些在 TCP/IP 网络栈里是根本看不见的。
Service 还有一个缺点，它比较适合代理集群内部的服务。如果想要把服务暴露到集群外部，就只能使用 NodePort 或者 LoadBalancer 这两种方式，而它们都缺乏足够的灵活性，难以管控，这就导致了一种很无奈的局面：我们的服务空有一身本领，却没有合适的机会走出去大展拳脚。
该怎么解决这个问题呢？
Kubernetes 还是沿用了 Service 的思路，既然 Service 是四层的负载均衡，那么我再引入一个新的 API 对象，在七层上做负载均衡是不是就可以了呢？
不过除了七层负载均衡，这个对象还应该承担更多的职责，也就是作为流量的总入口，统管集群的进出口数据，“扇入”“扇出”流量（也就是我们常说的“南北向”），让外部用户能够安全、顺畅、便捷地访问内部服务（图片来源）：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/e6/55/e6ce31b027ba2a8d94cdc553a2c97255.png?wh=1288x834"
        data-srcset="https://static001.geekbang.org/resource/image/e6/55/e6ce31b027ba2a8d94cdc553a2c97255.png?wh=1288x834, https://static001.geekbang.org/resource/image/e6/55/e6ce31b027ba2a8d94cdc553a2c97255.png?wh=1288x834 1.5x, https://static001.geekbang.org/resource/image/e6/55/e6ce31b027ba2a8d94cdc553a2c97255.png?wh=1288x834 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/e6/55/e6ce31b027ba2a8d94cdc553a2c97255.png?wh=1288x834"
        title="img" /></p>
<p>所以，这个 API 对象就顺理成章地被命名为 Ingress，意思就是集群内外边界上的入口。</p>
<h3 id="为什么要有-ingress-controller">为什么要有 Ingress Controller</h3>
<p>再对比一下 Service 我们就能更透彻地理解 Ingress。
Ingress 可以说是在七层上另一种形式的 Service，它同样会代理一些后端的 Pod，也有一些路由规则来定义流量应该如何分配、转发，只不过这些规则都使用的是 HTTP/HTTPS 协议。
你应该知道，Service 本身是没有服务能力的，它只是一些 iptables 规则，真正配置、应用这些规则的实际上是节点里的 kube-proxy 组件。如果没有 kube-proxy，Service 定义得再完善也没有用。
同样的，Ingress 也只是一些 HTTP 路由规则的集合，相当于一份静态的描述文件，真正要把这些规则在集群里实施运行，还需要有另外一个东西，这就是 Ingress Controller，它的作用就相当于 Service 的 kube-proxy，能够读取、应用 Ingress 规则，处理、调度流量。
按理来说，Kubernetes 应该把 Ingress Controller 内置实现，作为基础设施的一部分，就像 kube-proxy 一样。
不过 Ingress Controller 要做的事情太多，与上层业务联系太密切，所以 Kubernetes 把 Ingress Controller 的实现交给了社区，任何人都可以开发 Ingress Controller，只要遵守 Ingress 规则就好。
这就造成了 Ingress Controller“百花齐放”的盛况。
由于 Ingress Controller 把守了集群流量的关键入口，掌握了它就拥有了控制集群应用的“话语权”，所以众多公司纷纷入场，精心打造自己的 Ingress Controller，意图在 Kubernetes 流量进出管理这个领域占有一席之地。
这些实现中最著名的，就是老牌的反向代理和负载均衡软件 Nginx 了。从 Ingress Controller 的描述上我们也可以看到，HTTP 层面的流量管理、安全控制等功能其实就是经典的反向代理，而 Nginx 则是其中稳定性最好、性能最高的产品，所以它也理所当然成为了 Kubernetes 里应用得最广泛的 Ingress Controller。
不过，因为 Nginx 是开源的，谁都可以基于源码做二次开发，所以它又有很多的变种，比如社区的 Kubernetes Ingress Controller（https://github.com/kubernetes/ingress-nginx）、Nginx 公司自己的 Nginx Ingress Controller（https://github.com/nginxinc/kubernetes-ingress）、还有基于 OpenResty 的 Kong Ingress Controller（https://github.com/Kong/kubernetes-ingress-controller）等等。
根据 Docker Hub 上的统计，Nginx 公司的开发实现是下载量最多的 Ingress Controller，所以我将以它为例，讲解 Ingress 和 Ingress Controller 的用法。
下面的这张图就来自 Nginx 官网，比较清楚地展示了 Ingress Controller 在 Kubernetes 集群里的地位：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/eb/f8/ebebd12312fa5e6eb1ea90c930bd5ef8.png?wh=1920x706"
        data-srcset="https://static001.geekbang.org/resource/image/eb/f8/ebebd12312fa5e6eb1ea90c930bd5ef8.png?wh=1920x706, https://static001.geekbang.org/resource/image/eb/f8/ebebd12312fa5e6eb1ea90c930bd5ef8.png?wh=1920x706 1.5x, https://static001.geekbang.org/resource/image/eb/f8/ebebd12312fa5e6eb1ea90c930bd5ef8.png?wh=1920x706 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/eb/f8/ebebd12312fa5e6eb1ea90c930bd5ef8.png?wh=1920x706"
        title="img" /></p>
<h3 id="为什么要有-ingressclass">为什么要有 IngressClass</h3>
<p>那么到现在，有了 Ingress 和 Ingress Controller，我们是不是就可以完美地管理集群的进出流量了呢？
最初 Kubernetes 也是这么想的，一个集群里有一个 Ingress Controller，再给它配上许多不同的 Ingress 规则，应该就可以解决请求的路由和分发问题了。
但随着 Ingress 在实践中的大量应用，很多用户发现这种用法会带来一些问题，比如：
由于某些原因，项目组需要引入不同的 Ingress Controller，但 Kubernetes 不允许这样做；
Ingress 规则太多，都交给一个 Ingress Controller 处理会让它不堪重负；
多个 Ingress 对象没有很好的逻辑分组方式，管理和维护成本很高；
集群里有不同的租户，他们对 Ingress 的需求差异很大甚至有冲突，无法部署在同一个 Ingress Controller 上。
所以，Kubernetes 就又提出了一个 Ingress Class 的概念，让它插在 Ingress 和 Ingress Controller 中间，作为流量规则和控制器的协调人，解除了 Ingress 和 Ingress Controller 的强绑定关系。
现在，Kubernetes 用户可以转向管理 Ingress Class，用它来定义不同的业务逻辑分组，简化 Ingress 规则的复杂度。比如说，我们可以用 Class A 处理博客流量、Class B 处理短视频流量、Class C 处理购物流量。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/88/0e/8843704c6314706c9b6f4f2399ca940e.jpg?wh=1920x1306"
        data-srcset="https://static001.geekbang.org/resource/image/88/0e/8843704c6314706c9b6f4f2399ca940e.jpg?wh=1920x1306, https://static001.geekbang.org/resource/image/88/0e/8843704c6314706c9b6f4f2399ca940e.jpg?wh=1920x1306 1.5x, https://static001.geekbang.org/resource/image/88/0e/8843704c6314706c9b6f4f2399ca940e.jpg?wh=1920x1306 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/88/0e/8843704c6314706c9b6f4f2399ca940e.jpg?wh=1920x1306"
        title="img" /></p>
<p>这些 Ingress 和 Ingress Controller 彼此独立，不会发生冲突，所以上面的那些问题也就随着 Ingress Class 的引入迎刃而解了。</p>
<h3 id="如何使用-yaml-描述-ingressingress-class">如何使用 YAML 描述 Ingress/Ingress Class</h3>
<p>我们花了比较多的篇幅学习 Ingress、 Ingress Controller、Ingress Class 这三个对象，全是理论，你可能觉得学得有点累。但这也是没办法的事情，毕竟现实的业务就是这么复杂，而且这个设计架构也是社区经过长期讨论后达成的一致结论，是我们目前能获得的最佳解决方案。
好，了解了这三个概念之后，我们就可以来看看如何为它们编写 YAML 描述文件了。
和之前学习 Deployment、Service 对象一样，首先应当用命令 kubectl api-resources 查看它们的基本信息，输出列在这里了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl api-resources
NAME          SHORTNAMES   APIVERSION           NAMESPACED   KIND
ingresses       ing          networking.k8s.io/v1   true         Ingress
ingressclasses               networking.k8s.io/v1   false        IngressClass
</code></pre></td></tr></table>
</div>
</div><p>你可以看到，Ingress 和 Ingress Class 的 apiVersion 都是“networking.k8s.io/v1”，而且 Ingress 有一个简写“ing”，但 Ingress Controller 怎么找不到呢？
这是因为 Ingress Controller 和其他两个对象不太一样，它不只是描述文件，是一个要实际干活、处理流量的应用程序，而应用程序在 Kubernetes 里早就有对象来管理了，那就是 Deployment 和 DaemonSet，所以我们只需要再学习 Ingress 和 Ingress Class 的的用法就可以了。
先看 Ingress。
Ingress 也是可以使用 kubectl create 来创建样板文件的，和 Service 类似，它也需要用两个附加参数：
&ndash;class，指定 Ingress 从属的 Ingress Class 对象。
&ndash;rule，指定路由规则，基本形式是“URI=Service”，也就是说是访问 HTTP 路径就转发到对应的 Service 对象，再由 Service 对象转发给后端的 Pod。
好，现在我们就执行命令，看看 Ingress 到底长什么样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;
kubectl create ing ngx-ing --rule=&#34;ngx.test/=ngx-svc:80&#34; --class=ngx-ink $out
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: ngx-ing
  
  spec:
    ingressClassName: ngx-ink
  
    rules:
  
    - host: ngx.test
      http:
        paths:
        - path: /
          pathType: Exact
        backend:
            service:
            name: ngx-svc
            port:
                number: 80
</code></pre></td></tr></table>
</div>
</div><p>在这份 Ingress 的 YAML 里，有两个关键字段：“ingressClassName”和“rules”，分别对应了命令行参数，含义还是比较好理解的。
只是“rules”的格式比较复杂，嵌套层次很深。不过仔细点看就会发现它是把路由规则拆散了，有 host 和 http path，在 path 里又指定了路径的匹配方式，可以是精确匹配（Exact）或者是前缀匹配（Prefix），再用 backend 来指定转发的目标 Service 对象。
不过我个人觉得，Ingress YAML 里的描述还不如 kubectl create 命令行里的 &ndash;rule 参数来得直观易懂，而且 YAML 里的字段太多也很容易弄错，建议你还是让 kubectl 来自动生成规则，然后再略作修改比较好。
有了 Ingress 对象，那么与它关联的 Ingress Class 是什么样的呢？
其实 Ingress Class 本身并没有什么实际的功能，只是起到联系 Ingress 和 Ingress Controller 的作用，所以它的定义非常简单，在“spec”里只有一个必需的字段“controller”，表示要使用哪个 Ingress Controller，具体的名字就要看实现文档了。
比如，如果我要用 Nginx 开发的 Ingress Controller，那么就要用名字“nginx.org/ingress-controller”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: ngx-ink
spec:
  controller: nginx.org/ingress-controller
</code></pre></td></tr></table>
</div>
</div><p>Ingress 和 Service、Ingress Class 的关系我也画成了一张图，方便你参考：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6b/af/6bd934a9c8c81a9f194d2d90ede172af.jpg?wh=1920x1005"
        data-srcset="https://static001.geekbang.org/resource/image/6b/af/6bd934a9c8c81a9f194d2d90ede172af.jpg?wh=1920x1005, https://static001.geekbang.org/resource/image/6b/af/6bd934a9c8c81a9f194d2d90ede172af.jpg?wh=1920x1005 1.5x, https://static001.geekbang.org/resource/image/6b/af/6bd934a9c8c81a9f194d2d90ede172af.jpg?wh=1920x1005 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6b/af/6bd934a9c8c81a9f194d2d90ede172af.jpg?wh=1920x1005"
        title="img" /></p>
<h3 id="如何在-kubernetes-里使用-ingressingress-class">如何在 Kubernetes 里使用 Ingress/Ingress Class</h3>
<p>因为 Ingress Class 很小，所以我把它与 Ingress 合成了一个 YAML 文件，让我们用 kubectl apply 创建这两个对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f ingress.yml

</code></pre></td></tr></table>
</div>
</div><p>然后我们用 kubectl get 来查看对象的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get ingressclass
kubectl get ing
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/f9/b9/f9396112f84076528d9072e358d1ebb9.png?wh=1510x366"
        data-srcset="https://static001.geekbang.org/resource/image/f9/b9/f9396112f84076528d9072e358d1ebb9.png?wh=1510x366, https://static001.geekbang.org/resource/image/f9/b9/f9396112f84076528d9072e358d1ebb9.png?wh=1510x366 1.5x, https://static001.geekbang.org/resource/image/f9/b9/f9396112f84076528d9072e358d1ebb9.png?wh=1510x366 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/f9/b9/f9396112f84076528d9072e358d1ebb9.png?wh=1510x366"
        title="img" />命令 kubectl describe 可以看到更详细的 Ingress 信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl describe ing ngx-ing
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b7/13/b708b7d41ef44844af7bf02cbb334313.png?wh=1576x664"
        data-srcset="https://static001.geekbang.org/resource/image/b7/13/b708b7d41ef44844af7bf02cbb334313.png?wh=1576x664, https://static001.geekbang.org/resource/image/b7/13/b708b7d41ef44844af7bf02cbb334313.png?wh=1576x664 1.5x, https://static001.geekbang.org/resource/image/b7/13/b708b7d41ef44844af7bf02cbb334313.png?wh=1576x664 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b7/13/b708b7d41ef44844af7bf02cbb334313.png?wh=1576x664"
        title="img" />可以看到，Ingress 对象的路由规则 Host/Path 就是在 YAML 里设置的域名“ngx.test/”，而且已经关联了第 20 讲里创建的 Service 对象，还有 Service 后面的两个 Pod。
另外，不要对 Ingress 里“Default backend”的错误提示感到惊讶，在找不到路由的时候，它被设计用来提供一个默认的后端服务，但不设置也不会有什么问题，所以大多数时候我们都忽略它。</p>
<h3 id="如何在-kubernetes-里使用-ingress-controller">如何在 Kubernetes 里使用 Ingress Controller</h3>
<p>准备好了 Ingress 和 Ingress Class，接下来我们就需要部署真正处理路由规则的 Ingress Controller。
你可以在 GitHub 上找到 Nginx Ingress Controller 的项目（https://github.com/nginxinc/kubernetes-ingress），因为它以 Pod 的形式运行在 Kubernetes 里，所以同时支持 Deployment 和 DaemonSet 两种部署方式。这里我选择的是 Deployment，相关的 YAML 也都在我们课程的项目（https://github.com/chronolaw/k8s_study/tree/master/ingress)里复制了一份。
Nginx Ingress Controller 的安装略微麻烦一些，有很多个 YAML 需要执行，但如果只是做简单的试验，就只需要用到 4 个 YAML：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f common/ns-and-sa.yaml
kubectl apply -f rbac/rbac.yaml
kubectl apply -f common/nginx-config.yaml
kubectl apply -f common/default-server-secret.yaml
</code></pre></td></tr></table>
</div>
</div><p>前两条命令为 Ingress Controller 创建了一个独立的名字空间“nginx-ingress”，还有相应的账号和权限，这是为了访问 apiserver 获取 Service、Endpoint 信息用的；后两条则是创建了一个 ConfigMap 和 Secret，用来配置 HTTP/HTTPS 服务。
部署 Ingress Controller 不需要我们自己从头编写 Deployment，Nginx 已经为我们提供了示例 YAML，但创建之前为了适配我们自己的应用还必须要做几处小改动：
metadata 里的 name 要改成自己的名字，比如 ngx-kic-dep。
spec.selector 和 template.metadata.labels 也要修改成自己的名字，比如还是用 ngx-kic-dep。
containers.image 可以改用 apline 版本，加快下载速度，比如 nginx/nginx-ingress:2.2-alpine。
最下面的 args 要加上 -ingress-class=ngx-ink，也就是前面创建的 Ingress Class 的名字，这是让 Ingress Controller 管理 Ingress 的关键。
修改完之后，Ingress Controller 的 YAML 大概是这个样子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-kic-dep
  namespace: nginx-ingress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ngx-kic-dep
  template:
    metadata:
      labels:
        app: ngx-kic-dep
    ...
    spec:
      containers:
      - image: nginx/nginx-ingress:2.2-alpine
        ...
        args:
          - -ingress-class=ngx-ink
</code></pre></td></tr></table>
</div>
</div><p>有了 Ingress Controller，这些 API 对象的关联就更复杂了，你可以用下面的这张图来看出它们是如何使用对象名字联系起来的：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/bb/14/bb7a911e10c103fb839e01438e184914.jpg?wh=1920x736"
        data-srcset="https://static001.geekbang.org/resource/image/bb/14/bb7a911e10c103fb839e01438e184914.jpg?wh=1920x736, https://static001.geekbang.org/resource/image/bb/14/bb7a911e10c103fb839e01438e184914.jpg?wh=1920x736 1.5x, https://static001.geekbang.org/resource/image/bb/14/bb7a911e10c103fb839e01438e184914.jpg?wh=1920x736 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/bb/14/bb7a911e10c103fb839e01438e184914.jpg?wh=1920x736"
        title="img" /></p>
<p>确认 Ingress Controller 的 YAML 修改完毕之后，就可以用 kubectl apply 创建对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f kic.yml

</code></pre></td></tr></table>
</div>
</div><p>注意 Ingress Controller 位于名字空间“nginx-ingress”，所以查看状态需要用“-n”参数显式指定，否则我们只能看到“default”名字空间里的 Pod：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get deploy -n nginx-ingress
kubectl get pod -n nginx-ingress
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/63/a6/6389033863c8f809b4c0048be44903a6.png?wh=1476x364"
        data-srcset="https://static001.geekbang.org/resource/image/63/a6/6389033863c8f809b4c0048be44903a6.png?wh=1476x364, https://static001.geekbang.org/resource/image/63/a6/6389033863c8f809b4c0048be44903a6.png?wh=1476x364 1.5x, https://static001.geekbang.org/resource/image/63/a6/6389033863c8f809b4c0048be44903a6.png?wh=1476x364 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/63/a6/6389033863c8f809b4c0048be44903a6.png?wh=1476x364"
        title="img" />现在 Ingress Controller 就算是运行起来了。
不过还有最后一道工序，因为 Ingress Controller 本身也是一个 Pod，想要向外提供服务还是要依赖于 Service 对象。所以你至少还要再为它定义一个 Service，使用 NodePort 或者 LoadBalancer 暴露端口，才能真正把集群的内外流量打通。这个工作就交给你课下自己去完成了。
这里，我就用第 15 讲里提到的命令kubectl port-forward，它可以直接把本地的端口映射到 Kubernetes 集群的某个 Pod 里，在测试验证的时候非常方便。
下面这条命令就把本地的 8080 端口映射到了 Ingress Controller Pod 的 80 端口：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl port-forward -n nginx-ingress ngx-kic-dep-8859b7b86-cplgp 8080:80 &amp;
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/1f/67/1f9cyy6e78d19e23db9594a272fa4267.png?wh=1920x349"
        data-srcset="https://static001.geekbang.org/resource/image/1f/67/1f9cyy6e78d19e23db9594a272fa4267.png?wh=1920x349, https://static001.geekbang.org/resource/image/1f/67/1f9cyy6e78d19e23db9594a272fa4267.png?wh=1920x349 1.5x, https://static001.geekbang.org/resource/image/1f/67/1f9cyy6e78d19e23db9594a272fa4267.png?wh=1920x349 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/1f/67/1f9cyy6e78d19e23db9594a272fa4267.png?wh=1920x349"
        title="img" />我们在 curl 发测试请求的时候需要注意，因为 Ingress 的路由规则是 HTTP 协议，所以就不能用 IP 地址的方式访问，必须要用域名、URI。
你可以修改 /etc/hosts 来手工添加域名解析，也可以使用 &ndash;resolve 参数，指定域名的解析规则，比如在这里我就把“ngx.test”强制解析到“127.0.0.1”，也就是被 kubectl port-forward 转发的本地地址：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">curl --resolve ngx.test:8080:127.0.0.1 http://ngx.test:8080
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/24/ec/2410bb40faa73be25e8d9b3c46c6deec.png?wh=1920x767"
        data-srcset="https://static001.geekbang.org/resource/image/24/ec/2410bb40faa73be25e8d9b3c46c6deec.png?wh=1920x767, https://static001.geekbang.org/resource/image/24/ec/2410bb40faa73be25e8d9b3c46c6deec.png?wh=1920x767 1.5x, https://static001.geekbang.org/resource/image/24/ec/2410bb40faa73be25e8d9b3c46c6deec.png?wh=1920x767 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/24/ec/2410bb40faa73be25e8d9b3c46c6deec.png?wh=1920x767"
        title="img" />把这个访问结果和上一节课里的 Service 对比一下，你会发现最终效果是一样的，都是把请求转发到了集群内部的 Pod，但 Ingress 的路由规则不再是 IP 地址，而是 HTTP 协议里的域名、URI 等要素。</p>
<h3 id="小结-19">小结</h3>
<p>好了，今天就讲到这里，我们学习了 Kubernetes 里七层的反向代理和负载均衡对象，包括 Ingress、Ingress Controller、Ingress Class，它们联合起来管理了集群的进出流量，是集群入口的总管。
小结一下今天的主要内容：
Service 是四层负载均衡，能力有限，所以就出现了 Ingress，它基于 HTTP/HTTPS 协议定义路由规则。
Ingress 只是规则的集合，自身不具备流量管理能力，需要 Ingress Controller 应用 Ingress 规则才能真正发挥作用。
Ingress Class 解耦了 Ingress 和 Ingress Controller，我们应当使用 Ingress Class 来管理 Ingress 资源。
最流行的 Ingress Controller 是 Nginx Ingress Controller，它基于经典反向代理软件 Nginx。
再补充一点，目前的 Kubernetes 流量管理功能主要集中在 Ingress Controller 上，已经远不止于管理“入口流量”了，它还能管理“出口流量”，也就是 egress，甚至还可以管理集群内部服务之间的“东西向流量”。
此外，Ingress Controller 通常还有很多的其他功能，比如 TLS 终止、网络应用防火墙、限流限速、流量拆分、身份认证、访问控制等等，完全可以认为它是一个全功能的反向代理或者网关，感兴趣的话你可以找找这方面的资料。</p>
<h2 id="22实战演练玩转kubernetes2">22｜实战演练：玩转Kubernetes（2）</h2>
<p>我们的“中级篇”到今天马上就要结束了，感谢你这段时间坚持不懈的学习。
作为“中级篇”的收尾课程，我照例还是会对前面学过的内容做一个全面的回顾和总结，把知识点都串联起来，加深你对它们的印象。
下面我先梳理一下“中级篇”里讲过的 Kubernetes 知识要点，然后是实战演示，搭建 WordPress 网站。当然这次比前两次又有进步，不用 Docker，也不用裸 Pod，而是用我们新学习的 Deployment、Service、Ingress 等对象。</p>
<h3 id="kubernetes-技术要点回顾-1">Kubernetes 技术要点回顾</h3>
<p>Kubernetes 是云原生时代的操作系统，它能够管理大量节点构成的集群，让计算资源“池化”，从而能够自动地调度运维各种形式的应用。
搭建多节点的 Kubernetes 集群是一件颇具挑战性的工作，好在社区里及时出现了 kubeadm 这样的工具，可以“一键操作”，使用 kubeadm init、kubeadm join 等命令从无到有地搭建出生产级别的集群（17 讲）。
kubeadm 使用容器技术封装了 Kubernetes 组件，所以只要节点上安装了容器运行时（Docker、containerd 等），它就可以自动从网上拉取镜像，然后以容器的方式运行组件，非常简单方便。
在这个更接近实际生产环境的 Kubernetes 集群里，我们学习了 Deployment、DaemonSet、Service、Ingress、Ingress Controller 等 API 对象。
（18 讲）Deployment 是用来管理 Pod 的一种对象，它代表了运维工作中最常见的一类在线业务，在集群中部署应用的多个实例，而且可以很容易地增加或者减少实例数量，从容应对流量压力。
Deployment 的定义里有两个关键字段：一个是 replicas，它指定了实例的数量；另一个是 selector，它的作用是使用标签“筛选”出被 Deployment 管理的 Pod，这是一种非常灵活的关联机制，实现了 API 对象之间的松耦合。
（19 讲）DaemonSet 是另一种部署在线业务的方式，它很类似 Deployment，但会在集群里的每一个节点上运行一个 Pod 实例，类似 Linux 系统里的“守护进程”，适合日志、监控等类型的应用。
DaemonSet 能够任意部署 Pod 的关键概念是“污点”（taint）和“容忍度”（toleration）。Node 会有各种“污点”，而 Pod 可以使用“容忍度”来忽略“污点”，合理使用这两个概念就可以调整 Pod 在集群里的部署策略。
（20 讲）由 Deployment 和 DaemonSet 部署的 Pod，在集群中处于“动态平衡”的状态，总数量保持恒定，但也有临时销毁重建的可能，所以 IP 地址是变化的，这就为微服务等应用架构带来了麻烦。
Service 是对 Pod IP 地址的抽象，它拥有一个固定的 IP 地址，再使用 iptables 规则把流量负载均衡到后面的 Pod，节点上的 kube-proxy 组件会实时维护被代理的 Pod 状态，保证 Service 只会转发给健康的 Pod。
Service 还基于 DNS 插件支持域名，所以客户端就不再需要关心 Pod 的具体情况，只要通过 Service 这个稳定的中间层，就能够访问到 Pod 提供的服务。
（21 讲）Service 是四层的负载均衡，但现在的绝大多数应用都是 HTTP/HTTPS 协议，要实现七层的负载均衡就要使用 Ingress 对象。
Ingress 定义了基于 HTTP 协议的路由规则，但要让规则生效，还需要 Ingress Controller 和 Ingress Class 来配合工作。
Ingress Controller 是真正的集群入口，应用 Ingress 规则调度、分发流量，此外还能够扮演反向代理的角色，提供安全防护、TLS 卸载等更多功能。
Ingress Class 是用来管理 Ingress 和 Ingress Controller 的概念，方便我们分组路由规则，降低维护成本。
不过 Ingress Controller 本身也是一个 Pod，想要把服务暴露到集群外部还是要依靠 Service。Service 支持 NodePort、LoadBalancer 等方式，但 NodePort 的端口范围有限，LoadBalancer 又依赖于云服务厂商，都不是很灵活。
折中的办法是用少量 NodePort 暴露 Ingress Controller，用 Ingress 路由到内部服务，外部再用反向代理或者 LoadBalancer 把流量引进来。</p>
<h3 id="wordpress-网站基本架构-1">WordPress 网站基本架构</h3>
<p>简略回顾了 Kubernetes 里这些 API 对象，下面我们就来使用它们再搭建出 WordPress 网站，实践加深理解。
既然我们已经掌握了 Deployment、Service、Ingress 这些 Pod 之上的概念，网站自然会有新变化，架构图我放在了这里：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/96/07/9634b8850c3abf62047689b885d7ef07.jpg?wh=1920x1138"
        data-srcset="https://static001.geekbang.org/resource/image/96/07/9634b8850c3abf62047689b885d7ef07.jpg?wh=1920x1138, https://static001.geekbang.org/resource/image/96/07/9634b8850c3abf62047689b885d7ef07.jpg?wh=1920x1138 1.5x, https://static001.geekbang.org/resource/image/96/07/9634b8850c3abf62047689b885d7ef07.jpg?wh=1920x1138 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/96/07/9634b8850c3abf62047689b885d7ef07.jpg?wh=1920x1138"
        title="img" /></p>
<p>这次的部署形式比起 Docker、minikube 又有了一些细微的差别，重点是我们已经完全舍弃了 Docker，把所有的应用都放在 Kubernetes 集群里运行，部署方式也不再是裸 Pod，而是使用 Deployment，稳定性大幅度提升。
原来的 Nginx 的作用是反向代理，那么在 Kubernetes 里它就升级成了具有相同功能的 Ingress Controller。WordPress 原来只有一个实例，现在变成了两个实例（你也可以任意横向扩容），可用性也就因此提高了不少。而 MariaDB 数据库因为要保证数据的一致性，暂时还是一个实例。
还有，因为 Kubernetes 内置了服务发现机制 Service，我们再也不需要去手动查看 Pod 的 IP 地址了，只要为它们定义 Service 对象，然后使用域名就可以访问 MariaDB、WordPress 这些服务。
网站对外提供服务我选择了两种方式。
一种是让 WordPress 的 Service 对象以 NodePort 的方式直接对外暴露端口 30088，方便测试；另一种是给 Nginx Ingress Controller 添加“hostNetwork”属性，直接使用节点上的端口号，类似 Docker 的 host 网络模式，好处是可以避开 NodePort 的端口范围限制。
下面我们就按照这个基本架构来逐步搭建出新版本的 WordPress 网站，编写 YAML 声明。
这里有个小技巧，在实际操作的时候你一定要记得善用 kubectl create、kubectl expose 创建样板文件，节约时间的同时，也能避免低级的格式错误。</p>
<h3 id="1-wordpress-网站部署-mariadb">1. WordPress 网站部署 MariaDB</h3>
<p>首先我们还是要部署 MariaDB，这个步骤和在第 15 讲里做的也差不多。
先要用 ConfigMap 定义数据库的环境变量，有 DATABASE、USER、PASSWORD、ROOT_PASSWORD：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: maria-cm
  data:
    DATABASE: &#39;db&#39;
    USER: &#39;wp&#39;
    PASSWORD: &#39;123&#39;
    ROOT_PASSWORD: &#39;123&#39;
</code></pre></td></tr></table>
</div>
</div><p>然后我们需要把 MariaDB 由 Pod 改成 Deployment 的方式，replicas 设置成 1 个，template 里面的 Pod 部分没有任何变化，还是要用 envFrom把配置信息以环境变量的形式注入 Pod，相当于把 Pod 套了一个 Deployment 的“外壳”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">  apiVersion: apps/v1
  kind: Deployment
  metadata:
     labels:
   app: maria-dep
    name: maria-dep
  spec:
    replicas: 1
     selector:
    matchLabels:
     app: maria-dep
     template:
    metadata:
      labels:
        app: maria-dep
    spec:
      containers:

      - image: mariadb:10
        name: mariadb
        ports:
        - containerPort: 3306
        envFrom:
        - prefix: &#39;MARIADB_&#39;
         configMapRef:
           name: maria-cm
</code></pre></td></tr></table>
</div>
</div><p>我们还需要再为 MariaDB 定义一个 Service 对象，映射端口 3306，让其他应用不再关心 IP 地址，直接用 Service 对象的名字来访问数据库服务：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> apiVersion: v1
  kind: Service
    metadata:
  labels:
    app: maria-dep
  name: maria-svc
  spec:
  ports:

  - port: 3306
    protocol: TCP
    targetPort: 3306
    selector:
    app: maria-dep
</code></pre></td></tr></table>
</div>
</div><p>因为这三个对象都是数据库相关的，所以可以在一个 YAML 文件里书写，对象之间用 &mdash; 分开，这样用 kubectl apply 就可以一次性创建好：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f wp-maria.yml

</code></pre></td></tr></table>
</div>
</div><p>执行命令后，你应该用 kubectl get 查看对象是否创建成功，是否正常运行：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/2f/a4/2fa0050d5bf61224dc17dc67f6da16a4.png?wh=1664x852"
        data-srcset="https://static001.geekbang.org/resource/image/2f/a4/2fa0050d5bf61224dc17dc67f6da16a4.png?wh=1664x852, https://static001.geekbang.org/resource/image/2f/a4/2fa0050d5bf61224dc17dc67f6da16a4.png?wh=1664x852 1.5x, https://static001.geekbang.org/resource/image/2f/a4/2fa0050d5bf61224dc17dc67f6da16a4.png?wh=1664x852 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/2f/a4/2fa0050d5bf61224dc17dc67f6da16a4.png?wh=1664x852"
        title="img" /></p>
<h3 id="2-wordpress-网站部署-wordpress">2. WordPress 网站部署 WordPress</h3>
<p>第二步是部署 WordPress 应用。
因为刚才创建了 MariaDB 的 Service，所以在写 ConfigMap 配置的时候“HOST”就不应该是 IP 地址了，而应该是 DNS 域名，也就是 Service 的名字maria-svc，这点需要特别注意：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ConfigMap
metadata:
name: wp-cm
data:
HOST: &#39;maria-svc&#39;
USER: &#39;wp&#39;
PASSWORD: &#39;123&#39;
NAME: &#39;db&#39;
</code></pre></td></tr></table>
</div>
</div><p>WordPress 的 Deployment 写法和 MariaDB 也是一样的，给 Pod 套一个 Deployment 的“外壳”，replicas 设置成 2 个，用字段“envFrom”配置环境变量：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app: wp-dep
name: wp-dep
spec:
replicas: 2
selector:
matchLabels:
  app: wp-dep
template:
metadata:
  labels:
    app: wp-dep
spec:
  containers:

  - image: wordpress:5
    name: wordpress
    ports:
    - containerPort: 80
      envFrom:
    - prefix: &#39;WORDPRESS_DB_&#39;
      configMapRef:
        name: wp-cm
</code></pre></td></tr></table>
</div>
</div><p>然后我们仍然要为 WordPress 创建 Service 对象，这里我使用了“NodePort”类型，并且手工指定了端口号“30088”（必须在 30000~32767 之间）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Service
metadata:
labels:
app: wp-dep
name: wp-svc
spec:
ports:

- name: http80
  port: 80
  protocol: TCP
  targetPort: 80
  nodePort: 30088
  selector:
  app: wp-dep
  type: NodePort
</code></pre></td></tr></table>
</div>
</div><p>现在让我们用 kubectl apply 部署 WordPress：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply  -f wp-dep.yml

</code></pre></td></tr></table>
</div>
</div><p>这些对象的状态可以从下面的截图看出来：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/da/4f/daeb742118aca29577e1af91c89aff4f.png?wh=1920x1022"
        data-srcset="https://static001.geekbang.org/resource/image/da/4f/daeb742118aca29577e1af91c89aff4f.png?wh=1920x1022, https://static001.geekbang.org/resource/image/da/4f/daeb742118aca29577e1af91c89aff4f.png?wh=1920x1022 1.5x, https://static001.geekbang.org/resource/image/da/4f/daeb742118aca29577e1af91c89aff4f.png?wh=1920x1022 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/da/4f/daeb742118aca29577e1af91c89aff4f.png?wh=1920x1022"
        title="img" /></p>
<p>因为 WordPress 的 Service 对象是 NodePort 类型的，我们可以在集群的每个节点上访问 WordPress 服务。
比如一个节点的 IP 地址是“192.168.10.210”，那么你就在浏览器的地址栏里输入“http://192.168.10.210:30088”，其中的“30088”就是在 Service 里指定的节点端口号，然后就能够看到 WordPress 的安装界面了：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/1f/38/1f5fa840441d52e94d3e7609a3bb9438.png?wh=874x1162"
        data-srcset="https://static001.geekbang.org/resource/image/1f/38/1f5fa840441d52e94d3e7609a3bb9438.png?wh=874x1162, https://static001.geekbang.org/resource/image/1f/38/1f5fa840441d52e94d3e7609a3bb9438.png?wh=874x1162 1.5x, https://static001.geekbang.org/resource/image/1f/38/1f5fa840441d52e94d3e7609a3bb9438.png?wh=874x1162 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/1f/38/1f5fa840441d52e94d3e7609a3bb9438.png?wh=874x1162"
        title="img" /></p>
<h3 id="3-wordpress-网站部署-nginx-ingress-controller">3. WordPress 网站部署 Nginx Ingress Controller</h3>
<p>现在 MariaDB，WordPress 都已经部署成功了，第三步就是部署 Nginx Ingress Controller。
首先我们需要定义 Ingress Class，名字就叫“wp-ink”，非常简单：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
name: wp-ink
spec:
controller: nginx.org/ingress-controller
</code></pre></td></tr></table>
</div>
</div><p>然后用 kubectl create 命令生成 Ingress 的样板文件，指定域名是“wp.test”，后端 Service 是“wp-svc:80”，Ingress Class 就是刚定义的“wp-ink”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create ing wp-ing --rule=&#34;wp.test/=wp-svc:80&#34; --class=wp-ink $out

</code></pre></td></tr></table>
</div>
</div><p>得到的 Ingress YAML 就是这样，注意路径类型我还是用的前缀匹配“Prefix”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: wp-ing
spec:
ingressClassName: wp-ink
rules:

- host: wp.test
  http:
  paths:
  - path: /
    pathType: Prefix
    backend:
      service:
        name: wp-svc
        port:
          number: 80
</code></pre></td></tr></table>
</div>
</div><p>接下来就是最关键的 Ingress Controller 对象了，它仍然需要从 Nginx 项目的示例 YAML 修改而来，要改动名字、标签，还有参数里的 Ingress Class。
在之前讲基本架构的时候我说过了，这个 Ingress Controller 不使用 Service，而是给它的 Pod 加上一个特殊字段 hostNetwork，让 Pod 能够使用宿主机的网络，相当于另一种形式的 NodePort：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
name: wp-kic-dep
namespace: nginx-ingress
spec:
replicas: 1
selector:
matchLabels:
app: wp-kic-dep
template:
metadata:
labels:
app: wp-kic-dep
spec:
serviceAccountName: nginx-ingress

- # use host network

  hostNetwork: true
  containers:
  ...
</code></pre></td></tr></table>
</div>
</div><p>准备好 Ingress 资源后，我们创建这些对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f wp-ing.yml -f wp-kic.yml
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/f5/6a/f57224b4c64ecc6b04651d1986406c6a.png?wh=1614x666"
        data-srcset="https://static001.geekbang.org/resource/image/f5/6a/f57224b4c64ecc6b04651d1986406c6a.png?wh=1614x666, https://static001.geekbang.org/resource/image/f5/6a/f57224b4c64ecc6b04651d1986406c6a.png?wh=1614x666 1.5x, https://static001.geekbang.org/resource/image/f5/6a/f57224b4c64ecc6b04651d1986406c6a.png?wh=1614x666 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/f5/6a/f57224b4c64ecc6b04651d1986406c6a.png?wh=1614x666"
        title="img" />现在所有的应用都已经部署完毕，可以在集群外面访问网站来验证结果了。
不过你要注意，Ingress 使用的是 HTTP 路由规则，用 IP 地址访问是无效的，所以在集群外的主机上必须能够识别我们的“wp.test”域名，也就是说要把域名“wp.test”解析到 Ingress Controller 所在的节点上。
如果你用的是 Mac，那就修改 /etc/hosts；如果你用的是 Windows，就修改 C:\Windows\System32\Drivers\etc\hosts，添加一条解析规则就行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cat /etc/hosts
192.168.10.210  wp.test
</code></pre></td></tr></table>
</div>
</div><p>有了域名解析，在浏览器里你就不必使用 IP 地址，直接用域名“wp.test”走 Ingress Controller 就能访问我们的 WordPress 网站了：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/e1/ea/e10a1f53d9d163e74a53fb18d81cf5ea.png?wh=1550x1098"
        data-srcset="https://static001.geekbang.org/resource/image/e1/ea/e10a1f53d9d163e74a53fb18d81cf5ea.png?wh=1550x1098, https://static001.geekbang.org/resource/image/e1/ea/e10a1f53d9d163e74a53fb18d81cf5ea.png?wh=1550x1098 1.5x, https://static001.geekbang.org/resource/image/e1/ea/e10a1f53d9d163e74a53fb18d81cf5ea.png?wh=1550x1098 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/e1/ea/e10a1f53d9d163e74a53fb18d81cf5ea.png?wh=1550x1098"
        title="img" /></p>
<p>到这里，我们在 Kubernetes 上部署 WordPress 网站的工作就全部完成了。</p>
<h3 id="小结-20">小结</h3>
<p>这节课我们回顾了“中级篇”里的一些知识要点，我把它们总结成了思维导图，你课后可以对照着它查缺补漏，巩固学习成果。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6c/7c/6c051e3c12db763851b1yya34a90c67c.jpg?wh=1920x1543"
        data-srcset="https://static001.geekbang.org/resource/image/6c/7c/6c051e3c12db763851b1yya34a90c67c.jpg?wh=1920x1543, https://static001.geekbang.org/resource/image/6c/7c/6c051e3c12db763851b1yya34a90c67c.jpg?wh=1920x1543 1.5x, https://static001.geekbang.org/resource/image/6c/7c/6c051e3c12db763851b1yya34a90c67c.jpg?wh=1920x1543 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6c/7c/6c051e3c12db763851b1yya34a90c67c.jpg?wh=1920x1543"
        title="img" /></p>
<p>今天我们还在 Kubernetes 集群里再次搭建了 WordPress 网站，应用了新对象 Deployment、Service、Ingress，为网站增加了横向扩容、服务发现和七层负载均衡这三个非常重要的功能，提升了网站的稳定性和可用性，基本上解决了在“初级篇”所遇到的问题。
虽然这个网站离真正实用还差得比较远，但框架已经很完善了，你可以在这个基础上添加其他功能，比如创建证书 Secret、让 Ingress 支持 HTTPS 等等。
另外，我们保证了网站各项服务的高可用，但对于数据库 MariaDB 来说，虽然 Deployment 在发生故障时能够及时重启 Pod，新 Pod 却不会从旧 Pod 继承数据，之前网站的数据会彻底消失，这个后果是完全不可接受的。
所以在后续的“高级篇”里，我们会继续学习持久化存储对象 PersistentVolume，以及有状态的 StatefulSet 等对象，进一步完善我们的网站。</p>
<h2 id="23视频中级篇实操总结">23｜视频：中级篇实操总结</h2>
<p><a href="https://time.geekbang.org/column/article/546594" target="_blank" rel="noopener noreffer">https://time.geekbang.org/column/article/546594</a></p>
<h2 id="加餐docker-compose单机环境下的容器编排工具">加餐｜docker-compose：单机环境下的容器编排工具</h2>
<p>我们的课程学到了这里，你已经对 Kubernetes 有相当程度的了解了吧。
作为云原生时代的操作系统，Kubernetes 源自 Docker 又超越了 Docker，依靠着它的 master/node 架构，掌控成百上千台的计算节点，然后使用 YAML 语言定义各种 API 对象来编排调度容器，实现了对现代应用的管理。
不过，你有没有觉得，在 Docker 和 Kubernetes 之间，是否还缺了一点什么东西呢？
Kubernetes 的确是非常强大的容器编排平台，但强大的功能也伴随着复杂度和成本的提升，不说那几十个用途各异的 API 对象，单单说把 Kubernetes 运行起来搭建一个小型的集群，就需要耗费不少精力。但是，有的时候，我们只是想快速启动一组容器来执行简单的开发、测试工作，并不想承担 Kubernetes 里 apiserver、scheduler、etcd 这些组件的运行成本。
显然，在这种简易任务的应用场景里，Kubernetes 就显得有些“笨重”了。即使是“玩具”性质的 minikube、kind，对电脑也有比较高的要求，会“吃”掉不少的计算资源，属于“大材小用”。
那到底有没有这样的工具，既像 Docker 一样轻巧易用，又像 Kubernetes 一样具备容器编排能力呢？
今天我就来介绍 docker-compose，它恰好满足了刚才的需求，是一个在单机环境里轻量级的容器编排工具，填补了 Docker 和 Kubernetes 之间的空白位置。</p>
<h3 id="什么是-docker-compose">什么是 docker-compose</h3>
<p>还是让我们从 Docker 诞生那会讲起。
在 Docker 把容器技术大众化之后，Docker 周边涌现出了数不胜数的扩展、增强产品，其中有一个名字叫“Fig”的小项目格外令人瞩目。
Fig 为 Docker 引入了“容器编排”的概念，使用 YAML 来定义容器的启动参数、先后顺序和依赖关系，让用户不再有 Docker 冗长命令行的烦恼，第一次见识到了“声明式”的威力。
Docker 公司也很快意识到了 Fig 这个小工具的价值，于是就在 2014 年 7 月把它买了下来，集成进 Docker 内部，然后改名成了“docker-compose”。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/ec/ab/ecb1194e994c0a127d4818310dac14ab.png?wh=400x300"
        data-srcset="https://static001.geekbang.org/resource/image/ec/ab/ecb1194e994c0a127d4818310dac14ab.png?wh=400x300, https://static001.geekbang.org/resource/image/ec/ab/ecb1194e994c0a127d4818310dac14ab.png?wh=400x300 1.5x, https://static001.geekbang.org/resource/image/ec/ab/ecb1194e994c0a127d4818310dac14ab.png?wh=400x300 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/ec/ab/ecb1194e994c0a127d4818310dac14ab.png?wh=400x300"
        title="img" /></p>
<p>图片来自网络
从这段简短的历史中你可以看到，虽然 docker-compose 也是容器编排技术，也使用 YAML，但它的基因与 Kubernetes 完全不同，走的是 Docker 的技术路线，所以在设计理念和使用方法上有差异就不足为怪了。
docker-compose 自身的定位是管理和运行多个 Docker 容器的工具，很显然，它没有 Kubernetes 那么“宏伟”的目标，只是用来方便用户使用 Docker 而已，所以学习难度比较低，上手容易，很多概念都是与 Docker 命令一一对应的。
但这有时候也会给我们带来困扰，毕竟 docker-compose 和 Kubernetes 同属容器编排领域，用法不一致就容易导致认知冲突、混乱。考虑到这一点，我们在学习 docker-compose 的时候就要把握一个“度”，够用就行，不要太过深究，否则会对 Kubernetes 的学习造成一些不良影响。</p>
<h3 id="如何使用-docker-compose">如何使用 docker-compose</h3>
<p>docker-compose 的安装非常简单，它在 GitHub（https://github.com/docker/compose）上提供了多种形式的二进制可执行文件，支持 Windows、macOS、Linux 等操作系统，也支持 x86_64、arm64 等硬件架构，可以直接下载。
在 Linux 上安装的 Shell 命令我放在这里了，用的是最新的 2.6.1 版本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># intel x86_64

sudo curl -SL https://github.com/docker/compose/releases/download/v2.6.1/docker-compose-linux-x86_64 \
          -o /usr/local/bin/docker-compose

# apple m1

sudo curl -SL https://github.com/docker/compose/releases/download/v2.6.1/docker-compose-linux-aarch64 \
          -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
</code></pre></td></tr></table>
</div>
</div><p>安装完成之后，来看一下它的版本号，命令是 docker-compose version，用法和 docker version 是一样的：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker-compose version
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/12/79/12a28accca14eec348353521a89d4879.png?wh=996x120"
        data-srcset="https://static001.geekbang.org/resource/image/12/79/12a28accca14eec348353521a89d4879.png?wh=996x120, https://static001.geekbang.org/resource/image/12/79/12a28accca14eec348353521a89d4879.png?wh=996x120 1.5x, https://static001.geekbang.org/resource/image/12/79/12a28accca14eec348353521a89d4879.png?wh=996x120 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/12/79/12a28accca14eec348353521a89d4879.png?wh=996x120"
        title="img" />接下来，我们就要编写 YAML 文件，来管理 Docker 容器了，先用第 7 讲里的私有镜像仓库作为示范吧。
docker-compose 里管理容器的核心概念是“service”。注意，它与 Kubernetes 里的 Service 虽然名字很像，但却是完全不同的东西。docker-compose 里的“service”就是一个容器化的应用程序，通常是一个后台服务，用 YAML 定义这些容器的参数和相互之间的关系。
如果硬要和 Kubernetes 对比的话，和“service”最像的 API 对象应该算是 Pod 里的 container 了，同样是管理容器运行，但 docker-compose 的“service”又融合了一些 Service、Deployment 的特性。
下面的这个就是私有镜像仓库 Registry 的 YAML 文件，关键字段就是“services”，对应的 Docker 命令我也列了出来：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d -p 5000:5000 registry

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">services:
  registry:
    image: registry
    container_name: registry
    restart: always
    ports:
      - 5000:5000
</code></pre></td></tr></table>
</div>
</div><p>把它和 Kubernetes 对比一下，你会发现它和 Pod 定义非常像，“services”相当于 Pod，而里面的“service”就相当于“spec.containers”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: ngx-pod
spec:
  restartPolicy: Always
  containers:

  - image: nginx:alpine
    name: ngx
    ports:
    - containerPort: 80
</code></pre></td></tr></table>
</div>
</div><p>比如用 image 声明镜像，用 ports 声明端口，很容易理解，只是在用法上有些不一样，像端口映射用的就还是 Docker 的语法。
由于 docker-compose 的字段定义在官网（https://docs.docker.com/compose/compose-file/）上有详细的说明文档，我就不在这里费口舌解释了，你可以自行参考。
需要提醒的是，在 docker-compose 里，每个“service”都有一个自己的名字，它同时也是这个容器的唯一网络标识，有点类似 Kubernetes 里 Service 域名的作用。
好，现在我们就可以启动应用了，命令是 docker-compose up -d，同时还要用 -f 参数来指定 YAML 文件，和 kubectl apply 差不多：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker-compose -f reg-compose.yml up -d
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/8e/b5/8ed5ba47b5bc6c6dc4b999415772deb5.png?wh=1440x246"
        data-srcset="https://static001.geekbang.org/resource/image/8e/b5/8ed5ba47b5bc6c6dc4b999415772deb5.png?wh=1440x246, https://static001.geekbang.org/resource/image/8e/b5/8ed5ba47b5bc6c6dc4b999415772deb5.png?wh=1440x246 1.5x, https://static001.geekbang.org/resource/image/8e/b5/8ed5ba47b5bc6c6dc4b999415772deb5.png?wh=1440x246 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/8e/b5/8ed5ba47b5bc6c6dc4b999415772deb5.png?wh=1440x246"
        title="img" />因为 docker-compose 在底层还是调用的 Docker，所以它启动的容器用 docker ps 也能够看到：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3c/3e/3c89d2d81c380a120d5d05617602c43e.png?wh=1546x158"
        data-srcset="https://static001.geekbang.org/resource/image/3c/3e/3c89d2d81c380a120d5d05617602c43e.png?wh=1546x158, https://static001.geekbang.org/resource/image/3c/3e/3c89d2d81c380a120d5d05617602c43e.png?wh=1546x158 1.5x, https://static001.geekbang.org/resource/image/3c/3e/3c89d2d81c380a120d5d05617602c43e.png?wh=1546x158 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3c/3e/3c89d2d81c380a120d5d05617602c43e.png?wh=1546x158"
        title="img" /></p>
<p>不过，我们用 docker-compose ps 能够看到更多的信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker-compose -f reg-compose.yml ps
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/5f/5f/5fab07a5ba4736a65380729ba392645f.png?wh=1896x186"
        data-srcset="https://static001.geekbang.org/resource/image/5f/5f/5fab07a5ba4736a65380729ba392645f.png?wh=1896x186, https://static001.geekbang.org/resource/image/5f/5f/5fab07a5ba4736a65380729ba392645f.png?wh=1896x186 1.5x, https://static001.geekbang.org/resource/image/5f/5f/5fab07a5ba4736a65380729ba392645f.png?wh=1896x186 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/5f/5f/5fab07a5ba4736a65380729ba392645f.png?wh=1896x186"
        title="img" />下面我们把 Nginx 的镜像改个标签，上传到私有仓库里测试一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker tag nginx:alpine 127.0.0.1:5000/nginx:v1
docker push 127.0.0.1:5000/nginx:v1
</code></pre></td></tr></table>
</div>
</div><p>再用 curl 查看一下它的标签列表，就可以看到确实上传成功了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">curl 127.1:5000/v2/nginx/tags/list
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c5/d8/c56a4bfdc87ce8cf945fa055997486d8.png?wh=1300x126"
        data-srcset="https://static001.geekbang.org/resource/image/c5/d8/c56a4bfdc87ce8cf945fa055997486d8.png?wh=1300x126, https://static001.geekbang.org/resource/image/c5/d8/c56a4bfdc87ce8cf945fa055997486d8.png?wh=1300x126 1.5x, https://static001.geekbang.org/resource/image/c5/d8/c56a4bfdc87ce8cf945fa055997486d8.png?wh=1300x126 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c5/d8/c56a4bfdc87ce8cf945fa055997486d8.png?wh=1300x126"
        title="img" />想要停止应用，我们需要使用 docker-compose down 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker-compose -f reg-compose.yml down
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/4c/5e/4c681530438eb50b1e2ba90c0c8de45e.png?wh=1406x246"
        data-srcset="https://static001.geekbang.org/resource/image/4c/5e/4c681530438eb50b1e2ba90c0c8de45e.png?wh=1406x246, https://static001.geekbang.org/resource/image/4c/5e/4c681530438eb50b1e2ba90c0c8de45e.png?wh=1406x246 1.5x, https://static001.geekbang.org/resource/image/4c/5e/4c681530438eb50b1e2ba90c0c8de45e.png?wh=1406x246 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/4c/5e/4c681530438eb50b1e2ba90c0c8de45e.png?wh=1406x246"
        title="img" />通过这个小例子，我们就成功地把“命令式”的 Docker 操作，转换成了“声明式”的 docker-compose 操作，用法与 Kubernetes 十分接近，同时还没有 Kubernetes 那些昂贵的运行成本，在单机环境里可以说是最适合不过了。</p>
<h3 id="使用-docker-compose-搭建-wordpress-网站">使用 docker-compose 搭建 WordPress 网站</h3>
<p>不过，私有镜像仓库 Registry 里只有一个容器，不能体现 docker-compose 容器编排的好处，我们再用它来搭建一次 WordPress 网站，深入感受一下。
架构图和第 7 讲还是一样的：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643"
        data-srcset="https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643, https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643 1.5x, https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/59/ca/59dfbe961bcd233b83e1c1ec064e2eca.png?wh=1920x643"
        title="img" /></p>
<p>第一步还是定义数据库 MariaDB，环境变量的写法与 Kubernetes 的 ConfigMap 有点类似，但使用的字段是 environment，直接定义，不用再“绕一下”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">services:
  mariadb:
    image: mariadb:10
    container_name: mariadb
    restart: always
    environment:
      MARIADB_DATABASE: db
      MARIADB_USER: wp
      MARIADB_PASSWORD: 123
      MARIADB_ROOT_PASSWORD: 123
</code></pre></td></tr></table>
</div>
</div><p>我们可以再对比第 7 讲里启动 MariaDB 的 Docker 命令，可以发现 docker-compose 的 YAML 和命令行是非常像的，几乎可以直接照搬：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker run -d --rm \
    --env MARIADB_DATABASE=db \
    --env MARIADB_USER=wp \
    --env MARIADB_PASSWORD=123 \
    --env MARIADB_ROOT_PASSWORD=123 \
    mariadb:10
</code></pre></td></tr></table>
</div>
</div><p>第二步是定义 WordPress 网站，它也使用 environment 来设置环境变量：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">services:
  ...

  wordpress:
    image: wordpress:5
    container_name: wordpress
    restart: always
    environment:
      WORDPRESS_DB_HOST: mariadb  #注意这里，数据库的网络标识
      WORDPRESS_DB_USER: wp
      WORDPRESS_DB_PASSWORD: 123
      WORDPRESS_DB_NAME: db
    depends_on:
      - mariadb
</code></pre></td></tr></table>
</div>
</div><p>不过，因为 docker-compose 会自动把 MariaDB 的名字用做网络标识，所以在连接数据库的时候（字段 WORDPRESS_DB_HOST）就不需要手动指定 IP 地址了，直接用“service”的名字 mariadb 就行了。这是 docker-compose 比 Docker 命令要方便的一个地方，和 Kubernetes 的域名机制很像。
WordPress 定义里还有一个值得注意的是字段 depends_on，它用来设置容器的依赖关系，指定容器启动的先后顺序，这在编排由多个容器组成的应用的时候是一个非常便利的特性。
第三步就是定义 Nginx 反向代理了，不过很可惜，docker-compose 里没有 ConfigMap、Secret 这样的概念，要加载配置还是必须用外部文件，无法集成进 YAML。
Nginx 的配置文件和第 7 讲里也差不多，同样的，在 proxy_pass 指令里不需要写 IP 地址了，直接用 WordPress 的名字就行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">server {
  listen 80;
  default_type text/html;
  location / {
      proxy_http_version 1.1;
      proxy_set_header Host $host;
      proxy_pass http://wordpress;  #注意这里，网站的网络标识
  }
}
</code></pre></td></tr></table>
</div>
</div><p>然后我们就可以在 YAML 里定义 Nginx 了，加载配置文件用的是 volumes 字段，和 Kubernetes 一样，但里面的语法却又是 Docker 的形式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">services:
  ...

  nginx:
    image: nginx:alpine
    container_name: nginx
    hostname: nginx
    restart: always
    ports:
      - 80:80
    volumes:
      - ./wp.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - wordpress
</code></pre></td></tr></table>
</div>
</div><p>到这里三个“service”就都定义好了，我们用 docker-compose up -d 启动网站，记得还是要用 -f 参数指定 YAML 文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker-compose -f wp-compose.yml up -d

</code></pre></td></tr></table>
</div>
</div><p>启动之后，用 docker-compose ps 来查看状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/e2/b9/e23953ca3dd05a79a660c7be9509c1b9.png?wh=1426x782"
        data-srcset="https://static001.geekbang.org/resource/image/e2/b9/e23953ca3dd05a79a660c7be9509c1b9.png?wh=1426x782, https://static001.geekbang.org/resource/image/e2/b9/e23953ca3dd05a79a660c7be9509c1b9.png?wh=1426x782 1.5x, https://static001.geekbang.org/resource/image/e2/b9/e23953ca3dd05a79a660c7be9509c1b9.png?wh=1426x782 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/e2/b9/e23953ca3dd05a79a660c7be9509c1b9.png?wh=1426x782"
        title="img" /></p>
<p>我们也可以用 docker-compose exec 来进入容器内部，验证一下这几个容器的网络标识是否工作正常：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker-compose -f wp-compose.yml exec -it nginx sh
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/60/cb/6019120aa14369ce6cb83e880382c1cb.png?wh=1714x1204"
        data-srcset="https://static001.geekbang.org/resource/image/60/cb/6019120aa14369ce6cb83e880382c1cb.png?wh=1714x1204, https://static001.geekbang.org/resource/image/60/cb/6019120aa14369ce6cb83e880382c1cb.png?wh=1714x1204 1.5x, https://static001.geekbang.org/resource/image/60/cb/6019120aa14369ce6cb83e880382c1cb.png?wh=1714x1204 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/60/cb/6019120aa14369ce6cb83e880382c1cb.png?wh=1714x1204"
        title="img" />从截图里你可以看到，我们分别 ping 了 mariadb 和 wordpress 这两个服务，网络都是通的，不过它的 IP 地址段用的是“172.22.0.0/16”，和 Docker 默认的“172.17.0.0/16”不一样。
再打开浏览器，输入本机的“127.0.0.1”或者是虚拟机的 IP 地址（我这里是“http://192.168.10.208”)，就又可以看到熟悉的 WordPress 界面了：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/db/7d/db87232f578ea8556c452c2557db437d.png?wh=1920x1411"
        data-srcset="https://static001.geekbang.org/resource/image/db/7d/db87232f578ea8556c452c2557db437d.png?wh=1920x1411, https://static001.geekbang.org/resource/image/db/7d/db87232f578ea8556c452c2557db437d.png?wh=1920x1411 1.5x, https://static001.geekbang.org/resource/image/db/7d/db87232f578ea8556c452c2557db437d.png?wh=1920x1411 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/db/7d/db87232f578ea8556c452c2557db437d.png?wh=1920x1411"
        title="img" /></p>
<h3 id="小结-21">小结</h3>
<p>好了，今天我们暂时离开了 Kubernetes，回头看了一下 Docker 世界里的容器编排工具 docker-compose。
和 Kubernetes 比起来，docker-compose 有它自己的局限性，比如只能用于单机，编排功能比较简单，缺乏运维监控手段等等。但它也有优点：小巧轻便，对软硬件的要求不高，只要有 Docker 就能够运行。
所以虽然 Kubernetes 已经成为了容器编排领域的霸主，但 docker-compose 还是有一定的生存空间，像 GitHub 上就有很多项目提供了 docker-compose YAML 来快速搭建原型或者测试环境，其中的一个典型就是 CNCF Harbor。
对于我们日常工作来说，docker-compose 也是很有用的。如果是只有几个容器的简单应用，用 Kubernetes 来运行实在是有种“杀鸡用牛刀”的感觉，而用 Docker 命令、Shell 脚本又很不方便，这就是 docker-compose 出场的时候了，它能够让我们彻底摆脱“命令式”，全面使用“声明式”来操作容器。
我再简单小结一下今天的内容：
docker-compose 源自 Fig，是专门用来编排 Docker 容器的工具。
docker-compose 也使用 YAML 来描述容器，但语法语义更接近 Docker 命令行。
docker-compose YAML 里的关键概念是“service”，它是一个容器化的应用。
docker-compose 的命令与 Docker 类似，比较常用的有 up、ps、down，用来启动、查看和停止应用。
另外，docker-compose 里还有不少有用的功能，比如存储卷、自定义网络、特权进程等等，感兴趣的话可以再去看看官网资料。
欢迎留言交流你的学习想法，我们下节课回归正课，下节课见。</p>
<h2 id="24persistentvolume怎么解决数据持久化的难题">24｜PersistentVolume：怎么解决数据持久化的难题？</h2>
<p>经过了“初级篇”和“中级篇”的学习，相信你对 Kubernetes 的认识已经比较全面了，那么在接下来的“高级篇”里，我们再进一步，探索 Kubernetes 更深层次的知识点和更高级的应用技巧。
今天就先从 PersistentVolume 讲起。
早在第 14 讲介绍 ConfigMap/Secret 的时候，我们就遇到过 Kubernetes 里的 Volume 存储卷的概念，它使用字段 volumes 和 volumeMounts，相当于是给 Pod 挂载了一个“虚拟盘”，把配置信息以文件的形式注入进 Pod 供进程使用。
不过，那个时候的 Volume 只能存放较少的数据，离真正的“虚拟盘”还差得很远。
今天我们就一起来了解 Volume 的高级用法，看看 Kubernetes 管理存储资源的 API 对象 PersistentVolume、PersistentVolumeClaim、StorageClass，然后使用本地磁盘来创建实际可用的存储卷。</p>
<h3 id="什么是-persistentvolume">什么是 PersistentVolume</h3>
<p>在刚完成的“中级篇”实战中（22 讲），我们在 Kubernetes 集群里搭建了 WordPress 网站，但其中存在一个很严重的问题：Pod 没有持久化功能，导致 MariaDB 无法“永久”存储数据。
因为 Pod 里的容器是由镜像产生的，而镜像文件本身是只读的，进程要读写磁盘只能用一个临时的存储空间，一旦 Pod 销毁，临时存储也就会立即回收释放，数据也就丢失了。
为了保证即使 Pod 销毁后重建数据依然存在，我们就需要找出一个解决方案，让 Pod 用上真正的“虚拟盘”。怎么办呢？
其实，Kubernetes 的 Volume 对数据存储已经给出了一个很好的抽象，它只是定义了有这么一个“存储卷”，而这个“存储卷”是什么类型、有多大容量、怎么存储，我们都可以自由发挥。Pod 不需要关心那些专业、复杂的细节，只要设置好 volumeMounts，就可以把 Volume 加载进容器里使用。
所以，Kubernetes 就顺着 Volume 的概念，延伸出了 PersistentVolume 对象，它专门用来表示持久存储设备，但隐藏了存储的底层实现，我们只需要知道它能安全可靠地保管数据就可以了（由于 PersistentVolume 这个词很长，一般都把它简称为 PV）。
那么，集群里的 PV 都从哪里来呢？
作为存储的抽象，PV 实际上就是一些存储设备、文件系统，比如 Ceph、GlusterFS、NFS，甚至是本地磁盘，管理它们已经超出了 Kubernetes 的能力范围，所以，一般会由系统管理员单独维护，然后再在 Kubernetes 里创建对应的 PV。
要注意的是，PV 属于集群的系统资源，是和 Node 平级的一种对象，Pod 对它没有管理权，只有使用权。</p>
<h3 id="什么是-persistentvolumeclaimstorageclass">什么是 PersistentVolumeClaim/StorageClass</h3>
<p>现在有了 PV，我们是不是可以直接在 Pod 里挂载使用了呢？
还不行。因为不同存储设备的差异实在是太大了：有的速度快，有的速度慢；有的可以共享读写，有的只能独占读写；有的容量小，只有几百 MB，有的容量大到 TB、PB 级别……
这么多种存储设备，只用一个 PV 对象来管理还是有点太勉强了，不符合“单一职责”的原则，让 Pod 直接去选择 PV 也很不灵活。于是 Kubernetes 就又增加了两个新对象，PersistentVolumeClaim 和 StorageClass，用的还是“中间层”的思想，把存储卷的分配管理过程再次细化。
我们看这两个新对象。
PersistentVolumeClaim，简称 PVC，从名字上看比较好理解，就是用来向 Kubernetes 申请存储资源的。PVC 是给 Pod 使用的对象，它相当于是 Pod 的代理，代表 Pod 向系统申请 PV。一旦资源申请成功，Kubernetes 就会把 PV 和 PVC 关联在一起，这个动作叫做“绑定”（bind）。
但是，系统里的存储资源非常多，如果要 PVC 去直接遍历查找合适的 PV 也很麻烦，所以就要用到 StorageClass。
StorageClass 的作用有点像第 21 讲里的 IngressClass，它抽象了特定类型的存储系统（比如 Ceph、NFS），在 PVC 和 PV 之间充当“协调人”的角色，帮助 PVC 找到合适的 PV。也就是说它可以简化 Pod 挂载“虚拟盘”的过程，让 Pod 看不到 PV 的实现细节。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/5e/22/5e21d007a6152ec9594919300c2b6e22.jpg?wh=1920x1053"
        data-srcset="https://static001.geekbang.org/resource/image/5e/22/5e21d007a6152ec9594919300c2b6e22.jpg?wh=1920x1053, https://static001.geekbang.org/resource/image/5e/22/5e21d007a6152ec9594919300c2b6e22.jpg?wh=1920x1053 1.5x, https://static001.geekbang.org/resource/image/5e/22/5e21d007a6152ec9594919300c2b6e22.jpg?wh=1920x1053 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/5e/22/5e21d007a6152ec9594919300c2b6e22.jpg?wh=1920x1053"
        title="img" /></p>
<p>如果看到这里，你觉得还是差点理解也不要着急，我们找个生活中的例子来类比一下。毕竟和常用的 CPU、内存比起来，我们对存储系统的认识还是比较少的，所以 Kubernetes 里，PV、PVC 和 StorageClass 这三个新概念也不是特别好掌握。
看例子，假设你在公司里想要 10 张纸打印资料，于是你给前台打电话讲清楚了需求。
“打电话”这个动作，就相当于 PVC，向 Kubernetes 申请存储资源。
前台里有各种牌子的办公用纸，大小、规格也不一样，这就相当于 StorageClass。
前台根据你的需要，挑选了一个品牌，再从库存里拿出一包 A4 纸，可能不止 10 张，但也能够满足要求，就在登记表上新添了一条记录，写上你在某天申领了办公用品。这个过程就是 PVC 到 PV 的绑定。
而最后到你手里的 A4 纸包，就是 PV 存储对象。
好，大概了解了这些 API 对象，我们接下来可以结合 YAML 描述和实际操作再慢慢体会。</p>
<h3 id="如何使用-yaml-描述-persistentvolume">如何使用 YAML 描述 PersistentVolume</h3>
<p>Kubernetes 里有很多种类型的 PV，我们先看看最容易的本机存储“HostPath”，它和 Docker 里挂载本地目录的 -v 参数非常类似，可以用它来初步认识一下 PV 的用法。
因为 Pod 会在集群的任意节点上运行，所以首先，我们要作为系统管理员在每个节点上创建一个目录，它将会作为本地存储卷挂载到 Pod 里。
为了省事，我就在 /tmp 里建立名字是 host-10m-pv 的目录，表示一个只有 10MB 容量的存储设备。
有了存储，我们就可以使用 YAML 来描述这个 PV 对象了。
不过很遗憾，你不能用 kubectl create 直接创建 PV 对象，只能用 kubectl api-resources、kubectl explain 查看 PV 的字段说明，手动编写 PV 的 YAML 描述文件。
下面我给出一个 YAML 示例，你可以把它作为样板，编辑出自己的 PV：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: PersistentVolume
metadata:
  name: host-10m-pv
spec:
  storageClassName: host-test
  accessModes:

  - ReadWriteOnce
    capacity:
    storage: 10Mi
      hostPath:
    path: /tmp/host-10m-pv/
</code></pre></td></tr></table>
</div>
</div><p>PV 对象的文件头部分很简单，还是 API 对象的“老一套”，我就不再详细解释了，重点看它的 spec 部分，每个字段都很重要，描述了存储的详细信息。
“storageClassName”就是刚才说过的，对存储类型的抽象 StorageClass。这个 PV 是我们手动管理的，名字可以任意起，这里我写的是 host-test，你也可以把它改成 manual、hand-work 之类的词汇。
“accessModes”定义了存储设备的访问模式，简单来说就是虚拟盘的读写权限，和 Linux 的文件访问模式差不多，目前 Kubernetes 里有 3 种：
ReadWriteOnce：存储卷可读可写，但只能被一个节点上的 Pod 挂载。
ReadOnlyMany：存储卷只读不可写，可以被任意节点上的 Pod 多次挂载。
ReadWriteMany：存储卷可读可写，也可以被任意节点上的 Pod 多次挂载。
你要注意，这 3 种访问模式限制的对象是节点而不是 Pod，因为存储是系统级别的概念，不属于 Pod 里的进程。
显然，本地目录只能是在本机使用，所以这个 PV 使用了 ReadWriteOnce。
第三个字段“capacity”就很好理解了，表示存储设备的容量，这里我设置为 10MB。
再次提醒你注意，Kubernetes 里定义存储容量使用的是国际标准，我们日常习惯使用的 KB/MB/GB 的基数是 1024，要写成 Ki/Mi/Gi，一定要小心不要写错了，否则单位不一致实际容量就会对不上。
最后一个字段“hostPath”最简单，它指定了存储卷的本地路径，也就是我们在节点上创建的目录。
用这些字段把 PV 的类型、访问模式、容量、存储位置都描述清楚，一个存储设备就创建好了。</p>
<h3 id="如何使用-yaml-描述-persistentvolumeclaim">如何使用 YAML 描述 PersistentVolumeClaim</h3>
<p>有了 PV，就表示集群里有了这么一个持久化存储可以供 Pod 使用，我们需要再定义 PVC 对象，向 Kubernetes 申请存储。
下面这份 YAML 就是一个 PVC，要求使用一个 5MB 的存储设备，访问模式是 ReadWriteOnce：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: host-5m-pvc
spec:
  storageClassName: host-test
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Mi
</code></pre></td></tr></table>
</div>
</div><p>PVC 的内容与 PV 很像，但它不表示实际的存储，而是一个“申请”或者“声明”，spec 里的字段描述的是对存储的“期望状态”。
所以 PVC 里的 storageClassName、accessModes 和 PV 是一样的，但不会有字段 capacity，而是要用 resources.request 表示希望要有多大的容量。
这样，Kubernetes 就会根据 PVC 里的描述，去找能够匹配 StorageClass 和容量的 PV，然后把 PV 和 PVC“绑定”在一起，实现存储的分配，和前面打电话要 A4 纸的过程差不多。</p>
<h3 id="如何在-kubernetes-里使用-persistentvolume">如何在 Kubernetes 里使用 PersistentVolume</h3>
<p>现在我们已经准备好了 PV 和 PVC，就可以让 Pod 实现持久化存储了。
首先需要用 kubectl apply 创建 PV 对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f host-path-pv.yml

</code></pre></td></tr></table>
</div>
</div><p>然后用 kubectl get  查看它的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pv
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/5c/37/5ca80e12c71d162f5707d37bf6009c37.png?wh=1920x150"
        data-srcset="https://static001.geekbang.org/resource/image/5c/37/5ca80e12c71d162f5707d37bf6009c37.png?wh=1920x150, https://static001.geekbang.org/resource/image/5c/37/5ca80e12c71d162f5707d37bf6009c37.png?wh=1920x150 1.5x, https://static001.geekbang.org/resource/image/5c/37/5ca80e12c71d162f5707d37bf6009c37.png?wh=1920x150 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/5c/37/5ca80e12c71d162f5707d37bf6009c37.png?wh=1920x150"
        title="img" />从截图里我们可以看到，这个 PV 的容量是 10MB，访问模式是 RWO（ReadWriteOnce)，StorageClass 是我们自己定义的 host-test，状态显示的是 Available，也就是处于可用状态，可以随时分配给 Pod 使用。
接下来我们创建 PVC，申请存储资源：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f host-path-pvc.yml
kubectl get pvc
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/fd/1f/fd6f1cb75f5d349860928594db29a11f.png?wh=1920x367"
        data-srcset="https://static001.geekbang.org/resource/image/fd/1f/fd6f1cb75f5d349860928594db29a11f.png?wh=1920x367, https://static001.geekbang.org/resource/image/fd/1f/fd6f1cb75f5d349860928594db29a11f.png?wh=1920x367 1.5x, https://static001.geekbang.org/resource/image/fd/1f/fd6f1cb75f5d349860928594db29a11f.png?wh=1920x367 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/fd/1f/fd6f1cb75f5d349860928594db29a11f.png?wh=1920x367"
        title="img" />一旦 PVC 对象创建成功，Kubernetes 就会立即通过 StorageClass、resources 等条件在集群里查找符合要求的 PV，如果找到合适的存储对象就会把它俩“绑定”在一起。
PVC 对象申请的是 5MB，但现在系统里只有一个 10MB 的 PV，没有更合适的对象，所以 Kubernetes 也只能把这个 PV 分配出去，多出的容量就算是“福利”了。
你会看到这两个对象的状态都是 Bound，也就是说存储申请成功，PVC 的实际容量就是 PV 的容量 10MB，而不是最初申请的容量 5MB。
那么，如果我们把 PVC 的申请容量改大一些会怎么样呢？比如改成 100MB：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/25/0c/25241a47c63cf629b88590ba1773710c.png?wh=1920x350"
        data-srcset="https://static001.geekbang.org/resource/image/25/0c/25241a47c63cf629b88590ba1773710c.png?wh=1920x350, https://static001.geekbang.org/resource/image/25/0c/25241a47c63cf629b88590ba1773710c.png?wh=1920x350 1.5x, https://static001.geekbang.org/resource/image/25/0c/25241a47c63cf629b88590ba1773710c.png?wh=1920x350 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/25/0c/25241a47c63cf629b88590ba1773710c.png?wh=1920x350"
        title="img" /></p>
<p>你会看到 PVC 会一直处于 Pending 状态，这意味着 Kubernetes 在系统里没有找到符合要求的存储，无法分配资源，只能等有满足要求的 PV 才能完成绑定。</p>
<h3 id="如何为-pod-挂载-persistentvolume">如何为 Pod 挂载 PersistentVolume</h3>
<p>PV 和 PVC 绑定好了，有了持久化存储，现在我们就可以为 Pod 挂载存储卷。用法和第 14 讲里差不多，先要在 spec.volumes 定义存储卷，然后在 containers.volumeMounts 挂载进容器。
不过因为我们用的是 PVC，所以要在 volumes 里用字段 persistentVolumeClaim 指定 PVC 的名字。
下面就是 Pod 的 YAML 描述文件，把存储卷挂载到了 Nginx 容器的 /tmp 目录：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: host-pvc-pod
spec:
  volumes:

  - name: host-pvc-vol
    persistentVolumeClaim:
      claimName: host-5m-pvc
    containers:
    - name: ngx-pvc-pod
      image: nginx:alpine
      ports:
      - containerPort: 80
        volumeMounts:
      - name: host-pvc-vol
        mountPath: /tmp
</code></pre></td></tr></table>
</div>
</div><p>我把 Pod 和 PVC/PV 的关系画成了图（省略了字段 accessModes），你可以从图里看出它们是如何联系起来的：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a4/d8/a4d709808a0ef729604c884c50748bd8.jpg?wh=1920x1310"
        data-srcset="https://static001.geekbang.org/resource/image/a4/d8/a4d709808a0ef729604c884c50748bd8.jpg?wh=1920x1310, https://static001.geekbang.org/resource/image/a4/d8/a4d709808a0ef729604c884c50748bd8.jpg?wh=1920x1310 1.5x, https://static001.geekbang.org/resource/image/a4/d8/a4d709808a0ef729604c884c50748bd8.jpg?wh=1920x1310 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a4/d8/a4d709808a0ef729604c884c50748bd8.jpg?wh=1920x1310"
        title="img" /></p>
<p>现在我们创建这个 Pod，查看它的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f host-path-pod.yml
kubectl get pod -o wide
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d4/9d/d4a2771c2c32597a4e5e2e60823c159d.png?wh=1846x192"
        data-srcset="https://static001.geekbang.org/resource/image/d4/9d/d4a2771c2c32597a4e5e2e60823c159d.png?wh=1846x192, https://static001.geekbang.org/resource/image/d4/9d/d4a2771c2c32597a4e5e2e60823c159d.png?wh=1846x192 1.5x, https://static001.geekbang.org/resource/image/d4/9d/d4a2771c2c32597a4e5e2e60823c159d.png?wh=1846x192 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d4/9d/d4a2771c2c32597a4e5e2e60823c159d.png?wh=1846x192"
        title="img" />它被 Kubernetes 调到了 worker 节点上，那么 PV 是否确实挂载成功了呢？让我们用 kubectl exec 进入容器，执行一些命令看看：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c4/24/c42a618688eee98555cda33c5c1d6824.png?wh=1320x364"
        data-srcset="https://static001.geekbang.org/resource/image/c4/24/c42a618688eee98555cda33c5c1d6824.png?wh=1320x364, https://static001.geekbang.org/resource/image/c4/24/c42a618688eee98555cda33c5c1d6824.png?wh=1320x364 1.5x, https://static001.geekbang.org/resource/image/c4/24/c42a618688eee98555cda33c5c1d6824.png?wh=1320x364 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c4/24/c42a618688eee98555cda33c5c1d6824.png?wh=1320x364"
        title="img" /></p>
<p>容器的 /tmp 目录里生成了一个 a.txt 的文件，根据 PV 的定义，它就应该落在 worker 节点的磁盘上，所以我们就登录 worker 节点检查一下：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/9d/c4/9dc40b80e2e4edb2d9449e2d43b02ac4.png?wh=988x242"
        data-srcset="https://static001.geekbang.org/resource/image/9d/c4/9dc40b80e2e4edb2d9449e2d43b02ac4.png?wh=988x242, https://static001.geekbang.org/resource/image/9d/c4/9dc40b80e2e4edb2d9449e2d43b02ac4.png?wh=988x242 1.5x, https://static001.geekbang.org/resource/image/9d/c4/9dc40b80e2e4edb2d9449e2d43b02ac4.png?wh=988x242 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/9d/c4/9dc40b80e2e4edb2d9449e2d43b02ac4.png?wh=988x242"
        title="img" /></p>
<p>你会看到确实在 worker 节点的本地目录有一个 a.txt 的文件，再对一下时间，就可以确认是刚才在 Pod 里生成的文件。
因为 Pod 产生的数据已经通过 PV 存在了磁盘上，所以如果 Pod 删除后再重新创建，挂载存储卷时会依然使用这个目录，数据保持不变，也就实现了持久化存储。
不过还有一点小问题，因为这个 PV 是 HostPath 类型，只在本节点存储，如果 Pod 重建时被调度到了其他节点上，那么即使加载了本地目录，也不会是之前的存储位置，持久化功能也就失效了。
所以，HostPath 类型的 PV 一般用来做测试，或者是用于 DaemonSet 这样与节点关系比较密切的应用，我们下节课再讲实现真正任意的数据持久化。</p>
<h3 id="小结-22">小结</h3>
<p>好了，今天我们一起学习了 Kubernetes 里应对持久化存储的解决方案，一共有三个 API 对象，分别是 PersistentVolume、PersistentVolumeClaim、StorageClass。它们管理的是集群里的存储资源，简单来说就是磁盘，Pod 必须通过它们才能够实现数据持久化。
再小结一下今天的主要内容：
PersistentVolume 简称为 PV，是 Kubernetes 对存储设备的抽象，由系统管理员维护，需要描述清楚存储设备的类型、访问模式、容量等信息。
PersistentVolumeClaim 简称为 PVC，代表 Pod 向系统申请存储资源，它声明对存储的要求，Kubernetes 会查找最合适的 PV 然后绑定。
StorageClass 抽象特定类型的存储系统，归类分组 PV 对象，用来简化 PV/PVC 的绑定过程。
HostPath 是最简单的一种 PV，数据存储在节点本地，速度快但不能跟随 Pod 迁移。</p>
<h2 id="25persistentvolume--nfs怎么使用网络共享存储">25｜PersistentVolume + NFS：怎么使用网络共享存储？</h2>
<p>在上节课里我们看到了 Kubernetes 里的持久化存储对象 PersistentVolume、PersistentVolumeClaim、StorageClass，把它们联合起来就可以为 Pod 挂载一块“虚拟盘”，让 Pod 在其中任意读写数据。
不过当时我们使用的是 HostPath，存储卷只能在本机使用，而 Kubernetes 里的 Pod 经常会在集群里“漂移”，所以这种方式不是特别实用。
要想让存储卷真正能被 Pod 任意挂载，我们需要变更存储的方式，不能限定在本地磁盘，而是要改成网络存储，这样 Pod 无论在哪里运行，只要知道 IP 地址或者域名，就可以通过网络通信访问存储设备。
网络存储是一个非常热门的应用领域，有很多知名的产品，比如 AWS、Azure、Ceph，Kubernetes 还专门定义了 CSI（Container Storage Interface）规范，不过这些存储类型的安装、使用都比较复杂，在我们的实验环境里部署难度比较高。
所以今天的这次课里，我选择了相对来说比较简单的 NFS 系统（Network File System），以它为例讲解如何在 Kubernetes 里使用网络存储，以及静态存储卷和动态存储卷的概念。</p>
<h3 id="如何安装-nfs-服务器">如何安装 NFS 服务器</h3>
<p>作为一个经典的网络存储系统，NFS 有着近 40 年的发展历史，基本上已经成为了各种 UNIX 系统的标准配置，Linux 自然也提供对它的支持。
NFS 采用的是 Client/Server 架构，需要选定一台主机作为 Server，安装 NFS 服务端；其他要使用存储的主机作为 Client，安装 NFS 客户端工具。
所以接下来，我们在自己的 Kubernetes 集群里再增添一台名字叫 Storage 的服务器，在上面安装 NFS，实现网络存储、共享网盘的功能。不过这台 Storage 也只是一个逻辑概念，我们在实际安装部署的时候完全可以把它合并到集群里的某台主机里，比如这里我就复用了第 17 讲里的 Console。
新的网络架构如下图所示：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/78/07/786e13af0e2f62f9cd73f5ab555a4507.jpg?wh=1920x1235"
        data-srcset="https://static001.geekbang.org/resource/image/78/07/786e13af0e2f62f9cd73f5ab555a4507.jpg?wh=1920x1235, https://static001.geekbang.org/resource/image/78/07/786e13af0e2f62f9cd73f5ab555a4507.jpg?wh=1920x1235 1.5x, https://static001.geekbang.org/resource/image/78/07/786e13af0e2f62f9cd73f5ab555a4507.jpg?wh=1920x1235 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/78/07/786e13af0e2f62f9cd73f5ab555a4507.jpg?wh=1920x1235"
        title="img" /></p>
<p>在 Ubuntu 系统里安装 NFS 服务端很容易，使用 apt 即可：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo apt -y install nfs-kernel-server

</code></pre></td></tr></table>
</div>
</div><p>安装好之后，你需要给 NFS 指定一个存储位置，也就是网络共享目录。一般来说，应该建立一个专门的 /data 目录，这里为了简单起见，我就使用了临时目录 /tmp/nfs：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">mkdir -p /tmp/nfs

</code></pre></td></tr></table>
</div>
</div><p>接下来你需要配置 NFS 访问共享目录，修改 /etc/exports，指定目录名、允许访问的网段，还有权限等参数。这些规则比较琐碎，和我们的 Kubernetes 课程关联不大，我就不详细解释了，你只要把下面这行加上就行，注意目录名和 IP 地址要改成和自己的环境一致：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">/tmp/nfs 192.168.10.0/24(rw,sync,no_subtree_check,no_root_squash,insecure)

</code></pre></td></tr></table>
</div>
</div><p>改好之后，需要用 exportfs -ra 通知 NFS，让配置生效，再用 exportfs -v 验证效果：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo exportfs -ra
sudo exportfs -v
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/0c/d1/0cd8889ee51c6d8a8947f6bd615d6bd1.png?wh=1920x116"
        data-srcset="https://static001.geekbang.org/resource/image/0c/d1/0cd8889ee51c6d8a8947f6bd615d6bd1.png?wh=1920x116, https://static001.geekbang.org/resource/image/0c/d1/0cd8889ee51c6d8a8947f6bd615d6bd1.png?wh=1920x116 1.5x, https://static001.geekbang.org/resource/image/0c/d1/0cd8889ee51c6d8a8947f6bd615d6bd1.png?wh=1920x116 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/0c/d1/0cd8889ee51c6d8a8947f6bd615d6bd1.png?wh=1920x116"
        title="img" />现在，你就可以使用 systemctl 来启动 NFS 服务器了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo systemctl start  nfs-server
sudo systemctl enable nfs-server
sudo systemctl status nfs-server
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/29/5a/29fb58f93f0e764ca8309ed9eff5175a.png?wh=1832x486"
        data-srcset="https://static001.geekbang.org/resource/image/29/5a/29fb58f93f0e764ca8309ed9eff5175a.png?wh=1832x486, https://static001.geekbang.org/resource/image/29/5a/29fb58f93f0e764ca8309ed9eff5175a.png?wh=1832x486 1.5x, https://static001.geekbang.org/resource/image/29/5a/29fb58f93f0e764ca8309ed9eff5175a.png?wh=1832x486 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/29/5a/29fb58f93f0e764ca8309ed9eff5175a.png?wh=1832x486"
        title="img" />你还可以使用命令 showmount 来检查 NFS 的网络挂载情况：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">showmount -e 127.0.0.1
</code></pre></td></tr></table>
</div>
</div><h3 id="如何安装-nfs-客户端">如何安装 NFS 客户端</h3>
<p>有了 NFS 服务器之后，为了让 Kubernetes 集群能够访问 NFS 存储服务，我们还需要在每个节点上都安装 NFS 客户端。
这项工作只需要一条 apt 命令，不需要额外的配置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo apt -y install nfs-common

</code></pre></td></tr></table>
</div>
</div><p>同样，在节点上可以用 showmount 检查 NFS 能否正常挂载，注意 IP 地址要写成 NFS 服务器的地址，我在这里就是“192.168.10.208”：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/7e/9c/7ed89f8468d6d4fa315a6d456f2eee9c.png?wh=1182x186"
        data-srcset="https://static001.geekbang.org/resource/image/7e/9c/7ed89f8468d6d4fa315a6d456f2eee9c.png?wh=1182x186, https://static001.geekbang.org/resource/image/7e/9c/7ed89f8468d6d4fa315a6d456f2eee9c.png?wh=1182x186 1.5x, https://static001.geekbang.org/resource/image/7e/9c/7ed89f8468d6d4fa315a6d456f2eee9c.png?wh=1182x186 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/7e/9c/7ed89f8468d6d4fa315a6d456f2eee9c.png?wh=1182x186"
        title="img" /></p>
<p>现在让我们尝试手动挂载一下 NFS 网络存储，先创建一个目录 /tmp/test 作为挂载点：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">mkdir -p /tmp/test

</code></pre></td></tr></table>
</div>
</div><p>然后用命令 mount 把 NFS 服务器的共享目录挂载到刚才创建的本地目录上：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sudo mount -t nfs 192.168.10.208:/tmp/nfs /tmp/test

</code></pre></td></tr></table>
</div>
</div><p>最后测试一下，我们在 /tmp/test 里随便创建一个文件，比如 x.yml：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">touch /tmp/test/x.yml

</code></pre></td></tr></table>
</div>
</div><p>再回到 NFS 服务器，检查共享目录 /tmp/nfs，应该会看到也出现了一个同样的文件 x.yml，这就说明 NFS 安装成功了。之后集群里的任意节点，只要通过 NFS 客户端，就能把数据写入 NFS 服务器，实现网络存储。</p>
<h3 id="如何使用-nfs-存储卷">如何使用 NFS 存储卷</h3>
<p>现在我们已经为 Kubernetes 配置好了 NFS 存储系统，就可以使用它来创建新的 PV 存储对象了。
先来手工分配一个存储卷，需要指定 storageClassName 是 nfs，而 accessModes 可以设置成 ReadWriteMany，这是由 NFS 的特性决定的，它支持多个节点同时访问一个共享目录。
因为这个存储卷是 NFS 系统，所以我们还需要在 YAML 里添加 nfs 字段，指定 NFS 服务器的 IP 地址和共享目录名。
这里我在 NFS 服务器的 /tmp/nfs 目录里又创建了一个新的目录 1g-pv，表示分配了 1GB 的可用存储空间，相应的，PV 里的 capacity 也要设置成同样的数值，也就是 1Gi。
把这些字段都整理好后，我们就得到了一个使用 NFS 网络存储的 YAML 描述文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-1g-pv
spec:
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 1Gi
  nfs:
    path: /tmp/nfs/1g-pv
    server: 192.168.10.208
</code></pre></td></tr></table>
</div>
</div><p>现在就可以用命令 kubectl apply 来创建 PV 对象，再用 kubectl get pv 查看它的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f nfs-static-pv.yml
kubectl get pv
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3b/39/3bb0be2483e92467d3cac14fbc635739.png?wh=1688x300"
        data-srcset="https://static001.geekbang.org/resource/image/3b/39/3bb0be2483e92467d3cac14fbc635739.png?wh=1688x300, https://static001.geekbang.org/resource/image/3b/39/3bb0be2483e92467d3cac14fbc635739.png?wh=1688x300 1.5x, https://static001.geekbang.org/resource/image/3b/39/3bb0be2483e92467d3cac14fbc635739.png?wh=1688x300 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3b/39/3bb0be2483e92467d3cac14fbc635739.png?wh=1688x300"
        title="img" />再次提醒你注意，spec.nfs 里的 IP 地址一定要正确，路径一定要存在（事先创建好)，否则 Kubernetes 按照 PV 的描述会无法挂载 NFS 共享目录，PV 就会处于“pending”状态无法使用。
有了 PV，我们就可以定义申请存储的 PVC 对象了，它的内容和 PV 差不多，但不涉及 NFS 存储的细节，只需要用 resources.request 来表示希望要有多大的容量，这里我写成 1GB，和 PV 的容量相同：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-static-pvc
spec:
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
</code></pre></td></tr></table>
</div>
</div><p>创建 PVC 对象之后，Kubernetes 就会根据 PVC 的描述，找到最合适的 PV，把它们“绑定”在一起，也就是存储分配成功：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a7/8c/a7bbcc5dce117f9872cee3f08e6a6c8c.png?wh=1920x309"
        data-srcset="https://static001.geekbang.org/resource/image/a7/8c/a7bbcc5dce117f9872cee3f08e6a6c8c.png?wh=1920x309, https://static001.geekbang.org/resource/image/a7/8c/a7bbcc5dce117f9872cee3f08e6a6c8c.png?wh=1920x309 1.5x, https://static001.geekbang.org/resource/image/a7/8c/a7bbcc5dce117f9872cee3f08e6a6c8c.png?wh=1920x309 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a7/8c/a7bbcc5dce117f9872cee3f08e6a6c8c.png?wh=1920x309"
        title="img" /></p>
<p>我们再创建一个 Pod，把 PVC 挂载成它的一个 volume，具体的做法和上节课是一样的，用 persistentVolumeClaim 指定 PVC 的名字就可以了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: nfs-static-pod
spec:
  volumes:

  - name: nfs-pvc-vol
    persistentVolumeClaim:
      claimName: nfs-static-pvc
    containers:
    - name: nfs-pvc-test
      image: nginx:alpine
      ports:
      - containerPort: 80
        volumeMounts:
        - name: nfs-pvc-vol
          mountPath: /tmp
</code></pre></td></tr></table>
</div>
</div><p>Pod、PVC、PV 和 NFS 存储的关系可以用下图来形象地表示，你可以对比一下 HostPath PV 的用法，看看有什么不同：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/2a/a7/2a21d16b028afdea4f525439bd8f06a7.jpg?wh=1920x1125"
        data-srcset="https://static001.geekbang.org/resource/image/2a/a7/2a21d16b028afdea4f525439bd8f06a7.jpg?wh=1920x1125, https://static001.geekbang.org/resource/image/2a/a7/2a21d16b028afdea4f525439bd8f06a7.jpg?wh=1920x1125 1.5x, https://static001.geekbang.org/resource/image/2a/a7/2a21d16b028afdea4f525439bd8f06a7.jpg?wh=1920x1125 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/2a/a7/2a21d16b028afdea4f525439bd8f06a7.jpg?wh=1920x1125"
        title="img" /></p>
<p>因为我们在 PV/PVC 里指定了 storageClassName 是 nfs，节点上也安装了 NFS 客户端，所以 Kubernetes 就会自动执行 NFS 挂载动作，把 NFS 的共享目录 /tmp/nfs/1g-pv 挂载到 Pod 里的 /tmp，完全不需要我们去手动管理。
最后还是测试一下，用 kubectl apply 创建 Pod 之后，我们用 kubectl exec 进入 Pod，再试着操作 NFS 共享目录：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/bb/90/bbc244b6cd21b71f50807864718d8990.png?wh=1386x542"
        data-srcset="https://static001.geekbang.org/resource/image/bb/90/bbc244b6cd21b71f50807864718d8990.png?wh=1386x542, https://static001.geekbang.org/resource/image/bb/90/bbc244b6cd21b71f50807864718d8990.png?wh=1386x542 1.5x, https://static001.geekbang.org/resource/image/bb/90/bbc244b6cd21b71f50807864718d8990.png?wh=1386x542 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/bb/90/bbc244b6cd21b71f50807864718d8990.png?wh=1386x542"
        title="img" /></p>
<p>退出 Pod，再看一下 NFS 服务器的 /tmp/nfs/1g-pv 目录，你就会发现 Pod 里创建的文件确实写入了共享目录：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/87/d0/87cdc722da478db6f938db4d424be0d0.png?wh=756x354"
        data-srcset="https://static001.geekbang.org/resource/image/87/d0/87cdc722da478db6f938db4d424be0d0.png?wh=756x354, https://static001.geekbang.org/resource/image/87/d0/87cdc722da478db6f938db4d424be0d0.png?wh=756x354 1.5x, https://static001.geekbang.org/resource/image/87/d0/87cdc722da478db6f938db4d424be0d0.png?wh=756x354 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/87/d0/87cdc722da478db6f938db4d424be0d0.png?wh=756x354"
        title="img" /></p>
<p>而且更好的是，因为 NFS 是一个网络服务，不会受 Pod 调度位置的影响，所以只要网络通畅，这个 PV 对象就会一直可用，数据也就实现了真正的持久化存储。</p>
<h3 id="如何部署-nfs-provisoner">如何部署 NFS Provisoner</h3>
<p>现在有了 NFS 这样的网络存储系统，你是不是认为 Kubernetes 里的数据持久化问题就已经解决了呢？
对于这个问题，我觉得可以套用一句现在的流行语：“解决了，但没有完全解决。”
说它“解决了”，是因为网络存储系统确实能够让集群里的 Pod 任意访问，数据在 Pod 销毁后仍然存在，新创建的 Pod 可以再次挂载，然后读取之前写入的数据，整个过程完全是自动化的。
说它“没有完全解决”，是因为 PV 还是需要人工管理，必须要由系统管理员手动维护各种存储设备，再根据开发需求逐个创建 PV，而且 PV 的大小也很难精确控制，容易出现空间不足或者空间浪费的情况。
在我们的这个实验环境里，只有很少的 PV 需求，管理员可以很快分配 PV 存储卷，但是在一个大集群里，每天可能会有几百几千个应用需要 PV 存储，如果仍然用人力来管理分配存储，管理员很可能会忙得焦头烂额，导致分配存储的工作大量积压。
那么能不能让创建 PV 的工作也实现自动化呢？或者说，让计算机来代替人类来分配存储卷呢？
这个在 Kubernetes 里就是“动态存储卷”的概念，它可以用 StorageClass 绑定一个 Provisioner 对象，而这个 Provisioner 就是一个能够自动管理存储、创建 PV 的应用，代替了原来系统管理员的手工劳动。
有了“动态存储卷”的概念，前面我们讲的手工创建的 PV 就可以称为“静态存储卷”。
目前，Kubernetes 里每类存储设备都有相应的 Provisioner 对象，对于 NFS 来说，它的 Provisioner 就是“NFS subdir external provisioner”，你可以在 GitHub 上找到这个项目（https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner）。
NFS Provisioner 也是以 Pod 的形式运行在 Kubernetes 里的，在 GitHub 的 deploy 目录里是部署它所需的 YAML 文件，一共有三个，分别是 rbac.yaml、class.yaml 和 deployment.yaml。
不过这三个文件只是示例，想在我们的集群里真正运行起来还要修改其中的两个文件。
第一个要修改的是 rbac.yaml，它使用的是默认的 default 名字空间，应该把它改成其他的名字空间，避免与普通应用混在一起，你可以用“查找替换”的方式把它统一改成 kube-system。
第二个要修改的是 deployment.yaml，它要修改的地方比较多。首先要把名字空间改成和 rbac.yaml 一样，比如是 kube-system，然后重点要修改 volumes 和 env 里的 IP 地址和共享目录名，必须和集群里的 NFS 服务器配置一样。
按照我们当前的环境设置，就应该把 IP 地址改成 192.168.10.208，目录名改成 /tmp/nfs：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">spec:
  template:
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
      ...
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.10.208        #改IP地址
            - name: NFS_PATH
              value: /tmp/nfs              #改共享目录名
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.10.208         #改IP地址
            Path: /tmp/nfs                 #改共享目录名
</code></pre></td></tr></table>
</div>
</div><p>还有一件麻烦事，deployment.yaml 的镜像仓库用的是 gcr.io，拉取很困难，而国内的镜像网站上偏偏还没有它，为了让实验能够顺利进行，我不得不“曲线救国”，把它的镜像转存到了 Docker Hub 上。
所以你还需要把镜像的名字由原来的“k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2”改成“chronolaw/nfs-subdir-external-provisioner:v4.0.2”，其实也就是变动一下镜像的用户名而已。
把这两个 YAML 修改好之后，我们就可以在 Kubernetes 里创建 NFS Provisioner 了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f rbac.yaml
kubectl apply -f class.yaml
kubectl apply -f deployment.yaml
</code></pre></td></tr></table>
</div>
</div><p>使用命令 kubectl get，再加上名字空间限定 -n kube-system，就可以看到 NFS Provisioner 在 Kubernetes 里运行起来了。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/35/6d/35758cbe60ddf264bcf59d703fd4986d.png?wh=1920x407"
        data-srcset="https://static001.geekbang.org/resource/image/35/6d/35758cbe60ddf264bcf59d703fd4986d.png?wh=1920x407, https://static001.geekbang.org/resource/image/35/6d/35758cbe60ddf264bcf59d703fd4986d.png?wh=1920x407 1.5x, https://static001.geekbang.org/resource/image/35/6d/35758cbe60ddf264bcf59d703fd4986d.png?wh=1920x407 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/35/6d/35758cbe60ddf264bcf59d703fd4986d.png?wh=1920x407"
        title="img" /></p>
<h3 id="如何使用-nfs-动态存储卷">如何使用 NFS 动态存储卷</h3>
<p>比起静态存储卷，动态存储卷的用法简单了很多。因为有了 Provisioner，我们就不再需要手工定义 PV 对象了，只需要在 PVC 里指定 StorageClass 对象，它再关联到 Provisioner。
我们来看一下 NFS 默认的 StorageClass 定义：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner 
parameters:
  archiveOnDelete: &#34;false&#34;
</code></pre></td></tr></table>
</div>
</div><p>YAML 里的关键字段是 provisioner，它指定了应该使用哪个 Provisioner。另一个字段 parameters 是调节 Provisioner 运行的参数，需要参考文档来确定具体值，在这里的 archiveOnDelete: &ldquo;false&rdquo; 就是自动回收存储空间。
理解了 StorageClass 的 YAML 之后，你也可以不使用默认的 StorageClass，而是根据自己的需求，任意定制具有不同存储特性的 StorageClass，比如添加字段 onDelete: &ldquo;retain&rdquo; 暂时保留分配的存储，之后再手动删除：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client-retained
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  onDelete: &#34;retain&#34;
</code></pre></td></tr></table>
</div>
</div><p>接下来我们定义一个 PVC，向系统申请 10MB 的存储空间，使用的 StorageClass 是默认的 nfs-client：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-dyn-10m-pvc
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
</code></pre></td></tr></table>
</div>
</div><p>写好了 PVC，我们还是在 Pod 里用 volumes 和 volumeMounts 挂载，然后 Kubernetes 就会自动找到 NFS Provisioner，在 NFS 的共享目录上创建出合适的 PV 对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: nfs-dyn-pod
spec:
  volumes:

  - name: nfs-dyn-10m-vol
    persistentVolumeClaim:
      claimName: nfs-dyn-10m-pvc
    containers:
    - name: nfs-dyn-test
      image: nginx:alpine
      ports:
      - containerPort: 80
        volumeMounts:
        - name: nfs-dyn-10m-vol
          mountPath: /tmp
</code></pre></td></tr></table>
</div>
</div><p>使用 kubectl apply 创建好 PVC 和 Pod，让我们来查看一下集群里的 PV 状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/57/bb/570d73409db1edc757yy10e6aba56ebb.png?wh=1920x271"
        data-srcset="https://static001.geekbang.org/resource/image/57/bb/570d73409db1edc757yy10e6aba56ebb.png?wh=1920x271, https://static001.geekbang.org/resource/image/57/bb/570d73409db1edc757yy10e6aba56ebb.png?wh=1920x271 1.5x, https://static001.geekbang.org/resource/image/57/bb/570d73409db1edc757yy10e6aba56ebb.png?wh=1920x271 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/57/bb/570d73409db1edc757yy10e6aba56ebb.png?wh=1920x271"
        title="img" /></p>
<p>从截图你可以看到，虽然我们没有直接定义 PV 对象，但由于有 NFS Provisioner，它就自动创建一个 PV，大小刚好是在 PVC 里申请的 10MB。
如果你这个时候再去 NFS 服务器上查看共享目录，也会发现多出了一个目录，名字与这个自动创建的 PV 一样，但加上了名字空间和 PVC 的前缀：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a9/ea/a9b6942b824bc9f7841850ee15yy68ea.png?wh=1714x126"
        data-srcset="https://static001.geekbang.org/resource/image/a9/ea/a9b6942b824bc9f7841850ee15yy68ea.png?wh=1714x126, https://static001.geekbang.org/resource/image/a9/ea/a9b6942b824bc9f7841850ee15yy68ea.png?wh=1714x126 1.5x, https://static001.geekbang.org/resource/image/a9/ea/a9b6942b824bc9f7841850ee15yy68ea.png?wh=1714x126 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a9/ea/a9b6942b824bc9f7841850ee15yy68ea.png?wh=1714x126"
        title="img" /></p>
<p>我还是把 Pod、PVC、StorageClass 和 Provisioner 的关系画成了一张图，你可以清楚地看出来这些对象的关联关系，还有 Pod 是如何最终找到存储设备的：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/e3/1e/e3905990be6fb8739fb51a4ab9856f1e.jpg?wh=1920x856"
        data-srcset="https://static001.geekbang.org/resource/image/e3/1e/e3905990be6fb8739fb51a4ab9856f1e.jpg?wh=1920x856, https://static001.geekbang.org/resource/image/e3/1e/e3905990be6fb8739fb51a4ab9856f1e.jpg?wh=1920x856 1.5x, https://static001.geekbang.org/resource/image/e3/1e/e3905990be6fb8739fb51a4ab9856f1e.jpg?wh=1920x856 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/e3/1e/e3905990be6fb8739fb51a4ab9856f1e.jpg?wh=1920x856"
        title="img" /></p>
<h3 id="小结-23">小结</h3>
<p>好了，今天的这节课里我们继续学习 PV/PVC，引入了网络存储系统，以 NFS 为例研究了静态存储卷和动态存储卷的用法，其中的核心对象是 StorageClass 和 Provisioner。
我再小结一下今天的要点：
在 Kubernetes 集群里，网络存储系统更适合数据持久化，NFS 是最容易使用的一种网络存储系统，要事先安装好服务端和客户端。
可以编写 PV 手工定义 NFS 静态存储卷，要指定 NFS 服务器的 IP 地址和共享目录名。
使用 NFS 动态存储卷必须要部署相应的 Provisioner，在 YAML 里正确配置 NFS 服务器。
动态存储卷不需要手工定义 PV，而是要定义 StorageClass，由关联的 Provisioner 自动创建 PV 完成绑定。</p>
<h2 id="26statefulset怎么管理有状态的应用">26｜StatefulSet：怎么管理有状态的应用？</h2>
<p>在中级篇里，我们学习了 Deployment 和 DaemonSet 两种 API 对象，它们是在 Kubernetes 集群里部署应用的重要工具，不过它们也有一个缺点，只能管理“无状态应用”（Stateless Application），不能管理“有状态应用”（Stateful Application）。
“有状态应用”的处理比较复杂，要考虑的事情很多，但是这些问题我们其实可以通过组合之前学过的 Deployment、Service、PersistentVolume 等对象来解决。
今天我们就来研究一下什么是“有状态应用”，然后看看 Kubernetes 为什么会设计一个新对象——StatefulSet 来专门管理“有状态应用”。</p>
<h3 id="什么是有状态的应用">什么是有状态的应用</h3>
<p>我们先从 PersistentVolume 谈起，它为 Kubernetes 带来了持久化存储的功能，能够让应用把数据存放在本地或者远程的磁盘上。
那么你有没有想过，持久化存储，对应用来说，究竟意味着什么呢？
有了持久化存储，应用就可以把一些运行时的关键数据落盘，相当于有了一份“保险”，如果 Pod 发生意外崩溃，也只不过像是按下了暂停键，等重启后挂载 Volume，再加载原数据就能够满血复活，恢复之前的“状态”继续运行。
注意到了吗？这里有一个关键词——“状态”，应用保存的数据，实际上就是它某个时刻的“运行状态”。
所以从这个角度来说，理论上任何应用都是有状态的。
只是有的应用的状态信息不是很重要，即使不恢复状态也能够正常运行，这就是我们常说的“无状态应用”。“无状态应用”典型的例子就是 Nginx 这样的 Web 服务器，它只是处理 HTTP 请求，本身不生产数据（日志除外），不需要特意保存状态，无论以什么状态重启都能很好地对外提供服务。
还有一些应用，运行状态信息就很重要了，如果因为重启而丢失了状态是绝对无法接受的，这样的应用就是“有状态应用”。
“有状态应用”的例子也有很多，比如 Redis、MySQL 这样的数据库，它们的“状态”就是在内存或者磁盘上产生的数据，是应用的核心价值所在，如果不能够把这些数据及时保存再恢复，那绝对会是灾难性的后果。
理解了这一点，我们结合目前学到的知识思考一下：Deployment 加上 PersistentVolume，在 Kubernetes 里是不是可以轻松管理有状态的应用了呢？
的确，用 Deployment 来保证高可用，用 PersistentVolume 来存储数据，确实可以部分达到管理“有状态应用”的目的（你可以自己试着编写这样的 YAML）。
但是 Kubernetes 的眼光则更加全面和长远，它认为“状态”不仅仅是数据持久化，在集群化、分布式的场景里，还有多实例的依赖关系、启动顺序和网络标识等问题需要解决，而这些问题恰恰是 Deployment 力所不及的。
因为只使用 Deployment，多个实例之间是无关的，启动的顺序不固定，Pod 的名字、IP 地址、域名也都是完全随机的，这正是“无状态应用”的特点。
但对于“有状态应用”，多个实例之间可能存在依赖关系，比如 master/slave、active/passive，需要依次启动才能保证应用正常运行，外界的客户端也可能要使用固定的网络标识来访问实例，而且这些信息还必须要保证在 Pod 重启后不变。
所以，Kubernetes 就在 Deployment 的基础之上定义了一个新的 API 对象，名字也很好理解，就叫 StatefulSet，专门用来管理有状态的应用。</p>
<h3 id="如何使用-yaml-描述-statefulset">如何使用 YAML 描述 StatefulSet</h3>
<p>首先我们还是用命令 kubectl api-resources 来查看 StatefulSet 的基本信息，可以知道它的简称是 sts，YAML 文件头信息是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: xxx-sts
</code></pre></td></tr></table>
</div>
</div><p>和 DaemonSet 类似，StatefulSet 也可以看做是 Deployment 的一个特例，它也不能直接用 kubectl create 创建样板文件，但它的对象描述和 Deployment 差不多，你同样可以把 Deployment 适当修改一下，就变成了 StatefulSet 对象。
这里我给出了一个使用 Redis 的 StatefulSet，你来看看它与 Deployment 有什么差异：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-sts
spec:
  serviceName: redis-svc
  replicas: 2
  selector:
    matchLabels:
      app: redis-sts
  template:
    metadata:
      labels:
        app: redis-sts
    spec:
      containers:
      - image: redis:5-alpine
        name: redis
        ports:
        - containerPort: 6379
</code></pre></td></tr></table>
</div>
</div><p>我们会发现，YAML 文件里除了 kind 必须是“StatefulSet”，在 spec 里还多出了一个“serviceName”字段，其余的部分和 Deployment 是一模一样的，比如 replicas、selector、template 等等。
这两个不同之处其实就是 StatefulSet 与 Deployment 的关键区别。想要真正理解这一点，我们得结合 StatefulSet 在 Kubernetes 里的使用方法来分析。</p>
<h3 id="如何在-kubernetes-里使用-statefulset">如何在 Kubernetes 里使用 StatefulSet</h3>
<p>让我们用 kubectl apply 创建 StatefulSet 对象，用 kubectl get 先看看它是什么样的：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f redis-sts.yml
kubectl get sts
kubectl get pod
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/71/88/71b485401dca6946fe4788fa97e3fd88.png?wh=1268x414"
        data-srcset="https://static001.geekbang.org/resource/image/71/88/71b485401dca6946fe4788fa97e3fd88.png?wh=1268x414, https://static001.geekbang.org/resource/image/71/88/71b485401dca6946fe4788fa97e3fd88.png?wh=1268x414 1.5x, https://static001.geekbang.org/resource/image/71/88/71b485401dca6946fe4788fa97e3fd88.png?wh=1268x414 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/71/88/71b485401dca6946fe4788fa97e3fd88.png?wh=1268x414"
        title="img" />从截图里，你应该能够看到，StatefulSet 所管理的 Pod 不再是随机的名字了，而是有了顺序编号，从 0 开始分别被命名为 redis-sts-0、redis-sts-1，Kubernetes 也会按照这个顺序依次创建（0 号比 1 号的 AGE 要长一点)，这就解决了“有状态应用”的第一个问题：启动顺序。
有了启动的先后顺序，应用该怎么知道自己的身份，进而确定互相之间的依赖关系呢？
Kubernetes 给出的方法是使用 hostname，也就是每个 Pod 里的主机名，让我们再用 kubectl exec 登录 Pod 内部看看：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl exec -it redis-sts-0 -- sh
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/be/39/be44f94eaf07f3591c7a2a8b9cdd1739.png?wh=1308x468"
        data-srcset="https://static001.geekbang.org/resource/image/be/39/be44f94eaf07f3591c7a2a8b9cdd1739.png?wh=1308x468, https://static001.geekbang.org/resource/image/be/39/be44f94eaf07f3591c7a2a8b9cdd1739.png?wh=1308x468 1.5x, https://static001.geekbang.org/resource/image/be/39/be44f94eaf07f3591c7a2a8b9cdd1739.png?wh=1308x468 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/be/39/be44f94eaf07f3591c7a2a8b9cdd1739.png?wh=1308x468"
        title="img" />在 Pod 里查看环境变量 $HOSTNAME 或者是执行命令 hostname，都可以得到这个 Pod 的名字 redis-sts-0。
有了这个唯一的名字，应用就可以自行决定依赖关系了，比如在这个 Redis 例子里，就可以让先启动的 0 号 Pod 是主实例，后启动的 1 号 Pod 是从实例。
解决了启动顺序和依赖关系，还剩下第三个问题：网络标识，这就需要用到 Service 对象。
不过这里又有一点奇怪的地方，我们不能用命令 kubectl expose 直接为 StatefulSet 生成 Service，只能手动编写 YAML。但是这肯定难不倒你，经过了这么多练习，现在你应该能很轻松地写出一个 Service 对象。
因为不能自动生成，你在写 Service 对象的时候要小心一些，metadata.name 必须和 StatefulSet 里的 serviceName 相同，selector 里的标签也必须和 StatefulSet 里的一致：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Service
metadata:
  name: redis-svc
spec:
  selector:
    app: redis-sts
  ports:

  - port: 6379
    protocol: TCP
    targetPort: 6379
</code></pre></td></tr></table>
</div>
</div><p>写好 Service 之后，还是用 kubectl apply 创建这个对象：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/5f/c8/5f8e4dbedaa563801bb6bbe09c441dc8.png?wh=1584x1056"
        data-srcset="https://static001.geekbang.org/resource/image/5f/c8/5f8e4dbedaa563801bb6bbe09c441dc8.png?wh=1584x1056, https://static001.geekbang.org/resource/image/5f/c8/5f8e4dbedaa563801bb6bbe09c441dc8.png?wh=1584x1056 1.5x, https://static001.geekbang.org/resource/image/5f/c8/5f8e4dbedaa563801bb6bbe09c441dc8.png?wh=1584x1056 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/5f/c8/5f8e4dbedaa563801bb6bbe09c441dc8.png?wh=1584x1056"
        title="img" /></p>
<p>可以看到这个 Service 并没有什么特殊的地方，也是用标签选择器找到 StatefulSet 管理的两个 Pod，然后找到它们的 IP 地址。
不过，StatefulSet 的奥秘就在它的域名上。
还记得在第 20 讲里我们说过的 Service 的域名用法吗？Service 自己会有一个域名，格式是“对象名. 名字空间”，每个 Pod 也会有一个域名，形式是“IP 地址. 名字空间”。但因为 IP 地址不稳定，所以 Pod 的域名并不实用，一般我们会使用稳定的 Service 域名。
当我们把 Service 对象应用于 StatefulSet 的时候，情况就不一样了。
Service 发现这些 Pod 不是一般的应用，而是有状态应用，需要有稳定的网络标识，所以就会为 Pod 再多创建出一个新的域名，格式是“Pod 名. 服务名. 名字空间.svc.cluster.local”。当然，这个域名也可以简写成“Pod 名. 服务名”。
我们还是用 kubectl exec 进入 Pod 内部，用 ping 命令来验证一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl exec -it redis-sts-0 -- sh
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/f1/39/f1b058b5fb3e5218c638ca0534b92439.png?wh=1524x1338"
        data-srcset="https://static001.geekbang.org/resource/image/f1/39/f1b058b5fb3e5218c638ca0534b92439.png?wh=1524x1338, https://static001.geekbang.org/resource/image/f1/39/f1b058b5fb3e5218c638ca0534b92439.png?wh=1524x1338 1.5x, https://static001.geekbang.org/resource/image/f1/39/f1b058b5fb3e5218c638ca0534b92439.png?wh=1524x1338 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/f1/39/f1b058b5fb3e5218c638ca0534b92439.png?wh=1524x1338"
        title="img" />显然，在 StatefulSet 里的这两个 Pod 都有了各自的域名，也就是稳定的网络标识。那么接下来，外部的客户端只要知道了 StatefulSet 对象，就可以用固定的编号去访问某个具体的实例了，虽然 Pod 的 IP 地址可能会变，但这个有编号的域名由 Service 对象维护，是稳定不变的。
到这里，通过 StatefulSet 和 Service 的联合使用，Kubernetes 就解决了“有状态应用”的依赖关系、启动顺序和网络标识这三个问题，剩下的多实例之间内部沟通协调等事情就需要应用自己去想办法处理了。
关于 Service，有一点值得再多提一下。
Service 原本的目的是负载均衡，应该由它在 Pod 前面来转发流量，但是对 StatefulSet 来说，这项功能反而是不必要的，因为 Pod 已经有了稳定的域名，外界访问服务就不应该再通过 Service 这一层了。所以，从安全和节约系统资源的角度考虑，我们可以在 Service 里添加一个字段 clusterIP: None ，告诉 Kubernetes 不必再为这个对象分配 IP 地址。
我画了一张图展示 StatefulSet 与 Service 对象的关系，你可以参考一下它们字段之间的互相引用：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/49/22/490d814cf0f25db56537a20f3af57e22.jpg?wh=1920x1094"
        data-srcset="https://static001.geekbang.org/resource/image/49/22/490d814cf0f25db56537a20f3af57e22.jpg?wh=1920x1094, https://static001.geekbang.org/resource/image/49/22/490d814cf0f25db56537a20f3af57e22.jpg?wh=1920x1094 1.5x, https://static001.geekbang.org/resource/image/49/22/490d814cf0f25db56537a20f3af57e22.jpg?wh=1920x1094 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/49/22/490d814cf0f25db56537a20f3af57e22.jpg?wh=1920x1094"
        title="img" /></p>
<h3 id="如何实现-statefulset-的数据持久化">如何实现 StatefulSet 的数据持久化</h3>
<p>现在 StatefulSet 已经有了固定的名字、启动顺序和网络标识，只要再给它加上数据持久化功能，我们就可以实现对“有状态应用”的管理了。
这里就能用到上一节课里学的 PersistentVolume 和 NFS 的知识，我们可以很容易地定义 StorageClass，然后编写 PVC，再给 Pod 挂载 Volume。
不过，为了强调持久化存储与 StatefulSet 的一对一绑定关系，Kubernetes 为 StatefulSet 专门定义了一个字段“volumeClaimTemplates”，直接把 PVC 定义嵌入 StatefulSet 的 YAML 文件里。这样能保证创建 StatefulSet 的同时，就会为每个 Pod 自动创建 PVC，让 StatefulSet 的可用性更高。
“volumeClaimTemplates”这个字段好像有点难以理解，你可以把它和 Pod 的 template、Job 的 jobTemplate 对比起来学习，它其实也是一个“套娃”的对象组合结构，里面就是应用了 StorageClass 的普通 PVC 而已。
让我们把刚才的 Redis StatefulSet 对象稍微改造一下，加上持久化存储功能：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-pv-sts
spec:
  serviceName: redis-pv-svc
  volumeClaimTemplates:

  - metadata:
    name: redis-100m-pvc
    spec:
      storageClassName: nfs-client
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 100Mi
    replicas: 2
    selector:
    matchLabels:
      app: redis-pv-sts
    template:
    metadata:
      labels:
        app: redis-pv-sts
    spec:
      containers:
      - image: redis:5-alpine
        name: redis
        ports:
        - containerPort: 6379
          volumeMounts:
        - name: redis-100m-pvc
          mountPath: /data
</code></pre></td></tr></table>
</div>
</div><p>这个 YAML 文件比较长，内容比较多，不过你只要有点耐心，分功能模块逐个去看也能很快看明白。
首先 StatefulSet 对象的名字是 redis-pv-sts，表示它使用了 PV 存储。然后“volumeClaimTemplates”里定义了一个 PVC，名字是 redis-100m-pvc，申请了 100MB 的 NFS 存储。在 Pod 模板里用 volumeMounts 引用了这个 PVC，把网盘挂载到了 /data 目录，也就是 Redis 的数据目录。
下面的这张图就是这个 StatefulSet 对象完整的关系图：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/1a/0f/1a06987c87f3db948b591883a81bac0f.jpg?wh=4000x2946"
        data-srcset="https://static001.geekbang.org/resource/image/1a/0f/1a06987c87f3db948b591883a81bac0f.jpg?wh=4000x2946, https://static001.geekbang.org/resource/image/1a/0f/1a06987c87f3db948b591883a81bac0f.jpg?wh=4000x2946 1.5x, https://static001.geekbang.org/resource/image/1a/0f/1a06987c87f3db948b591883a81bac0f.jpg?wh=4000x2946 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/1a/0f/1a06987c87f3db948b591883a81bac0f.jpg?wh=4000x2946"
        title="img" /></p>
<p>最后使用 kubectl apply 创建这些对象，一个带持久化功能的“有状态应用”就算是运行起来了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f redis-pv-sts.yml

</code></pre></td></tr></table>
</div>
</div><p>你可以使用命令 kubectl get pvc 来查看 StatefulSet 关联的存储卷状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/33/f5/33eee3c5a5033e4bf73f5003669c4ff5.png?wh=1920x189"
        data-srcset="https://static001.geekbang.org/resource/image/33/f5/33eee3c5a5033e4bf73f5003669c4ff5.png?wh=1920x189, https://static001.geekbang.org/resource/image/33/f5/33eee3c5a5033e4bf73f5003669c4ff5.png?wh=1920x189 1.5x, https://static001.geekbang.org/resource/image/33/f5/33eee3c5a5033e4bf73f5003669c4ff5.png?wh=1920x189 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/33/f5/33eee3c5a5033e4bf73f5003669c4ff5.png?wh=1920x189"
        title="img" /></p>
<p>看这两个 PVC 的命名，不是随机的，是有规律的，用的是 PVC 名字加上 StatefulSet 的名字组合而成，所以即使 Pod 被销毁，因为它的名字不变，还能够找到这个 PVC，再次绑定使用之前存储的数据。
那我们就来实地验证一下吧，用 kubectl exec 运行 Redis 的客户端，在里面添加一些 KV 数据：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl exec -it redis-pv-sts-0 -- redis-cli
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/94/b7/94a96b1b8a000dcd852d2ea11yy8ddb7.png?wh=1562x530"
        data-srcset="https://static001.geekbang.org/resource/image/94/b7/94a96b1b8a000dcd852d2ea11yy8ddb7.png?wh=1562x530, https://static001.geekbang.org/resource/image/94/b7/94a96b1b8a000dcd852d2ea11yy8ddb7.png?wh=1562x530 1.5x, https://static001.geekbang.org/resource/image/94/b7/94a96b1b8a000dcd852d2ea11yy8ddb7.png?wh=1562x530 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/94/b7/94a96b1b8a000dcd852d2ea11yy8ddb7.png?wh=1562x530"
        title="img" />这里我设置了两个值，分别是 a=111 和 b=222。
现在我们模拟意外事故，删除这个 Pod：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete pod redis-pv-sts-0

</code></pre></td></tr></table>
</div>
</div><p>由于 StatefulSet 和 Deployment 一样会监控 Pod 的实例，发现 Pod 数量少了就会很快创建出新的 Pod，并且名字、网络标识也都会和之前的 Pod 一模一样：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/52/23/52e2f02a1d80d8bba2a42c8258cda923.png?wh=1300x236"
        data-srcset="https://static001.geekbang.org/resource/image/52/23/52e2f02a1d80d8bba2a42c8258cda923.png?wh=1300x236, https://static001.geekbang.org/resource/image/52/23/52e2f02a1d80d8bba2a42c8258cda923.png?wh=1300x236 1.5x, https://static001.geekbang.org/resource/image/52/23/52e2f02a1d80d8bba2a42c8258cda923.png?wh=1300x236 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/52/23/52e2f02a1d80d8bba2a42c8258cda923.png?wh=1300x236"
        title="img" /></p>
<p>那 Redis 里存储的数据怎么样了呢？是不是真的用到了持久化存储，也完全恢复了呢？
你可以再用 Redis 客户端登录去检查一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl exec -it redis-pv-sts-0 -- redis-cli
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c7/08/c78ca845ee20459dd2d8bayy3db71808.png?wh=1544x530"
        data-srcset="https://static001.geekbang.org/resource/image/c7/08/c78ca845ee20459dd2d8bayy3db71808.png?wh=1544x530, https://static001.geekbang.org/resource/image/c7/08/c78ca845ee20459dd2d8bayy3db71808.png?wh=1544x530 1.5x, https://static001.geekbang.org/resource/image/c7/08/c78ca845ee20459dd2d8bayy3db71808.png?wh=1544x530 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c7/08/c78ca845ee20459dd2d8bayy3db71808.png?wh=1544x530"
        title="img" />因为我们把 NFS 网络存储挂载到了 Pod 的 /data 目录，Redis 就会定期把数据落盘保存，所以新创建的 Pod 再次挂载目录的时候会从备份文件里恢复数据，内存里的数据就恢复原状了。</p>
<h3 id="小结-24">小结</h3>
<p>好了，今天我们学习了专门部署“有状态应用”的 API 对象 StatefulSet，它与 Deployment 非常相似，区别是由它管理的 Pod 会有固定的名字、启动顺序和网络标识，这些特性对于在集群里实施有主从、主备等关系的应用非常重要。
我再简单小结一下今天的内容：
StatefulSet 的 YAML 描述和 Deployment 几乎完全相同，只是多了一个关键字段 serviceName。
要为 StatefulSet 里的 Pod 生成稳定的域名，需要定义 Service 对象，它的名字必须和 StatefulSet 里的 serviceName 一致。
访问 StatefulSet 应该使用每个 Pod 的单独域名，形式是“Pod 名. 服务名”，不应该使用 Service 的负载均衡功能。
在 StatefulSet 里可以用字段“volumeClaimTemplates”直接定义 PVC，让 Pod 实现数据持久化存储。</p>
<h2 id="27滚动更新如何做到平滑的应用升级降级">27｜滚动更新：如何做到平滑的应用升级降级？</h2>
<p>上次课里我们学习了管理有状态应用的对象 StatefulSet，再加上管理无状态应用的 Deployment 和 DaemonSet，我们就能在 Kubernetes 里部署任意形式的应用了。
不过，只是把应用发布到集群里是远远不够的，要让应用稳定可靠地运行，还需要有持续的运维工作。
如果你还记得在第 18 节课里，我们学过 Deployment 的“应用伸缩”功能就是一种常见的运维操作，在 Kubernetes 里，使用命令 kubectl scale，我们就可以轻松调整 Deployment 下属的 Pod 数量，因为 StatefulSet 是 Deployment 的一种特例，所以它也可以使用 kubectl scale 来实现“应用伸缩”。
除了“应用伸缩”，其他的运维操作比如应用更新、版本回退等工作，该怎么做呢？这些也是我们日常运维中经常会遇到的问题。
今天我就以 Deployment 为例，来讲讲 Kubernetes 在应用管理方面的高级操作：滚动更新，使用 kubectl rollout 实现用户无感知的应用升级和降级。</p>
<h3 id="kubernetes-如何定义应用版本">Kubernetes 如何定义应用版本</h3>
<p>应用的版本更新，大家都知道是怎么回事，比如我们发布了 V1 版，过了几天加了新功能，要发布 V2 版。
不过说起来简单，版本更新实际做起来是一个相当棘手的事。因为系统已经上线运行，必须要保证不间断地对外提供服务，通俗地说就是“给空中的飞机换引擎”。尤其在以前，需要开发、测试、运维、监控、网络等各个部门的一大堆人来协同工作，费时又费力。
但是，应用的版本更新其实是有章可循的，现在我们有了 Kubernetes 这个强大的自动化运维管理系统，就可以把它的过程抽象出来，让计算机去完成那些复杂繁琐的人工操作。
在 Kubernetes 里，版本更新使用的不是 API 对象，而是两个命令：kubectl apply 和 kubectl rollout，当然它们也要搭配部署应用所需要的 Deployment、DaemonSet 等 YAML 文件。
不过在我们信心满满开始操作之前，首先要理解在 Kubernetes 里，所谓的“版本”到底是什么？
我们常常会简单地认为“版本”就是应用程序的“版本号”，或者是容器镜像的“标签”，但不要忘了，在 Kubernetes 里应用都是以 Pod 的形式运行的，而 Pod 通常又会被 Deployment 等对象来管理，所以应用的“版本更新”实际上更新的是整个 Pod。
那 Pod 又是由什么来决定的呢？
仔细回忆一下之前我们创建的那么多个对象，你就会发现，Pod 是由 YAML 描述文件来确定的，更准确地说，是 Deployment 等对象里的字段 template。
所以，在 Kubernetes 里应用的版本变化就是 template 里 Pod 的变化，哪怕 template 里只变动了一个字段，那也会形成一个新的版本，也算是版本变化。
但 template 里的内容太多了，拿这么长的字符串来当做“版本号”不太现实，所以 Kubernetes 就使用了“摘要”功能，用摘要算法计算 template 的 Hash 值作为“版本号”，虽然不太方便识别，但是很实用。
我们就拿第 18 讲里的 Nginx Deployment 作为例子吧，创建对象之后，使用 kubectl get 来查看 Pod 的状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/67/7b/67bc5178acde882a57265d6413158a7b.png?wh=1550x234"
        data-srcset="https://static001.geekbang.org/resource/image/67/7b/67bc5178acde882a57265d6413158a7b.png?wh=1550x234, https://static001.geekbang.org/resource/image/67/7b/67bc5178acde882a57265d6413158a7b.png?wh=1550x234 1.5x, https://static001.geekbang.org/resource/image/67/7b/67bc5178acde882a57265d6413158a7b.png?wh=1550x234 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/67/7b/67bc5178acde882a57265d6413158a7b.png?wh=1550x234"
        title="img" /></p>
<p>Pod 名字里的那串随机数“6796……”就是 Pod 模板的 Hash 值，也就是 Pod 的“版本号”。
如果你变动了 Pod YAML 描述，比如把镜像改成 nginx:stable-alpine，或者把容器名字改成 nginx-test，都会生成一个新的应用版本，kubectl apply 后就会重新创建 Pod：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/15/1e/15e17760079a03f046aa67f6e34b511e.png?wh=1560x236"
        data-srcset="https://static001.geekbang.org/resource/image/15/1e/15e17760079a03f046aa67f6e34b511e.png?wh=1560x236, https://static001.geekbang.org/resource/image/15/1e/15e17760079a03f046aa67f6e34b511e.png?wh=1560x236 1.5x, https://static001.geekbang.org/resource/image/15/1e/15e17760079a03f046aa67f6e34b511e.png?wh=1560x236 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/15/1e/15e17760079a03f046aa67f6e34b511e.png?wh=1560x236"
        title="img" /></p>
<p>你可以看到，Pod 名字里的 Hash 值变成了“7c6c……”，这就表示 Pod 的版本更新了。</p>
<h3 id="kubernetes-如何实现应用更新">Kubernetes 如何实现应用更新</h3>
<p>为了更仔细地研究 Kubernetes 的应用更新过程，让我们来略微改造一下 Nginx Deployment 对象，看看 Kubernetes 到底是怎么实现版本更新的。
首先修改 ConfigMap，让它输出 Nginx 的版本号，方便我们用 curl 查看版本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ConfigMap
metadata:
  name: ngx-conf
data:
  default.conf: |
    server {
      listen 80;
      location / {
        default_type text/plain;
        return 200
          &#39;ver : $nginx_version\nsrv : $server_addr:$server_port\nhost: $hostname\n&#39;;
      }
    }
</code></pre></td></tr></table>
</div>
</div><p>然后我们修改 Pod 镜像，明确地指定版本号是 1.21-alpine，实例数设置为 4 个：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-dep
spec:
  replicas: 4
  ... ...
      containers:
      - image: nginx:1.21-alpine
  ... ...
</code></pre></td></tr></table>
</div>
</div><p>把它命名为 ngx-v1.yml，然后执行命令 kubectl apply 部署这个应用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f ngx-v1.yml

</code></pre></td></tr></table>
</div>
</div><p>我们还可以为它创建 Service 对象，再用 kubectl port-forward 转发请求来查看状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl port-forward svc/ngx-svc 8080:80 &amp;
curl 127.1:8080
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/20/52/20d23af1305e2d2b4f66b951c09dac52.png?wh=1532x928"
        data-srcset="https://static001.geekbang.org/resource/image/20/52/20d23af1305e2d2b4f66b951c09dac52.png?wh=1532x928, https://static001.geekbang.org/resource/image/20/52/20d23af1305e2d2b4f66b951c09dac52.png?wh=1532x928 1.5x, https://static001.geekbang.org/resource/image/20/52/20d23af1305e2d2b4f66b951c09dac52.png?wh=1532x928 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/20/52/20d23af1305e2d2b4f66b951c09dac52.png?wh=1532x928"
        title="img" />从 curl 命令的输出中可以看到，现在应用的版本是 1.21.6。
现在，让我们编写一个新版本对象 ngx-v2.yml，把镜像升级到 nginx:1.22-alpine，其他的都不变。
因为 Kubernetes 的动作太快了，为了能够观察到应用更新的过程，我们还需要添加一个字段 minReadySeconds，让 Kubernetes 在更新过程中等待一点时间，确认 Pod 没问题才继续其余 Pod 的创建工作。
要提醒你注意的是，minReadySeconds 这个字段不属于 Pod 模板，所以它不会影响 Pod 版本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-dep
spec:
  minReadySeconds: 15      # 确认Pod就绪的等待时间 
  replicas: 4
  ... ...
      containers:
      - image: nginx:1.22-alpine
  ... ...
</code></pre></td></tr></table>
</div>
</div><p>现在我们执行命令 kubectl apply 来更新应用，因为改动了镜像名，Pod 模板变了，就会触发“版本更新”，然后用一个新命令：kubectl rollout status，来查看应用更新的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f ngx-v2.yml
kubectl rollout status deployment ngx-dep
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6d/7f/6d4023181fe180d50eb4cca7755a207f.png?wh=1920x486"
        data-srcset="https://static001.geekbang.org/resource/image/6d/7f/6d4023181fe180d50eb4cca7755a207f.png?wh=1920x486, https://static001.geekbang.org/resource/image/6d/7f/6d4023181fe180d50eb4cca7755a207f.png?wh=1920x486 1.5x, https://static001.geekbang.org/resource/image/6d/7f/6d4023181fe180d50eb4cca7755a207f.png?wh=1920x486 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6d/7f/6d4023181fe180d50eb4cca7755a207f.png?wh=1920x486"
        title="img" />更新完成后，你再执行 kubectl get pod，就会看到 Pod 已经全部替换成了新版本“d575……”，用 curl 访问 Nginx，输出信息也变成了“1.22.0”：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6a/64/6a1776c3yy1ec374510af9e560401064.png?wh=1594x646"
        data-srcset="https://static001.geekbang.org/resource/image/6a/64/6a1776c3yy1ec374510af9e560401064.png?wh=1594x646, https://static001.geekbang.org/resource/image/6a/64/6a1776c3yy1ec374510af9e560401064.png?wh=1594x646 1.5x, https://static001.geekbang.org/resource/image/6a/64/6a1776c3yy1ec374510af9e560401064.png?wh=1594x646 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6a/64/6a1776c3yy1ec374510af9e560401064.png?wh=1594x646"
        title="img" /></p>
<p>仔细查看 kubectl rollout status 的输出信息，你可以发现，Kubernetes 不是把旧 Pod 全部销毁再一次性创建出新 Pod，而是在逐个地创建新 Pod，同时也在销毁旧 Pod，保证系统里始终有足够数量的 Pod 在运行，不会有“空窗期”中断服务。
新 Pod 数量增加的过程有点像是“滚雪球”，从零开始，越滚越大，所以这就是所谓的“滚动更新”（rolling update）。
使用命令 kubectl describe 可以更清楚地看到 Pod 的变化情况：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl describe deploy ngx-dep
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3b/fa/3b88d6b0d609e3b99f33b4f8e997c3fa.png?wh=1232x550"
        data-srcset="https://static001.geekbang.org/resource/image/3b/fa/3b88d6b0d609e3b99f33b4f8e997c3fa.png?wh=1232x550, https://static001.geekbang.org/resource/image/3b/fa/3b88d6b0d609e3b99f33b4f8e997c3fa.png?wh=1232x550 1.5x, https://static001.geekbang.org/resource/image/3b/fa/3b88d6b0d609e3b99f33b4f8e997c3fa.png?wh=1232x550 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3b/fa/3b88d6b0d609e3b99f33b4f8e997c3fa.png?wh=1232x550"
        title="img" />一开始的时候 V1 Pod（即 ngx-dep-54b865d75）的数量是 4；
当“滚动更新”开始的时候，Kubernetes 创建 1 个 V2 Pod（即 ngx-dep-d575d5776)，并且把 V1 Pod 数量减少到 3；
接着再增加 V2 Pod 的数量到 2，同时 V1 Pod 的数量变成了 1；
最后 V2 Pod 的数量达到预期值 4，V1 Pod 的数量变成了 0，整个更新过程就结束了。
看到这里你是不是有点明白了呢，其实“滚动更新”就是由 Deployment 控制的两个同步进行的“应用伸缩”操作，老版本缩容到 0，同时新版本扩容到指定值，是一个“此消彼长”的过程。
这个滚动更新的过程我画了一张图，你可以参考它来进一步体会：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b3/31/b3abe70db73a9da71a1793722e743731.jpg?wh=1920x729"
        data-srcset="https://static001.geekbang.org/resource/image/b3/31/b3abe70db73a9da71a1793722e743731.jpg?wh=1920x729, https://static001.geekbang.org/resource/image/b3/31/b3abe70db73a9da71a1793722e743731.jpg?wh=1920x729 1.5x, https://static001.geekbang.org/resource/image/b3/31/b3abe70db73a9da71a1793722e743731.jpg?wh=1920x729 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b3/31/b3abe70db73a9da71a1793722e743731.jpg?wh=1920x729"
        title="img" /></p>
<h3 id="kubernetes-如何管理应用更新">Kubernetes 如何管理应用更新</h3>
<p>Kubernetes 的“滚动更新”功能确实非常方便，不需要任何人工干预就能简单地把应用升级到新版本，也不会中断服务，不过如果更新过程中发生了错误或者更新后发现有 Bug 该怎么办呢？
要解决这两个问题，我们还是要用 kubectl rollout 命令。
在应用更新的过程中，你可以随时使用 kubectl rollout pause 来暂停更新，检查、修改 Pod，或者测试验证，如果确认没问题，再用 kubectl rollout resume 来继续更新。
这两个命令比较简单，我就不多做介绍了，要注意的是它们只支持 Deployment，不能用在 DaemonSet、StatefulSet 上（最新的 1.24 支持了 StatefulSet 的滚动更新）。
对于更新后出现的问题，Kubernetes 为我们提供了“后悔药”，也就是更新历史，你可以查看之前的每次更新记录，并且回退到任何位置，和我们开发常用的 Git 等版本控制软件非常类似。
查看更新历史使用的命令是 kubectl rollout history：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl rollout history deploy ngx-dep
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306"
        data-srcset="https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306, https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306 1.5x, https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306"
        title="img" />它会输出一个版本列表，因为我们创建 Nginx Deployment 是一个版本，更新又是一个版本，所以这里就会有两条历史记录。
但 kubectl rollout history 的列表输出的有用信息太少，你可以在命令后加上参数 &ndash;revision 来查看每个版本的详细信息，包括标签、镜像名、环境变量、存储卷等等，通过这些就可以大致了解每次都变动了哪些关键字段：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl rollout history deploy --revision=2
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/0f/7c/0f8c4d0a230b97bb1a74d745c220677c.png?wh=1608x1054"
        data-srcset="https://static001.geekbang.org/resource/image/0f/7c/0f8c4d0a230b97bb1a74d745c220677c.png?wh=1608x1054, https://static001.geekbang.org/resource/image/0f/7c/0f8c4d0a230b97bb1a74d745c220677c.png?wh=1608x1054 1.5x, https://static001.geekbang.org/resource/image/0f/7c/0f8c4d0a230b97bb1a74d745c220677c.png?wh=1608x1054 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/0f/7c/0f8c4d0a230b97bb1a74d745c220677c.png?wh=1608x1054"
        title="img" />假设我们认为刚刚更新的 nginx:1.22-alpine 不好，想要回退到上一个版本，就可以使用命令 kubectl rollout undo，也可以加上参数 &ndash;to-revision 回退到任意一个历史版本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl rollout undo deploy ngx-dep
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/14/c7/149345b7df104ac70c23a6c877a9b1c7.png?wh=1408x406"
        data-srcset="https://static001.geekbang.org/resource/image/14/c7/149345b7df104ac70c23a6c877a9b1c7.png?wh=1408x406, https://static001.geekbang.org/resource/image/14/c7/149345b7df104ac70c23a6c877a9b1c7.png?wh=1408x406 1.5x, https://static001.geekbang.org/resource/image/14/c7/149345b7df104ac70c23a6c877a9b1c7.png?wh=1408x406 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/14/c7/149345b7df104ac70c23a6c877a9b1c7.png?wh=1408x406"
        title="img" />kubectl rollout undo 的操作过程其实和 kubectl apply 是一样的，执行的仍然是“滚动更新”，只不过使用的是旧版本 Pod 模板，把新版本 Pod 数量收缩到 0，同时把老版本 Pod 扩展到指定值。
这个 V2 到 V1 的“版本降级”的过程我同样画了一张图，它和从 V1 到 V2 的“版本升级”过程是完全一样的，不同的只是版本号的变化方向：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/0c/29/0cbb6eec008546c4f5106de5ece20329.jpg?wh=1920x695"
        data-srcset="https://static001.geekbang.org/resource/image/0c/29/0cbb6eec008546c4f5106de5ece20329.jpg?wh=1920x695, https://static001.geekbang.org/resource/image/0c/29/0cbb6eec008546c4f5106de5ece20329.jpg?wh=1920x695 1.5x, https://static001.geekbang.org/resource/image/0c/29/0cbb6eec008546c4f5106de5ece20329.jpg?wh=1920x695 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/0c/29/0cbb6eec008546c4f5106de5ece20329.jpg?wh=1920x695"
        title="img" /></p>
<h3 id="kubernetes-如何添加更新描述">Kubernetes 如何添加更新描述</h3>
<p>讲到这里，Kubernetes 里应用更新的功能就学得差不多了。
不过，你有没有觉得 kubectl rollout history 的版本列表好像有点太简单了呢？只有一个版本更新序号，而另一列 CHANGE-CAUSE 为什么总是显示成 <none> 呢？能不能像 Git 一样，每次更新也加上说明信息呢？<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306"
        data-srcset="https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306, https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306 1.5x, https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/7c/09/7cc86862b28829c58c00eeb0fcdfbd09.png?wh=1398x306"
        title="img" /></p>
<p>这当然是可以的，做法也很简单，我们只需要在 Deployment 的 metadata 里加上一个新的字段 annotations。
annotations 字段的含义是“注解”“注释”，形式上和 labels 一样，都是 Key-Value，也都是给 API 对象附加一些额外的信息，但是用途上区别很大。
annotations 添加的信息一般是给 Kubernetes 内部的各种对象使用的，有点像是“扩展属性”；
labels 主要面对的是 Kubernetes 外部的用户，用来筛选、过滤对象的。
如果用一个简单的比喻来说呢，annotations 就是包装盒里的产品说明书，而 labels 是包装盒外的标签贴纸。
借助 annotations，Kubernetes 既不破坏对象的结构，也不用新增字段，就能够给 API 对象添加任意的附加信息，这就是面向对象设计中典型的 OCP“开闭原则”，让对象更具扩展性和灵活性。
annotations 里的值可以任意写，Kubernetes 会自动忽略不理解的 Key-Value，但要编写更新说明就需要使用特定的字段 kubernetes.io/change-cause。
下面来操作一下，我们创建 3 个版本的 Nginx 应用，同时添加更新说明：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-dep
  annotations:
    kubernetes.io/change-cause: v1, ngx=1.21
... ...
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-dep
  annotations:
    kubernetes.io/change-cause: update to v2, ngx=1.22
... ...
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-dep
  annotations:
    kubernetes.io/change-cause: update to v3, change name
... ...
</code></pre></td></tr></table>
</div>
</div><p>你需要注意 YAML 里的 metadata 部分，使用 annotations.kubernetes.io/change-cause 描述了版本更新的情况，相比 kubectl rollout history &ndash;revision 的罗列大量信息更容易理解。
依次使用 kubectl apply 创建并更新对象之后，我们再用 kubectl rollout history 来看一下更新历史：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/74/69/74bcc2020yy6b121634b3cbf972fe669.png?wh=1398x356"
        data-srcset="https://static001.geekbang.org/resource/image/74/69/74bcc2020yy6b121634b3cbf972fe669.png?wh=1398x356, https://static001.geekbang.org/resource/image/74/69/74bcc2020yy6b121634b3cbf972fe669.png?wh=1398x356 1.5x, https://static001.geekbang.org/resource/image/74/69/74bcc2020yy6b121634b3cbf972fe669.png?wh=1398x356 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/74/69/74bcc2020yy6b121634b3cbf972fe669.png?wh=1398x356"
        title="img" /></p>
<p>这次显示的列表信息就好看多了，每个版本的主要变动情况列得非常清楚，和 Git 版本管理的感觉很像。</p>
<h3 id="小结-25">小结</h3>
<p>好，今天我们一起学习了 Kubernetes 里的高级应用管理功能：滚动更新，它会自动缩放新旧版本的 Pod 数量，能够在用户无感知的情况下实现服务升级或降级，让原本复杂棘手的运维工作变得简单又轻松。
再小结一下今天的要点：
在 Kubernetes 里应用的版本不仅仅是容器镜像，而是整个 Pod 模板，为了便于处理使用了摘要算法，计算模板的 Hash 值作为版本号。
Kubernetes 更新应用采用的是滚动更新策略，减少旧版本 Pod 的同时增加新版本 Pod，保证在更新过程中服务始终可用。
管理应用更新使用的命令是 kubectl rollout，子命令有 status、history、undo 等。
Kubernetes 会记录应用的更新历史，可以使用 history &ndash;revision 查看每个版本的详细信息，也可以在每次更新时添加注解 kubernetes.io/change-cause。
另外，在 Deployment 里还有其他一些字段可以对滚动更新的过程做更细致的控制，它们都在 spec.strategy.rollingUpdate 里，比如 maxSurge、maxUnavailable 等字段，分别控制最多新增 Pod 数和最多不可用 Pod 数，一般用默认值就足够了，你如果感兴趣也可以查看 Kubernetes 文档进一步研究。</p>
<h2 id="28应用保障如何让pod运行得更健康">28｜应用保障：如何让Pod运行得更健康？</h2>
<p>在前面这么多节的课程中，我们都是在研究如何使用各种 API 对象来管理、操作 Pod，而对 Pod 本身的关注却不是太多。
作为 Kubernetes 里的核心概念和原子调度单位，Pod 的主要职责是管理容器，以逻辑主机、容器集合、进程组的形式来代表应用，它的重要性是不言而喻的。
那么今天我们回过头来，在之前那些上层 API 对象的基础上，一起来看看在 Kubernetes 里配置 Pod 的两种方法：资源配额 Resources、检查探针 Probe，它们能够给 Pod 添加各种运行保障，让应用运行得更健康。</p>
<h3 id="容器资源配额">容器资源配额</h3>
<p>早在第 2 讲的时候我们就说过，创建容器有三大隔离技术：namespace、cgroup、chroot。其中的 namespace 实现了独立的进程空间，chroot 实现了独立的文件系统，但唯独没有看到 cgroup 的具体应用。
cgroup 的作用是管控 CPU、内存，保证容器不会无节制地占用基础资源，进而影响到系统里的其他应用。
不过，容器总是要使用 CPU 和内存的，该怎么处理好需求与限制这两者之间的关系呢？
Kubernetes 的做法与我们在第 24 讲里提到的 PersistentVolumeClaim 用法有些类似，就是容器需要先提出一个“书面申请”，Kubernetes 再依据这个“申请”决定资源是否分配和如何分配。
但是 CPU、内存与存储卷有明显的不同，因为它是直接“内置”在系统里的，不像硬盘那样需要“外挂”，所以申请和管理的过程也就会简单很多。
具体的申请方法很简单，只要在 Pod 容器的描述部分添加一个新字段 resources 就可以了，它就相当于申请资源的 Claim。
来看一个 YAML 示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: ngx-pod-resources
spec:
  containers:

  - image: nginx:alpine
    name: ngx
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 20m
        memory: 200Mi
</code></pre></td></tr></table>
</div>
</div><p>这个 YAML 文件定义了一个 Nginx Pod，我们需要重点学习的是 containers.resources，它下面有两个字段：
“requests”，意思是容器要申请的资源，也就是说要求 Kubernetes 在创建 Pod 的时候必须分配这里列出的资源，否则容器就无法运行。
“limits”，意思是容器使用资源的上限，不能超过设定值，否则就有可能被强制停止运行。
在请求 cpu 和 memory 这两种资源的时候，你需要特别注意它们的表示方式。
内存的写法和磁盘容量一样，使用 Ki、Mi、Gi 来表示 KB、MB、GB，比如 512Ki、100Mi、0.5Gi 等。
而 CPU 因为在计算机中数量有限，非常宝贵，所以 Kubernetes 允许容器精细分割 CPU，即可以 1 个、2 个地完整使用 CPU，也可以用小数 0.1、0.2 的方式来部分使用 CPU。这其实是效仿了 UNIX“时间片”的用法，意思是进程最多可以占用多少 CPU 时间。
不过 CPU 时间也不能无限分割，Kubernetes 里 CPU 的最小使用单位是 0.001，为了方便表示用了一个特别的单位 m，也就是“milli”“毫”的意思，比如说 500m 就相当于 0.5。
现在我们再来看这个 YAML，你就应该明白了，它向系统申请的是 1% 的 CPU 时间和 100MB 的内存，运行时的资源上限是 2%CPU 时间和 200MB 内存。有了这个申请，Kubernetes 就会在集群中查找最符合这个资源要求的节点去运行 Pod。
下面是我在网上找的一张动图，Kubernetes 会根据每个 Pod 声明的需求，像搭积木或者玩俄罗斯方块一样，把节点尽量“塞满”，充分利用每个节点的资源，让集群的效益最大化。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/39/91/397bfabd8234f8d859ca877a58f0d191.gif?wh=800x765"
        data-srcset="https://static001.geekbang.org/resource/image/39/91/397bfabd8234f8d859ca877a58f0d191.gif?wh=800x765, https://static001.geekbang.org/resource/image/39/91/397bfabd8234f8d859ca877a58f0d191.gif?wh=800x765 1.5x, https://static001.geekbang.org/resource/image/39/91/397bfabd8234f8d859ca877a58f0d191.gif?wh=800x765 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/39/91/397bfabd8234f8d859ca877a58f0d191.gif?wh=800x765"
        title="img" /></p>
<p>你可能会有疑问：如果 Pod 不写 resources 字段，Kubernetes 会如何处理呢？
这就意味着 Pod 对运行的资源要求“既没有下限，也没有上限”，Kubernetes 不用管 CPU 和内存是否足够，可以把 Pod 调度到任意的节点上，而且后续 Pod 运行时也可以无限制地使用 CPU 和内存。
我们课程里是实验环境，这样做是当然是没有问题的，但如果是生产环境就很危险了，Pod 可能会因为资源不足而运行缓慢，或者是占用太多资源而影响其他应用，所以我们应当合理评估 Pod 的资源使用情况，尽量为 Pod 加上限制。
看到这里估计你会继续追问：如果预估错误，Pod 申请的资源太多，系统无法满足会怎么样呢？
让我们来试一下吧，先删除 Pod 的资源限制 resources.limits，把 resources.request.cpu 改成比较极端的“10”，也就是要求 10 个 CPU：
&hellip;</p>
<pre><code>resources:
  requests:
    cpu: 10
</code></pre>
<p>然后使用 kubectl apply 创建这个 Pod，你可能会惊奇地发现，虽然我们的 Kubernetes 集群里只有 3 个 CPU，但 Pod 也能创建成功。
不过我们再用 kubectl get pod 去查看的话，就会发现它处于“Pending”状态，实际上并没有真正被调度运行：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b1/d4/b1154e089533df5cfabc18c7e9c442d4.png?wh=1380x176"
        data-srcset="https://static001.geekbang.org/resource/image/b1/d4/b1154e089533df5cfabc18c7e9c442d4.png?wh=1380x176, https://static001.geekbang.org/resource/image/b1/d4/b1154e089533df5cfabc18c7e9c442d4.png?wh=1380x176 1.5x, https://static001.geekbang.org/resource/image/b1/d4/b1154e089533df5cfabc18c7e9c442d4.png?wh=1380x176 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b1/d4/b1154e089533df5cfabc18c7e9c442d4.png?wh=1380x176"
        title="img" /></p>
<p>使用命令 kubectl describe 来查看具体原因，会发现有这么一句提示：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/95/02/9577c36e53c723b8e28ddb2d5e77e502.png?wh=1920x156"
        data-srcset="https://static001.geekbang.org/resource/image/95/02/9577c36e53c723b8e28ddb2d5e77e502.png?wh=1920x156, https://static001.geekbang.org/resource/image/95/02/9577c36e53c723b8e28ddb2d5e77e502.png?wh=1920x156 1.5x, https://static001.geekbang.org/resource/image/95/02/9577c36e53c723b8e28ddb2d5e77e502.png?wh=1920x156 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/95/02/9577c36e53c723b8e28ddb2d5e77e502.png?wh=1920x156"
        title="img" /></p>
<p>这就很明确地告诉我们 Kubernetes 调度失败，当前集群里的所有节点都无法运行这个 Pod，因为它要求的 CPU 实在是太多了。</p>
<h3 id="什么是容器状态探针">什么是容器状态探针</h3>
<p>现在，我们使用 resources 字段加上资源配额之后，Pod 在 Kubernetes 里的运行就有了初步保障，Kubernetes 会监控 Pod 的资源使用情况，让它既不会“饿死”也不会“撑死”。
但这只是最初级的运行保障，如果你开发或者运维过实际的后台服务就会知道，一个程序即使正常启动了，它也有可能因为某些原因无法对外提供服务。其中最常见的情况就是运行时发生“死锁”或者“死循环”的故障，这个时候从外部来看进程一切都是正常的，但内部已经是一团糟了。
所以，我们还希望 Kubernetes 这个“保姆”能够更细致地监控 Pod 的状态，除了保证崩溃重启，还必须要能够探查到 Pod 的内部运行状态，定时给应用做“体检”，让应用时刻保持“健康”，能够满负荷稳定工作。
那应该用什么手段来检查应用的健康状态呢？
因为应用程序各式各样，对于外界来说就是一个黑盒子，只能看到启动、运行、停止这三个基本状态，此外就没有什么好的办法来知道它内部是否正常了。
所以，我们必须把应用变成灰盒子，让部分内部信息对外可见，这样 Kubernetes 才能够探查到内部的状态。
这么说起来，检查的过程倒是有点像现在我们很熟悉的核酸检测，Kubernetes 用一根小棉签在应用的“检查口”里提取点数据，就可以从这些信息来判断应用是否“健康”了，这项功能也就被形象地命名为“探针”（Probe），也可以叫“探测器”。
Kubernetes 为检查应用状态定义了三种探针，它们分别对应容器不同的状态：
Startup，启动探针，用来检查应用是否已经启动成功，适合那些有大量初始化工作要做，启动很慢的应用。
Liveness，存活探针，用来检查应用是否正常运行，是否存在死锁、死循环。
Readiness，就绪探针，用来检查应用是否可以接收流量，是否能够对外提供服务。
你需要注意这三种探针是递进的关系：应用程序先启动，加载完配置文件等基本的初始化数据就进入了 Startup 状态，之后如果没有什么异常就是 Liveness 存活状态，但可能有一些准备工作没有完成，还不一定能对外提供服务，只有到最后的 Readiness 状态才是一个容器最健康可用的状态。
初次接触这三种状态可能有点难理解，我画了一张图，你可以看一下状态与探针的对应关系：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/ea/84/eaff5e640171984a4b1b2285982ee184.jpg?wh=1920x1000"
        data-srcset="https://static001.geekbang.org/resource/image/ea/84/eaff5e640171984a4b1b2285982ee184.jpg?wh=1920x1000, https://static001.geekbang.org/resource/image/ea/84/eaff5e640171984a4b1b2285982ee184.jpg?wh=1920x1000 1.5x, https://static001.geekbang.org/resource/image/ea/84/eaff5e640171984a4b1b2285982ee184.jpg?wh=1920x1000 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/ea/84/eaff5e640171984a4b1b2285982ee184.jpg?wh=1920x1000"
        title="img" /></p>
<p>那 Kubernetes 具体是如何使用状态和探针来管理容器的呢？
如果一个 Pod 里的容器配置了探针，Kubernetes 在启动容器后就会不断地调用探针来检查容器的状态：
如果 Startup 探针失败，Kubernetes 会认为容器没有正常启动，就会尝试反复重启，当然其后面的 Liveness 探针和 Readiness 探针也不会启动。
如果 Liveness 探针失败，Kubernetes 就会认为容器发生了异常，也会重启容器。
如果 Readiness 探针失败，Kubernetes 会认为容器虽然在运行，但内部有错误，不能正常提供服务，就会把容器从 Service 对象的负载均衡集合中排除，不会给它分配流量。
知道了 Kubernetes 对这三种状态的处理方式，我们就可以在开发应用的时候编写适当的检查机制，让 Kubernetes 用“探针”定时为应用做“体检”了。
在刚才图的基础上，我又补充了 Kubernetes 的处理动作，看这张图你就能很好地理解容器探针的工作流程了：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/64/d9/64fde55dd2eab68f9968ff34218646d9.jpg?wh=1920x1200"
        data-srcset="https://static001.geekbang.org/resource/image/64/d9/64fde55dd2eab68f9968ff34218646d9.jpg?wh=1920x1200, https://static001.geekbang.org/resource/image/64/d9/64fde55dd2eab68f9968ff34218646d9.jpg?wh=1920x1200 1.5x, https://static001.geekbang.org/resource/image/64/d9/64fde55dd2eab68f9968ff34218646d9.jpg?wh=1920x1200 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/64/d9/64fde55dd2eab68f9968ff34218646d9.jpg?wh=1920x1200"
        title="img" /></p>
<h3 id="如何使用容器状态探针">如何使用容器状态探针</h3>
<p>掌握了资源配额和检查探针的概念，我们进入今天的高潮部分，看看如何在 Pod 的 YAML 描述文件里定义探针。
startupProbe、livenessProbe、readinessProbe 这三种探针的配置方式都是一样的，关键字段有这么几个：
periodSeconds，执行探测动作的时间间隔，默认是 10 秒探测一次。
timeoutSeconds，探测动作的超时时间，如果超时就认为探测失败，默认是 1 秒。
successThreshold，连续几次探测成功才认为是正常，对于 startupProbe 和 livenessProbe 来说它只能是 1。
failureThreshold，连续探测失败几次才认为是真正发生了异常，默认是 3 次。
至于探测方式，Kubernetes 支持 3 种：Shell、TCP Socket、HTTP GET，它们也需要在探针里配置：
exec，执行一个 Linux 命令，比如 ps、cat 等等，和 container 的 command 字段很类似。
tcpSocket，使用 TCP 协议尝试连接容器的指定端口。
httpGet，连接端口并发送 HTTP GET 请求。
要使用这些探针，我们必须要在开发应用时预留出“检查口”，这样 Kubernetes 才能调用探针获取信息。这里我还是以 Nginx 作为示例，用 ConfigMap 编写一个配置文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ConfigMap
metadata:
  name: ngx-conf
data:
  default.conf: |
    server {
      listen 80;
      location = /ready {
        return 200 &#39;I am ready&#39;;
      }
    }
</code></pre></td></tr></table>
</div>
</div><p>你可能不是太熟悉 Nginx 的配置语法，我简单解释一下。
在这个配置文件里，我们启用了 80 端口，然后用 location 指令定义了 HTTP 路径 /ready，它作为对外暴露的“检查口”，用来检测就绪状态，返回简单的 200 状态码和一个字符串表示工作正常。
现在我们来看一下 Pod 里三种探针的具体定义：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: ngx-pod-probe
spec:
  volumes:

  - name: ngx-conf-vol
    configMap:
      name: ngx-conf
    containers:
  - image: nginx:alpine
    name: ngx
    ports:
    - containerPort: 80
      volumeMounts:
    - mountPath: /etc/nginx/conf.d
      name: ngx-conf-vol
      startupProbe:
      periodSeconds: 1
      exec:
        command: [&#34;cat&#34;, &#34;/var/run/nginx.pid&#34;]
      livenessProbe:
      periodSeconds: 10
      tcpSocket:
        port: 80
      readinessProbe:
      periodSeconds: 5
      httpGet:
        path: /ready
        port: 80
</code></pre></td></tr></table>
</div>
</div><p>StartupProbe 使用了 Shell 方式，使用 cat 命令检查 Nginx 存在磁盘上的进程号文件（/var/run/nginx.pid），如果存在就认为是启动成功，它的执行频率是每秒探测一次。
LivenessProbe 使用了 TCP Socket 方式，尝试连接 Nginx 的 80 端口，每 10 秒探测一次。
ReadinessProbe 使用的是 HTTP GET 方式，访问容器的 /ready 路径，每 5 秒发一次请求。
现在我们用 kubectl apply 创建这个 Pod，然后查看它的状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/ac/6c/ac6b405074a5e93d33dd7154f299486c.png?wh=1272x174"
        data-srcset="https://static001.geekbang.org/resource/image/ac/6c/ac6b405074a5e93d33dd7154f299486c.png?wh=1272x174, https://static001.geekbang.org/resource/image/ac/6c/ac6b405074a5e93d33dd7154f299486c.png?wh=1272x174 1.5x, https://static001.geekbang.org/resource/image/ac/6c/ac6b405074a5e93d33dd7154f299486c.png?wh=1272x174 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/ac/6c/ac6b405074a5e93d33dd7154f299486c.png?wh=1272x174"
        title="img" /></p>
<p>当然，因为这个 Nginx 应用非常简单，它启动后探针的检查都会是正常的，你可以用 kubectl logs 来查看 Nginx 的访问日志，里面会记录 HTTP GET 探针的执行情况：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/ed/6b/edf9fb3337bf3dd5a9b2fba8dfbc326b.png?wh=1920x527"
        data-srcset="https://static001.geekbang.org/resource/image/ed/6b/edf9fb3337bf3dd5a9b2fba8dfbc326b.png?wh=1920x527, https://static001.geekbang.org/resource/image/ed/6b/edf9fb3337bf3dd5a9b2fba8dfbc326b.png?wh=1920x527 1.5x, https://static001.geekbang.org/resource/image/ed/6b/edf9fb3337bf3dd5a9b2fba8dfbc326b.png?wh=1920x527 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/ed/6b/edf9fb3337bf3dd5a9b2fba8dfbc326b.png?wh=1920x527"
        title="img" /></p>
<p>从截图中你可以看到，Kubernetes 正是以大约 5 秒一次的频率，向 URI /ready 发送 HTTP 请求，不断地检查容器是否处于就绪状态。
为了验证另两个探针的工作情况，我们可以修改探针，比如把命令改成检查错误的文件、错误的端口号：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> startupProbe:
      exec:
        command: [&#34;cat&#34;, &#34;nginx.pid&#34;]  #错误的文件
    livenessProbe:
      tcpSocket:
        port: 8080                     #错误的端口号
</code></pre></td></tr></table>
</div>
</div><p>然后我们重新创建 Pod 对象，观察它的状态。
当 StartupProbe 探测失败的时候，Kubernetes 就会不停地重启容器，现象就是 RESTARTS 次数不停地增加，而 livenessProbe 和 readinessProbePod 没有执行，Pod 虽然是 Running 状态，也永远不会 READY：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/90/7f/900468e4b86c241a53256584e514b47f.png?wh=1348x182"
        data-srcset="https://static001.geekbang.org/resource/image/90/7f/900468e4b86c241a53256584e514b47f.png?wh=1348x182, https://static001.geekbang.org/resource/image/90/7f/900468e4b86c241a53256584e514b47f.png?wh=1348x182 1.5x, https://static001.geekbang.org/resource/image/90/7f/900468e4b86c241a53256584e514b47f.png?wh=1348x182 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/90/7f/900468e4b86c241a53256584e514b47f.png?wh=1348x182"
        title="img" /></p>
<p>因为 failureThreshold 的次数默认是三次，所以 Kubernetes 会连续执行三次 livenessProbe TCP Socket 探测，每次间隔 10 秒，30 秒之后都失败才重启容器：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c3/e1/c31bf2cf6672c62ebd42f305534dbae1.png?wh=1366x178"
        data-srcset="https://static001.geekbang.org/resource/image/c3/e1/c31bf2cf6672c62ebd42f305534dbae1.png?wh=1366x178, https://static001.geekbang.org/resource/image/c3/e1/c31bf2cf6672c62ebd42f305534dbae1.png?wh=1366x178 1.5x, https://static001.geekbang.org/resource/image/c3/e1/c31bf2cf6672c62ebd42f305534dbae1.png?wh=1366x178 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c3/e1/c31bf2cf6672c62ebd42f305534dbae1.png?wh=1366x178"
        title="img" /></p>
<p>你也可以自己试着改一下 readinessProbe，看看它失败时 Pod 会是什么样的状态。</p>
<h3 id="小结-26">小结</h3>
<p>好了，今天我们学习了两种为 Pod 配置运行保障的方式：Resources 和 Probe。Resources 就是为容器加上资源限制，而 Probe 就是主动健康检查，让 Kubernetes 实时地监控应用的运行状态。
再简单小结一下今天的内容：
资源配额使用的是 cgroup 技术，可以限制容器使用的 CPU 和内存数量，让 Pod 合理利用系统资源，也能够让 Kubernetes 更容易调度 Pod。
Kubernetes 定义了 Startup、Liveness、Readiness 三种健康探针，它们分别探测应用的启动、存活和就绪状态。
探测状态可以使用 Shell、TCP Socket、HTTP Get 三种方式，还可以调整探测的频率和超时时间等参数。</p>
<h2 id="29集群管理如何用名字空间分隔系统资源">29｜集群管理：如何用名字空间分隔系统资源？</h2>
<p>在上一节课里我们学习了资源配额和检查探针，它们可以保障 Pod 这个微观单位很好地运行。那么很自然地，我们就会想：在集群的宏观层次，会不会也有类似的方法来为 Kubernetes 提供运行保障呢？
这是毫无疑问的，因为 Kubernetes 在各个方面都考虑得非常周密，也有很多的手段来管理、控制集群的资源。
今天我们就来看看名字空间（namespace）的一些高级用法。</p>
<h3 id="为什么要有名字空间">为什么要有名字空间</h3>
<p>其实我们很早就接触过 Kubernetes 的名字空间，比如第 10 讲中查看 apiserver 等组件要用到 kube-system 名字空间，还有在第 20 讲里的 Service 对象，DNS 的完整域名里也会用到名字空间。
不过之前学习的重点是 Kubernetes 架构和 API 对象，对名字空间没有特别关注，而且也过去比较久了，所以现在我们来重新认识一下名字空间。
首先要明白，Kubernetes 的名字空间并不是一个实体对象，只是一个逻辑上的概念。它可以把集群切分成一个个彼此独立的区域，然后我们把对象放到这些区域里，就实现了类似容器技术里 namespace 的隔离效果，应用只能在自己的名字空间里分配资源和运行，不会干扰到其他名字空间里的应用。
你可能要问了：Kubernetes 的 Master/Node 架构已经能很好地管理集群，为什么还要引入名字空间这个东西呢？它的实际意义是什么呢？
我觉得，这恰恰是 Kubernetes面对大规模集群、海量节点时的一种现实考虑。因为集群很大、计算资源充足，会有非常多的用户在 Kubernetes 里创建各式各样的应用，可能会有百万数量级别的 Pod，这就使得资源争抢和命名冲突的概率大大增加了，情形和单机 Linux 系统里是非常相似的。
比如说，现在有一个 Kubernetes 集群，前端组、后端组、测试组都在使用它。这个时候就很容易命名冲突，比如后端组先创建了一个 Pod 叫“Web”，这个名字就被“占用”了，之后前端组和测试组就只能绞尽脑汁再新起一个不冲突的名字。接着资源争抢也容易出现，比如某一天，测试组不小心部署了有 Bug 的应用，在节点上把资源都给“吃”完了，就会导致其他组的同事根本无法工作。
所以，当多团队、多项目共用 Kubernetes 的时候，为了避免这些问题的出现，我们就需要把集群给适当地“局部化”，为每一类用户创建出只属于它自己的“工作空间”。
如果把 Kubernetes 比做一个大牧场的话，API 对象就是里面的鸡鸭牛羊，而名字空间就是圈养它们的围栏，有了各自合适的活动区域，就能更有效、更安全地利用 Kubernetes。</p>
<h3 id="如何使用名字空间">如何使用名字空间</h3>
<p>名字空间也是一种 API 对象，使用命令 kubectl api-resources 可以看到它的简称是“ns”，命令 kubectl create 不需要额外的参数，可以很容易地创建一个名字空间，比如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create ns test-ns 
kubectl get ns
</code></pre></td></tr></table>
</div>
</div><p>Kubernetes 初始化集群的时候也会预设 4 个名字空间：default、kube-system、kube-public、kube-node-lease。我们常用的是前两个，default 是用户对象默认的名字空间，kube-system 是系统组件所在的名字空间，相信你对它们已经很熟悉了。
想要把一个对象放入特定的名字空间，需要在它的 metadata 里添加一个 namespace 字段，比如我们要在“test-ns”里创建一个简单的 Nginx Pod，就要这样写：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Pod
metadata:
  name: ngx
  namespace: test-ns
spec:
  containers:
  - image: nginx:alpine
    name: ngx
</code></pre></td></tr></table>
</div>
</div><p>kubectl apply 创建这个对象之后，我们直接用 kubectl get 是看不到它的，因为默认查看的是“default”名字空间，想要操作其他名字空间的对象必须要用 -n 参数明确指定：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pod -n test-ns
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/9d/9f/9dc2521b55e1ac34ee59454339ddc59f.png?wh=1088x298"
        data-srcset="https://static001.geekbang.org/resource/image/9d/9f/9dc2521b55e1ac34ee59454339ddc59f.png?wh=1088x298, https://static001.geekbang.org/resource/image/9d/9f/9dc2521b55e1ac34ee59454339ddc59f.png?wh=1088x298 1.5x, https://static001.geekbang.org/resource/image/9d/9f/9dc2521b55e1ac34ee59454339ddc59f.png?wh=1088x298 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/9d/9f/9dc2521b55e1ac34ee59454339ddc59f.png?wh=1088x298"
        title="img" />因为名字空间里的对象都从属于名字空间，所以在删除名字空间的时候一定要小心，一旦名字空间被删除，它里面的所有对象也都会消失。
你可以执行一下 kubectl delete，试着删除刚才创建的名字空间“test-ns”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete ns test-ns
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/20/a7/20d2e6874d74767bc1711972ae1022a7.png?wh=1088x236"
        data-srcset="https://static001.geekbang.org/resource/image/20/a7/20d2e6874d74767bc1711972ae1022a7.png?wh=1088x236, https://static001.geekbang.org/resource/image/20/a7/20d2e6874d74767bc1711972ae1022a7.png?wh=1088x236 1.5x, https://static001.geekbang.org/resource/image/20/a7/20d2e6874d74767bc1711972ae1022a7.png?wh=1088x236 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/20/a7/20d2e6874d74767bc1711972ae1022a7.png?wh=1088x236"
        title="img" />就会发现删除名字空间后，它里面的 Pod 也会无影无踪了。</p>
<h3 id="什么是资源配额">什么是资源配额</h3>
<p>有了名字空间，我们就可以像管理容器一样，给名字空间设定配额，把整个集群的计算资源分割成不同的大小，按需分配给团队或项目使用。
不过集群和单机不一样，除了限制最基本的 CPU 和内存，还必须限制各种对象的数量，否则对象之间也会互相挤占资源。
名字空间的资源配额需要使用一个专门的 API 对象，叫做 ResourceQuota，简称是 quota，我们可以使用命令 kubectl create 创建一个它的样板文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;
kubectl create quota dev-qt $out
</code></pre></td></tr></table>
</div>
</div><p>因为资源配额对象必须依附在某个名字空间上，所以在它的 metadata 字段里必须明确写出 namespace（否则就会应用到 default 名字空间)。
下面我们先创建一个名字空间“dev-ns”，再创建一个资源配额对象“dev-qt”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Namespace
metadata:

  name: dev-ns
---

apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-qt
  namespace: dev-ns
spec:
  ... ...
</code></pre></td></tr></table>
</div>
</div><p>ResourceQuota 对象的使用方式比较灵活，既可以限制整个名字空间的配额，也可以只限制某些类型的对象（使用 scopeSelector），今天我们看第一种，它需要在 spec 里使用 hard 字段，意思就是“硬性全局限制”。
在 ResourceQuota 里可以设置各类资源配额，字段非常多，我简单地归了一下类，你可以课后再去官方文档上查找详细信息：
CPU 和内存配额，使用 request.<em>、limits.</em>，这是和容器资源限制是一样的。
存储容量配额，使 requests.storage 限制的是 PVC 的存储总量，也可以用 persistentvolumeclaims 限制 PVC 的个数。
核心对象配额，使用对象的名字（英语复数形式），比如 pods、configmaps、secrets、services。
其他 API 对象配额，使用 count/name.group 的形式，比如 count/jobs.batch、count/deployments.apps。
下面的这个 YAML 就是一个比较完整的资源配额对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-qt
  namespace: dev-ns
spec:
  hard:
    requests.cpu: 10
    requests.memory: 10Gi
    limits.cpu: 10
    limits.memory: 20Gi
    requests.storage: 100Gi
    persistentvolumeclaims: 100
    pods: 100
    configmaps: 100
    secrets: 100
    services: 10
    count/jobs.batch: 1
    count/cronjobs.batch: 1
    count/deployments.apps: 1
</code></pre></td></tr></table>
</div>
</div><p>我来稍微解释一下它为名字空间加上的全局资源配额：
所有 Pod 的需求总量最多是 10 个 CPU 和 10GB 的内存，上限总量是 10 个 CPU 和 20GB 的内存。
只能创建 100 个 PVC 对象，使用 100GB 的持久化存储空间。
只能创建 100 个 Pod，100 个 ConfigMap，100 个 Secret，10 个 Service。
只能创建 1 个 Job，1 个 CronJob，1 个 Deployment。
这个 YAML 文件比较大，字段比较多，如果你觉得不是太容易阅读的话，也可以把它拆成几个小的 YAML，分类限制资源数量，也许会更灵活一些。比如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ResourceQuota
metadata:
  name: cpu-mem-qt
  namespace: dev-ns
spec:
  hard:
    requests.cpu: 10
    requests.memory: 10Gi
    limits.cpu: 10
    limits.memory: 20Gi
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ResourceQuota
metadata:
  name: core-obj-qt
  namespace: dev-ns
spec:
  hard:
    pods: 100
    configmaps: 100
    secrets: 100
    services: 10
</code></pre></td></tr></table>
</div>
</div><h3 id="如何使用资源配额">如何使用资源配额</h3>
<p>现在让我们用 kubectl apply 创建这个资源配额对象，然后用 kubectl get 查看，记得要用 -n 指定名字空间：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f quota-ns.yml
kubectl get quota -n dev-ns
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c2/49/c2f22c420f62686c18831de0895fd449.png?wh=1598x402"
        data-srcset="https://static001.geekbang.org/resource/image/c2/49/c2f22c420f62686c18831de0895fd449.png?wh=1598x402, https://static001.geekbang.org/resource/image/c2/49/c2f22c420f62686c18831de0895fd449.png?wh=1598x402 1.5x, https://static001.geekbang.org/resource/image/c2/49/c2f22c420f62686c18831de0895fd449.png?wh=1598x402 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c2/49/c2f22c420f62686c18831de0895fd449.png?wh=1598x402"
        title="img" />你可以看到输出了 ResourceQuota 的全部信息，但都挤在了一起，看起来很困难，这时可以再用命令 kubectl describe 来查看对象，它会给出一个清晰的表格：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl describe quota -n dev-ns
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/6b/8b/6bc46986f7535393198c52e78b04yy8b.png?wh=1246x1044"
        data-srcset="https://static001.geekbang.org/resource/image/6b/8b/6bc46986f7535393198c52e78b04yy8b.png?wh=1246x1044, https://static001.geekbang.org/resource/image/6b/8b/6bc46986f7535393198c52e78b04yy8b.png?wh=1246x1044 1.5x, https://static001.geekbang.org/resource/image/6b/8b/6bc46986f7535393198c52e78b04yy8b.png?wh=1246x1044 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/6b/8b/6bc46986f7535393198c52e78b04yy8b.png?wh=1246x1044"
        title="img" />现在让我们尝试在这个名字空间里运行两个 busybox Job，同样要加上 -n 参数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create job echo1 -n dev-ns --image=busybox -- echo hello
kubectl create job echo2 -n dev-ns --image=busybox -- echo hello
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/9f/c8/9f7430yy76638fa76ec22b7d37b16ac8.png?wh=1920x204"
        data-srcset="https://static001.geekbang.org/resource/image/9f/c8/9f7430yy76638fa76ec22b7d37b16ac8.png?wh=1920x204, https://static001.geekbang.org/resource/image/9f/c8/9f7430yy76638fa76ec22b7d37b16ac8.png?wh=1920x204 1.5x, https://static001.geekbang.org/resource/image/9f/c8/9f7430yy76638fa76ec22b7d37b16ac8.png?wh=1920x204 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/9f/c8/9f7430yy76638fa76ec22b7d37b16ac8.png?wh=1920x204"
        title="img" />ResourceQuota 限制了名字空间里最多只能有一个 Job，所以创建第二个 Job 对象时会失败，提示超出了资源配额。
再用命令 kubectl describe 来查看，也会发现 Job 资源已经到达了上限：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/00/d8/004deb0cb87b3d3dbb050342b6f087d8.png?wh=1242x532"
        data-srcset="https://static001.geekbang.org/resource/image/00/d8/004deb0cb87b3d3dbb050342b6f087d8.png?wh=1242x532, https://static001.geekbang.org/resource/image/00/d8/004deb0cb87b3d3dbb050342b6f087d8.png?wh=1242x532 1.5x, https://static001.geekbang.org/resource/image/00/d8/004deb0cb87b3d3dbb050342b6f087d8.png?wh=1242x532 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/00/d8/004deb0cb87b3d3dbb050342b6f087d8.png?wh=1242x532"
        title="img" /></p>
<p>不过，只要我们删除刚才的 Job，就又可以运行一个新的离线业务了：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/f5/eb/f5db572d679257705a1dcab125e148eb.png?wh=1920x213"
        data-srcset="https://static001.geekbang.org/resource/image/f5/eb/f5db572d679257705a1dcab125e148eb.png?wh=1920x213, https://static001.geekbang.org/resource/image/f5/eb/f5db572d679257705a1dcab125e148eb.png?wh=1920x213 1.5x, https://static001.geekbang.org/resource/image/f5/eb/f5db572d679257705a1dcab125e148eb.png?wh=1920x213 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/f5/eb/f5db572d679257705a1dcab125e148eb.png?wh=1920x213"
        title="img" /></p>
<p>同样的，这个“dev-ns”里也只能创建一个 CronJob 和一个 Deployment，你可以课后自己尝试一下。</p>
<h3 id="默认资源配额">默认资源配额</h3>
<p>学到这里估计你也发现了，在名字空间加上了资源配额限制之后，它会有一个合理但比较“烦人”的约束：要求所有在里面运行的 Pod 都必须用字段 resources 声明资源需求，否则就无法创建。
比如说，现在我们想用命令 kubectl run 创建一个 Pod：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl run ngx --image=nginx:alpine -n dev-ns
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/45/d5/45e19f5fa9db52efc7b34a1bfd3a49d5.png?wh=1920x169"
        data-srcset="https://static001.geekbang.org/resource/image/45/d5/45e19f5fa9db52efc7b34a1bfd3a49d5.png?wh=1920x169, https://static001.geekbang.org/resource/image/45/d5/45e19f5fa9db52efc7b34a1bfd3a49d5.png?wh=1920x169 1.5x, https://static001.geekbang.org/resource/image/45/d5/45e19f5fa9db52efc7b34a1bfd3a49d5.png?wh=1920x169 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/45/d5/45e19f5fa9db52efc7b34a1bfd3a49d5.png?wh=1920x169"
        title="img" />发现给出了一个“Forbidden”的错误提示，说不满足配额要求。
Kubernetes 这样做的原因也很好理解，上一讲里我们说过，如果 Pod 里没有 resources 字段，就可以无限制地使用 CPU 和内存，这显然与名字空间的资源配额相冲突。为了保证名字空间的资源总量可管可控，Kubernetes 就只能拒绝创建这样的 Pod 了。
这个约束对于集群管理来说是好事，但对于普通用户来说却带来了一点麻烦，本来 YAML 文件就已经够大够复杂的了，现在还要再增加几个字段，再费心估算它的资源配额。如果有很多小应用、临时 Pod 要运行的话，这样做的人力成本就比较高，不是太划算。
那么能不能让 Kubernetes 自动为 Pod 加上资源限制呢？也就是说给个默认值，这样就可以省去反复设置配额的烦心事。
这个时候就要用到一个很小但很有用的辅助对象了—— LimitRange，简称是 limits，它能为 API 对象添加默认的资源配额限制。
你可以用命令 kubectl explain limits 来查看它的 YAML 字段详细说明，这里说几个要点：
spec.limits 是它的核心属性，描述了默认的资源限制。
type 是要限制的对象类型，可以是 Container、Pod、PersistentVolumeClaim。
default 是默认的资源上限，对应容器里的 resources.limits，只适用于 Container。
defaultRequest 默认申请的资源，对应容器里的 resources.requests，同样也只适用于 Container。
max、min 是对象能使用的资源的最大最小值。
这个 YAML 就示范了一个 LimitRange 对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limits
  namespace: dev-ns
spec:
  limits:

  - type: Container
    defaultRequest:
      cpu: 200m
      memory: 50Mi
    default:
      cpu: 500m
      memory: 100Mi
  - type: Pod
    max:
      cpu: 800m
      memory: 200Mi
</code></pre></td></tr></table>
</div>
</div><p>它设置了每个容器默认申请 0.2 的 CPU 和 50MB 内存，容器的资源上限是 0.5 的 CPU 和 100MB 内存，每个 Pod 的最大使用量是 0.8 的 CPU 和 200MB 内存。
使用 kubectl apply 创建 LimitRange 之后，再用 kubectl describe 就可以看到它的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl describe limitranges -n dev-ns
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/fd/1e/fdfab789e7b3f7c744eec4bfb137cd1e.png?wh=1688x524"
        data-srcset="https://static001.geekbang.org/resource/image/fd/1e/fdfab789e7b3f7c744eec4bfb137cd1e.png?wh=1688x524, https://static001.geekbang.org/resource/image/fd/1e/fdfab789e7b3f7c744eec4bfb137cd1e.png?wh=1688x524 1.5x, https://static001.geekbang.org/resource/image/fd/1e/fdfab789e7b3f7c744eec4bfb137cd1e.png?wh=1688x524 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/fd/1e/fdfab789e7b3f7c744eec4bfb137cd1e.png?wh=1688x524"
        title="img" />现在我们就可以不用编写 resources 字段直接创建 Pod 了，再运行之前的 kubectl run 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl run ngx --image=nginx:alpine -n dev-ns

</code></pre></td></tr></table>
</div>
</div><p>有了这个默认的资源配额作为“保底”，这次就没有报错，Pod 顺利创建成功，用 kubectl describe 查看 Pod 的状态，也可以看到 LimitRange 为它自动加上的资源配额：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/cf/92/cfd5fef8aefe5321b505859029075792.png?wh=948x988"
        data-srcset="https://static001.geekbang.org/resource/image/cf/92/cfd5fef8aefe5321b505859029075792.png?wh=948x988, https://static001.geekbang.org/resource/image/cf/92/cfd5fef8aefe5321b505859029075792.png?wh=948x988 1.5x, https://static001.geekbang.org/resource/image/cf/92/cfd5fef8aefe5321b505859029075792.png?wh=948x988 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/cf/92/cfd5fef8aefe5321b505859029075792.png?wh=948x988"
        title="img" /></p>
<h3 id="小结-27">小结</h3>
<p>今天我们学习了如何使用名字空间来管理 Kubernetes 集群资源。
在我们的实验环境里，因为只有一个用户（也就是你自己），可以独占全部资源，所以使用名字空间的意义不大。
但是在生产环境里会有很多用户共同使用 Kubernetes，必然会有对资源的竞争，为了公平起见，避免某些用户过度消耗资源，就非常有必要用名字空间做好集群的资源规划了。
再简单小结一下今天的内容：
名字空间是一个逻辑概念，没有实体，它的目标是为资源和对象划分出一个逻辑边界，避免冲突。
ResourceQuota 对象可以为名字空间添加资源配额，限制全局的 CPU、内存和 API 对象数量。
LimitRange 对象可以为容器或者 Pod 添加默认的资源配额，简化对象的创建工作。</p>
<h2 id="30系统监控如何使用metrics-server和prometheus">30｜系统监控：如何使用Metrics Server和Prometheus？</h2>
<p>在前面的两节课里，我们学习了对 Pod 和对集群的一些管理方法，其中的要点就是设置资源配额，让 Kubernetes 用户能公平合理地利用系统资源。
虽然有了这些方法，但距离我们把 Pod 和集群管好用好还缺少一个很重要的方面——集群的可观测性。也就是说，我们希望给集群也安装上“检查探针”，观察到集群的资源利用率和其他指标，让集群的整体运行状况对我们“透明可见”，这样才能更准确更方便地做好集群的运维工作。
但是观测集群是不能用“探针”这种简单的方式的，所以今天我就带你一起来看看 Kubernetes 为集群提供的两种系统级别的监控项目：Metrics Server 和 Prometheus，以及基于它们的水平自动伸缩对象 HorizontalPodAutoscaler。</p>
<h3 id="metrics-server">Metrics Server</h3>
<p>如果你对 Linux 系统有所了解的话，也许知道有一个命令 top 能够实时显示当前系统的 CPU 和内存利用率，它是性能分析和调优的基本工具，非常有用。Kubernetes 也提供了类似的命令，就是 kubectl top，不过默认情况下这个命令不会生效，必须要安装一个插件 Metrics Server 才可以。
Metrics Server 是一个专门用来收集 Kubernetes 核心资源指标（metrics）的工具，它定时从所有节点的 kubelet 里采集信息，但是对集群的整体性能影响极小，每个节点只大约会占用 1m 的 CPU 和 2MB 的内存，所以性价比非常高。
下面的这张图来自 Kubernetes 官网，你可以对 Metrics Server 的工作方式有个大概了解：它调用 kubelet 的 API 拿到节点和 Pod 的指标，再把这些信息交给 apiserver，这样 kubectl、HPA 就可以利用 apiserver 来读取指标了：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/8f/9e/8f4a22788c03b06377cabe791c67989e.png?wh=1562x572"
        data-srcset="https://static001.geekbang.org/resource/image/8f/9e/8f4a22788c03b06377cabe791c67989e.png?wh=1562x572, https://static001.geekbang.org/resource/image/8f/9e/8f4a22788c03b06377cabe791c67989e.png?wh=1562x572 1.5x, https://static001.geekbang.org/resource/image/8f/9e/8f4a22788c03b06377cabe791c67989e.png?wh=1562x572 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/8f/9e/8f4a22788c03b06377cabe791c67989e.png?wh=1562x572"
        title="img" /></p>
<p>在 Metrics Server 的项目网址（https://github.com/kubernetes-sigs/metrics-server）可以看到它的说明文档和安装步骤，不过如果你已经按照第 17 讲用 kubeadm 搭建了 Kubernetes 集群，就已经具备了全部前提条件，接下来只需要几个简单的操作就可以完成安装。
Metrics Server 的所有依赖都放在了一个 YAML 描述文件里，你可以使用 wget 或者 curl 下载：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

</code></pre></td></tr></table>
</div>
</div><p>但是在 kubectl apply 创建对象之前，我们还有两个准备工作要做。
第一个工作，是修改 YAML 文件。你需要在 Metrics Server 的 Deployment 对象里，加上一个额外的运行参数 &ndash;kubelet-insecure-tls，也就是这样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
spec:
  ... ... 
  template:
    spec:
      containers:
      - args:
        - --kubelet-insecure-tls
        ... ... 
</code></pre></td></tr></table>
</div>
</div><p>这是因为 Metrics Server 默认使用 TLS 协议，要验证证书才能与 kubelet 实现安全通信，而我们的实验环境里没有这个必要，加上这个参数可以让我们的部署工作简单很多（生产环境里就要慎用）。
第二个工作，是预先下载 Metrics Server 的镜像。看这个 YAML 文件，你会发现 Metrics Server 的镜像仓库用的是 gcr.io，下载很困难。好在它也有国内的镜像网站，你可以用第 17 讲里的办法，下载后再改名，然后把镜像加载到集群里的节点上。
这里我给出一段 Shell 脚本代码，供你参考：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">repo=registry.aliyuncs.com/google_containers
name=k8s.gcr.io/metrics-server/metrics-server:v0.6.1
src_name=metrics-server:v0.6.1
docker pull $repo/$src_name
docker tag $repo/$src_name $name
docker rmi $repo/$src_name
</code></pre></td></tr></table>
</div>
</div><p>两个准备工作都完成之后，我们就可以使用 YAML 部署 Metrics Server 了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f components.yaml

</code></pre></td></tr></table>
</div>
</div><p>Metrics Server 属于名字空间“kube-system”，可以用 kubectl get pod 加上 -n 参数查看它是否正常运行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pod -n kube-system
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b9/93/b93124cbc1b7d98b7c4f055f0723bf93.png?wh=1506x822"
        data-srcset="https://static001.geekbang.org/resource/image/b9/93/b93124cbc1b7d98b7c4f055f0723bf93.png?wh=1506x822, https://static001.geekbang.org/resource/image/b9/93/b93124cbc1b7d98b7c4f055f0723bf93.png?wh=1506x822 1.5x, https://static001.geekbang.org/resource/image/b9/93/b93124cbc1b7d98b7c4f055f0723bf93.png?wh=1506x822 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b9/93/b93124cbc1b7d98b7c4f055f0723bf93.png?wh=1506x822"
        title="img" />现在有了 Metrics Server 插件，我们就可以使用命令 kubectl top 来查看 Kubernetes 集群当前的资源状态了。它有两个子命令，node 查看节点的资源使用率，pod 查看 Pod 的资源使用率。
由于 Metrics Server 收集信息需要时间，我们必须等一小会儿才能执行命令，查看集群里节点和 Pod 状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl top node
kubectl top pod -n kube-system
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d4/61/d450b7e01f5f47ac56335f6c69707e61.png?wh=1800x1052"
        data-srcset="https://static001.geekbang.org/resource/image/d4/61/d450b7e01f5f47ac56335f6c69707e61.png?wh=1800x1052, https://static001.geekbang.org/resource/image/d4/61/d450b7e01f5f47ac56335f6c69707e61.png?wh=1800x1052 1.5x, https://static001.geekbang.org/resource/image/d4/61/d450b7e01f5f47ac56335f6c69707e61.png?wh=1800x1052 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d4/61/d450b7e01f5f47ac56335f6c69707e61.png?wh=1800x1052"
        title="img" />从这个截图里你可以看到：
集群里两个节点 CPU 使用率都不高，分别是 8% 和 4%，但内存用的很多，master 节点用了差不多一半（48%），而 worker 节点几乎用满了（89%）。
名字空间“kube-system”里有很多 Pod，其中 apiserver 最消耗资源，使用了 75m 的 CPU 和 363MB 的内存。</p>
<h3 id="horizontalpodautoscaler">HorizontalPodAutoscaler</h3>
<p>有了 Metrics Server，我们就可以轻松地查看集群的资源使用状况了，不过它另外一个更重要的功能是辅助实现应用的“水平自动伸缩”。
在第 18 讲里我们提到有一个命令 kubectl scale，可以任意增减 Deployment 部署的 Pod 数量，也就是水平方向的“扩容”和“缩容”。但是手动调整应用实例数量还是比较麻烦的，需要人工参与，也很难准确把握时机，难以及时应对生产环境中突发的大流量，所以最好能把这个“扩容”“缩容”也变成自动化的操作。
Kubernetes 为此就定义了一个新的 API 对象，叫做“HorizontalPodAutoscaler”，简称是“hpa”。顾名思义，它是专门用来自动伸缩 Pod 数量的对象，适用于 Deployment 和 StatefulSet，但不能用于 DaemonSet（原因很明显吧)。
HorizontalPodAutoscaler 的能力完全基于 Metrics Server，它从 Metrics Server 获取当前应用的运行指标，主要是 CPU 使用率，再依据预定的策略增加或者减少 Pod 的数量。
下面我们就来看看该怎么使用 HorizontalPodAutoscaler，首先要定义 Deployment 和 Service，创建一个 Nginx 应用，作为自动伸缩的目标对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-hpa-dep
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ngx-hpa-dep
  template:
    metadata:
      labels:
        app: ngx-hpa-dep
    spec:
      containers:

   - image: nginx:alpine
     name: nginx
     ports:
     - containerPort: 80
       resources:
       requests:
         cpu: 50m
         memory: 10Mi
       limits:
         cpu: 100m

            memory: 20Mi
---

apiVersion: v1
kind: Service
metadata:
  name: ngx-hpa-svc
spec:
  ports:

  - port: 80
    protocol: TCP
    targetPort: 80
    selector:
    app: ngx-hpa-dep
</code></pre></td></tr></table>
</div>
</div><p>在这个 YAML 里我只部署了一个 Nginx 实例，名字是 ngx-hpa-dep。注意在它的 spec 里一定要用 resources 字段写清楚资源配额，否则 HorizontalPodAutoscaler 会无法获取 Pod 的指标，也就无法实现自动化扩缩容。
接下来我们要用命令 kubectl autoscale 创建一个 HorizontalPodAutoscaler 的样板 YAML 文件，它有三个参数：
min，Pod 数量的最小值，也就是缩容的下限。
max，Pod 数量的最大值，也就是扩容的上限。
cpu-percent，CPU 使用率指标，当大于这个值时扩容，小于这个值时缩容。
好，现在我们就来为刚才的 Nginx 应用创建 HorizontalPodAutoscaler，指定 Pod 数量最少 2 个，最多 10 个，CPU 使用率指标设置的小一点，5%，方便我们观察扩容现象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;              # 定义Shell变量
kubectl autoscale deploy ngx-hpa-dep --min=2 --max=10 --cpu-percent=5 $out
</code></pre></td></tr></table>
</div>
</div><p>得到的 YAML 描述文件就是这样：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
name: ngx-hpa
spec:
maxReplicas: 10
minReplicas: 2
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: ngx-hpa-dep
targetCPUUtilizationPercentage: 5
</code></pre></td></tr></table>
</div>
</div><p>我们再使用命令 kubectl apply 创建这个 HorizontalPodAutoscaler 后，它会发现 Deployment 里的实例只有 1 个，不符合 min 定义的下限的要求，就先扩容到 2 个：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3e/6c/3ec01a9746274ac28b10d612f1512a6c.png?wh=1630x704"
        data-srcset="https://static001.geekbang.org/resource/image/3e/6c/3ec01a9746274ac28b10d612f1512a6c.png?wh=1630x704, https://static001.geekbang.org/resource/image/3e/6c/3ec01a9746274ac28b10d612f1512a6c.png?wh=1630x704 1.5x, https://static001.geekbang.org/resource/image/3e/6c/3ec01a9746274ac28b10d612f1512a6c.png?wh=1630x704 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3e/6c/3ec01a9746274ac28b10d612f1512a6c.png?wh=1630x704"
        title="img" /></p>
<p>从这张截图里你可以看到，HorizontalPodAutoscaler 会根据 YAML 里的描述，找到要管理的 Deployment，把 Pod 数量调整成 2 个，再通过 Metrics Server 不断地监测 Pod 的 CPU 使用率。
下面我们来给 Nginx 加上压力流量，运行一个测试 Pod，使用的镜像是“httpd:alpine”，它里面有 HTTP 性能测试工具 ab（Apache Bench）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl run test -it --image=httpd:alpine -- sh
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/d0/bd/d058182500cb83ac3e3c9cc01a42c9bd.png?wh=1896x354"
        data-srcset="https://static001.geekbang.org/resource/image/d0/bd/d058182500cb83ac3e3c9cc01a42c9bd.png?wh=1896x354, https://static001.geekbang.org/resource/image/d0/bd/d058182500cb83ac3e3c9cc01a42c9bd.png?wh=1896x354 1.5x, https://static001.geekbang.org/resource/image/d0/bd/d058182500cb83ac3e3c9cc01a42c9bd.png?wh=1896x354 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/d0/bd/d058182500cb83ac3e3c9cc01a42c9bd.png?wh=1896x354"
        title="img" />然后我们向 Nginx 发送一百万个请求，持续 1 分钟，再用 kubectl get hpa 来观察 HorizontalPodAutoscaler 的运行状况：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ab -c 10 -t 60 -n 1000000 &#39;http://ngx-hpa-svc/&#39;
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/65/b4/6538ecd78118fabeb8d7c8f4fbabdbb4.png?wh=1920x794"
        data-srcset="https://static001.geekbang.org/resource/image/65/b4/6538ecd78118fabeb8d7c8f4fbabdbb4.png?wh=1920x794, https://static001.geekbang.org/resource/image/65/b4/6538ecd78118fabeb8d7c8f4fbabdbb4.png?wh=1920x794 1.5x, https://static001.geekbang.org/resource/image/65/b4/6538ecd78118fabeb8d7c8f4fbabdbb4.png?wh=1920x794 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/65/b4/6538ecd78118fabeb8d7c8f4fbabdbb4.png?wh=1920x794"
        title="img" />因为 Metrics Server 大约每 15 秒采集一次数据，所以 HorizontalPodAutoscaler 的自动化扩容和缩容也是按照这个时间点来逐步处理的。
当它发现目标的 CPU 使用率超过了预定的 5% 后，就会以 2 的倍数开始扩容，一直到数量上限，然后持续监控一段时间，如果 CPU 使用率回落，就会再缩容到最小值。</p>
<h3 id="prometheus">Prometheus</h3>
<p>显然，有了 Metrics Server 和 HorizontalPodAutoscaler 的帮助，我们的应用管理工作又轻松了一些。不过，Metrics Server 能够获取的指标还是太少了，只有 CPU 和内存，想要监控到更多更全面的应用运行状况，还得请出这方面的权威项目“Prometheus”。
其实，Prometheus 的历史比 Kubernetes 还要早一些，它最初是由 Google 的离职员工在 2012 年创建的开源项目，灵感来源于 Borg 配套的 BorgMon 监控系统。后来在 2016 年，Prometheus 作为第二个项目加入了 CNCF，并在 2018 年继 Kubernetes 之后顺利毕业，成为了 CNCF 的不折不扣的“二当家”，也是云原生监控领域的“事实标准”。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/69/58/69f4b76ca7323433cyy28574f1ee9358.png?wh=1200x600"
        data-srcset="https://static001.geekbang.org/resource/image/69/58/69f4b76ca7323433cyy28574f1ee9358.png?wh=1200x600, https://static001.geekbang.org/resource/image/69/58/69f4b76ca7323433cyy28574f1ee9358.png?wh=1200x600 1.5x, https://static001.geekbang.org/resource/image/69/58/69f4b76ca7323433cyy28574f1ee9358.png?wh=1200x600 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/69/58/69f4b76ca7323433cyy28574f1ee9358.png?wh=1200x600"
        title="img" /></p>
<p>和 Kubernetes 一样，Prometheus 也是一个庞大的系统，我们这里就只做一个简略的介绍。
下面的这张图是 Prometheus 官方的架构图，几乎所有文章在讲 Prometheus 的时候必然要拿出来，所以我也没办法“免俗”：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/e6/64/e62cebb3acc995246f203d698dfdc964.png?wh=1351x811"
        data-srcset="https://static001.geekbang.org/resource/image/e6/64/e62cebb3acc995246f203d698dfdc964.png?wh=1351x811, https://static001.geekbang.org/resource/image/e6/64/e62cebb3acc995246f203d698dfdc964.png?wh=1351x811 1.5x, https://static001.geekbang.org/resource/image/e6/64/e62cebb3acc995246f203d698dfdc964.png?wh=1351x811 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/e6/64/e62cebb3acc995246f203d698dfdc964.png?wh=1351x811"
        title="img" /></p>
<p>Prometheus 系统的核心是它的 Server，里面有一个时序数据库 TSDB，用来存储监控数据，另一个组件 Retrieval 使用拉取（Pull）的方式从各个目标收集数据，再通过 HTTP Server 把这些数据交给外界使用。
在 Prometheus Server 之外还有三个重要的组件：
Push Gateway，用来适配一些特殊的监控目标，把默认的 Pull 模式转变为 Push 模式。
Alert Manager，告警中心，预先设定规则，发现问题时就通过邮件等方式告警。
Grafana 是图形化界面，可以定制大量直观的监控仪表盘。
由于同属于 CNCF，所以 Prometheus 自然就是“云原生”，在 Kubernetes 里运行是顺理成章的事情。不过它包含的组件实在是太多，部署起来有点麻烦，这里我选用了“kube-prometheus”项目（https://github.com/prometheus-operator/kube-prometheus/），感觉操作起来比较容易些。
下面就跟着我来在 Kubernetes 实验环境里体验一下 Prometheus 吧。
我们先要下载 kube-prometheus 的源码包，当前的最新版本是 0.11：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">wget https://github.com/prometheus-operator/kube-prometheus/archive/refs/tags/v0.11.0.tar.gz

</code></pre></td></tr></table>
</div>
</div><p>解压缩后，Prometheus 部署相关的 YAML 文件都在 manifests 目录里，有近 100 个，你可以先大概看一下。
和 Metrics Server 一样，我们也必须要做一些准备工作，才能够安装 Prometheus。
第一步，是修改 prometheus-service.yaml、grafana-service.yaml。
这两个文件定义了 Prometheus 和 Grafana 服务对象，我们可以给它们添加 type: NodePort（参考第 20 讲），这样就可以直接通过节点的 IP 地址访问（当然你也可以配置成 Ingress）。
第二步，是修改 kubeStateMetrics-deployment.yaml、prometheusAdapter-deployment.yaml，因为它们里面有两个存放在 gcr.io 的镜像，必须解决下载镜像的问题。
但很遗憾，我没有在国内网站上找到它们的下载方式，为了能够顺利安装，只能把它们下载后再上传到 Docker Hub 上。所以你需要修改镜像名字，把前缀都改成 chronolaw：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.5.0
image: k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.9.1
image: chronolaw/kube-state-metrics:v2.5.0
image: chronolaw/prometheus-adapter:v0.9.1
</code></pre></td></tr></table>
</div>
</div><p>这两个准备工作完成之后，我们要执行两个 kubectl create 命令来部署 Prometheus，先是 manifests/setup 目录，创建名字空间等基本对象，然后才是 manifests 目录：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create -f manifests/setup
kubectl create -f manifests
</code></pre></td></tr></table>
</div>
</div><p>Prometheus 的对象都在名字空间“monitoring”里，创建之后可以用 kubectl get 来查看状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/1b/09/1b4a1a1313ede9058b348c13a1020c09.png?wh=1894x878"
        data-srcset="https://static001.geekbang.org/resource/image/1b/09/1b4a1a1313ede9058b348c13a1020c09.png?wh=1894x878, https://static001.geekbang.org/resource/image/1b/09/1b4a1a1313ede9058b348c13a1020c09.png?wh=1894x878 1.5x, https://static001.geekbang.org/resource/image/1b/09/1b4a1a1313ede9058b348c13a1020c09.png?wh=1894x878 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/1b/09/1b4a1a1313ede9058b348c13a1020c09.png?wh=1894x878"
        title="img" /></p>
<p>确定这些 Pod 都运行正常，我们再来看看它对外的服务端口：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get svc -n monitoring
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/4c/59/4c423a203a688271d9d08b15a6782d59.png?wh=1920x531"
        data-srcset="https://static001.geekbang.org/resource/image/4c/59/4c423a203a688271d9d08b15a6782d59.png?wh=1920x531, https://static001.geekbang.org/resource/image/4c/59/4c423a203a688271d9d08b15a6782d59.png?wh=1920x531 1.5x, https://static001.geekbang.org/resource/image/4c/59/4c423a203a688271d9d08b15a6782d59.png?wh=1920x531 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/4c/59/4c423a203a688271d9d08b15a6782d59.png?wh=1920x531"
        title="img" />前面修改了 Grafana 和 Prometheus 的 Service 对象，所以这两个服务就在节点上开了端口，Grafana 是“30358”，Prometheus 有两个端口，其中“9090”对应的“30827”是 Web 端口。
在浏览器里输入节点的 IP 地址（我这里是“http://192.168.10.210”)，再加上端口号“30827”，我们就能看到 Prometheus 自带的 Web 界面，：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/1b/dc/1b73040e258dfa8776c2a0a657a885dc.png?wh=1906x1934"
        data-srcset="https://static001.geekbang.org/resource/image/1b/dc/1b73040e258dfa8776c2a0a657a885dc.png?wh=1906x1934, https://static001.geekbang.org/resource/image/1b/dc/1b73040e258dfa8776c2a0a657a885dc.png?wh=1906x1934 1.5x, https://static001.geekbang.org/resource/image/1b/dc/1b73040e258dfa8776c2a0a657a885dc.png?wh=1906x1934 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/1b/dc/1b73040e258dfa8776c2a0a657a885dc.png?wh=1906x1934"
        title="img" /></p>
<p>Web 界面上有一个查询框，可以使用 PromQL 来查询指标，生成可视化图表，比如在这个截图里我就选择了“node_memory_Active_bytes”这个指标，意思是当前正在使用的内存容量。
Prometheus 的 Web 界面比较简单，通常只用来调试、测试，不适合实际监控。我们再来看 Grafana，访问节点的端口“30358”（我这里是“http://192.168.10.210:30358”），它会要求你先登录，默认的用户名和密码都是“admin”：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a2/31/a2614b09347b3436c317644374c36e31.png?wh=1906x1934"
        data-srcset="https://static001.geekbang.org/resource/image/a2/31/a2614b09347b3436c317644374c36e31.png?wh=1906x1934, https://static001.geekbang.org/resource/image/a2/31/a2614b09347b3436c317644374c36e31.png?wh=1906x1934 1.5x, https://static001.geekbang.org/resource/image/a2/31/a2614b09347b3436c317644374c36e31.png?wh=1906x1934 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a2/31/a2614b09347b3436c317644374c36e31.png?wh=1906x1934"
        title="img" /></p>
<p>Grafana 内部已经预置了很多强大易用的仪表盘，你可以在左侧菜单栏的“Dashboards - Browse”里任意挑选一个：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/23/5a/23ddb3db05e36c2da4a8f8067366f55a.png?wh=1906x1934"
        data-srcset="https://static001.geekbang.org/resource/image/23/5a/23ddb3db05e36c2da4a8f8067366f55a.png?wh=1906x1934, https://static001.geekbang.org/resource/image/23/5a/23ddb3db05e36c2da4a8f8067366f55a.png?wh=1906x1934 1.5x, https://static001.geekbang.org/resource/image/23/5a/23ddb3db05e36c2da4a8f8067366f55a.png?wh=1906x1934 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/23/5a/23ddb3db05e36c2da4a8f8067366f55a.png?wh=1906x1934"
        title="img" /></p>
<p>比如我选择了“Kubernetes / Compute Resources / Namespace (Pods)”这个仪表盘，就会出来一个非常漂亮图表，比 Metrics Server 的 kubectl top 命令要好看得多，各种数据一目了然：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/1f/bd/1f6ccc0b6d358c29419276fbf74e38bd.png?wh=1920x1696"
        data-srcset="https://static001.geekbang.org/resource/image/1f/bd/1f6ccc0b6d358c29419276fbf74e38bd.png?wh=1920x1696, https://static001.geekbang.org/resource/image/1f/bd/1f6ccc0b6d358c29419276fbf74e38bd.png?wh=1920x1696 1.5x, https://static001.geekbang.org/resource/image/1f/bd/1f6ccc0b6d358c29419276fbf74e38bd.png?wh=1920x1696 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/1f/bd/1f6ccc0b6d358c29419276fbf74e38bd.png?wh=1920x1696"
        title="img" /></p>
<p>关于 Prometheus 就暂时介绍到这里，再往下讲可能就要偏离我们的 Kubernetes 主题了，如果你对它感兴趣的话，可以课后再去它的官网上看文档，或者参考其他的学习资料。</p>
<h3 id="小结-28">小结</h3>
<p>在云原生时代，系统的透明性和可观测性是非常重要的。今天我们一起学习了 Kubernetes 里的两个系统监控项目：命令行方式的 Metrics Server、图形化界面的 Prometheus，利用好它们就可以让我们随时掌握 Kubernetes 集群的运行状态，做到“明察秋毫”。
再简单小结一下今天的内容：
Metrics Server 是一个 Kubernetes 插件，能够收集系统的核心资源指标，相关的命令是 kubectl top。
Prometheus 是云原生监控领域的“事实标准”，用 PromQL 语言来查询数据，配合 Grafana 可以展示直观的图形界面，方便监控。
HorizontalPodAutoscaler 实现了应用的自动水平伸缩功能，它从 Metrics Server 获取应用的运行指标，再实时调整 Pod 数量，可以很好地应对突发流量。</p>
<h2 id="31网络通信cni是怎么回事又是怎么工作的">31｜网络通信：CNI是怎么回事？又是怎么工作的？</h2>
<p>到现在，我们对 Kubernetes 已经非常熟悉了，它是一个集群操作系统，能够管理大量计算节点和运行在里面的应用。不过，还有一个很重要的基础知识我们还没有学习，那就是“网络通信”。
早在“入门篇”的第 6 讲里，我们就简单介绍过 Docker 的网络模式，然后在“中级篇”的第 17 讲，我们又为 Kubernetes 安装了一个网络插件 Flannel。这些都与网络相关，但也只是浅尝辄止，并没有太多深究。
如果你是一个喜欢刨根问底的人，会不会很好奇：Flannel 到底是如何工作的呢？它为什么能够让 Kubernetes 集群正常通信呢？还有没有其他网络插件呢？
今天我们就来聊一下这个话题，讲讲 Kubernetes 的网络接口标准 CNI，以及 Calico、Cilium 等性能更好的网络插件。</p>
<h3 id="kubernetes-的网络模型">Kubernetes 的网络模型</h3>
<p>在学习 Kubernetes 的网络之前，我们还是要先简单回顾一下 Docker 的网络知识。
你对 Docker 的 null、host 和 bridge 三种网络模式还有印象吗？这里我重新画了一张图，描述了 Docker 里最常用的 bridge 网络模式：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/0b/85/0b7954a362b9e04db8b588fbed5b7185.jpg?wh=1920x1148"
        data-srcset="https://static001.geekbang.org/resource/image/0b/85/0b7954a362b9e04db8b588fbed5b7185.jpg?wh=1920x1148, https://static001.geekbang.org/resource/image/0b/85/0b7954a362b9e04db8b588fbed5b7185.jpg?wh=1920x1148 1.5x, https://static001.geekbang.org/resource/image/0b/85/0b7954a362b9e04db8b588fbed5b7185.jpg?wh=1920x1148 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/0b/85/0b7954a362b9e04db8b588fbed5b7185.jpg?wh=1920x1148"
        title="img" /></p>
<p>Docker 会创建一个名字叫“docker0”的网桥，默认是私有网段“172.17.0.0/16”。每个容器都会创建一个虚拟网卡对（veth pair），两个虚拟网卡分别“插”在容器和网桥上，这样容器之间就可以互联互通了。
Docker 的网络方案简单有效，但问题是它只局限在单机环境里工作，跨主机通信非常困难（需要做端口映射和网络地址转换）。
针对 Docker 的网络缺陷，Kubernetes 提出了一个自己的网络模型“IP-per-pod”，能够很好地适应集群系统的网络需求，它有下面的这 4 点基本假设：
集群里的每个 Pod 都会有唯一的一个 IP 地址。
Pod 里的所有容器共享这个 IP 地址。
集群里的所有 Pod 都属于同一个网段。
Pod 直接可以基于 IP 地址直接访问另一个 Pod，不需要做麻烦的网络地址转换（NAT）。
我画了一张 Kubernetes 网络模型的示意图，你可以看一下：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/81/6c/81d67c2f0a6e97b847c306c16048c06c.jpg?wh=1920x1114"
        data-srcset="https://static001.geekbang.org/resource/image/81/6c/81d67c2f0a6e97b847c306c16048c06c.jpg?wh=1920x1114, https://static001.geekbang.org/resource/image/81/6c/81d67c2f0a6e97b847c306c16048c06c.jpg?wh=1920x1114 1.5x, https://static001.geekbang.org/resource/image/81/6c/81d67c2f0a6e97b847c306c16048c06c.jpg?wh=1920x1114 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/81/6c/81d67c2f0a6e97b847c306c16048c06c.jpg?wh=1920x1114"
        title="img" /></p>
<p>这个网络让 Pod 摆脱了主机的硬限制，是一个“平坦”的网络模型，很好理解，通信自然也非常简单。
因为 Pod 都具有独立的 IP 地址，相当于一台虚拟机，而且直连互通，也就可以很容易地实施域名解析、负载均衡、服务发现等工作，以前的运维经验都能够直接使用，对应用的管理和迁移都非常友好。</p>
<h3 id="什么是-cni">什么是 CNI</h3>
<p>Kubernetes 定义的这个网络模型很完美，但要把这个模型落地实现就不那么容易了。所以 Kubernetes 就专门制定了一个标准：CNI（Container Networking Interface）。
CNI 为网络插件定义了一系列通用接口，开发者只要遵循这个规范就可以接入 Kubernetes，为 Pod 创建虚拟网卡、分配 IP 地址、设置路由规则，最后就能够实现“IP-per-pod”网络模型。
依据实现技术的不同，CNI 插件可以大致上分成“Overlay”“Route”和“Underlay”三种。
Overlay 的原意是“覆盖”，是指它构建了一个工作在真实底层网络之上的“逻辑网络”，把原始的 Pod 网络数据封包，再通过下层网络发送出去，到了目的地再拆包。因为这个特点，它对底层网络的要求低，适应性强，缺点就是有额外的传输成本，性能较低。
Route 也是在底层网络之上工作，但它没有封包和拆包，而是使用系统内置的路由功能来实现 Pod 跨主机通信。它的好处是性能高，不过对底层网络的依赖性比较强，如果底层不支持就没办法工作了。
Underlay 就是直接用底层网络来实现 CNI，也就是说 Pod 和宿主机都在一个网络里，Pod 和宿主机是平等的。它对底层的硬件和网络的依赖性是最强的，因而不够灵活，但性能最高。
自从 2015 年 CNI 发布以来，由于它的接口定义宽松，有很大的自由发挥空间，所以社区里就涌现出了非常多的网络插件，我们之前在第 17 讲里提到的 Flannel 就是其中之一。
Flannel（https://github.com/flannel-io/flannel/）由 CoreOS 公司（已被 Redhat 收购）开发，最早是一种 Overlay 模式的网络插件，使用 UDP 和 VXLAN 技术，后来又用 Host-Gateway 技术支持了 Route 模式。Flannel 简单易用，是 Kubernetes 里最流行的 CNI 插件，但它在性能方面表现不是太好，所以一般不建议在生产环境里使用。
现在还有两个常用 CNI 插件：Calico、Cilium，我们做个简略的介绍。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/a9/7a/a96dd70ef544e4a69ff6f705a79acb7a.png?wh=1920x746"
        data-srcset="https://static001.geekbang.org/resource/image/a9/7a/a96dd70ef544e4a69ff6f705a79acb7a.png?wh=1920x746, https://static001.geekbang.org/resource/image/a9/7a/a96dd70ef544e4a69ff6f705a79acb7a.png?wh=1920x746 1.5x, https://static001.geekbang.org/resource/image/a9/7a/a96dd70ef544e4a69ff6f705a79acb7a.png?wh=1920x746 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/a9/7a/a96dd70ef544e4a69ff6f705a79acb7a.png?wh=1920x746"
        title="img" /></p>
<p>Calico（https://github.com/projectcalico/calico）是一种 Route 模式的网络插件，使用 BGP 协议（Border Gateway Protocol）来维护路由信息，性能要比 Flannel 好，而且支持多种网络策略，具备数据加密、安全隔离、流量整形等功能。
Cilium（https://github.com/cilium/cilium）是一个比较新的网络插件，同时支持 Overlay 模式和 Route 模式，它的特点是深度使用了 Linux eBPF 技术，在内核层次操作网络数据，所以性能很高，可以灵活实现各种功能。在 2021 年它加入了 CNCF，成为了孵化项目，是非常有前途的 CNI 插件。</p>
<h3 id="cni-插件是怎么工作的">CNI 插件是怎么工作的</h3>
<p>Flannel 比较简单，我们先以它为例看看 CNI 在 Kubernetes 里的工作方式。
这里必须要说明一点，计算机网络很复杂，有 IP 地址、MAC 地址、网段、网卡、网桥、路由等许许多多的概念，而且数据会流经多个设备，理清楚脉络比较麻烦，今天我们会做一个大概的描述，不会讲那些太底层的细节。
我们先来在实验环境里用 Deployment 创建 3 个 Nginx Pod，作为研究对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create deploy ngx-dep --image=nginx:alpine --replicas=3

</code></pre></td></tr></table>
</div>
</div><p>使用命令 kubectl get pod 可以看到，有两个 Pod 运行在 master 节点上，IP 地址分别是“10.10.0.3”“10.10.0.4”，另一个 Pod 运行在 worker 节点上，IP 地址是“10.10.1.77”：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/e6/b8/e63ecfb640e7a032a27817c0b7ff49b8.png?wh=1920x281"
        data-srcset="https://static001.geekbang.org/resource/image/e6/b8/e63ecfb640e7a032a27817c0b7ff49b8.png?wh=1920x281, https://static001.geekbang.org/resource/image/e6/b8/e63ecfb640e7a032a27817c0b7ff49b8.png?wh=1920x281 1.5x, https://static001.geekbang.org/resource/image/e6/b8/e63ecfb640e7a032a27817c0b7ff49b8.png?wh=1920x281 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/e6/b8/e63ecfb640e7a032a27817c0b7ff49b8.png?wh=1920x281"
        title="img" /></p>
<p>Flannel 默认使用的是基于 VXLAN 的 Overlay 模式，整个集群的网络结构我画了一张示意图，你可以对比一下 Docker 的网络结构：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/96/b7/96ffd51d7c843596f6736d23467888b7.jpg?wh=1920x1037"
        data-srcset="https://static001.geekbang.org/resource/image/96/b7/96ffd51d7c843596f6736d23467888b7.jpg?wh=1920x1037, https://static001.geekbang.org/resource/image/96/b7/96ffd51d7c843596f6736d23467888b7.jpg?wh=1920x1037 1.5x, https://static001.geekbang.org/resource/image/96/b7/96ffd51d7c843596f6736d23467888b7.jpg?wh=1920x1037 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/96/b7/96ffd51d7c843596f6736d23467888b7.jpg?wh=1920x1037"
        title="img" /></p>
<p>从单机的角度来看的话，Flannel 的网络结构和 Docker 几乎是一模一样的，只不过网桥换成了“cni0”，而不是“docker0”。
接下来我们来操作一下，看看 Pod 里的虚拟网卡是如何接入 cni0 网桥的。
在 Pod 里执行命令 ip addr 就可以看到它里面的虚拟网卡“eth0”：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b8/84/b85c5c010689b1e3b7075aa2e0d2bc84.png?wh=1920x416"
        data-srcset="https://static001.geekbang.org/resource/image/b8/84/b85c5c010689b1e3b7075aa2e0d2bc84.png?wh=1920x416, https://static001.geekbang.org/resource/image/b8/84/b85c5c010689b1e3b7075aa2e0d2bc84.png?wh=1920x416 1.5x, https://static001.geekbang.org/resource/image/b8/84/b85c5c010689b1e3b7075aa2e0d2bc84.png?wh=1920x416 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b8/84/b85c5c010689b1e3b7075aa2e0d2bc84.png?wh=1920x416"
        title="img" /></p>
<p>你需要注意它的形式，第一个数字“3”是序号，意思是第 3 号设备，“@if45”就是它另一端连接的虚拟网卡，序号是 45。
因为这个 Pod 的宿主机是 master，我们就要登录到 master 节点，看看这个节点上的网络情况，同样还是用命令 ip addr：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/bb/e9/bb8342853bab79aea1842eae5f48bde9.png?wh=1920x389"
        data-srcset="https://static001.geekbang.org/resource/image/bb/e9/bb8342853bab79aea1842eae5f48bde9.png?wh=1920x389, https://static001.geekbang.org/resource/image/bb/e9/bb8342853bab79aea1842eae5f48bde9.png?wh=1920x389 1.5x, https://static001.geekbang.org/resource/image/bb/e9/bb8342853bab79aea1842eae5f48bde9.png?wh=1920x389 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/bb/e9/bb8342853bab79aea1842eae5f48bde9.png?wh=1920x389"
        title="img" /></p>
<p>这里就可以看到宿主机（master）节点上的第 45 号设备了，它的名字是 veth41586979@if3，“veth”表示它是一个虚拟网卡，而后面的“@if3”就是 Pod 里对应的 3 号设备，也就是“eth0”网卡了。
那么“cni0”网桥的信息该怎么查看呢？这需要在宿主机（master）上使用命令 brctl show：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/13/b3/13563817d53f094fe6fd6d734c7c49b3.png?wh=1920x387"
        data-srcset="https://static001.geekbang.org/resource/image/13/b3/13563817d53f094fe6fd6d734c7c49b3.png?wh=1920x387, https://static001.geekbang.org/resource/image/13/b3/13563817d53f094fe6fd6d734c7c49b3.png?wh=1920x387 1.5x, https://static001.geekbang.org/resource/image/13/b3/13563817d53f094fe6fd6d734c7c49b3.png?wh=1920x387 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/13/b3/13563817d53f094fe6fd6d734c7c49b3.png?wh=1920x387"
        title="img" /></p>
<p>从这张截图里，你可以发现“cni0”网桥上有 4 个虚拟网卡，第三个就是“veth41586979”，所以这个网卡就被“插”在了“cni0”网桥上，然后因为虚拟网卡的“结对”特性，Pod 也就连上了“cni0”网桥。
单纯用 Linux 命令不太容易看清楚网卡和网桥的联系，所以我把它们整合在了下面的图里，加上了虚线标记，这样你就能更清晰地理解 Pod、veth 和 cni0 的引用关系了：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/e3/14/e3c4f523cee0e39b74e94d1b96e5a014.jpg?wh=1920x1176"
        data-srcset="https://static001.geekbang.org/resource/image/e3/14/e3c4f523cee0e39b74e94d1b96e5a014.jpg?wh=1920x1176, https://static001.geekbang.org/resource/image/e3/14/e3c4f523cee0e39b74e94d1b96e5a014.jpg?wh=1920x1176 1.5x, https://static001.geekbang.org/resource/image/e3/14/e3c4f523cee0e39b74e94d1b96e5a014.jpg?wh=1920x1176 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/e3/14/e3c4f523cee0e39b74e94d1b96e5a014.jpg?wh=1920x1176"
        title="img" /></p>
<p>使用同样的方式，你可以知道另一个 Pod “10.10.0.4”的网卡是 veth2b3ef56d@if3，它也在“cni0”网桥上，所以借助这个网桥，本机的 Pod 就可以直接通信。
弄清楚了本机网络，我们再来看跨主机的网络，它的关键是节点的路由表，用命令 route 查看：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/df/39/df13160c3885b59233c0d90823cde239.png?wh=1920x489"
        data-srcset="https://static001.geekbang.org/resource/image/df/39/df13160c3885b59233c0d90823cde239.png?wh=1920x489, https://static001.geekbang.org/resource/image/df/39/df13160c3885b59233c0d90823cde239.png?wh=1920x489 1.5x, https://static001.geekbang.org/resource/image/df/39/df13160c3885b59233c0d90823cde239.png?wh=1920x489 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/df/39/df13160c3885b59233c0d90823cde239.png?wh=1920x489"
        title="img" /></p>
<p>它告诉我们有这些信息：
10.10.0.0/24 网段的数据，都要走 cni0 设备，也就是“cni0”网桥。
10.10.1.0/24 网段的数据，都要走 flannel.1 设备，也就是 Flannel。
192.168.10.0/24 网段的数据，都要走 ens160 设备，也就是我们宿主机的网卡。
假设我们要从 master 节点的“10.10.0.3”访问 worker 节点的“10.10.1.77”，因为 master 节点的“cni0”网桥管理的只是“10.10.0.0/24”这个网段，所以按照路由表，凡是“10.10.1.0/24”都要让 flannel.1 来处理，这样就进入了 Flannel 插件的工作流程。
然后 Flannel 就要来决定应该如何把数据发到另一个节点，在各种表里去查询。因为这个过程比较枯燥，我就不详细说了，你可以参考下面的示意图，用到的命令有 ip neighbor、bridge fdb 等等：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/8e/7b/8e2f69cb47cd0bf32e20a8420e9b577b.png?wh=1920x1112"
        data-srcset="https://static001.geekbang.org/resource/image/8e/7b/8e2f69cb47cd0bf32e20a8420e9b577b.png?wh=1920x1112, https://static001.geekbang.org/resource/image/8e/7b/8e2f69cb47cd0bf32e20a8420e9b577b.png?wh=1920x1112 1.5x, https://static001.geekbang.org/resource/image/8e/7b/8e2f69cb47cd0bf32e20a8420e9b577b.png?wh=1920x1112 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/8e/7b/8e2f69cb47cd0bf32e20a8420e9b577b.png?wh=1920x1112"
        title="img" /></p>
<p>Flannel 得到的结果就是要把数据发到“192.168.10.220”，也就是 worker 节点，所以它就会在原始网络包前面加上这些额外的信息，封装成 VXLAN 报文，用“ens160”网卡发出去，worker 节点收到后再拆包，执行类似的反向处理，就可以把数据交给真正的目标 Pod 了。</p>
<h3 id="使用-calico-网络插件">使用 Calico 网络插件</h3>
<p>看到这里，是不是觉得 Flannel 的 Overlay 处理流程非常复杂，绕来绕去很容易让人头晕，那下面我们就来看看另一个 Route 模式的插件 Calico。
你可以在 Calico 的网站（https://www.tigera.io/project-calico/）上找到它的安装方式，我选择的是“本地自助安装（Self-managed on-premises）”，可以直接下载 YAML 文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">wget https://projectcalico.docs.tigera.io/manifests/calico.yaml

</code></pre></td></tr></table>
</div>
</div><p>由于 Calico 使用的镜像较大，为了加快安装速度，可以考虑在每个节点上预先使用 docker pull 拉取镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">docker pull calico/cni:v3.23.1
docker pull calico/node:v3.23.1
docker pull calico/kube-controllers:v3.23.1
</code></pre></td></tr></table>
</div>
</div><p>Calico 的安装非常简单，只需要用 kubectl apply 就可以（记得安装之前最好把 Flannel 删除）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f calico.yaml

</code></pre></td></tr></table>
</div>
</div><p>安装之后我们来查看一下 Calico 的运行状态，注意它也是在“kube-system”名字空间：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/98/95/983c6a271a394d0febc83f21c108e195.png?wh=1520x304"
        data-srcset="https://static001.geekbang.org/resource/image/98/95/983c6a271a394d0febc83f21c108e195.png?wh=1520x304, https://static001.geekbang.org/resource/image/98/95/983c6a271a394d0febc83f21c108e195.png?wh=1520x304 1.5x, https://static001.geekbang.org/resource/image/98/95/983c6a271a394d0febc83f21c108e195.png?wh=1520x304 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/98/95/983c6a271a394d0febc83f21c108e195.png?wh=1520x304"
        title="img" /></p>
<p>我们仍然创建 3 个 Nginx Pod 来做实验：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create deploy ngx-dep --image=nginx:alpine --replicas=3

</code></pre></td></tr></table>
</div>
</div><p>我们会看到 master 节点上有两个 Pod，worker 节点上有一个 Pod，但它们的 IP 地址与刚才 Flannel 的明显不一样了，分别是“10.10.219.<em>”和“10.10.171.</em>”，这说明 Calico 的 IP 地址分配策略和 Flannel 是不同的：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/aa/81/aa8917fd298cc633fbdf190e3a767581.png?wh=1920x262"
        data-srcset="https://static001.geekbang.org/resource/image/aa/81/aa8917fd298cc633fbdf190e3a767581.png?wh=1920x262, https://static001.geekbang.org/resource/image/aa/81/aa8917fd298cc633fbdf190e3a767581.png?wh=1920x262 1.5x, https://static001.geekbang.org/resource/image/aa/81/aa8917fd298cc633fbdf190e3a767581.png?wh=1920x262 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/aa/81/aa8917fd298cc633fbdf190e3a767581.png?wh=1920x262"
        title="img" /></p>
<p>然后我们来看看 Pod 里的网卡情况，你会发现虽然还是有虚拟网卡，但宿主机上的网卡名字变成了 calica17a7ab6ab@if4，而且并没有连接到“cni0”网桥上：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/75/69/756e9ea5e78489c386219722f302c969.jpg?wh=1920x1236"
        data-srcset="https://static001.geekbang.org/resource/image/75/69/756e9ea5e78489c386219722f302c969.jpg?wh=1920x1236, https://static001.geekbang.org/resource/image/75/69/756e9ea5e78489c386219722f302c969.jpg?wh=1920x1236 1.5x, https://static001.geekbang.org/resource/image/75/69/756e9ea5e78489c386219722f302c969.jpg?wh=1920x1236 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/75/69/756e9ea5e78489c386219722f302c969.jpg?wh=1920x1236"
        title="img" /></p>
<p>这是不是很奇怪？
其实这是 Calico 的工作模式导致的正常现象。因为 Calico 不是 Overlay 模式，而是 Route 模式，所以它就没有用 Flannel 那一套，而是在宿主机上创建路由规则，让数据包不经过网桥直接“跳”到目标网卡去。
来看一下节点上的路由表就能明白：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/0f/d3/0f94b581ff1e72b08103bfbe900e53d3.png?wh=1920x663"
        data-srcset="https://static001.geekbang.org/resource/image/0f/d3/0f94b581ff1e72b08103bfbe900e53d3.png?wh=1920x663, https://static001.geekbang.org/resource/image/0f/d3/0f94b581ff1e72b08103bfbe900e53d3.png?wh=1920x663 1.5x, https://static001.geekbang.org/resource/image/0f/d3/0f94b581ff1e72b08103bfbe900e53d3.png?wh=1920x663 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/0f/d3/0f94b581ff1e72b08103bfbe900e53d3.png?wh=1920x663"
        title="img" /></p>
<p>假设 Pod A“10.10.219.67”要访问 Pod B“10.10.219.68”，那么查路由表，知道要走“cali051dd144e34”这个设备，而它恰好就在 Pod B 里，所以数据就会直接进 Pod B 的网卡，省去了网桥的中间步骤。
Calico 的网络架构我也画了一张示意图，你可以再对比 Flannel 来学习：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/yy/c7/yyb9c0ee93730542ebb5475a734991c7.jpg?wh=1920x1012"
        data-srcset="https://static001.geekbang.org/resource/image/yy/c7/yyb9c0ee93730542ebb5475a734991c7.jpg?wh=1920x1012, https://static001.geekbang.org/resource/image/yy/c7/yyb9c0ee93730542ebb5475a734991c7.jpg?wh=1920x1012 1.5x, https://static001.geekbang.org/resource/image/yy/c7/yyb9c0ee93730542ebb5475a734991c7.jpg?wh=1920x1012 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/yy/c7/yyb9c0ee93730542ebb5475a734991c7.jpg?wh=1920x1012"
        title="img" /></p>
<p>至于在 Calico 里跨主机通信是如何路由的，你完全可以对照着路由表，一步步地“跳”到目标 Pod 去（提示：tunl0 设备）。</p>
<h3 id="小结-29">小结</h3>
<p>好说了这么多，你应该看到了，Kubernetes 的整个网络数据传输过程有大量的细节，非常多的环节都参与其中，想把它彻底弄明白还真不是件容易的事情。
不过好在 CNI 通过“依赖倒置”的原则把这些工作都交给插件去解决了，不管下层是什么样的环境，不管插件是怎么实现的，我们在 Kubernetes 集群里只会有一个干净、整洁的网络空间。
我来简单小结一下今天的内容：
Kubernetes 使用的是“IP-per-pod”网络模型，每个 Pod 都会有唯一的 IP 地址，所以简单易管理。
CNI 是 Kubernetes 定义的网络插件接口标准，按照实现方式可以分成“Overlay”“Route”和“Underlay”三种，常见的 CNI 插件有 Flannel、Calico 和 Cilium。
Flannel 支持 Overlay 模式，它使用了 cni0 网桥和 flannel.1 设备，本机通信直接走 cni0，跨主机通信会把原始数据包封装成 VXLAN 包再走宿主机网卡发送，有性能损失。
Calico 支持 Route 模式，它不使用 cni0 网桥，而是创建路由规则，把数据包直接发送到目标网卡，所以性能高。</p>
<h2 id="32实战演练玩转kubernetes3">32｜实战演练：玩转Kubernetes（3）</h2>
<p>到今天，我们的“高级篇”课程也要结束了。比起前面的“初级篇”“中级篇”来说，这里的知识点比较多，难度也要高一些。如果你能够一篇不漏地学习下来，相信一定对 Kubernetes 有更深层次的认识和理解。
今天的这节课还是来对前面的知识做回顾与总结，提炼出文章里的学习要点和重点，你也可以顺便检验一下自己的掌握程度，试试在不回看课程的情况下，自己能不能流畅说出关联的操作细节。
复习之后，我们就来进行最后一次实战演练了。首先会继续改进贯穿课程始终的 WordPress 网站，把 MariaDB 改成 StatefulSet，加上 NFS 持久化存储；然后我们会在 Kubernetes 集群里安装 Dashboard，综合实践 Ingress、namespace 的用法。</p>
<h3 id="要点回顾一api-对象">要点回顾一：API 对象</h3>
<p>“高级篇”可以分成三个部分，第一部分讲的是 PersistentVolume、StatefulSet 等 API 对象。
（24 讲）PersistentVolume 简称 PV，是 Kubernetes 对持久化存储的抽象，代表了 LocalDisk、NFS、Ceph 等存储设备，和 CPU、内存一样，属于集群的公共资源。
因为不同存储设备之间的差异很大，为了更好地描述 PV 特征，就出现了 StorageClass，它的作用是分类存储设备，让我们更容易去选择 PV 对象。
PV 一般由系统管理员来创建，我们如果要使用 PV 就要用 PVC（PersistentVolumeClaim）去申请，说清楚需求的容量、访问模式等参数，然后 Kubernetes 就会查找最合适的 PV 分配给我们使用。
（25 讲）手动创建 PV 的工作量很大，麻烦而且容易出错，所以就有了“动态存储卷”的概念，需要在 StorageClass 里绑定一个 Provisioner 对象，由它来代替人工，根据 PVC 自动创建出符合要求的 PV。
有了 PV 和 PVC，我们就可以在 Pod 里用“persistentVolumeClaim”来引用 PVC，创建出可供容器使用的 Volume，然后在容器里用“volumeMounts”把它挂载到某个路径上，这样容器就可以读写 PV，实现数据的持久化存储了。
（26 讲）持久化存储的一个重要应用领域就是保存应用的状态数据，管理有状态的应用，就要使用新的对象 StatefulSet，可以认为它是管理无状态应用对象 Deployment 的一个特例。
StatefulSet 对象的 YAML 描述和 Deployment 非常像，“spec”里只是多了一个“serviceName”字段，但它部署应用的方式却与 Deployment 差距很大。
Deployment 创建的 Pod 是随机的名字，而 StatefulSet 会对 Pod 顺序编号、顺序创建，保证应用有一个确定的启动先后次序，这样就可以实现主从、主备等关系。
在使用 Service 为 StatefulSet 创建服务的时候，它也会为每个 Pod 单独创建域名，同样也是顺序编号，保证 Pod 有稳定的网络标识，外部用户就可以用这个域名来准确地访问到某个具体的 Pod。
StatefulSet 还使用“volumeClaimTemplates”字段来定义持久化存储，里面其实就是一个 PVC，每个 Pod 可以用这个模板来生成自己的 PVC 去申请 PV，实现存储卷与 Pod 的独立绑定。
通过启动顺序、稳定域名和存储模板这三个关键能力，StatefulSet 就可以很好地处理 Redis、MySQL 等有状态应用了。</p>
<h3 id="要点回顾二应用管理">要点回顾二：应用管理</h3>
<p>“高级篇”第二部分讲的是应用管理，包括滚动更新、资源配额和健康检查等内容。
（27 讲）在 Kubernetes 里部署好应用后，我们还需要对它做持续的运维管理，其中一项任务是版本的更新和回退。
版本更新很简单，只要编写一个新的 YAML（Deployment、DaemonSet、StatefulSet），再用 kubectl apply 应用就可以了。Kubernetes 采用的是“滚动更新”策略，实际上是两个同步进行的“扩容”和“缩容”动作，这样在更新的过程中始终会有 Pod 处于可用状态，能够平稳地对外提供服务。
应用的更新历史可以用命令 kubectl rollout history 查看，如果有什么意外，就可以用 kubectl rollout undo 来回退。这两个命令相当于给我们的更新流程上了一个保险，可以放心大胆操作，失败就用“S/L 大法”。
（28 讲）为了让 Pod 里的容器能够稳定运行，我们可以采用资源配额和检查探针这两种手段。
资源配额能够限制容器申请的 CPU 和内存数量，不至于过多或者过少，保持在一个合理的程度，更有利于 Kubernetes 调度。
检查探针是 Kubernetes 内置的应用监控工具，有 Startup、Liveness、Readiness 三种，分别探测启动、存活、就绪状态，探测的方式也有 exec、tcpSocket、httpGet 三种。组合运用这些就可以灵活地检查容器的状态，Kubernetes 发现不可用就会重启容器，让应用在总体上处于健康水平。</p>
<h3 id="要点回顾三集群管理">要点回顾三：集群管理</h3>
<p>“高级篇”第三部分讲的是集群管理，有名字空间、系统监控和网络通信等知识点。
（29 讲）Kubernetes 的集群里虽然有很多计算资源，但毕竟是有限的，除了要给 Pod 加上资源配额，我们也要为集群加上资源配额，方法就是用名字空间，把整体的资源池切分成多个小块，按需分配给不同的用户使用。
名字空间的资源配额使用的是“ResourceQuota”，除了基本的 CPU 和内存，它还能够限制存储容量和各种 API 对象的数量，这样就可以避免多用户互相挤占，更高效地利用集群资源。
（30 讲）系统监控是集群管理的另一个重要方面，Kubernetes 提供了 Metrics Server 和 Prometheus 两个工具：
Metrics Server 专门用来收集 Kubernetes 核心资源指标，可以用 kubectl top 来查看集群的状态，它也是水平自动伸缩对象 HorizontalPodAutoscaler 的前提条件。
Prometheus，继 Kubernetes 之后的第二个 CNCF 毕业项目，是云原生监控领域的“事实标准”，在集群里部署之后就可以用 Grafana 可视化监控各种指标，还可以集成自动报警等功能。
（31 讲）对于底层的基础网络设施，Kubernetes 定义了平坦的网络模型“IP-per-pod”，实现它就要符合 CNI 标准。常用的网络插件有 Flannel、Calico、Cilium 等，Flannel 使用 Overlay 模式，性能较低，Calico 使用 Route 模式，性能较高。
现在，“高级篇”的众多知识要点我们都完整地过了一遍，你是否已经都理解、掌握了它们呢？</p>
<h3 id="搭建-wordpress-网站-1">搭建 WordPress 网站</h3>
<p>接下来我们就来在第 22 讲的基础上继续优化 WordPress 网站，其中的关键是让数据库 MariaDB 实现数据持久化。
网站的整体架构图变化不大，前面的 Nginx、WordPress 还是原样，只需要修改 MariaDB：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/7c/1b/7cd3726d03ae12172b9073d1abf9fe1b.jpg?wh=1920x967"
        data-srcset="https://static001.geekbang.org/resource/image/7c/1b/7cd3726d03ae12172b9073d1abf9fe1b.jpg?wh=1920x967, https://static001.geekbang.org/resource/image/7c/1b/7cd3726d03ae12172b9073d1abf9fe1b.jpg?wh=1920x967 1.5x, https://static001.geekbang.org/resource/image/7c/1b/7cd3726d03ae12172b9073d1abf9fe1b.jpg?wh=1920x967 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/7c/1b/7cd3726d03ae12172b9073d1abf9fe1b.jpg?wh=1920x967"
        title="img" /></p>
<p>因为 MariaDB 由 Deployment 改成了 StatefulSet，所以我们要修改 YAML，添加“serviceName”“volumeClaimTemplates”这两个字段，定义网络标识和 NFS 动态存储卷，然后在容器部分用“volumeMounts”挂载到容器里的数据目录“/var/lib/mysql”。
修改后的 YAML 就是这个样子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: maria-sts
  name: maria-sts
spec:

  # headless svc

  serviceName: maria-svc

  # pvc

  volumeClaimTemplates:

  - metadata:
    name: maria-100m-pvc
    spec:
      storageClassName: nfs-client
      accessModes:

       - ReadWriteMany
         resources:
             requests:
         storage: 100Mi
         replicas: 1
         selector:
         matchLabels:
           app: maria-sts
         template:
         metadata:
           labels:
             app: maria-sts
         spec:
           containers:

      - image: mariadb:10
        name: mariadb
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 3306
          envFrom:
        - prefix: &#39;MARIADB_&#39;
          configMapRef:
            name: maria-cm
          volumeMounts:
        - name: maria-100m-pvc
          mountPath: /var/lib/mysql
</code></pre></td></tr></table>
</div>
</div><p>改完 MariaDB，我们还要再对 WordPress 做一点小修改。
还记得吗？StatefulSet 管理的每个 Pod 都有自己的域名，所以要把 WordPress 的环境变量改成 MariaDB 的新名字，也就是“maria-sts-0.maria-svc”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> apiVersion: v1
  kind: ConfigMap
  metadata:
name: wp-cm
  data:
HOST: &#39;maria-sts-0.maria-svc&#39;  #注意这里
USER: &#39;wp&#39;
PASSWORD: &#39;123&#39;
NAME: &#39;db&#39;
</code></pre></td></tr></table>
</div>
</div><p>改完这两个 YAML，我们就可以逐个创建 MariaDB、WordPress、Ingress 等对象了。
和之前一样，访问 NodePort 的“30088”端口，或者是用 Ingress Controller 的“wp.test”域名，都可以进入 WordPress 网站：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/fc/46/fc3b52f96f138f01b23e3a7487730746.png?wh=916x1166"
        data-srcset="https://static001.geekbang.org/resource/image/fc/46/fc3b52f96f138f01b23e3a7487730746.png?wh=916x1166, https://static001.geekbang.org/resource/image/fc/46/fc3b52f96f138f01b23e3a7487730746.png?wh=916x1166 1.5x, https://static001.geekbang.org/resource/image/fc/46/fc3b52f96f138f01b23e3a7487730746.png?wh=916x1166 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/fc/46/fc3b52f96f138f01b23e3a7487730746.png?wh=916x1166"
        title="img" /></p>
<p>StatefulSet 的持久化存储是否生效了呢？
你可以把这些对象都删除后重新创建，再进入网站，看看是否原来的数据依然存在。或者更简单一点，直接查看 NFS 的存储目录，应该可以看到 MariaDB 生成的一些数据库文件：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/42/18/428886b77e4797dc7ded5a43yyc0b218.png?wh=1920x124"
        data-srcset="https://static001.geekbang.org/resource/image/42/18/428886b77e4797dc7ded5a43yyc0b218.png?wh=1920x124, https://static001.geekbang.org/resource/image/42/18/428886b77e4797dc7ded5a43yyc0b218.png?wh=1920x124 1.5x, https://static001.geekbang.org/resource/image/42/18/428886b77e4797dc7ded5a43yyc0b218.png?wh=1920x124 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/42/18/428886b77e4797dc7ded5a43yyc0b218.png?wh=1920x124"
        title="img" /></p>
<p>这两种方式都能够证明，我们的 MariaDB 使用 StatefulSet 部署后数据已经保存在了磁盘上，不会因为对象的销毁而丢失。
到这里，第一个小实践你就已经完成了，给自己鼓鼓劲，我们一起来做第二个实践，在 Kubernetes 集群里安装 Dashboard。</p>
<h3 id="部署-dashboard">部署 Dashboard</h3>
<p>在“初级篇”的实战演练课里（第 15 讲），我简单介绍了 Kubernetes 的图形管理界面，也就是 Dashboard，不知道你是否还有印象。当时 Dashboard 是直接内置在 minikube 里的，不需要安装，一个命令启动，就能在浏览器里直观地管理 Kubernetes 集群了，非常方便。
那现在我们用 kubeadm 部署了实际的多节点集群，能否也用上 Dashboard 呢？接下来我就带你来一起动手，从零开始安装 Dashboard。
首先，你应该先去 Dashboard 的项目网站（https://github.com/kubernetes/dashboard），看一下它的说明文档，了解一下它的基本情况。
它的安装很简单，只需要一个 YAML 文件，可以直接下载：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.0/aio/deploy/recommended.yaml

</code></pre></td></tr></table>
</div>
</div><p>这个 YAML 里包含了很多对象，虽然文件比较大，但现在的你应该基本都能够看懂了，要点有这么几个：
所有的对象都属于“kubernetes-dashboard”名字空间。
Dashboard 使用 Deployment 部署了一个实例，端口号是 8443。
容器启用了 Liveness 探针，使用 HTTPS 方式检查存活状态。
Service 对象使用的是 443 端口，它映射了 Dashboard 的 8443 端口。
使用命令 kubectl apply 就可以轻松部署 Dashboard 了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f dashboard.yaml
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c5/79/c56f8936e187047a2b7d100f7ae0f779.png?wh=1586x250"
        data-srcset="https://static001.geekbang.org/resource/image/c5/79/c56f8936e187047a2b7d100f7ae0f779.png?wh=1586x250, https://static001.geekbang.org/resource/image/c5/79/c56f8936e187047a2b7d100f7ae0f779.png?wh=1586x250 1.5x, https://static001.geekbang.org/resource/image/c5/79/c56f8936e187047a2b7d100f7ae0f779.png?wh=1586x250 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c5/79/c56f8936e187047a2b7d100f7ae0f779.png?wh=1586x250"
        title="img" /></p>
<h3 id="部署-ingressingress-controller">部署 Ingress/Ingress Controller</h3>
<p>不过，为了给我们的实战增加一点难度，我们可以在前面配一个 Ingress 入口，用反向代理的方式来访问它。
由于 Dashboard 默认使用的是加密的 HTTPS 协议，拒绝明文 HTTP 访问，所以我们要先生成证书，让 Ingress 也走 HTTPS 协议。
简单起见，我直接用 Linux 里的命令行工具“openssl”来生成一个自签名的证书（如果你有条件，也可以考虑找 CA 网站申请免费证书）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">openssl req -x509 -days 365 -out k8s.test.crt -keyout k8s.test.key \
  -newkey rsa:2048 -nodes -sha256 \
    -subj &#39;/CN=k8s.test&#39; -extensions EXT -config &lt;( \
       printf &#34;[dn]\nCN=k8s.test\n[req]\ndistinguished_name = dn\n[EXT]\nsubjectAltName=DNS:k8s.test\nkeyUsage=digitalSignature\nextendedKeyUsage=serverAuth&#34;)
</code></pre></td></tr></table>
</div>
</div><p>openssl 的命令比较长，我简单解释一下：它生成的是一个 X509 格式的证书，有效期 365 天，私钥是 RSA2048 位，摘要算法是 SHA256，签发的网站是“k8s.test”。
运行命令行后会生成两个文件，一个是证书“k8s.test.crt”，另一个是私钥“k8s.test.key”，我们需要把这两个文件存入 Kubernetes 里供 Ingress 使用。
因为这两个文件属于机密信息，存储的方式当然就是用 Secret 了。你仍然可以用命令 kubectl create secret 来自动创建 YAML，不过类型不是“generic”，而是“tls”，同时还要用 -n 指定名字空间，用 &ndash;cert、&ndash;key 指定文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">export out=&#34;--dry-run=client -o yaml&#34;
kubectl create secret tls dash-tls -n kubernetes-dashboard --cert=k8s.test.crt --key=k8s.test.key $out &gt; cert.yml
</code></pre></td></tr></table>
</div>
</div><p>出来的 YAML 大概是这个样子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Secret
metadata:
  name: dash-tls
  namespace: kubernetes-dashboard
type: kubernetes.io/tls
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJU...
  tls.key: LS0tLS1CRUdJTiBQUklW...
</code></pre></td></tr></table>
</div>
</div><p>创建这个 Secret 对象之后，你可以再用 kubectl describe 来检查它的状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/26/cc/2615d5c6c3yy704cc63c5bf6df5b87cc.png?wh=1904x716"
        data-srcset="https://static001.geekbang.org/resource/image/26/cc/2615d5c6c3yy704cc63c5bf6df5b87cc.png?wh=1904x716, https://static001.geekbang.org/resource/image/26/cc/2615d5c6c3yy704cc63c5bf6df5b87cc.png?wh=1904x716 1.5x, https://static001.geekbang.org/resource/image/26/cc/2615d5c6c3yy704cc63c5bf6df5b87cc.png?wh=1904x716 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/26/cc/2615d5c6c3yy704cc63c5bf6df5b87cc.png?wh=1904x716"
        title="img" /></p>
<p>接下来我们就来编写 Ingress Class 和 Ingress 对象，为了保持名字空间的整齐，也把它放在“kubernetes-dashboard”名字空间里。
Ingress Class 对象很简单，名字是“dash-ink”，指定 Controller 还是我们之前用的 Nginx 官方的 Ingress Controller：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: dash-ink
  namespace: kubernetes-dashboard
spec:
  controller: nginx.org/ingress-controller
</code></pre></td></tr></table>
</div>
</div><p>Ingress 对象可以用 kubectl create 命令自动生成，如果你有点忘记的话，可以回头参考一下第 21 讲：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create ing dash-ing --rule=&#34;k8s.test/=kubernetes-dashboard:443&#34; --class=dash-ink -n kubernetes-dashboard $out

</code></pre></td></tr></table>
</div>
</div><p>但这次因为是 HTTPS 协议，所以我们要在 Ingress 里多加一点东西，一个是“annotations”字段，指定后端目标是 HTTPS 服务，另一个是“tls”字段，指定域名和证书，也就是刚才创建的 Secret：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: dash-ing
  namespace: kubernetes-dashboard
  annotations:
    nginx.org/ssl-services: &#34;kubernetes-dashboard&#34;
spec:
  ingressClassName: dash-ink
  tls:
    - hosts:
      - k8s.test
      secretName: dash-tls
  rules:

  - host: k8s.test
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443
</code></pre></td></tr></table>
</div>
</div><p>最后一个对象，就是 Ingress Controller 了，还是拿现成的模板修改，记得要把“args”里的 Ingress Class 改成我们自己的“dash-ink”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: apps/v1
kind: Deployment
metadata:
  name: dash-kic-dep
  namespace: nginx-ingress
spec:
  ...
        args:
          - -ingress-class=dash-ink
</code></pre></td></tr></table>
</div>
</div><p>要让我们在外面能够访问 Ingress Controller，还要为它再定义一个 Service，类型是“NodePort”，端口指定是“30443”：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">  apiVersion: v1
  kind: Service
  metadata:
name: dash-kic-svc
namespace: nginx-ingress
  spec:
ports:

  - port: 443
    protocol: TCP
    targetPort: 443
    nodePort: 30443
    selector:
    app: dash-kic-dep
    type: NodePort
</code></pre></td></tr></table>
</div>
</div><p>把上面的 Secret、Ingress Class、Ingress、Ingress Controller、Service 都创建好之后，我们再来确认一下它们的运行状态：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/40/b2/4062d4e5c8c57f74a480ee21ca3717b2.png?wh=1920x821"
        data-srcset="https://static001.geekbang.org/resource/image/40/b2/4062d4e5c8c57f74a480ee21ca3717b2.png?wh=1920x821, https://static001.geekbang.org/resource/image/40/b2/4062d4e5c8c57f74a480ee21ca3717b2.png?wh=1920x821 1.5x, https://static001.geekbang.org/resource/image/40/b2/4062d4e5c8c57f74a480ee21ca3717b2.png?wh=1920x821 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/40/b2/4062d4e5c8c57f74a480ee21ca3717b2.png?wh=1920x821"
        title="img" /></p>
<p>因为这些对象比较多，处于不同的名字空间，关联有点复杂，我画了一个简单的示意图，你可以看一下：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/b7/50/b720648a0fefab28fa940b7cd6afb350.jpg?wh=1920x793"
        data-srcset="https://static001.geekbang.org/resource/image/b7/50/b720648a0fefab28fa940b7cd6afb350.jpg?wh=1920x793, https://static001.geekbang.org/resource/image/b7/50/b720648a0fefab28fa940b7cd6afb350.jpg?wh=1920x793 1.5x, https://static001.geekbang.org/resource/image/b7/50/b720648a0fefab28fa940b7cd6afb350.jpg?wh=1920x793 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/b7/50/b720648a0fefab28fa940b7cd6afb350.jpg?wh=1920x793"
        title="img" /></p>
<h3 id="访问-dashboard">访问 Dashboard</h3>
<p>到这里，Dashboard 的部署工作就基本完成了。为了能正常访问，我们还要为它创建一个用户，才能登录进 Dashboard。
Dashboard 的网站上有一个简单示例（https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md），我们直接拿来用就行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user

  namespace: kubernetes-dashboard
---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:

- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
</code></pre></td></tr></table>
</div>
</div><p>这个 YAML 创建了一个 Dashboard 的管理员账号，名字叫“admin-user”，使用的是 Kubernetes 的 RBAC 机制，就不展开细讲了。
这个账号不能用简单的“用户名 + 密码”的方式登录，需要用到一个 Token，可以用 kubectl get secret、kubectl describe secret 查到：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get secret -n kubernetes-dashboard
kubectl describe secrets -n kubernetes-dashboard admin-user-token-xxxx
</code></pre></td></tr></table>
</div>
</div><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/0f/yy/0ffd4627b0efa2ba5774bf5c65faa1yy.png?wh=1920x1051"
        data-srcset="https://static001.geekbang.org/resource/image/0f/yy/0ffd4627b0efa2ba5774bf5c65faa1yy.png?wh=1920x1051, https://static001.geekbang.org/resource/image/0f/yy/0ffd4627b0efa2ba5774bf5c65faa1yy.png?wh=1920x1051 1.5x, https://static001.geekbang.org/resource/image/0f/yy/0ffd4627b0efa2ba5774bf5c65faa1yy.png?wh=1920x1051 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/0f/yy/0ffd4627b0efa2ba5774bf5c65faa1yy.png?wh=1920x1051"
        title="img" />Token 是一个很长的字符串，把它拷贝存好，再为它的测试域名“k8s.test”加上域名解析（修改 /etc/hosts)，然后我们就可以在浏览器里输入网址“https://k8s.test:30443”访问 Dashboard 了：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/c8/5d/c83cd71ab4d6696f5b837ea20056ff5d.png?wh=1920x958"
        data-srcset="https://static001.geekbang.org/resource/image/c8/5d/c83cd71ab4d6696f5b837ea20056ff5d.png?wh=1920x958, https://static001.geekbang.org/resource/image/c8/5d/c83cd71ab4d6696f5b837ea20056ff5d.png?wh=1920x958 1.5x, https://static001.geekbang.org/resource/image/c8/5d/c83cd71ab4d6696f5b837ea20056ff5d.png?wh=1920x958 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/c8/5d/c83cd71ab4d6696f5b837ea20056ff5d.png?wh=1920x958"
        title="img" /></p>
<p>下面的两张截图就是我查看集群里“kube-system”名字空间的情况，由于我们之前安装了 Metrics Server，所以 Dashboard 也能够以图形的方式显示 CPU 和内存状态，有那么一点 Prometheus + Grafana 的意思：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/3c/d9/3ca6e156150a6a06477bb2eb07e00cd9.png?wh=1920x1243"
        data-srcset="https://static001.geekbang.org/resource/image/3c/d9/3ca6e156150a6a06477bb2eb07e00cd9.png?wh=1920x1243, https://static001.geekbang.org/resource/image/3c/d9/3ca6e156150a6a06477bb2eb07e00cd9.png?wh=1920x1243 1.5x, https://static001.geekbang.org/resource/image/3c/d9/3ca6e156150a6a06477bb2eb07e00cd9.png?wh=1920x1243 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/3c/d9/3ca6e156150a6a06477bb2eb07e00cd9.png?wh=1920x1243"
        title="img" /></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/fa/b2/fae2168c30677d2370e8e71c3d98f1b2.png?wh=1920x1243"
        data-srcset="https://static001.geekbang.org/resource/image/fa/b2/fae2168c30677d2370e8e71c3d98f1b2.png?wh=1920x1243, https://static001.geekbang.org/resource/image/fa/b2/fae2168c30677d2370e8e71c3d98f1b2.png?wh=1920x1243 1.5x, https://static001.geekbang.org/resource/image/fa/b2/fae2168c30677d2370e8e71c3d98f1b2.png?wh=1920x1243 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/fa/b2/fae2168c30677d2370e8e71c3d98f1b2.png?wh=1920x1243"
        title="img" /></p>
<h3 id="小结-30">小结</h3>
<p>好了，今天我们一起回顾了“高级篇”里的要点，下面的这张思维导图就是对这些知识点的全面总结，你可以再认真研究一下：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/4a/30/4a9bb79b2e54096yyf5c5799837dd930.jpg?wh=1920x1312"
        data-srcset="https://static001.geekbang.org/resource/image/4a/30/4a9bb79b2e54096yyf5c5799837dd930.jpg?wh=1920x1312, https://static001.geekbang.org/resource/image/4a/30/4a9bb79b2e54096yyf5c5799837dd930.jpg?wh=1920x1312 1.5x, https://static001.geekbang.org/resource/image/4a/30/4a9bb79b2e54096yyf5c5799837dd930.jpg?wh=1920x1312 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/4a/30/4a9bb79b2e54096yyf5c5799837dd930.jpg?wh=1920x1312"
        title="img" /></p>
<p>今天我们有两个实战项目。首先是 WordPress，把后端的存储服务 MariaDB 改造成了 StatefulSet，挂载了 NFS 网盘，这样就实现了一个功能比较完善的网站，达到了基本可用的程度。
接着我们又在 Kubernetes 里安装了 Dashboard，主要部署在名字空间“kubernetes-dashboard”。Dashboard 自身的安装很简单，但我们又为它在前面搭建了一个反向代理，配上了安全证书，进一步实践了 Ingress 的用法。
不过这两个项目还没有完全覆盖“高级篇”的内容，你可以再接着改进它们，比如加上健康检查、资源配额、自动水平伸缩等，多动手来巩固所学的知识。</p>
<h2 id="33视频高级篇实操总结">33｜视频：高级篇实操总结</h2>
<p><a href="https://time.geekbang.org/column/article/558027" target="_blank" rel="noopener noreffer">https://time.geekbang.org/column/article/558027</a></p>
<h2 id="最后">最后</h2>
<p>接下来我就来说说四个可能的方向吧，你可以把它们看成是学习 Kubernetes 的“攻略指引”，帮助你走出属于自己的路。
第一个是阅读 Kubernetes 官网上的文档。
Kubernetes 官网（https://kubernetes.io/zh-cn/docs/home/）里的资料非常丰富详细，包括入门介绍、安装指导、基本概念、应用教程、运维任务、参考手册等等。
当然了，官网文档不是完全面向初学者的，不像我们的课程那样“循序渐进”，写得也不都是那么通俗易懂，要有一定的基础才能够看得下去。但它的优势就是全面、权威，覆盖了 Kubernetes 的每一个特性，你对 Kubernetes 有任何的疑惑和不解，都能够在这些文档里找到答案。
不过官网文档太多太杂也对我们的学习造成了困难，想要去按部就班地查找知识点会很麻烦，这个时候就要善用它的搜索功能了，用关键字来快速定位文章、页面，节约我们的时间和精力。</p>
<p>第二个学习方向是看 Kubernetes 的博客。
官网上的文档只是描述了 Kubernetes 的现状，而没有讲它的历史，想要知道 Kubernetes 里的这些 API 对象是怎么设计出来的，怎么一步步发展到今天的这个样子，就要去看它的技术博客文章了。
这里我推荐你去阅读英文博客（https://kubernetes.io/blog/），虽然中文官网也有博客，但翻译的不全，比较少，而英文博客从 2015 年开始，每个重要特性的变更几乎都有文章来介绍。而且博客和文档不同，它更注重面对普通用户，阐述的是技术决策的思考过程，也就更容易理解一些。
如果条件允许的话，我建议你从 2015 年的第一篇博客开始看起，最好每篇都简略地过一遍。把这些博客全看完，“以史为鉴”，你就能够理解 Kubernetes 的演变过程了，也会对 Kubernetes 的现状有更深刻的认识。</p>
<p>第三个是上 CNCF 网站（https://www.cncf.io/），看它的全景图，在里面找自己感兴趣的项目，然后在 Kubernetes 环境里部署应用起来，在实践中学习 Kubernetes。
CNCF 全景图里的项目非常多，其中由它托管的项目又分成毕业（Graduated）项目、孵化（Incubating）项目和沙盒（Sandbox）项目。
其实这些项目只要进入了 CNCF，质量都是比较高的，区别只在于成熟度的不同而已。毕业项目是最成熟的，已经被业界广泛承认和采用，可用于生产环境；孵化项目应用程度还不太广，贡献者也不是太多，只有少数生产实践；而沙盒项目则属于实验性质，还没有经过充分的测试验证。
这里我们也可以来简单了解下毕业项目和孵化项目，课后你可以挑自己感兴趣的深入研究。
这张图是目前 CNCF 里全部的 16 个毕业项目：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/ea/63/ea3b74e35e092477d06b5e2812b58363.png?wh=1878x1406"
        data-srcset="https://static001.geekbang.org/resource/image/ea/63/ea3b74e35e092477d06b5e2812b58363.png?wh=1878x1406, https://static001.geekbang.org/resource/image/ea/63/ea3b74e35e092477d06b5e2812b58363.png?wh=1878x1406 1.5x, https://static001.geekbang.org/resource/image/ea/63/ea3b74e35e092477d06b5e2812b58363.png?wh=1878x1406 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/ea/63/ea3b74e35e092477d06b5e2812b58363.png?wh=1878x1406"
        title="https://static001.geekbang.org/resource/image/ea/63/ea3b74e35e092477d06b5e2812b58363.png?wh=1878x1406" /></p>
<p>这里面我们已经全面学习了 Kubernetes，简单介绍过 containerd 和 Prometheus，其他我个人比较感兴趣的还有 Harbor、Helm、Vitess。
CNCF 的孵化项目目前有 39 个，比起毕业项目它们的知名度要略差一些，这个截图列出了一部分，其中我比较感兴趣的有 gRPC、SPIRE、NATS、OpenTelemetry：<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://static001.geekbang.org/resource/image/5e/a5/5e50b691b0097dde219f5b7b214903a5.png?wh=1638x1578"
        data-srcset="https://static001.geekbang.org/resource/image/5e/a5/5e50b691b0097dde219f5b7b214903a5.png?wh=1638x1578, https://static001.geekbang.org/resource/image/5e/a5/5e50b691b0097dde219f5b7b214903a5.png?wh=1638x1578 1.5x, https://static001.geekbang.org/resource/image/5e/a5/5e50b691b0097dde219f5b7b214903a5.png?wh=1638x1578 2x"
        data-sizes="auto"
        alt="https://static001.geekbang.org/resource/image/5e/a5/5e50b691b0097dde219f5b7b214903a5.png?wh=1638x1578"
        title="https://static001.geekbang.org/resource/image/5e/a5/5e50b691b0097dde219f5b7b214903a5.png?wh=1638x1578" /></p>
<p>第四个学习方向要量力而行，是参加 Kubernetes 的培训并且通过认证（https://kubernetes.io/zh-cn/training/）。
和很多其他的计算机技术一样，Kubernetes 也设立了官方的培训课程和资质认证，在国内大家都比较了解的应该就是 CKA（Certified Kubernetes Administrator）了，另外还有一个更高级的是 CKS（Certified Kubernetes Security Specialist）。
CKA 主要考查的是对 Kubernetes 的概念理解和集群管理维护能力，重点是动手操作，使用 kubectl 来解决各种实际环境里可能遇到的问题。它的难度并不太高，但考点覆盖面广，而且考试时间长达 2 个小时（以前是 3 个小时），对脑力和体力都有不小的挑战。
由于 Kubernetes 在云原生领域“一统天下”，CKA 认证近几年也就“火”了起来，相关的考试资料有很多，你可以轻易地在各大网站上找到，学完了我们的这个专栏课程，再适当地强化训练一下，拿到 CKA 证书应该不是什么太难的事情。
不过要注意的是，因为 Kubernetes 版本更新很频繁，所以 CKA 是有时效期的，三年（以前是两年）过后失效就得重考，你需要评估一下考试对自己收益再慎重做决定。</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2022-10-29 21:27:42&nbsp;<a class="git-hash" href="https://github.com/dillonzq/LoveIt/commit/59c69ca21b5c882e6fe4e7a51fc52106858b6432" target="_blank" title="commit by AdagioForSummerWind(2152343764@qq.com) 59c69ca21b5c882e6fe4e7a51fc52106858b6432: autofeat">
                                    <i class="fas fa-hashtag fa-fw"></i>59c69ca</a></span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://jefofrank.xyz/k8s_base/" data-title="K8s_base" data-hashtags="k8s"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://jefofrank.xyz/k8s_base/" data-hashtag="k8s"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Reddit" data-sharer="reddit" data-url="https://jefofrank.xyz/k8s_base/"><i class="fab fa-reddit fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://jefofrank.xyz/k8s_base/" data-title="K8s_base"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@2.14.0/icons/line.svg"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://jefofrank.xyz/k8s_base/" data-title="K8s_base"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="分享到 百度" data-sharer="baidu" data-url="https://jefofrank.xyz/k8s_base/" data-title="K8s_base"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@2.14.0/icons/baidu.svg"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/k8s/">k8s</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/interview/" class="prev" rel="prev" title="interview"><i class="fas fa-angle-left fa-fw"></i>interview</a>
            <a href="/k8s_advanced/" class="next" rel="next" title="K8s_advanced">K8s_advanced<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.89.0">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2021 - 2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://github.com/jf-011101" target="_blank">Jefo</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span><span class="icp-splitter">&nbsp;|&nbsp;</span><br class="icp-br"/>
                    <span class="icp"><a href="https://beian.miit.gov.cn/">赣ICP备2022007470号-1</a></span></br>
                <span id="busuanzi_container_site_pv">
                    访问量 <span id="busuanzi_value_site_pv"></span> 次
                </span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_uv">
                    访客数 <span id="busuanzi_value_site_uv"></span> 人次
                </span>
                </br><script>
                    function siteTime() {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = 2021;
                        var startMonth = 3;
                        var startDate = 27;
                        var startHour = 19;
                        var startMinute = 15;
                        var startSecond = 11;
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);
                        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                            minutes);
                        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                            diffMinutes * minutes) / seconds);
                        if (startYear == todayYear) {
                            
                            document.getElementById("sitetime").innerHTML = "已安全运行 " + diffDays + " 天 " + diffHours +
                                " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                        } else {
                            
                            document.getElementById("sitetime").innerHTML = "已安全运行 " + diffYears + " 年 " + diffDays +
                                " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                        }
                    }
                    setInterval(siteTime, 1000);
                </script>
                    <span id="sitetime">载入运行时间...</span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css"><script type="text/javascript" src="https://jefos-blog.disqus.com/embed.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.2.0/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@7.0.4/dist/typeit.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{},"data":{"id-1":"绿叶律动","id-2":"绿叶律动"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"J0OW8CCKJZ","algoliaIndex":"JF-2","algoliaSearchKey":"3b4a19e831c95174aca4c03fcdf95f5c","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
